<paper>
<cited id="M0">
<title id=" I08-6006.xml">statistical transliteration for cross language information retrieval using hmm alignment model and crf </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the practice of transcribing word or text written in one language into another language is called transliteration.
</prevsent>
<prevsent>a source language word can have more than one valid transliteration in target language.
</prevsent>
</prevsection>
<citsent citstr=" P00-1056 ">
for example for the hindi word below four different transliterations are possible . - gautam, gautham, gowtam, gowthamtherefore, in clir context, it becomes important to generate all possible transliterations to retrieve documents containing any of the given forms.most current transliteration systems use generative model for transliteration such as freely available giza++1 (och and ney , 2000),<papid> P00-1056 </papid>an implementation of the ibm alignment models (brown et al, 1993).<papid> J93-2003 </papid></citsent>
<aftsection>
<nextsent>these systems use giza++ (which uses hmm alignment) to get character level alignments (n-gram) from word aligned data.
</nextsent>
<nextsent>the transliteration system was built by counting up the alignments and converting the counts to conditional probabilities.
</nextsent>
<nextsent>the readers are strongly encouraged to refer to (nasreen and larkey , 2003) to have detailed understanding of this technique.
</nextsent>
<nextsent>in this paper, we present simple statistical technique for transliteration.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M1">
<title id=" I08-6006.xml">statistical transliteration for cross language information retrieval using hmm alignment model and crf </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the practice of transcribing word or text written in one language into another language is called transliteration.
</prevsent>
<prevsent>a source language word can have more than one valid transliteration in target language.
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
for example for the hindi word below four different transliterations are possible . - gautam, gautham, gowtam, gowthamtherefore, in clir context, it becomes important to generate all possible transliterations to retrieve documents containing any of the given forms.most current transliteration systems use generative model for transliteration such as freely available giza++1 (och and ney , 2000),<papid> P00-1056 </papid>an implementation of the ibm alignment models (brown et al, 1993).<papid> J93-2003 </papid></citsent>
<aftsection>
<nextsent>these systems use giza++ (which uses hmm alignment) to get character level alignments (n-gram) from word aligned data.
</nextsent>
<nextsent>the transliteration system was built by counting up the alignments and converting the counts to conditional probabilities.
</nextsent>
<nextsent>the readers are strongly encouraged to refer to (nasreen and larkey , 2003) to have detailed understanding of this technique.
</nextsent>
<nextsent>in this paper, we present simple statistical technique for transliteration.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M3">
<title id=" I08-6006.xml">statistical transliteration for cross language information retrieval using hmm alignment model and crf </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>prior work in arabic-english transliteration for machine translation purpose was done byarababi (arbabi et al, 1994).
</prevsent>
<prevsent>they developed hybrid neural network and knowledge-based system to generate multiple english spellings for arabic person names.
</prevsent>
</prevsection>
<citsent citstr=" P97-1017 ">
knight and graehl (knight and graehl , 1997) <papid> P97-1017 </papid>developed five stage statistical model to do back transliteration, that is, recover the original english name from its transliteration into japanese katakana.</citsent>
<aftsection>
<nextsent>stalls and knight (stalls and knight ,1998) adapted this approach for back transliteration from arabic to english of english names.
</nextsent>
<nextsent>al onaizan and knight (onaizan and knight , 2002)have produced simpler arabic/english translitera tor and evaluates how well their system can match source spelling.
</nextsent>
<nextsent>their work includes an evaluation of the transliterations in terms of their reasonableness according to human judges.
</nextsent>
<nextsent>none of these studies measures their performance on retrieval task or on other nlp tasks.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M4">
<title id=" I08-5004.xml">a hybrid named entity recognition system for south and south east asian languages </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>machine learning (ml) based approaches..
</prevsent>
<prevsent>the linguistic approaches typically use rules manually written by linguists.
</prevsent>
</prevsection>
<citsent citstr=" C96-1071 ">
there are several rule based ner systems, containing mainly lexicalized grammar, gazetteer lists, and list of trigger words, which are capable of providing 88%-92% f-measure accuracy for english (grishman, 1995; mcdonald, 1996; wakao et al, 1996).<papid> C96-1071 </papid>the main disadvantages of these rule-based techniques are that these require huge experience and grammatical knowledge of the particular language or domain and these systems are not transferable to other languages or domains.</citsent>
<aftsection>
<nextsent>ml based techniques for ner make use of alarge amount of ne annotated training data to acquire high level language knowledge.
</nextsent>
<nextsent>several ml techniques have been successfully used for the ner task of which hidden markov model (hmm) (bikelet al, 1997), <papid> A97-1029 </papid>maximum entropy (maxent) (borth wick, 1999), conditional random field (crf) (liand mccallum, 2004) are most common.</nextsent>
<nextsent>combinations of different ml approaches are also used.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M5">
<title id=" I08-5004.xml">a hybrid named entity recognition system for south and south east asian languages </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>there are several rule based ner systems, containing mainly lexicalized grammar, gazetteer lists, and list of trigger words, which are capable of providing 88%-92% f-measure accuracy for english (grishman, 1995; mcdonald, 1996; wakao et al, 1996).<papid> C96-1071 </papid>the main disadvantages of these rule-based techniques are that these require huge experience and grammatical knowledge of the particular language or domain and these systems are not transferable to other languages or domains.</prevsent>
<prevsent>ml based techniques for ner make use of alarge amount of ne annotated training data to acquire high level language knowledge.</prevsent>
</prevsection>
<citsent citstr=" A97-1029 ">
several ml techniques have been successfully used for the ner task of which hidden markov model (hmm) (bikelet al, 1997), <papid> A97-1029 </papid>maximum entropy (maxent) (borth wick, 1999), conditional random field (crf) (liand mccallum, 2004) are most common.</citsent>
<aftsection>
<nextsent>combinations of different ml approaches are also used.
</nextsent>
<nextsent>srihari et al (2000) <papid> A00-1034 </papid>combines maxent, hidden markov model (hmm) and handcrafted rules to build an ner system.</nextsent>
<nextsent>ner systems use gazetteer lists for identifying names.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M6">
<title id=" I08-5004.xml">a hybrid named entity recognition system for south and south east asian languages </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>several ml techniques have been successfully used for the ner task of which hidden markov model (hmm) (bikelet al, 1997), <papid> A97-1029 </papid>maximum entropy (maxent) (borth wick, 1999), conditional random field (crf) (liand mccallum, 2004) are most common.</prevsent>
<prevsent>combinations of different ml approaches are also used.</prevsent>
</prevsection>
<citsent citstr=" A00-1034 ">
srihari et al (2000) <papid> A00-1034 </papid>combines maxent, hidden markov model (hmm) and handcrafted rules to build an ner system.</citsent>
<aftsection>
<nextsent>ner systems use gazetteer lists for identifying names.
</nextsent>
<nextsent>both the linguistic approach (grishman,1995; wakao et al, 1996) <papid> C96-1071 </papid>and the ml based approach (borthwick, 1999; srihari et al, 2000) <papid> A00-1034 </papid>use gazetteer lists.</nextsent>
<nextsent>linguistic approach uses handcrafted rules which needs skilled linguistics.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M9">
<title id=" I08-5004.xml">a hybrid named entity recognition system for south and south east asian languages </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>an approach to lexical pattern learning for indian languages is described by ekbal and bandopadhyay (2007).
</prevsent>
<prevsent>they used seed data and annotated corpus to find the patterns for ner.
</prevsent>
</prevsection>
<citsent citstr=" W99-0612 ">
the ner task for hindi has been explored by cucerzan and yarowsky in their language independent ner work which used morphological and contextual evidences (cucerzan and yarowsky, 1999).<papid> W99-0612 </papid>they ran their experiment with 5 languages - romanian, english, greek, turkish and hindi.</citsent>
<aftsection>
<nextsent>among these the accuracy for hindi was the worst.
</nextsent>
<nextsent>for hindi the system achieved 41.70% f-value with very low recall of 27.84% and about 85% precision.a more successful hindi ner system was developed by wei li and andrew mccallum (2004) using conditional random fields (crfs) with feature induction.
</nextsent>
<nextsent>they were able to achieve 71.50% f-value using training set of size 340k words.
</nextsent>
<nextsent>in hindi the maximum accuracy is achieved by kumar and bhattacharyya, (2006).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M10">
<title id=" I08-5004.xml">a hybrid named entity recognition system for south and south east asian languages </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>here 18 we mention few attempts for nested ne detection.zhou et al (2004) described an approach to identify cascaded nes from biomedical texts.
</prevsent>
<prevsent>they detected the innermost nes first and then they derived rules to find the other nes containing these as sub strings.
</prevsent>
</prevsection>
<citsent citstr=" H05-1124 ">
another approach, described by mcdonald et al (2005), <papid> H05-1124 </papid>uses structural multilevel classification to deal with overlapping and discontinuous enti ties.</citsent>
<aftsection>
<nextsent>b. gu (2006) <papid> W06-3318 </papid>has treated the task of identifying the nested nes binary classification problem and solved it using support vector machines.</nextsent>
<nextsent>for each token in nested nes, they used two schemes to set its class label: labeling as the outermost entity or the inner entities.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M11">
<title id=" I08-5004.xml">a hybrid named entity recognition system for south and south east asian languages </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>they detected the innermost nes first and then they derived rules to find the other nes containing these as sub strings.
</prevsent>
<prevsent>another approach, described by mcdonald et al (2005), <papid> H05-1124 </papid>uses structural multilevel classification to deal with overlapping and discontinuous enti ties.</prevsent>
</prevsection>
<citsent citstr=" W06-3318 ">
b. gu (2006) <papid> W06-3318 </papid>has treated the task of identifying the nested nes binary classification problem and solved it using support vector machines.</citsent>
<aftsection>
<nextsent>for each token in nested nes, they used two schemes to set its class label: labeling as the outermost entity or the inner entities.
</nextsent>
<nextsent>the data used for the training of the systems was provided.
</nextsent>
<nextsent>the annotated data uses shakti standard format (ssf).
</nextsent>
<nextsent>for our development we have converted the ssf format data into the iob formatted text in which b ? xxx tag indicates the first word of an entity type xxx and ixxx is usedfor subsequent words of an entity.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M12">
<title id=" I08-4011.xml">a two stage approach to chinese partofspeech tagging </title>
<section> maximum entropy pos tagger.  </section>
<citcontext>
<prevsection>
<prevsent>our chinese part-of-speech taggers are based on the maximum entropy model.
</prevsent>
<prevsent>2.1 maximum entropy model.
</prevsent>
</prevsection>
<citsent citstr=" J96-1002 ">
the conditional maximum entropy model (berger, et. al., 1996) <papid> J96-1002 </papid>has the form )),(exp()|( )(1 yxfxyp kkxz ?= ? where ?= xypxz )|()( is normalization factor, and k?</citsent>
<aftsection>
<nextsent>is weight parameter associated with feature ).,( yxfk in the context of part-of-speech tagging, is the pos tag assigned to word, and represents the contextual information regarding the word in consideration, such as the surrounding words.
</nextsent>
<nextsent>a feature is real-valued, typically binary, function.
</nextsent>
<nextsent>for example, we may define binary feature which takes the value 1 if the current word of is story?
</nextsent>
<nextsent>and its pos tag is nns?; and 0 otherwise.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M13">
<title id=" I08-4011.xml">a two stage approach to chinese partofspeech tagging </title>
<section> maximum entropy pos tagger.  </section>
<citcontext>
<prevsection>
<prevsent>given set of training examples, the log likelihood of the model with gaussian prior (chen and rosenfeld, 1999) has the form 82 sixth sighan workshop on chinese language processing constxypl k ii +?= ??
</prevsent>
<prevsent>2 2 )()( 2 )|(log)( ? ??
</prevsent>
</prevsection>
<citsent citstr=" W02-2018 ">
malouf (2002) <papid> W02-2018 </papid>compared iterative procedures such as generalized iterative scaling (gis) and improved iterative scaling (iis) with numerical optimization techniques like limited-memory bfgs (l-bfgs) for estimating the maximum entropy model parameters and found that l-bfgs outperforms the other methods.</citsent>
<aftsection>
<nextsent>the use of l-bfgs requires the computation of the gradient of the log likelihood function.
</nextsent>
<nextsent>the first derivative with respect to parameter k?
</nextsent>
<nextsent>is given by 2~ ),(),( )( ? ?
</nextsent>
<nextsent>k kpkp yxfeyxfel ??= ? ?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M14">
<title id=" I08-5003.xml">named entity recognition for south and south east asian languages taking stock </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the workshop included two tracks.
</prevsent>
<prevsent>the first track was for regular research papers, while the second was organized on the lines of shared task.
</prevsent>
</prevsection>
<citsent citstr=" C00-2102 ">
fairly mature named entity recognition systems are now available for european languages (sang,2002; sang and de meulder, 2003), especially english, and even for east asian languages (sassano and utsuro, 2000).<papid> C00-2102 </papid></citsent>
<aftsection>
<nextsent>however, for south and south east asian languages, the problem of ner is still far from being solved.
</nextsent>
<nextsent>even though we can gain much insight from the methods used for english, there are many issues which make the nature of the problem different for ssea languages.
</nextsent>
<nextsent>for example, these languages do not have capitalization, which is major feature used byner systems for european languages.
</nextsent>
<nextsent>another characteristic of these languages is that most of them use scripts of brahmi origin, which have highly phonetic characteristics that could be utilized for multilingual ner.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M15">
<title id=" I08-5003.xml">named entity recognition for south and south east asian languages taking stock </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>this baseline system has been tested on the data provided for the shared task.
</prevsent>
<prevsent>we present the results for all five languages under the settings required for the shared task.
</prevsent>
</prevsection>
<citsent citstr=" E99-1001 ">
various techniques have been used for solving the ner problem (mikheev et al, 1999; <papid> E99-1001 </papid>borthwick, 1999; cucerzan and yarowsky, 1999; <papid> W99-0612 </papid>chieu and ng, 2003; klein et al, 2003; <papid> W03-0428 </papid>kim and woodland, 2000) ranging from naively using gazette ers to rules based techniques to purely statistical techniques, even hybrid approaches.</citsent>
<aftsection>
<nextsent>several workshops consisting of shared tasks (sang, 2002; sang and de meulder, 2003) have been held with specific focus on this problem.
</nextsent>
<nextsent>in this section we will mention some of techniques used previously.
</nextsent>
<nextsent>most of the approaches can be classified based on the features they use, whether they are rule based or machine learning based or hybrid approaches.
</nextsent>
<nextsent>some of the commonly used features are: ? word form and part of speech (pos) tags?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M16">
<title id=" I08-5003.xml">named entity recognition for south and south east asian languages taking stock </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>this baseline system has been tested on the data provided for the shared task.
</prevsent>
<prevsent>we present the results for all five languages under the settings required for the shared task.
</prevsent>
</prevsection>
<citsent citstr=" W99-0612 ">
various techniques have been used for solving the ner problem (mikheev et al, 1999; <papid> E99-1001 </papid>borthwick, 1999; cucerzan and yarowsky, 1999; <papid> W99-0612 </papid>chieu and ng, 2003; klein et al, 2003; <papid> W03-0428 </papid>kim and woodland, 2000) ranging from naively using gazette ers to rules based techniques to purely statistical techniques, even hybrid approaches.</citsent>
<aftsection>
<nextsent>several workshops consisting of shared tasks (sang, 2002; sang and de meulder, 2003) have been held with specific focus on this problem.
</nextsent>
<nextsent>in this section we will mention some of techniques used previously.
</nextsent>
<nextsent>most of the approaches can be classified based on the features they use, whether they are rule based or machine learning based or hybrid approaches.
</nextsent>
<nextsent>some of the commonly used features are: ? word form and part of speech (pos) tags?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M17">
<title id=" I08-5003.xml">named entity recognition for south and south east asian languages taking stock </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>this baseline system has been tested on the data provided for the shared task.
</prevsent>
<prevsent>we present the results for all five languages under the settings required for the shared task.
</prevsent>
</prevsection>
<citsent citstr=" W03-0428 ">
various techniques have been used for solving the ner problem (mikheev et al, 1999; <papid> E99-1001 </papid>borthwick, 1999; cucerzan and yarowsky, 1999; <papid> W99-0612 </papid>chieu and ng, 2003; klein et al, 2003; <papid> W03-0428 </papid>kim and woodland, 2000) ranging from naively using gazette ers to rules based techniques to purely statistical techniques, even hybrid approaches.</citsent>
<aftsection>
<nextsent>several workshops consisting of shared tasks (sang, 2002; sang and de meulder, 2003) have been held with specific focus on this problem.
</nextsent>
<nextsent>in this section we will mention some of techniques used previously.
</nextsent>
<nextsent>most of the approaches can be classified based on the features they use, whether they are rule based or machine learning based or hybrid approaches.
</nextsent>
<nextsent>some of the commonly used features are: ? word form and part of speech (pos) tags?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M18">
<title id=" I08-5003.xml">named entity recognition for south and south east asian languages taking stock </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>some of the commonly used features are: ? word form and part of speech (pos) tags?
</prevsent>
<prevsent>orthographic features like capitalization, decimal, digits ? word type patterns ? conjunction of types like capitalization, quotes, functional words etc. ? bag of words ? trigger words like new york city 6 tag name description nep person bob dylan, mohan das gandhi ned designation general manager, commissioner neo organization municipal corporation nea abbreviation nlp, b.j.p. neb brand pepsi, nike (ambiguous) netp title-person mahatma, dr., mr. neto title-object pride and prejudice, othello nel location new delhi, paris neti time 3rd september, 1991 (ambiguous) nen number 3.14, 4,500 nem measurers.
</prevsent>
</prevsection>
<citsent citstr=" P01-1041 ">
4,500, 5 kg nete terms maximum entropy, archeology table 1: the named entity tagset used for the shared task ? affixes like hyderabad, rampur, mehdipatnam, lingampally ? gazetteer features: class in the gazetteer ? left and right context ? token length, e.g. the number of letters in word ? previous history in the document or the corpus ? classes of preceding nes the machine learning techniques tried for ner include the following: ? hidden markov models or hmm (zhou and su, 2001) ? decision trees (isozaki, 2001) ? <papid> P01-1041 </papid>maximum entropy (borthwick et al, 1998) ? <papid> W98-1118 </papid>support vector machines or svm (takeuchi and collier, 2002) ? <papid> W02-2029 </papid>conditional random fields or crf (settles, 2004) <papid> W04-1221 </papid>different ways of classifying named entities have been used, i.e., there are more than one tagsets for ner.</citsent>
<aftsection>
<nextsent>for example, the conll 2003 shared task2 had only four tags: persons, locations, organizations 2http://www.cnts.ua.ac.be/conll2003/ner/ and miscellaneous.
</nextsent>
<nextsent>on the other hand, muc-63 has near ontology for information extraction purposes.
</nextsent>
<nextsent>in this (muc-6) tagset, there are three4 main kindsof nes: enamex (persons, locations and organi zations), times (time expressions) and numex (number expresssions).
</nextsent>
<nextsent>there has been some previous work on ner for ssea languages (mccallum and li, 2003; <papid> W03-0430 </papid>cucerzan and yarowsky, 1999), <papid> W99-0612 </papid>but most of the time such work was an offshoot of the work done for european languages.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M19">
<title id=" I08-5003.xml">named entity recognition for south and south east asian languages taking stock </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>some of the commonly used features are: ? word form and part of speech (pos) tags?
</prevsent>
<prevsent>orthographic features like capitalization, decimal, digits ? word type patterns ? conjunction of types like capitalization, quotes, functional words etc. ? bag of words ? trigger words like new york city 6 tag name description nep person bob dylan, mohan das gandhi ned designation general manager, commissioner neo organization municipal corporation nea abbreviation nlp, b.j.p. neb brand pepsi, nike (ambiguous) netp title-person mahatma, dr., mr. neto title-object pride and prejudice, othello nel location new delhi, paris neti time 3rd september, 1991 (ambiguous) nen number 3.14, 4,500 nem measurers.
</prevsent>
</prevsection>
<citsent citstr=" W98-1118 ">
4,500, 5 kg nete terms maximum entropy, archeology table 1: the named entity tagset used for the shared task ? affixes like hyderabad, rampur, mehdipatnam, lingampally ? gazetteer features: class in the gazetteer ? left and right context ? token length, e.g. the number of letters in word ? previous history in the document or the corpus ? classes of preceding nes the machine learning techniques tried for ner include the following: ? hidden markov models or hmm (zhou and su, 2001) ? decision trees (isozaki, 2001) ? <papid> P01-1041 </papid>maximum entropy (borthwick et al, 1998) ? <papid> W98-1118 </papid>support vector machines or svm (takeuchi and collier, 2002) ? <papid> W02-2029 </papid>conditional random fields or crf (settles, 2004) <papid> W04-1221 </papid>different ways of classifying named entities have been used, i.e., there are more than one tagsets for ner.</citsent>
<aftsection>
<nextsent>for example, the conll 2003 shared task2 had only four tags: persons, locations, organizations 2http://www.cnts.ua.ac.be/conll2003/ner/ and miscellaneous.
</nextsent>
<nextsent>on the other hand, muc-63 has near ontology for information extraction purposes.
</nextsent>
<nextsent>in this (muc-6) tagset, there are three4 main kindsof nes: enamex (persons, locations and organi zations), times (time expressions) and numex (number expresssions).
</nextsent>
<nextsent>there has been some previous work on ner for ssea languages (mccallum and li, 2003; <papid> W03-0430 </papid>cucerzan and yarowsky, 1999), <papid> W99-0612 </papid>but most of the time such work was an offshoot of the work done for european languages.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M20">
<title id=" I08-5003.xml">named entity recognition for south and south east asian languages taking stock </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>some of the commonly used features are: ? word form and part of speech (pos) tags?
</prevsent>
<prevsent>orthographic features like capitalization, decimal, digits ? word type patterns ? conjunction of types like capitalization, quotes, functional words etc. ? bag of words ? trigger words like new york city 6 tag name description nep person bob dylan, mohan das gandhi ned designation general manager, commissioner neo organization municipal corporation nea abbreviation nlp, b.j.p. neb brand pepsi, nike (ambiguous) netp title-person mahatma, dr., mr. neto title-object pride and prejudice, othello nel location new delhi, paris neti time 3rd september, 1991 (ambiguous) nen number 3.14, 4,500 nem measurers.
</prevsent>
</prevsection>
<citsent citstr=" W02-2029 ">
4,500, 5 kg nete terms maximum entropy, archeology table 1: the named entity tagset used for the shared task ? affixes like hyderabad, rampur, mehdipatnam, lingampally ? gazetteer features: class in the gazetteer ? left and right context ? token length, e.g. the number of letters in word ? previous history in the document or the corpus ? classes of preceding nes the machine learning techniques tried for ner include the following: ? hidden markov models or hmm (zhou and su, 2001) ? decision trees (isozaki, 2001) ? <papid> P01-1041 </papid>maximum entropy (borthwick et al, 1998) ? <papid> W98-1118 </papid>support vector machines or svm (takeuchi and collier, 2002) ? <papid> W02-2029 </papid>conditional random fields or crf (settles, 2004) <papid> W04-1221 </papid>different ways of classifying named entities have been used, i.e., there are more than one tagsets for ner.</citsent>
<aftsection>
<nextsent>for example, the conll 2003 shared task2 had only four tags: persons, locations, organizations 2http://www.cnts.ua.ac.be/conll2003/ner/ and miscellaneous.
</nextsent>
<nextsent>on the other hand, muc-63 has near ontology for information extraction purposes.
</nextsent>
<nextsent>in this (muc-6) tagset, there are three4 main kindsof nes: enamex (persons, locations and organi zations), times (time expressions) and numex (number expresssions).
</nextsent>
<nextsent>there has been some previous work on ner for ssea languages (mccallum and li, 2003; <papid> W03-0430 </papid>cucerzan and yarowsky, 1999), <papid> W99-0612 </papid>but most of the time such work was an offshoot of the work done for european languages.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M21">
<title id=" I08-5003.xml">named entity recognition for south and south east asian languages taking stock </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>some of the commonly used features are: ? word form and part of speech (pos) tags?
</prevsent>
<prevsent>orthographic features like capitalization, decimal, digits ? word type patterns ? conjunction of types like capitalization, quotes, functional words etc. ? bag of words ? trigger words like new york city 6 tag name description nep person bob dylan, mohan das gandhi ned designation general manager, commissioner neo organization municipal corporation nea abbreviation nlp, b.j.p. neb brand pepsi, nike (ambiguous) netp title-person mahatma, dr., mr. neto title-object pride and prejudice, othello nel location new delhi, paris neti time 3rd september, 1991 (ambiguous) nen number 3.14, 4,500 nem measurers.
</prevsent>
</prevsection>
<citsent citstr=" W04-1221 ">
4,500, 5 kg nete terms maximum entropy, archeology table 1: the named entity tagset used for the shared task ? affixes like hyderabad, rampur, mehdipatnam, lingampally ? gazetteer features: class in the gazetteer ? left and right context ? token length, e.g. the number of letters in word ? previous history in the document or the corpus ? classes of preceding nes the machine learning techniques tried for ner include the following: ? hidden markov models or hmm (zhou and su, 2001) ? decision trees (isozaki, 2001) ? <papid> P01-1041 </papid>maximum entropy (borthwick et al, 1998) ? <papid> W98-1118 </papid>support vector machines or svm (takeuchi and collier, 2002) ? <papid> W02-2029 </papid>conditional random fields or crf (settles, 2004) <papid> W04-1221 </papid>different ways of classifying named entities have been used, i.e., there are more than one tagsets for ner.</citsent>
<aftsection>
<nextsent>for example, the conll 2003 shared task2 had only four tags: persons, locations, organizations 2http://www.cnts.ua.ac.be/conll2003/ner/ and miscellaneous.
</nextsent>
<nextsent>on the other hand, muc-63 has near ontology for information extraction purposes.
</nextsent>
<nextsent>in this (muc-6) tagset, there are three4 main kindsof nes: enamex (persons, locations and organi zations), times (time expressions) and numex (number expresssions).
</nextsent>
<nextsent>there has been some previous work on ner for ssea languages (mccallum and li, 2003; <papid> W03-0430 </papid>cucerzan and yarowsky, 1999), <papid> W99-0612 </papid>but most of the time such work was an offshoot of the work done for european languages.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M22">
<title id=" I08-5003.xml">named entity recognition for south and south east asian languages taking stock </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>on the other hand, muc-63 has near ontology for information extraction purposes.
</prevsent>
<prevsent>in this (muc-6) tagset, there are three4 main kindsof nes: enamex (persons, locations and organi zations), times (time expressions) and numex (number expresssions).
</prevsent>
</prevsection>
<citsent citstr=" W03-0430 ">
there has been some previous work on ner for ssea languages (mccallum and li, 2003; <papid> W03-0430 </papid>cucerzan and yarowsky, 1999), <papid> W99-0612 </papid>but most of the time such work was an offshoot of the work done for european languages.</citsent>
<aftsection>
<nextsent>even including the current work shop, the work on ner for ssea languages is still in the initial stages as the results reported by papers in this workshop clearly show.
</nextsent>
<nextsent>the tagset being used for the nersseal-08 shared task consists of more tags than the four tags used for the conll 2003 shared task.
</nextsent>
<nextsent>the reason we opted for these tags was that we needed slightly finer tagset for machine translation (mt).
</nextsent>
<nextsent>the initial aim was to improve the performance of the mt system.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M24">
<title id=" I08-4023.xml">hmm and crf based hybrid model for chinese lexical analysis </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>several promising methods are proposed by previous researchers.
</prevsent>
<prevsent>in tradition, the chinese word segmentation technologies canbe categorized into three types, rule-based, machine learning, and hybrid.
</prevsent>
</prevsection>
<citsent citstr=" C04-1081 ">
among them, thema chine learning-based techniques showed excellent performance in many research studies (peng et al, 2004; <papid> C04-1081 </papid>zhou et al, 2005; gao et al, 2004).<papid> P04-1059 </papid></citsent>
<aftsection>
<nextsent>this method treats the word segmentation problem as sequence of word classification.
</nextsent>
<nextsent>the classifier online assigns either boundary?
</nextsent>
<nextsent>or non boundary?
</nextsent>
<nextsent>label to each word by learning from the large annotated corpora.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M25">
<title id=" I08-4023.xml">hmm and crf based hybrid model for chinese lexical analysis </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>several promising methods are proposed by previous researchers.
</prevsent>
<prevsent>in tradition, the chinese word segmentation technologies canbe categorized into three types, rule-based, machine learning, and hybrid.
</prevsent>
</prevsection>
<citsent citstr=" P04-1059 ">
among them, thema chine learning-based techniques showed excellent performance in many research studies (peng et al, 2004; <papid> C04-1081 </papid>zhou et al, 2005; gao et al, 2004).<papid> P04-1059 </papid></citsent>
<aftsection>
<nextsent>this method treats the word segmentation problem as sequence of word classification.
</nextsent>
<nextsent>the classifier online assigns either boundary?
</nextsent>
<nextsent>or non boundary?
</nextsent>
<nextsent>label to each word by learning from the large annotated corpora.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M26">
<title id=" I08-6003.xml">finding parallel texts on the web using cross language information retrieval </title>
<section> abstract </section>
<citcontext>
<prevsection>
<prevsent>discovering parallel corpora on the web is challenging task.
</prevsent>
<prevsent>in this paper, we use cross-language information retrieval techniques in combination with structural features to retrieve candidate page pairs from commercial search engine.
</prevsent>
</prevsection>
<citsent citstr=" J03-3002 ">
the candidate page pairs are then filtered using techniques described by resnik and smith (2003) <papid> J03-3002 </papid>to determine if they are translations.</citsent>
<aftsection>
<nextsent>the results allow the comparison of efficiency of different parameter settings and provide an estimate for the percentage of pages that are parallel for certain language pair.
</nextsent>
<nextsent>parallel corpora are invaluable resources in many areas of natural language processing (nlp).
</nextsent>
<nextsent>they are used in multilingual nlp as basis for the creation of translation models (brown et. al., 1990), <papid> J90-2002 </papid>lexical acquisition (gale and church, 1991) <papid> H91-1026 </papid>as well as for cross-language information retrieval (chen and nie, 2000).</nextsent>
<nextsent>parallel corpora can also benefit monolingual nlp via the induction of monolingual analysis tools for new languages or the improvement of tools for languages where tools already exist (hwa et. al., 2005; pad?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M30">
<title id=" I08-6003.xml">finding parallel texts on the web using cross language information retrieval </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the results allow the comparison of efficiency of different parameter settings and provide an estimate for the percentage of pages that are parallel for certain language pair.
</prevsent>
<prevsent>parallel corpora are invaluable resources in many areas of natural language processing (nlp).
</prevsent>
</prevsection>
<citsent citstr=" J90-2002 ">
they are used in multilingual nlp as basis for the creation of translation models (brown et. al., 1990), <papid> J90-2002 </papid>lexical acquisition (gale and church, 1991) <papid> H91-1026 </papid>as well as for cross-language information retrieval (chen and nie, 2000).</citsent>
<aftsection>
<nextsent>parallel corpora can also benefit monolingual nlp via the induction of monolingual analysis tools for new languages or the improvement of tools for languages where tools already exist (hwa et. al., 2005; pad?
</nextsent>
<nextsent>and lapata, 2005; yarowsky and ngai, 2001).<papid> N01-1026 </papid></nextsent>
<nextsent>for most of the mentioned work, large parallel corpora are required.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M31">
<title id=" I08-6003.xml">finding parallel texts on the web using cross language information retrieval </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the results allow the comparison of efficiency of different parameter settings and provide an estimate for the percentage of pages that are parallel for certain language pair.
</prevsent>
<prevsent>parallel corpora are invaluable resources in many areas of natural language processing (nlp).
</prevsent>
</prevsection>
<citsent citstr=" H91-1026 ">
they are used in multilingual nlp as basis for the creation of translation models (brown et. al., 1990), <papid> J90-2002 </papid>lexical acquisition (gale and church, 1991) <papid> H91-1026 </papid>as well as for cross-language information retrieval (chen and nie, 2000).</citsent>
<aftsection>
<nextsent>parallel corpora can also benefit monolingual nlp via the induction of monolingual analysis tools for new languages or the improvement of tools for languages where tools already exist (hwa et. al., 2005; pad?
</nextsent>
<nextsent>and lapata, 2005; yarowsky and ngai, 2001).<papid> N01-1026 </papid></nextsent>
<nextsent>for most of the mentioned work, large parallel corpora are required.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M32">
<title id=" I08-6003.xml">finding parallel texts on the web using cross language information retrieval </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>they are used in multilingual nlp as basis for the creation of translation models (brown et. al., 1990), <papid> J90-2002 </papid>lexical acquisition (gale and church, 1991) <papid> H91-1026 </papid>as well as for cross-language information retrieval (chen and nie, 2000).</prevsent>
<prevsent>parallel corpora can also benefit monolingual nlp via the induction of monolingual analysis tools for new languages or the improvement of tools for languages where tools already exist (hwa et. al., 2005; pad?</prevsent>
</prevsection>
<citsent citstr=" N01-1026 ">
and lapata, 2005; yarowsky and ngai, 2001).<papid> N01-1026 </papid></citsent>
<aftsection>
<nextsent>for most of the mentioned work, large parallel corpora are required.
</nextsent>
<nextsent>often these corpora have limited availability due to licensing restrictions (tiedemann and nygaard, 2004) and/or are domain specific (koehn, 2005).
</nextsent>
<nextsent>also parallel corpora are only available for limited set of language pairs.
</nextsent>
<nextsent>as result, researchers look to the world wide web as source for parallel corpora (resnik and smith, 2003; <papid> J03-3002 </papid>ma and liberman, 1999; chen and nie, 2000).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M42">
<title id=" I08-6003.xml">finding parallel texts on the web using cross language information retrieval </title>
<section> methodology.  </section>
<citcontext>
<prevsection>
<prevsent>to get random sample of pages from search engine that we can check for translational equivalents in another language, we select terms at random from bilingual dictionary.
</prevsent>
<prevsent>instead of using manually crafted bilingual dictionary, we chose to use translation lexicon automatically created from parallel data, because the translation probabilities are useful for our experiments.
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
in this study, the translation lexicon was created by aligning part of the german-english portion of the europarl corpus (koehn, 2005) using the giza++ package (och and ney, 2003).<papid> J03-1002 </papid></citsent>
<aftsection>
<nextsent>the drawback of using this translation lexicon is that the lexicon is domain-specific to parliamentary proceedings.
</nextsent>
<nextsent>we alleviated this domain-specificity by selecting mainly terms with medium frequency in the lexicon.
</nextsent>
<nextsent>we sorted the terms by frequency.
</nextsent>
<nextsent>according to zipfs law (zipf, 1949), the frequency of the terms is roughly inversely proportional to their rankin this list.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M49">
<title id=" I08-5010.xml">a character ngram based approach for improved recall in indian language ner </title>
<section> abstract </section>
<citcontext>
<prevsection>

<prevsent>named entity recognition (ner) is the task of identifying and classifying all proper nouns in document as person names, organization names, location names, date &time; expressions and miscellaneous.
</prevsent>
</prevsection>
<citsent citstr=" W99-0612 ">
previous work (cucerzan and yarowsky, 1999)<papid> W99-0612 </papid>was done using the complete words as features which suffers from low recall prob lem.</citsent>
<aftsection>
<nextsent>character n-gram based approach(klein et al, 2003) <papid> W03-0428 </papid>using generative models, was experimented on english language and it proved to be useful over the word based models.</nextsent>
<nextsent>applying the same technique on indian languages, we experimented with conditional random fields (crfs), discriminative model, and evaluated our system on two indian languages telugu and hindi.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M50">
<title id=" I08-5010.xml">a character ngram based approach for improved recall in indian language ner </title>
<section> abstract </section>
<citcontext>
<prevsection>
<prevsent>named entity recognition (ner) is the task of identifying and classifying all proper nouns in document as person names, organization names, location names, date &time; expressions and miscellaneous.
</prevsent>
<prevsent>previous work (cucerzan and yarowsky, 1999)<papid> W99-0612 </papid>was done using the complete words as features which suffers from low recall prob lem.</prevsent>
</prevsection>
<citsent citstr=" W03-0428 ">
character n-gram based approach(klein et al, 2003) <papid> W03-0428 </papid>using generative models, was experimented on english language and it proved to be useful over the word based models.</citsent>
<aftsection>
<nextsent>applying the same technique on indian languages, we experimented with conditional random fields (crfs), discriminative model, and evaluated our system on two indian languages telugu and hindi.
</nextsent>
<nextsent>the character n-gram based models showed considerable improvement over the word based models.
</nextsent>
<nextsent>this paper describes the features used and experiments to increase the recall of named entity recognition systems which is also language independent.
</nextsent>
<nextsent>the objective of ner is to classify all tokens in atext document into predefined classes such as person, organization, location, miscellaneous.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M52">
<title id=" I08-5010.xml">a character ngram based approach for improved recall in indian language ner </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the creation of subtask for ner in message understanding conference (muc) (chinchor, 1997) reflects the importance of ner in information extraction (ie).
</prevsent>
<prevsent>ner also finds aplication in question answering systems (toral et al, 2005; molla et al,2006), and machine translation (babych and hartley, 2003).
</prevsent>
</prevsection>
<citsent citstr=" N06-3009 ">
ner is an essential subtask in organizing and retrieving biomedical information (tsai, 2006).<papid> N06-3009 </papid></citsent>
<aftsection>
<nextsent>ner can be treated as two step process ? identification of proper nouns.
</nextsent>
<nextsent>classification of these identified proper nouns.
</nextsent>
<nextsent>challenges in named entity recognition.
</nextsent>
<nextsent>many named entities (nes) occur rarely in corpus if at all.ambiguity of nes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M55">
<title id=" I08-5010.xml">a character ngram based approach for improved recall in indian language ner </title>
<section> problem statement.  </section>
<citcontext>
<prevsection>
<prevsent>given an input sequence of words n1 = w1w2w3 ...wn, the ner task is to construct label sequence ln1 = l1l2l3 ...ln , where label li either belongs to the set of predefined classes for named entities or is none (representing words which are not propernouns).
</prevsent>
<prevsent>the general label sequence ln1 has the highest probability of occuring for the word sequence n1 among all possible label sequences, that is ln1 = argmax {pr (ln1 | n1 ) } 3.2 tagging scheme.
</prevsent>
</prevsection>
<citsent citstr=" W95-0107 ">
we followed the iob tagging scheme (ramshawand marcus, 1995) <papid> W95-0107 </papid>for all the three languages (en glish, hindi and telugu).</citsent>
<aftsection>
<nextsent>in this scheme each line contains word at the beginning followed by its tag.
</nextsent>
<nextsent>the tag encodes the type of named entity and whether the word is in the beginning or inside the ne.
</nextsent>
<nextsent>empty lines represent sentence (document) boundaries.
</nextsent>
<nextsent>an example of the iob tagging scheme is given in table 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M57">
<title id=" I08-5010.xml">a character ngram based approach for improved recall in indian language ner </title>
<section> features.  </section>
<citcontext>
<prevsection>
<prevsent>there are many types of features used inner systems.
</prevsent>
<prevsent>many systems use binary features i.e. the word-internal features, which indicate the presence or absence of particular property in the word.
</prevsent>
</prevsection>
<citsent citstr=" A97-1030 ">
(mikheev, 1997; wacholder et al, 1997; <papid> A97-1030 </papid>bikel et al., 1997).</citsent>
<aftsection>
<nextsent>following are examples of commonly used binary features: all-caps (ibm), internal capitalization (ebay), initial capital (abdul kalam), un capitalized word (can), 2-digit number (83), digit number (73), 4-digit number (1983), digit number (2007), all digits (8, 28, 1273)etc. the features that correspond to the capitalization are not applicable to indian languages.
</nextsent>
<nextsent>also, we have not used any of the binary features in any of our models.
</nextsent>
<nextsent>dictionaries: dictionaries are used to check if part of the named entity is present in the dictionary.
</nextsent>
<nextsent>these dictionaries are called as gazetteers.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M59">
<title id=" I08-4028.xml">an improved crf based chinese language processing system for sighan bakeoff 2007 </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>among 24 closed and open tracks in this bakeoff, we participated in 23 tracks, except the open ner track of msra.
</prevsent>
<prevsent>our systems are ranked 1st in 6 tracks, and get close to the top level in several other tracks.
</prevsent>
</prevsection>
<citsent citstr=" I05-3025 ">
recently, maximum entropy model(me) and crfs (low et al, 2005)(<papid> I05-3025 </papid>tseng et al, 2005) (<papid> I05-3027 </papid>haizhao et al, 2006) <papid> W06-0127 </papid>turned out to be promising in natural language processing tracks, and obtain excellent performances on most of the test corpora of bakeoff 2005 and bakeoff 2006.</citsent>
<aftsection>
<nextsent>compared to the generative models, like hmm, the primary advanta geof crfs is that it relaxes the independence assumptions, which makes it able to handle multiple interacting features between observation elements (wal lach et al, 2004).however, the me and crfs emphasize the relation of the basic units of sequence, like the chinese characters in these tracks.
</nextsent>
<nextsent>while, the higher level information, like the relationship of the words is ignored.
</nextsent>
<nextsent>from this point of view, the n-gram language model is incorporated in our crfs based systems in order to cover the word level language information.
</nextsent>
<nextsent>based on several pilot-experimental results, we found that the tagging errors always follow somepatterns.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M60">
<title id=" I08-4028.xml">an improved crf based chinese language processing system for sighan bakeoff 2007 </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>among 24 closed and open tracks in this bakeoff, we participated in 23 tracks, except the open ner track of msra.
</prevsent>
<prevsent>our systems are ranked 1st in 6 tracks, and get close to the top level in several other tracks.
</prevsent>
</prevsection>
<citsent citstr=" I05-3027 ">
recently, maximum entropy model(me) and crfs (low et al, 2005)(<papid> I05-3025 </papid>tseng et al, 2005) (<papid> I05-3027 </papid>haizhao et al, 2006) <papid> W06-0127 </papid>turned out to be promising in natural language processing tracks, and obtain excellent performances on most of the test corpora of bakeoff 2005 and bakeoff 2006.</citsent>
<aftsection>
<nextsent>compared to the generative models, like hmm, the primary advanta geof crfs is that it relaxes the independence assumptions, which makes it able to handle multiple interacting features between observation elements (wal lach et al, 2004).however, the me and crfs emphasize the relation of the basic units of sequence, like the chinese characters in these tracks.
</nextsent>
<nextsent>while, the higher level information, like the relationship of the words is ignored.
</nextsent>
<nextsent>from this point of view, the n-gram language model is incorporated in our crfs based systems in order to cover the word level language information.
</nextsent>
<nextsent>based on several pilot-experimental results, we found that the tagging errors always follow somepatterns.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M61">
<title id=" I08-4028.xml">an improved crf based chinese language processing system for sighan bakeoff 2007 </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>among 24 closed and open tracks in this bakeoff, we participated in 23 tracks, except the open ner track of msra.
</prevsent>
<prevsent>our systems are ranked 1st in 6 tracks, and get close to the top level in several other tracks.
</prevsent>
</prevsection>
<citsent citstr=" W06-0127 ">
recently, maximum entropy model(me) and crfs (low et al, 2005)(<papid> I05-3025 </papid>tseng et al, 2005) (<papid> I05-3027 </papid>haizhao et al, 2006) <papid> W06-0127 </papid>turned out to be promising in natural language processing tracks, and obtain excellent performances on most of the test corpora of bakeoff 2005 and bakeoff 2006.</citsent>
<aftsection>
<nextsent>compared to the generative models, like hmm, the primary advanta geof crfs is that it relaxes the independence assumptions, which makes it able to handle multiple interacting features between observation elements (wal lach et al, 2004).however, the me and crfs emphasize the relation of the basic units of sequence, like the chinese characters in these tracks.
</nextsent>
<nextsent>while, the higher level information, like the relationship of the words is ignored.
</nextsent>
<nextsent>from this point of view, the n-gram language model is incorporated in our crfs based systems in order to cover the word level language information.
</nextsent>
<nextsent>based on several pilot-experimental results, we found that the tagging errors always follow somepatterns.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M64">
<title id=" I08-4028.xml">an improved crf based chinese language processing system for sighan bakeoff 2007 </title>
<section> word segmentation.  </section>
<citcontext>
<prevsection>
<prevsent>2.2 multi-model integration.
</prevsent>
<prevsent>in order to integrate multi-model information, we use log-linear model(och et al, 2002) to compute the posterior probability: pr (w |c) = pm1 (w |c) = exp[ m=1 mhm(w,c)] ? ? exp[ m=1 mhm(w ?, c)] (2)where is the word sequence, and is the character sequence.
</prevsent>
</prevsection>
<citsent citstr=" P03-1021 ">
the decision rule here is: w0 = argmaxw {pr(w |c)} = argmaxw { ? m=1 mhm(w,c)} (3)the parameters m1 of this model can be optimized by standard approaches, such as the minimum error rate training used in machine translation (och, 2003).<papid> P03-1021 </papid></citsent>
<aftsection>
<nextsent>in fact, the crfs approach is special case of this framework when we define = 1 and use the following feature function: h1(w,c) = logp?(y |x) (4)in our approach, the logarithms of the scores generated by the two kinds of models are used as feature functions: h1(w,c) = logpcrf (w,c) = log ? i p?(wi|c) (5) h2(w,c) = logplm(w ) (6) the first feature function(eq.5) comes from crfs.instead of computing the score of the whole label sequence with character sequence throughp?(y |x) directly, we try to get the posterior probability of sub-sequence to be tagged as one whole word p?(wi|c).
</nextsent>
<nextsent>then we combine all the score of words together.
</nextsent>
<nextsent>the second feature function(eq.6) comes from n-gram language model, which aims to catch the words information.
</nextsent>
<nextsent>the log-linear model with the feature functions described above allows the dynamic programming search algorithm for efficient decoding.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M65">
<title id=" I08-4028.xml">an improved crf based chinese language processing system for sighan bakeoff 2007 </title>
<section> word segmentation.  </section>
<citcontext>
<prevsection>
<prevsent>then the best word sequence is searched on the word lattice with the decision rule(eq.3).
</prevsent>
<prevsent>since arbitrary sub-sequence can be viewed as candidate word in word lattice, we need to deal with the problem of oov words.
</prevsent>
</prevsection>
<citsent citstr=" W06-0121 ">
the unigram of an oov word is estimated as: unigram(oov word) = pl (7) where is the minimal value of unigram scores in the language model; is the length of the oov word, which is used as punishment factor toavoid overemphasizing the long oov words (xin hao wang et al, 2006).<papid> W06-0121 </papid></citsent>
<aftsection>
<nextsent>2.3 post-processing strategies.
</nextsent>
<nextsent>the division and combination rule, which has been proved to be useful in our system of bakeoff 2006(xinhao wang et al, 2006), <papid> W06-0121 </papid>is adopted for the postprocessing in the system.</nextsent>
<nextsent>156 sixth sighan workshop on chinese language processing 2.4 training data transition.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M67">
<title id=" I08-4028.xml">an improved crf based chinese language processing system for sighan bakeoff 2007 </title>
<section> named entity recognition.  </section>
<citcontext>
<prevsection>
<prevsent>157 sixth sighan workshop on chinese language processing character sequence ? ?
</prevsent>
<prevsent>label sequence per-b per-c per-c per-c per-c per-e n n class sequence ? person ? ?
</prevsent>
</prevsection>
<citsent citstr=" J95-4004 ">
table 1: class sequence exampletransformation-based learning is symbolic machine learning method, introduced by (eric brill, 1995).<papid> J95-4004 </papid></citsent>
<aftsection>
<nextsent>the main idea in tbl is to generate set of transformation rules that can correct tagging errors produced by the initial process.
</nextsent>
<nextsent>there are four main procedures in our tblframework: an initial state assignment which is operated by the system we described above; set of allowable templates for rules, ranging from words in 3 positions windows and name entity information in 3-word window with their combinations considered, and rules which are learned according to the tagging differences between training data and results generated by our system, at last, those rules are introduced to correct similar errors.
</nextsent>
<nextsent>the pos tagging track is to assign the part-of speech sequence for the correctly segmented word sequence.
</nextsent>
<nextsent>in our system, for the ctb corpus, thecrfs are adopted; however for the other four corpora, considering the limitations of resources andtime, the me model is adopted.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M69">
<title id=" I08-4005.xml">use of event types for temporal relation identification in chinese text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, researches at temporal relation extraction are still limited.
</prevsent>
<prevsent>temporal relation extraction includes the following issues: identifying events, anchoring events on the timeline, ordering events, and reasoning with contextually underspecified temporal expressions.
</prevsent>
</prevsection>
<citsent citstr=" P06-1095 ">
to extract temporal relations, several knowledge resources are necessary, such as tense and aspect of verbs, temporal adverbs, and world knowledge (mani, et al., 2006).<papid> P06-1095 </papid></citsent>
<aftsection>
<nextsent>in english, time bank (pustejovsky, et al, 2006), temporal information annotated corpus, is available to machine learning approaches for automatically extracting temporal relation.
</nextsent>
<nextsent>in chinese, li (2004) proposed machine learning based method for temporal relation identification, but they considered the relation between adjacent verbs in small scale corpus.
</nextsent>
<nextsent>there is no publicly available chinese resource for temporal information processing.
</nextsent>
<nextsent>we proposed (cheng, 2007) dependency structure based method to annotate temporal relations manually on limited set of event pairs and extend the relations using inference rules.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M70">
<title id=" I08-5008.xml">bengali named entity recognition using support vector machine </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the representative ma chine-learning approaches used inner are hidden markov model (hmm) (bbns identifinder in (bikel, 1999)), maximum entropy (new york universitys meme in (borthwick, 1999)), decision tree (new york universitys system in (se kine, 1998) and conditional random fields (crfs) (lafferty et al, 2001).
</prevsent>
<prevsent>support vector machines (svms) based ner system was proposed by yamada et al (2002) for japanese.
</prevsent>
</prevsection>
<citsent citstr=" N01-1025 ">
his system is an extension of kudos chunking system (kudo and matsumoto, 2001) <papid> N01-1025 </papid>that gave the best performance at conll-2000 shared tasks.</citsent>
<aftsection>
<nextsent>the other svm-based ner systems can be found in (takeuchi and collier, 2002) <papid> W02-2029 </papid>and (asahara and matsumoto, 2003).<papid> N03-1002 </papid></nextsent>
<nextsent>named entity identification in indian languages in general and particularly in bengali is difficult and challenging.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M71">
<title id=" I08-5008.xml">bengali named entity recognition using support vector machine </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>support vector machines (svms) based ner system was proposed by yamada et al (2002) for japanese.
</prevsent>
<prevsent>his system is an extension of kudos chunking system (kudo and matsumoto, 2001) <papid> N01-1025 </papid>that gave the best performance at conll-2000 shared tasks.</prevsent>
</prevsection>
<citsent citstr=" W02-2029 ">
the other svm-based ner systems can be found in (takeuchi and collier, 2002) <papid> W02-2029 </papid>and (asahara and matsumoto, 2003).<papid> N03-1002 </papid></citsent>
<aftsection>
<nextsent>named entity identification in indian languages in general and particularly in bengali is difficult and challenging.
</nextsent>
<nextsent>in english, the ne always appears with capitalized letter but there is no concept of capitalization in bengali.
</nextsent>
<nextsent>there has been very 51 little work in the area of ner in indian languages.
</nextsent>
<nextsent>in indian languages, particularly in bengali, the works inner can be found in (ekbal and bandyopadhyay, 2007a; ekbal and bandyopadhyay, 2007b) with the pattern directed shallow parsing approach and in (ekbal et al, 2007c) with the hmm.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M72">
<title id=" I08-5008.xml">bengali named entity recognition using support vector machine </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>support vector machines (svms) based ner system was proposed by yamada et al (2002) for japanese.
</prevsent>
<prevsent>his system is an extension of kudos chunking system (kudo and matsumoto, 2001) <papid> N01-1025 </papid>that gave the best performance at conll-2000 shared tasks.</prevsent>
</prevsection>
<citsent citstr=" N03-1002 ">
the other svm-based ner systems can be found in (takeuchi and collier, 2002) <papid> W02-2029 </papid>and (asahara and matsumoto, 2003).<papid> N03-1002 </papid></citsent>
<aftsection>
<nextsent>named entity identification in indian languages in general and particularly in bengali is difficult and challenging.
</nextsent>
<nextsent>in english, the ne always appears with capitalized letter but there is no concept of capitalization in bengali.
</nextsent>
<nextsent>there has been very 51 little work in the area of ner in indian languages.
</nextsent>
<nextsent>in indian languages, particularly in bengali, the works inner can be found in (ekbal and bandyopadhyay, 2007a; ekbal and bandyopadhyay, 2007b) with the pattern directed shallow parsing approach and in (ekbal et al, 2007c) with the hmm.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M73">
<title id=" I08-7003.xml">preliminary chinese term classification for ontology construction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>?(file manager), ?????
</prevsent>
<prevsent>?(description of concurrency), etc..
</prevsent>
</prevsection>
<citsent citstr=" W03-1730 ">
even though current general word segmentation and pos tagging can achieve precision of 99.6% and 97.58%, respectively (huaping zhang et al , 2003), <papid> W03-1730 </papid>its performance for domain specific corpus is much less satisfactory (luning ji et al , 2007), which is why terminology extraction algorithms need to be developed.</citsent>
<aftsection>
<nextsent>in this paper, very simple but effective method is proposed for tpos tagging which needs no training process or even context information.
</nextsent>
<nextsent>this method is based on the assumption that every term has headword.
</nextsent>
<nextsent>forgiven list of domain specific terms which are segmented and each word in the term already has pos tag, the tpos tagging algorithm then identifies the position of the headword and take the tag of the headword as the tag of the term.
</nextsent>
<nextsent>experiments show that this method is quite effective in giving good precision and minimal computing time.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M74">
<title id=" I08-7003.xml">preliminary chinese term classification for ontology construction </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>for example, an english word ending in the suffix able was very likely to be an adjective (brants, thorsten, 2000).
</prevsent>
<prevsent>some existing methods are based on the analysis of word morphology.
</prevsent>
</prevsection>
<citsent citstr=" N03-1033 ">
they exploited more features besides morphology or took morphology as supplementary means (toutanova et al , 2003; <papid> N03-1033 </papid>huihsin tseng et al , 2005; <papid> I05-3005 </papid>samuelsson, christer, 1993).</citsent>
<aftsection>
<nextsent>toutanova et al  demonstrated the use of both preceding and following tag contexts via dependency network representation and made use of some additional features such as lexical features including jointly conditioning on multiple consecutive words and other fine-grained modeling of word features (toutanova et al , 2003).<papid> N03-1033 </papid></nextsent>
<nextsent>huihisin et al  proposed variety of morphological word features, such as the tag sequence features from both left and right side of the current word for pos tagging and implemented them in maximum entropy markov model (huihsin tseng et al , 2005).<papid> I05-3005 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M75">
<title id=" I08-7003.xml">preliminary chinese term classification for ontology construction </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>for example, an english word ending in the suffix able was very likely to be an adjective (brants, thorsten, 2000).
</prevsent>
<prevsent>some existing methods are based on the analysis of word morphology.
</prevsent>
</prevsection>
<citsent citstr=" I05-3005 ">
they exploited more features besides morphology or took morphology as supplementary means (toutanova et al , 2003; <papid> N03-1033 </papid>huihsin tseng et al , 2005; <papid> I05-3005 </papid>samuelsson, christer, 1993).</citsent>
<aftsection>
<nextsent>toutanova et al  demonstrated the use of both preceding and following tag contexts via dependency network representation and made use of some additional features such as lexical features including jointly conditioning on multiple consecutive words and other fine-grained modeling of word features (toutanova et al , 2003).<papid> N03-1033 </papid></nextsent>
<nextsent>huihisin et al  proposed variety of morphological word features, such as the tag sequence features from both left and right side of the current word for pos tagging and implemented them in maximum entropy markov model (huihsin tseng et al , 2005).<papid> I05-3005 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M79">
<title id=" I08-4020.xml">two step chinese named entity recognition based on conditional random fields models </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>lafferty et al, 2001) framework.
</prevsent>
<prevsent>it integrates multiple features based on single chinese character or space separated ascii words.
</prevsent>
</prevsection>
<citsent citstr=" W06-0132 ">
the early designed system (feng et al, 2006) <papid> W06-0132 </papid>is used for the msra ner open track this year.</citsent>
<aftsection>
<nextsent>the output of an external part-of-speech tagging tool and some carefully collected small-scale-character-lists are used as open knowledge.
</nextsent>
<nextsent>some post process steps are also applied to complement the local limitation in models feature engineering.
</nextsent>
<nextsent>the remaining of this paper is organized as follows.
</nextsent>
<nextsent>section 2 introduces conditional random fields model.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M81">
<title id=" I08-5015.xml">experiments in telugu ner a conditional random field approach </title>
<section> features.  </section>
<citcontext>
<prevsection>
<prevsent>there are many types of features used in general ner systems.
</prevsent>
<prevsent>many systems use binary features i.e. the word-internal features, which indicate the presence or absence of particular property in the word.
</prevsent>
</prevsection>
<citsent citstr=" A97-1029 ">
(mikheev, 1997; wacholder et al, 1997; bikel et al, 1997).<papid> A97-1029 </papid></citsent>
<aftsection>
<nextsent>following are examples of binary features commonly used.
</nextsent>
<nextsent>all-caps (ibm), internal capitalization (ebay), initial capital (abdul kalam), un capitalized word (can), 2-digit number 106 (83), digit number 106 (28), 4-digit number (1273), digit number (1984), all digits (8, 31, 1228) etc. the features that correspond to the capitalization are not applicable to telugu.
</nextsent>
<nextsent>we have not used any binary features in our experiments.
</nextsent>
<nextsent>gazette ers are used to check if part of the named entity is present in the gazetteers.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M82">
<title id=" I08-5015.xml">experiments in telugu ner a conditional random field approach </title>
<section> conclusion &amp; future work.  </section>
<citcontext>
<prevsection>
<prevsent>this is due to the fact that the pos tagger also uses the same features in predicting the pos tags.
</prevsent>
<prevsent>prefix, suffix and word are three non-linguistic features that resulted in good performance.
</prevsent>
</prevsection>
<citsent citstr=" W03-0428 ">
we plan to experiment with the character n-gram approach(klein et al, 2003) <papid> W03-0428 </papid>and include gazetteer informa tion.</citsent>
<aftsection>




</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M83">
<title id=" I08-7010.xml">towards an annotated corpus of discourse relations in hindi </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>one of the questions that arises is how the pdtb style annotation can be carried over to languages other than english.
</prevsent>
<prevsent>it may prove to be challenge cross-linguistically, as the guidelines and methodology appropriate for english may not apply as well or directly to other languages, especially when they differ greatly in syntax and morphology.
</prevsent>
</prevsection>
<citsent citstr=" W05-0312 ">
to date, cross-linguistic investigations of connectives in this direction have been carried out for chinese (xue, 2005) <papid> W05-0312 </papid>and turkish (deniz and webber, 2008).</citsent>
<aftsection>
<nextsent>this paper explores discourse relation annotation in hindi, language with rich morphology and free word order.
</nextsent>
<nextsent>we describe our study of explicit connectives?
</nextsent>
<nextsent>in small corpus of hindi texts, discussing them from two perspectives.
</nextsent>
<nextsent>first, we consider the type and distribution of hindi connectives, proposing to annotate wider range only locally, between adjacent sentences.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M84">
<title id=" I08-4030.xml">crf based hybrid model for word segmentation ner and even pos tagging </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in crf models, margin of each character can be gotten, and the margin could be considered as the confidence of that character.
</prevsent>
<prevsent>for the segmentation task, we performed the maximum probability segmentation first, through which each character is assigned bio tag (b represents the beginning of word, represents in word and represents out of word).
</prevsent>
</prevsection>
<citsent citstr=" N06-2049 ">
if the confidence of character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the maximum probability segmentation (r. zhang et al, 2006).<papid> N06-2049 </papid></citsent>
<aftsection>
<nextsent>conditional random fields (crfs) are class of undirected graphical models with exponent distribution (lafferty et al, 2001).
</nextsent>
<nextsent>a common used special case of crfs is linear chain, which has distribution of: )),,,(exp(1)|( 1 1??
</nextsent>
<nextsent>= t ttkk txyyf xyp rrr ?
</nextsent>
<nextsent>(1) where ),,( 1 txyyf ttk r?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M85">
<title id=" I08-4030.xml">crf based hybrid model for word segmentation ner and even pos tagging </title>
<section> feature representation.  </section>
<citcontext>
<prevsection>
<prevsent>single(c0) shows whether current character can form word solely.
</prevsent>
<prevsent>3.2 named entity recognition.
</prevsent>
</prevsection>
<citsent citstr=" H05-1054 ">
most features described in (y. wu et al, 2005) <papid> H05-1054 </papid>are used in our systems.</citsent>
<aftsection>
<nextsent>specifically, the following is the feature templates we used: a) surname(c0): whether current character is in surname list, which includes all first characters of pns in the training corpora.
</nextsent>
<nextsent>1 http://sourceforge.net/project/showfiles.phpgroup_id=201943 b) personname(c0c1c2, c0c1): whether c0c1c2, c0c1 is in the person name list, which contains all pns in the training corpora.
</nextsent>
<nextsent>c) persontitle(c-2c-1): whether c-2c-1 is in the person title list, which is extracted from the previous two characters of each pn in the training corpora.
</nextsent>
<nextsent>d) locationname(c0c1,c0c1c2,c0c1c2c3): whether c0c1,c0c1c2,c0c1c2c3 is in the location name list, which includes all lns in the training corpora.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M87">
<title id=" I08-5014.xml">named entity recognition for indian languages </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the rule based approach uses the morphological and contextual evidence (kim and woodland, 2000) of natural language and consequently determines the named entities.
</prevsent>
<prevsent>this eventually leads to formation of some language specific rules for identifying named entities.
</prevsent>
</prevsection>
<citsent citstr=" W02-2019 ">
the statistical techniques use large annotated data to train model (malouf, 2002) (<papid> W02-2019 </papid>like hidden markov model) and subsequently examine it with the test data.</citsent>
<aftsection>
<nextsent>both the methods mentioned above require the efforts of language expert.
</nextsent>
<nextsent>an appropriately large set of annotated data is yet to be made available for the indian languages.
</nextsent>
<nextsent>consequently, the application of the statistical technique for indian languages is not very feasible.
</nextsent>
<nextsent>this paper deals with new technique to recognize named entities of different languages.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M88">
<title id=" I08-4016.xml">chinese ner using crfs and logic for the fourth sighan bakeoff </title>
<section> conditional random fields as base.  </section>
<citcontext>
<prevsection>
<prevsent>model conditional random fields (crfs) (lafferty et al, 2001) are undirected graphical models trained to maximize the conditional probability of the desired outputs given the corresponding inputs.
</prevsent>
<prevsent>crfs have been shown to perform well on chinese ner shared task on sighan-4 (zhou et al.
</prevsent>
</prevsection>
<citsent citstr=" W06-0130 ">
(2006), chen et al (2006<papid> W06-0130 </papid>a), chen et al (2006<papid> W06-0130 </papid>b)).</citsent>
<aftsection>
<nextsent>we employ crfs as the base model in our framework.
</nextsent>
<nextsent>in this base model, we design features similar to the state-of-theart crf models for chinese ner.
</nextsent>
<nextsent>we use character features, word segmentation features, part-of-speech (pos) features, and dictionary features, as described below.character features: these features are the current character, 2 characters preceding the current character and 2 following the current character.
</nextsent>
<nextsent>we extend the window size to 7 but find that it slightly hurts.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M96">
<title id=" I08-7002.xml">gazetteer preparation for named entity recognition in indian languages </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our approach is to trans literate the relevant english resources and name dictionaries into indian languages to make them useful for indian language ner task.
</prevsent>
<prevsent>but direct transliteration from english to an indian languages is not easy.
</prevsent>
</prevsection>
<citsent citstr=" P06-2025 ">
few attempts are taken to build english to indian language transliteration systems but the word agreement ratio (war) reached is upto 69.3% (ekbal et al, 2006).<papid> P06-2025 </papid>we have attempted to build transliteration system which uses an intermediate alphabet.</citsent>
<aftsection>
<nextsent>both the english and the indian language strings are transliterated to the intermediate alphabet and for aenglish-indian language pair, if the transliterated intermediate alphabet strings are same then we have concluded that the strings are the transliteration ofone another.
</nextsent>
<nextsent>we have transliterated the available english name lists into the intermediate alphabet and these might be used as gazetteers.
</nextsent>
<nextsent>the indian language words need to be transliterated to the intermediate format to check whether the word is in agazetteer or not.
</nextsent>
<nextsent>this system does not trans liter ate the english name lists into indian languages but makes them useful in indian languages ner task.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M97">
<title id=" I08-7002.xml">gazetteer preparation for named entity recognition in indian languages </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>the linguistic approach typically uses rule-based models manually written by linguists.
</prevsent>
<prevsent>ml based techniques make use of large amount of annotated training data to acquire high-level language knowledge.
</prevsent>
</prevsection>
<citsent citstr=" A97-1029 ">
several ml techniques like hidden markov model (hmm)(bikel et al, 1997), <papid> A97-1029 </papid>maximum entropy model(maxent) (borthwick, 1999), conditional random field(crf) (li and mccallum, 2004) etc. have been successfully used for the ner task.</citsent>
<aftsection>
<nextsent>both the approaches may make use of gazetteer information to build systems.
</nextsent>
<nextsent>there are many systems which use gazette ers to improve the accuracy.
</nextsent>
<nextsent>ralph grishman has developed rule-based ner system which uses some specialized name dictionaries including names of all countries, names of major cities, names of companies, common first names etc (grishman, 1995).
</nextsent>
<nextsent>another rule based ner system is developed by wakao et al (1996) <papid> C96-1071 </papid>which has used several gazette ers like organization names, location names, person names, human titles etc.we will now mention some ml based systems.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M98">
<title id=" I08-7002.xml">gazetteer preparation for named entity recognition in indian languages </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>there are many systems which use gazette ers to improve the accuracy.
</prevsent>
<prevsent>ralph grishman has developed rule-based ner system which uses some specialized name dictionaries including names of all countries, names of major cities, names of companies, common first names etc (grishman, 1995).
</prevsent>
</prevsection>
<citsent citstr=" C96-1071 ">
another rule based ner system is developed by wakao et al (1996) <papid> C96-1071 </papid>which has used several gazette ers like organization names, location names, person names, human titles etc.we will now mention some ml based systems.</citsent>
<aftsection>
<nextsent>mene is maxent based system developed by borthwick.
</nextsent>
<nextsent>this system has used 8 dictionaries (borthwick, 1999), which are: first names (1),  first names (245), corporate names (10),  corporate names (300), corporate names without suffix (10), names without suffix (300), colleges and universities (1), colleges and universities (225), corporate suffixes (244), date and time (51) etc. the italics numbers in bracket indicates the size of the dictionaries.
</nextsent>
<nextsent>the hybrid system developed by srihari et al(2000) <papid> A00-1034 </papid>combines several modules built by using maxent, hmmand handcrafted rules.</nextsent>
<nextsent>this system uses the following gazetteers: first name (8),  first name (000), family name (14),  family name (000) and big gazetteer of locations (250), gazetteer of locations (000).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M99">
<title id=" I08-7002.xml">gazetteer preparation for named entity recognition in indian languages </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>mene is maxent based system developed by borthwick.
</prevsent>
<prevsent>this system has used 8 dictionaries (borthwick, 1999), which are: first names (1),  first names (245), corporate names (10),  corporate names (300), corporate names without suffix (10), names without suffix (300), colleges and universities (1), colleges and universities (225), corporate suffixes (244), date and time (51) etc. the italics numbers in bracket indicates the size of the dictionaries.
</prevsent>
</prevsection>
<citsent citstr=" A00-1034 ">
the hybrid system developed by srihari et al(2000) <papid> A00-1034 </papid>combines several modules built by using maxent, hmmand handcrafted rules.</citsent>
<aftsection>
<nextsent>this system uses the following gazetteers: first name (8),  first name (000), family name (14),  family name (000) and big gazetteer of locations (250), gazetteer of locations (000).
</nextsent>
<nextsent>there are many other systems which have used name dictionaries to improve the accuracy.
</nextsent>
<nextsent>kozareva (2006) <papid> E06-3004 </papid>described methodology to generate gazetteer lists automatically for spanish and to build ner system with labeled and unlabeleddata.</nextsent>
<nextsent>the location gazetteer is built by finding location patterns which looks for specific prepositions.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M100">
<title id=" I08-7002.xml">gazetteer preparation for named entity recognition in indian languages </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>this system uses the following gazetteers: first name (8),  first name (000), family name (14),  family name (000) and big gazetteer of locations (250), gazetteer of locations (000).
</prevsent>
<prevsent>there are many other systems which have used name dictionaries to improve the accuracy.
</prevsent>
</prevsection>
<citsent citstr=" E06-3004 ">
kozareva (2006) <papid> E06-3004 </papid>described methodology to generate gazetteer lists automatically for spanish and to build ner system with labeled and unlabeleddata.</citsent>
<aftsection>
<nextsent>the location gazetteer is built by finding location patterns which looks for specific prepositions.
</nextsent>
<nextsent>and the person gazetteer is constructed with graph exploration algorithm.
</nextsent>
<nextsent>transliteration is also very important topic and lots of transliteration systems for different languages have been developed using different approaches.
</nextsent>
<nextsent>the basic approaches for transliteration are phoneme based or spelling-based.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M101">
<title id=" I08-7002.xml">gazetteer preparation for named entity recognition in indian languages </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>transliteration is also very important topic and lots of transliteration systems for different languages have been developed using different approaches.
</prevsent>
<prevsent>the basic approaches for transliteration are phoneme based or spelling-based.
</prevsent>
</prevsection>
<citsent citstr=" J98-4003 ">
a phonemebased statistical transliteration system from arabic to english was developed by knight and graehl(1998).<papid> J98-4003 </papid></citsent>
<aftsection>
<nextsent>this system uses finite state transducer that implements transformation rules to do back-transliteration.
</nextsent>
<nextsent>a spelling-based model that directly maps english letter sequences into arabic letters was developed by al-onaizan andknight(2002).
</nextsent>
<nextsent>several transliteration systems exist for english-japanese, english-chinese, english spanish and many other languages to english.
</nextsent>
<nextsent>butvery few attempts have been reported on the development of transliteration systems between indian languages and english.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M103">
<title id=" I08-4013.xml">chinese word segmentation and named entity recognition based on conditional random fields </title>
<section> chinese named entity recognition.  </section>
<citcontext>
<prevsection>
<prevsent>ner is always limited by its lower recall due to the imbalanced distribution where the none class dominates the entity classes.
</prevsent>
<prevsent>classifiers built on such dataset typically have higher precision and lower recall and tend to over produce the none 1 we use the crf++ v4.5 software from.
</prevsent>
</prevsection>
<citsent citstr=" P06-2060 ">
http://chasen.org/~taku/software/crf++/ 90 sixth sighan workshop on chinese language processing class (kambhatla, 2006).<papid> P06-2060 </papid></citsent>
<aftsection>
<nextsent>taking sighan bakeoff 2006 (levow, 2006) <papid> W06-0115 </papid>as an example, the recall is lower about 5% than the precision for each submitted system on msra and cityu closed track.</nextsent>
<nextsent>if we could improve ner recall but keep its relatively high precision, the overall f-measure will be improved as result.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M104">
<title id=" I08-4013.xml">chinese word segmentation and named entity recognition based on conditional random fields </title>
<section> chinese named entity recognition.  </section>
<citcontext>
<prevsection>
<prevsent>classifiers built on such dataset typically have higher precision and lower recall and tend to over produce the none 1 we use the crf++ v4.5 software from.
</prevsent>
<prevsent>http://chasen.org/~taku/software/crf++/ 90 sixth sighan workshop on chinese language processing class (kambhatla, 2006).<papid> P06-2060 </papid></prevsent>
</prevsection>
<citsent citstr=" W06-0115 ">
taking sighan bakeoff 2006 (levow, 2006) <papid> W06-0115 </papid>as an example, the recall is lower about 5% than the precision for each submitted system on msra and cityu closed track.</citsent>
<aftsection>
<nextsent>if we could improve ner recall but keep its relatively high precision, the overall f-measure will be improved as result.
</nextsent>
<nextsent>we design two kinds of effective features: 0/1 features and non-local features to achieve this objective.
</nextsent>
<nextsent>our final systems utilize these features together with the local features to perform ner task.
</nextsent>
<nextsent>2.1 local features.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M106">
<title id=" I08-4013.xml">chinese word segmentation and named entity recognition based on conditional random fields </title>
<section> chinese named entity recognition.  </section>
<citcontext>
<prevsection>
<prevsent>but often this assumption does not hold because non local dependencies are prevalent in natural language (including the ner task).
</prevsent>
<prevsent>how to utilize the non-local dependencies is key issue inner task.
</prevsent>
</prevsection>
<citsent citstr=" P06-1141 ">
up to now, few researches have been devoted to this issue; existing works mainly focus on using the non-local information for improving ner label consistency (krishnan and manning, 2006).<papid> P06-1141 </papid></citsent>
<aftsection>
<nextsent>there are two methods to use non-local information.
</nextsent>
<nextsent>one is to add additional edges to graphical model structure to represent the distant dependencies and the other is to encode the non-locality with non-local features.
</nextsent>
<nextsent>in the first approach, heuristic rules are used to find the dependencies (bunescu and mooney, 2004) <papid> P04-1056 </papid>or penalties for label inconsistency are required to handset ad-hoc (finkel et al, 2005).<papid> P05-1045 </papid></nextsent>
<nextsent>furthermore, high computational cost is spent for approximate inference.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M107">
<title id=" I08-4013.xml">chinese word segmentation and named entity recognition based on conditional random fields </title>
<section> chinese named entity recognition.  </section>
<citcontext>
<prevsection>
<prevsent>there are two methods to use non-local information.
</prevsent>
<prevsent>one is to add additional edges to graphical model structure to represent the distant dependencies and the other is to encode the non-locality with non-local features.
</prevsent>
</prevsection>
<citsent citstr=" P04-1056 ">
in the first approach, heuristic rules are used to find the dependencies (bunescu and mooney, 2004) <papid> P04-1056 </papid>or penalties for label inconsistency are required to handset ad-hoc (finkel et al, 2005).<papid> P05-1045 </papid></citsent>
<aftsection>
<nextsent>furthermore, high computational cost is spent for approximate inference.
</nextsent>
<nextsent>in order to establish the long dependencies easily and overcome the disadvantage of the approximate inference, krishnan and manning (2006) <papid> P06-1141 </papid>propose two-stage approach using crfs framework with extract inference.</nextsent>
<nextsent>they represent the non-locality with non-local features, and extract them from the output of the first stage crf with local context alone; then they incorporate the non-local features into the second crf.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M108">
<title id=" I08-4013.xml">chinese word segmentation and named entity recognition based on conditional random fields </title>
<section> chinese named entity recognition.  </section>
<citcontext>
<prevsection>
<prevsent>there are two methods to use non-local information.
</prevsent>
<prevsent>one is to add additional edges to graphical model structure to represent the distant dependencies and the other is to encode the non-locality with non-local features.
</prevsent>
</prevsection>
<citsent citstr=" P05-1045 ">
in the first approach, heuristic rules are used to find the dependencies (bunescu and mooney, 2004) <papid> P04-1056 </papid>or penalties for label inconsistency are required to handset ad-hoc (finkel et al, 2005).<papid> P05-1045 </papid></citsent>
<aftsection>
<nextsent>furthermore, high computational cost is spent for approximate inference.
</nextsent>
<nextsent>in order to establish the long dependencies easily and overcome the disadvantage of the approximate inference, krishnan and manning (2006) <papid> P06-1141 </papid>propose two-stage approach using crfs framework with extract inference.</nextsent>
<nextsent>they represent the non-locality with non-local features, and extract them from the output of the first stage crf with local context alone; then they incorporate the non-local features into the second crf.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M112">
<title id=" I08-4015.xml">the character based crf segmenter of msraneu for the 4th bakeoff </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>to further improve the performance of our segmenter, we employ word-based approach to increase the in-vocabulary (iv) word recall and post-processing to in crease the out-of-vocabulary (oov) word recall.
</prevsent>
<prevsent>we participate in the word segmentation closed test on all five corpora and our system achieved four second best and one the fifth in all the five corpora.
</prevsent>
</prevsection>
<citsent citstr=" W02-1815 ">
since chinese word segmentation was firstly treated as character-based tagging task in (xue and converse, 2002), <papid> W02-1815 </papid>this method has been widely accepted and further developed by researchers (peng et al, 2004), (<papid> C04-1081 </papid>tseng et al, 2005), (<papid> I05-3027 </papid>low et al., 2005), (<papid> I05-3025 </papid>zhao et al, 2006).</citsent>
<aftsection>
<nextsent>thus, as powerful sequence tagging model, crf became the dominant method in the bakeoff 2006 (levow, 2006).<papid> W06-0115 </papid></nextsent>
<nextsent>in this paper, we improve basic segmenter under the crf work frame in two aspects, namely iv and oov identification respectively.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M113">
<title id=" I08-4015.xml">the character based crf segmenter of msraneu for the 4th bakeoff </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>to further improve the performance of our segmenter, we employ word-based approach to increase the in-vocabulary (iv) word recall and post-processing to in crease the out-of-vocabulary (oov) word recall.
</prevsent>
<prevsent>we participate in the word segmentation closed test on all five corpora and our system achieved four second best and one the fifth in all the five corpora.
</prevsent>
</prevsection>
<citsent citstr=" C04-1081 ">
since chinese word segmentation was firstly treated as character-based tagging task in (xue and converse, 2002), <papid> W02-1815 </papid>this method has been widely accepted and further developed by researchers (peng et al, 2004), (<papid> C04-1081 </papid>tseng et al, 2005), (<papid> I05-3027 </papid>low et al., 2005), (<papid> I05-3025 </papid>zhao et al, 2006).</citsent>
<aftsection>
<nextsent>thus, as powerful sequence tagging model, crf became the dominant method in the bakeoff 2006 (levow, 2006).<papid> W06-0115 </papid></nextsent>
<nextsent>in this paper, we improve basic segmenter under the crf work frame in two aspects, namely iv and oov identification respectively.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M114">
<title id=" I08-4015.xml">the character based crf segmenter of msraneu for the 4th bakeoff </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>to further improve the performance of our segmenter, we employ word-based approach to increase the in-vocabulary (iv) word recall and post-processing to in crease the out-of-vocabulary (oov) word recall.
</prevsent>
<prevsent>we participate in the word segmentation closed test on all five corpora and our system achieved four second best and one the fifth in all the five corpora.
</prevsent>
</prevsection>
<citsent citstr=" I05-3027 ">
since chinese word segmentation was firstly treated as character-based tagging task in (xue and converse, 2002), <papid> W02-1815 </papid>this method has been widely accepted and further developed by researchers (peng et al, 2004), (<papid> C04-1081 </papid>tseng et al, 2005), (<papid> I05-3027 </papid>low et al., 2005), (<papid> I05-3025 </papid>zhao et al, 2006).</citsent>
<aftsection>
<nextsent>thus, as powerful sequence tagging model, crf became the dominant method in the bakeoff 2006 (levow, 2006).<papid> W06-0115 </papid></nextsent>
<nextsent>in this paper, we improve basic segmenter under the crf work frame in two aspects, namely iv and oov identification respectively.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M115">
<title id=" I08-4015.xml">the character based crf segmenter of msraneu for the 4th bakeoff </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>to further improve the performance of our segmenter, we employ word-based approach to increase the in-vocabulary (iv) word recall and post-processing to in crease the out-of-vocabulary (oov) word recall.
</prevsent>
<prevsent>we participate in the word segmentation closed test on all five corpora and our system achieved four second best and one the fifth in all the five corpora.
</prevsent>
</prevsection>
<citsent citstr=" I05-3025 ">
since chinese word segmentation was firstly treated as character-based tagging task in (xue and converse, 2002), <papid> W02-1815 </papid>this method has been widely accepted and further developed by researchers (peng et al, 2004), (<papid> C04-1081 </papid>tseng et al, 2005), (<papid> I05-3027 </papid>low et al., 2005), (<papid> I05-3025 </papid>zhao et al, 2006).</citsent>
<aftsection>
<nextsent>thus, as powerful sequence tagging model, crf became the dominant method in the bakeoff 2006 (levow, 2006).<papid> W06-0115 </papid></nextsent>
<nextsent>in this paper, we improve basic segmenter under the crf work frame in two aspects, namely iv and oov identification respectively.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M116">
<title id=" I08-4015.xml">the character based crf segmenter of msraneu for the 4th bakeoff </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we participate in the word segmentation closed test on all five corpora and our system achieved four second best and one the fifth in all the five corpora.
</prevsent>
<prevsent>since chinese word segmentation was firstly treated as character-based tagging task in (xue and converse, 2002), <papid> W02-1815 </papid>this method has been widely accepted and further developed by researchers (peng et al, 2004), (<papid> C04-1081 </papid>tseng et al, 2005), (<papid> I05-3027 </papid>low et al., 2005), (<papid> I05-3025 </papid>zhao et al, 2006).</prevsent>
</prevsection>
<citsent citstr=" W06-0115 ">
thus, as powerful sequence tagging model, crf became the dominant method in the bakeoff 2006 (levow, 2006).<papid> W06-0115 </papid></citsent>
<aftsection>
<nextsent>in this paper, we improve basic segmenter under the crf work frame in two aspects, namely iv and oov identification respectively.
</nextsent>
<nextsent>we use the result from word-based segmentation to revise the crf output so that we gain higher iv word recall.
</nextsent>
<nextsent>for the oov part post-processing rule is proposed to find those oov words which are wrongly segmented into several fractions.
</nextsent>
<nextsent>our system performs well in the fourth bakeoff, achieving four second best and on the fifth in all the five corpora.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M118">
<title id=" I08-4015.xml">the character based crf segmenter of msraneu for the 4th bakeoff </title>
<section> our word segmentation system.  </section>
<citcontext>
<prevsection>
<prevsent>thus, we use the trigram language model to select top (b is constant predefined before search and in our experiment 3 is used) best candidates with highest probability at each stage so that the search algorithm can work in practice.
</prevsent>
<prevsent>finally, when the whole sentence has been read, the best candidate with the highest probability will be selected as the segmentation result.
</prevsent>
</prevsection>
<citsent citstr=" N06-2049 ">
after we get word-based segmentation result, we use it to revise the crf tagging result similar to (zhang et al, 2006).<papid> N06-2049 </papid></citsent>
<aftsection>
<nextsent>since word-based segmentation result also corresponds to tag sequence according to the 6-tag set, we now have two tags for each character, word-based tag (wt) and crf tag (ct).
</nextsent>
<nextsent>which tag will be kept as the final result depends on marginal probability (mp) of the ct. here, we give short explanation about what is the mp of the ct. suppose there is sentence mcccc ...10?
</nextsent>
<nextsent>, where ic is the character this sentence containing.
</nextsent>
<nextsent>crf model gives this sentence optimal tag sequence mtttt ...10?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M119">
<title id=" I08-4015.xml">the character based crf segmenter of msraneu for the 4th bakeoff </title>
<section> our word segmentation system.  </section>
<citcontext>
<prevsection>
<prevsent>then, we scan these sentences to find all n-grams (n from 2 to 7) and count their occurrence.
</prevsent>
<prevsent>if certain n-gram appears more than threshold and this n-gram never appears in training corpus, the n-gram will be selected as word candidate.
</prevsent>
</prevsection>
<citsent citstr=" W04-1122 ">
then, we filter these word candidates according to the context entropy (luo and song, 2004).<papid> W04-1122 </papid></citsent>
<aftsection>
<nextsent>assume is word candidate appears times in the current sentence and last sentences and },...,,{ 10 laaa??
</nextsent>
<nextsent>is the set of left side characters of . left context entropy (lce) can be defined as: ? ?
</nextsent>
<nextsent>ia i wac nwacnwlce ),(log),( 1)( here, ),( wac is the count of concurrence of ia and . for the right context entropy, the definition is the same except change left into right.
</nextsent>
<nextsent>now, we define context entropy (ce) of word candidate as ))(),(min( wrcewlce . the word candidates with ce larger than predefined threshold will be bind as whole word in test corpus no matter what tag sequence the segmenter giving it.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M121">
<title id=" I08-4001.xml">an example based decoder for spoken language machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this paper, we propose an example-based decoder for statistical machine translation (smt) system, which is used for spoken language machine translation.
</prevsent>
<prevsent>in this way, it will help to solve the re-ordering problem and other problems for spoken language mt, such as lots of omissions, idioms etc. through experiments, we show that this approach obtains improvements over the baseline on chinese-english spoken language translation task.
</prevsent>
</prevsection>
<citsent citstr=" P02-1038 ">
the state-of-the-art statistical machine translation (smt) model is the log-linear model (och and ney, 2002), <papid> P02-1038 </papid>which provides framework to incorporate any useful knowledge for machine translation, such as translation model, language model etc. in smt system, one important problem is the re-ordering between words and phrases, especially when the source language and target language are very different in word order, such as chinese and english.</citsent>
<aftsection>
<nextsent>for the spoken language translation, there ordering problem will be more crucial, since the spoken language is more flexible in word order.
</nextsent>
<nextsent>in addition, lots of omissions and idioms make the translation more difficult.
</nextsent>
<nextsent>however, there exists some  useful  features, such as, most of the spoken text is shorter than the written text and there are some fixed translation structures.
</nextsent>
<nextsent>for example, ( ????
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M122">
<title id=" I08-4001.xml">an example based decoder for spoken language machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>/ would you please ? ?
</prevsent>
<prevsent>), (???/may i??).
</prevsent>
</prevsection>
<citsent citstr=" P05-1033 ">
we can learn these fixed structures and take them as rules, chiang (2005) <papid> P05-1033 </papid>presents method to learn these rules, and uses them in the smt.</citsent>
<aftsection>
<nextsent>generally, the number of these rules will be very large.
</nextsent>
<nextsent>in this paper, we propose an example-based decoder in smt model, which will use the translation examples to keep the translation structure, i.e. constraint the reordering, and make the omitted words having the chance to be translated.
</nextsent>
<nextsent>the rest of this paper is organized as follows: since our decoder is based on the inversion transduction grammars (itg) (wu, 1997), we introduce the itg in section 2 and describe the derived smt model.
</nextsent>
<nextsent>in section 3, we design the example-based decoder.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M124">
<title id=" I08-4001.xml">an example based decoder for spoken language machine translation </title>
<section> the smt model.  </section>
<citcontext>
<prevsection>
<prevsent>in the example, the region (1..3, 3..5) is independent, and the block ( ??
</prevsent>
<prevsent>the nearest cassino) is also independent.
</prevsent>
</prevsection>
<citsent citstr=" H05-1011 ">
in order to obtain the word alignment satisfying the itg constraint, wu(1997) propose dp algorithm, and we (chao and li, 2007) have transferred the constraint to four simple position judgment procedures in an explicit way, so that we can incorporate the itg constraint as feature into loglinear word alignment model (moore, 2005).<papid> H05-1011 </papid></citsent>
<aftsection>
<nextsent>after obtaining the word-aligned corpus, in which each word alignment satisfy the itg constraint, we can extract the blocks in straightforward way.
</nextsent>
<nextsent>for the word alignment forms hierarchical binary tree, we choose each constituent as block.
</nextsent>
<nextsent>each block is formed by combining one or more links, and must be independent.
</nextsent>
<nextsent>considering the data sparseness, we limit the length of each block as (here n=3~5).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M125">
<title id=" I08-4001.xml">an example based decoder for spoken language machine translation </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>table 1 shows the statistics for the training corpus, development set and test set.
</prevsent>
<prevsent>in order to compare with the other smt systems, we choose the moses1, which is an extension to the state-of-the-art smt system pharaoh (koehn, 2004).
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
we use the default tool in the moses to train the model and tune the weights, in which the word alignment tool is giza++ (och and ney 2003) <papid> J03-1002 </papid>and the language model tool is srilm(stolcke, 2002).</citsent>
<aftsection>
<nextsent>the test results are showed in table2.
</nextsent>
<nextsent>the first column lists the different mt systems, and the second column lists the bleu scores (pap ineni et. al, 2002) for the four decoders.
</nextsent>
<nextsent>the first system is the moses, and the second is our smt system described in section 2, which using cky-style decoder.
</nextsent>
<nextsent>we take them as base line systems.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M126">
<title id=" I08-4001.xml">an example based decoder for spoken language machine translation </title>
<section> related works.  </section>
<citcontext>
<prevsection>
<prevsent>test results for several systems.
</prevsent>
<prevsent>there is some works about the hybrid machine translation.
</prevsent>
</prevsection>
<citsent citstr=" W05-0833 ">
one way is to merge ebmt and smt resources, such as groves and way (2005).<papid> W05-0833 </papid></citsent>
<aftsection>
<nextsent>another way is to implement an exmaple-based decoder, watanabe and sumita (2003) presents an example-based decoder, which using information retrieval framework to retrieve the examples; and when decoding, which runs hill-climbing algorithm to modify the translation example ( ck, ek, ak) to obtain an alignment ( c0, k, k).
</nextsent>
<nextsent>in this paper, we proposed smt system with an example-based decoder for the spoken language machine translation.
</nextsent>
<nextsent>this approach will take advantage of the constituent tree within the translation examples to constrain the flexible word reordering in the spoken language, and it will also make the omitted words have the chance to be translated.
</nextsent>
<nextsent>combining with the re-ordering model and the translation models in the smt, the exam ple-based decoder obtains an improvement over the baseline phrase-based smt system.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M127">
<title id=" I08-5007.xml">named entity recognition for telugu </title>
<section> approaches toner.  </section>
<citcontext>
<prevsection>
<prevsent>ner emerged as one of the sub-tasks of thedarpa-sponsored message understanding conference (mucs).
</prevsent>
<prevsent>the task has important significance in the internet search engines and is an important task in many of the language engineering applications such as machine translation, question-answeringsystems, indexing for information retrieval and automatic summarization.
</prevsent>
</prevsection>
<citsent citstr=" C02-1054 ">
there has been considerable amount of work on ner in english (isozaki and kazawa, 2002; <papid> C02-1054 </papid>zhang and johnson, 2003; <papid> W03-0434 </papid>petasis et al, 2001; <papid> P01-1055 </papid>mikheev et al, 1999).<papid> E99-1001 </papid></citsent>
<aftsection>
<nextsent>much of the previous work on name finding is based on one of the following approaches: (1) hand-crafted or automatically acquired rules or finite state patterns (2) look up from large name listsor other specialized resources (3) data driven approaches exploiting the statistical properties of the language (statistical models).the earliest work in named-entity recognition involved hand-crafted rules based on pattern matching (appelt et al, 1993).
</nextsent>
<nextsent>for instance, sequence of capitalized words ending in inc.?
</nextsent>
<nextsent>is typically thename of an organization in the us, so one could implement rule to that effect.
</nextsent>
<nextsent>another example ofsuch rule is: title capitalized word ? title person name.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M128">
<title id=" I08-5007.xml">named entity recognition for telugu </title>
<section> approaches toner.  </section>
<citcontext>
<prevsection>
<prevsent>ner emerged as one of the sub-tasks of thedarpa-sponsored message understanding conference (mucs).
</prevsent>
<prevsent>the task has important significance in the internet search engines and is an important task in many of the language engineering applications such as machine translation, question-answeringsystems, indexing for information retrieval and automatic summarization.
</prevsent>
</prevsection>
<citsent citstr=" W03-0434 ">
there has been considerable amount of work on ner in english (isozaki and kazawa, 2002; <papid> C02-1054 </papid>zhang and johnson, 2003; <papid> W03-0434 </papid>petasis et al, 2001; <papid> P01-1055 </papid>mikheev et al, 1999).<papid> E99-1001 </papid></citsent>
<aftsection>
<nextsent>much of the previous work on name finding is based on one of the following approaches: (1) hand-crafted or automatically acquired rules or finite state patterns (2) look up from large name listsor other specialized resources (3) data driven approaches exploiting the statistical properties of the language (statistical models).the earliest work in named-entity recognition involved hand-crafted rules based on pattern matching (appelt et al, 1993).
</nextsent>
<nextsent>for instance, sequence of capitalized words ending in inc.?
</nextsent>
<nextsent>is typically thename of an organization in the us, so one could implement rule to that effect.
</nextsent>
<nextsent>another example ofsuch rule is: title capitalized word ? title person name.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M129">
<title id=" I08-5007.xml">named entity recognition for telugu </title>
<section> approaches toner.  </section>
<citcontext>
<prevsection>
<prevsent>ner emerged as one of the sub-tasks of thedarpa-sponsored message understanding conference (mucs).
</prevsent>
<prevsent>the task has important significance in the internet search engines and is an important task in many of the language engineering applications such as machine translation, question-answeringsystems, indexing for information retrieval and automatic summarization.
</prevsent>
</prevsection>
<citsent citstr=" P01-1055 ">
there has been considerable amount of work on ner in english (isozaki and kazawa, 2002; <papid> C02-1054 </papid>zhang and johnson, 2003; <papid> W03-0434 </papid>petasis et al, 2001; <papid> P01-1055 </papid>mikheev et al, 1999).<papid> E99-1001 </papid></citsent>
<aftsection>
<nextsent>much of the previous work on name finding is based on one of the following approaches: (1) hand-crafted or automatically acquired rules or finite state patterns (2) look up from large name listsor other specialized resources (3) data driven approaches exploiting the statistical properties of the language (statistical models).the earliest work in named-entity recognition involved hand-crafted rules based on pattern matching (appelt et al, 1993).
</nextsent>
<nextsent>for instance, sequence of capitalized words ending in inc.?
</nextsent>
<nextsent>is typically thename of an organization in the us, so one could implement rule to that effect.
</nextsent>
<nextsent>another example ofsuch rule is: title capitalized word ? title person name.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M130">
<title id=" I08-5007.xml">named entity recognition for telugu </title>
<section> approaches toner.  </section>
<citcontext>
<prevsection>
<prevsent>ner emerged as one of the sub-tasks of thedarpa-sponsored message understanding conference (mucs).
</prevsent>
<prevsent>the task has important significance in the internet search engines and is an important task in many of the language engineering applications such as machine translation, question-answeringsystems, indexing for information retrieval and automatic summarization.
</prevsent>
</prevsection>
<citsent citstr=" E99-1001 ">
there has been considerable amount of work on ner in english (isozaki and kazawa, 2002; <papid> C02-1054 </papid>zhang and johnson, 2003; <papid> W03-0434 </papid>petasis et al, 2001; <papid> P01-1055 </papid>mikheev et al, 1999).<papid> E99-1001 </papid></citsent>
<aftsection>
<nextsent>much of the previous work on name finding is based on one of the following approaches: (1) hand-crafted or automatically acquired rules or finite state patterns (2) look up from large name listsor other specialized resources (3) data driven approaches exploiting the statistical properties of the language (statistical models).the earliest work in named-entity recognition involved hand-crafted rules based on pattern matching (appelt et al, 1993).
</nextsent>
<nextsent>for instance, sequence of capitalized words ending in inc.?
</nextsent>
<nextsent>is typically thename of an organization in the us, so one could implement rule to that effect.
</nextsent>
<nextsent>another example ofsuch rule is: title capitalized word ? title person name.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M131">
<title id=" I08-5007.xml">named entity recognition for telugu </title>
<section> approaches toner.  </section>
<citcontext>
<prevsection>
<prevsent>machine learning techniques are relatively independent of language and domain and no expert knowledge is needed.
</prevsent>
<prevsent>there has been lot of work on ner for english employing the machine learning techniques, using both supervised learning and unsupervised learning.
</prevsent>
</prevsection>
<citsent citstr=" W99-0613 ">
unsupervised learning approaches do not require labelled training data training requires only very few seed lists and large unannotated corpora (collins and singer, 1999).<papid> W99-0613 </papid></citsent>
<aftsection>
<nextsent>supervised approaches can achieve good performance when large amounts of high quality training data is available.
</nextsent>
<nextsent>statistical methods such as hmm (bikel et al, 1997; <papid> A97-1029 </papid>zhou and su, 2001), decision tree model (baluja et al, 2000; isozaki, 2001), <papid> P01-1041 </papid>and conditional random fields (mccallum, 2003) have been used.</nextsent>
<nextsent>generative models such as hidden markov models (bikel et al, 1997; <papid> A97-1029 </papid>zhou and su, 2001) have shown excellent performance on the message understanding conference (muc) data-set (chinchor,1997).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M132">
<title id=" I08-5007.xml">named entity recognition for telugu </title>
<section> approaches toner.  </section>
<citcontext>
<prevsection>
<prevsent>unsupervised learning approaches do not require labelled training data training requires only very few seed lists and large unannotated corpora (collins and singer, 1999).<papid> W99-0613 </papid></prevsent>
<prevsent>supervised approaches can achieve good performance when large amounts of high quality training data is available.</prevsent>
</prevsection>
<citsent citstr=" A97-1029 ">
statistical methods such as hmm (bikel et al, 1997; <papid> A97-1029 </papid>zhou and su, 2001), decision tree model (baluja et al, 2000; isozaki, 2001), <papid> P01-1041 </papid>and conditional random fields (mccallum, 2003) have been used.</citsent>
<aftsection>
<nextsent>generative models such as hidden markov models (bikel et al, 1997; <papid> A97-1029 </papid>zhou and su, 2001) have shown excellent performance on the message understanding conference (muc) data-set (chinchor,1997).</nextsent>
<nextsent>however, developing large scale, high quality training data is itself costly affair.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M134">
<title id=" I08-5007.xml">named entity recognition for telugu </title>
<section> approaches toner.  </section>
<citcontext>
<prevsection>
<prevsent>unsupervised learning approaches do not require labelled training data training requires only very few seed lists and large unannotated corpora (collins and singer, 1999).<papid> W99-0613 </papid></prevsent>
<prevsent>supervised approaches can achieve good performance when large amounts of high quality training data is available.</prevsent>
</prevsection>
<citsent citstr=" P01-1041 ">
statistical methods such as hmm (bikel et al, 1997; <papid> A97-1029 </papid>zhou and su, 2001), decision tree model (baluja et al, 2000; isozaki, 2001), <papid> P01-1041 </papid>and conditional random fields (mccallum, 2003) have been used.</citsent>
<aftsection>
<nextsent>generative models such as hidden markov models (bikel et al, 1997; <papid> A97-1029 </papid>zhou and su, 2001) have shown excellent performance on the message understanding conference (muc) data-set (chinchor,1997).</nextsent>
<nextsent>however, developing large scale, high quality training data is itself costly affair.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M137">
<title id=" I08-6004.xml">some experiments in mining named entity transliteration pairs from comparable corpora </title>
<section> abstract </section>
<citcontext>
<prevsection>
<prevsent>further, such pairs may also be used for training transliteration systems, if they are transliterations of each other.
</prevsent>
<prevsent>in this paper, we profile the performance of mining methodology in mining parallel named entity transliteration pairs in english and an indian language, tamil, leveraging linguistic tools in english, and article-aligned comparable corpora in the two languages.
</prevsent>
</prevsection>
<citsent citstr=" N06-1011 ">
we adopt methodology parallel to that of [klementiev and roth, 2006], <papid> N06-1011 </papid>but we focus instead on mining parallel named entity transliteration pairs, using well-trained linear classifier to identify transliteration pairs.</citsent>
<aftsection>
<nextsent>we profile the performance at several operating parameters of our algorithm and present the results that show the potential of the approach in mining transliterations pairs; in addition, we uncover host of issues that need to be resolved, for effective mining of parallel named entity transliteration pairs.
</nextsent>
<nextsent>parallel named entity (ne) pairs are important resources in several nlp tasks, from supporting cross-lingual information retrieval (clir) systems, to improving machine translation (mt) systems.
</nextsent>
<nextsent>in addition, such pairs may also be used for developing transliteration systems, if they are transliterations of each other.
</nextsent>
<nextsent>transliteration of name, for the purpose of this work, is defined as its transcription in different language, preserving the phone tics, perhaps in different orthography [knight and graehl, 1997] <papid> P97-1017 </papid>1 . while traditional transliteration systems have relied on hand-crafted linguistic rules, more recently, statistical machine learning techniques have been shown to be effective in transliteration tasks [jung et al, 2000] [<papid> C00-1056 </papid>abduljaleel and larkey, 2003] [virga and kudhan pur , 2003] [haizhou et al, 2004].</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M142">
<title id=" I08-6004.xml">some experiments in mining named entity transliteration pairs from comparable corpora </title>
<section> motivation </section>
<citcontext>
<prevsection>
<prevsent>parallel named entity (ne) pairs are important resources in several nlp tasks, from supporting cross-lingual information retrieval (clir) systems, to improving machine translation (mt) systems.
</prevsent>
<prevsent>in addition, such pairs may also be used for developing transliteration systems, if they are transliterations of each other.
</prevsent>
</prevsection>
<citsent citstr=" P97-1017 ">
transliteration of name, for the purpose of this work, is defined as its transcription in different language, preserving the phone tics, perhaps in different orthography [knight and graehl, 1997] <papid> P97-1017 </papid>1 . while traditional transliteration systems have relied on hand-crafted linguistic rules, more recently, statistical machine learning techniques have been shown to be effective in transliteration tasks [jung et al, 2000] [<papid> C00-1056 </papid>abduljaleel and larkey, 2003] [virga and kudhan pur , 2003] [haizhou et al, 2004].</citsent>
<aftsection>
<nextsent>however, such data-driven approaches require significant amounts of training data, namely pairs of names in two different languages, possibly in different orthography, referred to as transliteration pairs, which are not readily available in many resource-poor languages.
</nextsent>
<nextsent>it is important to note at this point, that nes are found typically in news corpora in any given language.
</nextsent>
<nextsent>in addition, news articles covering the same event in two different languages may reasonably be expected to contain the same nes in the respective languages.
</nextsent>
<nextsent>the perpetual availability of news corpora in the worlds languages, points to the promise of mining transliteration pairs endlessly, provided an effective identification of such nes in specific languages and pairing them appropriately, could be devised.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M143">
<title id=" I08-6004.xml">some experiments in mining named entity transliteration pairs from comparable corpora </title>
<section> motivation </section>
<citcontext>
<prevsection>
<prevsent>parallel named entity (ne) pairs are important resources in several nlp tasks, from supporting cross-lingual information retrieval (clir) systems, to improving machine translation (mt) systems.
</prevsent>
<prevsent>in addition, such pairs may also be used for developing transliteration systems, if they are transliterations of each other.
</prevsent>
</prevsection>
<citsent citstr=" C00-1056 ">
transliteration of name, for the purpose of this work, is defined as its transcription in different language, preserving the phone tics, perhaps in different orthography [knight and graehl, 1997] <papid> P97-1017 </papid>1 . while traditional transliteration systems have relied on hand-crafted linguistic rules, more recently, statistical machine learning techniques have been shown to be effective in transliteration tasks [jung et al, 2000] [<papid> C00-1056 </papid>abduljaleel and larkey, 2003] [virga and kudhan pur , 2003] [haizhou et al, 2004].</citsent>
<aftsection>
<nextsent>however, such data-driven approaches require significant amounts of training data, namely pairs of names in two different languages, possibly in different orthography, referred to as transliteration pairs, which are not readily available in many resource-poor languages.
</nextsent>
<nextsent>it is important to note at this point, that nes are found typically in news corpora in any given language.
</nextsent>
<nextsent>in addition, news articles covering the same event in two different languages may reasonably be expected to contain the same nes in the respective languages.
</nextsent>
<nextsent>the perpetual availability of news corpora in the worlds languages, points to the promise of mining transliteration pairs endlessly, provided an effective identification of such nes in specific languages and pairing them appropriately, could be devised.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M165">
<title id=" I08-6004.xml">some experiments in mining named entity transliteration pairs from comparable corpora </title>
<section> experimental setup &amp; results.  </section>
<citcontext>
<prevsection>
<prevsent>this is partially due to the fact tamil is highly agglutinative language, where various affixes (prefixes and suffixes of other content words) stand for function words and prepositions in english, thus do not contribute to the word count.
</prevsent>
<prevsent>further, since our focus is on mining names, we expect the same nes to be covered in both the corpora, and hence we do not expect severe impact on mining.
</prevsent>
</prevsection>
<citsent citstr=" P05-1045 ">
corpus time period size articles words new indian express (english) 2007.01.01 to 2007.08.31 2,359 347,050 dina mani (tamil) 2007.01.01 to 2007.08.31 2,359 256,456 table 1: statistics on comparable corpora from the above corpora, we first extracted all the nes from the english side, using the stanford ner tool [finkel et al 2005].<papid> P05-1045 </papid></citsent>
<aftsection>
<nextsent>no multiword expressions were considered for this experiment.
</nextsent>
<nextsent>also, only those nes that have frequency count of more than threshold value of fe were considered, in order to avoid unusual names that are hard to identify in the comparable corpora.
</nextsent>
<nextsent>thus, we extracted from the above corpora, only subset of nes found in the english side to be matched with their potential transliteration pairs; for example, for parameter setting of fe to 10, we extract only 274 legitimate nes.
</nextsent>
<nextsent>from the tamil side of the corpora, we extracted all words, and grouped them in to equivalence classes, by considering prefix of 5 characters.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M177">
<title id=" I08-4033.xml">achilles nictatr chinese morphological analyzer for the fourth sighan bakeoff </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our post-evaluation results prove the effectiveness of this approach for pos tagging.
</prevsent>
<prevsent>many approaches have been proposed in chinese word segmentation in the past decades.
</prevsent>
</prevsection>
<citsent citstr=" W03-1730 ">
segmentation performance has been improved significantly, from the earliest maximal match (dictionary-based)approaches to hmm-based (zhang et al, 2003) <papid> W03-1730 </papid>approaches and recent state-of-the-art machine learning approaches such as maximum entropy (maxent) (xue and shen, 2003), <papid> W03-1728 </papid>support vector machine (svm) (kudo and matsumoto, 2001), <papid> N01-1025 </papid>conditional random fields (crf) (peng and mccallum, 2004), and minimum error rate training (gao et al, 2004).<papid> P04-1059 </papid></citsent>
<aftsection>
<nextsent>after analyzing the results presented in the first and second bake offs, (sproat and emerson,2003) <papid> W03-1719 </papid>and (emerson, 2005), <papid> I05-3017 </papid>we created new chinese word segmentation system named as achilles that consists of four modules mainly: regular expression extractor, dictionary-based ngram segmentation, crf-based subword tagging (zhang et al, 2006), and confidence-based segmentation.</nextsent>
<nextsent>of the four modules, the subword-based tagging, differing from the existing character-based tagging, was proposed in our work recently.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M178">
<title id=" I08-4033.xml">achilles nictatr chinese morphological analyzer for the fourth sighan bakeoff </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our post-evaluation results prove the effectiveness of this approach for pos tagging.
</prevsent>
<prevsent>many approaches have been proposed in chinese word segmentation in the past decades.
</prevsent>
</prevsection>
<citsent citstr=" W03-1728 ">
segmentation performance has been improved significantly, from the earliest maximal match (dictionary-based)approaches to hmm-based (zhang et al, 2003) <papid> W03-1730 </papid>approaches and recent state-of-the-art machine learning approaches such as maximum entropy (maxent) (xue and shen, 2003), <papid> W03-1728 </papid>support vector machine (svm) (kudo and matsumoto, 2001), <papid> N01-1025 </papid>conditional random fields (crf) (peng and mccallum, 2004), and minimum error rate training (gao et al, 2004).<papid> P04-1059 </papid></citsent>
<aftsection>
<nextsent>after analyzing the results presented in the first and second bake offs, (sproat and emerson,2003) <papid> W03-1719 </papid>and (emerson, 2005), <papid> I05-3017 </papid>we created new chinese word segmentation system named as achilles that consists of four modules mainly: regular expression extractor, dictionary-based ngram segmentation, crf-based subword tagging (zhang et al, 2006), and confidence-based segmentation.</nextsent>
<nextsent>of the four modules, the subword-based tagging, differing from the existing character-based tagging, was proposed in our work recently.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M179">
<title id=" I08-4033.xml">achilles nictatr chinese morphological analyzer for the fourth sighan bakeoff </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our post-evaluation results prove the effectiveness of this approach for pos tagging.
</prevsent>
<prevsent>many approaches have been proposed in chinese word segmentation in the past decades.
</prevsent>
</prevsection>
<citsent citstr=" N01-1025 ">
segmentation performance has been improved significantly, from the earliest maximal match (dictionary-based)approaches to hmm-based (zhang et al, 2003) <papid> W03-1730 </papid>approaches and recent state-of-the-art machine learning approaches such as maximum entropy (maxent) (xue and shen, 2003), <papid> W03-1728 </papid>support vector machine (svm) (kudo and matsumoto, 2001), <papid> N01-1025 </papid>conditional random fields (crf) (peng and mccallum, 2004), and minimum error rate training (gao et al, 2004).<papid> P04-1059 </papid></citsent>
<aftsection>
<nextsent>after analyzing the results presented in the first and second bake offs, (sproat and emerson,2003) <papid> W03-1719 </papid>and (emerson, 2005), <papid> I05-3017 </papid>we created new chinese word segmentation system named as achilles that consists of four modules mainly: regular expression extractor, dictionary-based ngram segmentation, crf-based subword tagging (zhang et al, 2006), and confidence-based segmentation.</nextsent>
<nextsent>of the four modules, the subword-based tagging, differing from the existing character-based tagging, was proposed in our work recently.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M180">
<title id=" I08-4033.xml">achilles nictatr chinese morphological analyzer for the fourth sighan bakeoff </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our post-evaluation results prove the effectiveness of this approach for pos tagging.
</prevsent>
<prevsent>many approaches have been proposed in chinese word segmentation in the past decades.
</prevsent>
</prevsection>
<citsent citstr=" P04-1059 ">
segmentation performance has been improved significantly, from the earliest maximal match (dictionary-based)approaches to hmm-based (zhang et al, 2003) <papid> W03-1730 </papid>approaches and recent state-of-the-art machine learning approaches such as maximum entropy (maxent) (xue and shen, 2003), <papid> W03-1728 </papid>support vector machine (svm) (kudo and matsumoto, 2001), <papid> N01-1025 </papid>conditional random fields (crf) (peng and mccallum, 2004), and minimum error rate training (gao et al, 2004).<papid> P04-1059 </papid></citsent>
<aftsection>
<nextsent>after analyzing the results presented in the first and second bake offs, (sproat and emerson,2003) <papid> W03-1719 </papid>and (emerson, 2005), <papid> I05-3017 </papid>we created new chinese word segmentation system named as achilles that consists of four modules mainly: regular expression extractor, dictionary-based ngram segmentation, crf-based subword tagging (zhang et al, 2006), and confidence-based segmentation.</nextsent>
<nextsent>of the four modules, the subword-based tagging, differing from the existing character-based tagging, was proposed in our work recently.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M181">
<title id=" I08-4033.xml">achilles nictatr chinese morphological analyzer for the fourth sighan bakeoff </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>many approaches have been proposed in chinese word segmentation in the past decades.
</prevsent>
<prevsent>segmentation performance has been improved significantly, from the earliest maximal match (dictionary-based)approaches to hmm-based (zhang et al, 2003) <papid> W03-1730 </papid>approaches and recent state-of-the-art machine learning approaches such as maximum entropy (maxent) (xue and shen, 2003), <papid> W03-1728 </papid>support vector machine (svm) (kudo and matsumoto, 2001), <papid> N01-1025 </papid>conditional random fields (crf) (peng and mccallum, 2004), and minimum error rate training (gao et al, 2004).<papid> P04-1059 </papid></prevsent>
</prevsection>
<citsent citstr=" W03-1719 ">
after analyzing the results presented in the first and second bake offs, (sproat and emerson,2003) <papid> W03-1719 </papid>and (emerson, 2005), <papid> I05-3017 </papid>we created new chinese word segmentation system named as achilles that consists of four modules mainly: regular expression extractor, dictionary-based ngram segmentation, crf-based subword tagging (zhang et al, 2006), and confidence-based segmentation.</citsent>
<aftsection>
<nextsent>of the four modules, the subword-based tagging, differing from the existing character-based tagging, was proposed in our work recently.
</nextsent>
<nextsent>we will give detail description to this approach in the following sections.in the followings, we illustrate our word segmentation process in section 2, where the subword based tagging is implemented by the crfs method.section 3 illustrates our feature-based part-ofspeech tagging approach.
</nextsent>
<nextsent>section 4 presents our experimental results.
</nextsent>
<nextsent>section 5 describes current state of-the-art methods for chinese word segmentation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M182">
<title id=" I08-4033.xml">achilles nictatr chinese morphological analyzer for the fourth sighan bakeoff </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>many approaches have been proposed in chinese word segmentation in the past decades.
</prevsent>
<prevsent>segmentation performance has been improved significantly, from the earliest maximal match (dictionary-based)approaches to hmm-based (zhang et al, 2003) <papid> W03-1730 </papid>approaches and recent state-of-the-art machine learning approaches such as maximum entropy (maxent) (xue and shen, 2003), <papid> W03-1728 </papid>support vector machine (svm) (kudo and matsumoto, 2001), <papid> N01-1025 </papid>conditional random fields (crf) (peng and mccallum, 2004), and minimum error rate training (gao et al, 2004).<papid> P04-1059 </papid></prevsent>
</prevsection>
<citsent citstr=" I05-3017 ">
after analyzing the results presented in the first and second bake offs, (sproat and emerson,2003) <papid> W03-1719 </papid>and (emerson, 2005), <papid> I05-3017 </papid>we created new chinese word segmentation system named as achilles that consists of four modules mainly: regular expression extractor, dictionary-based ngram segmentation, crf-based subword tagging (zhang et al, 2006), and confidence-based segmentation.</citsent>
<aftsection>
<nextsent>of the four modules, the subword-based tagging, differing from the existing character-based tagging, was proposed in our work recently.
</nextsent>
<nextsent>we will give detail description to this approach in the following sections.in the followings, we illustrate our word segmentation process in section 2, where the subword based tagging is implemented by the crfs method.section 3 illustrates our feature-based part-ofspeech tagging approach.
</nextsent>
<nextsent>section 4 presents our experimental results.
</nextsent>
<nextsent>section 5 describes current state of-the-art methods for chinese word segmentation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M184">
<title id=" I08-4033.xml">achilles nictatr chinese morphological analyzer for the fourth sighan bakeoff </title>
<section> introduction of main modules in.  </section>
<citcontext>
<prevsection>
<prevsent>if dictionary-based module recognizes ivs successfully, the subword-based iob tagging can recognize oovs.
</prevsent>
<prevsent>before the subword-based tagging, the character-based iob?
</prevsent>
</prevsection>
<citsent citstr=" I05-3027 ">
tagging approach has been widely used in chinese word segmentation recently (xue and shen, 2003; <papid> W03-1728 </papid>peng and mccallum, 2004; tseng et al, 2005).<papid> I05-3027 </papid></citsent>
<aftsection>
<nextsent>under the scheme, each character of word is labeled as b? if it is the first character of multiple-character word, or o? if the character functions as an independent word, or i? otherwise.?
</nextsent>
<nextsent>for example, ??(whole) ???(beijing city)?
</nextsent>
<nextsent>is labeled as ??(whole)/o ?(north)/b?(capital)/i?(city)/i?.
</nextsent>
<nextsent>we proposed the subword-based tagging (zhang et al, 2006) to improve the existing character-based tagging.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M185">
<title id=" I08-4033.xml">achilles nictatr chinese morphological analyzer for the fourth sighan bakeoff </title>
<section> part-of-speech tagging.  </section>
<citcontext>
<prevsection>
<prevsent>prior probability p(t ) is n gram language model of tag sequence.
</prevsent>
<prevsent>p(w |t ) is thought as an unigram model.
</prevsent>
</prevsection>
<citsent citstr=" J94-2001 ">
in this experiment we used trigram to model p(t ).differing from the interpolation smoothing algorithm used in(merialdo, 1994), <papid> J94-2001 </papid>both p(t ) and p(w |t ) were smoothed by back-off methods(katz, 1987).</citsent>
<aftsection>
<nextsent>because n-gram backoff model p(t ) is well-known, backoff implementation of p(w |t ) was given here only.
</nextsent>
<nextsent>it is of the following equation.
</nextsent>
<nextsent>r f r-oov r-iv ckip 0.938 0.931 0.935 0.640 0.966 cityu 0.943 0.933 0.938 0.686 0.965 ctb 0.941 0.943 0.942 0.663 0.961 ncc 0.931 0.933 0.932 0.592 0.950 sxu 0.932 0.929 0.930 0.487 0.971 table 1: post evaluation of word segmentation.
</nextsent>
<nextsent>p(w|t) = { p?(w|t) if p?(w|t) , 0 ?(t) p?(w) otherwise (6) where:- p?(w|t) and p?(w) are discounting relative frequencies of p(w|t) and p(w), calculated by back-off discounting algorithm.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M186">
<title id=" I08-4029.xml">description of the ncu chinese word segmentation and partofspeech tagging for sighan bakeoff 2007 </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>to support the above targets, it is necessary to detect the boundaries between words in given sentence.
</prevsent>
<prevsent>in tradition, the chinese word segmentation technologies can be categorized into three types, (heuristic) rule-based, machine learning, and hybrid.
</prevsent>
</prevsection>
<citsent citstr=" C04-1081 ">
among them, the machine learning-based techniques showed excellent performance in many recent research studies (peng et al, 2004; <papid> C04-1081 </papid>zhou et al., 2005; gao et al, 2004).<papid> P04-1059 </papid></citsent>
<aftsection>
<nextsent>this method treats the word segmentation problem as sequence of word classification.
</nextsent>
<nextsent>the classifier online assigns either boundary?
</nextsent>
<nextsent>or non-boundary?
</nextsent>
<nextsent>label to each word by learning from the large annotated corpora.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M187">
<title id=" I08-4029.xml">description of the ncu chinese word segmentation and partofspeech tagging for sighan bakeoff 2007 </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>to support the above targets, it is necessary to detect the boundaries between words in given sentence.
</prevsent>
<prevsent>in tradition, the chinese word segmentation technologies can be categorized into three types, (heuristic) rule-based, machine learning, and hybrid.
</prevsent>
</prevsection>
<citsent citstr=" P04-1059 ">
among them, the machine learning-based techniques showed excellent performance in many recent research studies (peng et al, 2004; <papid> C04-1081 </papid>zhou et al., 2005; gao et al, 2004).<papid> P04-1059 </papid></citsent>
<aftsection>
<nextsent>this method treats the word segmentation problem as sequence of word classification.
</nextsent>
<nextsent>the classifier online assigns either boundary?
</nextsent>
<nextsent>or non-boundary?
</nextsent>
<nextsent>label to each word by learning from the large annotated corpora.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M188">
<title id=" I08-4029.xml">description of the ncu chinese word segmentation and partofspeech tagging for sighan bakeoff 2007 </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>or non-boundary?
</prevsent>
<prevsent>label to each word by learning from the large annotated corpora.
</prevsent>
</prevsection>
<citsent citstr=" W03-0407 ">
machine learning-based word segmentation method is quite similar to the word sequence inference techniques, such as part-of-speech (pos) tagging (clark et al, 2003; <papid> W03-0407 </papid>gimenez and marquez, 2003), phrase chunking (lee and wu, 2007) and word dependency parsing (wu et al, 2006, <papid> W06-2937 </papid>2007).</citsent>
<aftsection>
<nextsent>in this paper, we present two prototype systems for chinese word segmentation and pos tagging 161 sixth sighan workshop on chinese language processing tasks.
</nextsent>
<nextsent>the former was basically an extension of previous literatures (ng and low, 2004; <papid> W04-3236 </papid>zhou et al, 2006), while the latter incorporates the unknown word and known word tagging into one step.</nextsent>
<nextsent>the two frameworks were designed based on two variant machine learning algorithms, namely crf and svm.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M189">
<title id=" I08-4029.xml">description of the ncu chinese word segmentation and partofspeech tagging for sighan bakeoff 2007 </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>or non-boundary?
</prevsent>
<prevsent>label to each word by learning from the large annotated corpora.
</prevsent>
</prevsection>
<citsent citstr=" W06-2937 ">
machine learning-based word segmentation method is quite similar to the word sequence inference techniques, such as part-of-speech (pos) tagging (clark et al, 2003; <papid> W03-0407 </papid>gimenez and marquez, 2003), phrase chunking (lee and wu, 2007) and word dependency parsing (wu et al, 2006, <papid> W06-2937 </papid>2007).</citsent>
<aftsection>
<nextsent>in this paper, we present two prototype systems for chinese word segmentation and pos tagging 161 sixth sighan workshop on chinese language processing tasks.
</nextsent>
<nextsent>the former was basically an extension of previous literatures (ng and low, 2004; <papid> W04-3236 </papid>zhou et al, 2006), while the latter incorporates the unknown word and known word tagging into one step.</nextsent>
<nextsent>the two frameworks were designed based on two variant machine learning algorithms, namely crf and svm.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M190">
<title id=" I08-4029.xml">description of the ncu chinese word segmentation and partofspeech tagging for sighan bakeoff 2007 </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>machine learning-based word segmentation method is quite similar to the word sequence inference techniques, such as part-of-speech (pos) tagging (clark et al, 2003; <papid> W03-0407 </papid>gimenez and marquez, 2003), phrase chunking (lee and wu, 2007) and word dependency parsing (wu et al, 2006, <papid> W06-2937 </papid>2007).</prevsent>
<prevsent>in this paper, we present two prototype systems for chinese word segmentation and pos tagging 161 sixth sighan workshop on chinese language processing tasks.</prevsent>
</prevsection>
<citsent citstr=" W04-3236 ">
the former was basically an extension of previous literatures (ng and low, 2004; <papid> W04-3236 </papid>zhou et al, 2006), while the latter incorporates the unknown word and known word tagging into one step.</citsent>
<aftsection>
<nextsent>the two frameworks were designed based on two variant machine learning algorithms, namely crf and svm.
</nextsent>
<nextsent>in our pilot study, the svm showed better performance than crf in the pos tagging task.
</nextsent>
<nextsent>to identify unknown words, we also encode the suffix and prefix features to represent the training example.
</nextsent>
<nextsent>the strategy was showed very effective for improving both known and unknown word chunking on both chinese and english phrase chunking (lee and wu, 2007).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M191">
<title id=" I08-4029.xml">description of the ncu chinese word segmentation and partofspeech tagging for sighan bakeoff 2007 </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>in section 3.2, the proposed pos tagging framework is then presented.
</prevsent>
<prevsent>3.1 word sequence classification.
</prevsent>
</prevsection>
<citsent citstr=" W95-0107 ">
similar to english text chunking (ramshaw and marcus, 1995; <papid> W95-0107 </papid>lee and wu, 2007), the word sequence classification model aims to classify each word via encoding its context features.</citsent>
<aftsection>
<nextsent>by encoding with bies (lmr tagging scheme) or iob2 style, both ws and ner problems can be viewed as sequence of word classification.
</nextsent>
<nextsent>during testing, we seek to find the optimal word type for each chinese character.
</nextsent>
<nextsent>these types strongly reflect the actual word boundaries for chinese words or named entity phrases.
</nextsent>
<nextsent>as reported by (zhou et al, 2006), the use of richer tag set can effectively enhance the performance.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M194">
<title id=" I08-4029.xml">description of the ncu chinese word segmentation and partofspeech tagging for sighan bakeoff 2007 </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>in this paper, we did not conduct the feature selection experiment for each tagging corpus, instead unified feature set was used due to the time line.
</prevsent>
<prevsent>we trust our pos tagger could be further improved by removing or adding new feature set.
</prevsent>
</prevsection>
<citsent citstr=" P07-2017 ">
the learner used in this paper (svm) is mainly developed by our own (wu et al, 2007).<papid> P07-2017 </papid></citsent>
<aftsection>
<nextsent>the cost factor is simply set as 0.15 for all languages.
</nextsent>
<nextsent>furthermore, to remove rare words, we eliminate the words which appear no more than twice in the training data.
</nextsent>
<nextsent>4.1 dataset and evaluations.
</nextsent>
<nextsent>in this year, we mainly focus on the close track for ws and pos tagging tracks.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M196">
<title id=" I08-4025.xml">training a perceptron with global and local features for chinese word segmentation </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>the architecture of our system is shown in figure 1.
</prevsent>
<prevsent>for each of the training corpora in the bakeoff, we produce 10-fold split: in each fold, 90% of the corpus is used for training and 10% is used to produce an n-best list of candidates.
</prevsent>
</prevsection>
<citsent citstr=" W04-3230 ">
the n-best list is produced using character-based conditional random field (crf) (lafferty et al, 2001; kudo et al, 2004) <papid> W04-3230 </papid>tagger.</citsent>
<aftsection>
<nextsent>the true segmentation can now be compared with the n-best list in order to train an averaged perceptron algorithm (collins, 2002<papid> W02-1001 </papid>a).</nextsent>
<nextsent>this system is then used to predict the best word segmentation from an n-best list for each sentence in the test data.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M197">
<title id=" I08-4025.xml">training a perceptron with global and local features for chinese word segmentation </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>for each of the training corpora in the bakeoff, we produce 10-fold split: in each fold, 90% of the corpus is used for training and 10% is used to produce an n-best list of candidates.
</prevsent>
<prevsent>the n-best list is produced using character-based conditional random field (crf) (lafferty et al, 2001; kudo et al, 2004) <papid> W04-3230 </papid>tagger.</prevsent>
</prevsection>
<citsent citstr=" W02-1001 ">
the true segmentation can now be compared with the n-best list in order to train an averaged perceptron algorithm (collins, 2002<papid> W02-1001 </papid>a).</citsent>
<aftsection>
<nextsent>this system is then used to predict the best word segmentation from an n-best list for each sentence in the test data.
</nextsent>
<nextsent>training corpus weight vector nbest candidates training with decoding with conditional random nbest candidates field local features global features average perceptron input sentence average perceptron output conditional random field (10fold split) figure 1: outline of the segmentation process 2.1 learning algorithm.
</nextsent>
<nextsent>given an unsegmented sentence x, the word segmentation problem can be defined as finding the 143 sixth sighan workshop on chinese language processing most probable segmentation (x) from set of possible segment ations of x. (x) = argmax ygen(x) ?(x, y) ? (1) the set of possible segment ations is given by gen(x) and the nave method is to first generate all possible segmented candidates.
</nextsent>
<nextsent>for long sentence, generating those candidates and picking the one with the highest score is time consuming.in our approach, n-best candidates for each training example are produced with the crf++ software (kudo et al, 2004).<papid> W04-3230 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M216">
<title id=" I08-4025.xml">training a perceptron with global and local features for chinese word segmentation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>all our results are significantly better.
</prevsent>
<prevsent>re-ranking over n-best lists has been applied to somany tasks in natural language that it is not possible to list them all here.
</prevsent>
</prevsection>
<citsent citstr=" D07-1033 ">
closest to our approach is the work in (kazama and torisawa, 2007).<papid> D07-1033 </papid></citsent>
<aftsection>
<nextsent>they proposed margin perceptron approach for named entity recognition with non-local features on an nbest list.
</nextsent>
<nextsent>in contrast to their approach, in our system, global features examine the entire sentence instead of partial phrases.
</nextsent>
<nextsent>for word segmentation, (wang and shi, 2006) implemented re-rankingmethod with pos tagging features.
</nextsent>
<nextsent>in their approach, character-based crf model produces the best list for each test sentence.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M217">
<title id=" I08-3020.xml">speech to speech machine translation biblical chatter from finnish to english </title>
<section> knowledge-based approaches.  </section>
<citcontext>
<prevsection>
<prevsent>this is similar to sentence parsing techniques that can be used to induce context-free grammar for language (charniak, 1997), and couldin fact be considered one of their more useful applications.
</prevsent>
<prevsent>the parsing generally depends on probabilistic model trained on sentences aligned with their syntactic and semantic representations, often in atree that could be generated by context-free grammar. the resulting semantic representation can then be used as the source of target-language generation process.the algorithm that generates such representa 124tion from raw input could be trained on tree bank, and an annotated form of the same corpus (where the derivations in the generation space are associated with counts for each decision made) can be used to train the output component to generatelanguage.
</prevsent>
</prevsection>
<citsent citstr=" W05-1601 ">
(belz, 2005) <papid> W05-1601 </papid>to incorporate the statistical component, which allows for robust generalization, per (knight and hatzivassiloglou, 1995), <papid> P95-1034 </papid>the nlg on the target side is filtered through language model (described above).</citsent>
<aftsection>
<nextsent>this helps address manyof the knowledge gap problems introduced by linguistic differences or in component of the system - the analyzer or generator.
</nextsent>
<nextsent>this approach does have significant advantages, particularly in that it is more focused on semantics (as opposed to statistical cooccurrence), so it may be less likely to distort meaning.
</nextsent>
<nextsent>on the other hand, it could misinterpret or mis communicate (or both),just like human translator.
</nextsent>
<nextsent>perhaps the crucial difference is that, while machine learning often has little to do with our understanding of cognitive processes, this sort of machine translation has greater potential for illuminating mysterious areas of the human process.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M218">
<title id=" I08-3020.xml">speech to speech machine translation biblical chatter from finnish to english </title>
<section> knowledge-based approaches.  </section>
<citcontext>
<prevsection>
<prevsent>this is similar to sentence parsing techniques that can be used to induce context-free grammar for language (charniak, 1997), and couldin fact be considered one of their more useful applications.
</prevsent>
<prevsent>the parsing generally depends on probabilistic model trained on sentences aligned with their syntactic and semantic representations, often in atree that could be generated by context-free grammar. the resulting semantic representation can then be used as the source of target-language generation process.the algorithm that generates such representa 124tion from raw input could be trained on tree bank, and an annotated form of the same corpus (where the derivations in the generation space are associated with counts for each decision made) can be used to train the output component to generatelanguage.
</prevsent>
</prevsection>
<citsent citstr=" P95-1034 ">
(belz, 2005) <papid> W05-1601 </papid>to incorporate the statistical component, which allows for robust generalization, per (knight and hatzivassiloglou, 1995), <papid> P95-1034 </papid>the nlg on the target side is filtered through language model (described above).</citsent>
<aftsection>
<nextsent>this helps address manyof the knowledge gap problems introduced by linguistic differences or in component of the system - the analyzer or generator.
</nextsent>
<nextsent>this approach does have significant advantages, particularly in that it is more focused on semantics (as opposed to statistical cooccurrence), so it may be less likely to distort meaning.
</nextsent>
<nextsent>on the other hand, it could misinterpret or mis communicate (or both),just like human translator.
</nextsent>
<nextsent>perhaps the crucial difference is that, while machine learning often has little to do with our understanding of cognitive processes, this sort of machine translation has greater potential for illuminating mysterious areas of the human process.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M219">
<title id=" I08-3020.xml">speech to speech machine translation biblical chatter from finnish to english </title>
<section> spoken features.  </section>
<citcontext>
<prevsection>
<prevsent>we have written up bare bones ibm model 1 in both c++ and python, using the standard em approach and gibbs samplingone.
</prevsent>
<prevsent>the latter allows for optimizations using linear algebra, and although it does not quite match the 125 perplexity or log-likelihood achieved by em, it is significantly faster, particularly on longer sentences.
</prevsent>
</prevsection>
<citsent citstr=" N07-1048 ">
since morpheme segmentation is at least somewhat helpful in speech recognition (creutz, 2006; creutz et al, 2007), <papid> N07-1048 </papid>it should still be considered potential component in speech-to-speech translation.</citsent>
<aftsection>
<nextsent>in terms of incorporating the knowledge-based approach into such system, we think it may yet be too early,but if existing understanding-and-generation frameworks for machine translation could be adapted to this use, it could be very fruitful, in particular since spoken language generation might be more effective from knowledge base, since it would know what it was trying to say, instead of relying on statistics alone, hoping the phonemes end up in meaningful order.the critical step of sst is, of course, translation.
</nextsent>
<nextsent>in an integrated system, as described above, the translation model could be trained on parallel spoken corpus (perhaps tokenized into phonemes, or segmented into morphemes), since there might be advantages to limiting the intermediate steps in theprocess.
</nextsent>
<nextsent>the bible is massively multilingual publication, and as it happens, its text is available aligned between finnish and english, and it is possible to find corresponding recordings in both languages.
</nextsent>
<nextsent>so, this corpus would enable direct approach to speech-to-speech translation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M220">
<title id=" I08-3020.xml">speech to speech machine translation biblical chatter from finnish to english </title>
<section> biblical chatter.  </section>
<citcontext>
<prevsection>
<prevsent>here, we use an unlimited vocabu lary?
</prevsent>
<prevsent>continuous speech recognizer (hirsimaki et al, 2006), trained on multi-speaker finnish acoustic model with varig ram (siivola et al, 2007) language model that includes bible n-grams.
</prevsent>
</prevsection>
<citsent citstr=" P07-2045 ">
then, for translation, moses (koehn et al, 2007) <papid> P07-2045 </papid>is trained on words and morphemes (derived from morfessorbaseline (creutz and lagus, 2005)).</citsent>
<aftsection>
<nextsent>for speech synthesis, we used festival (taylor, 1999), including the built-in english voice and finnish voice developed at helsinki university.
</nextsent>
<nextsent>126 5.3 results.
</nextsent>
<nextsent>the following is an example fragment, taken from the test corpus.
</nextsent>
<nextsent>niin daavid meni david slept with his lepoon isiensa?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M221">
<title id=" I08-6002.xml">identifying similar and co referring documents across languages </title>
<section> motivation.  </section>
<citcontext>
<prevsection>
<prevsent>chung and allan (2004) have worked on cross-document co-referencing using large scale corpus, where they have said ambiguous names from the same domain (here for example, politics) are harder to disambiguate when compared to names from different domains.
</prevsent>
<prevsent>in their work chung and allan compare the effectiveness of different statistical methods in cross-document coreference resolution task.
</prevsent>
</prevsection>
<citsent citstr=" A00-1020 ">
harabagiu and maiorano (2000) <papid> A00-1020 </papid>have worked on multilingual co-reference resolution on english and romanian language texts.</citsent>
<aftsection>
<nextsent>in their system, swizzle?
</nextsent>
<nextsent>they use data driven methodology which uses aligned bilingual corpora, linguistic rules and heuristics of english and romanian documents to find co-references.
</nextsent>
<nextsent>in the indian context, obtaining aligned bilingual corpora is difficult.
</nextsent>
<nextsent>document similarity between indian languages and english is tough since the sentence structure differs and indian languages are agglutinative in nature.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M222">
<title id=" I08-4007.xml">stochastic dependency parsing based on a admissible search </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>according to the binary head-modifier links, it is pretty easy to transform the dependency tree into the predicate send(to:boss,object:report(typed(yesterday,well))) for further interpretation or interaction, since the semantic gap between them is slight and the mappings are thus quite direct.
</prevsent>
<prevsent>furthermore, robust partial understanding can be achieved easily when full parse is not obtainable, and measured precisely by simply counting the correct attachments on the dependency tree (ohno et al, 2004).
</prevsent>
</prevsection>
<citsent citstr=" C96-1058 ">
all these willnot be so simple provided that conventional constituency grammar is used.in the dependency parsing paradigm, several deterministic, stochastic or machine-learning-based parsing algorithms have been proposed (eisner, 1996; <papid> C96-1058 </papid>covington 2001; kudo and matsumoto, 2002; <papid> W02-2016 </papid>yamada and matsumoto, 2003; nivre 2003; nivre and scholz, 2004; <papid> C04-1010 </papid>chen et al, 2005).</citsent>
<aftsection>
<nextsent>manyof them make hard decisions with local information while selecting among next parse states.
</nextsent>
<nextsent>as consequence, though the obtained dependency trees are good in some sense, the n-best output is not guaranteed to be globally optimal in general (klein and manning, 2003).<papid> N03-1016 </papid>on the other hand, a* search that guarantees optimality has been applied to many areas including ai.</nextsent>
<nextsent>klein and manning (2003) <papid> N03-1016 </papid>proposed to use a* search in pcfg parsing.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M223">
<title id=" I08-4007.xml">stochastic dependency parsing based on a admissible search </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>according to the binary head-modifier links, it is pretty easy to transform the dependency tree into the predicate send(to:boss,object:report(typed(yesterday,well))) for further interpretation or interaction, since the semantic gap between them is slight and the mappings are thus quite direct.
</prevsent>
<prevsent>furthermore, robust partial understanding can be achieved easily when full parse is not obtainable, and measured precisely by simply counting the correct attachments on the dependency tree (ohno et al, 2004).
</prevsent>
</prevsection>
<citsent citstr=" W02-2016 ">
all these willnot be so simple provided that conventional constituency grammar is used.in the dependency parsing paradigm, several deterministic, stochastic or machine-learning-based parsing algorithms have been proposed (eisner, 1996; <papid> C96-1058 </papid>covington 2001; kudo and matsumoto, 2002; <papid> W02-2016 </papid>yamada and matsumoto, 2003; nivre 2003; nivre and scholz, 2004; <papid> C04-1010 </papid>chen et al, 2005).</citsent>
<aftsection>
<nextsent>manyof them make hard decisions with local information while selecting among next parse states.
</nextsent>
<nextsent>as consequence, though the obtained dependency trees are good in some sense, the n-best output is not guaranteed to be globally optimal in general (klein and manning, 2003).<papid> N03-1016 </papid>on the other hand, a* search that guarantees optimality has been applied to many areas including ai.</nextsent>
<nextsent>klein and manning (2003) <papid> N03-1016 </papid>proposed to use a* search in pcfg parsing.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M224">
<title id=" I08-4007.xml">stochastic dependency parsing based on a admissible search </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>according to the binary head-modifier links, it is pretty easy to transform the dependency tree into the predicate send(to:boss,object:report(typed(yesterday,well))) for further interpretation or interaction, since the semantic gap between them is slight and the mappings are thus quite direct.
</prevsent>
<prevsent>furthermore, robust partial understanding can be achieved easily when full parse is not obtainable, and measured precisely by simply counting the correct attachments on the dependency tree (ohno et al, 2004).
</prevsent>
</prevsection>
<citsent citstr=" C04-1010 ">
all these willnot be so simple provided that conventional constituency grammar is used.in the dependency parsing paradigm, several deterministic, stochastic or machine-learning-based parsing algorithms have been proposed (eisner, 1996; <papid> C96-1058 </papid>covington 2001; kudo and matsumoto, 2002; <papid> W02-2016 </papid>yamada and matsumoto, 2003; nivre 2003; nivre and scholz, 2004; <papid> C04-1010 </papid>chen et al, 2005).</citsent>
<aftsection>
<nextsent>manyof them make hard decisions with local information while selecting among next parse states.
</nextsent>
<nextsent>as consequence, though the obtained dependency trees are good in some sense, the n-best output is not guaranteed to be globally optimal in general (klein and manning, 2003).<papid> N03-1016 </papid>on the other hand, a* search that guarantees optimality has been applied to many areas including ai.</nextsent>
<nextsent>klein and manning (2003) <papid> N03-1016 </papid>proposed to use a* search in pcfg parsing.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M225">
<title id=" I08-4007.xml">stochastic dependency parsing based on a admissible search </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>all these willnot be so simple provided that conventional constituency grammar is used.in the dependency parsing paradigm, several deterministic, stochastic or machine-learning-based parsing algorithms have been proposed (eisner, 1996; <papid> C96-1058 </papid>covington 2001; kudo and matsumoto, 2002; <papid> W02-2016 </papid>yamada and matsumoto, 2003; nivre 2003; nivre and scholz, 2004; <papid> C04-1010 </papid>chen et al, 2005).</prevsent>
<prevsent>manyof them make hard decisions with local information while selecting among next parse states.</prevsent>
</prevsection>
<citsent citstr=" N03-1016 ">
as consequence, though the obtained dependency trees are good in some sense, the n-best output is not guaranteed to be globally optimal in general (klein and manning, 2003).<papid> N03-1016 </papid>on the other hand, a* search that guarantees optimality has been applied to many areas including ai.</citsent>
<aftsection>
<nextsent>klein and manning (2003) <papid> N03-1016 </papid>proposed to use a* search in pcfg parsing.</nextsent>
<nextsent>dienes et al depicted primarily the idea of applying a* search to dependency parsing, but there is not yet formal evaluation results and discussions based on the literatures we have (dienes et al, 2003).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M228">
<title id=" I08-8001.xml">transformation based sentence splitting method for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>transformations are expanded to improve machine translation quality after automatically obtained from manually split corpus.
</prevsent>
<prevsent>through series of experiments we show that the transformation based sentence splitting is effective pre-processing to long sentence translation.
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
statistical approaches to machine translation have been studied actively, after the formalism of statistical machine translation (smt) is proposed by brown et al  (1993).<papid> J93-2003 </papid></citsent>
<aftsection>
<nextsent>although many approaches of them were effective, there are still lots of problems to solve.
</nextsent>
<nextsent>among others, we have an interest in the problems occurring with long sentence decoding.
</nextsent>
<nextsent>various problems occur when we try to translate long input sentences because longer sentence contains more possibilities of selecting translation options and reordering phrases.
</nextsent>
<nextsent>however, reordering models in traditional phrase-based systems are not sufficient to treat such complex cases when we translate long sentences (koehn et al  2003).<papid> N03-1017 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M229">
<title id=" I08-8001.xml">transformation based sentence splitting method for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>among others, we have an interest in the problems occurring with long sentence decoding.
</prevsent>
<prevsent>various problems occur when we try to translate long input sentences because longer sentence contains more possibilities of selecting translation options and reordering phrases.
</prevsent>
</prevsection>
<citsent citstr=" N03-1017 ">
however, reordering models in traditional phrase-based systems are not sufficient to treat such complex cases when we translate long sentences (koehn et al  2003).<papid> N03-1017 </papid></citsent>
<aftsection>
<nextsent>some methods which can offer powerful reordering policies have been proposed like syntax based machine translation (yamada and knight, 2001) and inversion transduction grammar (wu, 1997).<papid> J97-3002 </papid></nextsent>
<nextsent>although these approaches are effective, decoding long sentences is still difficult due to their computational complexity.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M230">
<title id=" I08-8001.xml">transformation based sentence splitting method for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>various problems occur when we try to translate long input sentences because longer sentence contains more possibilities of selecting translation options and reordering phrases.
</prevsent>
<prevsent>however, reordering models in traditional phrase-based systems are not sufficient to treat such complex cases when we translate long sentences (koehn et al  2003).<papid> N03-1017 </papid></prevsent>
</prevsection>
<citsent citstr=" J97-3002 ">
some methods which can offer powerful reordering policies have been proposed like syntax based machine translation (yamada and knight, 2001) and inversion transduction grammar (wu, 1997).<papid> J97-3002 </papid></citsent>
<aftsection>
<nextsent>although these approaches are effective, decoding long sentences is still difficult due to their computational complexity.
</nextsent>
<nextsent>as the length of an input sentence becomes longer, the analysis and decoding become more complex.
</nextsent>
<nextsent>the complexity causes approximations and errors inevitable during the decoding search.
</nextsent>
<nextsent>in order to reduce this kind of difficulty caused by the complexity, long sentence can be paraphrased by several shorter sentences with the same meaning.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M231">
<title id=" I08-8001.xml">transformation based sentence splitting method for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>so, we dont want to fully analyze the sentences to get series of sub-sentences, and our approach to this problem considers splitting only compound sentences.
</prevsent>
<prevsent>in the past years, many research works were concerned with sentence splitting methods to im prove machine translation quality.
</prevsent>
</prevsection>
<citsent citstr=" P98-1070 ">
this idea had been used in speech translation (furuse et al  1998) <papid> P98-1070 </papid>and example based machine translation (doi and sumita, 2004).<papid> C04-1017 </papid></citsent>
<aftsection>
<nextsent>these research works achieved meaningful results in terms of machine translation quality.
</nextsent>
<nextsent>unfortunately, however, the method of doi and sumita using n-gram is not available if the source language is korean.
</nextsent>
<nextsent>in korean language, most of sentences have special form of ending morphemes at the end.
</nextsent>
<nextsent>for that reason, we should determine not only the splitting position but also the ending morphemes that we should replace instead of connecting morphemes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M232">
<title id=" I08-8001.xml">transformation based sentence splitting method for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>so, we dont want to fully analyze the sentences to get series of sub-sentences, and our approach to this problem considers splitting only compound sentences.
</prevsent>
<prevsent>in the past years, many research works were concerned with sentence splitting methods to im prove machine translation quality.
</prevsent>
</prevsection>
<citsent citstr=" C04-1017 ">
this idea had been used in speech translation (furuse et al  1998) <papid> P98-1070 </papid>and example based machine translation (doi and sumita, 2004).<papid> C04-1017 </papid></citsent>
<aftsection>
<nextsent>these research works achieved meaningful results in terms of machine translation quality.
</nextsent>
<nextsent>unfortunately, however, the method of doi and sumita using n-gram is not available if the source language is korean.
</nextsent>
<nextsent>in korean language, most of sentences have special form of ending morphemes at the end.
</nextsent>
<nextsent>for that reason, we should determine not only the splitting position but also the ending morphemes that we should replace instead of connecting morphemes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M233">
<title id=" I08-8001.xml">transformation based sentence splitting method for statistical machine translation </title>
<section> methods.  </section>
<citcontext>
<prevsection>
<prevsent>we use transformation based approach to accomplish our goal.
</prevsent>
<prevsent>2.1 concept of transformation the transformation based learning (tbl) is kind of rule learning methods.
</prevsent>
</prevsection>
<citsent citstr=" J95-4004 ">
the formalism of tbl is introduced by brill (1995).<papid> J95-4004 </papid></citsent>
<aftsection>
<nextsent>in past years, the tbl approach was used to solve various problems in natural language processing such as part of speech (pos) tagging and parsing (brill, 1993).
</nextsent>
<nextsent>a transformation consists of two parts: triggering environment and rewriting rule.
</nextsent>
<nextsent>and the rewriting rule consists of source pattern and target pattern.
</nextsent>
<nextsent>our consideration is how to get the right transformations and apply them to split the long sentences.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M234">
<title id=" I08-4027.xml">word boundary token model for the sighan bakeoff 2007 </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in the sighan bakeoff 2007, we participated in the ckip and the cityu closed tasks.
</prevsent>
<prevsent>our chinese word segmentation system is based on three mod els: (a) word boundary token (wbt) model and (b) triple context matching model for unknown word extraction, and (c) word support model for segmentation disambiguation.
</prevsent>
</prevsection>
<citsent citstr=" I05-3020 ">
since the word support model and triple context matching model have been proposed in our previous work (tsai, 2005, <papid> I05-3020 </papid>2006a and 2006b) at the sighan bakeoff 2005 (thomas, 2005) and 2006 (levow, 2006), <papid> W06-0115 </papid>thema jor descriptions of this paper is on the wbt model.</citsent>
<aftsection>
<nextsent>the remainder of this paper is arranged as follows.
</nextsent>
<nextsent>in section 2, we present the wbt model forex tracting words from each chinese sentence.
</nextsent>
<nextsent>scored results and analyses of our cws system are presented in section 3.
</nextsent>
<nextsent>finally, in section 4, we present our conclusion and discuss the direction of future research.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M235">
<title id=" I08-4027.xml">word boundary token model for the sighan bakeoff 2007 </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in the sighan bakeoff 2007, we participated in the ckip and the cityu closed tasks.
</prevsent>
<prevsent>our chinese word segmentation system is based on three mod els: (a) word boundary token (wbt) model and (b) triple context matching model for unknown word extraction, and (c) word support model for segmentation disambiguation.
</prevsent>
</prevsection>
<citsent citstr=" W06-0115 ">
since the word support model and triple context matching model have been proposed in our previous work (tsai, 2005, <papid> I05-3020 </papid>2006a and 2006b) at the sighan bakeoff 2005 (thomas, 2005) and 2006 (levow, 2006), <papid> W06-0115 </papid>thema jor descriptions of this paper is on the wbt model.</citsent>
<aftsection>
<nextsent>the remainder of this paper is arranged as follows.
</nextsent>
<nextsent>in section 2, we present the wbt model forex tracting words from each chinese sentence.
</nextsent>
<nextsent>scored results and analyses of our cws system are presented in section 3.
</nextsent>
<nextsent>finally, in section 4, we present our conclusion and discuss the direction of future research.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M238">
<title id=" I08-4027.xml">word boundary token model for the sighan bakeoff 2007 </title>
<section> conclusions.  </section>
<citcontext>
<prevsection>
<prevsent>the coverage of 2-char, 3-char, 4-char and great than 4-char error words extracting by our cws for the ckip and the cityu closed ws tasks from table 7, it shows the major n-char unknown word extraction for improving our cws system is on 2-char unknown word extraction.
</prevsent>
<prevsent>it is because that the total coverage of 2-char word errors extraction of our cws system for the ckip and the cityu ws tasks is 75%.
</prevsent>
</prevsection>
<citsent citstr=" P06-2108 ">
in this paper, we describes chinese word segmentation system based on word boundary token model and triple context matching model (tsai, 2005) <papid> I05-3020 </papid>for extracting unknown words; and word support model (tsai, 2006<papid> P06-2108 </papid>a and 2006b) for resolving segmentation ambiguity.</citsent>
<aftsection>
<nextsent>to develop the word boundary model, we define wbt and classify wbt into three types of left, right and bi-direction.
</nextsent>
<nextsent>as per three types of wbt, we define wbt probability and wbt frequency.
</nextsent>
<nextsent>in the sighan bakeoff 2007, we take part in the ckip and the cityu closed word segmentation tasks.
</nextsent>
<nextsent>the scored results show that our cws can increase the bakeoff baseline system with 4.86% and 5.04% f-measures for the ckip and the cityu word segmentation tasks, respectively.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M242">
<title id=" I08-5013.xml">named entity recognition for south asian languages </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>conditional random fields (crfs) (lafferty et al. 2001) with variety of novel and traditional features have been used as classifier for above three modules.
</prevsent>
<prevsent>crfs are undirected graphical models, special case of which is linear chains which are well suited to sequence labeling tasks.
</prevsent>
</prevsection>
<citsent citstr=" N03-1028 ">
they have shown to be useful in part of speech tagging (lafferty et al 2001), shallow parsing (sha and pereira 2003), <papid> N03-1028 </papid>and named entity recognition for hindi newswire data (li and mccallum 2003).</citsent>
<aftsection>
<nextsent>named entity recognition (ner) has been considered as subtask of information extraction.
</nextsent>
<nextsent>different ner systems were evaluated as part of the sixth message understanding conference in 1995 (muc6).
</nextsent>
<nextsent>the target language was english.
</nextsent>
<nextsent>palmer and day (1997) <papid> A97-1028 </papid>have worked on chinese, english, french, japanese, portuguese and spanish and found that the difficulty of the ner task was different for the six languages but that large part of the task could be performed with simple methods.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M243">
<title id=" I08-5013.xml">named entity recognition for south asian languages </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>different ner systems were evaluated as part of the sixth message understanding conference in 1995 (muc6).
</prevsent>
<prevsent>the target language was english.
</prevsent>
</prevsection>
<citsent citstr=" A97-1028 ">
palmer and day (1997) <papid> A97-1028 </papid>have worked on chinese, english, french, japanese, portuguese and spanish and found that the difficulty of the ner task was different for the six languages but that large part of the task could be performed with simple methods.</citsent>
<aftsection>
<nextsent>89 cucerzan et al (1999) used both morphological and contextual clues for identifying named entities in english, greek, hindi, rumanian and turkish.
</nextsent>
<nextsent>with minimal supervision, they obtained overall measures between 40 and 70, depending on the languages used.
</nextsent>
<nextsent>collins (1999) showed that use of un labelled data for ner can reduce the requirements for supervision to just 7 simple seed rules.
</nextsent>
<nextsent>the conll shared task of 2002 and 2003 focused on language independent ner and has performed evaluations on english, spanish, dutch and german and participating systems have performed well.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M244">
<title id=" I08-5013.xml">named entity recognition for south asian languages </title>
<section> features.  </section>
<citcontext>
<prevsection>
<prevsent>for example, suffixes like -bad , -pur, -pally are good indicators of name of location.
</prevsent>
<prevsent>90 ? words are also assigned generalized word class (wc)?
</prevsent>
</prevsection>
<citsent citstr=" P02-1062 ">
similar to collins (2002), <papid> P02-1062 </papid>which replaces all letters with a?, digits with 0?, punctuation marks with p?, and other characters with ?-?.</citsent>
<aftsection>
<nextsent>there is similar brief class (bwc) (settles 2004)?<papid> W04-1221 </papid></nextsent>
<nextsent>which collapses consecutive characters into one.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M245">
<title id=" I08-5013.xml">named entity recognition for south asian languages </title>
<section> features.  </section>
<citcontext>
<prevsection>
<prevsent>90 ? words are also assigned generalized word class (wc)?
</prevsent>
<prevsent>similar to collins (2002), <papid> P02-1062 </papid>which replaces all letters with a?, digits with 0?, punctuation marks with p?, and other characters with ?-?.</prevsent>
</prevsection>
<citsent citstr=" W04-1221 ">
there is similar brief class (bwc) (settles 2004)?<papid> W04-1221 </papid></citsent>
<aftsection>
<nextsent>which collapses consecutive characters into one.
</nextsent>
<nextsent>thus the words d.d.t.?
</nextsent>
<nextsent>and ab-1946?
</nextsent>
<nextsent>would both be given the features wc=apapap, bwc=apapap and wc=aap0000, bwc=ap0 respectively, in above example hyphen forms the part of punctuation marks.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M246">
<title id=" I08-5013.xml">named entity recognition for south asian languages </title>
<section> experiments and discussion.  </section>
<citcontext>
<prevsection>
<prevsent>this is the reason why the accuracy of english system didnt fall when removed capitalization and introduced common noun phenomena since pos context and chunk context helps lot.
</prevsent>
<prevsent>since conll 2003 data is already pos tagged and chunked, hence pos and chunks correspond to capitalized data.
</prevsent>
</prevsection>
<citsent citstr=" N03-1033 ">
to make it more even, ran stanford pos tagger (toutanova et al 2003) <papid> N03-1033 </papid>on the same mono case conll 2003 data and then train the model using only word and pos con text.</citsent>
<aftsection>
<nextsent>the numbers drop on test set by more than 15% as shown in table 7.
</nextsent>
<nextsent>for development set the overall f-measure is around 74%.
</nextsent>
<nextsent>entity precision recall f-measure person 66.97 53.93 59.75 location 68.57 56.54 61.98 organization 71.64 53.55 61.29 misc.
</nextsent>
<nextsent>74.71 55.98 64.01 overall 69.69 54.84 61.38 table7: system trained on pos tagger ran on mono-case data these numbers are comparable to hindi data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M247">
<title id=" I08-5005.xml">aggregating machine learning and rule based heuristics for named entity recognition </title>
<section> classification into the correct category.  </section>
<citcontext>
<prevsection>
<prevsent>2 previous work.
</prevsent>
<prevsent>the linguistic methods generally use rules manually written by linguists.
</prevsent>
</prevsection>
<citsent citstr=" C96-1071 ">
there are several rule based ner systems, containing mainly lexicalized grammar, gazetteer lists, and list of trigger words, which are capable of providing upto 92% measure accuracy for english (mcdonald, 1996; wakao et al, 1996).<papid> C96-1071 </papid></citsent>
<aftsection>
<nextsent>linguistic approach uses hand-crafted rules which need skilled linguistics.
</nextsent>
<nextsent>the chief disadvantage of these rule-based techniques is that theyre quire huge experience and grammatical knowledge of the particular language or domain and these systems are not transferable to other languages or domains.
</nextsent>
<nextsent>however, given the closer nature of many indian languages, the cost of adaptation of re 25 source from one language to another could be quite less (singh and surana, 2007).<papid> W07-1306 </papid></nextsent>
<nextsent>various machine learning techniques have also been successfully used for the ner task.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M248">
<title id=" I08-5005.xml">aggregating machine learning and rule based heuristics for named entity recognition </title>
<section> classification into the correct category.  </section>
<citcontext>
<prevsection>
<prevsent>linguistic approach uses hand-crafted rules which need skilled linguistics.
</prevsent>
<prevsent>the chief disadvantage of these rule-based techniques is that theyre quire huge experience and grammatical knowledge of the particular language or domain and these systems are not transferable to other languages or domains.
</prevsent>
</prevsection>
<citsent citstr=" W07-1306 ">
however, given the closer nature of many indian languages, the cost of adaptation of re 25 source from one language to another could be quite less (singh and surana, 2007).<papid> W07-1306 </papid></citsent>
<aftsection>
<nextsent>various machine learning techniques have also been successfully used for the ner task.
</nextsent>
<nextsent>generally hidden markov model (bikel et al,1997), <papid> A97-1029 </papid>maximum entropy (borthwick, 1999), conditional random field (li and mccallum, 2004) are more popular machine learning techniques used for the purpose of ner.</nextsent>
<nextsent>hybrid systems have been generally more effective at the task of ner.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M249">
<title id=" I08-5005.xml">aggregating machine learning and rule based heuristics for named entity recognition </title>
<section> classification into the correct category.  </section>
<citcontext>
<prevsection>
<prevsent>however, given the closer nature of many indian languages, the cost of adaptation of re 25 source from one language to another could be quite less (singh and surana, 2007).<papid> W07-1306 </papid></prevsent>
<prevsent>various machine learning techniques have also been successfully used for the ner task.</prevsent>
</prevsection>
<citsent citstr=" A97-1029 ">
generally hidden markov model (bikel et al,1997), <papid> A97-1029 </papid>maximum entropy (borthwick, 1999), conditional random field (li and mccallum, 2004) are more popular machine learning techniques used for the purpose of ner.</citsent>
<aftsection>
<nextsent>hybrid systems have been generally more effective at the task of ner.
</nextsent>
<nextsent>given lesser data and more complex ne classes which were present in nersseal shared task, hybrid systems make more sense.
</nextsent>
<nextsent>srihari et al (2000) <papid> A00-1034 </papid>combines maxent, hidden markov model (hmm) and handcrafted rules to build an ner system.</nextsent>
<nextsent>though not much work has been done for other south asian languages, some previous work focuses on ner for hindi.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M250">
<title id=" I08-5005.xml">aggregating machine learning and rule based heuristics for named entity recognition </title>
<section> classification into the correct category.  </section>
<citcontext>
<prevsection>
<prevsent>hybrid systems have been generally more effective at the task of ner.
</prevsent>
<prevsent>given lesser data and more complex ne classes which were present in nersseal shared task, hybrid systems make more sense.
</prevsent>
</prevsection>
<citsent citstr=" A00-1034 ">
srihari et al (2000) <papid> A00-1034 </papid>combines maxent, hidden markov model (hmm) and handcrafted rules to build an ner system.</citsent>
<aftsection>
<nextsent>though not much work has been done for other south asian languages, some previous work focuses on ner for hindi.
</nextsent>
<nextsent>it has been previously attempted by cucerzan and yarowsky in their language independent ner work which used morphological and contextual evidences (cucerzan and yarowsky, 1999).<papid> W99-0612 </papid></nextsent>
<nextsent>they ran their experiment with 5 different languages.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M251">
<title id=" I08-5005.xml">aggregating machine learning and rule based heuristics for named entity recognition </title>
<section> classification into the correct category.  </section>
<citcontext>
<prevsection>
<prevsent>srihari et al (2000) <papid> A00-1034 </papid>combines maxent, hidden markov model (hmm) and handcrafted rules to build an ner system.</prevsent>
<prevsent>though not much work has been done for other south asian languages, some previous work focuses on ner for hindi.</prevsent>
</prevsection>
<citsent citstr=" W99-0612 ">
it has been previously attempted by cucerzan and yarowsky in their language independent ner work which used morphological and contextual evidences (cucerzan and yarowsky, 1999).<papid> W99-0612 </papid></citsent>
<aftsection>
<nextsent>they ran their experiment with 5 different languages.
</nextsent>
<nextsent>among these the accuracy for hindi was the worst.
</nextsent>
<nextsent>for hindi the system achieved 42% f-value with recall of 28% and about 85% precision.
</nextsent>
<nextsent>a result which highlights lack of good training data, and other various issues involved with linguistic handling of indian languages.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M252">
<title id=" I08-6001.xml">the effects of language relatedness on multilingual information retrieval a case study with indo european and semitic languages </title>
<section> the framework.  </section>
<citcontext>
<prevsection>
<prevsent>for our experiments, the training and test data were taken from the bible and quran respectively.
</prevsent>
<prevsent>as training data, the bible lends itself extremely well to multilingual lsa.
</prevsent>
</prevsection>
<citsent citstr=" P07-1110 ">
it is highly available in multiple languages1 (over 80 parallel translations in 50 languages, mostly public-domain, are available from single website, www.unboundbible.org); and very fine-grained alignment is possible (by verse) (resnik et al 1999, chew and abdelali 2007).<papid> P07-1110 </papid></citsent>
<aftsection>
<nextsent>many purpose-built parallel corpora are biased towards particular language groups (for example, the european union funds work in clir, but it tends to be biased towards european languages ? for example, see peters 2001).
</nextsent>
<nextsent>this is not as true of the bible, and the fact that it covers wider range of languages is reflection of the reasons it was translated in the first place.
</nextsent>
<nextsent>the question which is most commonly raised about use of the bible in this way is whether its coverage of vocabulary from other domains is sufficient to allow it to be used as training data for most applications.
</nextsent>
<nextsent>based on variety of experiments we have carried out (see for example chew et al  forthcoming), we believe this need not always be drawback ? it depends largely on the intended application.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M253">
<title id=" I08-6001.xml">the effects of language relatedness on multilingual information retrieval a case study with indo european and semitic languages </title>
<section> the framework.  </section>
<citcontext>
<prevsection>
<prevsent>we tokenized each of the 570 test documents, applying the weighting scheme described above to obtain vector of weighted frequencies of each term in the document, then multiplying that vector by ? s-1, also as described above.
</prevsent>
<prevsent>the result was set of projected document vectors in the 300 dimensional lsa space.
</prevsent>
</prevsection>
<citsent citstr=" W02-0506 ">
for some of our experiments, we used light stemmer for arabic (darwish 2002) <papid> W02-0506 </papid>to replace inflected forms in the training data with citation forms.</citsent>
<aftsection>
<nextsent>it is commonly accepted that morphology improves ir (abdou et al  2005, lavie et al  2004, larkey et al  2002, oard and gey 2002), and it will be seen that our results generally confirm this.
</nextsent>
<nextsent>for hebrew, we used the westminster leningrad codex in the training data.
</nextsent>
<nextsent>since this is available for download either with vowels or without vowels, no morphological pre-processing was required in this case; we simply substituted one version for the other in the training data when necessary.
</nextsent>
<nextsent>various measurements are used for evaluating ir systems performance (van rijsbergen 1979).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M258">
<title id=" I08-4010.xml">the fourth international chinese language processing bakeoff chinese word segmentation named entity recognition and chinese pos tagging </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>chinese tokenization, as the foundation of many downstream processing tasks, has attracted lots of research interest.
</prevsent>
<prevsent>however, it is still significant challenge for all the researchers.
</prevsent>
</prevsection>
<citsent citstr=" I05-3017 ">
sighan, the special interest group for chinese language processing of the association for computational linguistics, conducted three prior word segmentation bake offs, in 2003, 2005 and 2006(sproat and emerson, 2003; emerson, 2005; <papid> I05-3017 </papid>levow, 2006),<papid> W06-0115 </papid>which established benchmarks for word segmentation and named entity recognition.</citsent>
<aftsection>
<nextsent>the bakeoff presentations at sighan workshops highlighted new approaches in this field.
</nextsent>
<nextsent>the fourth bakeoff was jointly held with the first cips chinese language processing evaluation in the summer of 2007, and co-organized by sighan, chinese ldc, and the verifying center of chinese language and character standards of the state language commission of p.r.c. in this bakeoff, we continue the chinese word segmentation and named entity recognition tasks.
</nextsent>
<nextsent>furthermore, new evaluation task has been augmented, the task for chinese pos tagging.
</nextsent>
<nextsent>in this evaluation task, participating system will take given segmented corpus as the in put, and only the pos tagging performance will be evaluated.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M259">
<title id=" I08-4010.xml">the fourth international chinese language processing bakeoff chinese word segmentation named entity recognition and chinese pos tagging </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>chinese tokenization, as the foundation of many downstream processing tasks, has attracted lots of research interest.
</prevsent>
<prevsent>however, it is still significant challenge for all the researchers.
</prevsent>
</prevsection>
<citsent citstr=" W06-0115 ">
sighan, the special interest group for chinese language processing of the association for computational linguistics, conducted three prior word segmentation bake offs, in 2003, 2005 and 2006(sproat and emerson, 2003; emerson, 2005; <papid> I05-3017 </papid>levow, 2006),<papid> W06-0115 </papid>which established benchmarks for word segmentation and named entity recognition.</citsent>
<aftsection>
<nextsent>the bakeoff presentations at sighan workshops highlighted new approaches in this field.
</nextsent>
<nextsent>the fourth bakeoff was jointly held with the first cips chinese language processing evaluation in the summer of 2007, and co-organized by sighan, chinese ldc, and the verifying center of chinese language and character standards of the state language commission of p.r.c. in this bakeoff, we continue the chinese word segmentation and named entity recognition tasks.
</nextsent>
<nextsent>furthermore, new evaluation task has been augmented, the task for chinese pos tagging.
</nextsent>
<nextsent>in this evaluation task, participating system will take given segmented corpus as the in put, and only the pos tagging performance will be evaluated.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M260">
<title id=" I08-4017.xml">unsupervised segmentation helps supervised learning of character tagging for word segmentation and named entity recognition </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>it ranks at the top in all closed tests of word segmentation and gives promising results forall closed and open ner tasks in the bakeoff.
</prevsent>
<prevsent>tag set selection and unsupervised segmentation play critical role in this success.
</prevsent>
</prevsection>
<citsent citstr=" I05-3025 ">
a number of recent studies show that character sequence labeling is simple but effective formulation of chinese word segmentation and name entity recognition for machine learning (xue, 2003; low et al, 2005; <papid> I05-3025 </papid>zhao et al, 2006<papid> W06-0127 </papid>a; chen et al,2006).<papid> W06-0116 </papid></citsent>
<aftsection>
<nextsent>character tagging becomes prevailing technique for this kind of labeling task for chinese language processing, following the current trend of applying machine learning as core technology in the field of natural language processing.
</nextsent>
<nextsent>in particular,when full-fledged general-purpose sequence learning model such as crfs is involved, the only work to do forgiven application is to identify an ideal set of features and hyper parameters for the purpose 1the fourth international chinese language processingbakeoff &amp; the first cips chinese language processing evaluation, at http://www.china-language.gov.cn/bakeoff08/bakeoff 08 basic.html.
</nextsent>
<nextsent>of achieving the best learning model that we can with available training data.
</nextsent>
<nextsent>our work in this aspect provides solid foundation for applying an unsupervised segmentation criterion to enrich the supervised crfs learning for further performance enhancement on both word segmentation and ner.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M261">
<title id=" I08-4017.xml">unsupervised segmentation helps supervised learning of character tagging for word segmentation and named entity recognition </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>it ranks at the top in all closed tests of word segmentation and gives promising results forall closed and open ner tasks in the bakeoff.
</prevsent>
<prevsent>tag set selection and unsupervised segmentation play critical role in this success.
</prevsent>
</prevsection>
<citsent citstr=" W06-0127 ">
a number of recent studies show that character sequence labeling is simple but effective formulation of chinese word segmentation and name entity recognition for machine learning (xue, 2003; low et al, 2005; <papid> I05-3025 </papid>zhao et al, 2006<papid> W06-0127 </papid>a; chen et al,2006).<papid> W06-0116 </papid></citsent>
<aftsection>
<nextsent>character tagging becomes prevailing technique for this kind of labeling task for chinese language processing, following the current trend of applying machine learning as core technology in the field of natural language processing.
</nextsent>
<nextsent>in particular,when full-fledged general-purpose sequence learning model such as crfs is involved, the only work to do forgiven application is to identify an ideal set of features and hyper parameters for the purpose 1the fourth international chinese language processingbakeoff &amp; the first cips chinese language processing evaluation, at http://www.china-language.gov.cn/bakeoff08/bakeoff 08 basic.html.
</nextsent>
<nextsent>of achieving the best learning model that we can with available training data.
</nextsent>
<nextsent>our work in this aspect provides solid foundation for applying an unsupervised segmentation criterion to enrich the supervised crfs learning for further performance enhancement on both word segmentation and ner.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M265">
<title id=" I08-4017.xml">unsupervised segmentation helps supervised learning of character tagging for word segmentation and named entity recognition </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>it ranks at the top in all closed tests of word segmentation and gives promising results forall closed and open ner tasks in the bakeoff.
</prevsent>
<prevsent>tag set selection and unsupervised segmentation play critical role in this success.
</prevsent>
</prevsection>
<citsent citstr=" W06-0116 ">
a number of recent studies show that character sequence labeling is simple but effective formulation of chinese word segmentation and name entity recognition for machine learning (xue, 2003; low et al, 2005; <papid> I05-3025 </papid>zhao et al, 2006<papid> W06-0127 </papid>a; chen et al,2006).<papid> W06-0116 </papid></citsent>
<aftsection>
<nextsent>character tagging becomes prevailing technique for this kind of labeling task for chinese language processing, following the current trend of applying machine learning as core technology in the field of natural language processing.
</nextsent>
<nextsent>in particular,when full-fledged general-purpose sequence learning model such as crfs is involved, the only work to do forgiven application is to identify an ideal set of features and hyper parameters for the purpose 1the fourth international chinese language processingbakeoff &amp; the first cips chinese language processing evaluation, at http://www.china-language.gov.cn/bakeoff08/bakeoff 08 basic.html.
</nextsent>
<nextsent>of achieving the best learning model that we can with available training data.
</nextsent>
<nextsent>our work in this aspect provides solid foundation for applying an unsupervised segmentation criterion to enrich the supervised crfs learning for further performance enhancement on both word segmentation and ner.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M278">
<title id=" I08-4017.xml">unsupervised segmentation helps supervised learning of character tagging for word segmentation and named entity recognition </title>
<section> evaluation results.  </section>
<citcontext>
<prevsection>
<prevsent>the performance of both word segmentation and ner is measured in terms of the f-measure = 2rp/(r + ), where and are the recall and precision of segmentation or ner.
</prevsent>
<prevsent>we tested the techniques described above with the previous bakeoffs?
</prevsent>
</prevsection>
<citsent citstr=" W03-1719 ">
data5 (sproat and emerson,2003; <papid> W03-1719 </papid>emerson, 2005; <papid> I05-3017 </papid>levow, 2006).<papid> W06-0115 </papid></citsent>
<aftsection>
<nextsent>the evaluation results for the closed tests of word segmentation are reported in table 5 and those for the ner on two corpora of bakeoff-3 are in the upper part of table 7.
</nextsent>
<nextsent>?+/av?
</nextsent>
<nextsent>indicates whether av features are applied.
</nextsent>
<nextsent>for bakeoff-4, we participated in all five closed tracks of word segmentation, namely, cityu, ckip, ctb, ncc, and sxu, and in all closed and open ner tracks of cityu and msra.6 the evaluation 4http://zh.wikipedia.org/wiki/??
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M279">
<title id=" I08-4017.xml">unsupervised segmentation helps supervised learning of character tagging for word segmentation and named entity recognition </title>
<section> evaluation results.  </section>
<citcontext>
<prevsection>
<prevsent>the performance of both word segmentation and ner is measured in terms of the f-measure = 2rp/(r + ), where and are the recall and precision of segmentation or ner.
</prevsent>
<prevsent>we tested the techniques described above with the previous bakeoffs?
</prevsent>
</prevsection>
<citsent citstr=" I05-3017 ">
data5 (sproat and emerson,2003; <papid> W03-1719 </papid>emerson, 2005; <papid> I05-3017 </papid>levow, 2006).<papid> W06-0115 </papid></citsent>
<aftsection>
<nextsent>the evaluation results for the closed tests of word segmentation are reported in table 5 and those for the ner on two corpora of bakeoff-3 are in the upper part of table 7.
</nextsent>
<nextsent>?+/av?
</nextsent>
<nextsent>indicates whether av features are applied.
</nextsent>
<nextsent>for bakeoff-4, we participated in all five closed tracks of word segmentation, namely, cityu, ckip, ctb, ncc, and sxu, and in all closed and open ner tracks of cityu and msra.6 the evaluation 4http://zh.wikipedia.org/wiki/??
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M280">
<title id=" I08-4017.xml">unsupervised segmentation helps supervised learning of character tagging for word segmentation and named entity recognition </title>
<section> evaluation results.  </section>
<citcontext>
<prevsection>
<prevsent>the performance of both word segmentation and ner is measured in terms of the f-measure = 2rp/(r + ), where and are the recall and precision of segmentation or ner.
</prevsent>
<prevsent>we tested the techniques described above with the previous bakeoffs?
</prevsent>
</prevsection>
<citsent citstr=" W06-0115 ">
data5 (sproat and emerson,2003; <papid> W03-1719 </papid>emerson, 2005; <papid> I05-3017 </papid>levow, 2006).<papid> W06-0115 </papid></citsent>
<aftsection>
<nextsent>the evaluation results for the closed tests of word segmentation are reported in table 5 and those for the ner on two corpora of bakeoff-3 are in the upper part of table 7.
</nextsent>
<nextsent>?+/av?
</nextsent>
<nextsent>indicates whether av features are applied.
</nextsent>
<nextsent>for bakeoff-4, we participated in all five closed tracks of word segmentation, namely, cityu, ckip, ctb, ncc, and sxu, and in all closed and open ner tracks of cityu and msra.6 the evaluation 4http://zh.wikipedia.org/wiki/??
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M281">
<title id=" I08-4017.xml">unsupervised segmentation helps supervised learning of character tagging for word segmentation and named entity recognition </title>
<section> evaluation results.  </section>
<citcontext>
<prevsection>
<prevsent>this makes our official evaluation results extremely high but trivial, for part of this corpus is used as the msra ner test corpus for bakeoff-4.
</prevsent>
<prevsent>presented here are the results without using this aner.
</prevsent>
</prevsection>
<citsent citstr=" W06-0120 ">
bopen2 is the result of open1 using no ne list feature.results of word segmentation and ner for our system are presented in tables 6 and 7, respectively.for the purpose of comparison, the word segmentation performance of our system on bakeoff-4 data using the 2- and 4-tag sets and the best corresponding n-gram feature templates as in (tsai et al, 2006; <papid> W06-0120 </papid>low et al, 2005) <papid> I05-3025 </papid>are presented in table 8.7 this comparison reconfirms the conclusion in (zhao et cityu datasets in any other situation than the bakeoff.</citsent>
<aftsection>
<nextsent>7the templates for the 2-tag set, adopted from (tsai et al, 2006), <papid> W06-0120 </papid>include c2, c1, c0, c1, c3c1, c2c0, c2c1, c1c0, c1c1 and c0c1.</nextsent>
<nextsent>those for the 4-tag set, adopted from (xue, 2003) and (low et al, 2005), <papid> I05-3025 </papid>include c2, c1, c0, c1, c2, c2c1, c1c0, c1c1, c0c1and c1c2.al., 2006b) about tag set selection for character tagging for word segmentation that the 6-tag set is more effective than others, each with its own best corresponding feature template set.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M288">
<title id=" I08-4017.xml">unsupervised segmentation helps supervised learning of character tagging for word segmentation and named entity recognition </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>we realize that the ne lists available to us are farfrom sufficient for coping with all nes in bakeoff 4.
</prevsent>
<prevsent>it is reasonable that using richer external ne.
</prevsent>
</prevsection>
<citsent citstr=" W06-0126 ">
lists gives better ner performance in many cases (zhang et al, 2006).<papid> W06-0126 </papid></citsent>
<aftsection>
<nextsent>surprisingly, however, the nelist features used in our ner do not lead to any significant performance improvement, according to the evaluation results in table 7.
</nextsent>
<nextsent>this is certainly an other issue for our further inspection.
</nextsent>
<nextsent>without doubt our achievements in bakeoff-4 owes not only to the careful selection of character tag set and feature templates for exerting the strength ofcrfs learning but also to the effectiveness of our unsupervised segmentation approach.
</nextsent>
<nextsent>it is for the sake of simplicity that similar sets of character tags and feature templates are applied to two distinctive labeling tasks, word segmentation and ner.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M289">
<title id=" I08-4022.xml">chinese named entity recognition and word segmentation based on character </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>chinese word segmentation and ner are two of the most fundamental problems in chinese information processing and have attracted more and more attentions.
</prevsent>
<prevsent>many methods have been presented, of which, machine learning methods have obviously competitive advantage in such problems.
</prevsent>
</prevsection>
<citsent citstr=" W06-0127 ">
maximum entropy (ng and low, 2005) and crfs (hai zhao et al 2006, <papid> W06-0127 </papid>zhou junsheng et al. 2006) come to good performance in the former sighan bakeoff.</citsent>
<aftsection>
<nextsent>we consider both tasks as sequence labeling problem, and character-based conditional random fields (crfs) model is applied in this bakeoff.
</nextsent>
<nextsent>our system used crf++ package version 0.49 implemented by taku kudo from sourceforge1.
</nextsent>
<nextsent>the system is mainly based on crfs, while different strategies are introduced in word segmentation task and ner task.
</nextsent>
<nextsent>2.1 crfs.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M290">
<title id=" I08-4024.xml">chinese tagging based on maximum entropy model </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our models for the evaulation are based on the maximum entropy approach, we concentrated on the word segmentation task for the bakeoff and our best official results on all the corpora for this task are 0.9083 f-score on cityu, 0.8985 on ckip, 0.9077 on ctb, 0.8995 on ncc and 0.9146 on sxu.
</prevsent>
<prevsent>in the fourth sighan bakeoff, besides providing the evaluation tasks for the word segmentation and ner, it also introduced another important evaluation task, pos tagging for chinese language.
</prevsent>
</prevsection>
<citsent citstr=" W04-3236 ">
in this bakeoff, our models built for the tasks are similar to that in the work of ng and low (2004).<papid> W04-3236 </papid></citsent>
<aftsection>
<nextsent>the models are based on maximum entropy framework (ratnaparkhi, 1996; <papid> W96-0213 </papid>xue and shen, 2003).<papid> W03-1728 </papid></nextsent>
<nextsent>they are trained on the corpora for the tasks from the bakeoff.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M293">
<title id=" I08-4024.xml">chinese tagging based on maximum entropy model </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in the fourth sighan bakeoff, besides providing the evaluation tasks for the word segmentation and ner, it also introduced another important evaluation task, pos tagging for chinese language.
</prevsent>
<prevsent>in this bakeoff, our models built for the tasks are similar to that in the work of ng and low (2004).<papid> W04-3236 </papid></prevsent>
</prevsection>
<citsent citstr=" W96-0213 ">
the models are based on maximum entropy framework (ratnaparkhi, 1996; <papid> W96-0213 </papid>xue and shen, 2003).<papid> W03-1728 </papid></citsent>
<aftsection>
<nextsent>they are trained on the corpora for the tasks from the bakeoff.
</nextsent>
<nextsent>to understand the model, the implementation of the models is wholly done ourselves.
</nextsent>
<nextsent>we used visual studio .net 2003 and c++ as the implementation language.
</nextsent>
<nextsent>the improved iterative scaling (iis) (pietra et al, 1997) is used as the parameter estimation algorithm for the models.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M294">
<title id=" I08-4024.xml">chinese tagging based on maximum entropy model </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in the fourth sighan bakeoff, besides providing the evaluation tasks for the word segmentation and ner, it also introduced another important evaluation task, pos tagging for chinese language.
</prevsent>
<prevsent>in this bakeoff, our models built for the tasks are similar to that in the work of ng and low (2004).<papid> W04-3236 </papid></prevsent>
</prevsection>
<citsent citstr=" W03-1728 ">
the models are based on maximum entropy framework (ratnaparkhi, 1996; <papid> W96-0213 </papid>xue and shen, 2003).<papid> W03-1728 </papid></citsent>
<aftsection>
<nextsent>they are trained on the corpora for the tasks from the bakeoff.
</nextsent>
<nextsent>to understand the model, the implementation of the models is wholly done ourselves.
</nextsent>
<nextsent>we used visual studio .net 2003 and c++ as the implementation language.
</nextsent>
<nextsent>the improved iterative scaling (iis) (pietra et al, 1997) is used as the parameter estimation algorithm for the models.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M307">
<title id=" I08-4021.xml">a morpheme based partofspeech tagger for chinese </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>thirdly, there are many out-of-vocabulary (oov) words in real chinese text whose pos categories are not defined in the dictionary used.
</prevsent>
<prevsent>all these factors make it much more difficult to achieve high-performance pos tagger for chinese.
</prevsent>
</prevsection>
<citsent citstr=" W04-3236 ">
recent studies in chinese pos tagging focus on statistical or machine learning approaches with either characters or words as basic units for tagging (ng and low, 2004; <papid> W04-3236 </papid>fu and luke, 2006).</citsent>
<aftsection>
<nextsent>very little research has been devoted to resolving chinese pos tagging problems based on morphemes.
</nextsent>
<nextsent>in our system, we prefer morphemes to characters or words as tagging units for three reasons.
</nextsent>
<nextsent>first, words are made of morphemes instead of characters (wu and tseng, 1995; packard, 2000).
</nextsent>
<nextsent>second, most morphemes are productive in word formation (baayen, 1989; sproat and shih, 2002; nishimoto, 2003), particularly in the formation of morphologically-derived words (mdws) and proper nouns, which are the major source of oov words in chinese texts.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M308">
<title id=" I08-4021.xml">a morpheme based partofspeech tagger for chinese </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>second, most morphemes are productive in word formation (baayen, 1989; sproat and shih, 2002; nishimoto, 2003), particularly in the formation of morphologically-derived words (mdws) and proper nouns, which are the major source of oov words in chinese texts.
</prevsent>
<prevsent>third, packard (2000) indicates that chinese do have morphology.
</prevsent>
</prevsection>
<citsent citstr=" I05-3005 ">
more over, morphology proves to be very informative cue for predicting pos categories of chinese oov words (tseng et al 2005).<papid> I05-3005 </papid></citsent>
<aftsection>
<nextsent>therefore, we believe that morpheme-based framework would be more effective than the character- or word-based ones in capturing both word-internal morphological features and word-external contextual information for chinese pos disambiguation and unknown word guessing (uwg) as well.
</nextsent>
<nextsent>thus we present morpheme-based pos tagger for chinese in this paper.
</nextsent>
<nextsent>it consists of two main components, namely morpheme segmentation component for segmenting each word in sentence into sequence of morphemes, based on the forward maximum matching (fmm) technique, and lexical tagging component for labeling each segmented morpheme with proper tag indicating its position pattern informing word of specific type, based on lexicalized hidden markov models (hmms).
</nextsent>
<nextsent>lack of large morphological knowl 124 sixth sighan workshop on chinese language processing edge base is major obstacle to chinese morphological analysis (tseng and chen, 2002).<papid> W02-1811 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M309">
<title id=" I08-4021.xml">a morpheme based partofspeech tagger for chinese </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>thus we present morpheme-based pos tagger for chinese in this paper.
</prevsent>
<prevsent>it consists of two main components, namely morpheme segmentation component for segmenting each word in sentence into sequence of morphemes, based on the forward maximum matching (fmm) technique, and lexical tagging component for labeling each segmented morpheme with proper tag indicating its position pattern informing word of specific type, based on lexicalized hidden markov models (hmms).
</prevsent>
</prevsection>
<citsent citstr=" W02-1811 ">
lack of large morphological knowl 124 sixth sighan workshop on chinese language processing edge base is major obstacle to chinese morphological analysis (tseng and chen, 2002).<papid> W02-1811 </papid></citsent>
<aftsection>
<nextsent>to overcome this problem and to facilitate morpheme based pos tagging as well, we have also developed statistically-based technique for automatically extracting morphemes from pos-tagged corpora.
</nextsent>
<nextsent>we participated in four closed tracks for pos tagging at the fourth international chinese language processing bakeoff sponsored by the acl sighan and tested our system on different testing corpora.
</nextsent>
<nextsent>in this paper, we also made summary of this work and give some brief analysis on there sults.
</nextsent>
<nextsent>the rest of this paper is organized as follows: section 2 is brief description of our system.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M310">
<title id=" I08-7001.xml">development of bengali named entity tagged corpus and its use inner systems </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the use of the web as corpus for teaching and research on language technology has been proposed number of times (rundel, 2000; fletcher, 2001; robb, 2003; fletcher, 2003).
</prevsent>
<prevsent>there is long history of creating standard for western language resources.
</prevsent>
</prevsection>
<citsent citstr=" P06-2035 ">
the human language technology (hlt) society in europe has been particularly zealous for the standardization, making series of attempts such as eagles3, prole/simple (lenci et al, 2000), isle/mile (calzolari et al, 2003; bertagna et al, 2004) and more recently multilingual lexical database generation from parallel texts in 20 european languages (giguet and luquet, 2006).<papid> P06-2035 </papid></citsent>
<aftsection>
<nextsent>on the other hand, inspite of having great linguistic and cultural diversities, asian language resources have received much less attention than their western counterparts.
</nextsent>
<nextsent>a new project (takenobou et al, 2006) has been started to create common standard for asian language resources.
</nextsent>
<nextsent>they have extended an existing description framework, the 3 http://www.ilc.cnr.it/eagles96/home.html the 6th workshop on asian languae resources, 2008 1 mile (bertagna et al, 2004), to describe several lexical entries of japanese, chinese and thai.
</nextsent>
<nextsent>india is multilingual country with the enormous cultural diversities.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M311">
<title id=" I08-4003.xml">mining transliterations from web query results an incremental approach </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>or semantic translations.
</prevsent>
<prevsent>the increasing size of multilingual content on the web has made it live information source rich in transliterations.
</prevsent>
</prevsection>
<citsent citstr=" P06-1142 ">
research on automatic acquisition of transliteration pairs in batch mode has shown promising results (kuo et al, 2006).<papid> P06-1142 </papid></citsent>
<aftsection>
<nextsent>in dealing with the dynamic growth of the web, it is almost impossible to collect and store all its contents in local storage.
</nextsent>
<nextsent>therefore, there is need to develop an incremental learning algorithm to mine transliterations in an on-line manner.
</nextsent>
<nextsent>in general, an incremental learning technique is designed for adapting model towards changing environment.
</nextsent>
<nextsent>we are interested in deducing the incremental learning method for automatically constructing an english-chinese (e-c) transliteration lexicon from web query results.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M315">
<title id=" I08-4003.xml">mining transliterations from web query results an incremental approach </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>section 5 provides report on the experiments conducted and finally, we conclude in section 6.
</prevsent>
<prevsent>much of research on extraction of transliterations has been motivated by information retrieval techniques, where attempts to extracting transliteration pairs from large bodies of corpora have been made.
</prevsent>
</prevsection>
<citsent citstr=" P98-1069 ">
some have proposed extracting translations from parallel or comparable bitexts using co-occurrence analysis or context-vector approach (fung and yee, 1998; <papid> P98-1069 </papid>nie et al, 1999).</citsent>
<aftsection>
<nextsent>these methods compare the semantic similarities between source and target words without taking their phonetic similarities into account.
</nextsent>
<nextsent>another direction of research is focused on es 16 sixth sighan workshop on chinese language processingtablishing the phonetic relationship between transliteration pairs.
</nextsent>
<nextsent>this typically involves the encoding of phoneme- or grapheme-based mapping rules using generative model trained from large bilingual lexicon.
</nextsent>
<nextsent>suppose that ew and cw form an e-c transliteration pair.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M316">
<title id=" I08-4003.xml">mining transliterations from web query results an incremental approach </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>this typically involves the encoding of phoneme- or grapheme-based mapping rules using generative model trained from large bilingual lexicon.
</prevsent>
<prevsent>suppose that ew and cw form an e-c transliteration pair.
</prevsent>
</prevsection>
<citsent citstr=" J98-4003 ">
the phoneme-based approach (knight &amp; graehl, 1998) <papid> J98-4003 </papid>first converts ew into an intermediate phonemic representation and then converts the phonemic representation into its chinese counterpart cw.</citsent>
<aftsection>
<nextsent>the grapheme-based approach, also known as direct ortho graphical mapping (li et al, 2004), <papid> P04-1021 </papid>which treats transliteration as statistical machine translation problem under monotonic constraints, has also achieved promising results.</nextsent>
<nextsent>many efforts have also been channeled to tapping the wealth of the web for harvesting tran slit eration/translation pairs.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M317">
<title id=" I08-4003.xml">mining transliterations from web query results an incremental approach </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>suppose that ew and cw form an e-c transliteration pair.
</prevsent>
<prevsent>the phoneme-based approach (knight &amp; graehl, 1998) <papid> J98-4003 </papid>first converts ew into an intermediate phonemic representation and then converts the phonemic representation into its chinese counterpart cw.</prevsent>
</prevsection>
<citsent citstr=" P04-1021 ">
the grapheme-based approach, also known as direct ortho graphical mapping (li et al, 2004), <papid> P04-1021 </papid>which treats transliteration as statistical machine translation problem under monotonic constraints, has also achieved promising results.</citsent>
<aftsection>
<nextsent>many efforts have also been channeled to tapping the wealth of the web for harvesting tran slit eration/translation pairs.
</nextsent>
<nextsent>these include studying the query logs (brill et al, 2001), unrelated corpora (rapp, 1999), <papid> P99-1067 </papid>and comparable corpora (sproat et al 2006).<papid> P06-1010 </papid></nextsent>
<nextsent>to establish cross-lingual correspondence in the harvest, these algorithms usually relyon one or more statistical clues (lam et al, 2004), such as the correlation between word frequencies, and cog nates of similar spelling or pronunciations.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M318">
<title id=" I08-4003.xml">mining transliterations from web query results an incremental approach </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the grapheme-based approach, also known as direct ortho graphical mapping (li et al, 2004), <papid> P04-1021 </papid>which treats transliteration as statistical machine translation problem under monotonic constraints, has also achieved promising results.</prevsent>
<prevsent>many efforts have also been channeled to tapping the wealth of the web for harvesting tran slit eration/translation pairs.</prevsent>
</prevsection>
<citsent citstr=" P99-1067 ">
these include studying the query logs (brill et al, 2001), unrelated corpora (rapp, 1999), <papid> P99-1067 </papid>and comparable corpora (sproat et al 2006).<papid> P06-1010 </papid></citsent>
<aftsection>
<nextsent>to establish cross-lingual correspondence in the harvest, these algorithms usually relyon one or more statistical clues (lam et al, 2004), such as the correlation between word frequencies, and cog nates of similar spelling or pronunciations.
</nextsent>
<nextsent>in doing so, two things are needed: first, robust mechanism that establishes statistical relationships between bilingual words, such as phonetic similarity model which is motivated by transliteration modeling research; and second, an effective learning framework that is able to adaptively discover new events from the web.
</nextsent>
<nextsent>in chinese/japanese/korean (cjk) web pages, translated terms are frequently accompanied by their original latin words, with the latin words serving as the appositives of the cjk words.
</nextsent>
<nextsent>in other words, the e-c pairs are always closely collocated.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M319">
<title id=" I08-4003.xml">mining transliterations from web query results an incremental approach </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the grapheme-based approach, also known as direct ortho graphical mapping (li et al, 2004), <papid> P04-1021 </papid>which treats transliteration as statistical machine translation problem under monotonic constraints, has also achieved promising results.</prevsent>
<prevsent>many efforts have also been channeled to tapping the wealth of the web for harvesting tran slit eration/translation pairs.</prevsent>
</prevsection>
<citsent citstr=" P06-1010 ">
these include studying the query logs (brill et al, 2001), unrelated corpora (rapp, 1999), <papid> P99-1067 </papid>and comparable corpora (sproat et al 2006).<papid> P06-1010 </papid></citsent>
<aftsection>
<nextsent>to establish cross-lingual correspondence in the harvest, these algorithms usually relyon one or more statistical clues (lam et al, 2004), such as the correlation between word frequencies, and cog nates of similar spelling or pronunciations.
</nextsent>
<nextsent>in doing so, two things are needed: first, robust mechanism that establishes statistical relationships between bilingual words, such as phonetic similarity model which is motivated by transliteration modeling research; and second, an effective learning framework that is able to adaptively discover new events from the web.
</nextsent>
<nextsent>in chinese/japanese/korean (cjk) web pages, translated terms are frequently accompanied by their original latin words, with the latin words serving as the appositives of the cjk words.
</nextsent>
<nextsent>in other words, the e-c pairs are always closely collocated.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M337">
<title id=" I08-4004.xml">an effective hybrid machine learning approach for coreference resolution </title>
<section> coreference resolution.  </section>
<citcontext>
<prevsection>
<prevsent>formally, active learning studies the closed-loop phenomenon of learner selecting actions or making queries that influence what data are added to its training set.
</prevsent>
<prevsent>when actions or queries are selected properly, the data requirements for some problems decrease drastically (angluin, 1988; baum &amp; lang, 1991).
</prevsent>
</prevsection>
<citsent citstr=" P02-1064 ">
in our system, we used pool-based active learning framework that is similar as manabu sassano (2002) <papid> P02-1064 </papid>used, this is shown in figure 1.</citsent>
<aftsection>
<nextsent>figure 1: our active learning framework in this active learning framework, an initial classifier is trained by crfs [1] that uses only atomic features, and then two human teachers are asked to correct some selected wrong classified examples independently.
</nextsent>
<nextsent>during the process of correction, without any other available information, system only shows the examples that are made up of features to the human teachers; then these two human teachers have to use the information of some atomic features?
</nextsent>
<nextsent>combinations to decide whether two nps refer to the same entity.
</nextsent>
<nextsent>we record all these atomic features?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M338">
<title id=" I08-4014.xml">bupt systems in the sighan bakeoff 2007 </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a multiple knowledge source system and less knowledge conditional random field (crf) based systems are used for ner.
</prevsent>
<prevsent>experiments show that our ws and pos systems are robust.
</prevsent>
</prevsection>
<citsent citstr=" W03-1719 ">
in the last sighan bakeoff, there is no single system consistently outperforms the others on different test standards of chinese ws and ner standards(sproat and emerson, 2003).<papid> W03-1719 </papid></citsent>
<aftsection>
<nextsent>performances of some systems varied significantly on different corpus and different standards, this kind of systems can not satisfy demands in practical applications.
</nextsent>
<nextsent>the robustness, capability of keeping good performances for system by automatically fitting the different corpus and standard, thus become focal problem in ws and ner, it is the same for chinese part-of speech(pos) task which is new in the sighan bakeoff 2007.
</nextsent>
<nextsent>it is worthy to distinguish two kinds of different robustness, one is for different corpus (from different sources or different domain and so on) under same standard, we call it corpus robustness, and another is for different standards (for different application goals or demands and so on) for same corpus.
</nextsent>
<nextsent>we call it standard robustness.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="M339">
<title id=" I08-7009.xml">a discourse resource for turkish annotating discourse connectives in the metu corpus </title>
<section> issues and plans.  </section>
<citcontext>
<prevsection>
<prevsent>the 6th workshop on asian languae resources, 2008 69 second important issue that will have to be tackled in the project is determining how much material is needed to specify the argument of discourse connective.
</prevsent>
<prevsent>annotation will be on text spans, rather than on syntactic structure.
</prevsent>
</prevsection>
<citsent citstr=" W05-0305 ">
this reflects two facts: first, there is only small amount of syntactically tree banked data in the mtc, and secondly, as has been discovered for english, one can not assume that discourse units map directly to syntactic units (dinesh et al  2005).<papid> W05-0305 </papid></citsent>
<aftsection>
<nextsent>preliminary analysis also shows that discourse units may not coincide with clause in its entirety.
</nextsent>
<nextsent>for example, in examples (9) and (16), one can take arg1 to cover only the nominal complement of the matrix verb: the rest of the clause is not necessary to the discourse relation.
</nextsent>
<nextsent>the ways in which the arguments of discourse connective may diverge from syntactic units must be characterized for turkish as is being done for english (dinesh et al  2005).<papid> W05-0305 </papid></nextsent>
<nextsent>a third issue we will investigate is whether different senses of sub ordinator may be identified simply from the type of nominalizing suffix required on the subordinate verb.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
</paper>