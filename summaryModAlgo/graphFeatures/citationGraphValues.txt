ZE30	 W09-1205	[u'a latent variable model of synchronous syntactic semantic parsing for multiple languages ']	P05-1013	[u'\nsyntactic graphs do not use swap action.we adopt the head method of nivre and nilsson (2005) ', <papid> P05-1013 </papid>, u'to de-projectivise syntactic dependencies outside of parsing.1']
ZE120	 W09-0437	[u'a systematic analysis of translation model search spaces ']	N03-1017	[u'\nthe corpus was aligned with giza++ (och and ney,2003) ', <papid> J03-1002 </papid>, u'and symmetrized with the grow-diag-final and heuristic (koehn et al, 2003).', <papid> N03-1017 </papid>]
ZE208	 W09-0414	[u'the talpupc phrase based translation system for eaclwmt 2009 ']	J04-4002	[u'\na bilingual phrase (which in the context of smt do not necessarily coincide with their linguistic analogies) is any pair of source words and target words that satisfies two basic constraints: (1) words are consecutive along both sides of the bilingual phrase and (2) no word on either side of the phrase is aligned to word outside the phrase.given sentence pair and corresponding wordto-word alignment, phrases are extracted following the criterion in (och and ney, 2004).', <papid> J04-4002 </papid>]
ZE351	 W09-0416	[u'matrex the dcu mt system for wmt 2009 ']	P05-1033	[u'\n3) hpb: typical hierarchical phrase-based system (chiang, 2005).', <papid> P05-1033 </papid>, u'meanwhile, we also use word-level combination framework (rosti et al, 2007) ', <papid> N07-1029 </papid>, u'to combine the multiple translation hypotheses and employ new rescoring model to generate the final result.']
ZE462	 W09-1412	[u'tunable domain independent event extraction in the mira framework ']	P05-1012	[u'\nwe used one-best version of mira (cram mer, 2004; mcdonald et al, 2005) ', <papid> P05-1012 </papid>, u'to choose w. mira is an online learning algorithm that updates the weight vector for each training sentence xi according to the following rule: 95 wnew = argmin w ? wold?']
ZE659	 W09-0436	[u'disambiguating de for chinese english machine translation ']	P05-1033	[u'\npart of the solution to dealing with these ordering issues is hierarchical decoding, such as the hiero system (chiang, 2005), ', <papid> P05-1033 </papid>, u'method motivated by {(de) examples like the one in figure 1.']
ZE696	 W09-0436	[u'disambiguating de for chinese english machine translation ']	N03-1017	[u'\nfor our mt experiments, we used re implementation of moses (koehn et al, 2003), ', <papid> N03-1017 </papid>, u'astate-of-the-art phrase-based system.']
ZE827	 W09-1801	[u'summarization with a joint model for sentence extraction and compression ']	W00-0403	[u'\nin the last two decades, machine learning techniques have been employed in extractive summarization of single documents (kupiec et al, 1995; aone et al, 1999; osborne, 2002) ', <papid> W02-0401 </papid>, u'and multiple documents (radev and mckeown, 1998; ', <papid> J98-3005 </papid>, u'carbonell and goldstein, 1998; radev et al, 2000).', <papid> W00-0403 </papid>]
ZE977	 W09-0424	[u'joshua an open source toolkit for parsing based machine translation ']	N03-1017	[u'\nin real translation tasks, the grammars extracted from large training corpora are often far too large to fit into available memory.in such tasks, feature calculation is also very expensive in terms of time required; huge sets of extracted rules must be sorted in two directions for relative frequency calculation of such features as the translation probability p(f |e) and reverse translation probability p(e|f) (koehn et al, 2003).', <papid> N03-1017 </papid>]
ZE1053	 W08-1007	[u'a dependency driven parser for german dependency and constituency representations ']	P05-1013	[u'\nthe training data are projectivized and information about these transformations is encoded into the arc labels to enable deprojectivizition of the parser out put (nivre and nilsson, 2005).', <papid> P05-1013 </papid>]
ZE1116	 W09-1117	[u'improving translation lexicon induction from monolingual corpora via dependency contexts and partofspeech equivalences ']	N03-1017	[u'\nrecent trends in machine translation illustrate that highly accurate word and phrase translations can be learned automatically given enough parallel training data (koehn et al, 2003; ', <papid> N03-1017 </papid>, u'chiang, 2007).', <papid> J07-2003 </papid>]
ZE1287	 W09-1116	[u'glen glenda or glendale unsupervised and semi supervised learning of english noun gender ']	P03-1001	[u'\nwe also made use of the person-name/instance pairs automatically extracted by fleischman et al (2003).', <papid> P03-1001 </papid>, u'4 this data provides counts for pairs such as (zhang qiyue, spokeswoman) and (thorvaldstoltenberg, mediator).']
ZE1362	 W08-2123	[u'dependency based syntacticx2013semantic analysis with propbank and nombank ']	P05-1013	[u'\nto simplify implementation, we instead opted for the pseudo-projective approach (nivre and nilsson, 2005), ', <papid> P05-1013 </papid>, u'in which non projective links are lifted upwards in the tree to achieve projectivity, and special trace labels are used to enable recovery of the non projective links at parse time.']
ZE1384	 W09-0412	[u'nus at wmt09 domain adaptation experiments for english spanish machine translation of news commentary text ']	J04-4002	[u'\nwe then built separate english-to-spanish and spanish-to-english directed word alignments using ibm model 4(brown et al, 1993), ', <papid> J93-2003 </papid>, u'combined them using the in tersect+grow heuristic (och and ney, 2003), ', <papid> J03-1002 </papid>, u'and extracted phrase-level translation pairs of maximum length 7 using the alignment template approach (och and ney, 2004).', <papid> J04-4002 </papid>]
ZE1573	 W08-2004	[u'encoding tree pair based graphs in learning algorithms the textual entailment recognition case ']	W05-1203	[u'\nwhere 1 and 1stand for text and hypothesis, re spectively.several models, ranging from the simple lexical similarity between and to advanced logic form representations, have been proposed (cor ley and mihalcea, 2005; ', <papid> W05-1203 </papid>, u'glickman and dagan,2004; de salvo braz et al, 2005; bos and markert, 2005).', <papid> H05-1079 </papid>]
ZE1574	 W08-2004	[u'encoding tree pair based graphs in learning algorithms the textual entailment recognition case ']	H05-1079	[u'\nwhere 1 and 1stand for text and hypothesis, re spectively.several models, ranging from the simple lexical similarity between and to advanced logic form representations, have been proposed (cor ley and mihalcea, 2005; ', <papid> W05-1203 </papid>, u'glickman and dagan,2004; de salvo braz et al, 2005; bos and markert, 2005).', <papid> H05-1079 </papid>]
ZE1598	 W09-0413	[u'the universitaumlt karlsruhe translation system for the eaclwmt 2009 ']	N03-1017	[u'\nthe compound splitting was done with the frequency-based method described in koehn et al (2003).', <papid> N03-1017 </papid>]
ZE1684	 W09-0429	[u'edinburghs submission to all tracks of the wmt 2009 shared task with reordering and speed improvements to moses ']	P97-1003	[u'\nfor german english, we additionally incorpo ratedrule-based reordering ? we parse the input using the collins parser (collins, 1997) ', <papid> P97-1003 </papid>, u'and apply set of reordering rules to re-arrange the german sentence so that it corresponds more closely english word order (collins et al, 2005).', <papid> P05-1066 </papid>, u'compound splitting ? we split german compound words (mostly nouns), based on the frequency of the words in the potential de compositions (koehn and knight, 2003', <papid> P03-1040 </papid>, u'a).part-of-speech language model ? we use factored translation models (koehn and hoang, 2007) ', <papid> D07-1091 </papid>, u'to also output part-of-speech tags with each word in single phrase mapping and runa second n-gram model over them.']
ZE1748	 W09-0604	[u'is sentence compression an nlg task ']	A00-2024	[u'\nthe task of sentence compression (or sentence re duction) can be defined as summarizing single sentence by removing information from it (jing and mckeown, 2000).', <papid> A00-2024 </papid>]
ZE2018	 W09-1210	[u'efficient parsing of syntactic and semantic dependency structures ']	C96-1058	[u'\n(eisner, 1996; ', <papid> C96-1058 </papid>, u'nivre et al, 2004; ', <papid> W04-2407 </papid>, u'mcdonald and pereira, 2006).', <papid> E06-1011 </papid>]
ZE2023	 W09-1210	[u'efficient parsing of syntactic and semantic dependency structures ']	P05-1013	[u'\nnivre and nilsson (2005) ', <papid> P05-1013 </papid>, u'uses tree rewriting which is the most common technique.']
ZE2026	 W09-1210	[u'efficient parsing of syntactic and semantic dependency structures ']	P05-1012	[u'\nas learning technique, we use margin infused relaxed algorithm (mira) as developed by crammer et al (2003) and applied to dependency parsing by mcdonald et al (2005).', <papid> P05-1012 </papid>]
ZE2203	 W08-2102	[u'tag dynamic programming and the perceptron for efficient feature rich parsing ']	P05-1012	[u'\na derivation in our model is pair e,d? where is set of spines, and is set of dependencies 2note however that the lower-order parser that we use to restrict the search space of the tag-based parser is based on the work of mcdonald et al (2005).', <papid> P05-1012 </papid>]
ZE2219	 W08-2102	[u'tag dynamic programming and the perceptron for efficient feature rich parsing ']	P97-1003	[u'\nin the experiments in this paper, the following three-step process was used: (1) derivations were extracted from training set drawn from the penn wsj treebank, and then used to train parsingmodel; (2) the test data was parsed using there sulting model, giving derivation for each test data sentence; (3) the resulting test-data derivations were mapped back to penn-treebank style trees, using the method described in section 2.1.to achieve step (1), we first apply set of head finding rules which are similar to those described in (collins, 1997).', <papid> P97-1003 </papid>]
ZE2269	 W09-1212	[u'a second order joint eisner model for syntactic and semantic dependency parsing ']	C96-1058	[u'\nthe eisner (1996) ', <papid> C96-1058 </papid>, u'algorithm and its variants are commonly used in data-driven dependency pars ing.']
ZE2279	 W09-1212	[u'a second order joint eisner model for syntactic and semantic dependency parsing ']	P05-1012	[u'\nthese features are borrowed from existing and widely-known systems (xue and palmer, 2004; ', <papid> W04-3212 </papid>, u'mcdonald et al, 2005; ', <papid> P05-1012 </papid>, u'carreras et al, 2006; ', <papid> W06-2925 </papid>, u'surdeanu et al, 2007).']
ZE2334	 W09-0423	[u'smt and spe machine translation systems for wmt09 ']	N03-1017	[u'\nthe goal of statistical machine translation (smt) is to produce target sentence from source sentence . it is today common practice to use phrases as translation units (koehn et al, 2003; ', <papid> N03-1017 </papid>, u'och and ney, 2003) ', <papid> J03-1002 </papid>, u'and loglinear framework in order to introduce several models explaining the translation process: e?']
ZE2350	 W09-0438	[u'a deep learning approach to machine transliteration ']	N04-1033	[u'\nthe following methods were investigated: (monotone) phrase-based mt on character level: state-of-the-art phrase-based smt system (zens and ney, 2004) ', <papid> N04-1033 </papid>, u'was used for name transliteration, i.e. translation of characters instead of words.']
ZE2352	 W09-0434	[u'a quantitative analysis of reordering phenomena ']	J04-4002	[u'\nphrase-based models (och and ney, 2004; ', <papid> J04-4002 </papid>, u'koehn et al, 2003) ', <papid> N03-1017 </papid>, u'have been major paradigm in statistical machine translation in the last few years, showing state-of-the-art performance formany language pairs.']
ZE2353	 W09-0434	[u'a quantitative analysis of reordering phenomena ']	N03-1017	[u'\nphrase-based models (och and ney, 2004; ', <papid> J04-4002 </papid>, u'koehn et al, 2003) ', <papid> N03-1017 </papid>, u'have been major paradigm in statistical machine translation in the last few years, showing state-of-the-art performance formany language pairs.']
ZE2355	 W09-0434	[u'a quantitative analysis of reordering phenomena ']	P05-1033	[u'\nsome grammar-based models such as the hierarchical model (chiang, 2005) ', <papid> P05-1033 </papid>, u'and the syntactified target language phrases model (marcu et al, 2006) have shown better performance than phrase-based models on certain language pairs.to date our understanding of the variation in reordering performance between phrase-based and synchronous grammar models has been limited to relative bleu scores.']
ZE2395	 W08-2127	[u'parsing syntactic and semantic dependencies with two single stage maximum entropy models ']	P05-1013	[u'\nsince only projective sequences can be handled by the shift-reduce scheme, we apply the pseudo projective transformation introduced by (nivre and nilsson, 2005) ', <papid> P05-1013 </papid>, u'to projectivize those non-projectivesequences.']
ZE2490	 W08-2124	[u'a joint model for parsing syntactic and semantic dependencies ']	C96-1058	[u'\nit combines online peceptron learning (collins, 2002) ', <papid> W02-1001 </papid>, u'with parsing model based on the eisner algorithm (eisner, 1996), ', <papid> C96-1058 </papid>, u'extended so asto jointly assign syntactic and semantic la bels.']
ZE2495	 W08-2124	[u'a joint model for parsing syntactic and semantic dependencies ']	P05-1012	[u'\nwe use the features described in mcdonald et al (2005) ', <papid> P05-1012 </papid>, u'and carreras et al(2006) ', <papid> W06-2925 </papid>, u'as input for the syntactic parsing phase, except for the dynamic features from carreras et al (2006).', <papid> W06-2925 </papid>]
ZE2502	 W09-1218	[u'multilingual syntactic semantic dependency parsing with three stage approximate max margin linear models ']	C96-1058	[u'\nto reap the benefits of these advances, weuse higher-order projective dependency parsing algorithm (carreras, 2007) ', <papid> D07-1101 </papid>, u'which is an extension of the span-based parsing algorithm (eisner, 1996), ', <papid> C96-1058 </papid>, u'for syntactic dependency parsing.']
ZE2511	 W09-1218	[u'multilingual syntactic semantic dependency parsing with three stage approximate max margin linear models ']	P05-1013	[u'\nin order to avoid losing the benefits of higher-order parsing, we considered applying pseudo-projective transformation (nivre and nilsson, 2005).', <papid> P05-1013 </papid>]
ZE2683	 W09-1316	[u'incorporating syntactic dependency information towards improved coding of lengthy medical concepts in clinical reports ']	C96-1058	[u'\nsyntactic dependency parsing has received much focus from the natural language processing community (eisner, 1996; ', <papid> C96-1058 </papid>, u'kudo and matsumoto, 2000; ', <papid> W00-1303 </papid>, u'nivre and scholz, 2004; ', <papid> C04-1010 </papid>, u'yamada and matsumoto, 2003).']
ZE2763	 W08-2119	[u'a treetostring phrase based model for statistical machine translation ']	N03-1017	[u'\nthis system uses all features of conventional phrase-based smt as in(koehn et al, 2003).', <papid> N03-1017 </papid>]
ZE2840	 W09-0809	[u'syntactic reordering for english arabic phrase based machine translation ']	N03-1017	[u'\nthe emergence of phrase-based statistical machine translation (psmt) (koehn et al, 2003', <papid> N03-1017 </papid>, u'a)has been one of the major developments in statistical approaches to translation.']
ZE2880	 W09-0809	[u'syntactic reordering for english arabic phrase based machine translation ']	P05-1033	[u'\nif no rule has an opinion at certain point in sentence, the decoder is free to choose the phrase translation it prefers without reordering cost.separating the scoring from the source language reordering also has the advantage that the approach in essence is compatible with other approaches such as traditional psmt system(koehn et al, 2003', <papid> N03-1017 </papid>, u'b) or hierarchical phrase system (chiang, 2005).', <papid> P05-1033 </papid>]
ZE2952	 W08-1501	[u'mitigation of data sparsity in classifier based translation ']	N03-1017	[u'\nto generate the n-best lists, phrase based smt(koehn et al, 2003) ', <papid> N03-1017 </papid>, u'was used.']
ZE2974	 W08-2207	[u'knownet a proposal for building highly connected and dense knowledge bases from the web ']	C00-1072	[u'\ntopic signatures (ts) are word vectors related to particular topic (lin and hovy, 2000).', <papid> C00-1072 </papid>]
ZE3218	 W08-1902	[u'lexical access based on underspecified input ']	C00-1072	[u'\nour next steps will be to take closer look at the following work: clustering of similar words (lin, 1998), ', <papid> P98-2127 </papid>, u'topic signatures (lin and hovy, 2000) ', <papid> C00-1072 </papid>, u'and kilgariffs sketch engine (kilgarriff et al , 2004).we plan also to add other lexical functions to enrich our database with ws . we plan to experiment.']
ZE3255	 W09-1207	[u'multilingual dependency based syntactic and semantic parsing ']	P05-1013	[u'\ntroduced in (nivre and nilsson, 2005) ', <papid> P05-1013 </papid>, u'to handle the non-projective languages including czech, german and english.']
ZE3292	 W09-0430	[u'mining a comparable text corpus for a vietnamese french statistical machine translation system ']	N03-1017	[u'\nthen the two models and search module are used to decode the best translation (brown et al, 1993; ', <papid> J93-2003 </papid>, u'koehn et al, 2003).', <papid> N03-1017 </papid>]
ZE3317	 W09-1104	[u'data driven dependency parsing of new languages using incomplete and noisy training data ']	N03-1017	[u'\nby using only the bidirectional word alignment links, onecan implement very robust such filter, as the bidirectional links are generally reliable, even though they have low recall for overall translational correspondences (koehn et al, 2003).', <papid> N03-1017 </papid>]
ZE3330	 W09-1104	[u'data driven dependency parsing of new languages using incomplete and noisy training data ']	P05-1013	[u'\nnote that the (incorrect) alignment between heeft and you will notbe pursued because it would lead to heeft being dependent of itself and thus violating the well formed 3i.e., single headedness and acyclicity; we do not require the trees to be projective, but instead train pseudo-projective models (nivre and nilsson, 2005) ', <papid> P05-1013 </papid>, u'on the projected data (cf.']
ZE3362	 W08-1911	[u'looking up phrase rephrasings via a pivot language ']	N03-1017	[u'\nvarious approaches for word alignment from parallel corpora have been proposed (see e.g.(och and ney, 2003)), ', <papid> J03-1002 </papid>, u'and the phrase-based approach to statistical machine translation (koehnet al, 2003) ', <papid> N03-1017 </papid>, u'has led to the development of heuristics for obtaining alignments between phrases of any number of words.']
ZE3663	 W08-2131	[u'discriminative vs generative approaches in semantic role labeling ']	P05-1012	[u'\nthe parameters were determined based on the experimental results of the english task in (mcdonald et al, 2005), ', <papid> P05-1012 </papid>, u'i.e. we used projective parsing and first order feature set during training.']
ZE3670	 W09-1312	[u'towards automatic generation of gene summary ']	C00-1072	[u'\nlin and hovy (2000) ', <papid> C00-1072 </papid>, u'first introduced topic signatures which are topic relevant terms for summa rization.']
ZE3685	 W08-1106	[u'extractive vs nlg based abs tractive summarization of evaluative text the effect of corpus controversiality ']	W00-0403	[u'\nthese summarizers have been found to produce quantitatively similar results, and both significantly outperform baseline summarizer, which is the mead summarization framework with all options set to the default (radev et al, 2000).', <papid> W00-0403 </papid>, u'both summarizers relyon information extraction from the corpus.']
ZE3722	 W09-1704	[u'cross lingual predicate cluster acquisition to improve bilingual event extraction by inductive learning ']	N04-1033	[u'\nwe use the rwth aachen chinese-to-english statistical phrase-based machine translation system (zens and ney, 2004) ', <papid> N04-1033 </papid>, u'for these purposes.']
ZE3745	 W08-2219	[u'open knowledge extraction through compositional language processing ']	P97-1003	[u'\nparse each sentence using treebank-trained parser (collins, 1997; ', <papid> P97-1003 </papid>, u'charniak,.']
ZE3809	 W08-1112	[u'accurate and robust lfgbased generation for chinese ']	P97-1003	[u'\n: ? ?(n1)=?(n3)=?(n5)=f1 ?(n2)=?(n4)=f2 ?(n6)=?(n8)=f3 ?(n7)=f4 figure 1: c- and f-structures with ? links for the sentence ?l???ionof grammar transforms (johnson, 1998) ', <papid> J98-4004 </papid>, u'and lexicalisation (collins, 1997)) ', <papid> P97-1003 </papid>, u'has attracted substantial attention, to our knowledge, there has been lot less research on this subject for surface realisation, process that is generally regarded as the reverse process of parsing.']
ZE3902	 W09-1114	[u'monte carlo inference and maximization for phrase based translation ']	N03-1017	[u'\nour technique is based on novel gibbs sampler that draws samples from the posterior distribution of phrase-based translation model (koehn et al, 2003) ', <papid> N03-1017 </papid>, u'but operates in linear time with respect to the number of input words (section 2).']
ZE4038	 W09-0439	[u'stabilizing minimum error rate training ']	N03-1017	[u'\nmost recent approaches in smt, eg (koehn et al , 2003; ', <papid> N03-1017 </papid>, u'chiang, 2005), ', <papid> P05-1033 </papid>, u'use log-linear model to combine probabilistic features.']
ZE4039	 W09-0439	[u'stabilizing minimum error rate training ']	P05-1033	[u'\nmost recent approaches in smt, eg (koehn et al , 2003; ', <papid> N03-1017 </papid>, u'chiang, 2005), ', <papid> P05-1033 </papid>, u'use log-linear model to combine probabilistic features.']
ZE4042	 W09-0439	[u'stabilizing minimum error rate training ']	N04-1033	[u'\nin (zens and ney, 2004) ', <papid> N04-1033 </papid>, u'the downhill simplex method is used to estimate the weights; around 200 iterations are required for convergence to occur.']
ZE4105	 W09-1215	[u'parsing syntactic and semantic dependencies for multiple languages with a pipeline approach ']	C96-1058	[u'\ngraph-based algorithms (eisner, 1996; ', <papid> C96-1058 </papid>, u'mcdonald et al, 2005) ', <papid> P05-1012 </papid>, u'assume series of dependency tree candidates for sentence and the goal is to find the dependency tree with highest score.']
ZE4106	 W09-1215	[u'parsing syntactic and semantic dependencies for multiple languages with a pipeline approach ']	P05-1012	[u'\ngraph-based algorithms (eisner, 1996; ', <papid> C96-1058 </papid>, u'mcdonald et al, 2005) ', <papid> P05-1012 </papid>, u'assume series of dependency tree candidates for sentence and the goal is to find the dependency tree with highest score.']
T1	 P10-1147	[u'discriminative modeling of extraction sets for machine translation ']	N03-1017	[u'\nthis general paradigm was first pursued using contiguous phrases (och et al , 1999;', <papid> W99-0604 </papid>, u'koehn et al , 2003), ', <papid> N03-1017 </papid>, u'and has since been generalized to wide variety of hierarchical and syntacticformalisms.']
T140	 P11-1024	[u'an exponential translation model for target language morphology ']	N03-1017	[u'\nsymmetrization (koehn et al , 2003).', <papid> N03-1017 </papid>]
T526	 P09-4005	[u'a web based interactive computer aided translation tool ']	N03-1017	[u'\nphrase-based statistical machine translation methods acquire their translation knowledge in form of large phrase translation tables automatically from large amounts of translated texts (koehn et al,2003).', <papid> N03-1017 </papid>]
T718	 P10-1049	[u'training phrase translation models with leavingoneout ']	N03-1017	[u'\na phrase-based smt system takes source sentence and produces translation by segmenting the sentence into phrases and translating those phrases separately (koehn et al, 2003).', <papid> N03-1017 </papid>]
T720	 P10-1049	[u'training phrase translation models with leavingoneout ']	J04-4002	[u'\n(och and ney, 2004) ', <papid> J04-4002 </papid>, u'the best translation ei1 as defined by the models then can be written as ei1 = argmax i,ei1 { m?']
T858	 P11-1103	[u'reordering metrics for mt ']	N03-1017	[u'\nwe extracted phrases as in (koehn et al , 2003) ', <papid> N03-1017 </papid>, u'by running giza++ in both directions and merging alignments with the grow-diag-final heuristic.']
T1006	 P10-1058	[u'automatic generation of story highlights ']	A00-1043	[u'\ninterfacing extractive summarization with sentence compression module could improve the conciseness of the generated summaries and render them more informative (jing, 2000; ', <papid> A00-1043 </papid>, u'lin, 2003; ', <papid> W03-1101 </papid>, u'zajic et al , 2007).despite the bulk of work on sentence compression and summarization (see clarke and lapata 2008 and mani 2001 for overviews) only handful of approaches attempt to do both in joint model (daume?']
T1013	 P10-1058	[u'automatic generation of story highlights ']	A00-2024	[u'\njing and mckeown (2000) ', <papid> A00-2024 </papid>, u'first extract sentences, then remove redundant phrases, and use (manual) recombination rules to produce coherent output.']
T1152	 P11-1001	[u'a word class approach to labeling pscfg rules for machine translation ']	P05-1033	[u'\nour models improve translation quality over the single generic label approach of chiang(2005) ', <papid> P05-1033 </papid>, u'and perform on par with the syntactically motivated approach from zollmann and venugopal (2006) ', <papid> W06-3119 </papid>, u'on the nist large chineseto-english translation task.']
T1181	 P11-1001	[u'a word class approach to labeling pscfg rules for machine translation ']	N03-1017	[u'\nis non-negative real-valued weight assigned to the rule; in our model,w is the product of features raised to the power of weight i. chiang (2005) ', <papid> P05-1033 </papid>, u'learns single-nonterminal pscfg from bilingual corpus by first identifying initial phrase pairs using the technique from koehn et al(2003), ', <papid> N03-1017 </papid>, u'and then performing generalization operation to generate phrase pairs with gaps, which can be viewed as pscfg rules with generic x? nonterminal left-hand-sides and substitution sites.']
T1212	 P10-2026	[u'better filtration and augmentation for hierarchical phrase based translation rules ']	P05-1033	[u'\nhierarchical phrase-based (hpb) model (chiang, 2005) ', <papid> P05-1033 </papid>, u'is the state-of-the-art statistical machine translation (smt) model.']
T1228	 P10-2026	[u'better filtration and augmentation for hierarchical phrase based translation rules ']	N03-1017	[u'\n(koehn et al, 2003) ', <papid> N03-1017 </papid>, u'introduced the concept of lexical weighting to check how well words of the phrase translate to each other.']
T1297	 P10-1063	[u'trust rank inducing trust in automatic translations via ranking ']	J04-4002	[u'\nthe mt system used by trust rank (trustrankmt) is statistical phrase-based mt system similar to (och and ney, 2004).', <papid> J04-4002 </papid>]
T1518	 P11-1070	[u'web scale features for full scale parsing ']	P05-1012	[u'\nfor the dependency case,we can integrate them into the dynamic programming of base parser; we use the discriminatively trained mst dependency parser (mcdonald et al, 2005; ', <papid> P05-1012 </papid>, u'mcdonald and pereira, 2006).', <papid> E06-1011 </papid>]
T1777	 P09-2060	[u'toward smaller faster and better hierarchical phrase based smt ']	N03-1017	[u'\nphrase-based translation (koehn et al, 2003) ', <papid> N03-1017 </papid>, u'and hierarchical phrase-based translation (chiang,2005) ', <papid> P05-1033 </papid>, u'are the state of the art in statistical machine translation (smt) techniques.']
T1778	 P09-2060	[u'toward smaller faster and better hierarchical phrase based smt ']	P05-1033	[u'\nphrase-based translation (koehn et al, 2003) ', <papid> N03-1017 </papid>, u'and hierarchical phrase-based translation (chiang,2005) ', <papid> P05-1033 </papid>, u'are the state of the art in statistical machine translation (smt) techniques.']
T1900	 P11-1027	[u'faster and smaller ngram language models ']	J04-4002	[u'\ndecoders with integrated language models (och and ney, 2004; ', <papid> J04-4002 </papid>, u'chiang, 2005) ', <papid> P05-1033 </papid>, u'score partial translation hypotheses in an incremental way.']
T1901	 P11-1027	[u'faster and smaller ngram language models ']	P05-1033	[u'\ndecoders with integrated language models (och and ney, 2004; ', <papid> J04-4002 </papid>, u'chiang, 2005) ', <papid> P05-1033 </papid>, u'score partial translation hypotheses in an incremental way.']
T2309	 P10-1151	[u'a transition based parser for 2planar dependency structures ']	P05-1012	[u'\ndependency-based syntactic parsing has becomea widely used technique in natural language processing, and many different parsing models have been proposed in recent years (yamada and matsumoto, 2003; nivre et al, 2004; ', <papid> W04-2407 </papid>, u'mcdonald et al, 2005', <papid> P05-1012 </papid>, u'a; titov and henderson, 2007; ', <papid> W07-2218 </papid>, u'martins et al, 2009).', <papid> P09-1039 </papid>]
T2328	 P10-1151	[u'a transition based parser for 2planar dependency structures ']	C96-1058	[u'\nlike context-free grammars, projective dependency trees are not sufficient to represent all the linguistic phenomena observed in natural languages, but they have the advantage of being efficiently parsable: their parsing problem can be solved in cubic time with chart parsing techniques (eisner, 1996; ', <papid> C96-1058 </papid>, u'gomez-rodrguez et al, 2008),while in the case of general non-projective dependency forests, it is only tractable under strong independence assumptions (mcdonald et al, 2005', <papid> P05-1012 </papid>, u'b; mcdonald and satta, 2007).', <papid> W07-2216 </papid>]
T2340	 P10-1151	[u'a transition based parser for 2planar dependency structures ']	P05-1013	[u'\nhowever, wehave found this not to be problem when measuring multiplanarity in natural language treebanks, since the effective problem size can be reduced by noting that each connected component of the crossings graph can be treated separately, and that nodes that are not part of cycle need not be considered.5 given that non-projective sentence sin natural language tend to have small proportion of non-projective links (nivre and nilsson,2005), ', <papid> P05-1013 </papid>, u'the connected components of their crossings graphs are very small, and k-colourings for them can quickly be found by brute-force search.']
T2396	 P09-2037	[u'hidden markov tree model in dependency based machine translation ']	N03-1017	[u'\nthis decomposition can be seen asa tree-shaped analogy to the popular n-gram approaches to statistical machine translation (e.g.(koehn et al, 2003)), ', <papid> N03-1017 </papid>, u'in which translation and language models are trainable separately too.']
T2944	 P11-1046	[u'optimal head driven parsing complexity for linear context free rewriting systems ']	P97-1003	[u'\nhead-driven strategies allow for the techniques of lexicalization and markov ization that are widely used in (projective) statistical parsing (collins, 1997).', <papid> P97-1003 </papid>]
T2950	 P11-1021	[u'a large scale distributed syntactic semantic and lexical language model for machine translation ']	P05-1033	[u'\nfinally, we ap 201 ply our language models to the task of re-ranking the n-best list from hiero (chiang, 2005; ', <papid> P05-1033 </papid>, u'chiang, 2007), ', <papid> J07-2003 </papid>, u'state-of-the-art parsing-based mt system, we achieve significantly better translation quality measured by the bleu score and readability?.']
T2967	 P11-1021	[u'a large scale distributed syntactic semantic and lexical language model for machine translation ']	N03-1017	[u'\nit is expected that putting the our composite language into one pass decoder of both phrase-based (koehn et al, 2003) ', <papid> N03-1017 </papid>, u'and parsing-based (chiang, 2005; ', <papid> P05-1033 </papid>, u'chiang, 2007) ', <papid> J07-2003 </papid>, u'mt systems should result in much improved bleu scores.']
T2975	 P11-1003	[u'effective use of function words for rule generalization in forest based translation ']	P05-1033	[u'\nthus, flat phrases (koehn et al,2003), hierarchical phrases (chiang, 2005),', <papid> P05-1033 </papid>, u'and syntactic tree fragments (galley et al, 2006; ', <papid> P06-1121 </papid>, u'mi and huang, 2008; ', <papid> D08-1022 </papid>, u'wu et al, 2010) ', <papid> P10-1034 </papid>, u'are gradually used in smt.']
T3099	 P10-1003	[u'bitext dependency parsing with bilingual subtree constraints ']	N03-1017	[u'\nthe main problem to be addressed is mapping words on the source side to the target subtree because there are many to many mappings and reordering problems that often occur in translation (koehn etal., 2003).', <papid> N03-1017 </papid>]
T3108	 P10-1003	[u'bitext dependency parsing with bilingual subtree constraints ']	C96-1058	[u'\nin this paper, we employ the graph-based mst parsing model proposed by mcdonald and pereira 22(2006), which is an extension of the projective parsing algorithm of eisner (1996).', <papid> C96-1058 </papid>]
T3335	 P10-2035	[u'tree based deterministic dependency parsing x2014 an application to nivrersquos method x2014 ']	C96-1058	[u'\ninstead of selecting parsing action for two words, as in nivres model, our tree-basedmodel first chooses the most probable head candidate from among the trees through tournament and then decides the parsing action between two trees.global-optimization parsing methods are another common approach (eisner, 1996; ', <papid> C96-1058 </papid>, u'mcdonald et al, 2005).', <papid> P05-1012 </papid>]
T3336	 P10-2035	[u'tree based deterministic dependency parsing x2014 an application to nivrersquos method x2014 ']	P05-1012	[u'\ninstead of selecting parsing action for two words, as in nivres model, our tree-basedmodel first chooses the most probable head candidate from among the trees through tournament and then decides the parsing action between two trees.global-optimization parsing methods are another common approach (eisner, 1996; ', <papid> C96-1058 </papid>, u'mcdonald et al, 2005).', <papid> P05-1012 </papid>]
T3380	 P10-2070	[u'wrapping up a summary from representation to generation ']	N03-1017	[u'\nin our framework, reconstruction consists of identifying the original long string.to model our interpretation of the noisy channel, we make use of one of the most popular classes of smt systems: the phrase based model (pbm) (zens et al, 2002; och and ney, 2001;koehn et al, 2003).', <papid> N03-1017 </papid>]
T3395	 P10-2034	[u'learning common grammar from multilingual corpus ']	P05-1033	[u'\nthe proposed framework can be used for probabilistic grammar models other than pcfg.grammar induction using bilingual parallel corpora has been studied mainly in machine translation research (wu, 1997; ', <papid> J97-3002 </papid>, u'melamed, 2003; ', <papid> N03-1021 </papid>, u'eisner,2003; ', <papid> P03-2041 </papid>, u'chiang, 2005; ', <papid> P05-1033 </papid>, u'blunsom et al, 2009; snyder et al, 2009).', <papid> P09-1009 </papid>]
T3427	 P09-2026	[u'leveraging structural relations for fluent compress ions at multiple compression rates ']	P05-1012	[u'\nthe system uses as input the paired corpus, the corresponding pos tagged corpus, the paired corpus parsed using the charniak parser (charniak, 2000), ', <papid> A00-2018 </papid>, u'and dependency parses from the mst parser (mcdonald et al, 2005).', <papid> P05-1012 </papid>]
T3440	 P10-1094	[u'cross language document summarization based on machine translation quality prediction ']	C00-1072	[u'\nfor single document summarization, the sentence score is usually computed by empirical combination of number of statistical and linguistic feature values, such as term frequency, sentence position, cue words, stigma words, topic signature (luhn 1969; lin and hovy, 2000).', <papid> C00-1072 </papid>]
T3644	 P10-1034	[u'fine grained treetostring translation rule extraction ']	P05-1033	[u'\nthe baseline system for comparison is joshua(li et al, 2009), freely available decoder for hierarchical phrase-based smt (chiang, 2005).', <papid> P05-1033 </papid>]
T3701	 P11-1043	[u'model based aligner combination using dual decomposition ']	N03-1017	[u'\ncommon combination methods include the union or intersection of directional alignments, as well as heuristic interpol ations between the union and intersection like grow-diag-final (koehn et al,2003).', <papid> N03-1017 </papid>]
T3739	 P11-1043	[u'model based aligner combination using dual decomposition ']	J04-4002	[u'\nfinally, we evaluated our bidirectional model in alarge-scale end-to-end phrase-based machine translation system from chinese to english, based on the alignment template approach (och and ney, 2004).', <papid> J04-4002 </papid>]
T3749	 P11-1066	[u'phrase based translation model for question retrieval in community question answer archives ']	J04-4002	[u'\nto this end, inspired by the phrase-based statistical machine translation (smt) systems (koehn et al,2003; och and ney, 2004), ', <papid> J04-4002 </papid>, u'we propose phrase based translation model (p-trans) for question retrieval, and we assume that question retrieval should be performed at the phrase level.']
T3750	 P11-1066	[u'phrase based translation model for question retrieval in community question answer archives ']	P05-1033	[u'\nphrase-based machine translation models (koehn et al, 2003; d. chiang, 2005; ', <papid> P05-1033 </papid>, u'och and ney, 2004) ', <papid> J04-4002 </papid>, u'have shown superior performance compared to word-based translation models.']
T3786	 P10-1109	[u'a tree transducer model for synchronous tree adjoining grammars ']	P05-1033	[u'\nto overcome this problem and certain dependency problems, shieber and schabes (1990) ', <papid> C90-3045 </papid>, u'and shieber (2007) ', <papid> W07-0412 </papid>, u'suggest stronger model called synchronous tree-adjoining grammar (stag), which in addition to the substitution operation of stsg (chiang, 2005) ', <papid> P05-1033 </papid>, u'also has an adjoining operation.let us recall the model in some detail.']
T4085	 P10-2033	[u'improving arabictoenglish statistical machine translation by reordering post verbal subjects for alignment ']	N03-1017	[u'\nthese issues are particularly problematic in phrase-based smt (koehn et al, 2003).', <papid> N03-1017 </papid>]
T4143	 P10-2067	[u'active learning based elicitation for semi supervised word alignment ']	N03-1017	[u'\ncorpus-based approaches to machine translation have become predominant, with phrase-based statistical machine translation (pb-smt) (koehn et al., 2003) ', <papid> N03-1017 </papid>, u'being the most actively progressing area.']
T4239	 P10-1076	[u'boosting based system combination for machine translation ']	N03-1017	[u'\nmany smt frameworks have been developed, including phrase-based smt (koehn et al, 2003), ', <papid> N03-1017 </papid>, u'hierarchical phrase-based smt (chiang, 2005), ', <papid> P05-1033 </papid>, u'syntax-based smt (eisner, 2003; ', <papid> P03-2041 </papid>, u'ding and palmer, 2005; ', <papid> P05-1067 </papid>, u'liu et al, 2006; ', <papid> P06-1077 </papid>, u'galley et al, 2006; cowan et al, 2006), ', <papid> W06-1628 </papid>, u'etc. with the emergence of various structurally different smt systems, more and more studies are focused on combining multiple smt systems for achieving higher translation accuracy rather than using single translation system.']
T4240	 P10-1076	[u'boosting based system combination for machine translation ']	P05-1033	[u'\nmany smt frameworks have been developed, including phrase-based smt (koehn et al, 2003), ', <papid> N03-1017 </papid>, u'hierarchical phrase-based smt (chiang, 2005), ', <papid> P05-1033 </papid>, u'syntax-based smt (eisner, 2003; ', <papid> P03-2041 </papid>, u'ding and palmer, 2005; ', <papid> P05-1067 </papid>, u'liu et al, 2006; ', <papid> P06-1077 </papid>, u'galley et al, 2006; cowan et al, 2006), ', <papid> W06-1628 </papid>, u'etc. with the emergence of various structurally different smt systems, more and more studies are focused on combining multiple smt systems for achieving higher translation accuracy rather than using single translation system.']
T4285	 P10-1086	[u'bilingual sense similarity for statistical machine translation ']	P05-1033	[u'\nthe hierarchical phrase-based translation method (chiang, 2005; ', <papid> P05-1033 </papid>, u'chiang, 2007) ', <papid> J07-2003 </papid>, u'is formal syntax based translation modeling method; its translation model is weighted synchronous context free grammar (scfg).']
T4366	 P09-2030	[u'co feedback ranking for query focused summarization ']	C00-1072	[u'\namong them, query relevance, centro id (radev et al, 2004) and signature term (lin and hovy, 2000) ', <papid> C00-1072 </papid>, u'are most remarkable.']
T4372	 P10-1112	[u'simple accurate parsing with an all fragments grammar ']	N03-1017	[u'\nin the domain of syntactic parsing, the idea that all training fragments1 might be relevant to parsing has long history, including tree-substitution grammar (data-oriented parsing) approaches (scha, 1990; bod, 1993; ', <papid> E93-1006 </papid>, u'goodman, 1996', <papid> W96-0214 </papid>, u'a; chiang, 2003) and tree kernel approaches (collins and duffy, 2002).', <papid> P02-1034 </papid>, u'for machine translation, the key modern advancement has been the ability to represent and memorize large training substructures, be it in contiguous phrases (koehn et al, 2003) ', <papid> N03-1017 </papid>, u'or syntactic trees 1in this paper, fragment means an elementary tree in tree-substitution grammar, while subtree means fragment that bottoms out in terminals.']
T4374	 P10-1112	[u'simple accurate parsing with an all fragments grammar ']	P05-1033	[u'\n(galley et al, 2004; ', <papid> N04-1035 </papid>, u'chiang, 2005; ', <papid> P05-1033 </papid>, u'deneefe and knight, 2009).', <papid> D09-1076 </papid>]
T4499	 P11-1082	[u'coreference resolution with world knowledge ']	P03-1001	[u'\nlow-frequency extractions are typically assumed to be noisy and discarded.we combine the extractions produced by fleischman et al  (2003) ', <papid> P03-1001 </papid>, u'and ng (2007) to form database consisting of 1.057 million np pairs, and create binary-valued feature for our coreference models using this database.']
T5301	 P11-1069	[u'shift reduce ccg parsing ']	P05-1012	[u'\nthe comparison reported in this section is similar to the comparison between the chart based mst parser (mcdonald et al, 2005) ', <papid> P05-1012 </papid>, u'and shift reduce malt parser (nivre et al, 2006) ', <papid> W06-2933 </papid>, u'for dependency parsing.']
T5444	 P10-1028	[u'learning phrase based spelling error models from click through data ']	N03-1017	[u'\nto this end, inspired by the phrase-based statistical machine translation (smt) systems (koehn et al, 2003; ', <papid> N03-1017 </papid>, u'och and ney, 2004), ', <papid> J04-4002 </papid>, u'we propose phrase-based error model where we assume that query spelling correction is performed at the phrase level.']
T5445	 P10-1028	[u'learning phrase based spelling error models from click through data ']	J04-4002	[u'\nto this end, inspired by the phrase-based statistical machine translation (smt) systems (koehn et al, 2003; ', <papid> N03-1017 </papid>, u'och and ney, 2004), ', <papid> J04-4002 </papid>, u'we propose phrase-based error model where we assume that query spelling correction is performed at the phrase level.']
T5668	 P11-1105	[u'a joint sequence translation model with integrated reordering ']	J04-4002	[u'\n, wj1) where = 4 (5-gram model) for the standard monolingual model (x = lm ) and = 8 (same as the operation model5) for the prior probability model (x = pr).in order to improve end-to-end accuracy, we introduce new features for our model and shift fromthe generative6 model to the standard log-linear approach (och and ney, 2004) ', <papid> J04-4002 </papid>, u'to tune7 them.']
T5669	 P11-1105	[u'a joint sequence translation model with integrated reordering ']	N03-1017	[u'\nour lexical features are standard (koehn et al, 2003).', <papid> N03-1017 </papid>]
T5728	 P09-3012	[u'creating a gold standard for sentence clustering in multi document summarization ']	W00-0403	[u'\nsentence clustering has therefore often been used as an early step in mds (hatzi vassiloglou et al, 2001; marcu and gerber, 2001;radev et al, 2000).', <papid> W00-0403 </papid>]
T5892	 P10-2003	[u'learning lexicalized reordering models from reordering graphs ']	N03-1017	[u'\nphrase-based translation systems (koehn et al,2003; ', <papid> N03-1017 </papid>, u'och and ney, 2004) ', <papid> J04-4002 </papid>, u'prove to be the stateof-the-art as they have delivered translation performance in recent machine translation evaluations.']
T5893	 P10-2003	[u'learning lexicalized reordering models from reordering graphs ']	J04-4002	[u'\nphrase-based translation systems (koehn et al,2003; ', <papid> N03-1017 </papid>, u'och and ney, 2004) ', <papid> J04-4002 </papid>, u'prove to be the stateof-the-art as they have delivered translation performance in recent machine translation evaluations.']
T6379	 P10-1032	[u'exploring syntactic structural features for subtree alignment using bilingual tree kernels ']	N03-1017	[u'\nhowever, utilizing syntactic translational equivalences alone for machine translation loses the capability of modeling non-syntactic phrases (koehn et al, 2003).', <papid> N03-1017 </papid>]
T6394	 P10-1037	[u'using smaller constituents rather than sentences inactive learning for japanese dependency parsing ']	P05-1012	[u'\nnote 1iwatate et al (2008) ', <papid> C08-1046 </papid>, u'compare their proposed algorithm with various ones that include sassanos, cascaded chunking (kudo and matsumoto, 2002), ', <papid> W02-2016 </papid>, u'and one in (mcdonald et al., 2005).', <papid> P05-1012 </papid>]
T6669	 P10-1080	[u'letterphoneme alignment an exploration ']	N04-1033	[u'\n(zens and ney, 2004)).', <papid> N04-1033 </papid>]
T6926	 P10-1047	[u'syntaxtomorphology mapping in factored phrase based statistical machine translation from english to turkish ']	N03-1017	[u'\nin standard phrase-based framework (koehn et al, 2003).', <papid> N03-1017 </papid>, u'they have reported that, given the typical complexity of turkish words, there was substantial percentage of words whose morphological structure was incorrect: either the morphemes were not applicable for the part-of-speech category of the root word selected, or the morphemes were in the wrong order.']
T7005	 P10-1062	[u'error detection for statistical machine translation using linguistic features ']	N03-1017	[u'\nto obtain machine-generated translation hypotheses for our error detection, we use state-of-the-art phrase-based machine translation system moses (koehn et al, 2003; ', <papid> N03-1017 </papid>, u'koehn et al, 2007).', <papid> P07-2045 </papid>]
T7019	 P11-1044	[u'an algorithm for unsupervised transliteration mining with an application to word alignment ']	N03-1017	[u'\nwe build phrase-based mt system for transliteration using the moses toolkit (koehn et al, 2003).', <papid> N03-1017 </papid>]
T7064	 P11-1008	[u'exact decoding of syntactic translation models through lagrangian relaxation ']	P05-1033	[u'\nthe decoding problem for broad range of these systems (e.g., (chiang, 2005; ', <papid> P05-1033 </papid>, u'marcu et al, 2006; shen et al, 2008)) ', <papid> P08-1066 </papid>, u'corresponds to the intersection of (weighted) hypergraph withan n-gram language model.1 the hypergraph represents large set of possible translations, and is created by applying synchronous grammar to the source language string.']
T7144	 P10-2002	[u'a joint rule selection model for hierarchical phrase based translation ']	N03-1017	[u'\nitcan not only maintain the strength of phrase translation in traditional phrase-based models (koehnet al, 2003; ', <papid> N03-1017 </papid>, u'xiong et al, 2006), ', <papid> P06-1066 </papid>, u'but also characterize the complicated long distance reordering similar to syntactic based statistical machine translation (smt) models (yamada and knight, 2001; ', <papid> P01-1067 </papid>, u'quirk et al, 2005; ', <papid> P05-1034 </papid>, u'galley et al, 2006; ', <papid> P06-1121 </papid>, u'liu et al, 2006; ', <papid> P06-1077 </papid>, u'marcu et al, 2006; ', <papid> W06-1606 </papid>, u'mi et al, 2008; ', <papid> P08-1023 </papid>, u'shen et al., 2008).', <papid> P08-1066 </papid>]
T7153	 P10-2002	[u'a joint rule selection model for hierarchical phrase based translation ']	P05-1033	[u'\nin hierarchical phrase-based smt systems, due to the flexibility of rule matching, huge number of hierarchical rules could be automatically learnt from bilingual training corpus (chiang, 2005).', <papid> P05-1033 </papid>]
T7299	 P11-1083	[u'how to train your multi bottom up tree transducer ']	P05-1033	[u'\nchiang (2005) ', <papid> P05-1033 </papid>, u'and graehl et al (2008) ', <papid> J08-3004 </papid>, u'argue thatstsg have sufficient expressive power for syntax based machine translation, but zhang et al (2008', <papid> P08-1064 </papid>, u'a)show that the additional expressive power of tree sequences helps the translation process.']
T7393	 P10-1110	[u'dynamic programming for linear time incremental parsing ']	P05-1012	[u'\nin terms of search strategy, most parsing algorithms in current use for data-driven parsing can be divided into two broad categories: dynamic programming which includes the dominant cky algorithm, and greedy search which includes most incremental parsing methods such asshift-reduce.1 both have pros and cons: the former performs an exact search (in cubic time) over an exponentially large space, while the latter ismuch faster (in linear-time) and is psycholinguistically motivated (frazier and rayner, 1982), butits greedy nature may suffer from severe searcher rors, as it only explores tiny fraction of the whole space even with beam.can we combine the advantages of both approaches, that is, construct an incremental parser 1mcdonald et al (2005', <papid> P05-1012 </papid>, u'b) is notable exception: the mst algorithm is exact search but not dynamic programming.']
T7417	 P10-1110	[u'dynamic programming for linear time incremental parsing ']	C96-1058	[u'\nas concrete example, figure 4 simulates an edge-factored model (eisner, 1996; ', <papid> C96-1058 </papid>, u'mcdonald etal., 2005', <papid> P05-1012 </papid>, u'a) using shift-reduce with dynamic programming, which is similar to bilexical pcfg parsing using cky (eisner and satta, 1999).', <papid> P99-1059 </papid>]
T7517	 P10-1074	[u'hierarchical joint learning improving joint parsing and named entity recognition with non jointly labeled data ']	P97-1003	[u'\nno discussion ofjoint modeling would be complete without mention of (miller et al, 2000), ', <papid> A00-2030 </papid>, u'who trained collins style generative parser (collins, 1997) ', <papid> P97-1003 </papid>, u'over syntactic structure augmented with the template entity and template relations annotations for the muc-7 shared task.one significant limitation for many joint models is the lack of jointly annotated data.']
T7671	 P10-1016	[u'pseudo word for phrase based machine translation ']	N03-1017	[u'\nthe pipeline of most phrase-based statistical machine translation (pb-smt) systems starts from automatically word aligned parallel corpus generated from word-based models (brown et al, 1993), ', <papid> J93-2003 </papid>, u'proceeds with step of induction of phrase table (koehn et al, 2003) ', <papid> N03-1017 </papid>, u'or synchronous grammar (chiang, 2007) ', <papid> J07-2003 </papid>, u'and with model weights tuning step.']
T7856	 P09-2058	[u'optimizing word alignment combination for phrase table training ']	N03-1017	[u'\nit is fundamental and often necessary step before linguistic knowledge acquisitions, such as training phrase translation table in phrasal machine translation (mt) system (koehn et al, 2003), ', <papid> N03-1017 </papid>, u'or extracting hierarchial phrase rules or synchronized grammars in syntax-based translation framework.most word alignment models distinguish translation direction in deriving word alignment matrix.']
T7968	 P10-1067	[u'comparable entity mining from comparative questions ']	P02-1006	[u'\nbootstrapping methods have been shown to be very effective in previous information extraction research (riloff, 1996; riloff and jones, 1999; ravichandran and hovy, 2002; ', <papid> P02-1006 </papid>, u'mooney and bunescu, 2005; kozareva et al, 2008).', <papid> P08-1119 </papid>]
T7986	 P10-1096	[u'bayesian synchronous tree substitution grammar induction and its application to sentence compression ']	J04-4002	[u'\none approach is to use word alignments (where these can be reliably estimated, as in our testbed application) to align subtrees and extract rules (och and ney, 2004; ', <papid> J04-4002 </papid>, u'galley et al, 2004) but this leaves open the question of finding the right level of generality of the rules ? how deep the rules should be and how much lexicalization they should involve ? necessitating resorting to heuristics such as minimality of rules, and leading to1throughout the paper we will use the word stsg to refer to the tree-to-tree version of the formalism, although the string-to-tree version is also commonly used.']
T7995	 P10-1096	[u'bayesian synchronous tree substitution grammar induction and its application to sentence compression ']	A00-1043	[u'\nsentence compression is the task of summarizing sentence while retaining most of the informational content and remaining grammatical (jing, 2000).', <papid> A00-1043 </papid>, u'in extractive sentence compression, which we focus on in this paper, an order-preserving subset of the words in the sentence are selected to form the summary, that is, we summarize by deleting words (knight and marcu, 2002).']
T8021	 P11-1065	[u'learning hierarchical translation structure with linguistic annotations ']	P05-1033	[u'\nthe hiero system (chiang, 2005)', <papid> P05-1033 </papid>, u'utilised an itg-flavour which focused on hierarchical phrase-pairs to capture context-driven translation and reordering patterns with gaps?, offering competitive performance particularly for language pairs with extensive reordering.']
T8033	 P11-1065	[u'learning hierarchical translation structure with linguistic annotations ']	N03-1017	[u'\ninterestingly, early on (koehn et al,2003) ', <papid> N03-1017 </papid>, u'exemplified the difficulties of integrating linguistic information in translation systems.']
T8084	 P10-1064	[u'bridging smt and tm with translation recommendation ']	N03-1017	[u'\nnote that we remove the exact matches in the tm from our dataset, because exact matches will be reused and not presented to the post-editor in typical tm setting.as for the smt system, we use standard log-linear pb-smt model (och and ney, 2002): ', <papid> P02-1038 </papid>, u'giza++ implementation of ibm word alignment model 4,1 the refinement and phrase extraction heuristics described in (koehn et al., 2003), ', <papid> N03-1017 </papid>, u'minimum-error-rate training (och, 2003), ', <papid> P03-1021 </papid>, u'5-gram language model with kneser-ney smoothing (kneser and ney, 1995) trained with srilm (stolcke, 2002) on the english side of the training data, and moses (koehn et al, 2007) ', <papid> P07-2045 </papid>, u'todecode.']
T8209	 P10-1146	[u'learning to translate with source and target syntax ']	P05-1033	[u'\nthe simplest of these (chiang, 2005)', <papid> P05-1033 </papid>, u'make no use of information from syntactic theories or syntactic annotations, whereas others have successfully incorporated syntactic information on the target side (galley et al, 2004; ', <papid> N04-1035 </papid>, u'galley et al, 2006) ', <papid> P06-1121 </papid>, u'or the source side (liu et al, 2006; ', <papid> P06-1077 </papid>, u'huanget al, 2006).', <papid> W06-3601 </papid>]
T8236	 P10-1146	[u'learning to translate with source and target syntax ']	P97-1003	[u'\nwe parsed the source sides of both parallel texts using the berkeley parser (petrov et al, 2006), ', <papid> P06-1055 </papid>, u'trained on the chinese treebank 6 and arabic treebank parts 13, and the english sides using re implementation of the collins parser (collins, 1997).', <papid> P97-1003 </papid>]
T8242	 P09-2031	[u'reducing smt rule table with monolingual key phrase ']	P05-1033	[u'\nin statistical machine translation (smt) community, the state-of-the-art method is to use rules that contain hierarchical structures to model translation, such as the hierarchical phrase-based model (chiang, 2005).', <papid> P05-1033 </papid>]
T8252	 P09-2031	[u'reducing smt rule table with monolingual key phrase ']	N03-1017	[u'\nmethod (koehnet al, 2003).', <papid> N03-1017 </papid>]
T8338	 P11-1063	[u'incremental syntactic language models for phrase based translation ']	N03-1017	[u'\nneither phrase-based (koehn et al, 2003) ', <papid> N03-1017 </papid>, u'nor hierarchical phrase-based translation (chiang, 2005) ', <papid> P05-1033 </papid>, u'take explicit advantage of the syntactic structure of either source or target language.']
T8340	 P11-1063	[u'incremental syntactic language models for phrase based translation ']	P05-1033	[u'\nneither phrase-based (koehn et al, 2003) ', <papid> N03-1017 </papid>, u'nor hierarchical phrase-based translation (chiang, 2005) ', <papid> P05-1033 </papid>, u'take explicit advantage of the syntactic structure of either source or target language.']
T8550	 P11-1059	[u'semantic representation of negation using focus detection ']	H05-1079	[u'\nsome nlp applications deal indirectly with negation, e.g., machine translation (van munster, 1988), text classification (rose et al , 2003) and recognizing ent ailments (bos and markert, 2005).', <papid> H05-1079 </papid>, u'regarding corpora, the bio scope corpus annotates negation marks and linguistic scopes exclusively on biomedical texts.']
T8658	 P10-2005	[u'diversify and combine improving word alignment for machine translation on low resource languages ']	N03-1017	[u'\nusually people start from the intersection of two sets of alignments, and gradually add links in the union based on certain heuristics, as in (koehn et al, 2003), ', <papid> N03-1017 </papid>, u'to achieve better balance compared to using either intersection (high precision) or union (high recall).in (ayan and dorr, 2006) ', <papid> N06-1013 </papid>, u'maximum entropy approach was proposed to combine multiple alignments based on set of linguistic and alignment features.']
T8680	 P10-1002	[u'dependency parsing and projection based on word pair classification ']	P05-1012	[u'\nsupervised dependency parsing achieves the state of-the-art in recent years (mcdonald et al, 2005', <papid> P05-1012 </papid>, u'a; mcdonald and pereira, 2006; ', <papid> E06-1011 </papid>, u'nivre et al, 2006).', <papid> W06-2933 </papid>, u'since it is costly and difficult to build human annotated treebanks, lot of works have also been devoted to the utilization of unannotated text.']
T8710	 P10-1002	[u'dependency parsing and projection based on word pair classification ']	C96-1058	[u'\nfollow the edge based factor ization method(eisner, 1996), ', <papid> C96-1058 </papid>, u'we factorize the score of dependency tree s(x,y) into its dependency edges, and design dynamic programming algorithm to search for the candidate parse with maximumscore.']
T9069	 P10-4002	[u'cdec a decoder alignment and learning framework for finite state and context free translation models ']	N03-1017	[u'\nphrase-based models (koehn et al, 2003), ', <papid> N03-1017 </papid>, u'lexical translation models (brown et al, 1993), ', <papid> J93-2003 </papid>, u'and finite-state conditional random fields (sha and pereira, 2003) ', <papid> N03-1028 </papid>, u'exemplify the former, and hierarchical phrase-based models the latter (chiang, 2007).', <papid> J07-2003 </papid>]
T9255	 P11-1104	[u'reordering with source language collocations ']	P05-1033	[u'\nin order to further improve the reordering performance, many structure-based methods are proposed, including the reordering model in hierarchical phrase-based smt systems (chiang, 2005) ', <papid> P05-1033 </papid>, u'and syntax-based smt systems (zhang et al, 2007; marton and resnik, 2008; ', <papid> P08-1114 </papid>, u'ge, 2010; ', <papid> N10-1127 </papid>, u'visweswariah et al, 2010).', <papid> C10-1126 </papid>]
T9281	 P11-1104	[u'reordering with source language collocations ']	N03-1017	[u'\nfor phrase-based smt models, koehn et al (2003) ', <papid> N03-1017 </papid>, u'linearly modeled the distance of phrase movements, which results in poor global reordering.']
T9329	 P11-1089	[u'a discriminative model for joint morphological disambiguation and dependency parsing ']	P05-1012	[u'\nparsing is performed using the usual pipeline approach, first with the tree tagger analyzer (schmid, 1994) and then with state-of-the-art dependency parser (mcdon ald et al, 2005).', <papid> P05-1012 </papid>]
T9361	 P11-1042	[u'unsupervised word alignment with arbitrary features ']	W03-0301	[u'\nto choose the regularization strength ? and the initial learning rate 0,3 we trained several models on 10,000-sentence-pair subset of the french english hansa rds, and chose values that minimized the alignment error rate, as evaluated on 447 sentence set of manually created alignments (mihalceaand pedersen, 2003).', <papid> W03-0301 </papid>]
T9372	 P11-1042	[u'unsupervised word alignment with arbitrary features ']	N03-1017	[u'\nwe symmetrize the alignments from both model types using the grow-diag-final-and heuristic (koehn et al, 2003) ', <papid> N03-1017 </papid>, u'producing, in total, six alignment sets.']
T9466	 P10-3010	[u'transition based parsing with confidence weighted classification ']	P05-1012	[u'\nthe two dominating approaches have been graph-based parsing, e.g. mst-parsing (mcdonald et al, 2005', <papid> P05-1012 </papid>, u'b) and transition-based parsing, e.g. the malt parser (nivre et al, 2006', <papid> W06-2933 </papid>, u'a).']
T10014	 P10-2025	[u'word alignment with synonym regularization ']	W03-0301	[u'\nfor an empirical evaluation of the proposed method, we used bilingual parallel corpus of english-french hansa rds (mihalcea and pedersen,2003).', <papid> W03-0301 </papid>]
ZC1	 W06-2937	[u'the exploration of deterministic and efficient dependency parsing ']	P05-1012	[u'\n241over the past decades, many state-of-the-art parsing algorithm were proposed, such as head-word lexicalized pcfg (collins, 1998), maximum entropy (charniak, 2000), ', <papid> A00-2018 </papid>, u'maximum/minimum spanning tree (mst) (mcdonald et al, 2005), ', <papid> P05-1012 </papid>, u'bottom-up deterministic parsing (yamada and matsumoto, 2003), and constant-time deterministic parsing (nivre, 2003).']
ZC5	 W06-2928	[u'dependency parsing with reference to slovene spanish and swedish ']	P05-1012	[u'\nas described in (corston-oliver et al, 2006), we reimplemented the parser described in (mcdonald et al, 2005) ', <papid> P05-1012 </papid>, u'and validated their results for czech and english.']
ZC6	 W06-2928	[u'dependency parsing with reference to slovene spanish and swedish ']	C96-1058	[u'\nat both training and run time, edges are scored independently, and eisners o(n3) decoder (eisner, 1996) ', <papid> C96-1058 </papid>, u'is used to find the optimalparse.']
ZC27	 W07-0702	[u'ccg supertags in factored statistical machine translation ']	P05-1033	[u'\nrecently 1www.nist.gov/speech/tests/mt/mt06eval official results.html there have been few syntax-based models that show performance comparable to the phrase-basedmodels (chiang, 2005; ', <papid> P05-1033 </papid>, u'marcu et al, 2006).', <papid> W06-1606 </papid>]
ZC34	 W07-0702	[u'ccg supertags in factored statistical machine translation ']	N03-1017	[u'\nfactored translation with source words determining target words and ccg supertags for our experiments we used the following fea tures: the translation probabilities pr(sfs1 |t ft 1 ) and pr(tft1 |s fs 1 ), the lexical weights (koehn et al, 2003) ', <papid> N03-1017 </papid>, u'lex(sfs1 |t ft 1 ) and lex(t ft 1 |s fs 1 ), and phrase penalty e, which allows the model to learn preference for longer or shorter phrases.']
ZC49	 W06-1656	[u'boosting unsupervised relation extraction by using ner ']	P02-1006	[u'\n(ravichandran and hovy 2002) ', <papid> P02-1006 </papid>, u'also use bootstrapping, and learn simple surface patterns for extracting binary relations from the web.']
ZC156	 W07-0406	[u'machine translation as tree labeling ']	N03-1017	[u'\nhowever, attempts to retrofit syntactic information into the phrase-based paradigm have not met with enormous success (koehn et al, 2003; ', <papid> N03-1017 </papid>, u'och et al, 2003)1, and purely phrase-based machine translation systems continue to outperform these syntax/phrase-based hybrids.']
ZC157	 W07-0406	[u'machine translation as tree labeling ']	P05-1033	[u'\nas our first step, we will assume that the system will be learning from corpus consisting of triples f, e, a?, where: (i) is sentence from our source language, which is parsed (the words of the sentence and the nodes of the parse tree may or may not be annotated with auxiliary information), (ii) is gold-standard translation of sentence (the wordsof sentence may or may not be annotated with auxiliary information), and (iii) is an automatically generated word alignment (e.g. via giza++) between source sentence and destination sentence e.1(chiang, 2005) ', <papid> P05-1033 </papid>, u'also reports that with his hierarchical generalization of the phrase-based approach, the addition of parser information doesnt lead to any improvements.']
ZC194	 W06-3102	[u'initial explorations in english to turkish statistical machine translation ']	J04-4002	[u'\nlimitations of basic word-based models prompted researchers to exploit morphological and/or syntac tic/phrasal structure (niessen and ney, (2004), ', <papid> J04-2003 </papid>, u'lee,(2004), ', <papid> N04-4015 </papid>, u'yamada and knight (2001), ', <papid> P01-1067 </papid>, u'marcu and wong (2002), ', <papid> W02-1018 </papid>, u'och and ney (2004),', <papid> J04-4002 </papid>, u'koehn et al  (2003), ', <papid> N03-1017 </papid>, u'among others.)in the context of the agglutinative languages similar to turkish (in at least morphological aspects) , there has been some recent work on translating from and to finnish with the significant amount of datain the europarl corpus.']
ZC195	 W06-3102	[u'initial explorations in english to turkish statistical machine translation ']	N03-1017	[u'\nlimitations of basic word-based models prompted researchers to exploit morphological and/or syntac tic/phrasal structure (niessen and ney, (2004), ', <papid> J04-2003 </papid>, u'lee,(2004), ', <papid> N04-4015 </papid>, u'yamada and knight (2001), ', <papid> P01-1067 </papid>, u'marcu and wong (2002), ', <papid> W02-1018 </papid>, u'och and ney (2004),', <papid> J04-4002 </papid>, u'koehn et al  (2003), ', <papid> N03-1017 </papid>, u'among others.)in the context of the agglutinative languages similar to turkish (in at least morphological aspects) , there has been some recent work on translating from and to finnish with the significant amount of datain the europarl corpus.']
ZC198	 W06-3102	[u'initial explorations in english to turkish statistical machine translation ']	P05-1033	[u'\nto find such groups of morphemes and functional words, we applied asequence of morpheme groupings by extracting frequently occuring n-grams of morphemes as follows (much like the grouping used by chiang (2005): ', <papid> P05-1033 </papid>, u'in aseries of iterations, we obtained high-frequency bigrams from the morphemic representation of parallel texts, of either morphemes, or of previously such identified morpheme groups and neighboring morphemes until up to four morphemes or one root 3 morpheme could be combined.']
ZB93	 W04-3227	[u'phrase pair rescoring with term weighting for statistical machine translation ']	N03-1017	[u'\nthe advantage of using phrase-based translation in statistical framework has been shown in many studies such as (koehn et al  2003; ', <papid> N03-1017 </papid>, u'vogel et al  2003; zens et al . 2002; marcu and wong, 2002).', <papid> W02-1018 </papid>]
ZB279	 W04-3225	[u'adaptive language and translation models for interactive machine translation ']	W03-0301	[u'\nthis comes back to the task of word alignment, which is very difficult task for computers (mihalcea and pedersen, 2003).', <papid> W03-0301 </papid>]
ZB291	 W05-0205	[u'towards intelligent search assistance for inquiry based learning ']	P02-1006	[u'\nwhile some qa systems are promising (harabagiu, et al, 2000; ', <papid> C00-1043 </papid>, u'ravichandran and hovy, 2002), ', <papid> P02-1006 </papid>, u'they can only handle factual questions as in trec (voorhees, 2001), and the context for the whole task is largely not considered.']
S279	 P09-1055	[u'a global model for joint lemmatization and partofspeech prediction ']	N04-1033	[u'\nthe coreof our engine is the dynamic programming algorithm for monotone phrasal decoding (zens and ney, 2004).', <papid> N04-1033 </papid>]
S438	 P07-1083	[u'alignment based discriminative string similarity ']	N03-1017	[u'\nexists in machine translation, where phrases are often pairs of word sequences consistent with word-based alignments (koehn et al, 2003).', <papid> N03-1017 </papid>]
S514	 P08-1064	[u'a tree sequence alignment based treetotree translation model ']	N03-1017	[u'\nphrase-based modeling method (koehn et al , 2003; ', <papid> N03-1017 </papid>, u'och and ney, 2004', <papid> J04-4002 </papid>, u'a) is simple, but powerful mechanism to machine translation since it can model local reorderings and translations of multiword expressions well.']
S515	 P08-1064	[u'a tree sequence alignment based treetotree translation model ']	J04-4002	[u'\nphrase-based modeling method (koehn et al , 2003; ', <papid> N03-1017 </papid>, u'och and ney, 2004', <papid> J04-4002 </papid>, u'a) is simple, but powerful mechanism to machine translation since it can model local reorderings and translations of multiword expressions well.']
S518	 P08-1064	[u'a tree sequence alignment based treetotree translation model ']	P05-1033	[u'\nrecently, many syntax-based models have been proposed to address the above deficiencies (wu, 1997; ', <papid> J97-3002 </papid>, u'chiang, 2005; ', <papid> P05-1033 </papid>, u'eisner, 2003; ', <papid> P03-2041 </papid>, u'ding and palmer, 2005; ', <papid> P05-1067 </papid>, u'quirk et al  2005; ', <papid> P05-1034 </papid>, u'cowan et al , 2006; ', <papid> W06-1628 </papid>, u'zhang et al , 2007; bod, 2007; yamada and knight, 2001; ', <papid> P01-1067 </papid>, u'liu et al , 2006; ', <papid> P06-1077 </papid>, u'liu et al , 2007; ', <papid> P07-1089 </papid>, u'gildea, 2003; ', <papid> P03-1011 </papid>, u'poutsma, 2000; ', <papid> C00-2092 </papid>, u'hearne and way, 2003).']
S589	 P09-1009	[u'unsupervised multilingual grammar induction ']	P05-1033	[u'\nmore general formalisms for the same purpose were later developed (wu and wong, 1998; chiang, 2005; ', <papid> P05-1033 </papid>, u'melamed, 2003; ', <papid> N03-1021 </papid>, u'eisner, 2003; ', <papid> P03-2041 </papid>, u'zhang and gildea, 2005; ', <papid> P05-1059 </papid>, u'blunsom et al,2008).']
S633	 P07-1092	[u'machine translation by triangulation making effective use of multi parallel corpora ']	N03-1017	[u'\nstatistical machine translation (brown et al, 1993) ', <papid> J93-2003 </papid>, u'has seen many improvements in recent years, most notably the transition from word- to phrase-basedmodels (koehn et al, 2003).', <papid> N03-1017 </papid>]
S650	 P07-1092	[u'machine translation by triangulation making effective use of multi parallel corpora ']	N04-1033	[u'\nthis represents the translation probability of aphrase when it is decomposed into series of independent word-for-word translation steps (koehn et al., 2003), ', <papid> N03-1017 </papid>, u'and has proven very effective feature (zens and ney, 2004; ', <papid> N04-1033 </papid>, u'foster et al, 2006).', <papid> W06-1607 </papid>]
S697	 P09-1093	[u'a syntax free approach to japanese sentence compression ']	P05-1012	[u'\nfor example, is optimized as follows: w(new) = w(old) ?  w(old) (11) 829 our parameter optimization procedure can be replaced by another one such as mira (mcdon ald et al, 2005) ', <papid> P05-1012 </papid>, u'or crfs (lafferty et al, 2001).']
S709	 P08-1077	[u'large scale acquisition of paraphrases for learning surface patterns ']	P02-1006	[u'\nfor example, in question answering,paraphrases have been used to find multiple patterns that pinpoint the same answer (ravichandranand hovy, 2002); ', <papid> P02-1006 </papid>, u'in statistical machine translation, they have been used to find translations for unseen source language phrases (callison-burch et al., 2006); in multi-document summarization, they have been used to identify phrases from different sentences that express the same information (barzi lay et al, 1999); ', <papid> P99-1071 </papid>, u'in information retrieval they have been used for query expansion (anick and tipirneni, 1999).learning paraphrases requires one to ensure identity of meaning.']
S888	 P07-1037	[u'super tagged phrase based statistical machine translation ']	P05-1033	[u'\nonly quite recently have (chiang, 2005) ', <papid> P05-1033 </papid>, u'and (marcu et al, 2006) ', <papid> W06-1606 </papid>, u'shown that incorporating some form of syntactic structure could show improvements over baseline pbsmt system.']
S944	 P09-1111	[u'an optimal time binarization algorithm for linear context free rewriting systems with fanout two ']	P05-1033	[u'\nrewriting, up to some bounded degree, and have recently been exploited, in some syntactic variant, in syntax-based machine translation (chiang, 2005; ', <papid> P05-1033 </papid>, u'melamed, 2003) ', <papid> N03-1021 </papid>, u'as well as in the modeling of syntax-semantic interface (nesson and shieber, 2006).']
S978	 P09-1042	[u'dependency grammar induction via bitext projection constraints ']	C96-1058	[u'\nrecently, dependency parsing has gained popularity as simpler,computationally more efficient alternative to constituency parsing and has spurred several supervised learning approaches (eisner, 1996; ', <papid> C96-1058 </papid>, u'yamada and matsumoto, 2003a; nivre and nilsson, 2005;', <papid> P05-1013 </papid>, u'mcdonald et al, 2005)', <papid> P05-1012 </papid>, u'as well as unsupervised induction (klein and manning, 2004; ', <papid> P04-1061 </papid>, u'smith and eisner, 2006).', <papid> P06-1072 </papid>]
S979	 P09-1042	[u'dependency grammar induction via bitext projection constraints ']	P05-1013	[u'\nrecently, dependency parsing has gained popularity as simpler,computationally more efficient alternative to constituency parsing and has spurred several supervised learning approaches (eisner, 1996; ', <papid> C96-1058 </papid>, u'yamada and matsumoto, 2003a; nivre and nilsson, 2005;', <papid> P05-1013 </papid>, u'mcdonald et al, 2005)', <papid> P05-1012 </papid>, u'as well as unsupervised induction (klein and manning, 2004; ', <papid> P04-1061 </papid>, u'smith and eisner, 2006).', <papid> P06-1072 </papid>]
S980	 P09-1042	[u'dependency grammar induction via bitext projection constraints ']	P05-1012	[u'\nrecently, dependency parsing has gained popularity as simpler,computationally more efficient alternative to constituency parsing and has spurred several supervised learning approaches (eisner, 1996; ', <papid> C96-1058 </papid>, u'yamada and matsumoto, 2003a; nivre and nilsson, 2005;', <papid> P05-1013 </papid>, u'mcdonald et al, 2005)', <papid> P05-1012 </papid>, u'as well as unsupervised induction (klein and manning, 2004; ', <papid> P04-1061 </papid>, u'smith and eisner, 2006).', <papid> P06-1072 </papid>]
S1076	 P09-1023	[u'summarizing definition from wikipedia ']	C00-1072	[u'\nin addition, infobox could be considered as topic signature (lin and hovy, 2000) ', <papid> C00-1072 </papid>, u'or keywords about the topic.']
S1094	 P09-1021	[u'active learning for multilingual statistical machine translation ']	J04-4002	[u'\n(ueffing et al, 2007; ', <papid> P07-1004 </papid>, u'haffari et al, 2009) show that treating u+as source for new feature function in loglinear model for smt (och and ney, 2004) ', <papid> J04-4002 </papid>, u'allows us to maximally take advantage of unlabeled data by finding weight for this feature using minimum error-rate training (mert) (och, 2003).', <papid> P03-1021 </papid>, u'since each entry in u+ has multiple translations, there are two options when building the auxiliary table for particular language pair (f d, e): (i) to use the corresponding translation ed of the source language in self-training setting, or (ii) touse the consensus translation among all the translation candidates (e1, .., ed) in co-training setting (sharing information between multiple smt models).']
S1120	 P09-1088	[u'a gibbs sampler for phrasal synchronous grammar induction ']	N03-1017	[u'\nthe field of machine translation has seen many advances in recent years, most notably the shift from word-based (brown et al, 1993) ', <papid> J93-2003 </papid>, u'to phrase based models which use token n-grams as translation units (koehn et al, 2003).', <papid> N03-1017 </papid>]
S1270	 P09-1053	[u'paraphrase identification as probabilistic quasi synchronous recognition ']	W05-1203	[u'\nsuch overlap features give the best-published classification accuracy for the paraphrase identification task (zhang and patrick, 2005; finch et al , 2005;', <papid> I05-5003 </papid>, u'wan et al , 2006; corley and mihalcea, 2005, ', <papid> W05-1203 </papid>, u'inter alia), but do not explicitly model correspondence structure (or alignment?)']
S1283	 P09-1053	[u'paraphrase identification as probabilistic quasi synchronous recognition ']	P05-1012	[u'\nt(i)|, |t(i)| | ti)4in our experiments, we use the parser described by mcdonald et al  (2005), ', <papid> P05-1012 </papid>, u'trained on sections 221 of the wsj penn treebank, transformed to dependency trees following yamada and matsumoto (2003).']
S1397	 P08-1040	[u'sentence simplification for semantic role labeling ']	A00-1043	[u'\nas result, the feature is less noisy 3http://www2.parc.com/isl/groups/nltt/xle/ and generalizes better across syntactic variation than feature extracted from the original sentence.another group of related work focuses on summarizing sentences through series of deletions (jing, 2000; ', <papid> A00-1043 </papid>, u'dorr et al, 2003; galley  mckeown, 2007).', <papid> N07-1023 </papid>]
S1407	 P07-1119	[u'substring based transliteration ']	N03-1017	[u'\nas will beshown later, however, not all models can be represented with such state space.the phrase-based approach developed for statistical machine translation (koehn et al, 2003) ', <papid> N03-1017 </papid>, u'is designed to overcome the restrictions on many-to many mappings in word-based translation models.']
S1408	 P07-1119	[u'substring based transliteration ']	N04-1033	[u'\nthe first is an adaptation of the monotone search algorithm outlined in (zensand ney, 2004).', <papid> N04-1033 </papid>, u'the second encodes the substring based transliteration model as transducer.']
S1450	 P08-1007	[u'maxsim a maximum similarity metric for machine translation evaluation ']	P05-1012	[u'\nnote that each figure above represents 6 translation tasks: the europarl and news commentary datasets each with 3 language pairs (german-english, spanish-english, french-english).in our work, we train the mstparser4 (mcdon ald et al, 2005) ', <papid> P05-1012 </papid>, u'on the penn treebank wall street journal (wsj) corpus, and use it to extract dependency relations from sentence.']
S1455	 P07-1121	[u'learning synchronous grammars for semantic parsing with lambda calculus ']	P05-1033	[u'\nboth formalisms have led to smt systems whose performance is state-of-the-art (chiang, 2005; ', <papid> P05-1033 </papid>, u'galley et al, 2006).', <papid> P06-1121 </papid>]
S1522	 P07-2052	[u'minimally lexicalized dependency parsing ']	P05-1012	[u'\ndependency parsing has been actively studied in recent years (yamada and matsumoto, 2003; nivreand scholz, 2004; ', <papid> C04-1010 </papid>, u'isozaki et al, 2004; ', <papid> C04-1040 </papid>, u'mcdonald et al, 2005; ', <papid> P05-1012 </papid>, u'mcdonald and pereira, 2006; ', <papid> E06-1011 </papid>, u'corston-oliver et al, 2006).']
S1576	 P09-1039	[u'concise integer linear programming formulations for dependency parsing ']	C96-1058	[u'\nthere has been extensive work on data-driven dependency parsing for both projective parsing (eis ner, 1996; ', <papid> C96-1058 </papid>, u'paskin, 2001; yamada and matsumoto, 2003; nivre and scholz, 2004; mcdonald et al, 2005', <papid> H05-1066 </papid>, u'a) and non-projective parsing systems (nivreand nilsson, 2005; hall and novak, 2005; mcdonald et al, 2005', <papid> H05-1066 </papid>, u'b).']
S1788	 P09-1024	[u'automatically generating wikipedia articles a structure aware approach ']	W00-0403	[u'\nthis strategy is commonly used in multi-document summarization (barzilay et al, 1999; ', <papid> P99-1071 </papid>, u'goldstein et al, 2000; ', <papid> W00-0405 </papid>, u'radev et al, 2000), ', <papid> W00-0403 </papid>, u'where the combination step eliminates the redundancy across selected ex cerpts.']
S1811	 P08-1078	[u'contextual preferences ']	P02-1006	[u'\nrecently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bi-directional entailment rules) (lin and pantel, 2001; ravichandran and hovy, 2002; ', <papid> P02-1006 </papid>, u'shinyama et al., 2002; szpektor et al, 2004; ', <papid> W04-3206 </papid>, u'sekine, 2005).', <papid> I05-5011 </papid>, u'a common practice is to try matching the structure of h, or of the left-hand-side of rule r, within t. however, context should be considered to allow valid matching.']
S1867	 P09-1087	[u'quadratic time dependency parsing for machine translation ']	P05-1012	[u'\nefficiency is prime concern in syntactic mt decoding, yet significant developments in statistical parsing with respect to asymptotic efficiency havent yet been explored in mt. recently, mcdonald et al (2005', <papid> P05-1012 </papid>, u'b) formalized dependency parsing as maximum spanning tree (mst) problem, which can be solved in quadratic time relative to the length of the sentence.']
S1871	 P09-1087	[u'quadratic time dependency parsing for machine translation ']	P05-1033	[u'\nhierarchical approaches to machine translation have proven increasingly successful in recent years (chiang, 2005; ', <papid> P05-1033 </papid>, u'marcu et al, 2006; ', <papid> W06-1606 </papid>, u'shen et al, 2008), ', <papid> P08-1066 </papid>, u'and often outperform phrase-based systems (och and ney, 2004; ', <papid> J04-4002 </papid>, u'koehn et al, 2003)', <papid> N03-1017 </papid>, u'on target-language fluency and adequacy.']
S1874	 P09-1087	[u'quadratic time dependency parsing for machine translation ']	J04-4002	[u'\nhierarchical approaches to machine translation have proven increasingly successful in recent years (chiang, 2005; ', <papid> P05-1033 </papid>, u'marcu et al, 2006; ', <papid> W06-1606 </papid>, u'shen et al, 2008), ', <papid> P08-1066 </papid>, u'and often outperform phrase-based systems (och and ney, 2004; ', <papid> J04-4002 </papid>, u'koehn et al, 2003)', <papid> N03-1017 </papid>, u'on target-language fluency and adequacy.']
S1875	 P09-1087	[u'quadratic time dependency parsing for machine translation ']	N03-1017	[u'\nhierarchical approaches to machine translation have proven increasingly successful in recent years (chiang, 2005; ', <papid> P05-1033 </papid>, u'marcu et al, 2006; ', <papid> W06-1606 </papid>, u'shen et al, 2008), ', <papid> P08-1066 </papid>, u'and often outperform phrase-based systems (och and ney, 2004; ', <papid> J04-4002 </papid>, u'koehn et al, 2003)', <papid> N03-1017 </papid>, u'on target-language fluency and adequacy.']
S2138	 P08-1110	[u'a deductive approach to dependency parsing ']	C96-1058	[u'\nhowever, many of the most widely used algorithms (eis ner, 1996; ', <papid> C96-1058 </papid>, u'yamada and matsumoto, 2003) do not use formal grammar at all.']
S2149	 P08-1110	[u'a deductive approach to dependency parsing ']	P05-1012	[u'\nthis is the principle behind the parser defined by eisner (1996), ', <papid> C96-1058 </papid>, u'which is still in wide use today (corston-oliver et al, 2006; mcdonald et al, 2005', <papid> P05-1012 </papid>, u'a).']
S2361	 P09-1059	[u'automatic adaptation of annotation standards chinese word segmentation and pos tagging x2013 a case study ']	P05-1012	[u'\nthis is similar to feature design in discriminative dependency parsing (mcdonald et al, 2005; ', <papid> P05-1012 </papid>, u'mc 525donald and pereira, 2006), ', <papid> E06-1011 </papid>, u'where the basic features, composed of words and poss in the context, are also conjoined with link direction and distance in order to obtain more special features.']
S2376	 P08-1114	[u'soft syntactic constraints for hierarchical phrased based translation ']	P05-1033	[u'\n(cowan et al, 2006; ', <papid> W06-1628 </papid>, u'zollmann and venugopal, 2006; ', <papid> W06-3119 </papid>, u'marcu et al, 2006; ', <papid> W06-1606 </papid>, u'galley et al, 2006) ', <papid> P06-1121 </papid>, u'and numerous others).chiang (2005) ', <papid> P05-1033 </papid>, u'distinguishes statistical mt approaches that are syntactic?']
S2389	 P08-1114	[u'soft syntactic constraints for hierarchical phrased based translation ']	N03-1017	[u'\n= log p(e?|f?) , directly analogous to the corresponding feature in non-hierarchical phrase-based models like pharaoh (koehn et al, 2003).', <papid> N03-1017 </papid>]
S2516	 P08-1067	[u'forest reranking discriminative parsing with non local features ']	P05-1012	[u'\nalternatively, discriminative parsing is tractable with exact and efficient search based on dynamic programming (dp) if all features are restricted to be local, that is, only looking at local window with inthe factored search space (taskar et al, 2004; ', <papid> W04-3201 </papid>, u'mcdonald et al, 2005).', <papid> P05-1012 </papid>]
S2635	 P08-2007	[u'the complexity of phrase alignment problems ']	N03-1017	[u'\nin this paper, we assume all phrases have length at least one:   and   k. 25 both the conditional model of denero et al (2006) ', <papid> W06-3105 </papid>, u'and the joint model of marcu and wong(2002) ', <papid> W02-1018 </papid>, u'operate in a, as does the phrase-based decoding framework of koehn et al (2003).', <papid> N03-1017 </papid>]
S2654	 P08-1102	[u'a cascaded linear model for joint chinese word segmentation and partofspeech tagging ']	J04-4002	[u'\nto cope with this problem, we propose cascaded linear model inspired by the log-linear model (och and ney, 2004) ', <papid> J04-4002 </papid>, u'widely used in statistical machine translation to incorporate different kinds of knowledge sources.']
S2803	 P09-1116	[u'phrase clustering for discriminative learning ']	N03-1017	[u'\nthe disambiguation power of phrases is also evidenced by the improvements of phrase-based machine translation systems (koehn et. al., 2003) ', <papid> N03-1017 </papid>, u'over word-based ones.']
S2840	 P08-1068	[u'simple semi supervised dependency parsing ']	P05-1012	[u'\nfor many different part facto riza tions and structure domains y(?), it is possible to solve the above maximization efficiently, and several recent efforts have concentrated on designing new maximization algorithms with increased context sensitivity (eisner, 2000; mcdonald et al, 2005', <papid> P05-1012 </papid>, u'b; mcdonald and pereira, 2006; ', <papid> E06-1011 </papid>, u'carreras, 2007).', <papid> D07-1101 </papid>]
S2910	 P08-1068	[u'simple semi supervised dependency parsing ']	P99-1065	[u'\n9we ensured that the sentences of the penn treebank were excluded from the text used for the clustering.10following collins et al (1999), ', <papid> P99-1065 </papid>, u'we used coarsened version of the czech part of speech tags; this choice also matches the conditions of previous work (mcdonald et al, 2005', <papid> P05-1012 </papid>, u'b; mcdonald and pereira, 2006).', <papid> E06-1011 </papid>]
S2968	 P08-1068	[u'simple semi supervised dependency parsing ']	P05-1013	[u'\nparser accuracy nivre and nilsson (2005) ', <papid> P05-1013 </papid>, u'80.1 mcdonald et al (2005', <papid> P05-1012 </papid>, u'b) 84.4 hall and novak (2005) 85.1 mcdonald and pereira (2006) ', <papid> E06-1011 </papid>, u'85.2 dep1c 86.07 dep2c 87.13 table 5: unlabeled parent-prediction accuracies of czech parsers on the pdt 1.0 test set, for our models and for previous work.']
S3099	 P08-1089	[u'pivot approach for extracting paraphrase patterns from bilingual corpora ']	P02-1006	[u'\nparaphrases are important in plenty of natural language processing (nlp) applications, such as question answering (qa) (lin and pantel, 2001; ravichandran and hovy, 2002), ', <papid> P02-1006 </papid>, u'machine translation (mt) (kauchak and barzilay, 2006; ', <papid> N06-1058 </papid>, u'callison-burch et al, 2006), multi-document summarization (mckeown et al, 2002), and natural language generation (iordanskaja et al, 1991).']
S3123	 P08-1089	[u'pivot approach for extracting paraphrase patterns from bilingual corpora ']	N03-1017	[u'\nlw was originally used to validate the quality of phrase translation pair in mt (koehn et al, 2003).', <papid> N03-1017 </papid>]
S3135	 P08-1108	[u'integrating graph based and transition based dependency parsers ']	C96-1058	[u'\nthis type of model has been used by, among others, eisner (1996), ', <papid> C96-1058 </papid>, u'mcdonald et al (2005', <papid> P05-1012 </papid>, u'a), and nakagawa (2007).', <papid> D07-1100 </papid>]
S3136	 P08-1108	[u'integrating graph based and transition based dependency parsers ']	P05-1012	[u'\nthis type of model has been used by, among others, eisner (1996), ', <papid> C96-1058 </papid>, u'mcdonald et al (2005', <papid> P05-1012 </papid>, u'a), and nakagawa (2007).', <papid> D07-1100 </papid>]
S3217	 P08-1021	[u'correcting misuse of verb forms ']	P97-1003	[u'\nis parsed (collins, 1997) ', <papid> P97-1003 </papid>, u'as: (s (np my father) (vp is (np work)) (pp in the laboratory))2the abbreviations (is or has) and (would or had) compound the ambiguities.']
S3240	 P09-1016	[u'transliteration alignment ']	W03-0301	[u'\nthey indicate how close the alignment under investigation is to the gold standard alignment (mihalcea and pedersen,2003).', <papid> W03-0301 </papid>]
S3504	 P08-1049	[u'unsupervised translation induction for chinese abbreviations using monolingual corpora ']	N03-1017	[u'\nin an extreme case, there are even re-ordering between full-form phrase and its abbreviation.while the research in statistical machine translation (smt) has made significant progress, most smt systems (koehn et al, 2003; ', <papid> N03-1017 </papid>, u'chiang, 2007; galley et al, 2006) ', <papid> P06-1121 </papid>, u'relyon parallel corpora to extract translation entries.']
S3520	 P08-1049	[u'unsupervised translation induction for chinese abbreviations using monolingual corpora ']	J04-4002	[u'\nusing the toolkit moses (koehn et al, 2007), we built phrase-based baseline system by following 429 the standard procedure: running giza++ (och and ney, 2000) ', <papid> P00-1056 </papid>, u'in both directions, applying refinement rules to obtain many-to-many word alignment, and then extracting and scoring phrases using heuristics (och and ney, 2004).', <papid> J04-4002 </papid>]
S3571	 P07-1050	[u'kbest spanning tree parsing ']	P05-1012	[u'\nthe maximum spanning tree algorithm1 was recently introduced as viable solution for non projective dependency parsing (mcdonald et al,2005', <papid> P05-1012 </papid>, u'b).']
S3587	 P07-1050	[u'kbest spanning tree parsing ']	C96-1058	[u'\nthe dp algorithms are generally variants of the cky bottom-up chart parsing algorithm such as that proposed by eisner (1996).', <papid> C96-1058 </papid>]
S3588	 P07-1050	[u'kbest spanning tree parsing ']	P99-1065	[u'\nother dp solutions use constituency based parsers to produce phrase-structure trees, from which dependency structures are extracted (collinset al, 1999).', <papid> P99-1065 </papid>]
S3671	 P08-1012	[u'bayesian learning of non compositional phrases with synchronous parsing ']	J04-4002	[u'\nformally, given word-based itg alignment, the bootstrapping algorithm finds all the phrase pairs according to the definition of och and ney (2004) ', <papid> J04-4002 </papid>, u'and chiang (2005) ', <papid> P05-1033 </papid>, u'with the additional constraint that each phrase pair contains at most one word link.']
S3672	 P08-1012	[u'bayesian learning of non compositional phrases with synchronous parsing ']	P05-1033	[u'\nformally, given word-based itg alignment, the bootstrapping algorithm finds all the phrase pairs according to the definition of och and ney (2004) ', <papid> J04-4002 </papid>, u'and chiang (2005) ', <papid> P05-1033 </papid>, u'with the additional constraint that each phrase pair contains at most one word link.']
S3704	 P07-1090	[u'ordering phrases with function words ']	J04-4002	[u'\nin pursuit of better translation, phrase-based models (och and ney, 2004) ', <papid> J04-4002 </papid>, u'have significantly improved the quality over classical word-based models (brown et al., 1993).', <papid> J93-2003 </papid>]
S3707	 P07-1090	[u'ordering phrases with function words ']	N03-1017	[u'\nthe basic phrase reordering model is simple un lexicalized, context-insensitive distortion penalty model (koehn et al, 2003).', <papid> N03-1017 </papid>]
S3712	 P07-1090	[u'ordering phrases with function words ']	P05-1033	[u'\nproposals to alleviate this problem include utilizing bilingual phrase cluster or words at the phrase boundary (na gata et al, 2006) ', <papid> P06-1090 </papid>, u'as the phrase identity.the benefit of introducing lexical evidence with out being fully lexicalized has been demonstrated by recent state-of-the-art formally syntax-based model1, hiero (chiang, 2005).', <papid> P05-1033 </papid>]
S3729	 P07-1059	[u'statistical machine translation for query expansion in answer retrieval ']	J04-4002	[u'\nwe present two approaches to smt-based query expansion, both of which are implemented in the framework of phrase-based smt (och and ney, 2004; ', <papid> J04-4002 </papid>, u'koehn et al, 2003).', <papid> N03-1017 </papid>, u'our first query expansion model trains an end to-end phrase-based smt model on 10 million question-answer pairs extracted from faq pages.']
S3730	 P07-1059	[u'statistical machine translation for query expansion in answer retrieval ']	N03-1017	[u'\nwe present two approaches to smt-based query expansion, both of which are implemented in the framework of phrase-based smt (och and ney, 2004; ', <papid> J04-4002 </papid>, u'koehn et al, 2003).', <papid> N03-1017 </papid>, u'our first query expansion model trains an end to-end phrase-based smt model on 10 million question-answer pairs extracted from faq pages.']
S3760	 P08-1009	[u'cohesive phrase based decoding for statistical machine translation ']	N03-1017	[u'\nphrase-based decoding (koehn et al, 2003) ', <papid> N03-1017 </papid>, u'is adominant formalism in statistical machine translation.']
S3870	 P07-1076	[u'using corpus statistics on entities to improve semi supervised relation extraction from the web ']	P02-1006	[u'\nravichandran and hovy (ravichandran and hovy 2002) ', <papid> P02-1006 </papid>, u'also use bootstrapping, and learn simple surface patterns for extracting binary relations from the web.']
S4021	 P08-1061	[u'semi supervised convex training for dependency parsing ']	P05-1012	[u'\nsupervised learning algorithms still represent the state of the art approach for inferring dependency parsers from data (mcdonald et al, 2005', <papid> P05-1012 </papid>, u'a; mcdonald and pereira, 2006; ', <papid> E06-1011 </papid>, u'wang et al, 2007).']
S4045	 P08-1061	[u'semi supervised convex training for dependency parsing ']	C96-1058	[u'\nin particular, one would assume that the score of complete spanning tree forgiven sentence, whether probabilistically motivated or not, can be decomposed as sum of local scores for each link (a word pair) (eisner, 1996; ', <papid> C96-1058 </papid>, u'eisner and satta, 1999; ', <papid> P99-1059 </papid>, u'mcdonald et al, 2005', <papid> P05-1012 </papid>, u'a).']
S4399	 P08-1035	[u'a generic sentence trimmer with crfs ']	A00-1043	[u'\nour approach is broadly in line with prior work (jing, 2000; ', <papid> A00-1043 </papid>, u'dorr et al, 2003; ', <papid> W03-0501 </papid>, u'riezler et al, 2003; ', <papid> N03-1026 </papid>, u'clarke and lapata, 2006), ', <papid> P06-2019 </papid>, u'in that we make use ofsome form of syntactic knowledge to constrain com pres sions we generate.']
S4417	 P07-1079	[u'hpsg parsing with shallow dependency constraints ']	C96-1058	[u'\nseveral efficient, accurate and robust approaches to data-driven dependency parsing have been proposed recently (nivre and scholz, 2004; ', <papid> C04-1010 </papid>, u'mcdonald et al,2005; ', <papid> H05-1066 </papid>, u'buchholz and marsi, 2006) ', <papid> W06-2920 </papid>, u'for syntactic analysis of natural language using bilexical dependency relations (eisner, 1996).', <papid> C96-1058 </papid>]
S4561	 P07-3002	[u'inducing combinatory categorial grammars with genetic algorithms ']	P97-1003	[u'\nin particular, hockenmaier and steedman (2001) report generative model for ccg parsing roughly akin to the collins parser (collins, 1997) ', <papid> P97-1003 </papid>, u'specific to ccg.']
S4586	 P09-1113	[u'distant supervision for relation extraction without labeled data ']	P02-1006	[u'\na third approach has been to use very small number of seed instances or patterns to do bootstrap learning (brin, 1998; riloff and jones, 1999; agichtein and gravano, 2000; ravichandran and hovy, 2002; ', <papid> P02-1006 </papid>, u'etzioni et al, 2005; pennacchiotti and pantel, 2006; ', <papid> W06-3909 </papid>, u'bunescu and mooney, 2007; ', <papid> P07-1073 </papid>, u'rozenfeld and feldman, 2008).']
H52	 D10-1091	[u'assessing phrase based translation models with oracle decoding ']	N03-1017	[u'\nin first step, the corpus is aligned at the word level, by using alignment tools such as giza++(och and ney, 2003) ', <papid> J03-1002 </papid>, u'and some symmetrisation heuris tics; phrases are then extracted by other heuristics (koehn et al, 2003) ', <papid> N03-1017 </papid>, u'and assigned numerical weights.']
H194	 D12-1096	[u'parser showdown at the wall street corral an empirical investigation of error types in parser output ']	P97-1003	[u'\nbikel (2004) ', <papid> J04-4004 </papid>, u'implementation of collins (1997).', <papid> P97-1003 </papid>]
H213	 D11-1047	[u'efficient retrieval of tree translation examples for syntax based machine translation ']	N03-1017	[u'\nwe use the term translation rule in very broad sense here, as it may refer to substring pairs as in (koehn et al, 2003), ', <papid> N03-1017 </papid>, u'synchronous grammar rules as in (chiang, 2007) ', <papid> J07-2003 </papid>, u'or treelet pairs as in (quirk et al., 2005; ', <papid> P05-1034 </papid>, u'nakazawa and kurohashi, 2008).']
A89	 C00-1011	[u'parsing with the shortest derivation ']	P97-1003	[u'\n40,000 sentences) and section 23 for testing (see collins 1997, ', <papid> P97-1003 </papid>, u'1999; charniak 1997, 2000; l~,atnalmrkhi 1999); we only tested on sentences _  40 words (2245 sentences).']
A174	 A97-1011	[u'a non projective dependency parser ']	C96-1058	[u'\nthis kind of restriction is present in many dependency-based parsing systems (mccord, 1990; sleator and tem-perley, 1991; eisner, 1996).', <papid> C96-1058 </papid>]
A593	 A00-2005	[u'bagging and boosting a treebank parser ']	P97-1003	[u'\nthe parser induction algorithm used in all of the experiments in this pa-per was distribution of collins model 2 parser (collins, 1997).', <papid> P97-1003 </papid>]
A800	 C00-2118	[u'automatic lexical acquisition based on statistical distributions ']	P97-1003	[u'\nthe last two counts (caus and anim) were per-formed on 29-million word parsed corpus (\\gall street journal 1988, provided by michael collins (collins, 1997)).', <papid> P97-1003 </papid>]
A839	 C00-1081	[u'a stochastic parser based on a structural word prediction model ']	P97-1003	[u'\nalso, in a, sta.te-ofthe-a.rt english pa.rser (collins, 1997) ', <papid> P97-1003 </papid>, u'only the words tha, occur more tha,n times in training data.']
A1071	 A00-2030	[u'a novel use of statistical parsing to extract information from text ']	P97-1003	[u'\nfinally, our newly constructed parser, like that of (collins 1997), ', <papid> P97-1003 </papid>, u'was based on generative statistical model.']
ZD39	 W07-0719	[u'context aware discriminative phrase selection for statistical machine translation ']	N03-1017	[u'\ntranslations tables in phrase-based statistical machine translation (smt) are often built on the basis of maximum-likelihood estimation (mle), being one of the major limitations of this approach that the source sentence context in which phrases occur is completely ignored (koehn et al , 2003).', <papid> N03-1017 </papid>]
ZD66	 W07-0721	[u'analysis of statistical and morphological classes to generate weigthed reordering hypotheses on a statistical machine translation system ']	N03-1017	[u'\nnowadays, statistical machine translation is mainly basedon phrases (koehn et al , 2003).', <papid> N03-1017 </papid>]
ZD132	 W08-0510	[u'design of the moses decoder for statistical machine translation ']	J04-4002	[u'\nearly systems such as the alignment template system (ats) (och and ney 2004) ', <papid> J04-4002 </papid>, u'and pharaoh (koehn 2004) were widely used and accepted by the research community.']
ZD140	 W08-0510	[u'design of the moses decoder for statistical machine translation ']	P05-1033	[u'\nwe note that hiero (chiang 2005) ', <papid> P05-1033 </papid>, u'is written in scripting language with performance critical components rewritten in compiled language.']
ZD162	 W08-0410	[u'inductive detection of language features via clustering minimal pairs toward feature rich grammars in machine translation ']	P05-1033	[u'\nin fact, some structure based systems have already shown that they can out perform phrase-based smt systems (chiang, 2005).', <papid> P05-1033 </papid>]
ZD233	 W08-0315	[u'the talpupc ngrambased statistical machine translation system for aclwmt 2008 ']	J04-4002	[u'\na source-to-target lexicon model and target-tosource lexicon model, these models use word-to word ibm model 1 probabilities (och and ney, 2004) ', <papid> J04-4002 </papid>, u'to estimate the lexical weights for each tuple in the translation table.']
ZD271	 W08-0409	[u'improving word alignment using syntactic dependencies ']	N03-1017	[u'\n73 ment and phrase-extraction heuristics described in (koehn et al, 2003), ', <papid> N03-1017 </papid>, u'minimum-error-rate training(och, 2003), ', <papid> P03-1021 </papid>, u'trigram language model with kneser ney smoothing trained with srilm (stolcke, 2002) on the english side of the training data, and moses (koehn et al, 2007) ', <papid> P07-2045 </papid>, u'to decode.']
ZD319	 W08-0301	[u'an empirical study in source word deletion for phrase based statistical machine translation ']	N03-1017	[u'\na series of experiments were run to compare the performance of the three swd models against the baseline, which is the standard phrase-based approach to smt as elaborated in (koehn et al,2003).', <papid> N03-1017 </papid>]
ZD448	 W08-0304	[u'regularization and search for minimum error rate training ']	N03-1017	[u'\nphrases were extracted using the typical approach described in koehn et al (2003) ', <papid> N03-1017 </papid>, u'of running giza++ (och  ney, 2003) ', <papid> J03-1002 </papid>, u'in both directions and then merging the alignments using the grow-diag-final heuristic.from the merged alignments we also extracted bidirectional lexical reordering model conditioned on the source and the target phrases (tillmann, 2004) (', <papid> N04-4026 </papid>, u'koehn et al, 2007).', <papid> P07-2045 </papid>]
ZD456	 W07-1406	[u'recognizing textual entailment using sentence similarity based on dependency tree skeletons ']	W05-1203	[u'\nconventional methods for rte define measures for the similarity between and either by assuming an independence between words (corley and mihalcea, 2005) ', <papid> W05-1203 </papid>, u'in bow fashion or by exploiting syntactic interpretations.']
ZD488	 W08-0404	[u'generalizing local translation models ']	N03-1017	[u'\nthe use of conditional probabilities in standard lexical models also gives us straightforward way to generalize them in the same way as phrase models.consider the lexical model pw(ry|rx), defined following koehn et al (2003), ', <papid> N03-1017 </papid>, u'with denoting the most frequent word alignment observed for the rule in the training set.']
ZD552	 W08-0628	[u'adaptive information extraction for complex biomedical tasks ']	P02-1006	[u'\na natural way to label sentences is to obtain (by hand or learning) patterns characterizing each field (feng et al, 2006; ravichandran and hovy, 2002).', <papid> P02-1006 </papid>]
ZD856	 W08-0313	[u'first steps towards a general purpose french english statistical machine translation system ']	N03-1017	[u'\nthe goal of statistical machine translation (smt) isto produce target sentence from source sentence . it is today common practice to use phrases as translation units (koehn et al, 2003; ', <papid> N03-1017 </papid>, u'och and ney, 2003) ', <papid> J03-1002 </papid>, u'and loglinear framework in order to introduce several models explaining the translation process: e?']
ZD909	 W07-2220	[u'data driven dependency parsing across languages and domains perspectives from the conll2007 shared task ']	C96-1058	[u'\nthe optimal parse can be found using spanning tree algorithm (eisner, 1996; ', <papid> C96-1058 </papid>, u'mcdonald et al, 2005).', <papid> H05-1066 </papid>]
ZD955	 W07-0735	[u'englishtoczech factored machine translation ']	P05-1033	[u'\nas an alternative option to our verb-modifier experiments, structured language models (chelba and jelinek, 1998) ', <papid> P98-1035 </papid>, u'might be considered to improve clause coherence, until full-featured syntax-based mt models (yamada and knight (2002), ', <papid> P02-1039 </papid>, u'eisner (2003), ', <papid> P03-2041 </papid>, u'chiang (2005) ', <papid> P05-1033 </papid>, u'among many others) are tested when translating to morphologically rich lan guages.']
ZD957	 W07-0909	[u'cross lingual and semantic retrieval for cultural heritage appreciation ']	P02-1006	[u'\nalthough knowledge-bases of entailment-rules and paraphrases learned by acquisition algorithms were used in other nlp applications, such as qa (lin and pantel, 2001; ravichandran and hovy, 2002) ', <papid> P02-1006 </papid>, u'and ie (sudo et al, 2003;', <papid> P03-1029 </papid>, u'romano et al, 2006), ', <papid> E06-1052 </papid>, u'to the best of our knowledge the output of such algorithms was never applied to ir before.']
ZD1065	 W08-0302	[u'rich source side context for statistical machine translation ']	N03-1017	[u'\nphrase-based mt systems are straightforward to train from parallel corpora (koehn et al, 2003) ', <papid> N03-1017 </papid>, u'and, like the original ibm models (brown et al, 1990), ', <papid> J90-2002 </papid>, u'benefit from standard language models built on large monolingual, target-language corpora (brants et al, 2007).', <papid> D07-1090 </papid>]
ZD1089	 W08-0302	[u'rich source side context for statistical machine translation ']	P05-1033	[u'\nmodels, including the lexicalized reordering model and the lexical translation model in the moses mt system, or hierarchical or syntactic models (chiang,2005).', <papid> P05-1033 </papid>]
ZD1239	 W08-0403	[u'prior derivation models for formally syntax based translation using linguistically syntactic parsing and tree kernels ']	J04-4002	[u'\nthere are two major elements accounting for such an improvement: namely the incorporation of phrasal translation structures adopted from widely applied phrase-based models (och and ney, 2004) ', <papid> J04-4002 </papid>, u'to handle local fluency, and the engagement of synchronous context-free grammars (scfg), which enhances the generative capacity of the underlying model that is limited by finite-state machinery.']
ZD1254	 W08-0403	[u'prior derivation models for formally syntax based translation using linguistically syntactic parsing and tree kernels ']	N03-1017	[u'\nlexical weights (koehn et al, 2003) ', <papid> N03-1017 </papid>, u'in both di rections: pw(?|?)']
ZD1261	 W08-0314	[u'the university of washington machine translation system for acl wmt 2008 ']	N03-1017	[u'\nthe system developed for this years shared taskis state-of-the-art, two-pass phrase-based statistical machine translation system based on log-lineartranslation model (koehn et al 2003).', <papid> N03-1017 </papid>]
ZD1660	 W08-0318	[u'towards better machine translation quality for the german english language pairs ']	N03-1017	[u'\nfor all language pairs, we used the moses decoder (koehn et al, 2007), ', <papid> P07-2045 </papid>, u'which follows the phrase-based statistical machine translation approach (koehn et al, 2003), ', <papid> N03-1017 </papid>, u'with default settings as startingpoint.']
ZD1718	 W08-0308	[u'improved treetostring transducer for machine translation ']	J04-4002	[u'\n= argmax pr(t |s) = argmax pr(s)pr(s|t )since the n-gram model tends to favor short translations, penalty is added to the translation templates with fewer rhs symbols than lhs leaf symbols: penalty(t) = exp(|t.rhs| ? |t.lhsleaf |) where |t.rhs| denotes the number of symbols inthe rhs of t, and |t.lhsleaf | denotes the number of leaves in the lhs of t. the length penalty is analogous to the length feature widely used in loglinear models for mt (huang et al, 2006; ', <papid> W06-3601 </papid>, u'liu et al, 2006; ', <papid> P06-1077 </papid>, u'och and ney, 2004).', <papid> J04-4002 </papid>]
ZD1742	 W08-0406	[u'syntactic reordering integrated with phrase based smt ']	N03-1017	[u'\nthe emergence of phrase-based statistical machine translation (psmt) (koehn et al, 2003) ', <papid> N03-1017 </papid>, u'has beenone of the major developments in statistical approaches to translation.']
ZD1780	 W08-0322	[u'kernel regression framework for machine translation ucl system description for wmt 2008 shared translation task ']	N03-1017	[u'\nour system is actually designed as hybrid of the classic phrase-based smt model (koehn et al, 2003) ', <papid> N03-1017 </papid>, u'and the kernel regression model as follows: first, for each source sentence small relevant set of sentence pairs are retrieved from the large-scale parallel corpus.']
ZD1797	 W07-0731	[u'the syntax augmented mt samt system at the shared task for the 2007 acl workshop on statistical machine translation ']	P05-1033	[u'\nas chiang (2005) ', <papid> P05-1033 </papid>, u'and koehn et al (2003) ', <papid> N03-1017 </papid>, u'note, purely lexical phrase-based?']
ZD1800	 W07-0731	[u'the syntax augmented mt samt system at the shared task for the 2007 acl workshop on statistical machine translation ']	N03-1017	[u'\nas chiang (2005) ', <papid> P05-1033 </papid>, u'and koehn et al (2003) ', <papid> N03-1017 </papid>, u'note, purely lexical phrase-based?']
ZD1801	 W07-0731	[u'the syntax augmented mt samt system at the shared task for the 2007 acl workshop on statistical machine translation ']	J04-4002	[u'\nhowever, such models are typically applied over limited source sentence ranges to prevent errors introduced by these models and to maintain efficient decoding (och and ney, 2004).', <papid> J04-4002 </papid>, u'to address these concerns, hierarchically structured models as in chiang (2005) ', <papid> P05-1033 </papid>, u'define weighted transduction rules, interpret able as components ofa probabilistic synchronous grammar (aho and ullman, 1969) that represent translation and reordering operations.']
P104	 N10-1093	[u'improving data driven dependency parsing using clausal information ']	P05-1012	[u'\nall the experiments are done using modified version of mst parser (mcdonald et al, 2005', <papid> P05-1012 </papid>, u'a and the references therein) (henceforth mst) on the icon 2009 parsing contest2 (husain, 2009) data.']
P152	 N12-1035	[u'insertion and deletion models for statistical machine translation ']	P05-1033	[u'\nin hierarchical phrase-based translation (chiang, 2005), ', <papid> P05-1033 </papid>, u'we deal with rules ? ??, ?,?']
P190	 N07-1062	[u'efficient phrase table representation for machine translation with applications to online mt and speech translation ']	N03-1017	[u'\nwe have investigated this and our results are inline with (koehn et al, 2003) ', <papid> N03-1017 </papid>, u'showing that the translation quality does not improve if we utilize phrases beyond certain length.']
P191	 N07-1062	[u'efficient phrase table representation for machine translation with applications to online mt and speech translation ']	N04-1033	[u'\nfor the experiments, we use state-of-the-art phrase-based statistical machine translation system as described in (zens and ney, 2004).', <papid> N04-1033 </papid>]
P194	 N07-1062	[u'efficient phrase table representation for machine translation with applications to online mt and speech translation ']	P05-1033	[u'\nit should be rather straightforward to apply this data structure as well as the phrase-match algorithm to the hierarchical approach of (chiang, 2005).', <papid> P05-1033 </papid>]
P278	 N12-1023	[u'structured ramp loss minimization for machine translation ']	N03-1017	[u'\nthisis done to fit the standard assumptions in mt sys tems: the evaluation metric (e.g., bleu) depends on 1for phrase-based mt, segmentation of the source and target sentences into phrases and an alignment between them (koehn et al , 2003).', <papid> N03-1017 </papid>]
P279	 N12-1023	[u'structured ramp loss minimization for machine translation ']	P05-1033	[u'\nfor hierarchical phrase-based mt, derivation under synchronous cfg (chiang, 2005).', <papid> P05-1033 </papid>]
P379	 N09-1021	[u'phrase based query degradation modeling for vocabulary independent ranked utterance retrieval ']	N03-1017	[u'\nin particular, we adopt the approach of phrase-based statistical machine translation (koehn et al, 2003; ', <papid> N03-1017 </papid>, u'koehn and hoang, 2007).', <papid> D07-1091 </papid>]
P436	 N10-1141	[u'model combination for machine translation ']	J04-4002	[u'\nourphrase-based system is similar to the alignment template system described by och and ney (2004).', <papid> J04-4002 </papid>, u'translation is performed using standard left to-right beam-search decoder.']
P449	 N09-1027	[u'preference grammars softening syntactic constraints to improve statistical machine translation ']	P05-1033	[u'\nour model falls some where between pscfgs that extract nonterminal symbols from parse trees and treat them as part of 236 the derivation (zollmann and venugopal, 2006) ', <papid> W06-3119 </papid>, u'and unlabeled hierarchical structures (chiang, 2005); ', <papid> P05-1033 </papid>, u'we treat nonterminal labels as random variables chose nat each node, with each (unlabeled) rule expressing preferences?']
P504	 N10-1016	[u'learning translation boundaries for phrase based decoding ']	P05-1033	[u'\nit has been known that phrase-based decoding(phrase segmentation/translation/reordering (chi ang, 2005)) ', <papid> P05-1033 </papid>, u'should be constrained to some extent not only for transferring the np-hard problem (knight,1999) ', <papid> J99-4005 </papid>, u'into tractable one in practice but also for improving translation quality.']
P510	 N10-1016	[u'learning translation boundaries for phrase based decoding ']	N03-1017	[u'\n(cherry, 2008) ', <papid> P08-1009 </papid>, u'and (marton and resnik,2008) ', <papid> P08-1114 </papid>, u'introduce syntactic constraints into the standard phrase-based decoding (koehn et al, 2003) ', <papid> N03-1017 </papid>, u'and hierarchical phrase-based decoding (chiang, 2005)', <papid> P05-1033 </papid>, u'respectively by using counting feature which accumulates whenever hypotheses violate syntactic boundaries of source-side parse trees.']
P685	 N09-1063	[u'hierarchical search for parsing ']	P97-1003	[u'\nfor example, the lexicalized grammars of collins (1997) ', <papid> P97-1003 </papid>, u'and charniak (1997) and the state split grammars of petrov et al (2006) are all too large to construct un pruned charts in memory.']
P803	 N09-1026	[u'efficient parsing for transducer grammars ']	P05-1033	[u'\nfor instance, zollmann et al (2006)follow chiang (2005) ', <papid> P05-1033 </papid>, u'in disallowing adjacent non terminals.']
P979	 N12-1054	[u'vine pruning for efficient multi pass dependency parsing ']	P05-1012	[u'\nfor the non-pruning models, we use standard setof features proposed in the discriminative graph based dependency parsing literature (mcdonald et al., 2005; ', <papid> P05-1012 </papid>, u'carreras, 2007; ', <papid> D07-1101 </papid>, u'koo and collins, 2010).', <papid> P10-1001 </papid>]
P1071	 N09-1025	[u'11001 new features for statistical machine translation ']	P05-1033	[u'\nwe also add more than 10,000 features to hiero (chiang, 2005) ', <papid> P05-1033 </papid>, u'and obtain +1.5 b???']
P1102	 N09-1025	[u'11001 new features for statistical machine translation ']	P97-1003	[u'\nthe syntax-based system, we ran re implementation of the collins parser (collins, 1997) ', <papid> P97-1003 </papid>, u'on the english half of the bitext to produce parse trees, then restructured and relabeled them as described in section 3.2.']
P1144	 N10-1133	[u'quantifying the limits and success of extractive summarization systems across domains ']	W03-0510	[u'\nfinally, the study most similar to ours was done by (lin and hovy, 2003), ', <papid> W03-0510 </papid>, u'who used the articles with less than 30 sentences from the duc 2001 dataset to find oracle extracts of 100 and 150 (5) words.']
P1185	 N07-2007	[u'combination of statistical word alignments based on multiple preprocessing schemes ']	N03-1017	[u'\nthe baseline we measure against in all of these experiments is the state-of-the-art grow-diag-final (gdf ) alignment refinement heuristic commonly used inphrase-based smt (koehn et al, 2003).', <papid> N03-1017 </papid>]
P1198	 N07-1061	[u'a comparison of pivot methods for phrase based statistical machine translation ']	N03-1017	[u'\nwe use phrase-based smt system, pharaoh, (koehn et al, 2003; ', <papid> N03-1017 </papid>, u'koehn, 2004), which is based on log-linear formulation (och and ney, 2002).', <papid> P02-1038 </papid>]
P1208	 N07-1061	[u'a comparison of pivot methods for phrase based statistical machine translation ']	J04-4002	[u'\nand a. pw(f? |e?) takes the highest pw(f? |e?,a) if there are multiple alignments a. this discussion, which is partly based on section 4.1.2 of (och and ney, 2004), ', <papid> J04-4002 </papid>, u'means that the lexical translation probability pw(f? |e?) is another probability estimated using the word translation probability w(f |e).']
P1283	 N07-2015	[u'are very large nbest lists useful for smt ']	N03-1017	[u'\nmany research groups use decoder based on log-linear approach incorporating phrases as main paradigm (koehn et al, 2003).', <papid> N03-1017 </papid>]
P1367	 N07-2008	[u'a fast method for parallel document identification ']	N03-1017	[u'\nthey have been employed in word sense disambiguation (diaband resnik, 2002), ', <papid> P02-1033 </papid>, u'automatic construction of bilingual dictionaries (mcewan et al, 2002), and inducing statistical machine translation models (koehn et al., 2003).', <papid> N03-1017 </papid>]
P1786	 N07-2024	[u'detection of nonnative sentences using machine translated training data ']	P97-1003	[u'\nthe trigrams are intended to detect local mistakes.parse parse score from model 2 of the statistical parser (collins, 1997), ', <papid> P97-1003 </papid>, u'normalized by the number of words.']
P1811	 N10-1080	[u'the best lexical metric for phrase based statistical mt system optimization ']	N03-1017	[u'\nphrases were extracted using the grow heuristic (koehn et al, 2003).', <papid> N03-1017 </papid>]
P1871	 N09-1049	[u'hierarchical phrase based translation with weighted finite state transducers ']	P05-1033	[u'\nhierarchical phrase-based translation generates translation hypotheses via the application of hierarchical rules in cyk parsing (chiang, 2005).', <papid> P05-1033 </papid>]
P1908	 N09-1068	[u'hierarchical bayesian domain adaptation ']	C96-1058	[u'\nat the heart ofour model is the eisner dependency grammar chart parsing algorithm (eisner, 1996), ', <papid> C96-1058 </papid>, u'which allows for efficient computation of inside and outside scores.the eisner algorithm, originally designed for generative parsing, decomposes the probability of dependency parse into the probabilities of each attachment of dependent to its parent, and the probabilities of each parent stopping taking dependents.']
P1909	 N09-1068	[u'hierarchical bayesian domain adaptation ']	P05-1012	[u'\nwe used the same features as (mcdonald et al , 2005), ', <papid> P05-1012 </papid>, u'augmented with information about whether or not dependent is the first dependent (information they did not have).']
P1945	 N10-1115	[u'an efficient algorithm for easy first nondirectional dependency parsing ']	P05-1012	[u'\nsupported by the lynn and william frankel center for computer sciences, ben gurion university current dependency parsers can be categorized into three families: local-and-greedy transition based parsers (e.g., malt parser (nivre et al, 2006)), globally optimized graph-based parsers (e.g., mst parser (mcdonald et al, 2005)), ', <papid> P05-1012 </papid>, u'and hybrid systems (e.g., (sagae and lavie, 2006', <papid> P06-2089 </papid>, u'b; nivre and mcdonald, 2008)), ', <papid> P08-1108 </papid>, u'which combine the output of various parsers into new and improved parse, and which are orthogonal to our approach.']
P2310	 N12-1047	[u'batch tuning strategies for statistical machine translation ']	N03-1017	[u'\n432 template max fren enfr zhen tgt unal 50 50 50 31 count bin 11 11 11 11 word pair 6724 1298 1291 1664 length bin 63 63 63 63 total 6848 1422 1415 1769 table 3: sparse feature templates used in big.performed by ibm2 and hmm models and symmetrized using diag-and (koehn et al , 2003).', <papid> N03-1017 </papid>]
P2364	 N10-1090	[u'a simple approach for hpsg super tagging using dependency information ']	P05-1012	[u'\ntwo representative methods for dependency parsing are transition based model like malt parser (nivre, 2003) and graph-based model like mstparser1 (mcdonald etal., 2005).', <papid> P05-1012 </papid>]
P2466	 N10-1040	[u'improving phrase based translation with prototypes of short phrases ']	J04-4002	[u'\nwe can then provide these translations to the decoder, along with their scores, to incorporate them as it builds the complete translation of s. this differs from approaches such as (och and ney, 2004) ', <papid> J04-4002 </papid>, u'because we generate new phrase pairs in isolation, rather than incorporating everything into the sentence-level decoder.']
P2467	 N10-1040	[u'improving phrase based translation with prototypes of short phrases ']	N03-1017	[u'\nwe then find phrases that are consistent with (koehn et al, 2003).', <papid> N03-1017 </papid>]
P2579	 N10-1139	[u'expected sequence similarity maximization ']	J04-4002	[u'\nlattices were generated using phrase-based mtsystem similar to the alignment template system described in (och and ney, 2004).', <papid> J04-4002 </papid>]
P2634	 N12-1052	[u'cross lingual word clusters for direct transfer of linguistic structure ']	P97-1003	[u'\nmore fundamental structures such as part-of-speech tag sequences (ratna parkhi, 1996) ', <papid> W96-0213 </papid>, u'or syntactic parse trees (collins, 1997; ', <papid> P97-1003 </papid>, u'kubler et al, 2009), on the other hand, comprise thecore linguistic analysis for many important down stream tasks such as machine translation (chiang, the majority of this work was performed while the author was an internat google, new york, ny.']
P2919	 N10-1063	[u'extracting parallel sentences from comparable corpora using document level alignment ']	N03-1017	[u'\na standard phrasal smt system (koehn et al, 2003) ', <papid> N03-1017 </papid>, u'serves as our testbed, using conventional set of models: phrasal mod 408 els of source given target and target given source;lexical weighting models in both directions, language model, word count, phrase count, distortion penalty, and lexicalized reordering model.']
P3348	 N10-3010	[u'a data mining approach to learn reorder rules for smt ']	P05-1033	[u'\nin recent years smt systems (brown et al, 1990), (', <papid> J90-2002 </papid>, u'yamada and knight, 2001), (', <papid> P01-1067 </papid>, u'chiang, 2005), (', <papid> P05-1033 </papid>, u'charniak et al, 2003) have been in focus.']
P3432	 N09-1028	[u'using a dependency parser to improve smt for subjectobjectverb languages ']	J04-4002	[u'\nphrase based systems (och, 2002; koehn et.al., 2003;och and ney, 2004) ', <papid> J04-4002 </papid>, u'advanced the machine translation field by allowing translations of word sequences(a.k.a., phrases) instead of single words.']
P4046	 N10-2003	[u'phrasal a statistical machine translation toolkit for exploring new model features ']	N03-1017	[u'\n(liang et al , 2006).', <papid> N06-1014 </papid>, u'2 from the word-to-wordalignments, the system extracts phrase table (koehn et al , 2003) ', <papid> N03-1017 </papid>, u'and hierarchical reordering model (galley and manning, 2008).', <papid> D08-1089 </papid>]
P4354	 N10-1138	[u'probabilistic frame semantic parsing ']	P05-1012	[u'\nwe pre process sentences in our dataset with standard set of annotations: postags from mxpost (ratnaparkhi, 1996) ', <papid> W96-0213 </papid>, u'and dependency parses from the mst parser (mcdonald et al, 2005) ', <papid> P05-1012 </papid>, u'since manual syntactic parses are not available for most of the framenet-annotated documents.']
P4381	 N09-1039	[u'positive results for parsing with a bounded stack using a model based right corner transform ']	P97-1003	[u'\nsince this transform takes probabilistic grammar as in put, it can also easily accommodate horizontal and vertical markov isation (annotating grammar symbols with parent and sibling categories) as described by collins (1997) ', <papid> P97-1003 </papid>, u'and subsequently.the remainder of this paper is organized as follows: section 2 describes related approaches to parsing with stack bounds; section 3 describes an existing bounded-stack parsing framework using right corner transform defined over individual trees; section 4 describes redefinition of this transform to ap 344 ply to entire probabilistic grammars, cast as infinite sets of genera ble trees; and section 5 describes an evaluation of this transform on the wall street journal corpus of the penn treebank showing improved results for transformed bounded-stack version of probabilistic grammar over the original unbounded grammar.']
P4389	 N10-1145	[u'tree edit models for recognizing textual ent ailments paraphrases and answers to questions ']	C96-1058	[u'\nthe child nodes forgiven parent are represented in head-outward fashion such that the left and right children are separate lists, with the left- and right-most elements as the last members oftheir respective lists, as in most generative dependency models (eisner, 1996).', <papid> C96-1058 </papid>]
P4482	 N12-1046	[u'encouraging consistent translation choices ']	P05-1033	[u'\nmoreover, throughout this paper we use the hierarchical phrase-based translation system(hiero), which is based on synchronous context free grammar (scfg) model (chiang, 2005).', <papid> P05-1033 </papid>]
P4516	 N09-1046	[u'using a maximum entropy model to build segmentation lattices for mt ']	N03-1017	[u'\nword alignment was carried out by running giza++ implementation of ibm model 4 initial ized with 5 iterations of model 1, 5 of the hmm aligner, and 3 iterations of model 4 (och and ney, 2003) ', <papid> J03-1002 </papid>, u'in both directions and then symmetrizing using the grow-diag-final-and heuristic (koehn et al, 2003).', <papid> N03-1017 </papid>]
P4572	 N10-1131	[u'an extractive supervised two stage method for sentence compression ']	A00-1043	[u'\nsentence compression is the task of producing shorter form of single given sentence, so that thenew form is grammatical and retains the most important information of the original one (jing, 2000).', <papid> A00-1043 </papid>, u'sentence compression is valuable in many applications, for example when displaying texts on small screens (corston-oliver, 2001), in subtitle generation (vandeghinste and pan, 2004), ', <papid> W04-1015 </papid>, u'and in text summarization (madnani et al, 2007).']
P4592	 N10-1131	[u'an extractive supervised two stage method for sentence compression ']	C00-1072	[u'\nin summarization, such words are called signature terms and are thought to be descriptive of the input; they can be identified using the log-likelihood ratio ? of each word (lin and hovy, 2000; ', <papid> C00-1072 </papid>, u'gupta et al, 2007).', <papid> P07-2049 </papid>]
P4620	 N10-1091	[u'ensemble models for dependency parsing cheap and good ']	C96-1058	[u'\nto guarantee that the resulting dependency tree is well-formed, most previous work used the dynamic programming algorithm of eisner (1996) ', <papid> C96-1058 </papid>, u'for re parsing (sagae and lavie, 2006; ', <papid> N06-2033 </papid>, u'hallet al, 2007).', <papid> D07-1097 </papid>, u'6 however, it is not clear that this step is necessary.in other words, how many sentences are not well formed if one uses simple word-by-word voting scheme?']
P4870	 N10-1128	[u'context free reordering finite state translation ']	N03-1017	[u'\nin the derivation on the left, memorized phrase pair captures the movement ofthe verb (koehn et al, 2003).', <papid> N03-1017 </papid>]
P4932	 N10-1113	[u'bitextbased resolution of german subject object ambiguities ']	N03-1017	[u'\nfigure 2: disambiguation algorithm we used standard heuristics for improving word alignment (och and ney, 2003; ', <papid> J03-1002 </papid>, u'koehn et al, 2003), ', <papid> N03-1017 </papid>, u'but there were many misalignments of ambiguous german words.']
P5114	 N07-1063	[u'an efficient twopass approach to synchronous cfg driven statistical mt ']	P05-1033	[u'\nsyntax-driven (galley et al, 2006) ', <papid> P06-1121 </papid>, u'and hierarchical translation models (chiang, 2005) ', <papid> P05-1033 </papid>, u'take advantage of probabilistic synchronous context free grammars (pscfgs) to represent structured, lexical reordering constraints during the decoding process.']
P5153	 N07-1063	[u'an efficient twopass approach to synchronous cfg driven statistical mt ']	N03-1017	[u'\n(koehn et al, 2003), ', <papid> N03-1017 </papid>, u'and syntactic parse trees of the target training sentences, generated by the stanford parser (d. klein, 2003) pre-trained on the penn treebank.']
P5215	 N07-2032	[u'subtree mining for relation extraction from wikipedia ']	P02-1006	[u'\nsome other works (brin, 1998; agichtein and gravano, 2000; ravichandran and hovy, 2002) ', <papid> P02-1006 </papid>, u'relyon the abundance of web data to obtain easy patterns and learn such patterns based mostly on lexical information.']
P5276	 N09-1029	[u'learning bilingual linguistic reordering model for statistical machine translation ']	P05-1033	[u'\neach sequences of consecutive phrases, mapping to cells in cky matrix, are then translated through bilingual phrase table and scored as implemented in (koehn et al, 2005; chiang, 2005).', <papid> P05-1033 </papid>]
P5293	 N09-1029	[u'learning bilingual linguistic reordering model for statistical machine translation ']	N03-1017	[u'\nfor example, in phrase-based smt systems (koehn et al, 2003; ', <papid> N03-1017 </papid>, u'koehn, 2004), distortion model is used, in which reordering probabilities depend on relative positions of target side phrases between adjacent blocks.']
P5435	 N07-2035	[u'analysis and system combination of phrase and ngrambased statistical machine translation systems ']	N04-1033	[u'\nthe so-calledphrase-based and -gram-based models are two examples of these approaches (zens and ney, 2004; ', <papid> N04-1033 </papid>, u'marino et al, 2006).']
P5631	 N09-2024	[u'a simple sentence level extraction algorithm for comparable data ']	N03-1017	[u'\ntypically, phrase-based smt system includes feature that scores phrase pairs using lexical weights (koehn et al., 2003) ', <papid> N03-1017 </papid>, u'which are computed for two directions:source to target and target to source.']
P5648	 N07-2053	[u'selective phrase pair extraction for improved statistical machine translation ']	N03-1017	[u'\nphrase translation tables are the heart of phrase based statistical machine translation (smt) systems.they provide pairs of phrases that are used to construct large set of potential translations for each input sentence, along with feature values associated with each phrase pair that are used to select the best translation from this set.1 the most widely used method for building phrase translation tables (koehn et al, 2003) ', <papid> N03-1017 </papid>, u'selects, froma word alignment of parallel bilingual training corpus, all pairs of phrases (up to given length) that are consistent with the alignment.']
P5658	 N07-2053	[u'selective phrase pair extraction for improved statistical machine translation ']	W03-0301	[u'\nin this section, we report experiments conducted with canadian hansa rds data from the 2003 hlt naacl word-alignment workshop (mihalcea and pedersen, 2003).', <papid> W03-0301 </papid>]
P5674	 N10-1140	[u'accurate non hierarchical phrase based translation ']	J04-4002	[u'\nphrase-based machine translation models (och andney, 2004) ', <papid> J04-4002 </papid>, u'advanced the state of the art by extending the basic translation unit from words to phrases.by conditioning translations on more than single word, statistical machine translation (smt) system benefits from the larger context of phrase pair to properly handle multi-word units and local reorderings.']
P5675	 N10-1140	[u'accurate non hierarchical phrase based translation ']	N03-1017	[u'\nexperimentally, it was found that longer phrases yield better mt output (koehn et al,2003).', <papid> N03-1017 </papid>]
P5741	 N12-1036	[u'trans ahead a computer assisted translation and writing tool ']	N03-1017	[u'\nwe obtain phrase pairs through number of steps, namely, leveraging ibm models for bidirectional word alignments, grow-diagonal final heuristics to extract phrasal equivalences (koehn et al, 2003).', <papid> N03-1017 </papid>]
P5789	 N07-2009	[u'generalized graphical abstractions for statistical machine translation ']	N03-1017	[u'\nfor comparison, we use the mt training program,giza++ (och and ney, 2003), ', <papid> J03-1002 </papid>, u'the phrase-base decoder, pharaoh (koehn et al, 2003), ', <papid> N03-1017 </papid>, u'and the word based decoder, rewrite (germann, 2003).', <papid> N03-1010 </papid>]
P5813	 N09-1069	[u'online em for unsupervised models ']	P05-1012	[u'\none sees this clear trend in the supervised nlp literature examples include the perceptron algorithm for tagging (collins, 2002), ', <papid> W02-1001 </papid>, u'mira for dependency parsing(mcdonald et al , 2005), ', <papid> P05-1012 </papid>, u'exponentiated gradient algorithms (collins et al , 2008), stochastic gradient for constituency parsing (finkel et al , 2008), ', <papid> P08-1109 </papid>, u'justto name few.']
P5838	 N09-1038	[u'minimal length linearizations for mildly context sensitive dependency trees ']	P05-1012	[u'\ndependency-tree linearizations in the last few years there has been resurgence of interest in computation on dependency-tree structures for natural language sentences, spurred bywork such as mcdonald et al (2005', <papid> P05-1012 </papid>, u'a,b) showing that working with dependency-tree syntactic representations in which each word in the sentence corresponds to node in the dependency tree (and vice versa) can lead to algorithmic benefits over constituency-structure representations.']
P5922	 N09-2055	[u'statistical post editing of a rule based machine translation system ']	N03-1017	[u'\neach model can represent an important feature for the translation, such as phrase-based, language, or lexical models (koehn et al, 2003).', <papid> N03-1017 </papid>]
P6168	 N10-1044	[u'time efficient creation of an accurate sentence fusion corpus ']	A00-1043	[u'\nfurthermore, since research in the related task of sentence compression has benefited from the availability of training data (jing, 2000; ', <papid> A00-1043 </papid>, u'knight and marcu, 2002; mcdonald, 2006; ', <papid> E06-1038 </papid>, u'cohn and lapata, 2008), ', <papid> C08-1018 </papid>, u'we expect that the creation of this corpus might encourage the development of supervised learning techniques for automated sentence fusion.in this work, we present methodology for creating such corpus using amazons mechanicalturk1, widely used online marketplace for crowd sourced task completion.']
P6260	 N10-1107	[u'an mdl based approach to extracting subword units for graphemetophoneme conversion ']	N03-1017	[u'\nthere is large body of work on phrase extraction starting from word alignments; see koehn et al (2003) ', <papid> N03-1017 </papid>, u'for review.']
P6262	 N10-1107	[u'an mdl based approach to extracting subword units for graphemetophoneme conversion ']	N04-1033	[u'\nwould give the correct pronunciation ae t. therefore, we search for the best pronunciation over all segment ations of the word, adapting the monotone search algorithm proposed by zens and ney (2004) ', <papid> N04-1033 </papid>, u'for phrase-based machine translation.3 4.2 smoothing.']
P6272	 N10-1146	[u'syntactic semantic structures for textual entailment recognition ']	H05-1047	[u'\nearly deep semantic models (e.g., (norvig, 1987)) as well as more recent ones (e.g., (tatu and moldovan, 2005; ', <papid> H05-1047 </papid>, u'bos and markert, 2005; ', <papid> H05-1079 </papid>, u'roth and sammons, 2007)) ', <papid> W07-1418 </papid>, u'rely on specific world knowledge encoded in rules for drawing decisions.']
P6273	 N10-1146	[u'syntactic semantic structures for textual entailment recognition ']	H05-1079	[u'\nearly deep semantic models (e.g., (norvig, 1987)) as well as more recent ones (e.g., (tatu and moldovan, 2005; ', <papid> H05-1047 </papid>, u'bos and markert, 2005; ', <papid> H05-1079 </papid>, u'roth and sammons, 2007)) ', <papid> W07-1418 </papid>, u'rely on specific world knowledge encoded in rules for drawing decisions.']
P6497	 N07-1064	[u'statistical phrase based post editing ']	N03-1017	[u'\nthe advent of statistical machine translation, and most recently phrase-based approaches (pbmt, see marcu and wong (2002), ', <papid> W02-1018 </papid>, u'koehn et al (2003)) ', <papid> N03-1017 </papid>, u'into the commercial arena seems to hold the promise of solution to this problem: because the mt system learns directly from existing translations, it can be automatically customized to new domains andtasks.']
P6521	 N10-1129	[u'improved models of distortion cost for statistical machine translation ']	J04-4002	[u'\n( ei1|f 1 ) directly according to log-linear model (och and ney, 2004), ', <papid> J04-4002 </papid>, u'which gives the decision rule e?']
P6523	 N10-1129	[u'improved models of distortion cost for statistical machine translation ']	N03-1017	[u'\nthis search is made tractable by the use of beams (koehn et al, 2003).', <papid> N03-1017 </papid>]
P6578	 N10-1130	[u'why synchronous tree substitution grammars ']	P05-1033	[u'\nn . note that our distinction between nonterminals and terminals is rather uncommon for stsg [see chiang (2005)], ', <papid> P05-1033 </papid>, u'but improves the generative power.']
P6665	 N09-1013	[u'context dependent alignment models for statistical machine translation ']	N03-1017	[u'\n3.2.2 alignment error rate since mt systems are usually built on the union of the two sets of alignments (koehn et al, 2003), ', <papid> N03-1017 </papid>, u'we consider the union of alignments in the two directions as well as those in each direction.']
P6944	 N09-3011	[u'loss sensitive discriminative training of machine transliteration models ']	P05-1012	[u'\ntherefore, our transliteration system would work well in cases where person names were misspelled or in cases in which single name had many reasonable translations in the foreign language.the training algorithm we propose in this paper is based on the k-best mira algorithm which has been used earlier in structured prediction problems (mcdonald et al , 2005', <papid> P05-1012 </papid>, u'a; mcdonald et al ,2005', <papid> P05-1012 </papid>, u'b).']
P6975	 N10-2002	[u'summarizing textual information about locations in a geo spatial information display system ']	C00-1072	[u'\nit is therefore important to deploy topic recognition (lin and hovy, 2000) ', <papid> C00-1072 </papid>, u'and/or topic clustering (osinski and weiss, 2005) to identify and group relevant pieces of each text into single-topic chunks?.']
P6985	 N10-1014	[u'unsupervised syntactic alignment with inversion transduction grammars ']	P05-1033	[u'\nstill others induce their syntax from the data (chiang, 2005).', <papid> P05-1033 </papid>]
P7063	 N12-1004	[u'fast inference in phrase extraction models with belief propagation ']	N03-1017	[u'\nmodern statistical machine translation (mt) systems most commonly infer their transfer rules from word-level alignments (koehn et al , 2007; ', <papid> P07-2045 </papid>, u'li and khudanpur, 2008; ', <papid> W08-0402 </papid>, u'galley et al , 2004), ', <papid> N04-1035 </papid>, u'typically using deterministic heuristic to convert these to phrase alignments (koehn et al , 2003).', <papid> N03-1017 </papid>]
P7175	 N07-3002	[u'learning structured classifiers for statistical dependency parsing ']	P05-1012	[u'\nover the past decade, there has been tremendous progress on learning parsing models from treebank data (magerman, 1995; ', <papid> P95-1037 </papid>, u'collins, 1999; charniak, 1997; ratnaparkhi, 1999; charniak, 2000; ', <papid> A00-2018 </papid>, u'wang et al, 2005; ', <papid> W05-1516 </papid>, u'mcdonald et al, 2005)', <papid> P05-1012 </papid>]
P7181	 N07-3002	[u'learning structured classifiers for statistical dependency parsing ']	P97-1003	[u'\nmost of the early work in this area was based on postulating generative probability models of language that included parse structures (magerman, 1995; ', <papid> P95-1037 </papid>, u'collins, 1997; ', <papid> P97-1003 </papid>, u'charniak, 1997).']
ZJ70	 W12-4205	[u'using parallel features in parsing of machine translated sentences for correction of grammatical errors ']	P99-1065	[u'\ncoarse morphological tag ? czech two-letter coarse morphological tag, as described in (collins et al, 1999),', <papid> P99-1065 </papid>, u'4 ? lemma ? morphological lemma,?']
ZJ92	 W12-4004	[u'resolving task specification and path inconsistency in taxonomy construction ']	P02-1006	[u'\nmost research conducted in the nlp community focuses on extracting local relations between concept pairs (hearst, 1992; ', <papid> C92-2082 </papid>, u'berland and charniak, 1999; ', <papid> P99-1008 </papid>, u'ravichandran and hovy, 2002; ', <papid> P02-1006 </papid>, u'girju et al, 2003; etzioni et al, 2005; pantel and pennacchiotti, 2006;', <papid> P06-1015 </papid>, u'kozareva et al, 2008).', <papid> P08-1119 </papid>]
ZJ187	 W12-2603	[u'machine translation for multilingual summary content evaluation ']	N03-1017	[u'\nour translation service (turchi et al, 2012) ', <papid> E12-2006 </papid>, u'isbased on the most popular class of statistical machine translation systems (smt): the phrase-based model (koehn et al, 2003).', <papid> N03-1017 </papid>]
ZJ211	 W12-4502	[u'latent structure perceptron with feature induction for unrestricted coreference resolution ']	P05-1012	[u'\nthis approach is similar to previous structure learning mode lings for dependency parsing (mcdonald et al, 2005; ', <papid> P05-1012 </papid>, u'fernandes and milidiu?, 2012).']
ZJ305	 W12-3153	[u'twitter translation using translation based cross lingual retrieval ']	J04-4002	[u'\nwe use the probabilistic translation-based retrieval technique of xu et al (2001) that naturally integrates translation tables for cross-lingual retrieval.the retrieval results are then used as input to standard smt pipeline to train translation models, starting from unsupervised induction of word alignments (och and ney, 2000) ', <papid> P00-1056 </papid>, u'to phrase-extraction (och and ney, 2004) ', <papid> J04-4002 </papid>, u'and phrase-based decoding (koehn et al, 2007).', <papid> P07-2045 </papid>]
ZJ472	 W12-3160	[u'optimization strategies for online large margin learning in machine translation ']	P05-1012	[u'\n(mcdonald et al, 2005).', <papid> P05-1012 </papid>]
ZJ644	 W12-3142	[u'upm system for wmt 2012 ']	N03-1017	[u'\nin order to extract phrases (koehn et al 2003), ', <papid> N03-1017 </papid>, u'the considered alignment was grow-diag-final.']
ZJ957	 W98-1206	[u'the effect of alternative tree representations on tree bank grammars ']	P97-1003	[u'\nin addition, many more sophis-ticated parsing models are elaborations of such pcfg models, so understanding the properties of pcfgs is likely to be useful (charniak, 1997; collins, 1997).', <papid> P97-1003 </papid>]
ZJ1532	 W97-0107	[u'reestimation and best first parsing algorithm for probabilistic dependency grammars ']	C96-1058	[u'\neisner (eisner, 1996) ', <papid> C96-1058 </papid>, u'proposed an o(n 3) parsing algorithm for pdg.']
ZJ1590	 W12-3407	[u'combining rule based and statistical syntactic analyzers ']	P05-1012	[u'\nwe will use two statistical dependency parsers, malt parser (nivre et al, 2007', <papid> D07-1096 </papid>, u'b) and mst (mcdonald et al 2005).', <papid> P05-1012 </papid>]
ZJ1748	 W12-3412	[u'generative constituent parsing and discriminative dependency reranking experiments on english and french ']	P97-1003	[u'\nwe will omit it in there mainder of the description.bigram unlike the previous template, bigram features model the conjunction of the governor and the dependent of dependency relation, like bilexical dependencies in (collins, 1997).', <papid> P97-1003 </papid>]
ZJ1766	 W12-3412	[u'generative constituent parsing and discriminative dependency reranking experiments on english and french ']	P05-1012	[u'\n93.8 this work 91.1 89.8 93.9 table 3: comparison on ptb test set for french, see table 4, we compare our system with the mate parser (bohnet, 2010), ', <papid> C10-1011 </papid>, u'an improvement over the mst parser (mcdonald et al, 2005) ', <papid> P05-1012 </papid>, u'with hash kernels, using the melt part-of-speechtagger (denis and sagot, 2009) and our own lemma tiser.']
ZJ2161	 W12-3128	[u'using syntactic head information in hierarchical phrase based translation ']	P05-1033	[u'\nchiangs hierarchical phrase-based (hpb) translation model utilizes synchronous context free grammar (scfg) for translation derivation (chiang, 2005; ', <papid> P05-1033 </papid>, u'chiang, 2007) ', <papid> J07-2003 </papid>, u'and has been widely adopted in statistical machine translation (smt).']
ZJ2182	 W12-3128	[u'using syntactic head information in hierarchical phrase based translation ']	J04-4002	[u'\nfor rule extraction, we first identify initial phrase pairs on word-aligned sentence pairs by using the same criterion as most phrase-based translation models(och and ney, 2004) ', <papid> J04-4002 </papid>, u'and chiangs hpb model (chi ang, 2005; ', <papid> P05-1033 </papid>, u'chiang, 2007).', <papid> J07-2003 </papid>]
ZJ2207	 W12-3128	[u'using syntactic head information in hierarchical phrase based translation ']	N03-1017	[u'\nrefinement (koehn et al, 2003).', <papid> N03-1017 </papid>]
ZJ2323	 W12-3201	[u'rediscovering acl discoveries through the lens of acl anthology network citing sentences ']	P97-1003	[u'\n1997 collins (1997)', <papid> P97-1003 </papid>, u's parser and its re-implementation and extension by bikel (2002) have by now been applied to variety of lan-.']
ZJ2331	 W12-3201	[u'rediscovering acl discoveries through the lens of acl anthology network citing sentences ']	P05-1033	[u'\n2005 chiang (2005) ', <papid> P05-1033 </papid>, u'introduces hiero, hierarchical phrase-based model for statistical machine translation..']
ZJ2602	 W12-3150	[u'ghkm rule extraction and scope3 parsing in moses ']	P05-1033	[u'\nover the last few years, syntax-based rule extraction has largely developed along two lines, one originating in hierarchical phrase-based translation (chiang,2005; ', <papid> P05-1033 </papid>, u'chiang, 2007) ', <papid> J07-2003 </papid>, u'and the other in ghkm (gal ley et al, 2004; ', <papid> N04-1035 </papid>, u'galley et al, 2006).', <papid> P06-1121 </papid>, u'hierarchical rule extraction generalizes the established phrase-based extraction method to produceformally-syntactic synchronous context-free grammar rules without any requirement for linguistic annotation of the training data.']
ZJ2644	 W12-3152	[u'constructing parallel corpora for six indian languages via crowdsourcing ']	N03-1017	[u'\nfigure 6 (top) contains an example of particularly poor alignment produced by the default alignment heuristic, the grow-diag and method described in koehn et al (2003).', <papid> N03-1017 </papid>, u'as means of testing this, we varied the alignment combination heuristics using five alternatives described in koehn et al (2003) ', <papid> N03-1017 </papid>, u'and available in the symal program distributed with moses (koehn et al., 2007).', <papid> P07-2045 </papid>]
ZJ2882	 W12-3127	[u'using categorial grammar to label translation rules ']	N03-1017	[u'\nhowever, such non-compositional translations are important in translation (fox, 2002), ', <papid> W02-1039 </papid>, u'andthey have been repeatedly shown to improve translation performance (koehn et al , 2003; ', <papid> N03-1017 </papid>, u'deneefe et al ., 2007).', <papid> D07-1079 </papid>]
ZJ2967	 W12-3709	[u'multilingual sentiment analysis using machine translation ']	N03-1017	[u'\nfor more details see (koehn et al , 2003).', <papid> N03-1017 </papid>, u'three different smt systems were used to translate the human annotated sentences: two existing online services such as google translate and bing translator5 and an instance of the open source phrase-based statistical machine translation toolkit moses (koehn et al , 2007).', <papid> P07-2045 </papid>]
ZJ3033	 W12-4402	[u'report of news 2012 machine transliteration shared task ']	N03-1017	[u'\nthis system is based on phrase-based statistical machine transliteration (smt) (finch and sumita, 2008),an approach initially developed for machine translation (koehn et al, 2003), ', <papid> N03-1017 </papid>, u'where the smt systems log-linear model is augmented with set of features specifically suited to the task of transliteration.']
ZJ3457	 W12-3137	[u'the rwth aachen machine translation system for wmt 2012 ']	N04-1033	[u'\nwe exchange the base line lexical scoring with noisy-or (zens and ney, 2004) ', <papid> N04-1033 </papid>, u'lexical scoring variant tnoisyor(?).']
ZJ3754	 W12-3136	[u'qcri at wmt12 experiments in spanish english and german english machine translation of news text ']	N03-1017	[u'\nwe used the state-of the-art phrase-based model (koehn et al, 2003) ', <papid> N03-1017 </papid>, u'for statistical machine translation (smt) with several non-standard settings, e.g., data selection and phrase table combination.']
ZJ4073	 W12-4207	[u'head finalization reordering for chinesetojapanese machine translation ']	J04-4002	[u'\nthen, symmetrization matrix is built (och and ney, 2004) ', <papid> J04-4002 </papid>, u'by using word-to-word alignments, and wide variety now at baidu japan inc. ? now at nara institute of science and technology (naist) of heuristics can be used to extract the bilingual phrases (zens et al, 2002; koehn et al, 2003).', <papid> N03-1017 </papid>]
ZJ4074	 W12-4207	[u'head finalization reordering for chinesetojapanese machine translation ']	N03-1017	[u'\nthen, symmetrization matrix is built (och and ney, 2004) ', <papid> J04-4002 </papid>, u'by using word-to-word alignments, and wide variety now at baidu japan inc. ? now at nara institute of science and technology (naist) of heuristics can be used to extract the bilingual phrases (zens et al, 2002; koehn et al, 2003).', <papid> N03-1017 </papid>]
ZJ4401	 W12-3125	[u'on hierarchical reordering and permutation parsing for phrase based decoding ']	N03-1017	[u'\nmovement can be modeled with distortion penalty or lexicalized re-ordering probabilities (koehn et al, 2003; ', <papid> N03-1017 </papid>, u'koehn et al, 2007), ', <papid> P07-2045 </papid>, u'while decoding can be constrained by distortion limits orby mimicking the restrictions of inversion transduction grammars (wu, 1997; ', <papid> J97-3002 </papid>, u'zens et al, 2004).', <papid> C04-1030 </papid>, u'recently, we have begun to see deterministic permutation parsers incorporated into phrase-based de coders.']
ZJ4425	 W12-3125	[u'on hierarchical reordering and permutation parsing for phrase based decoding ']	J04-4002	[u'\nning standard phrase extraction (och and ney, 2004) ', <papid> J04-4002 </papid>, u'without phrase-length limit, noting the corners ofeach phrase found.']
ZJ4529	 W98-1116	[u'what grammars tell us about corpora the case of reduced relative clauses ']	P97-1003	[u'\nnj 08903 u.s.a. suzanne~ruccs , rutgers , edu empirically-induced models that learn lin-guistically meaningflll grammar (collins, 1997) ', <papid> P97-1003 </papid>, u'seem to give tile best practical results in statis-tical natural language processing.']
ZJ4675	 W12-4506	[u'ict system description for conll2012 ']	N03-1017	[u'\nalthough we could compute lexical,syntactic or semantic similar score to obtain accurate similarity, here for simplicity, we just compute the lexical similarity using the phrase table extracted by phrased-based machine translation decoder (koehn et al, 2003).', <papid> N03-1017 </papid>]
ZJ4919	 W12-3135	[u'syntax aware phrase based statistical machine translation system description ']	N03-1017	[u'\nwith its moderate context sensitivity and reliance on n-gram language models,phrase-based statistical machine translation (pb smt) (koehn et al, 2003) ', <papid> N03-1017 </papid>, u'is usually quite good at performing small word order changes ? for instance, the inversion of adjective and noun inenglish-to-french translation and vice versa.']
ZJ4929	 W12-3135	[u'syntax aware phrase based statistical machine translation system description ']	C96-1058	[u'\nin many respects, it is inspired by the work on dependency parsing by eisner (1996) (', <papid> C96-1058 </papid>, u'edge factorization) and mcdonald et al (2005) (', <papid> P05-1012 </papid>, u'choice of features for edge scores).']
ZJ4930	 W12-3135	[u'syntax aware phrase based statistical machine translation system description ']	P05-1012	[u'\nin many respects, it is inspired by the work on dependency parsing by eisner (1996) (', <papid> C96-1058 </papid>, u'edge factorization) and mcdonald et al (2005) (', <papid> P05-1012 </papid>, u'choice of features for edge scores).']
ZJ5282	 W12-3159	[u'direct error rate minimization for statistical machine translation ']	N03-1017	[u'\nsince beam search involves pruning, it is crucial tohave good future cost estimation in order to minimize the number of search errors (koehn et al,2003).', <papid> N03-1017 </papid>]
ZJ5296	 W12-3159	[u'direct error rate minimization for statistical machine translation ']	P05-1033	[u'\nwe use another technique to speed up direct search by storing and re-using search graphs, which consist of lattices in the case of phrase-based decoding (och et al, 1999) ', <papid> W99-0604 </papid>, u'and hypergraphs in the caseof hierarchical decoding (chiang, 2005).', <papid> P05-1033 </papid>]
ZJ5395	 W12-3121	[u'combining quality prediction and system selection for improved automatic translation output ']	J04-4002	[u'\nusing these parallel datasets, we train statistical phrase-based mt system similar to (och and ney, 2004) ', <papid> J04-4002 </papid>, u'as primary systems (sys1).']
ZJ5410	 W12-3410	[u'using an svm ensemble system for improved tamil dependency parsing ']	C96-1058	[u'\nthe main drawback of these methods is that for projective trees, the worst case scenario for most methods is complexity of o(n3)(eisner, 1996).', <papid> C96-1058 </papid>]
ZJ5451	 W12-3158	[u'leaveoneout phrase model training for largescale deployment ']	N03-1017	[u'\nthe goal of this work is two-fold: (i) investigating forced alignment training as phrase table pruning method for large-scale commercial smt systems and (ii)proposing several extensions to the training procedure to deal with practical issues and stimulate further research.generative phrase translation models have the inherent problem of over-fitting to the training data(koehn et al, 2003; ', <papid> N03-1017 </papid>, u'denero et al, 2006).', <papid> W06-3105 </papid>]
ZJ5886	 W12-3157	[u'phrase model training for statistical machine translation with word lattices of preprocessing alternatives ']	J04-4002	[u'\n(1) generalizing the noisy channel approach (brownet al, 1990) ', <papid> J90-2002 </papid>, u'and making use of the maximum approximation (viterbi), the decoder directly models the posterior probability by log-linear combination of several feature functions hm(ei1, k 1 , j 1 ) weighted with scaling factors m, which results in the decision rule (och and ney, 2004) ', <papid> J04-4002 </papid>, u'ei1 = argmax i,ei1,k,s 1 { m?']
J14	 H05-1095	[u'translating with noncontiguous phrases ']	J04-4002	[u'\nthis is the strategy that is usually adopted in other phrase-based mt approaches (zens and ney, 2003; och and ney, 2004).', <papid> J04-4002 </papid>]
J17	 H05-1095	[u'translating with noncontiguous phrases ']	P05-1033	[u'\nthe compositional bi-phrase feature function hcomp: this is introduced to compensate for1recent work from chiang (chiang, 2005) ', <papid> P05-1033 </papid>, u'addresses similar concerns to those motivating our work by introducing synchronous cfg for bi-phrases.']
J256	 H05-1024	[u'alignment link projection using transformation based learning ']	N03-1017	[u'\nthe standard method to overcome this problem to use the model in both directions (interchanging the source and target languages) and applying heuristic-based combination techniques to produce refined alignment (och and ney, 2000; ', <papid> P00-1056 </papid>, u'koehn et al, 2003)', <papid> N03-1017 </papid>, u'henceforth referred to as ra.?']
J264	 H05-1024	[u'alignment link projection using transformation based learning ']	P97-1003	[u'\nin our experiments, we used dependency parser only in english (a version of the collins parser (collins, 1997) ', <papid> P97-1003 </papid>, u'that has been adapted for building dependencies) but not in the other language.?']
X201	 S12-1034	[u'monolingual distributional similarity for texttotext generation ']	P02-1006	[u'\nparaphrases, i.e. differing textual realizations of the same meaning, are acrucial components of text-to-text generation systems, and have been successfully applied to tasks such as multi-document summarization (barzilay etal., 1999; ', <papid> P99-1071 </papid>, u'barzilay, 2003), query expansion (anick and tipirneni, 1999; riezler et al, 2007), ', <papid> P07-1059 </papid>, u'question answering (mckeown, 1979; ', <papid> P79-1016 </papid>, u'ravichandran andhovy, 2002), ', <papid> P02-1006 </papid>, u'sentence compression (cohn and la pata, 2008; ', <papid> C08-1018 </papid>, u'zhao et al, 2009), ', <papid> P09-1094 </papid>, u'and simplification (wubben et al, 2012).', <papid> P12-1107 </papid>]
X211	 S12-1034	[u'monolingual distributional similarity for texttotext generation ']	P05-1033	[u'\nfollowing ganitkevitch et al (2011)', <papid> D11-1108 </papid>, u', we formulate our paraphrases as syntactically annotated synchronous context-free grammar (scfg) (aho and ullman, 1972; chiang, 2005).', <papid> P05-1033 </papid>]
X231	 S12-1061	[u'soft cardinality a parameterized similarity function for text comparison ']	W05-1203	[u'\nin this paper, we combine elements from the perspective of cognitive psychology and computer science to propose model for building similarity functions suitable for the task of semantic text sim ilarity.we identify four main families of text similarity functions: i) resemblance coefficients based on sets (e.g. jac cards (1901) and dices (1945) coefficients) ii) functions in metric spaces (e.g. cosine tf-idf similarity (salton et al., 1975)); iii) the edit distance family of measures (e.g. leven stein (1966) distance, lcs (hirschberg, 1977));and iv) hybrid approaches ((monge and elkan, 1996; cohen et al, 2003; corley and mihalcea, 2005; ', <papid> W05-1203 </papid>, u'jimenez et al., 2010)).']
X320	 S12-1058	[u'zhijun wu chinese semantic dependency parsing with third order features ']	C96-1058	[u'\ntwo main approaches to syntactic dependency paring are maximum spanning tree (mst) based dependency parsing and transition based dependency parsing (eisner, 1996; ', <papid> C96-1058 </papid>, u'nivre et al, 2004; ', <papid> W04-2407 </papid>, u'mcdonald and pereira, 2006).', <papid> E06-1011 </papid>]
X323	 S12-1058	[u'zhijun wu chinese semantic dependency parsing with third order features ']	P05-1012	[u'\ndependency tree parsing as the search for the maximum spanning tree in directed graph was proposed by mcdonald et al (2005', <papid> P05-1012 </papid>, u'c).']
X461	 S12-1082	[u'deep purple estimating sentence semantic similarity using ngram regression models and web snippets ']	H05-1079	[u'\nsts is closely related to the problems of paraphrasing, which is bidirectional and based on semantic equivalence (madnani and dorr, 2010) ', <papid> J10-3003 </papid>, u'and textual entailment, which is directional and based on relations between semantics (dagan et al, 2006).related methods incorporate measurements of similarity at various levels: lexical (malakasiotis and androutsopoulos, 2007), ', <papid> W07-1407 </papid>, u'syntactic (malakasiotis, 2009; ', <papid> P09-3004 </papid>, u'zanzotto et al, 2009), and semantic (rinaldi et al, 2003; ', <papid> W03-1604 </papid>, u'bos and markert, 2005).', <papid> H05-1079 </papid>]
X862	 S12-1057	[u'mixcd system description for evaluating chinese word similarity at semeval2012 ']	W05-1203	[u'\nit performs well in evaluating semantic similarity between two texts (zhang et al, 2008; corley and mihalcea, 2005; ', <papid> W05-1203 </papid>, u'pedersen, 2010).', <papid> N10-1047 </papid>]
X885	 S12-1065	[u'ualacant using online machine translation for cross lingual textual entailment ']	N03-1017	[u'\nthey train phrase-based statistical mt (pbsmt) system (koehn et al, 2003) ', <papid> N03-1017 </papid>, u'translating from lh to lt , and use the translation table obtained as by-product of the training process to extract set of features which are processed by support vector machine classifier (theodoridis and koutroumbas, 2009, sect.']
X1097	 S12-1107	[u'dirrelcond3 detecting textual entailment across languages with conditions on directional text relatedness scores ']	W05-1203	[u'\nthe key members of the conditions were derived from (cor ley and mihalcea, 2005) ', <papid> W05-1203 </papid>, u'formula initially for text similarity, while the entailment condition used as starting point was that from (tatar et al, 2009).']
X1172	 S12-1031	[u'the effects of semantic annotations on precision parse ranking ']	P97-1003	[u'\nmost start-of-the-art natural language parsers (char niak, 2000; ', <papid> A00-2018 </papid>, u'clark and curran, 2004; ', <papid> P04-1014 </papid>, u'collins, 1997) ', <papid> P97-1003 </papid>, u'use lexicalised features for parse ranking.']
X1255	 S12-1074	[u'njuparser achievements on semantic dependency parsing ']	C96-1058	[u'\ncurrently, there are many dependency parsers available, such as eisners probabilistic dependency parser (eisner, 1996), ', <papid> C96-1058 </papid>, u'mcdonalds mst parser (mcdonald et al 2005', <papid> P05-1012 </papid>, u'a; mcdonald et al 2005', <papid> P05-1012 </papid>, u'b) and nivres malt parser (nivre, 2006).']
X1256	 S12-1074	[u'njuparser achievements on semantic dependency parsing ']	P05-1012	[u'\ncurrently, there are many dependency parsers available, such as eisners probabilistic dependency parser (eisner, 1996), ', <papid> C96-1058 </papid>, u'mcdonalds mst parser (mcdonald et al 2005', <papid> P05-1012 </papid>, u'a; mcdonald et al 2005', <papid> P05-1012 </papid>, u'b) and nivres malt parser (nivre, 2006).']
X1661	 S12-1050	[u'semeval2012 task 5 chinese semantic dependency parsing ']	P05-1013	[u'\nthe system-combining strategy involves three steps: ? parsing each sentence using nivres arc standard, nivres arc eager (nivre and nilsson, 2005; ', <papid> P05-1013 </papid>, u'nivre, 2008), ', <papid> J08-4003 </papid>, u'and liangs dynamic algorithm (huang and sagae, 2010); ? ', <papid> P10-1110 </papid>, u'combining parses given by the three parsers into weighted directed graph; ? using the chu-liu-edmonds algorithm to search for the final parse for each sen tence.']
X1795	 P99-1033	[u'dependency parsing with an extended finite state approach ']	C96-1058	[u'\ndependency-based statistical language modeling and analysis have also become quite popular in statistical natural language process-ing (lafferty et al, 1992; eisner, 1996; ', <papid> C96-1058 </papid>, u'chelba and et al, 1997).']
X1908	 S12-1102	[u'soft cardinality  ml learning adaptive similarity functions for cross lingual textual entailment ']	W05-1203	[u'\nalthough, there are asymmetric measures such as the monge-elkan measure (1996) and the measure proposed by corley and mihalcea (corley and mihalcea, 2005), ', <papid> W05-1203 </papid>, u'they are outnumbered by the symmetric measures.']
X1978	 S12-1108	[u'ict a translation based method for cross lingual textual entailment ']	H05-1079	[u'\nlogic-based approach is to map the language expressions to logical meaning representations, and then relyon logical entailment checks, possibly by invoking theorem provers (rinaldi et al., 2003; ', <papid> W03-1604 </papid>, u'bos  markert, 2005; ', <papid> H05-1079 </papid>, u'tatu  moldovan, 2005, ', <papid> H05-1047 </papid>, u'2007).']
X1979	 S12-1108	[u'ict a translation based method for cross lingual textual entailment ']	H05-1047	[u'\nlogic-based approach is to map the language expressions to logical meaning representations, and then relyon logical entailment checks, possibly by invoking theorem provers (rinaldi et al., 2003; ', <papid> W03-1604 </papid>, u'bos  markert, 2005; ', <papid> H05-1079 </papid>, u'tatu  moldovan, 2005, ', <papid> H05-1047 </papid>, u'2007).']
X1982	 S12-1108	[u'ict a translation based method for cross lingual textual entailment ']	N03-1017	[u'\neffective models, such as phrase-based model (koehn et al, 2003), ', <papid> N03-1017 </papid>, u'hierarchical phrase-based model (hpb) (chiang, 2005), ', <papid> P05-1033 </papid>, u'and syntax-based (liu et al, 2006) ', <papid> P06-1077 </papid>, u'model have been proposed to improve the translation quality.']
X1983	 S12-1108	[u'ict a translation based method for cross lingual textual entailment ']	P05-1033	[u'\neffective models, such as phrase-based model (koehn et al, 2003), ', <papid> N03-1017 </papid>, u'hierarchical phrase-based model (hpb) (chiang, 2005), ', <papid> P05-1033 </papid>, u'and syntax-based (liu et al, 2006) ', <papid> P06-1077 </papid>, u'model have been proposed to improve the translation quality.']
Q55	 P03-1040	[u'feature rich statistical translation of noun phrases ']	N03-1017	[u'\ndetails of this model are described by koehn et al (2003).', <papid> N03-1017 </papid>]
Q83	 P04-1027	[u'an empirical study of information synthesis task ']	W00-0403	[u'\ntopic-oriented multi-document summarization has already been studied in other evaluation initiatives which provide test beds to compare alternative approaches (over, 2003; goldstein et al, 2000;radev et al, 2000).', <papid> W00-0403 </papid>]
Q104	 P00-1065	[u'automatic labeling of semantic roles ']	P97-1003	[u'\nthe system is trained by rst using the collins parser(collins, 1997) ', <papid> P97-1003 </papid>, u'to parse the 36,995 training sentences, matching annotated frame elements to parse constituents, and extracting various features from the string of words and the parse tree.']
Q112	 P01-1037	[u'the role of lexico semantic feedback in open domain textual question answering ']	A00-1023	[u'\nassuming that it is very likely that the answer is named entity, (srihari and li, 2000) ', <papid> A00-1023 </papid>, u'describes ne-supported q system that functions quite well when the expected answer type is one of the categories covered by the ne recognizer.']
U3	 P11-2078	[u'online language model biasing for statistical machine translation ']	N03-1017	[u'\nwe estimate the bilingual word co-occurrence matrix ? from an unsupervised, automatic word alignment induced over the parallel training corpus p. we use the giza++ toolkit (al-onaizan et al, 1999) to estimate the parameters of ibm model 4 (brown et al, 1993), ', <papid> J93-2003 </papid>, u'and combine the forward and backward viterbi alignments to obtain many-to many word alignments as described in koehn et al (2003).', <papid> N03-1017 </papid>]
U11	 P11-2032	[u'bayesian word alignment for statistical machine translation ']	N03-1017	[u'\nword alignment is crucial early step in the training of most statistical machine translation (smt) systems, in which the estimated alignments are used for constraining the set of candidates in phrase/grammar extraction (koehn et al, 2003; ', <papid> N03-1017 </papid>, u'chiang, 2007; ', <papid> J07-2003 </papid>, u'galle yet al, 2006).']
U109	 P12-3004	[u'niu trans an open source toolkit for phrase based and syntax based machine translation ']	N03-1017	[u'\nthe niu trans toolkit supports most statistical machine translation (smt) paradigms developed over the past decade, and allows for training and decoding with several state-of-the-art models, including: the phrase-based model (koehn et al, 2003), ', <papid> N03-1017 </papid>, u'the hierarchical phrase-based model (chiang, 2007), ', <papid> J07-2003 </papid>, u'and various syntax-based models (galley et al, 2004; ', <papid> N04-1035 </papid>, u'liu et al, 2006).', <papid> P06-1077 </papid>]
U142	 P11-1160	[u'partial parsing from bitext projections ']	P05-1012	[u'\nprojectivity can be relaxed in some parsers (mcdonald et al, 2005; ', <papid> P05-1012 </papid>, u'nivre, 2009).', <papid> P09-1040 </papid>]
U281	 P11-2069	[u'monolingual alignment by edit rate computation on sentential paraphrase pairs ']	J04-4002	[u'\n396 statistical word alignment the giza++ tool (och and ney, 2004) ', <papid> J04-4002 </papid>, u'computes statistical word alignment models of increasing complexity from parallel corpora.']
U297	 P12-1050	[u'modified distortion matrices for phrase based statistical machine translation ']	N03-1017	[u'\non one hand, the phrase-based approach (psmt) (och, 2002; zens et al., 2002; koehn et al, 2003), ', <papid> N03-1017 </papid>, u'with its shallow and loose modeling of linguistic equivalences, appears as the most competitive choice for closely related language pairs with similar clause structures, both in terms of quality and of efficiency.']
U299	 P12-1050	[u'modified distortion matrices for phrase based statistical machine translation ']	P05-1033	[u'\non the other, tree-based approaches (wu, 1997; ', <papid> J97-3002 </papid>, u'yamada, 2002; chiang, 2005) ', <papid> P05-1033 </papid>, u'gain advantage, at the cost of higher complexity and isomorphism assumptions, on language pairs with radically different word orders.']
U678	 P11-2119	[u'unary constraints for efficient context free parsing ']	P97-1003	[u'\nx unary productions for all non-terminals x, (5) collapsing unary chains to single (possibly composite) unary production (klein and manning, 2001), (', <papid> P01-1044 </papid>, u'6) introducing new categories such as aux (charniak, 1997), and (7) collapsing of categories such as prt and advp (collins, 1997).', <papid> P97-1003 </papid>]
U709	 P11-3001	[u'word alignment combination over multiple word segmentation ']	N03-1017	[u'\nwe used phrase-based mt system similar to (koehn et al, 2003), ', <papid> N03-1017 </papid>, u'and generated two baseline alignments using giza++ enhanced by gdf heuristics (koehn et al., 2003) ', <papid> N03-1017 </papid>, u'and linear discriminative word alignment model (diwa) (liu et al, 2010) ', <papid> J10-3002 </papid>, u'on training set with the three segment ations respectively.']
U947	 P11-2051	[u'corpus expansion for statistical machine translation with semantic role label substitution rules ']	J04-4002	[u'\ntwo phrase pairs with thesame semantic signature are considered valid substitutions of each other.the extraction of ssrs is similar to the well known phrase extraction algorithm (och and ney, 2004).', <papid> J04-4002 </papid>]
U971	 P11-1144	[u'semi supervised frame semantic parsing for unknown predicates ']	P05-1012	[u'\nthe raw sentences in all the training and test documents were preprocessed using mxpost (ratnaparkhi, 1996) ', <papid> W96-0213 </papid>, u'and the mst dependency parser (mcdonald et al , 2005) ', <papid> P05-1012 </papid>, u'following das et al  (2010a).']
U1001	 P11-3002	[u'sentence ordering driven by local and global coherence for summary generation ']	A00-2024	[u'\nbut it remains an open question whether sentence ordering is non-trivial for single-document summarization, as it has long been recognized as an actual strategy taken by human summarizers (jing, 1998; jing and mckeown, 2000) ', <papid> A00-2024 </papid>, u'and acknowledged early in work on sentence ordering for multi-document summarization (barzilay et al , 2002).']
U1048	 P11-2035	[u'joint training of dependency parsing filters through latent support vector machines ']	P05-1012	[u'\npotential arcs are scored using rich linear models that are discriminatively trained to maximize parsing accuracy (mcdonald et al, 2005).', <papid> P05-1012 </papid>]
U1100	 P11-1124	[u'consistent translation using discriminative learning  a translation memory inspired approach ']	P05-1033	[u'\nfor example, (koehnand senellart, 2010) showed that reusing these translations as large rules in hierarchical system (chi ang, 2005) ', <papid> P05-1033 </papid>, u'can be beneficial when the fuzzy match score is above 70%, while (zhechev and van genabith, 2010) ', <papid> W10-3806 </papid>, u'reported that it is only beneficial to phrase-based system when the fuzzy match score is above 90%.']
U1103	 P11-1124	[u'consistent translation using discriminative learning  a translation memory inspired approach ']	N03-1017	[u'\nspecifically, we use the phrase translation probabilities p(f? m|em) andp(em|f? m), as well as the lexical translation probabilities plex(f? m|em) and plex(em|f? m) as calculated in (koehn et al, 2003).', <papid> N03-1017 </papid>]
U1184	 P11-2030	[u'word alignment via sub modular maximization over matroids ']	W03-0301	[u'\nwe evaluated our approaches using the english french hansa rds data from the 2003 naacl shared task (mihalcea and pedersen, 2003).', <papid> W03-0301 </papid>]
U1271	 P12-1018	[u'machine translation without words through substring alignment ']	N03-1017	[u'\nin the formalism presented above, this means that each ei must be included in at most one span, and for each span = v. traditionally, these models are run in both directions and combined using heuristics to create many-to-many alignments (koehn et al, 2003).', <papid> N03-1017 </papid>]
U1370	 P12-1063	[u'computational approaches to sentence completion ']	W00-0603	[u'\nthe deep read system (hirschman et al, 1999) ', <papid> P99-1042 </papid>, u'initiated long line of research into reading comprehension based on test prep material (charniak et al, 2000; ', <papid> W00-0601 </papid>, u'riloff and the len, 2000; ', <papid> W00-0603 </papid>, u'wang et al, 2000; ', <papid> W00-1316 </papid>, u'ng et al, 2000).', <papid> W00-1316 </papid>]
U1616	 P12-1079	[u'a topic similarity model for hierarchical phrase based translation ']	N03-1017	[u'\n(koehn et al , 2003).', <papid> N03-1017 </papid>]
U1677	 P11-1126	[u'hypothesis mixture decoding for statistical machine translation ']	J04-4002	[u'\nbesides tremendous efforts on constructing more complicated and accurate models for statistical machine translation (smt) (och and ney, 2004; ', <papid> J04-4002 </papid>, u'chiang, 2005; ', <papid> P05-1033 </papid>, u'galley et al, 2006; ', <papid> P06-1121 </papid>, u'shen et al, 2008; ', <papid> P08-1066 </papid>, u'chiang 2010), ', <papid> P10-1146 </papid>, u'many researchers have concentrated on the approaches that improve translation quality using information between hypotheses from one or more smt systems as well.']
U1678	 P11-1126	[u'hypothesis mixture decoding for statistical machine translation ']	P05-1033	[u'\nbesides tremendous efforts on constructing more complicated and accurate models for statistical machine translation (smt) (och and ney, 2004; ', <papid> J04-4002 </papid>, u'chiang, 2005; ', <papid> P05-1033 </papid>, u'galley et al, 2006; ', <papid> P06-1121 </papid>, u'shen et al, 2008; ', <papid> P08-1066 </papid>, u'chiang 2010), ', <papid> P10-1146 </papid>, u'many researchers have concentrated on the approaches that improve translation quality using information between hypotheses from one or more smt systems as well.']
U1937	 P12-1013	[u'efficient tree based approximation for entailment graph learning ']	P02-1006	[u'\nsemantic inference applications such as qa and ie crucially relyon entailment rules (ravichandran and hovy, 2002; ', <papid> P02-1006 </papid>, u'shinyama and sekine, 2006) ', <papid> N06-1039 </papid>, u'or equivalently inference rules, that is, rules that describe directional inference relation between two fragments of text.']
U1969	 P11-2074	[u'discriminative feature tied mixture modeling for statistical machine translation ']	N03-1017	[u'\nsignificant progress has been made in statistical machine translation (smt) in recent years.among all the proposed approaches, the phrase based method (koehn et al, 2003) ', <papid> N03-1017 </papid>, u'has become the widely adopted one in smt due to its capability of capturing local context information from adjacent words.']
U2022	 P11-1155	[u'using bilingual information for cross language document summarization ']	C00-1072	[u'\nin the task of single document summarization, various features have been investigated for ranking sentences in document, including term frequency, sentence position, cue words, stigma words, and topic signature (luhn 1969; lin and hovy, 2000).', <papid> C00-1072 </papid>]
U2145	 P12-1096	[u'a ranking based approach to word reordering for statistical machine translation ']	N03-1017	[u'\nin phrase-based models (och, 2002; koehn et al,2003), ', <papid> N03-1017 </papid>, u'phrase is introduced to serve as the fundamental translation element and deal with local reordering, while distance based distortion model is used to coarsely depict the exponentially decayed word movement probabilities in language translation.']
U2149	 P12-1096	[u'a ranking based approach to word reordering for statistical machine translation ']	P05-1033	[u'\ncalized distortion models, including both generative (koehn et al, 2005) and discriminative (zens and ney, 2006; ', <papid> W06-3108 </papid>, u'xiong et al, 2006) ', <papid> P06-1066 </papid>, u'variants, to achieve finer-grained estimations, while other work took into account the hierarchical language structures in translation (chiang, 2005; ', <papid> P05-1033 </papid>, u'galley and manning, 2008).', <papid> D08-1089 </papid>]
U2220	 P12-1033	[u'smaller alignment models for better translations unsupervised word alignment with the l0norm ']	J04-4002	[u'\nmoreover, they are widely disseminated in the open-source giza++ toolkit (och and ney, 2004).', <papid> J04-4002 </papid>]
U2287	 P12-1048	[u'translation model adaptation for statistical machine translation with monolingual topic information ']	N03-1017	[u'\nin recent years, statistical machine translation(smt) has been rapidly developing with more and more novel translation models being proposed and put in to practice (koehn et al , 2003; ', <papid> N03-1017 </papid>, u'och and ney, 2004; ', <papid> J04-4002 </papid>, u'galley et al , 2006; ', <papid> P06-1121 </papid>, u'liu et al , 2006; ', <papid> P06-1077 </papid>, u'chiang, 2007; ', <papid> J07-2003 </papid>, u'chiang, 2010).', <papid> P10-1146 </papid>]
U2288	 P12-1048	[u'translation model adaptation for statistical machine translation with monolingual topic information ']	J04-4002	[u'\nin recent years, statistical machine translation(smt) has been rapidly developing with more and more novel translation models being proposed and put in to practice (koehn et al , 2003; ', <papid> N03-1017 </papid>, u'och and ney, 2004; ', <papid> J04-4002 </papid>, u'galley et al , 2006; ', <papid> P06-1121 </papid>, u'liu et al , 2006; ', <papid> P06-1077 </papid>, u'chiang, 2007; ', <papid> J07-2003 </papid>, u'chiang, 2010).', <papid> P10-1146 </papid>]
U2308	 P12-1048	[u'translation model adaptation for statistical machine translation with monolingual topic information ']	N04-1033	[u'\ncombination method (zens and ney,2004) ', <papid> N04-1033 </papid>, u'which has shown good performance in calculating similarities between bags-of-words in different languages.']
U2406	 P12-2004	[u'a feature rich constituent context model for grammar induction ']	P05-1033	[u'\nour log-linear variant of the ccm extends robustly to long sentences, enabling constituent grammar induction to be used in settings that typically include long sentences, such as machine translation reordering (chiang, 2005; ', <papid> P05-1033 </papid>, u'denero and uszkoreit, 2011; ', <papid> D11-1018 </papid>, u'dyer et al, 2011).', <papid> W11-2139 </papid>]
U2469	 P12-1049	[u'a statistical model for unsupervised and semi supervised transliteration mining ']	N03-1017	[u'\n5.1.1 training we word-aligned the parallel phrases of the training data using giza++ (och and ney, 2003), ', <papid> J03-1002 </papid>, u'andsymmetrized the alignments using the grow-diag final-and heuristic (koehn et al, 2003).', <papid> N03-1017 </papid>]
U2517	 P12-2057	[u'translation model size reduction for hierarchical phrase based statistical machine translation ']	N03-1017	[u'\nphrase based model (koehn et al, 2003) ', <papid> N03-1017 </papid>, u'and hierarchical phrase-based model (chiang, 2005; ', <papid> P05-1033 </papid>, u'chiang, 2007)', <papid> J07-2003 </papid>, u'show state-of-the-art performance in various language pairs.']
U2518	 P12-2057	[u'translation model size reduction for hierarchical phrase based statistical machine translation ']	P05-1033	[u'\nphrase based model (koehn et al, 2003) ', <papid> N03-1017 </papid>, u'and hierarchical phrase-based model (chiang, 2005; ', <papid> P05-1033 </papid>, u'chiang, 2007)', <papid> J07-2003 </papid>, u'show state-of-the-art performance in various language pairs.']
U2557	 P12-1075	[u'unsupervised relation discovery with sense disambiguation ']	P02-1006	[u'\nre is an essential part of information extraction and is useful for question answering (ravichandran and hovy, 2002), ', <papid> P02-1006 </papid>, u'textual entailment (szpektor et al , 2004) ', <papid> W04-3206 </papid>, u'and many other applications.a common approach to re is to assume that relations to be extracted are part of predefined ontology.']
U2764	 P12-2011	[u'pattern learning for relation extraction with a hierarchical topic model ']	P02-1006	[u'\nmanually selected seeds can also be used (ravichandran and hovy, 2002; ', <papid> P02-1006 </papid>, u'kozareva and hovy, 2010).', <papid> P10-1150 </papid>]
ZI86	 W12-0113	[u'bootstrapping method for chunk alignment in phrase based smt ']	N03-1017	[u'\nthe effectiveness of the mwe-aligned and chunk aligned parallel corpus is demonstrated by using the standard log-linear pb-smt model as our baseline system: giza++ implementation of ibm word alignment model 4, phrase-extraction heuristics described in (koehn et al , 2003), ', <papid> N03-1017 </papid>, u'minimum-error-rate training (och, 2003) ', <papid> P03-1021 </papid>, u'on held-out development set, target language model trained using srilm toolkit (stolcke, 2002) with kneser-ney smoothing (kneser and ney, 1995) and the moses decoder (koehn et al , 2007).', <papid> P07-2045 </papid>]
ZI97	 W12-2007	[u'measuring the use of factual information in test taker essays ']	P02-1006	[u'\ntraditionally, the goal of an information extraction system is automated population of structured databases of events or concepts of interest and their properties by analyzing large corpora of text (chin chor et al, 1993; ', <papid> J93-3001 </papid>, u'onyshkevych, 1993; ', <papid> M93-1003 </papid>, u'grishman and sundheim, 1995; ', <papid> M95-1001 </papid>, u'ravichandran and hovy, 2002;', <papid> P02-1006 </papid>, u'agichtein and gravano, 2000; davidov and rappoport, 2009).', <papid> D09-1028 </papid>, u'1for example, barack obama picks out precisely one person, and the same one in 2010 as it did in 1990.']
ZI230	 W11-2107	[u'meteor 13 automatic metric for reliable optimization and evaluation of machine translation systems ']	N03-1017	[u'\nphrases are extracted using standard phrase-based heuristics (koehn et al,2003) ', <papid> N03-1017 </papid>, u'and used to build translation table and lexicalized reordering model.']
ZI344	 W11-2140	[u'noisy sms machine translation in low density languages ']	N03-1017	[u'\nsince the total amount of haitian english parallel data provided is quite limited, we found additional data and augmented the available set with data gathered by the crisis commons group and made it available to other wmt participants.the combined training corpus from which we extracted our grammar consisted of 123,609 sentence pairs, which was then filtered for length and aligned using the giza++ implementation of ibm model4 (och and ney, 2003) ', <papid> J03-1002 </papid>, u'to obtain one-to-many alignments in either direction and symmetrized using the grow-diag-final-and method (koehn et al, 2003).', <papid> N03-1017 </papid>]
ZI368	 W11-2160	[u'joshua 30 syntax based machine translation with the thrax grammar extractor ']	N03-1017	[u'\nwe calculate these weights as given in (koehn et al, 2003): ', <papid> N03-1017 </papid>, u'let be the alignment between ? and ?, so (i, j) ? if and only if the ith word of ? is aligned to the jth word of ?.']
ZI1004	 W11-2143	[u'cmu syntax based machine translation at wmt 2011 ']	N03-1017	[u'\nthe non-syntactic grammar was extracted from the parallel corpus and word alignments following the standard heuristics of phrase-based smt (koehn et al, 2003).', <papid> N03-1017 </papid>]
ZI1011	 W11-2143	[u'cmu syntax based machine translation at wmt 2011 ']	P05-1033	[u'\nalthough we found mixed results in their application within our current system, the success of hiero-derived mt systems (chiang, 2005; ', <papid> P05-1033 </papid>, u'chiang, 2010) ', <papid> P10-1146 </papid>, u'shows that high translation quality can be achieved with rules that are only partially abstract.']
ZI1228	 W12-0503	[u'hybrid combination of constituency and dependency trees into an ensemble dependency parser ']	C96-1058	[u'\ntokeep their tree constraint, they applied eisners algorithm for re parsing (eisner, 1996).', <papid> C96-1058 </papid>]
ZI1509	 W11-2165	[u'generative models of monolingual and bilingual gappy patterns ']	P05-1033	[u'\na growing body of machine translation research aims to exploit lexical patterns (e.g., grams and phrase pairs) with gaps (simard et al., 2005; ', <papid> H05-1095 </papid>, u'chiang, 2005; ', <papid> P05-1033 </papid>, u'xiong et al, 2011).', <papid> P11-1129 </papid>, u'typically, these gappy patterns?']
ZI1511	 W11-2165	[u'generative models of monolingual and bilingual gappy patterns ']	N03-1017	[u'\nbeginning with the success of phrase-based translation models (koehn et al, 2003), ', <papid> N03-1017 </papid>, u'trend arose of modeling larger and increasingly complex structural units in translation.']
ZI1757	 W12-0111	[u'tree based hybrid machine translation ']	P05-1033	[u'\nthe underlying models in smt lack linguistic sophistication when compared to rbmt systems and there is trend towards incorporating more linguistic knowledge by creating hybrid systems that can exploit the linguistic knowledge contained in hand-crafted rules and the knowledge extracted from large amounts of text.hierarchical phrases (chiang, 2005) ', <papid> P05-1033 </papid>, u'are encoded in tree structure just as linguistic trees.']
ZI2088	 W12-0103	[u'full machine translation for factoid question answering ']	P02-1006	[u'\nby, and that san francisco is substring of a. many factoid questions explicitly express an hyponymy relation about the answer type, and also several other relations describing its context (i.e. spatial, temporal, etc.).the qa problem can be approached from several points of view, ranging from simple surface pattern matching (ravichandran and hovy, 2002), ', <papid> P02-1006 </papid>, u'to automated reasoning (moldovan et al, 2007) or supercomputing (ferrucci et al, 2010).']
ZI2109	 W11-2701	[u'a new sentence compression dataset and its use in an abs tractive generateandrank sentence compressor ']	A00-1043	[u'\nsentence compression is the task of producing ashorter form of grammatical source (input) sentence, so that the new form will still be grammatical and it will retain the most important information of the source (jing, 2000).', <papid> A00-1043 </papid>]
ZI2246	 W11-2126	[u'agreement constraints for statistical machine translation into german ']	P05-1033	[u'\nrule extraction follows the hierarchical phrase-based algorithm of chiang (2005), ', <papid> P05-1033 </papid>, u'algorithm of chiang (2007).']
ZI2248	 W11-2126	[u'agreement constraints for statistical machine translation into german ']	N03-1017	[u'\nplex (rhst|rhss) and plex (rhss|rhst), the direct and indirect lexical weights (koehn et al, 2003).?', <papid> N03-1017 </papid>]
ZI2253	 W11-2705	[u'exploring linguistically rich patterns for question generation ']	P02-1006	[u'\nfor example, parisis located in france is common way to say that parisis in france, indicated by the words located in.the use of patterns is widely accepted as an effective approach in the field of natural language processing (nlp), in tasks like question-answering (qa) (soubbotin, 2001; ravichandran and hovy,2002) ', <papid> P02-1006 </papid>, u'or question generation (qg) (wyse and pi wek, 2009; mendes et al, 2011).']
ZI2663	 W11-2211	[u'lightly supervised training for hierarchical phrase based machine translation ']	N03-1017	[u'\nin contrast to schwenk, we do not apply lightly-supervised training to conventional phrase-based system (och et al, 1999; ', <papid> W99-0604 </papid>, u'koehn et al, 2003) ', <papid> N03-1017 </papid>, u'but to hierarchical phrase-based translation (hpbt) system.']
ZI2664	 W11-2211	[u'lightly supervised training for hierarchical phrase based machine translation ']	P05-1033	[u'\nin hierarchical phrase-based translation (chiang,2005) ', <papid> P05-1033 </papid>, u'weighted synchronous context-free grammar is induced from parallel text, the search is based on cyk+ parsing (chappelier and rajman,1998) and typically carried out using the cube pruning algorithm (huang and chiang, 2007).', <papid> P07-1019 </papid>]
ZI2735	 W12-2020	[u'an interactive analytic tool for peer review exploration ']	C00-1072	[u'\nthus topic-signature acquisition algorithm (lin and hovy,2000), ', <papid> C00-1072 </papid>, u'which extracts topic words through comparing the vocabulary distribution of target corpus against that of generic background corpus using statistic metric, suits our application better than other approaches, such as probabilistic graphical models (e.g. lda) and frequency based methods.']
ZI2796	 W11-2157	[u'the universitat dalacant hybrid machine translation system for wmt 2011 ']	N03-1017	[u'\ntranslationphrase-based statistical machine translation systems (koehn et al, 2003) ', <papid> N03-1017 </papid>, u'translate sentences by maximising the translation probability as defined by the log-linear combination of number of feature functions, whose weights are chosen to opti 457mise translation quality (och, 2003).', <papid> P03-1021 </papid>]
ZI3775	 W11-2166	[u'extraction programs a unified approach to translation rule extraction ']	P05-1033	[u'\na prominent early example is phrase-based extraction (och et al, 1999).', <papid> W99-0604 </papid>, u'around the middle of the last decade, two extraction paradigms were proposed for syntax-basedmachine translation: the hiero paradigm of (chi ang, 2005) ', <papid> P05-1033 </papid>, u'and the ghkm paradigm of (galley et al., 2004).']
ZI3817	 W11-2504	[u'reranking bilingually extracted paraphrases using monolingual distributional similarity ']	N03-1017	[u'\nthe corresponding foreign phrase (festgenommen) is identified using word alignment and phrase extraction techniques from phrase-based statistical machine translation (koehn et al, 2003).', <papid> N03-1017 </papid>, u'other occurrences of the foreign phrase in the parallel corpus may align to distinct english phrase,such as jailed.']
ZI3974	 W11-2150	[u'illcuva translation system for emnlpwmt 2011 ']	N03-1017	[u'\nwhile first systems following this approach performed translation on the word level, modern state of-the-art phrase-based smt systems (och and ney,2002; ', <papid> P02-1038 </papid>, u'koehn et al, 2003) ', <papid> N03-1017 </papid>, u'start-out from word aligned parallel corpus working with (in principle)arbitrarily large phrase pairs (also called blocks) acquired from word-aligned parallel data under simple definition of translational equivalence (zens et al., 2002).']
ZI4041	 W12-2018	[u'a comparison of greedy and optimal assessment of natural language student input using wordtoword similarity metrics ']	W05-1203	[u'\nthe methods proposed so far that relyon the principle of compositionality to compute the semantic similarity of longer texts have been primarily greedy methods (corley  mihalcea, 2005; ', <papid> W05-1203 </papid>, u'lintean  rus, 2012).']
ZI4086	 W11-2120	[u'the uzh system combination system for wmt 2011 ']	N03-1017	[u'\nfor the translation direction deen, the german source text is reordered based 1described at http://www.statmt.org/wmt11/ baseline.html 166 on syntactic parsing with pro3gresde (sennrich etal., 2009), and reordering rules similar to those described by (collins et al, 2005).', <papid> P05-1066 </papid>, u'the moses phrase table consists of five features: phrase translation probabilities in both translation directions (p(t|s) and p(s|t)), lexical weights (lex(t|s) and lex(s|t)), and constant phrase penalty (koehn et al, 2003).', <papid> N03-1017 </papid>]
ZI4117	 W11-2402	[u'towards a probabilistic model for lexical entailment ']	W05-1203	[u'\nother works (corley and mihalcea, 2005; ', <papid> W05-1203 </papid>, u'zanzotto and moschitti, 2006) ', <papid> P06-1051 </papid>, u'have applied heuristic formu 10 las to estimate the similarity between text fragments based on similarity function between their terms.the abovementioned methods do not capture several important aspects of entailment.']
ZI4726	 W11-2124	[u'wider context by using bilingual language models in machine translation ']	N03-1017	[u'\nin many state-of-the art smt systems, the phrase based (koehn et al, 2003) ', <papid> N03-1017 </papid>, u'approach is used.']
ZI4850	 W11-2139	[u'the cmuark german english translation system ']	N03-1017	[u'\nthe parallel data were aligned using the giza++ implementation of ibm model 4 run in both directions and then symmetrized using the grow-diag-final-and heuristic (och and ney, 2002; ', <papid> P02-1038 </papid>, u'brown et al, 1993; ', <papid> J93-2003 </papid>, u'koehn et al, 2003).', <papid> N03-1017 </papid>]
ZF19	 W09-2808	[u'syntax driven sentence revision for broadcast news summarization ']	A00-2024	[u'\njing and mckeown (1999), jing and mckeown (2000) ', <papid> A00-2024 </papid>, u'found that human summarization can be traced back to six cut-and paste operations of text and proposed revision method consisting of sentence reduction and combination modules with sentence extraction part.']
ZF156	 W09-2807	[u'a classification algorithm for predicting the structure of summaries ']	A00-2024	[u'\nclose to the problem studied here is jing and mckeowns (jing and mckeown,2000) ', <papid> A00-2024 </papid>, u'cut-and-paste method founded on endres niggemeyers observations.']
ZF176	 W09-2201	[u'coupling semi supervised learning of categories and relations ']	P02-1006	[u'\nthis process ite rates, gradually expanding the amount of labeled data.such approaches have shown promise in applications such as web page classification (blum and mitchell, 1998), named entity classification (collins and singer, 1999), ', <papid> W99-0613 </papid>, u'parsing (mcclosky et al, 2006), ', <papid> N06-1020 </papid>, u'and machine translation (ueffing, 2006).bootstrapping approaches to information extraction can yield impressive results with little initial human effort (brin, 1998; agichtein and gravano, 2000; ravichandran and hovy, 2002; ', <papid> P02-1006 </papid>, u'pasca et al,2006).']
ZF371	 W09-2310	[u'coupling hierarchical word reordering and decoding in phrase based statistical machine translation ']	N03-1017	[u'\nthe state-of-the-artsmt system moses implements distance-based reordering model (koehn et al, 2003) ', <papid> N03-1017 </papid>, u'and distortion model, operating with rewrite patterns extracted from phrase alignment table (tillman, 2004).many smt models implement the brute force approach, introducing several constrains for there ordering search as described in kanthak et al (2005) ', <papid> W05-0831 </papid>, u'and crego et al (2005).']
ZF399	 W09-1804	[u'a new objective function for word alignment ']	J04-4002	[u'\none is machine translation, where programs extract translation rules from word-aligned corpora (och and ney, 2004; ', <papid> J04-4002 </papid>, u'galley et al, 2004;chiang, 2007; quirk et al, 2005).', <papid> P05-1034 </papid>]
ZF739	 W09-2307	[u'discriminative reordering with chinese grammatical relations features ']	N03-1017	[u'\nbasic reordering models in phrase-based systems use linear distance as the cost for phrase movements (koehn et al, 2003).', <papid> N03-1017 </papid>]
ZF783	 W09-2301	[u'decoding with syntactic and non syntactic phrases in a syntax based machine translation system ']	N03-1017	[u'\nthe dominance of traditional phrase-based statistical machine translation (pbsmt) models (koehn etal., 2003) ', <papid> N03-1017 </papid>, u'has recently been challenged by the development and improvement of number of new models that explicity take into account the syntax of the sentences being translated.']
ZF1267	 W09-2306	[u'a study of translation rule classification for syntax based statistical machine translation ']	N03-1017	[u'\nphrase-based statistical machine translation models (marcu and wong, 2002; ', <papid> W02-1018 </papid>, u'koehn et al, 2003; ', <papid> N03-1017 </papid>, u'och and ney, 2004; ', <papid> J04-4002 </papid>, u'koehn, 2004; koehn et al, 2007)', <papid> P07-2045 </papid>, u'have achieved significant improvements in translation accuracy over the original ibm word-based model.']
ZF1268	 W09-2306	[u'a study of translation rule classification for syntax based statistical machine translation ']	J04-4002	[u'\nphrase-based statistical machine translation models (marcu and wong, 2002; ', <papid> W02-1018 </papid>, u'koehn et al, 2003; ', <papid> N03-1017 </papid>, u'och and ney, 2004; ', <papid> J04-4002 </papid>, u'koehn, 2004; koehn et al, 2007)', <papid> P07-2045 </papid>, u'have achieved significant improvements in translation accuracy over the original ibm word-based model.']
ZF1689	 W09-2309	[u'reordering model using syntactic information of a source tree for statistical machine translation ']	J04-4002	[u'\na popular statistical machine translation paradigms is the phrase-based model (koehn et al , 2003; ochand ney, 2004).', <papid> J04-4002 </papid>]
ZF1791	 W09-1908	[u'proactive learning for building machine translation systems for minority languages ']	N03-1017	[u'\nwhile the amount of parallel data required to build such systems is orders of magnitude smaller than corresponding phrase based statistical systems (koehn et al, 2003), ', <papid> N03-1017 </papid>, u'the variety of linguistic annotation required is greater.']
ZF1794	 W09-1908	[u'proactive learning for building machine translation systems for minority languages ']	P05-1033	[u'\nrecent research in syntax based machine translation (yamada and knight,2001; ', <papid> P01-1067 </papid>, u'chiang, 2005) ', <papid> P05-1033 </papid>, u'incorporates syntactic information to ameliorate the reordering problem faced by pb-smt approaches.']
I33	 E09-1003	[u'on the use of comparable corpora to improve smt performance ']	N03-1017	[u'\nit is today common practice to use phrases as translation units (koehn et al, 2003; ', <papid> N03-1017 </papid>, u'och andney, 2003) ', <papid> J03-1002 </papid>, u'instead of the original word-based ap proach.']
I81	 E12-1073	[u'validation of sub sentential paraphrases acquired from parallel monolingual corpora ']	J04-4002	[u'\n(giza) the giza++ tool (och and ney, 2004) ', <papid> J04-4002 </papid>, u'computes statistical word alignment models of increasing complexity from parallel corpora.']
I129	 E12-1018	[u'tree representations in probabilistic models for extended named entities detection ']	P97-1003	[u'\na similar lexicalized model has been proposed also by collins (collins, 1997)', <papid> P97-1003 </papid>]
I291	 E09-1062	[u'performance confidence estimation for automatic summarization ']	C00-1072	[u'\nlog-likelihood ratio for words in the input number of topic signature words (lin and hovy,2000; ', <papid> C00-1072 </papid>, u'conroy et al, 2006) ', <papid> P06-2020 </papid>, u'and percentage of signature words in the vocabulary.']
I438	 E09-1092	[u'deriving generalized knowledge from corpora using wordnet abstraction ']	P97-1003	[u'\n((:i (:q det named-entity) enter[v] (:q the room[n])) (:i (:q det female-individual) have[v] (:q det room[n])) (:i (:q det female-individual) sleep[v]) (:i (:q det female-individual) have[v] (:q det (:f plur clothe[n]))) (:i (:q det (:f plur clothe[n])) washed[a])) here the upper-case sentences are automatically generated verbalizations of the abstracted lfs shown beneath them.1 the initial development of knext was based on the hand-constructed parse trees in the penn treebank version of the brown corpus, but subsequently schubert and collaborators refined and extended the system to work with parse trees obtained with statistical parsers (e.g., that of collins(1997) ', <papid> P97-1003 </papid>, u'or charniak (2000)) ', <papid> A00-2018 </papid>, u'applied to larger corpora, such as the british national corpus (bnc), 100 million-word, mixed genre collection, along with web corpora of comparable size (see work ofvan durme et al (2008) and van durme and schubert (2008) for details).']
I465	 E09-1026	[u'semi supervised semantic role labeling ']	H05-1047	[u'\nand erk, 2005), and the modeling of textual entailment relations (tatu and moldovan, 2005).', <papid> H05-1047 </papid>]
I513	 E09-1020	[u'an alignment algorithm using belief propagation and a structure based distortion model ']	W03-0301	[u'\nevaluation was done with the scripts and gold standard provided during the workshop hlt-naacl 20031 (mihalcea and pedersen, 2003).', <papid> W03-0301 </papid>]
ZG2	 W10-1604	[u'using common sense to generate culturally contextual ized machine translation ']	N03-1017	[u'\nnowadays, the majority of the researches has being centered around the phrase-based statistical mt (pb-smt) approach such as (koehn et al., 2003) ', <papid> N03-1017 </papid>, u'and (och and ney, 2004).', <papid> J04-4002 </papid>]
ZG3	 W10-1604	[u'using common sense to generate culturally contextual ized machine translation ']	J04-4002	[u'\nnowadays, the majority of the researches has being centered around the phrase-based statistical mt (pb-smt) approach such as (koehn et al., 2003) ', <papid> N03-1017 </papid>, u'and (och and ney, 2004).', <papid> J04-4002 </papid>]
ZG125	 W10-3222	[u'chained machine translation using morphemes as pivot language ']	N03-1017	[u'\nthe authors assume the reader to be familiar with current approaches to machine translation, so that we briefly introduce the phrase based statistical machine translation model(koehn et al, 2003) ', <papid> N03-1017 </papid>, u'here, which is the foundation of chained smt system.']
R0	 P06-2066	[u'mildly non projective dependency structures ']	P99-1065	[u'\ndependency-based representations have become increasingly popular in syntactic parsing, especially for languages that exhibit free or flexible word order, such as czech (collins et al, 1999), ', <papid> P99-1065 </papid>, u'bulgarian (marinov and nivre, 2005), and turkish (eryigitand oflazer, 2006).']
R1	 P06-2066	[u'mildly non projective dependency structures ']	P05-1013	[u'\nthis is especially relevant for languages with free or flexible word order.however, recent results in non-projective dependency parsing, especially using data-driven methods, indicate that most non-projective structures required for the analysis of natural language are very nearly projective, differing only minimally from the best projective approximation (nivre and nilsson, 2005; ', <papid> P05-1013 </papid>, u'hall and novk, 2005; mcdonald and pereira, 2006).', <papid> E06-1011 </papid>]
R5	 P06-2066	[u'mildly non projective dependency structures ']	C96-1058	[u'\nthe projectivity constraint also leads to favourable parsing complexities: chart-based parsing of projective dependency grammars can be done in cubic time(eisner, 1996); ', <papid> C96-1058 </papid>, u'hard-wiring projectivity into deterministic dependency parser leads to linear-time parsing in the worst case (nivre, 2003).']
R17	 P05-1038	[u'lexicalization in cross linguistic probabilistic parsing the case of french ']	P99-1065	[u'\ninterest in parsing models for languages other than english has been growing, starting with work on czech (collins et al, 1999) ', <papid> P99-1065 </papid>, u'and chinese (bikel and chiang, 2000; ', <papid> W00-1201 </papid>, u'levy and manning, 2003).', <papid> P03-1056 </papid>, u'probabilistic parsing for german has also been explored by range of authors (dubey and keller, 2003; ', <papid> P03-1013 </papid>, u'schiehlen, 2004).', <papid> C04-1056 </papid>]
R22	 P05-1038	[u'lexicalization in cross linguistic probabilistic parsing the case of french ']	P97-1003	[u'\nin general, these authors have found that existing lexicalized parsing models for english (e.g., collins 1997) ', <papid> P97-1003 </papid>, u'do not straightforwardly generalize to new languages; this typically manifests itself in severe reduction in parsing performance compared to the results for english.']
R86	 P06-1123	[u'empirical lower bounds on the complexity of translational equivalence ']	N03-1017	[u'\nis relevant to finite-state phrase-based models thatuse no parse trees (koehn et al , 2003), ', <papid> N03-1017 </papid>, u'tree-to string models that relyon one parse tree (yamada and knight, 2001), ', <papid> P01-1067 </papid>, u'and tree-to-tree models that relyon two parse trees (groves et al , 2004, ', <papid> C04-1154 </papid>, u'e.g.).']
R106	 P06-1123	[u'empirical lower bounds on the complexity of translational equivalence ']	W03-0301	[u'\nthe french/english data were those used by mihalcea and pedersen (2003).', <papid> W03-0301 </papid>]
R120	 P06-1123	[u'empirical lower bounds on the complexity of translational equivalence ']	P05-1033	[u'\nonly 9 of the 171 cases (5.2%) could be covered by phrases of length 10 or less.analogous techniques for tree-structured translation models involve either allowing each nonterminal to generate both terminals and other nonterminals (groves et al , 2004; ', <papid> C04-1154 </papid>, u'chiang, 2005), ', <papid> P05-1033 </papid>, u'or, given constraining parse tree, to flatten?']
M122	 I08-4001	[u'an example based decoder for spoken language machine translation ']	P05-1033	[u'\nwe can learn these fixed structures and take them as rules, chiang (2005) ', <papid> P05-1033 </papid>, u'presents method to learn these rules, and uses them in the smt.']
M222	 I08-4007	[u'stochastic dependency parsing based on a admissible search ']	C96-1058	[u'\nall these willnot be so simple provided that conventional constituency grammar is used.in the dependency parsing paradigm, several deterministic, stochastic or machine-learning-based parsing algorithms have been proposed (eisner, 1996; ', <papid> C96-1058 </papid>, u'covington 2001; kudo and matsumoto, 2002; ', <papid> W02-2016 </papid>, u'yamada and matsumoto, 2003; nivre 2003; nivre and scholz, 2004; ', <papid> C04-1010 </papid>, u'chen et al, 2005).']
M229	 I08-8001	[u'transformation based sentence splitting method for statistical machine translation ']	N03-1017	[u'\nhowever, reordering models in traditional phrase-based systems are not sufficient to treat such complex cases when we translate long sentences (koehn et al  2003).', <papid> N03-1017 </papid>]
O9	 N04-1032	[u'shallow semantic parsing of chinese ']	P99-1065	[u'\nthe collins parser is state-of-the-art statistical parser that has high performance on english (collins, 1999) and czech(collins et al  1999).', <papid> P99-1065 </papid>]
O412	 N07-1017	[u'the domain restriction hypothesis relating term similarity and semantic consistency ']	P02-1006	[u'\nto do so, espresso uses slight modification of the state of the art algorithm described in (ravichandran and hovy, 2002).', <papid> P02-1006 </papid>]
O447	 N06-1024	[u'fully parsing the penn treebank ']	P97-1003	[u'\nan early exception to this was (collins,1997) ', <papid> P97-1003 </papid>, u'itself, where model 2 used function tags during the training process for heuristics to identify arguments (e.g., the tmp tag on the np in figure 1 disqualifies the np-tmp from being treated as anargument).']
O669	 N06-1017	[u'unknown word sense detection as outlier detection ']	H05-1047	[u'\nthis is problematic for the use of framenet analyses as basis for inferences over text, as e.g. in tatu and moldovan (2005).', <papid> H05-1047 </papid>]
O711	 N06-1056	[u'learning for semantic parsing with statistical machine translation ']	P05-1033	[u'\ncomplete mrs are then formed by combining these nl sub strings and their translations under parsing framework called the synchronous cfg (aho and ullman, 1972),which forms the basis of most existing statistical syntax-based translation models (yamada and knight, 2001; ', <papid> P01-1067 </papid>, u'chiang, 2005).', <papid> P05-1033 </papid>]
O770	 N06-1022	[u'multilevel coarsetofine pcfg parsing ']	P05-1012	[u'\nmost recently, mcdonald et al (2005) ', <papid> P05-1012 </papid>, u'have implemented dependency parser with good accuracy (it is almost as good at dependency parsing as charniak (2000)) ', <papid> A00-2018 </papid>, u'and very impressive speed (it is about ten times faster than collins (1997) ', <papid> P97-1003 </papid>, u'and four times faster than charniak (2000)).', <papid> A00-2018 </papid>]
O772	 N06-1022	[u'multilevel coarsetofine pcfg parsing ']	P97-1003	[u'\nmost recently, mcdonald et al (2005) ', <papid> P05-1012 </papid>, u'have implemented dependency parser with good accuracy (it is almost as good at dependency parsing as charniak (2000)) ', <papid> A00-2018 </papid>, u'and very impressive speed (it is about ten times faster than collins (1997) ', <papid> P97-1003 </papid>, u'and four times faster than charniak (2000)).', <papid> A00-2018 </papid>]
O859	 N04-1041	[u'automatically labeling semantic classes ']	P03-1001	[u'\nthere have been several approaches to automatically discovering lexico-semantic information from text (hearst 1992; ', <papid> C92-2082 </papid>, u'riloff and shepherd 1997; ', <papid> W97-0313 </papid>, u'riloff and jones 1999; berland and charniak 1999; ', <papid> P99-1008 </papid>, u'pantel and lin 2002; fleischman et al  2003; ', <papid> P03-1001 </papid>, u'girju et al  2003).']
O880	 N03-1017	[u'statistical phrase based translation ']	P97-1003	[u'\nto identify these, we use word-aligned corpus annotated with parse trees generated by statistical syntactic parsers [collins, 1997; ', <papid> P97-1003 </papid>, u'schmidt and schulte im walde, 2000].']
ZH701	 W11-1203	[u'learning the optimal use of dependency parsing information for finding translations with comparable corpora ']	P05-1012	[u'\nthe english corpus nhtsa waspos-tagged and stemmed with stepp tagger (tsuruoka et al, 2005; okazaki et al, 2008) ', <papid> D08-1047 </papid>, u'and dependency parsed using the mst parser (mcdonald et al., 2005).', <papid> P05-1012 </papid>]
ZH1363	 W10-4006	[u'multiword expression sensitive word alignment ']	N03-1017	[u'\nit alone does not effectively capture many-to-many word correspondences, but instead relies on the ability of subsequent heuristic phrase extraction algorithms, such as grow-diag final (koehn et al , 2003), ', <papid> N03-1017 </papid>, u'to resolve them.']
ZH1384	 W11-0103	[u'deterministic statistical mapping of sentences to underspecified semantics ']	C96-1058	[u'\nthis encoding allows us to perform the text-to-nlf mapping using any existing statistical methods for labeled dependency parsing (e.g. eisner (1996), ', <papid> C96-1058 </papid>, u'yamada and matsumoto (2003), mcdonald, crammer, pereira (2005)).']
ZH1643	 W11-1402	[u'understanding differences in perceived peer review helpfulness using natural language processing ']	P05-1012	[u'\n3we used mst parser (mcdonald et al, 2005) ', <papid> P05-1012 </papid>, u'for syntactic analysis.']
ZH1649	 W11-1402	[u'understanding differences in perceived peer review helpfulness using natural language processing ']	C00-1072	[u'\nour domain topic set contains 288 words extracted from the collection of student papers using topic-lexicon extraction software5; our feature (domainword)5the software extracts topic words based on topic signatures (lin and hovy, 2000), ', <papid> C00-1072 </papid>, u'and was kindly provided by annie louis.']
ZH1803	 W11-0112	[u'integrating logical representations with probabilistic information using markov logic ']	H05-1079	[u'\nthey, therefore, tend to have high precision at the cost of low recall (bos and markert, 2005).', <papid> H05-1079 </papid>, u'recent advances in computational linguistics have yielded robust methods that use weighted or probabilistic models.']
ZH1917	 W11-1611	[u'evaluating sentence compression pitfalls and suggested remedies ']	A00-1043	[u'\na syntactic approach considers the alignment over parse trees (jing, 2000), ', <papid> A00-1043 </papid>, u'and similar technique has been used with dependency trees to evaluate the quality of sentence fusions (marsi and krahmer, 2005).', <papid> W05-1612 </papid>]
ZH2026	 W11-1015	[u'a general purpose rule extractor for scfgbased machine translation ']	P05-1033	[u'\nsome scfg rule extraction techniques require only viterbi word alignment links between the source and target sides of the input corpus (chiang, 2005), ', <papid> P05-1033 </papid>, u'while methods based on linguistic constituency structure require the source and/or target side of the input to be parsed.']
ZH2038	 W11-1015	[u'a general purpose rule extractor for scfgbased machine translation ']	N03-1017	[u'\nthis is analogous to the word-alignment consistency constraint of phrase-based smt phrase extraction (koehn et al, 2003).', <papid> N03-1017 </papid>]
ZH2059	 W11-1007	[u'improving reordering for statistical machine translation with smoothed priors and syntactic features ']	N03-1017	[u'\nword order in the translation output relies on how the phrases are reordered based on both language model scores and distortion cost/penalty (koehn et al, 2003), ', <papid> N03-1017 </papid>, u'among allthe features utilized in maximum-entropy (loglinear) model (och and ney, 2002).', <papid> P02-1038 </papid>]
ZH2063	 W11-1007	[u'improving reordering for statistical machine translation with smoothed priors and syntactic features ']	J04-4002	[u'\nin (och and ney,2004; ', <papid> J04-4002 </papid>, u'tillmann, 2004; ', <papid> N04-4026 </papid>, u'kumar and byrne, 2005; ', <papid> H05-1021 </papid>, u'al onaizan and papineni, 2006; xiong et al, 2006;', <papid> P06-1066 </papid>, u'zens and ney, 2006; ', <papid> W06-3108 </papid>, u'ni et al, 2009), ', <papid> P09-2061 </papid>, u'people presented models that use lexical features from the phrases to predict their orientations.']
ZH2613	 W11-1215	[u'cross lingual slot filling from comparable corpora ']	N04-1033	[u'\n3.3.3 document and name translation we use statistical, phrase-based mt system (zens and ney, 2004) ', <papid> N04-1033 </papid>, u'to translate chinese documents into english for type approaches.']
ZH2890	 W11-0303	[u'punctuation making a point in unsupervised dependency parsing ']	P97-1003	[u'\nstatistical parsers reap dramatic gains from punctuation (engel et al , 2002; ', <papid> W02-1007 </papid>, u'roark, 2001; charniak, 2000; ', <papid> A00-2018 </papid>, u'johnson, 1998; ', <papid> J98-4004 </papid>, u'collins, 1997, ', <papid> P97-1003 </papid>, u'inter alia).']
ZH3065	 W11-1302	[u'distributed structures and distributional meaning ']	W05-1203	[u'\nalong with the previous task specific kernels, weuse simpler feature (lex) that is extremely effective in determining the entailment between andh . this simple feature is the lexical similarity between andh computed using wordnet-based metrics asin (corley and mihalcea, 2005).', <papid> W05-1203 </papid>]
ZH3197	 W11-1012	[u'utilizing target side semantic role labels to assist hierarchical phrase based machine translation ']	P05-1033	[u'\ntranslation proposed by chiang (2005), ', <papid> P05-1033 </papid>, u'the hierarchicalphrase-based machine translation model (commonly known as the hiero model) has achieved results comparable, if not superior, to conventional phrase-based approaches.']
ZH3587	 W11-0417	[u'empty categories in hindi dependency treebank analysis and recovery ']	P97-1003	[u'\nadditionally, it is evident that successful detection of such nodes will help the annotation process as well.there have been many approaches for the recovery of empty categories in the treebanks like penn treebank, both ml based (collins, 1997; ', <papid> P97-1003 </papid>, u'johnson, 2002; dienes and dubey, 2003', <papid> W03-1005 </papid>, u'a,b; higgins, 2003)', <papid> E03-1049 </papid>, u'and rule based (r campbell, 2004).']
ZH3817	 W11-0131	[u'structured composition of semantic vectors ']	P97-1003	[u'\nthe structured vector ial semantic framework subsumes variants of several common parsing algorithms, two of which will be discussed:lexicalized parsing (charniak, 1996; collins, 1997, ', <papid> P97-1003 </papid>, u'etc.) and relational clustering (akin to latent annotations (matsuzaki et al , 2005; ', <papid> P05-1010 </papid>, u'petrov et al , 2006; ', <papid> P06-1055 </papid>, u'gesmundo et al , 2009)).', <papid> W09-1205 </papid>]
ZH3847	 W11-1604	[u'comparing phrase based and syntax based paraphrase generation ']	N03-1017	[u'\none popular approach ? arguably the most successful so far ? is statistical phrase-based machine translation (pbmt), which learns phrase translation rules from aligned bilingual text corpora (och et al, 1999; ', <papid> W99-0604 </papid>, u'vogel et al, 2000; zens et al, 2002; koehn et al, 2003).', <papid> N03-1017 </papid>]
ZH3853	 W11-1604	[u'comparing phrase based and syntax based paraphrase generation ']	J04-4002	[u'\nprior work has explored the use of pbmt for paraphrase generation (quirk etal., 2004; ', <papid> W04-3219 </papid>, u'bannard and callison-burch, 2005; madnani et al, 2007; ', <papid> W07-0716 </papid>, u'callison-burch, 2008; zhao et al, 2009; ', <papid> P09-1094 </papid>, u'wubben et al, 2010) ', <papid> W10-4223 </papid>, u'however, since many researchers believe thatpbmt has reached performance ceiling, ongoing research looks into more structural approaches to statistical mt (marcu and wong, 2002; ', <papid> W02-1018 </papid>, u'och andney, 2004; ', <papid> J04-4002 </papid>, u'khalilov and fonollosa, 2009).']
ZH3972	 W11-1605	[u'text specificity and impact on quality of news summaries ']	A00-2024	[u'\njing and mckeown (2000) ', <papid> A00-2024 </papid>, u'studied what edits people use to create summaries from sentences in the source text.']
ZH4665	 W11-1006	[u'an evaluation and possible improvement path for current smt behavior on ambiguous nouns ']	N03-1017	[u'\nit combines phrase translation model (which is based onthe noisy channel model) and phrase-based decoder in order to find the most probable translation of foreign sentence (koehn et al, 2003).', <papid> N03-1017 </papid>]
ZH4689	 W11-1610	[u'paraphrastic sentence compression with a character based metric tightening without deletion ']	N03-1017	[u'\nthe corresponding foreign phrase (festgenom men) is identified using word alignment and phrase extraction techniques from phrase-based statistical machine translation (koehn et al, 2003).', <papid> N03-1017 </papid>]
ZH4840	 W11-1606	[u'towards strict sentence intersection decoding and evaluation strategies ']	A00-2024	[u'\nthis distinguishes our approach from traditional sentence fusion approaches (jing and mckeown, 2000; ', <papid> A00-2024 </papid>, u'barzilay and mckeown, 2005; ', <papid> J05-3002 </papid>, u'filippova and strube, 2008', <papid> W08-1105 </papid>, u'b) which generally attempt to retain common information but are typically evaluated in an abs tractive summarization context in which additional information in the fusion output does not negatively impact judgments.']
ZH4999	 W10-4110	[u'mining largescale parallel corpora from multilingual patents an english chinese example and its application to smt ']	J04-4002	[u'\nfor statistical machine translation (smt), tremendous strides have been made in two decades, including brown et al (1993), ', <papid> J93-2003 </papid>, u'och and ney (2004) ', <papid> J04-4002 </papid>, u'and chiang (2007).', <papid> J07-2003 </papid>]
ZH5111	 W11-1009	[u'a dependency based statistical translation model ']	N03-1017	[u'\nseveral efforts are being made to incorporate syntactic analysis into phrase-base statistical translation (pbt) (och 2002; koehn et. al. 2003), ', <papid> N03-1017 </papid>, u'which represents the state of the art in terms of robustness in modeling local word reordering and efficiency in decoding.']
ZH5119	 W11-1009	[u'a dependency based statistical translation model ']	P05-1033	[u'\nmany similar approaches are based on constituent grammars, among which we mention (chiang, 2005) ', <papid> P05-1033 </papid>, u'who introduced hierarchical translation models.']
ZH5293	 W11-1708	[u'a link to the past constructing historical social networks ']	P02-1006	[u'\nrelation types and labels are then deduced from the most common patterns (ravichandran and hovy, 2002; ', <papid> P02-1006 </papid>, u'culotta etal, 2006).', <papid> N06-1038 </papid>]
C147	 C08-1107	[u'learning entailment rules for unary templates ']	P02-1006	[u'\n(ravichandran and hovy, 2002; ', <papid> P02-1006 </papid>, u'szpektor et al, 2004; ', <papid> W04-3206 </papid>, u'sekine, 2005).', <papid> I05-5011 </papid>, u'directional measures most rule learning methods apply symmetric similarity measure between two templates, viewing them as paraphrasing eachother.']
C333	 C10-1057	[u'fact rank random walks on a web of facts ']	P02-1006	[u'\nrecent work has also explored extraction-related issues such as, scal ability (pasca et al , 2006; ravichandran and hovy, 2002; ', <papid> P02-1006 </papid>, u'pantel et al , 2004; ', <papid> C04-1111 </papid>, u'etzioni et al , 2004), learning extraction schemas (cafarella etal., 2007a; banko et al , 2007), and organizing extracted facts (cafarella et al , 2007b).']
C593	 C10-2176	[u'automatic treebank conversion via informed decoding ']	P99-1065	[u'\ncollins et al (1999) ', <papid> P99-1065 </papid>, u'1the terminology decoding is referred to the parsing phase of parser.2note that although we use chinese treebanks, our approach is language independent.']
C1194	 C10-1118	[u'discriminative induction of subtree alignment using limited labeled data ']	N03-1017	[u'\nutilizing the syntactic rules only has been argued to be ineffective (koehn et al, 2003).', <papid> N03-1017 </papid>]
C1322	 C10-1039	[u'opinosis a graph based approach to abs tractive summarization of highly redundant opinions ']	A00-2024	[u'\nexisting work in abs tractive summarization has been quite limited and can be categorized into two categories: (1) approaches using prior knowledge (radev and mckeown, 1998) (', <papid> J98-3005 </papid>, u'finley and harabagiu, 2002) (dejong, 1982) and(2) approaches using natural language generation (nlg) systems (saggion and lapalme, 2002) (', <papid> J02-4005 </papid>, u'jing and mckeown, 2000).', <papid> A00-2024 </papid>]
C1331	 C10-1039	[u'opinosis a graph based approach to abs tractive summarization of highly redundant opinions ']	W00-0403	[u'\nthus, we use mead (radev et al, 2000)', <papid> W00-0403 </papid>, u'as our baseline.']
C1332	 C10-1051	[u'a novel reordering model based on multilayer phrase for statistical machine translation ']	J04-4002	[u'\nsome of these models are content independent, such as distortion models (och and ney, 2004; ', <papid> J04-4002 </papid>, u'koehn et al, 2003) which penalize translation according to jump distance of phrases, and flat reordering model (wu, 1995; zens et al, 2004)', <papid> C04-1030 </papid>, u'which assigns constant probabilities for monotone order and non-monotone order.']
C1340	 C10-1051	[u'a novel reordering model based on multilayer phrase for statistical machine translation ']	P05-1033	[u'\nthe formally syn tax-based models use synchronous context-free grammar (scfg) but induce grammar from parallel text without relying on any linguistic annotations or assumptions (chiang, 2005; ', <papid> P05-1033 </papid>, u'xiong et al, 2006).', <papid> P06-1066 </papid>]
B39	 C04-1051	[u'unsupervised construction of large paraphrase corpora exploiting massively parallel news sources ']	W03-0301	[u'\nwhile these models have proven effective at the word alignment task (mihalcea  pedersen 2003), ', <papid> W03-0301 </papid>, u'there are significant practical limitations in their output.']
B175	 C04-1002	[u'linear time dependency analysis for japanese ']	C96-1058	[u'\nin english as well as in japanese,dependency analysis has been studied (e.g., (laf ferty et al, 1992; collins, 1996; ', <papid> P96-1025 </papid>, u'eisner, 1996)).', <papid> C96-1058 </papid>]
G35	 D08-1066	[u'phrase translation probabilities with itg priors and smoothing as learning objective ']	N03-1017	[u'\na major component in phrase-based statistical machine translation (pbsmt) (zens et al , 2002;koehn et al , 2003) ', <papid> N03-1017 </papid>, u'is the table of conditional probabilities of phrase translation pairs.']
G60	 D08-1066	[u'phrase translation probabilities with itg priors and smoothing as learning objective ']	P05-1033	[u'\nwe use binary synchronous context free grammar (bscfg), based on inversion transduction grammar (itg) (wu, 1997; ', <papid> J97-3002 </papid>, u'chiang, 2005', <papid> P05-1033 </papid>, u'a), to define the set of eligible segment ations for an aligned sentence pair.']
G96	 D08-1066	[u'phrase translation probabilities with itg priors and smoothing as learning objective ']	J04-4002	[u'\n(koehn et al , 2003; ', <papid> N03-1017 </papid>, u'och and ney, 2004)).', <papid> J04-4002 </papid>]
G159	 D07-1111	[u'dependency parsing and domain adaptation with lr models and parser ensembles ']	P05-1013	[u'\nthe resulting algorithm is projective, and non projectivity is handled by pseudo-projective transformations as described in (nivre and nilsson, 2005).', <papid> P05-1013 </papid>]
G188	 D09-1135	[u'refining grammars for parsing with hierarchical semantic knowledge ']	P97-1003	[u'\nlexicalized pcfgs use the structural features on the lexical head of phrasal node in tree, and get significant improvements for parsing (collins, 1997; ', <papid> P97-1003 </papid>, u'charniak, 1997; collins, 1999; charniak, 2000).', <papid> A00-2018 </papid>]
G211	 D07-1036	[u'improving statistical machine translation performance by training data selection and optimization ']	N03-1017	[u'\nmethod to refine it (koehn et al , 2003).', <papid> N03-1017 </papid>]
G218	 D09-1023	[u'feature rich translation by quasi synchronous lattice parsing ']	N03-1017	[u'\n1 if we view mt as machine learning problem, feature sand formalisms imply structural independence assumptions, which are in turn exploited by efficient inference algorithms, including decoders (koehn et al, 2003; ', <papid> N03-1017 </papid>, u'yamada and knight, 2001).', <papid> P01-1067 </papid>]
G228	 D09-1023	[u'feature rich translation by quasi synchronous lattice parsing ']	P05-1033	[u'\n5additionally, since phrase features can be any function of words and alignments, we permit features that consider phrase pairs in which target word outside the target phrase aligns to source word inside the source phrase, as well as phrase pairs with gaps (chiang, 2005; ', <papid> P05-1033 </papid>, u'ittycheriah and roukos, 2007).', <papid> N07-1008 </papid>]
G368	 D07-1038	[u'syntactic realignment models for machine translation ']	J04-4002	[u'\nexamples of realistic systems are the phrase-based ats system of och and ney (2004), ', <papid> J04-4002 </papid>, u'the phrasal-syntax hybrid systemhiero (chiang, 2005), ', <papid> P05-1033 </papid>, u'and the ghkm syntax system (galley et al, 2004; galley et al, 2006).', <papid> P06-1121 </papid>]
G369	 D07-1038	[u'syntactic realignment models for machine translation ']	P05-1033	[u'\nexamples of realistic systems are the phrase-based ats system of och and ney (2004), ', <papid> J04-4002 </papid>, u'the phrasal-syntax hybrid systemhiero (chiang, 2005), ', <papid> P05-1033 </papid>, u'and the ghkm syntax system (galley et al, 2004; galley et al, 2006).', <papid> P06-1121 </papid>]
G450	 D08-1011	[u'indirecthmmbased hypothesis alignment for combining outputs from machine translation systems ']	N03-1017	[u'\nsys-13 is phrasal system proposed by koehn et al (2003), ', <papid> N03-1017 </papid>, u'sys-14 is hierarchical system proposed by chiang (2007), and sys-15 is syntax-based system proposed by galley et al (2006).', <papid> P06-1121 </papid>]
G475	 D07-1089	[u'multiple alignment of citation sentences with conditional random fields and posterior decoding ']	W03-0301	[u'\nas point of comparison, the smt community has been evaluating performance of word-alignment systems on an even smaller dataset of 447 pairs of non-overlapping sentences (mihalcea and pedersen, 2003).', <papid> W03-0301 </papid>]
G559	 D09-1136	[u'bayesian learning of phrasal treetostring templates ']	N03-1017	[u'\nthe word alignment is computed using giza++ 2 for the selected 73,597 sentence pairs inthe fbis corpus in both directions and then combined using union and heuristic diagonal growing (koehn et al, 2003).', <papid> N03-1017 </papid>]
G596	 D10-1035	[u'unsupervised discovery of negative categories in lexicon bootstrapping ']	P02-1006	[u'\nautomatically acquiring semantic lexicons from textis essential for overcoming the knowledge bottleneck in many nlp tasks, e.g. question answering (ravichandran and hovy, 2002).', <papid> P02-1006 </papid>]
G613	 D07-1090	[u'large language models in machine translation ']	J04-4002	[u'\nthe mathematics of the problem were formalized by (brown et al, 1993), ', <papid> J93-2003 </papid>, u'and re-formulated by (och and ney, 2004) ', <papid> J04-4002 </papid>, u'in terms of the optimization e?']
G1019	 D08-1050	[u'adapting a lexicalized grammar parser to contrasting domains ']	P97-1003	[u'\njudge et al (2006) ', <papid> P06-1063 </papid>, u'produced corpus of 4,000questions annotated with syntactic trees, and obtained an improvement in parsing accuracy for bikels re implementation of the collins parser (collins, 1997) ', <papid> P97-1003 </papid>, u'by training new parser model with combination of newspaper and question data.']
G1095	 D09-1040	[u'improved statistical machine translation using monolinguallyderived paraphrases ']	N03-1017	[u'\nphrase-based systems, flat and hierarchical alike (koehn et al , 2003; ', <papid> N03-1017 </papid>, u'koehn, 2004', <papid> W04-3250 </papid>, u'b; koehn et al , 2007; ', <papid> P07-2045 </papid>, u'chiang, 2005; ', <papid> P05-1033 </papid>, u'chiang, 2007), ', <papid> J07-2003 </papid>, u'have achieveda much better translation coverage than word based ones (brown et al , 1993), but untranslated words remain major problem in smt.']
G1098	 D09-1040	[u'improved statistical machine translation using monolinguallyderived paraphrases ']	P05-1033	[u'\nphrase-based systems, flat and hierarchical alike (koehn et al , 2003; ', <papid> N03-1017 </papid>, u'koehn, 2004', <papid> W04-3250 </papid>, u'b; koehn et al , 2007; ', <papid> P07-2045 </papid>, u'chiang, 2005; ', <papid> P05-1033 </papid>, u'chiang, 2007), ', <papid> J07-2003 </papid>, u'have achieveda much better translation coverage than word based ones (brown et al , 1993), but untranslated words remain major problem in smt.']
G1163	 D07-1123	[u'incremental dependency parsing using online learning ']	P05-1013	[u'\nwhile the parsing algorithm produces projective trees only, non projective arcs can be handled using preprocessing step before training the model and postprocessing step after parsing the sentences.the projectivization algorithm (nivre and nilsson, 2005) ', <papid> P05-1013 </papid>, u'iteratively moves each non projective arc upward in the tree until the whole tree is projective.']
G1739	 D09-1141	[u'improved statistical machine translation for resource poor languages using related resource rich languages ']	J04-4002	[u'\nwe then built separate directed word alignments for englishx andxenglish (x?{indonesian, spanish}) using ibm model 4(brown et al, 1993), ', <papid> J93-2003 </papid>, u'combined them using the in tersect+grow heuristic (och and ney, 2003), ', <papid> J03-1002 </papid>, u'and extracted phrase-level translation pairs of maximum length seven using the alignment template approach (och and ney, 2004).', <papid> J04-4002 </papid>]
G1882	 D09-1008	[u'effective use of linguistic and contextual information for statistical machine translation ']	P05-1033	[u'\nmarton and resnik (2008) ', <papid> P08-1114 </papid>, u'introduced features defined on constituent labels to improve the hiero system (chiang, 2005).', <papid> P05-1033 </papid>]
G1975	 D07-1096	[u'the conll 2007 shared task on dependency parsing ']	P97-1003	[u'\neven before the 2006 shared task, the parsers of collins (1997) ', <papid> P97-1003 </papid>, u'and charniak (2000), originally developed for english, had been adapted for dependency parsing of czech, and the parsing methodology proposed by kudo and matsumoto (2002) ', <papid> W02-2016 </papid>, u'and yamada and matsumoto (2003) had been evaluated on both japanese and english.']
G2532	 D08-1052	[u'ltag dependency parsing with bidirectional incremental construction ']	P05-1012	[u'\npenn treebank was previously used to train and evaluate various dependency parsers (yamada and matsumoto, 2003; mcdonald et al, 2005).', <papid> P05-1012 </papid>]
G2989	 D07-1077	[u'chinese syntactic reordering for statistical machine translation ']	P05-1033	[u'\nmore recently developed hierarchical systems (e.g., (yamada and knight, 2001; ', <papid> P01-1067 </papid>, u'chiang, 2005; ', <papid> P05-1033 </papid>, u'marcu et al, 2006)) ', <papid> W06-1606 </papid>, u'may be better equipped to deal with reordering of this type; how ever, in this example they would effectively have to first identify the span of the relative clause, and then move it into the correct position, without any explicit representation of the source language syntax.in this paper, we describe set of syntactic reordering rules that exploit systematic differences between chinese and english word order.']
G3023	 D07-1077	[u'chinese syntactic reordering for statistical machine translation ']	P97-1003	[u'\ndev nist06 baseline 31.57 28.52 reorder 32.86 30.86 gain +1.29 +2.34 table 2: bleu score of the baseline and reordered systems.presented in collins (1997).', <papid> P97-1003 </papid>]
G3146	 D07-1015	[u'structured prediction models via the matrix tree theorem ']	P05-1012	[u'\nthese structures are equivalent to non-projective dependency parses (mcdonald et al, 2005', <papid> P05-1012 </papid>, u'b), and more generally could be relevant to any task that involves learning mapping from graph to an underlying spanning tree.']
G3201	 D07-1015	[u'structured prediction models via the matrix tree theorem ']	C96-1058	[u'\ndecoding can be carried out using viterbistyle dynamic-programming algorithms, for example the o(n3) algorithm of eisner (1996).', <papid> C96-1058 </papid>]
G3298	 D07-1100	[u'multilingual dependency parsing using global features ']	P05-1012	[u'\nin order to find solution using the marginal distribution, weadopt the maximum spanning tree (mst) framework proposed by mcdonald et al (2005', <papid> P05-1012 </papid>, u'a).']
G3300	 D07-1100	[u'multilingual dependency parsing using global features ']	C96-1058	[u'\nthebest projective parse tree is obtained using the eisner algorithm (eisner, 1996) ', <papid> C96-1058 </papid>, u'with the scores, and thebest non-projective one is obtained using the chu liu-edmonds (cle) algorithm (mcdonald et al, 2005', <papid> P05-1012 </papid>, u'b).']
G3411	 D07-1033	[u'a new perceptron algorithm for sequence labeling with non local features ']	P05-1012	[u'\niii and marcu, 2005)2, mira (crammer et al, 2006) used in (mcdonald et al, 2005), ', <papid> P05-1012 </papid>, u'and max-margin markov networks (taskar et al, 2003).']
G3649	 D08-1024	[u'online large margin training of syntactic and structural translation features ']	P05-1033	[u'\none recent example of this limitation is series of experiments by marton and resnik (2008), ', <papid> P08-1114 </papid>, u'inwhich they added syntactic features to hiero (chi ang, 2005; ', <papid> P05-1033 </papid>, u'chiang, 2007), which ordinarily uses no linguistically motivated syntactic information.']
G3663	 D08-1024	[u'online large margin training of syntactic and structural translation features ']	P05-1012	[u'\nwe follow mcdonald et al  (2005) ', <papid> P05-1012 </papid>, u'in applying this technique to mira.']
G3684	 D08-1024	[u'online large margin training of syntactic and structural translation features ']	N03-1017	[u'\nthe baseline model was hiero with the following baseline features (chiang, 2005; ', <papid> P05-1033 </papid>, u'chiang, 2007): ? two language models ? phrase translation probabilities p( | e) and p(e | ) ? lexical weighting in both directions (koehn et al ., 2003) ? ', <papid> N03-1017 </papid>, u'word penalty ? penalties for: ? automatically extracted rules?']
G3721	 D07-1056	[u'phrase reordering model integrating syntactic knowledge for smt ']	N03-1017	[u'\nin phrase-based smt systems (koehn et al , 2003; ', <papid> N03-1017 </papid>, u'koehn, 2004), foreign sentences are firstly segmented into phrases which consists of adjacent words.']
G3723	 D07-1056	[u'phrase reordering model integrating syntactic knowledge for smt ']	J04-4002	[u'\nthere have been considerable amount of efforts to improve the reordering model in smt systems, ranging from the fundamental distance-based distortion model (och and ney, 2004; ', <papid> J04-4002 </papid>, u'koehn et al , 2003), ', <papid> N03-1017 </papid>, u'flat reordering model (wu, 1996; ', <papid> P96-1021 </papid>, u'zens et al , 2004; ', <papid> C04-1030 </papid>, u'kumar et al , 2005), to lexicalized reordering model (tillmann, 2004; ', <papid> N04-4026 </papid>, u'kumar et al , 2005; koehn et al , 2005), hierarchical phrase-based model (chiang, 2005), ', <papid> P05-1033 </papid>, u'and maximum entropy-based phrase reordering model (xiong et al , 2006).', <papid> P06-1066 </papid>]
G3731	 D07-1056	[u'phrase reordering model integrating syntactic knowledge for smt ']	P05-1033	[u'\nthere have been considerable amount of efforts to improve the reordering model in smt systems, ranging from the fundamental distance-based distortion model (och and ney, 2004; ', <papid> J04-4002 </papid>, u'koehn et al , 2003), ', <papid> N03-1017 </papid>, u'flat reordering model (wu, 1996; ', <papid> P96-1021 </papid>, u'zens et al , 2004; ', <papid> C04-1030 </papid>, u'kumar et al , 2005), to lexicalized reordering model (tillmann, 2004; ', <papid> N04-4026 </papid>, u'kumar et al , 2005; koehn et al , 2005), hierarchical phrase-based model (chiang, 2005), ', <papid> P05-1033 </papid>, u'and maximum entropy-based phrase reordering model (xiong et al , 2006).', <papid> P06-1066 </papid>]
G3966	 D07-1099	[u'fast and robust multilingual dependency parsing with a generative latent variable model ']	P05-1013	[u'\nwe used the pseudo-projective transformation introduced in (nivre and nilsson, 2005) ', <papid> P05-1013 </papid>, u'to castnon-projective parsing tasks as projective.']
G4077	 D07-1097	[u'single malt or blended a study in multilingual parser optimization ']	P05-1013	[u'\nsince the parsing algorithm only produces projective dependency graphs, we may use pseudo-projective parsing to recover non-projective dependencies, i.e., projectivize training data and encode information about these transformations in extended arc labels to support deprojectivization of the parser output(nivre and nilsson, 2005).', <papid> P05-1013 </papid>]
G4509	 D09-1037	[u'a bayesian model of syntax directed tree to string grammar induction ']	N03-1017	[u'\nin contrast, standard phrase-based models (koehn et al,2003) ', <papid> N03-1017 </papid>, u'assume mostly monotone mapping between source and target, and therefore cannot adequately model these phenomena.']
G4570	 D09-1088	[u'an alternative to head driven approaches for parsing a relatively free word order language ']	P99-1065	[u'\nas the interest of the nlp community grows to encompass more languages, we observe efforts towards adapting an english parser for parsing other languages (e.g., (collins et al, 1999)), ', <papid> P99-1065 </papid>, u'or towards designing language-independent framework based on principles underlying the models for parsing english (bikel, 2002).']
G4718	 D09-1059	[u'statistical bistratal dependency parsing ']	C96-1058	[u'\nin slightly more general formulation, it was first published by eisner (1996).', <papid> C96-1058 </papid>]
G4719	 D09-1059	[u'statistical bistratal dependency parsing ']	P05-1012	[u'\nstarting from mcdonald et al (2005), ', <papid> P05-1012 </papid>, u'it has been widely used in recent statistical dependency parsing frameworks.the algorithm works by creating open structures, which consist of dependency link and the set of links that it spans, and closed structures, consisting of the left or right half of completesubtree.']
G4727	 D09-1059	[u'statistical bistratal dependency parsing ']	P05-1013	[u'\nwe used pseudo-projective edge label encoding to handle non projectivity (nivre and nilsson, 2005).', <papid> P05-1013 </papid>, u'to implement the model, we constructed feature representations ? , ? , and ? . the surface-.']
G4734	 D07-1103	[u'improving translation quality by discarding most of the phrase table ']	N03-1017	[u'\nthese joint counts are estimated using the phrase induction algorithm described in (koehn etal., 2003), ', <papid> N03-1017 </papid>, u'with symmetrized word alignments generated using ibm model 2 (brown et al, 1993).', <papid> J93-2003 </papid>]
G4754	 D08-1090	[u'language and translation model adaptation using comparable corpora ']	P05-1033	[u'\nwhile both adaptation methods are integrated into hierarchical translation model (chiang, 2005),', <papid> P05-1033 </papid>, u'they are largely implementation independent.']
G4776	 D07-1013	[u'characterizing the errors of data driven dependency parsing models ']	P05-1012	[u'\n(i,j,l)a s(i, j, l) this problem is equivalent to finding the highest scoring directed spanning tree in the graph gx originating out of the root node 0, which can be solved for both the labeled and unlabeled case in o(n2) time(mcdonald et al, 2005', <papid> P05-1012 </papid>, u'b).']
G4785	 D07-1013	[u'characterizing the errors of data driven dependency parsing models ']	P05-1013	[u'\n, wn in o(n) time, producing projective dependency graph satisfying conditions 14 in section 2.1, possibly after adding arcs (0, i, lr) for every node 6= 0 that is root in the output graph (where lr is special label for root modifiers).nivre and nilsson (2005) ', <papid> P05-1013 </papid>, u'showed how the restriction to projective dependency graphs could be lifted by using graph transformation techniques to pre process training data and post-process parser output, so-called pseudo-projective parsing.to learn transition scores, these systems use discriminative learning methods, e.g., memory-based learning or support vector machines.']
G4829	 D07-1126	[u'a multilingual dependency analysis system using online passive aggressive learning ']	P05-1012	[u'\nresearch on dependency parsing is mainly based on machine learning methods, which can be called history-based (yamada and matsumoto, 2003; nivre et al, 2006) ', <papid> W06-2933 </papid>, u'and discriminative learning methods (mcdonald et al, 2005', <papid> P05-1012 </papid>, u'a; corston-oliver et al,2006).']
G4853	 D07-1126	[u'a multilingual dependency analysis system using online passive aggressive learning ']	C96-1058	[u'\nthere are some algorithms 1149 figure 1: this is an example of dependency tree to determine these relations of each word to another words, for instance, the modified cky algorithm (eisner, 1996) ', <papid> C96-1058 </papid>, u'is used to define these relations forgiven sentence.']
G5014	 D09-1050	[u'acquiring translation equivalences of multiword expressions by normalized correlation frequencies ']	N03-1017	[u'\n(och et al, 1999; ', <papid> W99-0604 </papid>, u'koehn et al, 2003; ', <papid> N03-1017 </papid>, u'liang et al, 2006).', <papid> N06-1014 </papid>]
G5061	 D08-1078	[u'predicting success in machine translation ']	N03-1017	[u'\nthis test set is from held out portion of the europarl corpus.the automatic alignments were extracted by appending the manually aligned sentences on to the respective europarl v3 corpora and aligning the musing giza++ (och and ney, 2003) ', <papid> J03-1002 </papid>, u'and the grow final-diag algorithm (koehn et al, 2003).', <papid> N03-1017 </papid>]
G5073	 D10-1052	[u'discriminative word alignment with a function word reordering model ']	J04-4002	[u'\nwe only enforce one criterion on li,st and ri,st : they have to be the maximal alignment blocks satisfying the consistent heuristic (och and ney, 2004) ', <papid> J04-4002 </papid>, u'that endor start with yi,st on the source side respec tively.2 to infer these phrases, we decompose li,stinto (o(li,st ), d(fwi1,st ), b(s?)); similarly, ri,st into (o(ri,st ),d(fwi+1,st ), b(?/s?) )).']
G5079	 D10-1052	[u'discriminative word alignment with a function word reordering model ']	N03-1017	[u'\nin the experiments reported below, weinitialized a(1) with the model 4 alignments sym metrized by using the grow-diag-final-and heuristic (koehn et al, 2003).', <papid> N03-1017 </papid>]
G5087	 D10-1052	[u'discriminative word alignment with a function word reordering model ']	P05-1033	[u'\nfor translation experiments, we used cdec (dyeret al, 2010), ', <papid> P10-4002 </papid>, u'fast implementation of hierarchical phrase-based translation models (chiang, 2005),', <papid> P05-1033 </papid>, u'which represents state-of-the-art translation sys tem.we constructed the list of function words in english manually and in chinese from (howard, 2002).punctuation marks were added to the list, resulting in 883 and 359 tokens in the chinese and english lists, respectively.']
G5155	 D08-1010	[u'maximum entropy based rule selection model for syntax based statistical machine translation ']	P05-1033	[u'\n(2007) showed improv ents by integrating wordsense-disambiguation (wsd) system into phrase based (koehn, 2004) and hierarchical phrase based (chiang, 2005) ', <papid> P05-1033 </papid>, u'smt system, respectively.similar to wsd, carpuat and wu (2007', <papid> D07-1007 </papid>, u'a) used contextual information to solve the ambiguity problem for phrases.']
G5167	 D08-1010	[u'maximum entropy based rule selection model for syntax based statistical machine translation ']	N03-1017	[u'\nmethod(koehn et al, 2003).', <papid> N03-1017 </papid>]
G5381	 D07-1101	[u'experiments with a higher order projective dependency parser ']	C96-1058	[u'\nwe extend the projective parsing algorithm of eisner (1996) ', <papid> C96-1058 </papid>, u'for our case,and train models using the averaged perceptron.']
G5389	 D07-1101	[u'experiments with a higher order projective dependency parser ']	P05-1012	[u'\nin turn, those features were inspired by successful previous work in first order dependency parsing (mcdonald et al, 2005).', <papid> P05-1012 </papid>, u'the most basic feature patterns consider the surface form, part-of-speech, lemma and other morphosyntactic attributes of the head or the modifier of adependency.']
G5498	 D09-1038	[u'better synchronous binarization for machine translation ']	P05-1033	[u'\nrecently statistical machine translation (smt) systems based on synchronous context free grammar (scfg) have been extensively investigated (chiang, 2005; ', <papid> P05-1033 </papid>, u'galley et al, 2004; galley et al, 2006) ', <papid> P06-1121 </papid>, u'and have achieved state-of-the-art performance.']
G5565	 D08-1077	[u'syntactic models for structural word insertion and deletion during translation ']	P05-1033	[u'\nhierarchical systems, such as (chiang, 2005) ', <papid> P05-1033 </papid>, u'in principle have the capacity to learn insertions and deletions grounded by minimal lexical cues.']
G5567	 D09-1114	[u'the feature sub space method for smt system combination ']	J04-4002	[u'\nsince the success of phrase-based methods (och and ney, 2004; ', <papid> J04-4002 </papid>, u'koehn, 2004), ', <papid> W04-3250 </papid>, u'models based on formal syntax (chiang, 2005) ', <papid> P05-1033 </papid>, u'or linguistic syntax (liu et al, 2006; ', <papid> P06-1077 </papid>, u'marcu et al, 2006) ', <papid> W06-1606 </papid>, u'have also achieved state-of-the-art perfor mance.']
G5569	 D09-1114	[u'the feature sub space method for smt system combination ']	P05-1033	[u'\nsince the success of phrase-based methods (och and ney, 2004; ', <papid> J04-4002 </papid>, u'koehn, 2004), ', <papid> W04-3250 </papid>, u'models based on formal syntax (chiang, 2005) ', <papid> P05-1033 </papid>, u'or linguistic syntax (liu et al, 2006; ', <papid> P06-1077 </papid>, u'marcu et al, 2006) ', <papid> W06-1606 </papid>, u'have also achieved state-of-the-art perfor mance.']
G5655	 D07-1049	[u'smoothed bloom filter language models tera scale lms on the cheap ']	P05-1033	[u'\nstandard ngram language models assign probabilities to translation hypotheses in the target language, typically as smoothed trigram models (chiang, 2005).', <papid> P05-1033 </papid>]
G5670	 D08-1017	[u'stacking dependency parsers ']	P05-1012	[u'\na typical approach in graph-based dependency parsing has been to assume factor ized model, where local features are used but global function is optimized (mcdonald et al, 2005', <papid> P05-1012 </papid>, u'b).']
G5781	 D08-1017	[u'stacking dependency parsers ']	C96-1058	[u'\nspecifically, we view stacked learning asa way of approximating non-local features in linear model, rather than making empirically dubious independence (mcdonald et al, 2005', <papid> P05-1012 </papid>, u'b) or structural assumptions (e.g., projectivity, eisner, 1996), ', <papid> C96-1058 </papid>, u'using search approximations (sagae and lavie, 2005; ', <papid> W05-1513 </papid>, u'hallet al, 2006; ', <papid> P06-2041 </papid>, u'mcdonald and pereira, 2006), ', <papid> E06-1011 </papid>, u'solving (generally np-hard) integer linear program (riedel and clarke, 2006), ', <papid> W06-1616 </papid>, u'or adding latent variables (titov and henderson, 2007).', <papid> W07-2218 </papid>]
G6387	 D09-1073	[u'tree kernel based svm with structured syntactic knowledge for btg based phrase reordering ']	N03-1017	[u'\nphrase-based method (koehn et al , 2003; ', <papid> N03-1017 </papid>, u'och and ney, 2004; ', <papid> J04-4002 </papid>, u'koehn et al , 2007) and syntax based method (wu, 1997; ', <papid> J97-3002 </papid>, u'yamada and knight, 2001; ', <papid> P01-1067 </papid>, u'eisner, 2003; ', <papid> P03-2041 </papid>, u'chiang, 2005; ', <papid> P05-1033 </papid>, u'cowan et al , 2006; ', <papid> W06-1628 </papid>, u'marcu et al , 2006; ', <papid> W06-1606 </papid>, u'liu et al , 2007; ', <papid> P07-1089 </papid>, u'zhang et al , 2007c, 2008a, 2008b; shen et al , 2008; ', <papid> P08-1066 </papid>, u'mi and huang, 2008) ', <papid> D08-1022 </papid>, u'represent the state-of-the-art technologies in statistical machine translation (smt).']
G6389	 D09-1073	[u'tree kernel based svm with structured syntactic knowledge for btg based phrase reordering ']	J04-4002	[u'\nphrase-based method (koehn et al , 2003; ', <papid> N03-1017 </papid>, u'och and ney, 2004; ', <papid> J04-4002 </papid>, u'koehn et al , 2007) and syntax based method (wu, 1997; ', <papid> J97-3002 </papid>, u'yamada and knight, 2001; ', <papid> P01-1067 </papid>, u'eisner, 2003; ', <papid> P03-2041 </papid>, u'chiang, 2005; ', <papid> P05-1033 </papid>, u'cowan et al , 2006; ', <papid> W06-1628 </papid>, u'marcu et al , 2006; ', <papid> W06-1606 </papid>, u'liu et al , 2007; ', <papid> P07-1089 </papid>, u'zhang et al , 2007c, 2008a, 2008b; shen et al , 2008; ', <papid> P08-1066 </papid>, u'mi and huang, 2008) ', <papid> D08-1022 </papid>, u'represent the state-of-the-art technologies in statistical machine translation (smt).']
G6394	 D09-1073	[u'tree kernel based svm with structured syntactic knowledge for btg based phrase reordering ']	P05-1033	[u'\nphrase-based method (koehn et al , 2003; ', <papid> N03-1017 </papid>, u'och and ney, 2004; ', <papid> J04-4002 </papid>, u'koehn et al , 2007) and syntax based method (wu, 1997; ', <papid> J97-3002 </papid>, u'yamada and knight, 2001; ', <papid> P01-1067 </papid>, u'eisner, 2003; ', <papid> P03-2041 </papid>, u'chiang, 2005; ', <papid> P05-1033 </papid>, u'cowan et al , 2006; ', <papid> W06-1628 </papid>, u'marcu et al , 2006; ', <papid> W06-1606 </papid>, u'liu et al , 2007; ', <papid> P07-1089 </papid>, u'zhang et al , 2007c, 2008a, 2008b; shen et al , 2008; ', <papid> P08-1066 </papid>, u'mi and huang, 2008) ', <papid> D08-1022 </papid>, u'represent the state-of-the-art technologies in statistical machine translation (smt).']
G6576	 D08-1018	[u'better binarization for the cky parsing ']	P97-1003	[u'\nthe last issue is how our binarization perform son lexicalized parser, like collins (1997).', <papid> P97-1003 </papid>]
G6615	 D10-1054	[u'maximum entropy based phrase reordering for hierarchical phrase based translation ']	P05-1033	[u'\nthe hierarchical phrase-based (hpb) model (chi ang, 2005; ', <papid> P05-1033 </papid>, u'chiang, 2007) ', <papid> J07-2003 </papid>, u'has been widely adopted in statistical machine translation (smt).']
G6646	 D10-1054	[u'maximum entropy based phrase reordering for hierarchical phrase based translation ']	N03-1017	[u'\nmethod (koehn et al, 2003).', <papid> N03-1017 </papid>]
G6685	 D07-1045	[u'smooth bilingual ngram translation ']	N04-1033	[u'\nfor instance, it was suggested to use ibm-1 probabilities (och et al, 2004), ', <papid> N04-1021 </papid>, u'or other lexical translation probabilities (koehn et al, 2003; zens and ney, 2004).', <papid> N04-1033 </papid>]
G6810	 D09-1107	[u'sinuhe x2013 statistical machine translation using a globally trained conditional exponential family translation model ']	N03-1017	[u'\nwhile theoretically sound, this approach is computationally challenging both in practice (denero et al, 2008)', <papid> D08-1033 </papid>, u'and in theory (denero and klein, 2008), ', <papid> P08-2007 </papid>, u'may suffer from reference reach ability problems (denero et al, 2006), ', <papid> W06-3105 </papid>, u'and in the end may lead to inferior translation quality (koehn et al, 2003).', <papid> N03-1017 </papid>]
G6838	 D09-1021	[u'non projective parsing for statistical machine translation ']	P05-1033	[u'\nsyntax-based models for statistical machine translation (smt) have recently shown impressive results; many such approaches are based on either synchronous grammars (e.g., (chiang, 2005)), ', <papid> P05-1033 </papid>, u'or tree transducers (e.g., (marcu et al, 2006)).', <papid> W06-1606 </papid>]
G6841	 D09-1021	[u'non projective parsing for statistical machine translation ']	N03-1017	[u'\nthe method thereby retains the full set of lexical entries of phrase-based systems (e.g., (koehn et al, 2003)).', <papid> N03-1017 </papid>, u'1?']
G6843	 D09-1021	[u'non projective parsing for statistical machine translation ']	P05-1012	[u'\ninspired by work in discriminative dependency parsing (e.g., (mcdonald et al, 2005)), ', <papid> P05-1012 </papid>, u'we add probabilistic constraints to the model through discriminative model that links lexical dependencies in the target language to features of the source language string.']
G6864	 D09-1021	[u'non projective parsing for statistical machine translation ']	P97-1003	[u'\nthis procedure uses the head finding rulesof (collins, 1997).', <papid> P97-1003 </papid>]
G7066	 D07-1058	[u'parsimonious data oriented parsing ']	P97-1003	[u'\nmore over, as p-dop is formulated as an enrichment of the treebank probabilistic context-free grammar(pcfg), it allows for much easier comparison to alternative approaches to statistical parsing (collins, 1997; ', <papid> P97-1003 </papid>, u'charniak, 1997; johnson, 1998; ', <papid> J98-4004 </papid>, u'klein and manning, 2003; ', <papid> P03-1054 </papid>, u'petrov et al, 2006).', <papid> P06-1055 </papid>]
G7212	 D07-1127	[u'global learning of labeled dependency trees ']	P05-1012	[u'\none promising approach is based on exact search and structural learning (mcdonald et al, 2005; ', <papid> P05-1012 </papid>, u'mcdonald and pereira, 2006).', <papid> E06-1011 </papid>]
G7215	 D07-1127	[u'global learning of labeled dependency trees ']	C96-1058	[u'\nin our approach, we adopt eisner (1996)', <papid> C96-1058 </papid>, u's bottom up chart-parsing algorithm in mcdonald et al(2005)', <papid> P05-1012 </papid>, u's formulation, which finds the best projective dependency tree for an input string ']
L194	 I08-2101	[u'a multi document multilingual automatic summarization system ']	A00-2024	[u'\nvector space models (salton et. al., 1994), compression of sentences with automatic translation approaches (knight and marcu, 2000), hidden markov model (jing and mckeown, 2000), ', <papid> A00-2024 </papid>, u'topic signatures based methods (lin and hovy, 2000, lacatusu et al , 2006) are among the most popular techniques that have been used in the summarization systems of this category.']
L479	 I08-2118	[u'learning decision lists with known rules for text mining ']	W00-0603	[u'\nsome problems in text mining where rule-based systems have been successfully used are part of speech tagging (brill, 1992), ', <papid> A92-1021 </papid>, u'named entity annotation (grishman, 1997;appelt et al, 1995), information extraction (may nard et al, 2001), question answering (riloff and thelen, 2000) ', <papid> W00-0603 </papid>, u'and classification (han et al, 2003; liand yamanishi, 1999; sasaki and kita, 1998).']
L649	 I08-1033	[u'improving word alignment by adjusting chinese word segmentation ']	N03-1017	[u'\nhowever for remedy, many of the current word alignment methods combine the results of both alignment directions, via intersection or 249 grow-diag-final heuristic, to improve the alignment reliability (koehn et al, 2003; ', <papid> N03-1017 </papid>, u'liang et al, 2006; ', <papid> N06-1014 </papid>, u'ayan et al, 2006; denero et al, 2007).']
W908	 P98-2130	[u'formal aspects and parsing issues of dependency theory ']	C96-1058	[u'\nalso, number of parsers have been developed for some dependency frameworks (covington 1990) (', <papid> J90-4003 </papid>, u'kwon, yoon 1991) (sleator, temperley 1993) (hahn et al 1994) (lombardo, lesmo 1996), including stochastic treatment (eisner 1996) ', <papid> C96-1058 </papid>, u'and an object-oriented parallel parsing method (neuhaus, hahn 1996).']
