<paper>
<cited id="K0">
<title id=" H91-1037.xml">partial parsing a report on work in progress </title>
<section> date of incident: date.  </section>
<citcontext>
<prevsection>
<prevsent>processing sentences to the point of finding semantic interpretations is the topic of this paper.
</prevsent>
<prevsent>pilot experiments are 204 reported here on alternative algorithms to find interpret able fragments even when no global syntactic or semantic analysis can be found.
</prevsent>
</prevsection>
<citsent citstr=" H91-1065 ">
we are particularly exploring probabilistic models for this processing, and have described experiments with various probabilistie models elsewhere (aynso, et al 1990; meteer, et al, 1991).<papid> H91-1065 </papid></citsent>
<aftsection>
<nextsent>the discourse component has two roles.
</nextsent>
<nextsent>one is resolving references.
</nextsent>
<nextsent>the second role is to use hypotheses regarding what domain-specific events are being described in each paragraph or article.
</nextsent>
<nextsent>particular events correspond totemplates.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="K1">
<title id=" H91-1037.xml">partial parsing a report on work in progress </title>
<section> date of incident: date.  </section>
<citcontext>
<prevsection>
<prevsent>hand-scoring of the results indicated that 85% of the core nps were identified correctly.
</prevsent>
<prevsent>subsequent analysis suggested that half the errors could be removed with only little additional work, suggesting that over 90% performance is achievable.
</prevsent>
</prevsection>
<citsent citstr=" A88-1019 ">
in related test, we explored the bracketings produced by church parts program (church, 1988).<papid> A88-1019 </papid></citsent>
<aftsection>
<nextsent>we extracted 200 sentences of wsj text by taking every tenth sentence from collection of manually corrected parse trees (data from the treebank project at the university of pennsylvania).
</nextsent>
<nextsent>we evaluated the np bracketings in these 200 sentences by hand, and tried to classify the errors.
</nextsent>
<nextsent>of 1226 phrases in the 200 sentences, 131 were errors, for 10.7% error rate.
</nextsent>
<nextsent>the errors were classified by hand as follows: ? two consecutive but unrelated phrases grouped as one: 10 ? phrase consisted of single word, which was not an np: 70 ? missed phrases (those that should have been bracketed but were not): 12 ? ellided head (e.g. part of conjoined pre modifier toan np): 4 ? missed premodffiers: 4 ? head of phrase was verb form that was missed: 4 ? other: 27 the 90% success rate in both tests suggests that identification core nps can be achieved using only local information and with minimal knowledge of the words.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="K2">
<title id=" H91-1037.xml">partial parsing a report on work in progress </title>
<section> date of incident: date.  </section>
<citcontext>
<prevsection>
<prevsent>thus far, we have tested this hypothesis on propositional phrase attachment.
</prevsent>
<prevsent>such semantic kffowledge called selection restrictions or case frames governs what phrases make sense with particular verb or noun (what arguments go with particular verb or noun).
</prevsent>
</prevsection>
<citsent citstr=" P87-1005 ">
traditionally such semantic knowledge is handcrafted, though some software aids exist to enable greater productivity (ayuso et al., 1987; <papid> P87-1005 </papid>bates, 1989; <papid> H89-1008 </papid>grishman et al, 1986; <papid> J86-3002 </papid>weischedel, et al, 1989).<papid> H89-1013 </papid></citsent>
<aftsection>
<nextsent>instead of han drafting this semantic knowledge, our goal is to learn that knowledge from examples, using three step process: 1.
</nextsent>
<nextsent>simple manual semantic annotation,.
</nextsent>
<nextsent>2.
</nextsent>
<nextsent>supervised training based on parsed sentences,.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="K3">
<title id=" H91-1037.xml">partial parsing a report on work in progress </title>
<section> date of incident: date.  </section>
<citcontext>
<prevsection>
<prevsent>thus far, we have tested this hypothesis on propositional phrase attachment.
</prevsent>
<prevsent>such semantic kffowledge called selection restrictions or case frames governs what phrases make sense with particular verb or noun (what arguments go with particular verb or noun).
</prevsent>
</prevsection>
<citsent citstr=" H89-1008 ">
traditionally such semantic knowledge is handcrafted, though some software aids exist to enable greater productivity (ayuso et al., 1987; <papid> P87-1005 </papid>bates, 1989; <papid> H89-1008 </papid>grishman et al, 1986; <papid> J86-3002 </papid>weischedel, et al, 1989).<papid> H89-1013 </papid></citsent>
<aftsection>
<nextsent>instead of han drafting this semantic knowledge, our goal is to learn that knowledge from examples, using three step process: 1.
</nextsent>
<nextsent>simple manual semantic annotation,.
</nextsent>
<nextsent>2.
</nextsent>
<nextsent>supervised training based on parsed sentences,.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="K4">
<title id=" H91-1037.xml">partial parsing a report on work in progress </title>
<section> date of incident: date.  </section>
<citcontext>
<prevsection>
<prevsent>thus far, we have tested this hypothesis on propositional phrase attachment.
</prevsent>
<prevsent>such semantic kffowledge called selection restrictions or case frames governs what phrases make sense with particular verb or noun (what arguments go with particular verb or noun).
</prevsent>
</prevsection>
<citsent citstr=" J86-3002 ">
traditionally such semantic knowledge is handcrafted, though some software aids exist to enable greater productivity (ayuso et al., 1987; <papid> P87-1005 </papid>bates, 1989; <papid> H89-1008 </papid>grishman et al, 1986; <papid> J86-3002 </papid>weischedel, et al, 1989).<papid> H89-1013 </papid></citsent>
<aftsection>
<nextsent>instead of han drafting this semantic knowledge, our goal is to learn that knowledge from examples, using three step process: 1.
</nextsent>
<nextsent>simple manual semantic annotation,.
</nextsent>
<nextsent>2.
</nextsent>
<nextsent>supervised training based on parsed sentences,.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="K5">
<title id=" H91-1037.xml">partial parsing a report on work in progress </title>
<section> date of incident: date.  </section>
<citcontext>
<prevsection>
<prevsent>thus far, we have tested this hypothesis on propositional phrase attachment.
</prevsent>
<prevsent>such semantic kffowledge called selection restrictions or case frames governs what phrases make sense with particular verb or noun (what arguments go with particular verb or noun).
</prevsent>
</prevsection>
<citsent citstr=" H89-1013 ">
traditionally such semantic knowledge is handcrafted, though some software aids exist to enable greater productivity (ayuso et al., 1987; <papid> P87-1005 </papid>bates, 1989; <papid> H89-1008 </papid>grishman et al, 1986; <papid> J86-3002 </papid>weischedel, et al, 1989).<papid> H89-1013 </papid></citsent>
<aftsection>
<nextsent>instead of han drafting this semantic knowledge, our goal is to learn that knowledge from examples, using three step process: 1.
</nextsent>
<nextsent>simple manual semantic annotation,.
</nextsent>
<nextsent>2.
</nextsent>
<nextsent>supervised training based on parsed sentences,.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="K6">
<title id=" H91-1037.xml">partial parsing a report on work in progress </title>
<section> estimation of probabilities..  </section>
<citcontext>
<prevsection>
<prevsent>however, the degree of reduction of error rate should not be taken as the final word, for the following reasons: 20,000 words of training data is much less than one would want.
</prevsent>
<prevsent>an additional 70,000 words of training data should soon be available through treebank.
</prevsent>
</prevsection>
<citsent citstr=" H90-1052 ">
since many of the headwords in the 20,000 word corpus are not of import in the muc-3 domain, their semantic type is vague, i.e.,  unknown event ,  unknown entity , etc. related work in addition to the work discussed earlier on tools to increase the portability of natural anguage systems, another ecent paper (hindle and rooth, 1990) <papid> H90-1052 </papid>is directly related to our goal of inferring case frame information from examples.</citsent>
<aftsection>
<nextsent>hindle and rooth focussed only on prepositional phrase attachment using probabilistic model, whereas our work applies to all case relations.
</nextsent>
<nextsent>their work used an unsupervised training corpus of 13 million words to judge the strength of prepositional affinity to verbs, e.g., how likely it is for to to attach to the word go, for from to attach to the word leave, or for to to attach to the word flight.
</nextsent>
<nextsent>this lexical affinity is measured independent of the object of the preposition.
</nextsent>
<nextsent>by contrast, we are exploring induction of semantic relations from supervised training, where very little training may be available.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="K7">
<title id=" H89-2050.xml">contextually based data derived pronunciation networks for automatic speech recognition </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the number of leaves in the tree and branching of the tree is determined by the data.
</prevsent>
<prevsent>in the next paragraph we give brief description of context trees.
</prevsent>
</prevsection>
<citsent citstr=" H89-1053 ">
a more detailed description of context trees and how they are created is given in chen and shrager (1989).<papid> H89-1053 </papid></citsent>
<aftsection>
<nextsent>an alternate method for grouping contextual factors, based on binary splitting of sub spaces until preset number of clusters is formed, is given by sagayama (1989).
</nextsent>
<nextsent>a context ree is an n-ary decision tree which models the relationship between contextual factors and the allo phones which occur in different contexts.
</nextsent>
<nextsent>we create context rees from set of training exemplars using data derived from the hand-transcribed  sx  sentences of the im it database (lamel, et al, 1986; fisher et al, 1987).
</nextsent>
<nextsent>an illustrative tree describing the distribution of allo phones in context for some phoneme is shown in figure 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="K8">
<title id=" H90-1026.xml">bbn atis system progress report  june 1990 </title>
<section> delphi and parlance/learner.  </section>
<citcontext>
<prevsection>
<prevsent>delphi can be changed very quickly, but has no easy way to build knowledge bases rapidly.
</prevsent>
<prevsent>the parlance tm system is commercial nl interface to relational databases, and is based on an atn parser and grammar.
</prevsent>
</prevsection>
<citsent citstr=" H89-2031 ">
parlance has the advantage of an extensive knowledge acquisition system called the learner tm, so our previously reported approach (ingria and ramshaw 1989) <papid> H89-2031 </papid>has been to use the learner to create lexicon with morphological nd syntactic information, domain model with semantic information, and mapping rules from the domain model to the database.</citsent>
<aftsection>
<nextsent>these knowledge bases were then imported for use by delphi.
</nextsent>
<nextsent>as side-effect of using the learner, parlance configuration for the atis domain was created, so results using that system are reported here for comparison.
</nextsent>
<nextsent>on the blind test the original score of the bbn delphi system on the 93 sentence atis blind test was 53 sentences correct, 2 sentences not correct and 38 sentences not answered.
</nextsent>
<nextsent>of the two sentences judged not correct, one is the result of mistake in the canonical answer set provided by hist.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="K9">
<title id=" H90-1021.xml">the atis spoken language systems pilot corpus </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>99 reference answer the reference answer consists of the set of tuples result-ing from the evaluation of the reference sql with respect to the official atis database.
</prevsent>
<prevsent>this is actually redundant, but makes scoring easier for most sites.
</prevsent>
</prevsection>
<citsent citstr=" H89-2019 ">
the tuples are formatted according the common answer specification (gas) format (boisen et al 1989).<papid> H89-2019 </papid></citsent>
<aftsection>
<nextsent>this format amounts to representing the answer in lisp syntax to aid in au-tomatic scoring.
</nextsent>
<nextsent>corpus i les all of the items mentioned above were formatted into files and shipped to the national institute of standards and technology (nist).
</nextsent>
<nextsent>nist then distributed the cor-pus to interested sites.
</nextsent>
<nextsent>a file format document exists to help sites install the data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
</paper>