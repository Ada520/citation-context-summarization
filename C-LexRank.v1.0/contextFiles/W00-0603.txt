in addition, a number of researchers have built systems to take reading comprehension examinations designed to evaluate childrenâ€™s reading levels (charniak et al, 2000; hirschman et al, 1999; ng et al, 2000; riloff and thelen, 2000; wang et al, 2000). 
quarc (riloff and thelen, 2000) utilizes manually generated rules that selects a sentence deemed to contain the answer based on a combination of syntactic similarity and semantic correspondence (i.e., semantic categories of nouns). 
in prior work (hirschman et al, 1999; charniak et al, 2000; riloffand thelen, 2000) the number and type of information sources used for computation is specific to and rlifferent for each question type. 
it is interesting to note that the words automatically determined by out procedure are also part of those words found manually in the prior work of (l:tiloff and thelen, 2000). 
as such, all subsequent work (charniak et al, 2000; riloff and thelen, 2000; wang et al, 2000) uses humsent as the main scoring metric, and it is also the scoring metric that we adopted in this paper. 
et al, 1999), (charniak et al, 2000), (riloff and thelen, 2000), and (wang et al, 2000) are 36.3%, 
subsequently, the work of (riloffand thelen, 2000) and (chaxniak et al, 2000) improved the accuracy further to 39.7% and 41%, respectively. 
it has been observed in prior work (charniak et al, 2000; riloff and thelen, 2000) that such sentences may be more likely to be the answer sentences to some question type (for example, dateline can answer to when questions). 
4 evaluation to evaluate our learning approach, we trained aquarea$ on the same development set of stories and tested it on the same test set of stories as those used in all past work on the reading comprehension task (hirschman et al, 1999; charniak et al, 2000; riloffand thelen, 2000; wang et al, 2000). 
keywords in questions it has been observed in the work of (riloff and thelen, 2000) that certain words in a when or where question tend to indicate that the dateline is an ~n~wer sentence to the question. 
naturally, our current work on question answering for the reading comprehension task is most related to those of (hirschman et al, 1999; charniak et al, 2000; riloffand thelen, 2000; wang et al, 2000). 
3.1 feature representation our feature representation was designed to capture the information sources that prior work (hirschman et al, 1999; cha_niak et al, 2000; riloff and thelen, 2000) used in their computations or rules. 
we designed these 4 features to capture information that will be helpful to the why questions, since it has been observed in prior work (charniak et al, 2000; riloff and thelen, 2000) that the answer 127 sentence to a why question tends to follow (or precede) the sentence in the story that has the most number of word matches with the question. 
the words used in (riloff and thelen, 2000) are "happen", "take place" "this", "story". 
based on these technologies, riloff and thelen (2000) improved the humsent accuracy to 40% by applying a set of heuristic rules that assign handcrafted weights to matching words and ne. 
refer to the readme le of minipar downloaded from http://www.cs.ualberta.ca/ lindek/minipar.htm 5 experimental results we selected the features used in quarc (riloff and thelen, 2000) to establish the reference performance level. 
as was observed by (riloff and thelen, 2000) and (charniak et al, 2000), the correct answer sentence often precedes/follows the sentence with the highest number of matching words. 
then (riloff and thelen, 2000) and (charniak et al, 2000) reported improvements to 39.7% and 41%, respectively. 
we also compared our results across various interrogatives with those previously reported in (riloff and thelen, 2000). 
