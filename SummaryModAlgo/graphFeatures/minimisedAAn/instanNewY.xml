<paper>
<cited id="Y0">
<title id=" W02-0503.xml">acquisition system for arabic noun morphology </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>al-fedaghi and al-anzi (1989) present an algorithm to generate the root and the pattern of given arabic word.
</prevsent>
<prevsent>the main concept in the algorithm is to locate the position of the roots letters in the pattern and examine the letters in the same position in given word to see whether the tri graph forms valid arabic root or not.
</prevsent>
</prevsection>
<citsent citstr=" W98-1009 ">
al-shalabi (1998) <papid> W98-1009 </papid>developed system that removes the longest possible prefix from the word where the three letters of the root must lie somewhere in the first four or five characters of the remainder.</citsent>
<aftsection>
<nextsent>then he generates some combinations and checks each one of them with all the roots in the file.
</nextsent>
<nextsent>al-shalabi reduced the processing, but he discussed this from point of view of verbs not nouns.
</nextsent>
<nextsent>anne roeck and waleed al-fares (2000) developed clustering algorithm for arabic words sharing the same verbal root.
</nextsent>
<nextsent>they used root-based clusters to substitute for dictionaries in indexing for information retrieval.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1">
<title id=" W02-0503.xml">acquisition system for arabic noun morphology </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>anne roeck and waleed al-fares (2000) developed clustering algorithm for arabic words sharing the same verbal root.
</prevsent>
<prevsent>they used root-based clusters to substitute for dictionaries in indexing for information retrieval.
</prevsent>
</prevsection>
<citsent citstr=" P00-1025 ">
beesley and karttunen (2000) <papid> P00-1025 </papid>described new technique for constructing finite-state transducers that involves reapplying regular-expression compiler to its own output.</citsent>
<aftsection>
<nextsent>they implemented the system in an algorithm called compile replace.
</nextsent>
<nextsent>this technique has proved useful for handling non-concatenate phenomena, and they demonstrate it on malay full-stem reduplication and arabic stem inter-digitations.
</nextsent>
<nextsent>most verbs in the arabic language follow clear rules that define their morphology and generate their paradigms.
</nextsent>
<nextsent>those nouns that are not derived from roots do not seem to follow similar set of well-defined rules.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y2">
<title id=" W02-0503.xml">acquisition system for arabic noun morphology </title>
<section> nouns in the arabic language.  </section>
<citcontext>
<prevsection>
<prevsent>city ????
</prevsent>
<prevsent>north proper noun is the name of specific person, place, organization, thing, idea, event, date, time, or other entity.
</prevsent>
</prevsection>
<citsent citstr=" W98-1001 ">
some of them are solid (inert) nouns some of them are derived [abuleil and evens 1998].<papid> W98-1001 </papid></citsent>
<aftsection>
<nextsent>in this paper we focus on the following nouns: genus nouns, agent nouns, instrument nouns, adjectives, proper adjectives (adjectives derived from proper nouns), proper nouns, and adverbs.
</nextsent>
<nextsent>some of these nouns are not derived from verbs and some are.
</nextsent>
<nextsent>all of these nouns use the same pattern when it comes to the dual form either for masculine or feminine, but there are many ways to form the plural noun.
</nextsent>
<nextsent>some of the nouns have both masculine and feminine forms, some of them have just feminine forms and some have just masculine forms.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y4">
<title id=" W00-0403.xml">centroidbased summarization of multiple documents sentence extraction utility based evaluation and user studies </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>event clusters range from2 to 10 documents from which mead produces umm aries in the form of sentence xtracts.
</prevsent>
<prevsent>a key feature of mead is its use of cluster centro ids, which consist of words which are central not only to one article in cluster, but to all the articles.
</prevsent>
</prevsection>
<citsent citstr=" J98-3005 ">
mead is significantly different from previous work on multi-document summarization \[radev &amp; mckeown, 1998; <papid> J98-3005 </papid>carbonell and goldstein, 1998; mani and bloedorn, 1999; mekeown et ai., 1999\], 21 which use techniques such as graph matching, maximal marginal relevance, or language generation.</citsent>
<aftsection>
<nextsent>finally, evaluation of multi-document summaries a difficult problem.
</nextsent>
<nextsent>there is not yet widely accepted evaluation scheme.
</nextsent>
<nextsent>we propose utility-based evaluation scheme, which can be used to evaluate both single-document and multi-document summaries.
</nextsent>
<nextsent>2.1 cluster-based sentence utility (cbsu).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y5">
<title id=" W00-1421.xml">planning word order dependant focus assignments </title>
<section> focus  and  word  order.  </section>
<citcontext>
<prevsection>


</prevsection>
<citsent citstr=" W98-1406 ">
determination as sentence lanning tasks this paper addresses aspects of the control of in-tonation belonging to the area of sentence planning \[beale et al, 1998: <papid> W98-1406 </papid>wanner and hovy, 1996\].</citsent>
<aftsection>
<nextsent>in many languages, intonation can reflect pragmati-cally motivated conceptual decisions.
</nextsent>
<nextsent>in particu-lar, focus/background structures (fbss) reflect the speaker beliefs of the listener information state.
</nextsent>
<nextsent>since fbss are realized in german primarily by word order dependent accent placements, focus plan-ning and word order determination are subtasks of sentence planning.
</nextsent>
<nextsent>due to the complex interactions anaong the vari-ous subtasks of sentence planning \[hovy and wan- * the research reported in this paper is funded by the dfg (german science foundation) in the priority program  lan-guage production  under grant lie 146t/3-i.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y6">
<title id=" W02-0502.xml">generating hebrew verb morphology by default inheritance hierarchies </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for instance, inferential-realizational definitions do not entail that concatenative and nonconcatenative morphology are fundamentally different in their grammatical status; they do not necessitate the postula tion of any relation between inflectional marking sand morphosyntactic properties other than the relation of simple exponence; and they are compatible with the assumption that word forms morphological representation is not distinct from its phonological representation.various means of defining languages inflectional morphology in inferential-realizational terms are imaginable.
</prevsent>
<prevsent>in an important series of articles (corbett and fraser, 1993; fraser and corbett, 1995; fraser and corbett, 1997), greville corbett and norman fraser proposed network morphology, an inferential-realizational morphological framework that makes extensive use of non monotonic inheritance hierarchies to represent the information constituting languages inflectional system.
</prevsent>
</prevsection>
<citsent citstr=" E89-1009 ">
analyses in network morphology are implemented in datr, formal language for representing lexical knowledge designed and implemented by roger evans and gerald gazdar (evans and gazdar, 1989).<papid> E89-1009 </papid></citsent>
<aftsection>
<nextsent>in recent work, we have extended datr, creatingkatr, which is both formal language and computer program that generates desired forms by interpreting that language.
</nextsent>
<nextsent>in this paper, we show how katr can be used to provide an inferential-realizational definition of hebrew verb morphology.
</nextsent>
<nextsent>our objectives are twofold.first, we propose some general strategies for exploiting the capabilities of non monotonic inheritance hierarchies in accounting for the properties ofroot-and-pattern?
</nextsent>
<nextsent>verb inflection in hebrew; second, we discuss some specific capabilities that distinguish katr from datr and show why these added capabilities are helpful to account for the hebrew facts.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y7">
<title id=" W01-1505.xml">sissa  an infrastructure for nlp application development </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in recent years there has been growing interest in the commercial deployment of nlp technologies and in infrastructures for sharing nlp tools and resources.
</prevsent>
<prevsent>such interest makes more and more urgent the availability of tool sets that allow an easy and quick integration of linguistic resources and modules and the rapid prototyping of nlp applications.
</prevsent>
</prevsection>
<citsent citstr=" A97-1035 ">
an example of the efforts in such direction is gate (a general architecture for text engineering, (cunningham et al, 1997)), <papid> A97-1035 </papid>which provides software infrastructure on top of which heterogeneous nlp processing modules may be evaluated and refined individually, or may be combined into larger application systems.</citsent>
<aftsection>
<nextsent>this paper presents sissa (sistema integratodi sup porto allo sviluppo di applicazioni - integrated system of support to application devel opment), project with twofold aim:   the definition of common meta formalism (called fist) for the unification of different formalisms for grammar description, and the implementation of grammar repository for storing grammars written using fist;   the implementation of an infrastructure forthe rapid prototyping and testing of architectures for nlp systems, starting from linguistic processors made available by sissa it self.in this paper we concentrate on the latter aspect, i.e. the infrastructure for designing nlp architectures.
</nextsent>
<nextsent>to this end, sissa provides the user with graphical environment for (1) selecting the linguistic activities which are relevant to the particular application at hand, along with the linguistic processors that execute them; (2) checking that the chosen architectural hypothesis corresponds to the functional specifications of the application;(3) connecting to sissa new linguistic processors, this way making them available for the pro totyping/design activities.
</nextsent>
<nextsent>thus, the design of the architecture of an nlp system amounts to a) identifying sequence of linguistic activities to be performed; b) connecting them in specific processing chain; and c) associating each linguistic activity to suitable processor, selected among those made available by sissa.
</nextsent>
<nextsent>the term project is used to refer to the product of the users activity, namely, the architecture of the nlp application the user is building.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y8">
<title id=" W01-1505.xml">sissa  an infrastructure for nlp application development </title>
<section> sissa at work.  </section>
<citcontext>
<prevsection>
<prevsent>the bottom bar shows which of the processors/filters is currently active (using the istatemonitor interface described in section 2.2).
</prevsent>
<prevsent>3.2 integration of processors.
</prevsent>
</prevsection>
<citsent citstr=" W97-1503 ">
differently from the activity of creation and editing of projects, only the final part of the work involved in the integration of processors is accomplished via the sissa graphical interface (more written in and running under windows: (prodanof et al, 1998; prodanof et al, 2000)) and the pre processor and the parser of geppetto (itc-irst, written in common lisp and running under solaris: (ciravegna et al, 1997; <papid> W97-1503 </papid>ciravegna et al., 1998)).</citsent>
<aftsection>
<nextsent>figure 2: the starting page of sissa.precisely, the registration in the processor repository of the availability of the processors).
</nextsent>
<nextsent>in order to make processor sissa-compliant, the following steps are necessary:  to provide it with wrapper so that it communicates via the corba idls of sissa;  to make translation between the processors native input/output and the corresponding linguistic representation specified by process-data;   to register the processor in the processor repository using the sissa graphical interface; during this step the class of the processor, its corbaloc: url and activation string have to be specified.
</nextsent>
<nextsent>the release 1.2 of the sissa manager has been completed and is currently under use at the sites involved in the sissa project.
</nextsent>
<nextsent>given the emphasis on rapid prototyping, sissa has been developed with flexibility during the development phase as primary goal.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y9">
<title id=" W01-1309.xml">from temporal expressions to temporal information semantic tagging of news messages </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>temporal expressions are defined for this system as chunks of text that express some sort of direct or inferred temporal information.
</prevsent>
<prevsent>the set of these expressions investigated in the present paper includes dates (e.g. 08.04.2001), prepositional phrases (pps) containing some time expression (e.g. on friday), and verbs referring to situation (e.g. opened).
</prevsent>
</prevsection>
<citsent citstr=" P00-1010 ">
related work by mani and wilson (2000) <papid> P00-1010 </papid>focuses only on the core temporal expressions neglecting the temporal information conveyed by prepositions (e.g. friday vs. by friday).the main part of the system is temporal expression tagger that employs finite state transducers based on hand-written rules.</citsent>
<aftsection>
<nextsent>the tagger was trained on economic news articles obtained from two german newspapers and an on line newsagency (financial times deutschland, die tageszeitung and www.comdirect.de).based on the syntactic classification of temporal expressions semantic representation of the extracted chunks is proposed.
</nextsent>
<nextsent>a clear-cut distinction between the syntactic tagging process and the semantic interpretation is maintained.
</nextsent>
<nextsent>the advantage of this approach is that second level is created that represents the meaning of the extractedchunks.
</nextsent>
<nextsent>having defined the semantic representation of the temporal expressions, further inferences, in particular on temporal relations, can bedrawn.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y12">
<title id=" W01-1410.xml">machine translation with grammar association some improvements and the lococ model </title>
<section> loco c: new association model.  </section>
<citcontext>
<prevsection>
<prevsent>following data-driven approach, grammar association system needs to learn from examples an association model capable to estimate the probabilities required by our recently developed framework, that is, the probability of each rule in the grammar that models the output language,conditioned on its left-hand side and the derivation of the input sentence.
</prevsent>
<prevsent>among the different association models we have studied (prat, 1998), it is worth emphasizing one we have specifically developed for playing that role in grammar association systems: the loco model.
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
we based our design on the ibm models 1 and 2 (brown et al, 1993), <papid> J93-2003 </papid>but taking into account that our model must generate correct derivations in given grammar, not any se begin some end animals  eat  animals  (a)  some animals eat animals   begin some end animals  eat are  animals  dangerous (b)  some animals are dangerous  begin  animals  some end eat are  animals  dangerous (c)   animals are dangerous  begin snakes rats people some end eat are snakes rats people dangerous (d) expansion of animals figure 3: using category animals for snakes ,  rats  and  people  in the example of figure 1.</citsent>
<aftsection>
<nextsent>quence of rules.5 moreover, we wanted to model the probability estimation for each output rule as an adequately weighted mixture,6 along with keeping the maximum-likelihood re-estimation of its parameters within the growth transformation framework (baum and eagon, 1967; gopalakr5in those simple ibm translation models, an output sequence (of words) is randomly generated from given input one by first choosing its length and then, for each position in the output sequence, independently choosing an element (word).
</nextsent>
<nextsent>if the relation between input and output derivations (sequences of rules) has to be explicitly modelled, the choices of output elements can no longer be independent be cause rule is only applicable if its left-hand side has just appeared in the output derivation.
</nextsent>
<nextsent>6in ibm models, all words in the input sequence have the same influence in the random choice of output words (model 1) or they have relative influence depending on their positions (model 2).
</nextsent>
<nextsent>in the case of derivations, we are interested in modelling those relative influences taking into account rule identities (instead of rule positions).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y13">
<title id=" W01-1507.xml">international standards for multilingual resource sharing the isle computational lexicon working group </title>
<section> the structure of the prospective.  </section>
<citcontext>
<prevsection>
<prevsent>to give concrete example, almost all theoretical frameworks claim that lexical items have complex semantic organization, but some of them try to describe it through multidimensional internal structure (cf.
</prevsent>
<prevsent>the qualia structure in the generative lexicon, pustejovsky 1995), others by specifying network of semantic relations (cf.
</prevsent>
</prevsection>
<citsent citstr=" P98-1013 ">
wordnet, miller et al 1990), and others in terms of argumental frames (cf framenet, baker et al 1998; <papid> P98-1013 </papid>lexical conceptual structures, jackendoff 1992; etc.).</citsent>
<aftsection>
<nextsent>a way out of this theoretical variation is to augment the expressive power of the lexical representation language both horizontally, i.e. by distributing the linguistic information over mutually independent  coding layers , and vertically, by further specifying the information conveyed by each such layer.
</nextsent>
<nextsent>this solution will contribute to solve the issues raised by theoretical variation by defining common level onto which different types of resources will be mapped without loss of information.
</nextsent>
<nextsent>this appears to be necessary condition to guarantee an efficient re-use and interchange of lexical data, often coming from resources developed according to very different architectural and theoretical criteria.
</nextsent>
<nextsent>with respect to this issue, the mile is designed to meet the following desiderata: ? factor out linguistically independent (but possibly correlated) primitive units of lexical information; ? make explicit information which is otherwise only indirectly accessible by nlp systems; ? relyon lexical analysis which have the highest degree of inter-theoretical agreement; ? avoid framework-specific representational solutions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y14">
<title id=" W00-1416.xml">on identifying sets </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>in any case, the search starts with structure defining an empty description, which means nothing and could refer to anything.
</prevsent>
<prevsent>structures are then extended and considered in turn until the interpretation satisfies the system goals (for example because it allows only specified value, the intended referent, for particular variable).
</prevsent>
</prevsection>
<citsent citstr=" P97-1026 ">
the process of extension sim-ply consists of deriving more elaborate form with richer meaning using the generator linguistic resources--it is useful to think of obtaining this by carrying out step of derivation in lexicalized grammar (stone and doran, 1997)--<papid> P97-1026 </papid>and then con-sulting the model of the context to obtain an updated interpretation.</citsent>
<aftsection>
<nextsent>to extend these data structures to sets, we cannot introduce set variables and maintain the alternative candidate set values those variables might ultimately refer to--for one thing, there are just too many sets to represent an interpretation this way.
</nextsent>
<nextsent>here.
</nextsent>
<nextsent>is suggestion: reinterpret_data.structures like (5) as compatible with descriptions of collec-tions as well as singletons.
</nextsent>
<nextsent>this should have some intuitive appeal.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y15">
<title id=" W00-1416.xml">on identifying sets </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>(se .(donellan, ..!
</prevsent>
<prevsent>966;: kx~0nfeld, 1986) on the dis- .- tinction.)
</prevsent>
</prevsection>
<citsent citstr=" W98-1403 ">
(green et al, 1998<papid> W98-1403 </papid>a; green et al, 1998<papid> W98-1403 </papid>b) show how such descriptions may be represented and formulated in nlg at high-level process of con-tent or rhetorical planning.</citsent>
<aftsection>
<nextsent>at the same time, plu- rals and singulars are alike in offering resources for reference--such as pronouns, one-anaphora or ag-gregated expressions--that bypass explicit descrip-tion altogether?
</nextsent>
<nextsent>the use of these resources may be ....
</nextsent>
<nextsent>~quite-closety dependent onthe surface  form being generated and so could reflect relatively late deci-sion in the generation process (dale and haddock, 122 1991; reiter, 1994; <papid> W94-0319 </papid>dalianis, 1996).</nextsent>
<nextsent>these complexities notwithstanding, we can ex-pect many descriptions of sets, like descriptions of individuals, to be formulated from scratch to achieve purely referential goals during the sen-tence planning.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y23">
<title id=" W00-1416.xml">on identifying sets </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>at the same time, plu- rals and singulars are alike in offering resources for reference--such as pronouns, one-anaphora or ag-gregated expressions--that bypass explicit descrip-tion altogether?
</prevsent>
<prevsent>the use of these resources may be ....
</prevsent>
</prevsection>
<citsent citstr=" W94-0319 ">
~quite-closety dependent onthe surface  form being generated and so could reflect relatively late deci-sion in the generation process (dale and haddock, 122 1991; reiter, 1994; <papid> W94-0319 </papid>dalianis, 1996).</citsent>
<aftsection>
<nextsent>these complexities notwithstanding, we can ex-pect many descriptions of sets, like descriptions of individuals, to be formulated from scratch to achieve purely referential goals during the sen-tence planning.
</nextsent>
<nextsent>plaase: of .nlg, io:.he:tween ~gon=.
</nextsent>
<nextsent>tent planning and surface realization (rainbow and korelsky, 1992; reiter, 1994).<papid> W94-0319 </papid></nextsent>
<nextsent>i have shown that using covers to abstract collective and distributive readings--and using sets of assignments repre-sent plural references--yields search space for this problem which largely mirrors that for singu- lars, and which avoids computation and search over sets of collections.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y25">
<title id=" W01-1004.xml">using hlt for acquiring retrieving and publishing knowledge in akt </title>
<section> knowledge acquisition.  </section>
<citcontext>
<prevsection>
<prevsent>2.1 taxonomy construction.
</prevsent>
<prevsent>we propose to introduce automation in the stage of taxonomy construction mainly in order to eliminate or reduce the need for extensive elicitation of data.
</prevsent>
</prevsection>
<citsent citstr=" J92-4003 ">
in the literature approaches to construction of taxonomies of concepts have been proposed (brown et al  1992, <papid> J92-4003 </papid>mcmahon and smith 1996, <papid> J96-2003 </papid>sanderson and croft 1999).</citsent>
<aftsection>
<nextsent>such approaches either use large collection of documents as their sole data source, or they can attempt to use existing concepts to extend the taxonomy (agirre et al 2000, scott 1998).
</nextsent>
<nextsent>we intend to develop semi-automatic method that, starting from seed ontology sketched by the user, produces the final ontology via cycle of refinements by eliciting knowledge from collection of texts.
</nextsent>
<nextsent>in this approach the role of the user should only be that of proposing an initial ontology and validating/changing the different versions proposed by the system.
</nextsent>
<nextsent>we intend to integrate methodology for automatic hierarchy definition (such as (sanderson and croft 1999)) with method for the identification of terms related to concept in hierarchy (such as (scott 1998)).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y26">
<title id=" W01-1004.xml">using hlt for acquiring retrieving and publishing knowledge in akt </title>
<section> knowledge acquisition.  </section>
<citcontext>
<prevsection>
<prevsent>2.1 taxonomy construction.
</prevsent>
<prevsent>we propose to introduce automation in the stage of taxonomy construction mainly in order to eliminate or reduce the need for extensive elicitation of data.
</prevsent>
</prevsection>
<citsent citstr=" J96-2003 ">
in the literature approaches to construction of taxonomies of concepts have been proposed (brown et al  1992, <papid> J92-4003 </papid>mcmahon and smith 1996, <papid> J96-2003 </papid>sanderson and croft 1999).</citsent>
<aftsection>
<nextsent>such approaches either use large collection of documents as their sole data source, or they can attempt to use existing concepts to extend the taxonomy (agirre et al 2000, scott 1998).
</nextsent>
<nextsent>we intend to develop semi-automatic method that, starting from seed ontology sketched by the user, produces the final ontology via cycle of refinements by eliciting knowledge from collection of texts.
</nextsent>
<nextsent>in this approach the role of the user should only be that of proposing an initial ontology and validating/changing the different versions proposed by the system.
</nextsent>
<nextsent>we intend to integrate methodology for automatic hierarchy definition (such as (sanderson and croft 1999)) with method for the identification of terms related to concept in hierarchy (such as (scott 1998)).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y27">
<title id=" W01-1004.xml">using hlt for acquiring retrieving and publishing knowledge in akt </title>
<section> knowledge acquisition.  </section>
<citcontext>
<prevsection>
<prevsent>finally an ontology defined in this way has the particular advantage that it overcomes the well-known tennis problem?
</prevsent>
<prevsent>associated with many predefined ontologies such as wordnet, i.e where terms closely related in given domain are structurally very distant such as ball and court, for example.
</prevsent>
</prevsection>
<citsent citstr=" M98-1007 ">
in addition we intend to employ classic information extraction techniques (described below) such as named entity recognition (humphreys et al  1998) <papid> M98-1007 </papid>in order to pre-process the text, as the identification of complex terms such as proper names, dates, numbers, etc, allows to reduce data sparseness in learning (ciravegna 2000).</citsent>
<aftsection>
<nextsent>we plan to introduce many cycles of ontology learning and validation.
</nextsent>
<nextsent>at each stage the defined ontology can be: i) validated/corrected by user/expert; ii) used to retrieve larger set of appropriate documents to be used for further refinement (jrvelin and keklinen 2000); iii) passed on to the next development stage.
</nextsent>
<nextsent>2.2 learning other relations.
</nextsent>
<nextsent>this stage proceeds to build on the skeletal ontology in order to specify, as much as possible without human intervention, relations among concepts in the ontology, other than isas.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y28">
<title id=" W01-1004.xml">using hlt for acquiring retrieving and publishing knowledge in akt </title>
<section> knowledge extraction.  </section>
<citcontext>
<prevsection>
<prevsent>the goal for research in adaptive ie is to create systems adaptable to new applications/domains by using only an analysts knowledge, i.e. knowledge about the domain/scenario.
</prevsent>
<prevsent>there are two directions of research in adaptive ie, both involving the use of machine learning.
</prevsent>
</prevsection>
<citsent citstr=" C00-2136 ">
on the one hand machine learning is used to automate as much as possible the tasks an ie expert would perform in application development (cardie 1997) (yangarber et al  2000).<papid> C00-2136 </papid></citsent>
<aftsection>
<nextsent>the goal here is to reduce the porting time to new application (and hence the cost).
</nextsent>
<nextsent>this area of research comes mainly from the muc community.
</nextsent>
<nextsent>currently, the technology makes use mainly of nlp-intensive technologies and the type of texts addressed are mainly journal articles.
</nextsent>
<nextsent>on the other hand, there is an attempt to make ie systems adaptable to new domains/applications by using only an analysts knowledge, i.e. knowledge about the domain/scenario only (kushmerick et al  1997), (califf 1998), (muslea et al  1998), (freitag and mccallum 1999), (soderland 1999), (freitag and kushmerick 2000), (ciravegna 2001a).most research has so far focused on web related texts (e.g. web pages, email, etc.) successful commercial products have been created and there is an increasing interest on ie in the web-related market.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y29">
<title id=" W01-1004.xml">using hlt for acquiring retrieving and publishing knowledge in akt </title>
<section> knowledge publishing.  </section>
<citcontext>
<prevsection>
<prevsent>the framework incorporates powerful agent modelling module, which is used to tailor the explanations to the users knowledge, task, and preferences.
</prevsent>
<prevsent>we are now also extending the personal isation techniques to account for user interests.
</prevsent>
</prevsection>
<citsent citstr=" W01-1002 ">
the main challenge for nlg will be to develop robust and efficient techniques for knowledge publishing which can operate on large-scale knowledge resources and support the personalised presentation of diverse information, such as speech, video, text, graphics (see (maybury 2001)).<papid> W01-1002 </papid></citsent>
<aftsection>
<nextsent>the other challenge in using nlg for knowledge publishing is to develop tools and techniques that will enable knowledge engineers, instead of linguists, to create and customise the linguistic resources (e.g., domain lexicon) at the same time as they create and editthe ontology.
</nextsent>
<nextsent>in order to allow such interoperability with the ka tools, we will integrate the nlg tools in the gate infrastructure, discussed next.
</nextsent>
<nextsent>the range and complexity of the task of knowledge management make imperative the need for standardisation.
</nextsent>
<nextsent>while there has been much talk about the re-use of knowledge components such ontologies, much less has been undertaken to standardise the infrastructure for tools and their development.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y30">
<title id=" W01-1004.xml">using hlt for acquiring retrieving and publishing knowledge in akt </title>
<section> hlt infrastructure.  </section>
<citcontext>
<prevsection>
<prevsent>while there has been much talk about the re-use of knowledge components such ontologies, much less has been undertaken to standardise the infrastructure for tools and their development.
</prevsent>
<prevsent>the types of data structures typically involved are large and complex, and without good tools to manage and allow succinct viewing of the data we will continue to work below our potential.
</prevsent>
</prevsection>
<citsent citstr=" A97-1035 ">
the university of sheffield has pioneered in the gate and gate 2 projects the development of an architecture for text engineering (cunningham et al  1997), (<papid> A97-1035 </papid>cunningham et al  2000).</citsent>
<aftsection>
<nextsent>given the modular architecture and component structure of gate, it is natural to build on this basis to extend the capabilities of gate so as to provide the most suitable possible environment for tool development, implementation and evaluation in akt.
</nextsent>
<nextsent>the system will provide single interaction and deployment point for the roll-out of hlt in knowledge management.
</nextsent>
<nextsent>we expect gate2 to act as the skeleton for large range of knowledge management activities within akt and plan to extend its capabilities within the life of the akt project by integrating with suitable onto logical and lexical databases in order to permit the use of the gate system with large bodies of heterogeneous data
</nextsent>
<nextsent>we have presented how we plan to use hlt for helping km in akt.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y31">
<title id=" W01-0502.xml">a sequential model for multiclass classification </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>examples include part-of speech tagging, word-sense disambiguation, accent restoration, word choice selection in machine translation, context-sensitive spelling correction, word selection in speech recognition and identifying discourse markers.
</prevsent>
<prevsent>machine learning methods have become themost popular technique in variety of classification problems of these sort, and have shown significant success.
</prevsent>
</prevsection>
<citsent citstr=" P94-1013 ">
a partial list consists of bayesian classifiers (gale et al, 1993), decision lists (yarowsky, 1994), <papid> P94-1013 </papid>bayesian hybrids (gold ing, 1995), <papid> W95-0104 </papid>hmms (charniak, 1993), inductive logic methods (zelle and mooney, 1996), memory this research is supported by nsf grants iis-9801638, iis 0085836 and sbr-987345.based methods (zavrel et al, 1997), <papid> W97-1016 </papid>linear classifiers (roth, 1998; roth, 1999) and transformation based learning (brill, 1995).<papid> J95-4004 </papid>in many of these classification problems significant source of difficulty is the fact that the number of candidates is very large ? all words in words selection problems, all possible tags in tagging problems etc. since general purpose learning algorithms do not handle these multi-class classification problems well (see below), most of the studies do not address the whole problem; rather, small set of candidates (typically two) is first selected, and the classifier is trained to choose among these.</citsent>
<aftsection>
<nextsent>while this approach is important in that it allows the research community to develop better learning methods and evaluate them in range of applications, it is important to realize that an important stage ismissing.
</nextsent>
<nextsent>this could be significant when the classification methods are to be embedded as part ofa higher level nlp tasks such as machine translation or information extraction, where the small set of candidates the classifier can handle may not be fixed and could be hard to determine.
</nextsent>
<nextsent>in this work we develop general approach tothe study of multi-class classifiers.
</nextsent>
<nextsent>we suggest sequential learning model that utilizes (almost) general purpose classifiers to sequentially restrict the number of competing classes while maintaining,with high probability, the presence of the true out come in the candidate set.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y32">
<title id=" W01-0502.xml">a sequential model for multiclass classification </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>examples include part-of speech tagging, word-sense disambiguation, accent restoration, word choice selection in machine translation, context-sensitive spelling correction, word selection in speech recognition and identifying discourse markers.
</prevsent>
<prevsent>machine learning methods have become themost popular technique in variety of classification problems of these sort, and have shown significant success.
</prevsent>
</prevsection>
<citsent citstr=" W95-0104 ">
a partial list consists of bayesian classifiers (gale et al, 1993), decision lists (yarowsky, 1994), <papid> P94-1013 </papid>bayesian hybrids (gold ing, 1995), <papid> W95-0104 </papid>hmms (charniak, 1993), inductive logic methods (zelle and mooney, 1996), memory this research is supported by nsf grants iis-9801638, iis 0085836 and sbr-987345.based methods (zavrel et al, 1997), <papid> W97-1016 </papid>linear classifiers (roth, 1998; roth, 1999) and transformation based learning (brill, 1995).<papid> J95-4004 </papid>in many of these classification problems significant source of difficulty is the fact that the number of candidates is very large ? all words in words selection problems, all possible tags in tagging problems etc. since general purpose learning algorithms do not handle these multi-class classification problems well (see below), most of the studies do not address the whole problem; rather, small set of candidates (typically two) is first selected, and the classifier is trained to choose among these.</citsent>
<aftsection>
<nextsent>while this approach is important in that it allows the research community to develop better learning methods and evaluate them in range of applications, it is important to realize that an important stage ismissing.
</nextsent>
<nextsent>this could be significant when the classification methods are to be embedded as part ofa higher level nlp tasks such as machine translation or information extraction, where the small set of candidates the classifier can handle may not be fixed and could be hard to determine.
</nextsent>
<nextsent>in this work we develop general approach tothe study of multi-class classifiers.
</nextsent>
<nextsent>we suggest sequential learning model that utilizes (almost) general purpose classifiers to sequentially restrict the number of competing classes while maintaining,with high probability, the presence of the true out come in the candidate set.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y33">
<title id=" W01-0502.xml">a sequential model for multiclass classification </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>examples include part-of speech tagging, word-sense disambiguation, accent restoration, word choice selection in machine translation, context-sensitive spelling correction, word selection in speech recognition and identifying discourse markers.
</prevsent>
<prevsent>machine learning methods have become themost popular technique in variety of classification problems of these sort, and have shown significant success.
</prevsent>
</prevsection>
<citsent citstr=" W97-1016 ">
a partial list consists of bayesian classifiers (gale et al, 1993), decision lists (yarowsky, 1994), <papid> P94-1013 </papid>bayesian hybrids (gold ing, 1995), <papid> W95-0104 </papid>hmms (charniak, 1993), inductive logic methods (zelle and mooney, 1996), memory this research is supported by nsf grants iis-9801638, iis 0085836 and sbr-987345.based methods (zavrel et al, 1997), <papid> W97-1016 </papid>linear classifiers (roth, 1998; roth, 1999) and transformation based learning (brill, 1995).<papid> J95-4004 </papid>in many of these classification problems significant source of difficulty is the fact that the number of candidates is very large ? all words in words selection problems, all possible tags in tagging problems etc. since general purpose learning algorithms do not handle these multi-class classification problems well (see below), most of the studies do not address the whole problem; rather, small set of candidates (typically two) is first selected, and the classifier is trained to choose among these.</citsent>
<aftsection>
<nextsent>while this approach is important in that it allows the research community to develop better learning methods and evaluate them in range of applications, it is important to realize that an important stage ismissing.
</nextsent>
<nextsent>this could be significant when the classification methods are to be embedded as part ofa higher level nlp tasks such as machine translation or information extraction, where the small set of candidates the classifier can handle may not be fixed and could be hard to determine.
</nextsent>
<nextsent>in this work we develop general approach tothe study of multi-class classifiers.
</nextsent>
<nextsent>we suggest sequential learning model that utilizes (almost) general purpose classifiers to sequentially restrict the number of competing classes while maintaining,with high probability, the presence of the true out come in the candidate set.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y34">
<title id=" W01-0502.xml">a sequential model for multiclass classification </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>examples include part-of speech tagging, word-sense disambiguation, accent restoration, word choice selection in machine translation, context-sensitive spelling correction, word selection in speech recognition and identifying discourse markers.
</prevsent>
<prevsent>machine learning methods have become themost popular technique in variety of classification problems of these sort, and have shown significant success.
</prevsent>
</prevsection>
<citsent citstr=" J95-4004 ">
a partial list consists of bayesian classifiers (gale et al, 1993), decision lists (yarowsky, 1994), <papid> P94-1013 </papid>bayesian hybrids (gold ing, 1995), <papid> W95-0104 </papid>hmms (charniak, 1993), inductive logic methods (zelle and mooney, 1996), memory this research is supported by nsf grants iis-9801638, iis 0085836 and sbr-987345.based methods (zavrel et al, 1997), <papid> W97-1016 </papid>linear classifiers (roth, 1998; roth, 1999) and transformation based learning (brill, 1995).<papid> J95-4004 </papid>in many of these classification problems significant source of difficulty is the fact that the number of candidates is very large ? all words in words selection problems, all possible tags in tagging problems etc. since general purpose learning algorithms do not handle these multi-class classification problems well (see below), most of the studies do not address the whole problem; rather, small set of candidates (typically two) is first selected, and the classifier is trained to choose among these.</citsent>
<aftsection>
<nextsent>while this approach is important in that it allows the research community to develop better learning methods and evaluate them in range of applications, it is important to realize that an important stage ismissing.
</nextsent>
<nextsent>this could be significant when the classification methods are to be embedded as part ofa higher level nlp tasks such as machine translation or information extraction, where the small set of candidates the classifier can handle may not be fixed and could be hard to determine.
</nextsent>
<nextsent>in this work we develop general approach tothe study of multi-class classifiers.
</nextsent>
<nextsent>we suggest sequential learning model that utilizes (almost) general purpose classifiers to sequentially restrict the number of competing classes while maintaining,with high probability, the presence of the true out come in the candidate set.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y35">
<title id=" W01-0502.xml">a sequential model for multiclass classification </title>
<section> multi-class classification.  </section>
<citcontext>
<prevsection>
<prevsent>in conventional spell ers, the output of this classifier is then givento the user who selects the intended word.
</prevsent>
<prevsent>in context sensitive spelling correction (golding and roth,1999; mangu and brill, 1997) an additional classifier is then utilized to predict among words that are supported by the first classifier, using contextual and lexical information of the surrounding words.
</prevsent>
</prevsection>
<citsent citstr=" P99-1005 ">
in all studies done so far, however, the first classifier ? the confusion sets ? were constructed manually by the researchers.other word predictions tasks have also constructed manually the list of confusion sets (lee and pereira, 1999; <papid> P99-1005 </papid>dagan et al, 1999; lee, 1999) <papid> P99-1004 </papid>and justifications where given as to why this is reasonable way to construct it.</citsent>
<aftsection>
<nextsent>(even-zohar androth, 2000) present similar task in which the confusion sets generation was automated.
</nextsent>
<nextsent>their study also quantified experimentally the advantage in using early classifiers to restrict the size of the confusion set.
</nextsent>
<nextsent>many other nlp tasks, such as pos tagging, name entity recognition and shallow parsing require multi-class classifiers.
</nextsent>
<nextsent>in several of these cases the number of classes could be very large (e.g., pos tagging in some languages, pos tagging when finer proper noun tag is used).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y36">
<title id=" W01-0502.xml">a sequential model for multiclass classification </title>
<section> multi-class classification.  </section>
<citcontext>
<prevsection>
<prevsent>in conventional spell ers, the output of this classifier is then givento the user who selects the intended word.
</prevsent>
<prevsent>in context sensitive spelling correction (golding and roth,1999; mangu and brill, 1997) an additional classifier is then utilized to predict among words that are supported by the first classifier, using contextual and lexical information of the surrounding words.
</prevsent>
</prevsection>
<citsent citstr=" P99-1004 ">
in all studies done so far, however, the first classifier ? the confusion sets ? were constructed manually by the researchers.other word predictions tasks have also constructed manually the list of confusion sets (lee and pereira, 1999; <papid> P99-1005 </papid>dagan et al, 1999; lee, 1999) <papid> P99-1004 </papid>and justifications where given as to why this is reasonable way to construct it.</citsent>
<aftsection>
<nextsent>(even-zohar androth, 2000) present similar task in which the confusion sets generation was automated.
</nextsent>
<nextsent>their study also quantified experimentally the advantage in using early classifiers to restrict the size of the confusion set.
</nextsent>
<nextsent>many other nlp tasks, such as pos tagging, name entity recognition and shallow parsing require multi-class classifiers.
</nextsent>
<nextsent>in several of these cases the number of classes could be very large (e.g., pos tagging in some languages, pos tagging when finer proper noun tag is used).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y37">
<title id=" W01-0502.xml">a sequential model for multiclass classification </title>
<section> example: pos tagging.  </section>
<citcontext>
<prevsection>
<prevsent>(i) relatively large number of classes (about 50).
</prevsent>
<prevsent>(ii) natural decomposition of the feature space to contextual and lexical features.
</prevsent>
</prevsection>
<citsent citstr=" J97-3003 ">
(iii) lexical knowledge (for unknownwords) and the word lemma (for known words) provide, w.h.p, one sided error (mikheev, 1997).<papid> J97-3003 </papid></citsent>
<aftsection>
<nextsent>4.1 the tagger classifiers.
</nextsent>
<nextsent>the domain in our experiment is defined using the following set of features, all of which are computed relative to the target word  2 . contextual features (as in (brill, 1995; <papid> J95-4004 </papid>roth and zelenko, 1998)): <papid> P98-2186 </papid>let ? 2ud fiksu?</nextsent>
<nextsent>2?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y39">
<title id=" W01-0502.xml">a sequential model for multiclass classification </title>
<section> example: pos tagging.  </section>
<citcontext>
<prevsection>
<prevsent>(iii) lexical knowledge (for unknownwords) and the word lemma (for known words) provide, w.h.p, one sided error (mikheev, 1997).<papid> J97-3003 </papid></prevsent>
<prevsent>4.1 the tagger classifiers.</prevsent>
</prevsection>
<citsent citstr=" P98-2186 ">
the domain in our experiment is defined using the following set of features, all of which are computed relative to the target word  2 . contextual features (as in (brill, 1995; <papid> J95-4004 </papid>roth and zelenko, 1998)): <papid> P98-2186 </papid>let ? 2ud fiksu?</citsent>
<aftsection>
<nextsent>2?
</nextsent>
<nextsent>fipy be the tags of the word preceding, (following) the target word, respectively.
</nextsent>
<nextsent>1.
</nextsent>
<nextsent>? 2d fi . 2.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y42">
<title id=" W01-0502.xml">a sequential model for multiclass classification </title>
<section> example: pos tagging.  </section>
<citcontext>
<prevsection>
<prevsent>) in the training data.
</prevsent>
<prevsent>for that reason, in the next experiments we do not use the baseline at all, since it could hide the phenomenon addressed.
</prevsent>
</prevsection>
<citsent citstr=" J95-2001 ">
(in practice, one might want to use more sophisticated baseline, as in (dermatas and kokkinakis, 1995).)<papid> J95-2001 </papid></citsent>
<aftsection>
<nextsent>ff?
</nextsent>
<nextsent>e? ? sm( :fiffi_ &amp; p?
</nextsent>
<nextsent>) sm( fip &amp; _t? ? ) ?fl??
</nextsent>
<nextsent>,?fl ?,wfl??
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y44">
<title id=" W01-1511.xml">covering treebanks with glarf </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>applications using annotated corpora are often, by design, limited by the information found inthose corpora.
</prevsent>
<prevsent>since most english treebanks provide limited predicate-argument (pred-arg) information, parsers based on these treebanks do not produce more detailed predicate argument structures (pred-arg structures).
</prevsent>
</prevsection>
<citsent citstr=" H94-1020 ">
the penn treebank ii (marcus et al, 1994) <papid> H94-1020 </papid>marks subjects (sbj), logical objects of pass ives (lgs), some reduced relative clauses (rrc), as well as other grammatical information, but does not mark each constituent with grammatical role.</citsent>
<aftsection>
<nextsent>in ourview, full pred-arg description of sentence would do just that: assign each constituent grammatical role that relates that constituent to one or more other constituents in the sentence.
</nextsent>
<nextsent>for example, the role head relates constituent to its parent and the role obj relates constituent to the head of its parent.
</nextsent>
<nextsent>we believe that the absence of this detail limits the range of applications for treebank-based parsers.
</nextsent>
<nextsent>in particular, they limit the extent to which it is possible to generalize, e.g., marking ind-obj and obj roles allows one to generalize single pattern to cover two related examples (john gave mary abook?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y45">
<title id=" W01-1511.xml">covering treebanks with glarf </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we designed glarf with four objectives in mind: (1) capturing regularizations non canonical constructions (e.g., pass ives, filler gap constructions, etc.) are represented in termsof their canonical counterparts (simple declarative clauses); (2) representing all phenomena using one simple data structure: the typed feature structure (3) consistently labeling all arguments and adjuncts for phrases with clear heads; and (4) producing clear and consistent pred-args for phrases that do not have heads, e.g., conjoined structures, named entities, etc. ? rather than trying to squeeze these phrases into an x-bar mold, we customized our representations to reflect their head-less properties.
</prevsent>
<prevsent>we believe that framework for pred-arg needs to satisfy these objectives to adequately cover corpus like ptb.we believe that glarf, because of its uniform treatment of pred-arg relations, will be valuable for many applications, including question answering, information extraction, and machine translation.
</prevsent>
</prevsection>
<citsent citstr=" P93-1004 ">
in particular, for mt, we expect it will benefit procedures which learn translation rules from syntactically analyzed parallel corpora, such as (matsumoto et al, 1993; <papid> P93-1004 </papid>meyers et al, 1996).<papid> C96-1078 </papid></citsent>
<aftsection>
<nextsent>much closer alignments willbe possible using glarf, because of its multiple levels of representation, than would be possible with surface structure alone (an example is provided at the end of section 2).
</nextsent>
<nextsent>for this reason, we are currently investigating the extension of our mapping procedure to treebanks of japanese (the kyoto corpus) and spanish (the uam treebank (moreno et al, 2000)).
</nextsent>
<nextsent>ultimately, we intend to create parallel trilingual treebank using combination of automatic methods and human correction.
</nextsent>
<nextsent>such treebank would be valuable resource for corpus-trained mt systems.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y46">
<title id=" W01-1511.xml">covering treebanks with glarf </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we designed glarf with four objectives in mind: (1) capturing regularizations non canonical constructions (e.g., pass ives, filler gap constructions, etc.) are represented in termsof their canonical counterparts (simple declarative clauses); (2) representing all phenomena using one simple data structure: the typed feature structure (3) consistently labeling all arguments and adjuncts for phrases with clear heads; and (4) producing clear and consistent pred-args for phrases that do not have heads, e.g., conjoined structures, named entities, etc. ? rather than trying to squeeze these phrases into an x-bar mold, we customized our representations to reflect their head-less properties.
</prevsent>
<prevsent>we believe that framework for pred-arg needs to satisfy these objectives to adequately cover corpus like ptb.we believe that glarf, because of its uniform treatment of pred-arg relations, will be valuable for many applications, including question answering, information extraction, and machine translation.
</prevsent>
</prevsection>
<citsent citstr=" C96-1078 ">
in particular, for mt, we expect it will benefit procedures which learn translation rules from syntactically analyzed parallel corpora, such as (matsumoto et al, 1993; <papid> P93-1004 </papid>meyers et al, 1996).<papid> C96-1078 </papid></citsent>
<aftsection>
<nextsent>much closer alignments willbe possible using glarf, because of its multiple levels of representation, than would be possible with surface structure alone (an example is provided at the end of section 2).
</nextsent>
<nextsent>for this reason, we are currently investigating the extension of our mapping procedure to treebanks of japanese (the kyoto corpus) and spanish (the uam treebank (moreno et al, 2000)).
</nextsent>
<nextsent>ultimately, we intend to create parallel trilingual treebank using combination of automatic methods and human correction.
</nextsent>
<nextsent>such treebank would be valuable resource for corpus-trained mt systems.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y47">
<title id=" W01-1511.xml">covering treebanks with glarf </title>
<section> previous treebanks.  </section>
<citcontext>
<prevsection>
<prevsent>similarly, attempts to convert susanne into the ptb framework would lose information.
</prevsent>
<prevsent>in summary, glarfs ability to represent varying levels of detail allows different types of treebank formats to be converted into glarf, even if they cannot be converted into each other.
</prevsent>
</prevsection>
<citsent citstr=" W97-0307 ">
perhaps, glarf can become lingua franca among annotated treebanks.the negra corpus (brants et al, 1997) <papid> W97-0307 </papid>provides pred-arg information for german, similar in granularity to glarf.</citsent>
<aftsection>
<nextsent>the most significant difference is that glarf regularizes some phenomena which negra version of english would probably not, e.g., control phenomena.
</nextsent>
<nextsent>another novel feature of glarf is the ability to represent paraphrases (in the harri sian sense) that are not entirely syntactic, e.g., nominalizations as sentences.
</nextsent>
<nextsent>other schemes seem to only regularize strictly syntactic phenomena.
</nextsent>
<nextsent>in glarf, each sentence is represented by typed feature structure.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y49">
<title id=" W01-1511.xml">covering treebanks with glarf </title>
<section> the structure of glarf.  </section>
<citcontext>
<prevsection>
<prevsent>these two roles are represented as two arcs which share the same head.
</prevsent>
<prevsent>this sort of structure sharing analysis originates with relational grammar andre lated frameworks (perlmutter, 1984; johnson and postal, 1980) and is common in feature structure frameworks (lfg, hpsg, etc.).
</prevsent>
</prevsection>
<citsent citstr=" P93-1014 ">
following (john sonet al, 1993)<papid> P93-1014 </papid>2, arcs are typed.</citsent>
<aftsection>
<nextsent>there are five different types of role labels: attribute roles: gram-number (grammatical number), mood, tense, sem-feature (se mantic features like temporal/locative), etc.  surface-only relations (prefixed with s-), e.g., the surface subject (s-sbj) of passive.
</nextsent>
<nextsent> logical-only roles (prefixed with l-), e.g., the logical object (l-obj) of passive. intermediate roles (prefixed with i-) representing neither surface, nor logical positions.
</nextsent>
<nextsent>in john seemed to be kidnapped by aliens?, john?
</nextsent>
<nextsent>is the surface subject of seem?, the logical object of kidnapped?, and the intermediate subject of to be?.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y52">
<title id=" W00-0404.xml">extracting key paragraph based on topic and event detection towards multi document summarization </title>
<section> the first was named wednesday as voni lynn.  </section>
<citcontext>
<prevsection>
<prevsent>fi- nally~ we report some experiments using the tdt1 corpus which has been developed by the tdt (topic detection and tracking) pilot study (allan and carbonell, 1998) with discussion of evaluation.
</prevsent>
<prevsent>2 domain dependency f words.
</prevsent>
</prevsection>
<citsent citstr=" A97-1043 ">
the domain dependency of words that how strongly word features given set of data (documents) con-tributes to event extraction, as we previously re-ported (fukumoto et al: 1997).<papid> A97-1043 </papid></citsent>
<aftsection>
<nextsent>in the study, we hypothesi~d that the articles from the wall street journal corpus can be structured by three levels, i.e. domain, article and paragraph.
</nextsent>
<nextsent>it word is nil event in given article, it satisfies the two conditions: (1) the dispersion value of the word in the paragraph level is smaller than that of the art.iele, since the .word appears throughout paragr~q~hs in the para-graph level rather than articles in the article level.
</nextsent>
<nextsent>(2) the dispersion value of the word in the arti-cle is smaller than that of the domain, as the word appears across articles rather than domains.
</nextsent>
<nextsent>however, ~here are two problems to adapt it to multl-document summarization task.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y53">
<title id=" W00-0404.xml">extracting key paragraph based on topic and event detection towards multi document summarization </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>5.1 data.
</prevsent>
<prevsent>the tdt1 corpus comprises set of documents (.15,863) that includes both newswire (reuters) 7..965 and manual transcription of the broadcast news speech (cnn) 7,898 documents.
</prevsent>
</prevsection>
<citsent citstr=" A92-1021 ">
a set of 25 target events were defined 2 all documents were tagged by the tagger (brill, 1992).<papid> A92-1021 </papid></citsent>
<aftsection>
<nextsent>%ve used nouns in the documents.
</nextsent>
<nextsent>h t p://morph.ldc.upenn.edu/tdt i i i i i i i ! i 34 i i i i i i / i i 5.2 event extract ion.
</nextsent>
<nextsent>we collected 300 documents from the tdt1 corpus, each of which is mmolated with respect one of 25 events.
</nextsent>
<nextsent>the result is shown in table 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y54">
<title id=" W00-0404.xml">extracting key paragraph based on topic and event detection towards multi document summarization </title>
<section> re la ted  work.  </section>
<citcontext>
<prevsection>
<prevsent>1997).
</prevsent>
<prevsent>the alternative approach largely escapes this con-straint, by viewing the task as one of identi~,ing certain passages(typically sentences) which, by some metric, are deemed to be the most representative, of the document content.
</prevsent>
</prevsection>
<citsent citstr=" C96-2166 ">
a variety of approaches ex-ist for determining the salient sentences in the text: statistical techniques based oll word distribution (kupiec et al, 1995), (zechner, 1996), (<papid> C96-2166 </papid>salton et al., 1991), (teufell and moens, 1997), symbolic tech-niques based on discourse structure (marcu, 1997) <papid> W97-0713 </papid>and semantic relations between words (barzil~v and elhadad, 1997).</citsent>
<aftsection>
<nextsent>all of their results demonstrate hat passage xtraction techniques are useful first step in document summarization, although most of them have focused on single document.
</nextsent>
<nextsent>some researchers have started to apply single-document summarization technique to multi- document.
</nextsent>
<nextsent>stein et. al. proposed method for summarizing multi-document using single-document summarizer (stralkowsik et al, 1998), (stralkowski et al. 1999).
</nextsent>
<nextsent>their method first summarizes each document of multi-document, then groups the sum-maries in clusters and finally, orders these summaries in logical way (stein et al, 1999).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y55">
<title id=" W00-0404.xml">extracting key paragraph based on topic and event detection towards multi document summarization </title>
<section> re la ted  work.  </section>
<citcontext>
<prevsection>
<prevsent>1997).
</prevsent>
<prevsent>the alternative approach largely escapes this con-straint, by viewing the task as one of identi~,ing certain passages(typically sentences) which, by some metric, are deemed to be the most representative, of the document content.
</prevsent>
</prevsection>
<citsent citstr=" W97-0713 ">
a variety of approaches ex-ist for determining the salient sentences in the text: statistical techniques based oll word distribution (kupiec et al, 1995), (zechner, 1996), (<papid> C96-2166 </papid>salton et al., 1991), (teufell and moens, 1997), symbolic tech-niques based on discourse structure (marcu, 1997) <papid> W97-0713 </papid>and semantic relations between words (barzil~v and elhadad, 1997).</citsent>
<aftsection>
<nextsent>all of their results demonstrate hat passage xtraction techniques are useful first step in document summarization, although most of them have focused on single document.
</nextsent>
<nextsent>some researchers have started to apply single-document summarization technique to multi- document.
</nextsent>
<nextsent>stein et. al. proposed method for summarizing multi-document using single-document summarizer (stralkowsik et al, 1998), (stralkowski et al. 1999).
</nextsent>
<nextsent>their method first summarizes each document of multi-document, then groups the sum-maries in clusters and finally, orders these summaries in logical way (stein et al, 1999).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y56">
<title id=" W00-0404.xml">extracting key paragraph based on topic and event detection towards multi document summarization </title>
<section> re la ted  work.  </section>
<citcontext>
<prevsection>
<prevsent>however, as she admits, (i) the order the information should not only depend on topic cov-ered, (ii) background information that helps clari~  related information should be placed first.
</prevsent>
<prevsent>more seri-ously, as barzilay and mani claim, summarization of multiple documents requires information about sim-ilarities and differences across documents.
</prevsent>
</prevsection>
<citsent citstr=" P99-1071 ">
there-fore it is difficult to identi~  these information using single-document summarizer technique (mani and bloedorn, 1997), (barzilay et al, 1999).<papid> P99-1071 </papid></citsent>
<aftsection>
<nextsent>a method proposed by mani et. al. deal with the problem, i.e. they tried to detect the similar-ities and differences in information content among documents (mani and bloedorn, 1997).
</nextsent>
<nextsent>they used spreading activation algorithm and graph match-ing in order to identify similarities and differences across documents.
</nextsent>
<nextsent>the output is presented as set of paragraphs with similar and unique words high-lighted.
</nextsent>
<nextsent>however, if the same information is men- nun: table 4: the results of key paragraph extraction accuracy %10 paa correct(%) para 2 58 44(75.8) 117 4 107 80(74.7) 214 8 202 138(68.3) 404 16 281 175(62~) 563 total 648 437(67.4) 1,298 %20 correct(%) para 91(77.7) 175 160(74.7) 321 278(68.8) 606 361(64.1) 844 890(68.5) 1,946 total correct(%) 135(77.1) 240(74.7) 416(68.6) 536(63.5) 1,327(68.1)  tioned several times in different documents, much of the summary will be redundant.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y60">
<title id=" W00-0404.xml">extracting key paragraph based on topic and event detection towards multi document summarization </title>
<section> re la ted  work.  </section>
<citcontext>
<prevsection>
<prevsent>they used paraphrasing rules which are maaaually derived from the result of syntactic analysis to iden-tify theme intersection and used language generation to reformulate them as coherent, summary.
</prevsent>
<prevsent>while promising to obtain high accuracy: the result of sum-marization task has not been reported.
</prevsent>
</prevsection>
<citsent citstr=" J98-3005 ">
like mani and barzil~, techniques, our ap-proach focuses on the problem that how to identi~  differences and similarities across documents, rather than the problem that how to form the actual sum- mar:,, (sparck, 1993), (mckeown and radev, 1995), (radev and mckeown, 1998).<papid> J98-3005 </papid></citsent>
<aftsection>
<nextsent>however, while barzi- lav approach used paraphrasing rules to eliminate redmadancy in summary, we proposed omain de-pendency of words to address robustness of the tech-nique.
</nextsent>



</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y61">
<title id=" W00-1301.xml">pattern based disambiguation for natural language processing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>while the n-gram reins supreme in language modeling, there has been some interesting work done building language models based on linguistically richer features.
</prevsent>
<prevsent>bahl, brown et al (1989) describe language model that builds decision tree that is allowed to ask questions about the history up to twenty words back.
</prevsent>
</prevsection>
<citsent citstr=" W97-0309 ">
saul and pereira (1997) <papid> W97-0309 </papid>describe language model that can in essence skip over uninformative words in the history.</citsent>
<aftsection>
<nextsent>della pietra et al (1994) discuss an approach to language modeling based on link grammars, where the model can look beyond the two previous words to condition on linguistically relevant words in the history.
</nextsent>
<nextsent>the language model described by chelba and jelinek (1998) <papid> P98-1035 </papid>similarly conditions on linguistically relevant words by assigning partial phrase structure to the history and percolating headwords.</nextsent>
<nextsent>samuellson, tapanainen et al (1996) describe method for learning particular pattern-based disambiguation for natural language processing useful type of pattern, which they call barrier.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y62">
<title id=" W00-1301.xml">pattern based disambiguation for natural language processing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>saul and pereira (1997) <papid> W97-0309 </papid>describe language model that can in essence skip over uninformative words in the history.</prevsent>
<prevsent>della pietra et al (1994) discuss an approach to language modeling based on link grammars, where the model can look beyond the two previous words to condition on linguistically relevant words in the history.</prevsent>
</prevsection>
<citsent citstr=" P98-1035 ">
the language model described by chelba and jelinek (1998) <papid> P98-1035 </papid>similarly conditions on linguistically relevant words by assigning partial phrase structure to the history and percolating headwords.</citsent>
<aftsection>
<nextsent>samuellson, tapanainen et al (1996) describe method for learning particular pattern-based disambiguation for natural language processing useful type of pattern, which they call barrier.
</nextsent>
<nextsent>given two symbols and y, and set of symbols s, they learn conditions of the form: take an action if there is an preceded by y, with no intervening symbols from s. in their paper they demonstrate how such patterns can be useful for part of speech tagging.
</nextsent>
<nextsent>even-zohar and roth (2000) show that by including linguistic features based on relations such as subject and object, they can better disambiguate between verb pairs.
</nextsent>
<nextsent>below we provide the standard definition for regular expressions, and then define less expressive language formafism, which we will refer to as reduced regular expressions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y63">
<title id=" W02-0227.xml">a minimum message length approach for argument interpretation </title>
<section> related research.  </section>
<citcontext>
<prevsection>
<prevsent>the results of our evaluation are reported in section 5, followed by concluding remarks.
</prevsent>
<prevsent>our research integrates plan recognition for discourse understanding with the application of the mml principle (wallace and boulton, 1968).
</prevsent>
</prevsection>
<citsent citstr=" J99-1001 ">
the system described in (carberry and lambert,1999) <papid> J99-1001 </papid>recognized users intentions during expert consultation dialogues.</citsent>
<aftsection>
<nextsent>this system considered several knowledge sources for discourse understanding.it used plan libraries as its main knowledge representation formalism, and handled short conversational turns.
</nextsent>
<nextsent>in contrast, our system relies on bns and handles unrestricted arguments.bns have been used in several systems that perform plan recognition for discourse understanding, e.g., (charniak and goldman, 1993; horvitz andpaek, 1999; zukerman, 2001).
</nextsent>
<nextsent>charniak and goldmans system handled complex narratives, using abn and marker passing for plan recognition.
</nextsent>
<nextsent>it automatically built and incrementally extended bn from propositions read in story, so that the bn represented hypotheses that became plausible as the story unfolded.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y64">
<title id=" W00-1015.xml">flexible speech act based dialogue management </title>
<section> conclusion.  </section>
<citcontext>
<prevsection>
<prevsent>we concen-trated on how to dynamically calculate collec-tion of predictions for how to continue dialogue (dialogue primitives), how to account for differ-ent dialogue strategies and utterances with sev-eral communicative goals through combinations of primitives, and how to map the user utterances onto primitives.
</prevsent>
<prevsent>the approach as been imple-mented and tested in several prototype systems, e.g., horoscope, movie, and telephone rate service (feldes et al, 1998).
</prevsent>
</prevsection>
<citsent citstr=" E91-1015 ">
dialogue grammars have previously been used to manage dialogue (bunt, 1989; bilange, 1991; <papid> E91-1015 </papid>traum and hinkelman, 1992; jsnsson, 1993; mast et al, 1994; novick and sutton, 1994; <papid> P94-1014 </papid>chino and tsuboi, 1996), but we are not aware of an ap-proach where speech acts are translated into collection of primitives with propositional content.</citsent>
<aftsection>
<nextsent>previous grammar approaches use the speech acts directly or assume one-to-one correspondence between utterance and speech act.
</nextsent>
<nextsent>through the natural division of the knowledge into type and content, we have achieved flex-ible dialogue manager that adapts to users  be-haviour.
</nextsent>
<nextsent>we can take advantage of the predictive capabilites of speech act grammars and still be able to account for multi-functional utterances.
</nextsent>
<nextsent>we have also demonstrated that our approach is flexible: 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y65">
<title id=" W00-1015.xml">flexible speech act based dialogue management </title>
<section> conclusion.  </section>
<citcontext>
<prevsection>
<prevsent>we concen-trated on how to dynamically calculate collec-tion of predictions for how to continue dialogue (dialogue primitives), how to account for differ-ent dialogue strategies and utterances with sev-eral communicative goals through combinations of primitives, and how to map the user utterances onto primitives.
</prevsent>
<prevsent>the approach as been imple-mented and tested in several prototype systems, e.g., horoscope, movie, and telephone rate service (feldes et al, 1998).
</prevsent>
</prevsection>
<citsent citstr=" P94-1014 ">
dialogue grammars have previously been used to manage dialogue (bunt, 1989; bilange, 1991; <papid> E91-1015 </papid>traum and hinkelman, 1992; jsnsson, 1993; mast et al, 1994; novick and sutton, 1994; <papid> P94-1014 </papid>chino and tsuboi, 1996), but we are not aware of an ap-proach where speech acts are translated into collection of primitives with propositional content.</citsent>
<aftsection>
<nextsent>previous grammar approaches use the speech acts directly or assume one-to-one correspondence between utterance and speech act.
</nextsent>
<nextsent>through the natural division of the knowledge into type and content, we have achieved flex-ible dialogue manager that adapts to users  be-haviour.
</nextsent>
<nextsent>we can take advantage of the predictive capabilites of speech act grammars and still be able to account for multi-functional utterances.
</nextsent>
<nextsent>we have also demonstrated that our approach is flexible: 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y66">
<title id=" W00-0101.xml">sentences vs phrases syntactic complexity in multimedia information retrieval </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>complicating the problem was the fact that we had large collection (400,000+ images), and creating test subset meant that most queries would generate almost no relevant results.
</prevsent>
<prevsent>finally, we wanted to focus more on precision than on recall, because our work with users had made it clear that precision was far more important in this application.
</prevsent>
</prevsection>
<citsent citstr=" P98-1066 ">
to evaluate precision at 20 for this collection, we used the crossing measure introduced in flank 1998.<papid> P98-1066 </papid></citsent>
<aftsection>
<nextsent>the crossing measure (in which any image ranked above another, better-matching image counts as an error) is both finer-grained and better suited to ranking application in which user evaluations are not binary.
</nextsent>
<nextsent>we calibrated the crossing measure (on subset of the queries) as follows: precision at 20 images for all terms 53 precision at 5 images for all 59 terms precision at 20 images for 100 any term crossing measure at 20 91 images that is, we calculated the precision  for all terms  as binary measure with respect to query, and scored an error if any terms in the query were not matched.
</nextsent>
<nextsent>for the  any term  precision measure, we scored an error only if the image failed to match any term in the query in such way that user would consider it partial match.
</nextsent>
<nextsent>thus, for example, for an  all terms  match, tall glass of beer succeeded only when the images howed (and captions mentioned) all three terms tall, glass, and beer, or their synonyms.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y67">
<title id=" W02-0207.xml">annotating the semantic consistency of speech recognition hypotheses </title>
<section> id like the video are.  </section>
<citcontext>
<prevsection>
<prevsent>5 reliability of annotations.
</prevsent>
<prevsent>5.1 the kappa statistic.
</prevsent>
</prevsection>
<citsent citstr=" J96-2004 ">
to measure the reliability of annotations we used the kappa statistic (carletta, 1996).<papid> J96-2004 </papid></citsent>
<aftsection>
<nextsent>the value of kappa statistic (k) for semantic consistency in our experiment was 0.58, which shows that there was not high level of agreement between annotators7.
</nextsent>
<nextsent>in the field of content analysis, where the kappa statistic originated, 0.8 is usually taken to indicate good reliability, 0.68 0.8 allows to draw tentative conclusions.
</nextsent>
<nextsent>the distribution of semantic consistency classes and domain assignments is given in fig.
</nextsent>
<nextsent>2.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y68">
<title id=" W01-0803.xml">document structuring a la sdrt </title>
<section> comparison with rst.  </section>
<citcontext>
<prevsection>
<prevsent>since rst is descriptive theory without any formal background, there exists wide range of interpretations and several notions of rhetorical structure.
</prevsent>
<prevsent>for some authors, e.g.
</prevsent>
</prevsection>
<citsent citstr=" W00-1403 ">
(marcu et al, 2000), <papid> W00-1403 </papid>the rhetorical structure is very surfacic: it is an ordered tree isomorphic to the linear ized structure of the text and rhetorical relation can be viewed as nickname for small set of cue phrases.</citsent>
<aftsection>
<nextsent>for other authors, the rhetorical structure is more abstract: it aims at representing meaning.for example, in (rags project, 1999; bouayad agha et al, 2000), the rhetorical structure is an unordered tree in which terminal nodes represent elementary propositions, while nonterminal nodes represent rhetorical relations which are abstract relations such as cause.
</nextsent>
<nextsent>this rhetorical representation is mapped into document representation which is an ordered tree reflecting the sur facic structure of the text.
</nextsent>
<nextsent>our approach is closer to the ragsone if we consider our logical form as equivalent to their rhetorical structures.
</nextsent>
<nextsent>however, we differ basically on the following point: their rhetorical structure is tree, while our logical form, when graphically represented, is (connex) graph andnot tree.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y69">
<title id=" W00-0105.xml">dependency of context based word sense disambiguation from representation and domain complexity </title>
<section> given an input space x~*  of.  </section>
<citcontext>
<prevsection>
<prevsent>thirdly, some of the features used in representation may be dependent from other features, and again the model would result unnecessarily complex.
</prevsent>
<prevsent>the problem of noise and over fitting are well known in the area of machine learning (russell and norvig (1999)), therefore we will not discuss the matter in detail here.
</prevsent>
</prevsection>
<citsent citstr=" J99-2002 ">
an analysis of this issue as applied to probabilistic wsd learners may be found in bruce and wiebe (1999).<papid> J99-2002 </papid></citsent>
<aftsection>
<nextsent>for the purpose of this paper, we assume that the representation space is optimized with respect to the choice of the relevant model parameters.
</nextsent>
<nextsent>our objective will be to determine the size of s, given and c, and given certain performance objectives.
</nextsent>
<nextsent>as we said, the aim of wsd learning process, when instructed with sequence of examples in x, is to produce an hypothesis which, in some sense,  corresponds  to the 29 concept under consideration.
</nextsent>
<nextsent>because is finite sequence, only concepts with finite number of positive examples can be learned with total success, i.e. the learner can output an hypothesis h= c~ . in general, and this is the case for linguistic oncepts, we can only hope that is good approximation of ci..
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y72">
<title id=" W01-0719.xml">combining linguistic and machine learning techniques for email summarization </title>
<section> machine learning for content.  </section>
<citcontext>
<prevsection>
<prevsent>this section describes the three steps involved in this classification task:1) what representation is appropriate for the information to be classified as relevant or non-relevant (candidate phrases), 2) which features should be associated with each candidate, 3) which classification models should be used.
</prevsent>
<prevsent>case 1 cnp: scientific/jj and/cc technical/jj articles/nns snp1: scientific/jj articles/nns snp2: technical/jj articles/nns case 2 cnp: scientific/jj thesauri/nns and databases/nns snp1: scientific/jj thesauri/nns snp2: scientific/jj databases/nns case 3 cnp: physics/nn and/cc biology/nn skilled/jj researchers/nns snp1: physics/nn skilled/jj researchers/nns snp2: biology/nn skilled/jj researchers/nns table 1: resolving coordination of nps 2.1 candidate phrases.
</prevsent>
</prevsection>
<citsent citstr=" W98-0610 ">
of the major syntactic constituents of sentence, e.g. noun phrases, verb phrases, and prepositional phrases, we assume that noun phrases (nps) carry the most content ful information about the document, well-supported hypothesis (smeaton, 1999; wacholder, 1998).<papid> W98-0610 </papid>as considered by wacholder (1998), <papid> W98-0610 </papid>the simple nps are the maximal nps that contain premodifiers but not post-nominal constituents such as prepositions or clauses.</citsent>
<aftsection>
<nextsent>we chose simple npsfor content representation because they are semantically and syntactically coherent and they are less ambiguous than complex nps.
</nextsent>
<nextsent>for extracting simple noun phrases we first used ramshaw andmarcuss base np chunker (ramshaw and marcus, 1995).<papid> W95-0107 </papid></nextsent>
<nextsent>the base np is either simple np or coordination of simple nps.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y74">
<title id=" W01-0719.xml">combining linguistic and machine learning techniques for email summarization </title>
<section> machine learning for content.  </section>
<citcontext>
<prevsection>
<prevsent>of the major syntactic constituents of sentence, e.g. noun phrases, verb phrases, and prepositional phrases, we assume that noun phrases (nps) carry the most content ful information about the document, well-supported hypothesis (smeaton, 1999; wacholder, 1998).<papid> W98-0610 </papid>as considered by wacholder (1998), <papid> W98-0610 </papid>the simple nps are the maximal nps that contain premodifiers but not post-nominal constituents such as prepositions or clauses.</prevsent>
<prevsent>we chose simple npsfor content representation because they are semantically and syntactically coherent and they are less ambiguous than complex nps.</prevsent>
</prevsection>
<citsent citstr=" W95-0107 ">
for extracting simple noun phrases we first used ramshaw andmarcuss base np chunker (ramshaw and marcus, 1995).<papid> W95-0107 </papid></citsent>
<aftsection>
<nextsent>the base np is either simple np or coordination of simple nps.
</nextsent>
<nextsent>we used heuristics based on pos tags to automatically split the coordinate nps into simple ones, properly assigning the premodifiers.
</nextsent>
<nextsent>table 1 presents some coordinate nps (cnp) encountered in our data collection and the results of our algorithm which split them into simple nps (snp1 and snp2).
</nextsent>
<nextsent>2.2 features used for classification.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y75">
<title id=" W01-0719.xml">combining linguistic and machine learning techniques for email summarization </title>
<section> machine learning for content.  </section>
<citcontext>
<prevsection>
<prevsent>conference deadline in the case of the first noun phrase, for example, its importance is found in the two noun mod ifiers: conference and workshop as much as in the head announcement, due to their presence as heads or modifiers in the candidate nps 2-4.
</prevsent>
<prevsent>our new feature will be: tf  idf conference +tf  idf workshop + tf  idf announcement . giving.
</prevsent>
</prevsection>
<citsent citstr=" W01-1011 ">
these linguistic observations we divided the set of features into three groups, as we mentioned also in (tzoukermann et al, 2001): <papid> W01-1011 </papid>1) one associated with the head of the noun phrase; 2) one associated with the whole np and 3) one that represents the new tf*idf measure discussed above.</citsent>
<aftsection>
<nextsent>2.2.1 features associated with the head we choose two features to characterize the head of the noun phrases:  head tfidf: the tf*idf measure of the head of the candidate np.
</nextsent>
<nextsent>for the np in example (1) this feature will be tf  idf announcement . head focc: the position of the first occurrence of the head in text (the number of words that precede the first occurrence of the head divided by the total number of words in the document).
</nextsent>
<nextsent>2.2.2 features associated with the whole np we select six features that we consider relevant in determining the relative importance of the noun phrase:  np tfidf: the tf*idf measure of the whole np.
</nextsent>
<nextsent>for the np in the example (1) this feature will be tf  idf conference workshop announcement .  np focc: the position of the first occurrence of the noun phrase in the document. np length words: noun phrase length measured in number of words, normalized by dividing it with the total number of words in the candidate np list. np length chars: noun phrase length measured in number of characters, normalized by dividing it with the total number of characters in the candidate nps list.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y76">
<title id=" W00-0402.xml">mining discourse markers for chinese textual summarization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>discourse analysis goes beyond the levels of syntactic and semantic analysis, which typically treats each sentence as an isolated, independent unit.
</prevsent>
<prevsent>the function of discourse analysis is to divide text into discourse segments, and to recognize and re-construct the discourse structure of the text as intended by its author.
</prevsent>
</prevsection>
<citsent citstr=" J81-2001 ">
results of discourse analysis can be used to solve many important nlp problems such as anaphoric reference (hirst 1981), <papid> J81-2001 </papid>tense and aspect analysis (hwang and schubert 1992), <papid> P92-1030 </papid>intention recognition (grosz and sidner 1986; <papid> J86-3001 </papid>litman and allen 1990), or can be directly applied to computational nlp applications uch as text abstraction (ono et al 1994; <papid> C94-1056 </papid>sou et al 1996) and text generation (mckeown 1985; lin et al 1991).</citsent>
<aftsection>
<nextsent>automatic text abstraction has received considerable attention (see paice (1990) for comprehensive view).
</nextsent>
<nextsent>while some statistical approaches have had some success in extracting one or more sentences which can serve as summary (brandow et al 1995; kupiec et al 1995; salton et al 1997), summarization general has remained an elusive task.
</nextsent>
<nextsent>mckeown and radev (1995) develop system summons to summarize full text input using templates produced by the message understanding systems, developed under arpa human language technology.
</nextsent>
<nextsent>unlike previous approaches, their system summarizes series of news articles on the same event, producing paragraph consisting of one or more sentences.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y77">
<title id=" W00-0402.xml">mining discourse markers for chinese textual summarization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>discourse analysis goes beyond the levels of syntactic and semantic analysis, which typically treats each sentence as an isolated, independent unit.
</prevsent>
<prevsent>the function of discourse analysis is to divide text into discourse segments, and to recognize and re-construct the discourse structure of the text as intended by its author.
</prevsent>
</prevsection>
<citsent citstr=" P92-1030 ">
results of discourse analysis can be used to solve many important nlp problems such as anaphoric reference (hirst 1981), <papid> J81-2001 </papid>tense and aspect analysis (hwang and schubert 1992), <papid> P92-1030 </papid>intention recognition (grosz and sidner 1986; <papid> J86-3001 </papid>litman and allen 1990), or can be directly applied to computational nlp applications uch as text abstraction (ono et al 1994; <papid> C94-1056 </papid>sou et al 1996) and text generation (mckeown 1985; lin et al 1991).</citsent>
<aftsection>
<nextsent>automatic text abstraction has received considerable attention (see paice (1990) for comprehensive view).
</nextsent>
<nextsent>while some statistical approaches have had some success in extracting one or more sentences which can serve as summary (brandow et al 1995; kupiec et al 1995; salton et al 1997), summarization general has remained an elusive task.
</nextsent>
<nextsent>mckeown and radev (1995) develop system summons to summarize full text input using templates produced by the message understanding systems, developed under arpa human language technology.
</nextsent>
<nextsent>unlike previous approaches, their system summarizes series of news articles on the same event, producing paragraph consisting of one or more sentences.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y78">
<title id=" W00-0402.xml">mining discourse markers for chinese textual summarization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>discourse analysis goes beyond the levels of syntactic and semantic analysis, which typically treats each sentence as an isolated, independent unit.
</prevsent>
<prevsent>the function of discourse analysis is to divide text into discourse segments, and to recognize and re-construct the discourse structure of the text as intended by its author.
</prevsent>
</prevsection>
<citsent citstr=" J86-3001 ">
results of discourse analysis can be used to solve many important nlp problems such as anaphoric reference (hirst 1981), <papid> J81-2001 </papid>tense and aspect analysis (hwang and schubert 1992), <papid> P92-1030 </papid>intention recognition (grosz and sidner 1986; <papid> J86-3001 </papid>litman and allen 1990), or can be directly applied to computational nlp applications uch as text abstraction (ono et al 1994; <papid> C94-1056 </papid>sou et al 1996) and text generation (mckeown 1985; lin et al 1991).</citsent>
<aftsection>
<nextsent>automatic text abstraction has received considerable attention (see paice (1990) for comprehensive view).
</nextsent>
<nextsent>while some statistical approaches have had some success in extracting one or more sentences which can serve as summary (brandow et al 1995; kupiec et al 1995; salton et al 1997), summarization general has remained an elusive task.
</nextsent>
<nextsent>mckeown and radev (1995) develop system summons to summarize full text input using templates produced by the message understanding systems, developed under arpa human language technology.
</nextsent>
<nextsent>unlike previous approaches, their system summarizes series of news articles on the same event, producing paragraph consisting of one or more sentences.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y79">
<title id=" W00-0402.xml">mining discourse markers for chinese textual summarization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>discourse analysis goes beyond the levels of syntactic and semantic analysis, which typically treats each sentence as an isolated, independent unit.
</prevsent>
<prevsent>the function of discourse analysis is to divide text into discourse segments, and to recognize and re-construct the discourse structure of the text as intended by its author.
</prevsent>
</prevsection>
<citsent citstr=" C94-1056 ">
results of discourse analysis can be used to solve many important nlp problems such as anaphoric reference (hirst 1981), <papid> J81-2001 </papid>tense and aspect analysis (hwang and schubert 1992), <papid> P92-1030 </papid>intention recognition (grosz and sidner 1986; <papid> J86-3001 </papid>litman and allen 1990), or can be directly applied to computational nlp applications uch as text abstraction (ono et al 1994; <papid> C94-1056 </papid>sou et al 1996) and text generation (mckeown 1985; lin et al 1991).</citsent>
<aftsection>
<nextsent>automatic text abstraction has received considerable attention (see paice (1990) for comprehensive view).
</nextsent>
<nextsent>while some statistical approaches have had some success in extracting one or more sentences which can serve as summary (brandow et al 1995; kupiec et al 1995; salton et al 1997), summarization general has remained an elusive task.
</nextsent>
<nextsent>mckeown and radev (1995) develop system summons to summarize full text input using templates produced by the message understanding systems, developed under arpa human language technology.
</nextsent>
<nextsent>unlike previous approaches, their system summarizes series of news articles on the same event, producing paragraph consisting of one or more sentences.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y81">
<title id=" W00-0402.xml">mining discourse markers for chinese textual summarization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>johnson et al (1993) describes text processing system that can identify anaphors that they may be utilized to enhance sentence selection.
</prevsent>
<prevsent>it is based on the assumption that sentences which contain non- anaphoric noun phrases and introduce key concepts into the text are worthy of inclusion in an abstract.
</prevsent>
</prevsection>
<citsent citstr=" C92-3162 ">
ono et al (1994), <papid> C94-1056 </papid>sou et al (1992) <papid> C92-3162 </papid>and marcu (1997) <papid> W97-0713 </papid>focus on discourse structure in summarization using the rhetorical structure theory (rst).</citsent>
<aftsection>
<nextsent>the theory has been exploited in a. number of computational systems (e.g. hovy 1993).
</nextsent>
<nextsent>the main idea is to build discourse tree where each node of the tree represents rst relation.
</nextsent>
<nextsent>summarization is achieved by trimming unimportant sentences on the basis of the relative saliency or rhetorical relations.
</nextsent>
<nextsent>on the other hand, cohesion can also provide context aid in the resolution of ambiguity as well as in text summarization (halliday and hasan 1976; morris and hirst 1991; <papid> J91-1002 </papid>hearst 1997).<papid> J97-1003 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y82">
<title id=" W00-0402.xml">mining discourse markers for chinese textual summarization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>johnson et al (1993) describes text processing system that can identify anaphors that they may be utilized to enhance sentence selection.
</prevsent>
<prevsent>it is based on the assumption that sentences which contain non- anaphoric noun phrases and introduce key concepts into the text are worthy of inclusion in an abstract.
</prevsent>
</prevsection>
<citsent citstr=" W97-0713 ">
ono et al (1994), <papid> C94-1056 </papid>sou et al (1992) <papid> C92-3162 </papid>and marcu (1997) <papid> W97-0713 </papid>focus on discourse structure in summarization using the rhetorical structure theory (rst).</citsent>
<aftsection>
<nextsent>the theory has been exploited in a. number of computational systems (e.g. hovy 1993).
</nextsent>
<nextsent>the main idea is to build discourse tree where each node of the tree represents rst relation.
</nextsent>
<nextsent>summarization is achieved by trimming unimportant sentences on the basis of the relative saliency or rhetorical relations.
</nextsent>
<nextsent>on the other hand, cohesion can also provide context aid in the resolution of ambiguity as well as in text summarization (halliday and hasan 1976; morris and hirst 1991; <papid> J91-1002 </papid>hearst 1997).<papid> J97-1003 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y83">
<title id=" W00-0402.xml">mining discourse markers for chinese textual summarization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the main idea is to build discourse tree where each node of the tree represents rst relation.
</prevsent>
<prevsent>summarization is achieved by trimming unimportant sentences on the basis of the relative saliency or rhetorical relations.
</prevsent>
</prevsection>
<citsent citstr=" J91-1002 ">
on the other hand, cohesion can also provide context aid in the resolution of ambiguity as well as in text summarization (halliday and hasan 1976; morris and hirst 1991; <papid> J91-1002 </papid>hearst 1997).<papid> J97-1003 </papid></citsent>
<aftsection>
<nextsent>mani et al (1998) describes method based on text coherence which models text in terms of macro-level relations between clauses or sentences tohelp determine the overall argumentative structure of the text.
</nextsent>
<nextsent>they examine the extent to which cohesion and coherence can each be used to establish saliency of textual units.
</nextsent>
<nextsent>the sifas (s,yntactic marker based eull- text abstration system) system has been designed and implemented to use discourse markers in the automatic summarization of chinese.
</nextsent>
<nextsent>section 2 provides an introduction to discourse markers in chinese.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y84">
<title id=" W00-0402.xml">mining discourse markers for chinese textual summarization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the main idea is to build discourse tree where each node of the tree represents rst relation.
</prevsent>
<prevsent>summarization is achieved by trimming unimportant sentences on the basis of the relative saliency or rhetorical relations.
</prevsent>
</prevsection>
<citsent citstr=" J97-1003 ">
on the other hand, cohesion can also provide context aid in the resolution of ambiguity as well as in text summarization (halliday and hasan 1976; morris and hirst 1991; <papid> J91-1002 </papid>hearst 1997).<papid> J97-1003 </papid></citsent>
<aftsection>
<nextsent>mani et al (1998) describes method based on text coherence which models text in terms of macro-level relations between clauses or sentences tohelp determine the overall argumentative structure of the text.
</nextsent>
<nextsent>they examine the extent to which cohesion and coherence can each be used to establish saliency of textual units.
</nextsent>
<nextsent>the sifas (s,yntactic marker based eull- text abstration system) system has been designed and implemented to use discourse markers in the automatic summarization of chinese.
</nextsent>
<nextsent>section 2 provides an introduction to discourse markers in chinese.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y85">
<title id=" W00-0402.xml">mining discourse markers for chinese textual summarization </title>
<section> sifas system architecture.  </section>
<citcontext>
<prevsection>
<prevsent>our concern in this project is to identify so in the discourse sense as in (2) in contrast to soused as an adverb in the sentential sense as in (1).
</prevsent>
<prevsent>similar difficulties are found in chinese, as discussed in section 7.
</prevsent>
</prevsection>
<citsent citstr=" P95-1018 ">
from the perspective of discourse analysis, the study of discourse markers basically involves four distinct but fundamental issues: 1) the occurrence and the frequency of occurrence of discourse markers (moser and moore 1995), <papid> P95-1018 </papid>2) determining whether candidate linguistic item is discourse marker (identification / disambiguation) (hirschberg and litman 1993; <papid> J93-3003 </papid>siegel and mckeown 1994), 3) determination or selection of the discourse function of an identified discourse marker (moser and moore 1995), <papid> P95-1018 </papid>and 4) the coverage capabilities (in terms of levels of embedding) among rhetorical relations, as well as among individual discourse markers.</citsent>
<aftsection>
<nextsent>discussion of these problems for chinese compound sentences can be found in wang et al (1994).
</nextsent>
<nextsent>previous attempts to address the above problems in chinese text have usually been based on the investigators  intuition and knowledge, or on small number of constructed examples.
</nextsent>
<nextsent>in our current research, we adopt heuristics-based 13 corpus-based learning to discover the correlation between various linguistic features and different aspects of approaches, and use machine discourse marker usage.
</nextsent>
<nextsent>our research framework statistical analysis discourse analysis text abstraction i natural language 1 \[ understanding \[ ! analysis &amp; application is shown in figure i. raw corpus . ......
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y87">
<title id=" W00-0402.xml">mining discourse markers for chinese textual summarization </title>
<section> sifas system architecture.  </section>
<citcontext>
<prevsection>
<prevsent>our concern in this project is to identify so in the discourse sense as in (2) in contrast to soused as an adverb in the sentential sense as in (1).
</prevsent>
<prevsent>similar difficulties are found in chinese, as discussed in section 7.
</prevsent>
</prevsection>
<citsent citstr=" J93-3003 ">
from the perspective of discourse analysis, the study of discourse markers basically involves four distinct but fundamental issues: 1) the occurrence and the frequency of occurrence of discourse markers (moser and moore 1995), <papid> P95-1018 </papid>2) determining whether candidate linguistic item is discourse marker (identification / disambiguation) (hirschberg and litman 1993; <papid> J93-3003 </papid>siegel and mckeown 1994), 3) determination or selection of the discourse function of an identified discourse marker (moser and moore 1995), <papid> P95-1018 </papid>and 4) the coverage capabilities (in terms of levels of embedding) among rhetorical relations, as well as among individual discourse markers.</citsent>
<aftsection>
<nextsent>discussion of these problems for chinese compound sentences can be found in wang et al (1994).
</nextsent>
<nextsent>previous attempts to address the above problems in chinese text have usually been based on the investigators  intuition and knowledge, or on small number of constructed examples.
</nextsent>
<nextsent>in our current research, we adopt heuristics-based 13 corpus-based learning to discover the correlation between various linguistic features and different aspects of approaches, and use machine discourse marker usage.
</nextsent>
<nextsent>our research framework statistical analysis discourse analysis text abstraction i natural language 1 \[ understanding \[ ! analysis &amp; application is shown in figure i. raw corpus . ......
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y91">
<title id=" W01-0511.xml">classifying the semantic relations in noun compounds via a domain specific lexical hierarchy </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>several approaches have been proposed for empirical noun compound interpretation.
</prevsent>
<prevsent>lauer and dras (1994) point out that there are three components to the problem: identification of the compound from within the text, syntactic analysis of the compound(left versus right association), and the interpretation of the underlying semantics.
</prevsent>
</prevsection>
<citsent citstr=" P95-1007 ">
several researchers have tackled the syntactic analysis (lauer, 1995; <papid> P95-1007 </papid>pustejovsky et al, 1993; <papid> J93-2005 </papid>liberman and sproat,1992), usually using variation of the idea of finding the sub constituents elsewhere in the corpus and using those to predict how the larger compounds are structured.</citsent>
<aftsection>
<nextsent>we are interested in the third task, interpretation of the underlying semantics.
</nextsent>
<nextsent>most related work relies on hand-written rules of one kind or another.finin (1980) examines the problem of noun compound interpretation in detail, and constructs acomplex set of rules.
</nextsent>
<nextsent>vanderwende (1994) <papid> C94-2125 </papid>uses sophisticated system to extract semantic information automatically from an on-line dictionary, and then manipulates set of hand-written rules with hand assigned weights to create an interpretation.</nextsent>
<nextsent>rindflesch et al (2000) use hand-coded rule based systems to extract the factual assertions from biomedical text.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y92">
<title id=" W01-0511.xml">classifying the semantic relations in noun compounds via a domain specific lexical hierarchy </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>several approaches have been proposed for empirical noun compound interpretation.
</prevsent>
<prevsent>lauer and dras (1994) point out that there are three components to the problem: identification of the compound from within the text, syntactic analysis of the compound(left versus right association), and the interpretation of the underlying semantics.
</prevsent>
</prevsection>
<citsent citstr=" J93-2005 ">
several researchers have tackled the syntactic analysis (lauer, 1995; <papid> P95-1007 </papid>pustejovsky et al, 1993; <papid> J93-2005 </papid>liberman and sproat,1992), usually using variation of the idea of finding the sub constituents elsewhere in the corpus and using those to predict how the larger compounds are structured.</citsent>
<aftsection>
<nextsent>we are interested in the third task, interpretation of the underlying semantics.
</nextsent>
<nextsent>most related work relies on hand-written rules of one kind or another.finin (1980) examines the problem of noun compound interpretation in detail, and constructs acomplex set of rules.
</nextsent>
<nextsent>vanderwende (1994) <papid> C94-2125 </papid>uses sophisticated system to extract semantic information automatically from an on-line dictionary, and then manipulates set of hand-written rules with hand assigned weights to create an interpretation.</nextsent>
<nextsent>rindflesch et al (2000) use hand-coded rule based systems to extract the factual assertions from biomedical text.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y93">
<title id=" W01-0511.xml">classifying the semantic relations in noun compounds via a domain specific lexical hierarchy </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>we are interested in the third task, interpretation of the underlying semantics.
</prevsent>
<prevsent>most related work relies on hand-written rules of one kind or another.finin (1980) examines the problem of noun compound interpretation in detail, and constructs acomplex set of rules.
</prevsent>
</prevsection>
<citsent citstr=" C94-2125 ">
vanderwende (1994) <papid> C94-2125 </papid>uses sophisticated system to extract semantic information automatically from an on-line dictionary, and then manipulates set of hand-written rules with hand assigned weights to create an interpretation.</citsent>
<aftsection>
<nextsent>rindflesch et al (2000) use hand-coded rule based systems to extract the factual assertions from biomedical text.
</nextsent>
<nextsent>lapata (2000) classifies nominalizations according to whether the modifier is the subject or the object of the underlying verb expressed by the head noun.1 in the related sub-area of information extraction (cardie, 1997; riloff, 1996), the main goal is to find every instance of particular entities or events of interest.
</nextsent>
<nextsent>these systems use empirical techniques to learn which terms signal entities of interest, in order to fill in pre-defined templates.
</nextsent>
<nextsent>our goals are more general than those of information extraction, andso this work should be helpful for that task.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y94">
<title id=" W01-0511.xml">classifying the semantic relations in noun compounds via a domain specific lexical hierarchy </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>how ever, our approach will not solve issues surrounding previously unseen proper nouns, which are often important for information extraction tasks.there have been several efforts to incorporate lexical hierarchies into statistical processing, primarily for the problem of prepositional phrase (pp) attachment.
</prevsent>
<prevsent>the current standard formulation is: given verb followed by noun and prepositional phrase, represented by the tuple v, n1, p, n2, determine which of or n1 the pp consisting of and n2 attaches to, or is most closely associated with.
</prevsent>
</prevsection>
<citsent citstr=" J93-1005 ">
because the data is sparse, empirical methods that train on word occurrences alone (hindle and rooth,1993) <papid> J93-1005 </papid>have been supplanted by algorithms that generalize one or both of the nouns according to class membership measures (resnik, 1993; resnik and hearst, 1993; <papid> W93-0307 </papid>brill and resnik, 1994; <papid> C94-2195 </papid>li and abe,1998), <papid> J98-2002 </papid>but the statistics are computed for the particular preposition and verb.it is not clear how to use the results of such analysis after they are found; the semantics of the rela 1nominalizations are compounds whose head noun is nominal ized verb and whose modifier is either the subject or the object of the verb.</citsent>
<aftsection>
<nextsent>we do not distinguish the ncs on the basis of their formation.
</nextsent>
<nextsent>tion ship between the terms must still be determined.
</nextsent>
<nextsent>in our framework we would cast this problem as finding the relationship r(p, n2) that best characterizes the preposition and the np that follows it, andthen seeing if the categorization algorithm determines their exists any relationship r?(n1, r(p, n2)) or r?(v,r(p, n2)).
</nextsent>
<nextsent>the algorithms used in the related work reflect the fact that they condition probabilities on particular verb and noun.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y95">
<title id=" W01-0511.xml">classifying the semantic relations in noun compounds via a domain specific lexical hierarchy </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>how ever, our approach will not solve issues surrounding previously unseen proper nouns, which are often important for information extraction tasks.there have been several efforts to incorporate lexical hierarchies into statistical processing, primarily for the problem of prepositional phrase (pp) attachment.
</prevsent>
<prevsent>the current standard formulation is: given verb followed by noun and prepositional phrase, represented by the tuple v, n1, p, n2, determine which of or n1 the pp consisting of and n2 attaches to, or is most closely associated with.
</prevsent>
</prevsection>
<citsent citstr=" W93-0307 ">
because the data is sparse, empirical methods that train on word occurrences alone (hindle and rooth,1993) <papid> J93-1005 </papid>have been supplanted by algorithms that generalize one or both of the nouns according to class membership measures (resnik, 1993; resnik and hearst, 1993; <papid> W93-0307 </papid>brill and resnik, 1994; <papid> C94-2195 </papid>li and abe,1998), <papid> J98-2002 </papid>but the statistics are computed for the particular preposition and verb.it is not clear how to use the results of such analysis after they are found; the semantics of the rela 1nominalizations are compounds whose head noun is nominal ized verb and whose modifier is either the subject or the object of the verb.</citsent>
<aftsection>
<nextsent>we do not distinguish the ncs on the basis of their formation.
</nextsent>
<nextsent>tion ship between the terms must still be determined.
</nextsent>
<nextsent>in our framework we would cast this problem as finding the relationship r(p, n2) that best characterizes the preposition and the np that follows it, andthen seeing if the categorization algorithm determines their exists any relationship r?(n1, r(p, n2)) or r?(v,r(p, n2)).
</nextsent>
<nextsent>the algorithms used in the related work reflect the fact that they condition probabilities on particular verb and noun.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y96">
<title id=" W01-0511.xml">classifying the semantic relations in noun compounds via a domain specific lexical hierarchy </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>how ever, our approach will not solve issues surrounding previously unseen proper nouns, which are often important for information extraction tasks.there have been several efforts to incorporate lexical hierarchies into statistical processing, primarily for the problem of prepositional phrase (pp) attachment.
</prevsent>
<prevsent>the current standard formulation is: given verb followed by noun and prepositional phrase, represented by the tuple v, n1, p, n2, determine which of or n1 the pp consisting of and n2 attaches to, or is most closely associated with.
</prevsent>
</prevsection>
<citsent citstr=" C94-2195 ">
because the data is sparse, empirical methods that train on word occurrences alone (hindle and rooth,1993) <papid> J93-1005 </papid>have been supplanted by algorithms that generalize one or both of the nouns according to class membership measures (resnik, 1993; resnik and hearst, 1993; <papid> W93-0307 </papid>brill and resnik, 1994; <papid> C94-2195 </papid>li and abe,1998), <papid> J98-2002 </papid>but the statistics are computed for the particular preposition and verb.it is not clear how to use the results of such analysis after they are found; the semantics of the rela 1nominalizations are compounds whose head noun is nominal ized verb and whose modifier is either the subject or the object of the verb.</citsent>
<aftsection>
<nextsent>we do not distinguish the ncs on the basis of their formation.
</nextsent>
<nextsent>tion ship between the terms must still be determined.
</nextsent>
<nextsent>in our framework we would cast this problem as finding the relationship r(p, n2) that best characterizes the preposition and the np that follows it, andthen seeing if the categorization algorithm determines their exists any relationship r?(n1, r(p, n2)) or r?(v,r(p, n2)).
</nextsent>
<nextsent>the algorithms used in the related work reflect the fact that they condition probabilities on particular verb and noun.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y97">
<title id=" W01-0511.xml">classifying the semantic relations in noun compounds via a domain specific lexical hierarchy </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>how ever, our approach will not solve issues surrounding previously unseen proper nouns, which are often important for information extraction tasks.there have been several efforts to incorporate lexical hierarchies into statistical processing, primarily for the problem of prepositional phrase (pp) attachment.
</prevsent>
<prevsent>the current standard formulation is: given verb followed by noun and prepositional phrase, represented by the tuple v, n1, p, n2, determine which of or n1 the pp consisting of and n2 attaches to, or is most closely associated with.
</prevsent>
</prevsection>
<citsent citstr=" J98-2002 ">
because the data is sparse, empirical methods that train on word occurrences alone (hindle and rooth,1993) <papid> J93-1005 </papid>have been supplanted by algorithms that generalize one or both of the nouns according to class membership measures (resnik, 1993; resnik and hearst, 1993; <papid> W93-0307 </papid>brill and resnik, 1994; <papid> C94-2195 </papid>li and abe,1998), <papid> J98-2002 </papid>but the statistics are computed for the particular preposition and verb.it is not clear how to use the results of such analysis after they are found; the semantics of the rela 1nominalizations are compounds whose head noun is nominal ized verb and whose modifier is either the subject or the object of the verb.</citsent>
<aftsection>
<nextsent>we do not distinguish the ncs on the basis of their formation.
</nextsent>
<nextsent>tion ship between the terms must still be determined.
</nextsent>
<nextsent>in our framework we would cast this problem as finding the relationship r(p, n2) that best characterizes the preposition and the np that follows it, andthen seeing if the categorization algorithm determines their exists any relationship r?(n1, r(p, n2)) or r?(v,r(p, n2)).
</nextsent>
<nextsent>the algorithms used in the related work reflect the fact that they condition probabilities on particular verb and noun.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y98">
<title id=" W01-0511.xml">classifying the semantic relations in noun compounds via a domain specific lexical hierarchy </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in our framework we would cast this problem as finding the relationship r(p, n2) that best characterizes the preposition and the np that follows it, andthen seeing if the categorization algorithm determines their exists any relationship r?(n1, r(p, n2)) or r?(v,r(p, n2)).
</prevsent>
<prevsent>the algorithms used in the related work reflect the fact that they condition probabilities on particular verb and noun.
</prevsent>
</prevsection>
<citsent citstr=" W95-0105 ">
resnik (1993), resnik (1995) <papid> W95-0105 </papid>use classes in wordnet (fellbaum, 1998) and measure of conceptual association to generalize over the nouns.</citsent>
<aftsection>
<nextsent>brill and resnik (1994) <papid> C94-2195 </papid>use brills transformation-basedalgorithm along with simple counts within lexical hierarchy in order to generalize over individualwords.</nextsent>
<nextsent>li and abe (1998) <papid> J98-2002 </papid>use minimum description length-based algorithm to find an optimal tree cut over wordnet for each classification problem, finding improvements over both lexical association(hindle and rooth, 1993) <papid> J93-1005 </papid>and conceptual association, and equaling the transformation-based results.our approach differs from these in that we are using machine learning techniques to determine which level of the lexical hierarchy is appropriate for generalizing across nouns.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y103">
<title id=" W01-1602.xml">variant transduction a method for rapid development of interactive spoken interfaces </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a major part of this eort is aimed at coping with variation in the spoken language inputby users.
</prevsent>
<prevsent>one approach to handling variation is to write large natural language grammar manually and hope that its coverage is sucient for multiple applications (dowdinget al, 1994).
</prevsent>
</prevsection>
<citsent citstr=" M98-1009 ">
another approach is to create simulation of the intended system (typ ically with human in the loop) and then record users interacting with the simulation.the recordings are then transcribed and annotated with semantic information relating tothe domain; the transcriptions and annotations can then be used to create statistical understanding model (miller et al, 1998) <papid> M98-1009 </papid>orused as guidance for manual grammar development (aust et al, 1995).</citsent>
<aftsection>
<nextsent>building mixed initiative spoken language systems currently usually involves the design of semantic representations speci to the application domain.
</nextsent>
<nextsent>these representations areused to pass data between the language processing components: understanding, dialog,con rmation generation, and response generation.
</nextsent>
<nextsent>however, such representations tend to be domain-speci c, and this makes it dicult to port to new domains or to use machine learning techniques without extensive hand labeling of data with the semantic representations.
</nextsent>
<nextsent>furthermore, the use of intermediate semantic representations still requires nal transduction step from the intermediate representation to the action format expected by the application back-end (e.g. sql database query or procedure call).for situations when the eort and expertise available to build an application is small,the methods mentioned above are impractical, and highly directed dialog systems with little allowance for language variability are constructed.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y104">
<title id=" W01-1602.xml">variant transduction a method for rapid development of interactive spoken interfaces </title>
<section> input interpretation.  </section>
<citcontext>
<prevsection>
<prevsent>in our experiments, we have used dierent classi ers, including boostexter (schapire and singer, 2000), and classier based onphi-correlation statistics for the text features (see alshawi and douglas (2000)for our earlier application of phi statistics in learning machine translation models from examples).
</prevsent>
<prevsent>other classi ers such as decision trees (quinlan, 1993) or support vector machines (vapnik, 1995) could be used instead.matcher the matcher can compute distortion mapping and associated distance between the output of the speech recognizer and variant v. various match ers can be used such as those suggested in example-based approaches to machine translation (sumita and iida, 1995).
</prevsent>
</prevsection>
<citsent citstr=" P93-1024 ">
so far we have used weighted string edit distance matcher and experimented with dierent substitution weights including ones based on measures of statistical similarity between words such as the one described by pereira et al (1993).<papid> P93-1024 </papid></citsent>
<aftsection>
<nextsent>the output of the matcher is real number (the distance) and distortion mapping represented as sequence of edit operations (wagner and fischer, 1974).
</nextsent>
<nextsent>using these two components, the method for mapping the user utterance to an executable action is as follows: 1.
</nextsent>
<nextsent>the language model derived from con-.
</nextsent>
<nextsent>text is activated in the speech recognizer.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y105">
<title id=" W01-1006.xml">semiautomatic practical ontology construction by using a thesaurus computational dictionaries and large corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>afterwards, we construct practical ontology by the semi-automatic construction method given below.
</prevsent>
<prevsent>we extend the existing kadokawa thesaurus (ohno &amp; hama nishi, 1981) by inserting additional semantic relations into the hierarchy of the thesaurus.
</prevsent>
</prevsection>
<citsent citstr=" C96-2161 ">
uramoto (1996) <papid> C96-2161 </papid>and tokunaga (1997) propose thesaurus extension methods for positioning unknown words in an existing thesaurus.</citsent>
<aftsection>
<nextsent>our approach differs in that the objects inserted are not words but semantic relations.
</nextsent>
<nextsent>additional semantic relations can be classified as case relations and other semantic relations.
</nextsent>
<nextsent>the former can be obtained by converting the established valency information in bilingual dictionaries of cobalt-j/k (collocation-based language translator from japanese to korean) and cobalt-k/j (collocation-based language translator from korean to japanese) (moon &amp; lee, 2000) <papid> C00-1079 </papid>mt systems, as well as from the case frame in the sejong electronic dictionary1.</nextsent>
<nextsent>the latter can be acquired from concept co-occurrence information, which is extracted automatically from corpus (li et al, 2000).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y106">
<title id=" W01-1006.xml">semiautomatic practical ontology construction by using a thesaurus computational dictionaries and large corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our approach differs in that the objects inserted are not words but semantic relations.
</prevsent>
<prevsent>additional semantic relations can be classified as case relations and other semantic relations.
</prevsent>
</prevsection>
<citsent citstr=" C00-1079 ">
the former can be obtained by converting the established valency information in bilingual dictionaries of cobalt-j/k (collocation-based language translator from japanese to korean) and cobalt-k/j (collocation-based language translator from korean to japanese) (moon &amp; lee, 2000) <papid> C00-1079 </papid>mt systems, as well as from the case frame in the sejong electronic dictionary1.</citsent>
<aftsection>
<nextsent>the latter can be acquired from concept co-occurrence information, which is extracted automatically from corpus (li et al, 2000).
</nextsent>
<nextsent>the remainder of this paper is organized as follows.
</nextsent>
<nextsent>we describe the principles of ontology design and an orl used to represent our lip ontology in the next section.
</nextsent>
<nextsent>in section 3, we describe the semi-automatic ontology construction methodology in detail.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y107">
<title id=" W01-1006.xml">semiautomatic practical ontology construction by using a thesaurus computational dictionaries and large corpora </title>
<section> ontology application.  </section>
<citcontext>
<prevsection>
<prevsent>answer yes no set the answer to the most frequently appearing sense figure 6.
</prevsent>
<prevsent>the proposed wsd algorithm table 2.
</prevsent>
</prevsection>
<citsent citstr=" P89-1010 ">
imported relation instances types number taxonomic relations 1,100 case relations 19,459 other semantic relations 1,650 total 22,209 between random variables (church &amp; hanks, 1989).<papid> P89-1010 </papid></citsent>
<aftsection>
<nextsent>resnik (1995) suggested measure of semantic similarity in an is-a taxonomy, based on the notion of information content.
</nextsent>
<nextsent>however, his method differs from ours in that we consider all semantic relations in the ontology, not taxonomy relations only.
</nextsent>
<nextsent>to implement this idea, we bind source concepts (sc) and semantic relations (sr) into one entity, since sr is mainly influenced by sc, not the destination concepts (dc).
</nextsent>
<nextsent>therefore, if two entities,   sc, sr , and dc have probabilities p( sc, sr ) and p(dc), then their mutual information i( sc, sr , dc) is defined as: ???
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y108">
<title id=" W01-0813.xml">applying natural language generation to indicative summarization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this yields two rules-of-thumb for guiding content calculation: 1) reporting differences from the normand 2) reporting information rel event to the query.
</prevsent>
<prevsent>we have implemented these rules as part of the content planning module of our centrifusersummarization system.
</prevsent>
</prevsection>
<citsent citstr=" W94-0319 ">
the summarizers architecture follows the consensus nlg architecture(reiter, 1994), <papid> W94-0319 </papid>including the stages of content calculation and content planning.</citsent>
<aftsection>
<nextsent>we follow the generation of sample indicative multi document query-based summary, shown in the bottom half of figure 1, focusing on these two stages in the remainder of the paper.
</nextsent>
<nextsent>summary content information about topics and structure of the document may be based on higher-level document features.
</nextsent>
<nextsent>such information typically does not occur as strings in the document text.
</nextsent>
<nextsent>our approach, therefore, is to identify and extract the document features that are relevant for indicative summaries.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y109">
<title id=" W01-0813.xml">applying natural language generation to indicative summarization </title>
<section> computing potential content:.  </section>
<citcontext>
<prevsection>
<prevsent>care article.
</prevsent>
<prevsent>each document in the collection is represented by such tree, which breaks each documents topic into subtopics.we build these document topic trees automatically for structured documents using simple approach that utilizes section headers, which suffices for our current domain and genre.
</prevsent>
</prevsection>
<citsent citstr=" W98-1123 ">
other methods such as layout identification (hu et al, 1999) and text segmentation / rhetorical parsing (yaari, 1999; kan et al, 1998; <papid> W98-1123 </papid>marcu, 1997) <papid> P97-1013 </papid>can serve as the basis for constructing such trees inboth structured and unstructured documents, re spectively.</citsent>
<aftsection>
<nextsent>4.1 normative topicality as composite topic.
</nextsent>
<nextsent>treesas stated in rule 1, the summarizer needs normative values calculated for each document feature to properly compute differences between documents.
</nextsent>
<nextsent>the composite topic tree embodies this paradigm.
</nextsent>
<nextsent>it is data structure that compiles knowledge about all possible topics and their structure in articles of the same intersection of domain and genre, (i.e., rule 1s notion of document type?).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y110">
<title id=" W01-0813.xml">applying natural language generation to indicative summarization </title>
<section> computing potential content:.  </section>
<citcontext>
<prevsection>
<prevsent>care article.
</prevsent>
<prevsent>each document in the collection is represented by such tree, which breaks each documents topic into subtopics.we build these document topic trees automatically for structured documents using simple approach that utilizes section headers, which suffices for our current domain and genre.
</prevsent>
</prevsection>
<citsent citstr=" P97-1013 ">
other methods such as layout identification (hu et al, 1999) and text segmentation / rhetorical parsing (yaari, 1999; kan et al, 1998; <papid> W98-1123 </papid>marcu, 1997) <papid> P97-1013 </papid>can serve as the basis for constructing such trees inboth structured and unstructured documents, re spectively.</citsent>
<aftsection>
<nextsent>4.1 normative topicality as composite topic.
</nextsent>
<nextsent>treesas stated in rule 1, the summarizer needs normative values calculated for each document feature to properly compute differences between documents.
</nextsent>
<nextsent>the composite topic tree embodies this paradigm.
</nextsent>
<nextsent>it is data structure that compiles knowledge about all possible topics and their structure in articles of the same intersection of domain and genre, (i.e., rule 1s notion of document type?).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y111">
<title id=" W01-0813.xml">applying natural language generation to indicative summarization </title>
<section> current status and future work.  </section>
<citcontext>
<prevsection>
<prevsent>centrifuser is fully implemented; it produces the sample summary in figure 1.
</prevsent>
<prevsent>we have concentrated on implementing the most commonly occuring document feature, topicality, and have additionally incorporated three other document features into our framework (document-derived content types and special content and the title metadata).future work will include extending our document feature analysis to model context (to model adding features only when appropriate), as well as incorporating additional document features.
</prevsent>
</prevsection>
<citsent citstr=" A00-2023 ">
we are also exploring the use of stochastic corpus modeling (langkilde, 2000; <papid> A00-2023 </papid>bangalore and ram bow, 2000) <papid> C00-1007 </papid>to replace our template-based realizerwith probabilistic one that can produce felicitous sentence patterns based on contextual analy sis.</citsent>
<aftsection>
<nextsent>we have presented model for indicative multi document summarization based on natural language generation.
</nextsent>
<nextsent>in our model, summary content is based on document features describing topic and structure instead of extracted text.
</nextsent>
<nextsent>given these features, generation model uses text plan, derived from analysis of naturally occurring indicative summaries plus guidelines for summarization, to guide the system in describing document topics as typical, rare, intricate, or relevant to the user query.
</nextsent>
<nextsent>we showed how the topicality document feature can be derived from the set of input documents and represented as topic tree for each document along with merged composite topic for all documents in the collection against which prototypicality and query relevance can be computed.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y112">
<title id=" W01-0813.xml">applying natural language generation to indicative summarization </title>
<section> current status and future work.  </section>
<citcontext>
<prevsection>
<prevsent>centrifuser is fully implemented; it produces the sample summary in figure 1.
</prevsent>
<prevsent>we have concentrated on implementing the most commonly occuring document feature, topicality, and have additionally incorporated three other document features into our framework (document-derived content types and special content and the title metadata).future work will include extending our document feature analysis to model context (to model adding features only when appropriate), as well as incorporating additional document features.
</prevsent>
</prevsection>
<citsent citstr=" C00-1007 ">
we are also exploring the use of stochastic corpus modeling (langkilde, 2000; <papid> A00-2023 </papid>bangalore and ram bow, 2000) <papid> C00-1007 </papid>to replace our template-based realizerwith probabilistic one that can produce felicitous sentence patterns based on contextual analy sis.</citsent>
<aftsection>
<nextsent>we have presented model for indicative multi document summarization based on natural language generation.
</nextsent>
<nextsent>in our model, summary content is based on document features describing topic and structure instead of extracted text.
</nextsent>
<nextsent>given these features, generation model uses text plan, derived from analysis of naturally occurring indicative summaries plus guidelines for summarization, to guide the system in describing document topics as typical, rare, intricate, or relevant to the user query.
</nextsent>
<nextsent>we showed how the topicality document feature can be derived from the set of input documents and represented as topic tree for each document along with merged composite topic for all documents in the collection against which prototypicality and query relevance can be computed.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y113">
<title id=" W01-0716.xml">learning to identify animate references </title>
<section> evaluation and discussion.  </section>
<citcontext>
<prevsection>
<prevsent>an alternative solution is to weight the senses with respect to the text.
</prevsent>
<prevsent>in this way, if sense is more likely to be used in text, its animacy/inanimacy will have greater influence on the classification process.
</prevsent>
</prevsection>
<citsent citstr=" W95-0105 ">
at present, we are trying to integrate the word sense disambiguation method proposed in (resnik, 1995) <papid> W95-0105 </papid>into our system.</citsent>
<aftsection>
<nextsent>we hope that this will particularly improve the classification of animate entities.
</nextsent>
<nextsent>most of the work on animacy/gender recognition has been done in the field of anaphora resolution.
</nextsent>
<nextsent>the automatic recognition of np gender on the basis of statistical information has been attempted before (hale and charniak, 1998).
</nextsent>
<nextsent>that method operates by counting the frequency with which np is identified as the antecedent of gender-marked pronoun by simplistic pronoun resolution system.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y114">
<title id=" W01-0716.xml">learning to identify animate references </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in (denber, 1998), wordnet was used to determine the animacy of nouns and associate them with gender-marked pronouns.
</prevsent>
<prevsent>the details presented are sparse and no evaluation is given.
</prevsent>
</prevsection>
<citsent citstr=" W99-0611 ">
cardie and wagstaff (1999) <papid> W99-0611 </papid>combined the use of wordnet with proper name gazette ers in order to obtain information on the compatibility of coreferential nps in their clustering algorithm.</citsent>
<aftsection>
<nextsent>again, no evaluation was presented with respect to the accuracy of this animacy classification task.
</nextsent>
<nextsent>in this paper, two step method for animacy recognition was proposed.
</nextsent>
<nextsent>in the first step, it tries to determine the animacy of senses from wordnet on the basis of an annotated corpus.
</nextsent>
<nextsent>in the second step, this information is used by an instance based learning algorithm to determine the animacy of noun.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y115">
<title id=" W02-0604.xml">unsupervised learning of morphology without morphemes </title>
<section> morphological learning.  </section>
<citcontext>
<prevsection>
<prevsent>brent (1993), for example, aims at finding the right set of suffixes from corpus, but the algorithm cannot double as morphological parser.
</prevsent>
<prevsent>more recently, efforts have been developing which identify morphemes and perform some sort of analysis.
</prevsent>
</prevsection>
<citsent citstr=" N01-1024 ">
schone and jurafsky (2001) <papid> N01-1024 </papid>employ great many sophisticated post-hoc adjustments to obtain the right conflation sets for words by pure corpus analysis without annotations.</citsent>
<aftsection>
<nextsent>their procedure uses morpheme-based model, provides an analysis ofthe words, and does in sense discover morphological relations.
</nextsent>
<nextsent>goldsmith (2001<papid> J01-2001 </papid>b), goldsmith (2001<papid> J01-2001 </papid>a), inspired by de marc kens (1995) thesis on minimum description length, attempts to provide both list of morpheme sand an analysis of each word in corpus.</nextsent>
<nextsent>also, ba roni (2000) aims at finding set of prefixes from corpus, together with an affix-stem parse of each of the words.while they might differ in their methods or objectives, all of the above morphological applications share common characteristic in that they are learners designed exclusively for the acquisition of morphological facts from corpora and do not generate new words based on the information they acquire.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y116">
<title id=" W02-0604.xml">unsupervised learning of morphology without morphemes </title>
<section> morphological learning.  </section>
<citcontext>
<prevsection>
<prevsent>schone and jurafsky (2001) <papid> N01-1024 </papid>employ great many sophisticated post-hoc adjustments to obtain the right conflation sets for words by pure corpus analysis without annotations.</prevsent>
<prevsent>their procedure uses morpheme-based model, provides an analysis ofthe words, and does in sense discover morphological relations.</prevsent>
</prevsection>
<citsent citstr=" J01-2001 ">
goldsmith (2001<papid> J01-2001 </papid>b), goldsmith (2001<papid> J01-2001 </papid>a), inspired by de marc kens (1995) thesis on minimum description length, attempts to provide both list of morpheme sand an analysis of each word in corpus.</citsent>
<aftsection>
<nextsent>also, ba roni (2000) aims at finding set of prefixes from corpus, together with an affix-stem parse of each of the words.while they might differ in their methods or objectives, all of the above morphological applications share common characteristic in that they are learners designed exclusively for the acquisition of morphological facts from corpora and do not generate new words based on the information they acquire.
</nextsent>
<nextsent>1.2 parsing and generation.
</nextsent>
<nextsent>only handful of programs can both parse and generate words.
</nextsent>
<nextsent>once again, these programs fall intotwo very distinct categories.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y120">
<title id=" W02-0604.xml">unsupervised learning of morphology without morphemes </title>
<section> morphological learning.  </section>
<citcontext>
<prevsection>
<prevsent>once again, these programs fall intotwo very distinct categories.
</prevsent>
<prevsent>in view of the disparity between these programs, it is useful to distinguish between genuine morphological learners ableto generate from acquired knowledge and genera tors/parsers that implement man-made analysis.
</prevsent>
</prevsection>
<citsent citstr=" C92-1025 ">
the latter group is perhaps the most well known, so let us begin with them.kimmo-type applications of two-level morphology (koskenniemi, 1983; ant worth, 1990; karttunen et al, 1992; <papid> C92-1025 </papid>karttunen, 1993; karttunen, 1994) <papid> C94-1066 </papid>can provide morphological analysis of the words in corpus and generate new words based ona set of rules; but these programs must first be provided with that set of rules and lexicon containing morphemes by the user.</citsent>
<aftsection>
<nextsent>similar work in one and two-level morphology has been done using the attribute-logic engine (carpenter, 1992).
</nextsent>
<nextsent>some of these systems (e.g.
</nextsent>
<nextsent>(karttunen et al, 1987)) have front-end that compiles more traditional linearly ordered morphological rules into the finite-state automata of two-level morphology.
</nextsent>
<nextsent>once again, these applications require set of man-made lexical rulesto function.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y121">
<title id=" W02-0604.xml">unsupervised learning of morphology without morphemes </title>
<section> morphological learning.  </section>
<citcontext>
<prevsection>
<prevsent>once again, these programs fall intotwo very distinct categories.
</prevsent>
<prevsent>in view of the disparity between these programs, it is useful to distinguish between genuine morphological learners ableto generate from acquired knowledge and genera tors/parsers that implement man-made analysis.
</prevsent>
</prevsection>
<citsent citstr=" C94-1066 ">
the latter group is perhaps the most well known, so let us begin with them.kimmo-type applications of two-level morphology (koskenniemi, 1983; ant worth, 1990; karttunen et al, 1992; <papid> C92-1025 </papid>karttunen, 1993; karttunen, 1994) <papid> C94-1066 </papid>can provide morphological analysis of the words in corpus and generate new words based ona set of rules; but these programs must first be provided with that set of rules and lexicon containing morphemes by the user.</citsent>
<aftsection>
<nextsent>similar work in one and two-level morphology has been done using the attribute-logic engine (carpenter, 1992).
</nextsent>
<nextsent>some of these systems (e.g.
</nextsent>
<nextsent>(karttunen et al, 1987)) have front-end that compiles more traditional linearly ordered morphological rules into the finite-state automata of two-level morphology.
</nextsent>
<nextsent>once again, these applications require set of man-made lexical rulesto function.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y132">
<title id=" W00-1308.xml">enriching the knowledge sources used in a maximum entropy partofspeech tagger </title>
<section> the baseline maximum entropy model.  </section>
<citcontext>
<prevsection>


</prevsection>
<citsent citstr=" W96-0213 ">
we started with maximum entropy based tagger that uses features very similar to the ones proposed in ratnaparkhi (1996).<papid> W96-0213 </papid></citsent>
<aftsection>
<nextsent>the tagger learns loglinear conditional probability model from tagged text, using maximum entropy method.
</nextsent>
<nextsent>the model assigns probability for every tag in the set of possible tags given word and its context h, which is usually def med as the sequence of several words and tags preceding the word.
</nextsent>
<nextsent>this model can be used for estimating the probability of tag sequence h...tn given sentence w~.
</nextsent>
<nextsent>.wn: n p(t,...t i wl..
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y135">
<title id=" W00-1308.xml">enriching the knowledge sources used in a maximum entropy partofspeech tagger </title>
<section> the baseline maximum entropy model.  </section>
<citcontext>
<prevsection>
<prevsent>the constraints in our model are that the expectations of these features according to the joint distribution are equal to the expectations of the features in the empirical (training data) distribution ~ : ep~h.,)fi (h, t) = e~h,,) ~ (h, t).
</prevsent>
<prevsent>having defined set of constraints that our model should accord with, we proceed to find the model satisfying the constraints that maxi-mizes the conditional entropy of . the intu-ition is that such model assumes nothing apart from that it should satisfy the given constraints.
</prevsent>
</prevsection>
<citsent citstr=" J96-1002 ">
following berger et al (1996), <papid> J96-1002 </papid>we approxi-mate p(h,t), the joint distribution of contexts and tags, by the product of ~(h) , the empirical distribution of histories h, and the conditional distribution p(t h): p(h,t) = ~(h).</citsent>
<aftsection>
<nextsent>p(t lh).
</nextsent>
<nextsent>then for the example above, our constraints would be the following, for e {1,2}: ~(h, t)f (h, t) = ~ ,~(h)p(t \[h)f (h, t) heh.tet hsh, t~t this approximation is used to enable efficient computation.
</nextsent>
<nextsent>the expectation for fea-ture is: f= ~(h)p( lh ) (h , ) h~ ,te where is the space of possible contexts when predicting part of speech tag t. since the contexts contain sequences of words and tags and other information, the space is huge.
</nextsent>
<nextsent>but using this approximation, we can instead sum just over the smaller space of observed contexts in the training sample, because the empirical prior ~(h) is zero for unseen contexts h: f = 2~(h)p( lh ) (h , ) (1) h~ x, t~t the model that is solution to this constrained optimization task is an exponential (or equivalently, loglinear) model with the para-metric form: p(t\[h) = j=l...k ea// ) t~t j=i...,k where the denominator is normalizing term (sometimes referred to as the partition function).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y151">
<title id=" W00-1308.xml">enriching the knowledge sources used in a maximum entropy partofspeech tagger </title>
<section> features for particle disambiguation.  </section>
<citcontext>
<prevsection>
<prevsent>overall accuracy unknown word accuracy \[ 96.86% 86.91% table 11 accuracy of the final model for ease of comparison, the accuracies of all models on the test and development sets are shown in table 7.
</prevsent>
<prevsent>we note that accuracy is lower on the development set.
</prevsent>
</prevsection>
<citsent citstr=" A00-2018 ">
this presumably corre-sponds with charniak (2000: <papid> A00-2018 </papid>136) observation that section 23 of the penn treebank is easier than some others.</citsent>
<aftsection>
<nextsent>table 8 shows the different number of feature templates of each kind that have been instantiated for the different models as well as the total number of features each model has.
</nextsent>
<nextsent>it can be seen that the features which help disambiguate verb forms, which look at capital-ization and the first of the feature templates for particles are very small number as compared to the features of the other kinds.
</nextsent>
<nextsent>the improvement in classification accuracy therefore comes at the price of adding very few parameters to the maximum entropy model and does not result in increased model complexity.
</nextsent>
<nextsent>conclusion even when the accuracy figures for corpus-based part-of-speech taggers start to look extremely similar, it is still possible to move performance levels up.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y152">
<title id=" W01-1401.xml">example based machine translation using dpmatching between work sequences </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>there are two approaches in corpus-based translation: 1.
</prevsent>
<prevsent>statistical machine translation (smt): smt.
</prevsent>
</prevsection>
<citsent citstr=" J90-2002 ">
learns models for translation from corpora and dictionaries and searches for the best translation according to the models in run-time (brown et al ., 1990; <papid> J90-2002 </papid>knight, 1997; ney et al , 2000).</citsent>
<aftsection>
<nextsent>ebmt uses the corpus directly.
</nextsent>
<nextsent>ebmt retrieves the translation examples that are best matched to an input expression and adjusts the examples to obtain the translation (nagao, 1981; sadler 1989; sato and nagao, 1990; <papid> C90-3044 </papid>sumita and iida, 1991; <papid> P91-1024 </papid>kitano, 1993; furuse et al , 1994; watanabe and maruyama, 1994; cranias et al , 1994; jones, 1996; veale and way, 1997; carl, 1999, andriamanankasina et al , 1999; brown, 2000).<papid> C00-1019 </papid></nextsent>
<nextsent>figure 1 configuration (1) sentence aligned bilingual corpus (2) bilingual dictionary (3) thesauri retrieval + adjustment input sentence target sentence this paper pursues ebmt and proposes new approach by using the distance between word sequences.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y153">
<title id=" W01-1401.xml">example based machine translation using dpmatching between work sequences </title>
<section> example-based machine translation (ebmt):.  </section>
<citcontext>
<prevsection>
<prevsent>learns models for translation from corpora and dictionaries and searches for the best translation according to the models in run-time (brown et al ., 1990; <papid> J90-2002 </papid>knight, 1997; ney et al , 2000).</prevsent>
<prevsent>ebmt uses the corpus directly.</prevsent>
</prevsection>
<citsent citstr=" C90-3044 ">
ebmt retrieves the translation examples that are best matched to an input expression and adjusts the examples to obtain the translation (nagao, 1981; sadler 1989; sato and nagao, 1990; <papid> C90-3044 </papid>sumita and iida, 1991; <papid> P91-1024 </papid>kitano, 1993; furuse et al , 1994; watanabe and maruyama, 1994; cranias et al , 1994; jones, 1996; veale and way, 1997; carl, 1999, andriamanankasina et al , 1999; brown, 2000).<papid> C00-1019 </papid></citsent>
<aftsection>
<nextsent>figure 1 configuration (1) sentence aligned bilingual corpus (2) bilingual dictionary (3) thesauri retrieval + adjustment input sentence target sentence this paper pursues ebmt and proposes new approach by using the distance between word sequences.
</nextsent>
<nextsent>the following sections show the algorithm, experimental results, and implications and prospects.
</nextsent>
<nextsent>2 the proposed method.
</nextsent>
<nextsent>2.1 configuration.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y155">
<title id=" W01-1401.xml">example based machine translation using dpmatching between work sequences </title>
<section> example-based machine translation (ebmt):.  </section>
<citcontext>
<prevsection>
<prevsent>learns models for translation from corpora and dictionaries and searches for the best translation according to the models in run-time (brown et al ., 1990; <papid> J90-2002 </papid>knight, 1997; ney et al , 2000).</prevsent>
<prevsent>ebmt uses the corpus directly.</prevsent>
</prevsection>
<citsent citstr=" P91-1024 ">
ebmt retrieves the translation examples that are best matched to an input expression and adjusts the examples to obtain the translation (nagao, 1981; sadler 1989; sato and nagao, 1990; <papid> C90-3044 </papid>sumita and iida, 1991; <papid> P91-1024 </papid>kitano, 1993; furuse et al , 1994; watanabe and maruyama, 1994; cranias et al , 1994; jones, 1996; veale and way, 1997; carl, 1999, andriamanankasina et al , 1999; brown, 2000).<papid> C00-1019 </papid></citsent>
<aftsection>
<nextsent>figure 1 configuration (1) sentence aligned bilingual corpus (2) bilingual dictionary (3) thesauri retrieval + adjustment input sentence target sentence this paper pursues ebmt and proposes new approach by using the distance between word sequences.
</nextsent>
<nextsent>the following sections show the algorithm, experimental results, and implications and prospects.
</nextsent>
<nextsent>2 the proposed method.
</nextsent>
<nextsent>2.1 configuration.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y156">
<title id=" W01-1401.xml">example based machine translation using dpmatching between work sequences </title>
<section> example-based machine translation (ebmt):.  </section>
<citcontext>
<prevsection>
<prevsent>learns models for translation from corpora and dictionaries and searches for the best translation according to the models in run-time (brown et al ., 1990; <papid> J90-2002 </papid>knight, 1997; ney et al , 2000).</prevsent>
<prevsent>ebmt uses the corpus directly.</prevsent>
</prevsection>
<citsent citstr=" C00-1019 ">
ebmt retrieves the translation examples that are best matched to an input expression and adjusts the examples to obtain the translation (nagao, 1981; sadler 1989; sato and nagao, 1990; <papid> C90-3044 </papid>sumita and iida, 1991; <papid> P91-1024 </papid>kitano, 1993; furuse et al , 1994; watanabe and maruyama, 1994; cranias et al , 1994; jones, 1996; veale and way, 1997; carl, 1999, andriamanankasina et al , 1999; brown, 2000).<papid> C00-1019 </papid></citsent>
<aftsection>
<nextsent>figure 1 configuration (1) sentence aligned bilingual corpus (2) bilingual dictionary (3) thesauri retrieval + adjustment input sentence target sentence this paper pursues ebmt and proposes new approach by using the distance between word sequences.
</nextsent>
<nextsent>the following sections show the algorithm, experimental results, and implications and prospects.
</nextsent>
<nextsent>2 the proposed method.
</nextsent>
<nextsent>2.1 configuration.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y161">
<title id=" W00-0901.xml">comparing corpora using frequency profiling </title>
<section> introduction.  </section>
<citcontext>
<prevsection>
<prevsent>the second type of comparison is one that views corpora as equals (as in the brown and lob comparison).
</prevsent>
<prevsent>it aims to discover features in the corpora that distinguish one tiom another.
</prevsent>
</prevsection>
<citsent citstr=" W97-0122 ">
homogeneity within each of the corpora is important here since we may find that the results reflect sections within one of the corpora which are unlike other sections in either of the corpora under consideration (kilgarriff 1997).<papid> W97-0122 </papid></citsent>
<aftsection>
<nextsent>comparability is of interest too, since the corpora should have been sampled for in the same way.
</nextsent>
<nextsent>in other words, the corpora should have been built using the same stratified sampling method and with, if possible, randornised methods of sample selection.
</nextsent>
<nextsent>this is the case with brown and lob, since lob was designed to be comparable tothe brown corpus.
</nextsent>
<nextsent>the final issue, which has been addressed elsewhere, is the one regarding the reliability of the statistical tests in relation to the size of the corpora under consideration.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y162">
<title id=" W00-0901.xml">comparing corpora using frequency profiling </title>
<section> introduction.  </section>
<citcontext>
<prevsection>
<prevsent>ignoring the actual frequency of occurrence as in the mann- whitney test discards most of the evidence we have about he distribution of words.
</prevsent>
<prevsent>the test is often used when comparing ordinal rating scales (oakes 1998: 17).
</prevsent>
</prevsection>
<citsent citstr=" J93-1003 ">
dunning (1993) <papid> J93-1003 </papid>reports that we should not relyon the assumption of normal distribution when performing statistical text analysis and suggests that parametric analysis based on the binomial or multinomial distributions a better alternative for smaller texts.</citsent>
<aftsection>
<nextsent>the chi-squared value becomes unreliable when the expected frequency is less than 5 and possibly overestimates with high frequency words and when comparing relatively small corpus to much larger one.
</nextsent>
<nextsent>he proposes the log-likelihood ratio as an alternative to pearson~ chi-squared test.
</nextsent>
<nextsent>for this reason, we chose to use the log-likelihood ratio in our work as described in the next section.
</nextsent>
<nextsent>in fact, cressie and read (1984) show that pearson~ 2 (chi-squared) and the likelihood ratio 2 (dunning~ log-likelihood) are two statistics in continuum defined by the power- divergence family of statistics.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y163">
<title id=" W02-0405.xml">using summaries in document retrieval </title>
<section> defining summary for news.  </section>
<citcontext>
<prevsection>
<prevsent>in this experiment, the results of each query were evaluated using two different user perspectives, highly relevant references only and all references.
</prevsent>
<prevsent>through this approach we were able to determine whether search able lead satisfied the goal that motivated its creation.
</prevsent>
</prevsection>
<citsent citstr=" P98-2222 ">
articles for this investigation, the leading text of news documents is used as basis for creating document summaries ? specifically the definition of search able lead found in wasson (1998).<papid> P98-2222 </papid></citsent>
<aftsection>
<nextsent>brandow et al (1995) compared summaries they created using tf-idf-based sentence extraction to fixed amounts of leading text ? philadelphia, july 2002, pp.
</nextsent>
<nextsent>37-44.
</nextsent>
<nextsent>association for computational linguistics.
</nextsent>
<nextsent>proceedings of the workshop on automatic summarization (including duc 2002), approximately 60, 150 and 250 words long, in three separate trials ? generated using slightly modified version of our production search able lead text processing software.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y165">
<title id=" W02-0405.xml">using summaries in document retrieval </title>
<section> the experiment.  </section>
<citcontext>
<prevsection>
<prevsent>after all, articles written before the game took place obviously could not include game statistics.
</prevsent>
<prevsent>more than half the topics focused on named entities.
</prevsent>
</prevsection>
<citsent citstr=" W97-0315 ">
this is consistent with our observations of customer search topics applied to news data, and this user behavior has also been reported elsewhere (e.g., thompson &amp; dozier, 1997).<papid> W97-0315 </papid></citsent>
<aftsection>
<nextsent>one effect of this was that the recall and precision rates we would observe in this experiment were higher than what is commonly reported for boolean search results.
</nextsent>
<nextsent>because many proper names are relatively unambiguous, and because articles about some named entity almost always mention the name, some of the queries had much higher accuracy rates than might otherwise be expected, and that pulled overall average accuracy rates up somewhat.
</nextsent>
<nextsent>the boolean search exxon, for example, virtually assures us of 100% recall regardless of which evaluation scope is used.
</nextsent>
<nextsent>although individual exxon service stations are mentioned periodically in the news, most news articles that mention exxon are in fact about the major oil company, ensuring fairly high precision for the all references evaluation scope.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y166">
<title id=" W02-0405.xml">using summaries in document retrieval </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>f-measure .479 .392 table 4.
</prevsent>
<prevsent>averages for thirty queries combining both the highly relevant reference and all reference evaluation scopes.
</prevsent>
</prevsection>
<citsent citstr=" A00-1038 ">
as for other retrieval tasks, when creating document categorization system, we did gain some benefits when weighting terms found in headlines and leading text in news documents bit higher (wasson, 2000), <papid> A00-1038 </papid>but that effect is limited to news data.</citsent>
<aftsection>
<nextsent>a colleague investigating an internal tf-idf-based search engine found no benefits to putting extra emphasis on terms found in the first paragraph of news articles, but that was rather limited test.
</nextsent>
<nextsent>neither of these were evaluated from multiple user perspectives, although in the case of wasson (2000) <papid> A00-1038 </papid>the original project goal was to identify and categorize only highly relevant documents</nextsent>
<nextsent>customers of online services approach their information seeking tasks from many perspectives, and yet most ir evaluations are conducted from single user perspective.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y168">
<title id=" W00-0502.xml">task tolerance of mt output in integrated text processes </title>
<section> results.  </section>
<citcontext>
<prevsection>
<prevsent>the user responses and results for the extraction exercise are shown in exhibit 4.
</prevsent>
<prevsent>in the extraction exercise, each user was asked to identify named entities in each translation: persons, locations, organizations, dates, times, and money/percent.
</prevsent>
</prevsection>
<citsent citstr=" M95-1003 ">
this extraction exercise was modeled after the  named entity  task of the message understanding conference (muc) (chinchor and dungca, 1995).<papid> M95-1003 </papid></citsent>
<aftsection>
<nextsent>exhibit 4 contains two charts.
</nextsent>
<nextsent>the top row of both charts contain list of users who participated in the extraction exercise.
</nextsent>
<nextsent>the first column of both charts lists seven documents seen by the users by their document identification codes.
</nextsent>
<nextsent>in the top chart, recall scores appear under each of the users for each translation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y169">
<title id=" W01-1611.xml">confidence based adapt ivity in response generation for a spoken dialogue system </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>cooperation is used to refer to the participants  ability to collaborate with each other on given task and to provide informative and helpful contributions in given task context.
</prevsent>
<prevsent>naturalness, however, describes the participants  reaction which is appropriate in the current communicative context, and usually presupposes reasoning through which the participants can adapt themselves to the requirements of the situation and to the knowledge level of their partner.
</prevsent>
</prevsection>
<citsent citstr=" W98-1417 ">
naturalness in spoken interaction can be characterised by features such as incrementalityand immediacy (jokinen et al, 1998).<papid> W98-1417 </papid></citsent>
<aftsection>
<nextsent>speakers exchange information and present the new information in stepwise manner, constructing common ground by providing new pieces of relevant information to complete the task thatthe interaction was initiated for.
</nextsent>
<nextsent>they monitor their own presentations and react to the partner contributions immediately, often simultaneously, to prevent potential misunderstandings growing and causing problems to the interaction.
</nextsent>
<nextsent>spoken dialogue systems that aim at natural interaction with the user should thu shave capabilities for incremental and immediate management of interaction.
</nextsent>
<nextsent>in other words, they should be able to produce responses that take into account the requirements of an incremental and immediate interactive situation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y172">
<title id=" W01-1611.xml">confidence based adapt ivity in response generation for a spoken dialogue system </title>
<section> condence-based adapt ivity.  </section>
<citcontext>
<prevsection>
<prevsent>since politeness is expected with strangers, more explicitness is therefore appropriate at the start of dialogue and less appropriate as the dialogue proceeds: it is thus inversely related to the con dence of the partner which gets established in the shared situation.
</prevsent>
<prevsent>this pattern of gradual change from more formal initial register to more informal register as the dialogue progresses is well known, at least in cultures in which register is not dictated strictly by social hierarchy.
</prevsent>
</prevsection>
<citsent citstr=" E89-1036 ">
dierences between english dialogues (dynamic register adaptivity)and japanese dialogues ( xed register through out) have been studied (kume et al, 1989).<papid> E89-1036 </papid></citsent>
<aftsection>
<nextsent>generation of referring expressions is mainly concerned with enabling successful discrimination of the correct referent.
</nextsent>
<nextsent>however, referring expressions in dialogue systems are also strongly aected by the level of con dence.
</nextsent>
<nextsent>when there is some doubt, it is safer to use highly explicit referring expressions.
</nextsent>
<nextsent>when there is high level of con dence, it is normal to take certain risks for the sake of uent interaction.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y173">
<title id=" W01-1611.xml">confidence based adapt ivity in response generation for a spoken dialogue system </title>
<section> condence-based adapt ivity.  </section>
<citcontext>
<prevsection>
<prevsent>obligations are connected to particular activity and role in the activity, varying also according to the speakers  familiarity and relative status with each other.in dialogue systems, communicative obligations are usually part of the system control structure.
</prevsent>
<prevsent>the system can take the initiative, give helpful information in anticipation of theuser questions or to resolve problematic situations (misheard words, ambiguous referents, etc.), or simply react to the user input as best as it can.
</prevsent>
</prevsection>
<citsent citstr=" P94-1001 ">
obligations are thus used as basic motivation for action (traum and allen, 1994).<papid> P94-1001 </papid></citsent>
<aftsection>
<nextsent>in our framework communicative obligations are dispersed among the agents and evaluator control.
</nextsent>
<nextsent>this allows us to make the obligations overt, since they can be implemented as simple dialogue agents.
</nextsent>
<nextsent>however, as their application order is not xed, the overall architecture supports exible interaction where the basic communicative ability of the system is shown in the functioning of the system itself.
</nextsent>
<nextsent>the systems sco operation is not only pre-assigned disposition to act in helpful way, but involves reasoning about the appropriate act in the context.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y174">
<title id=" W02-0308.xml">unsupervised corpus based method for extending a biomedical terminology </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>in other words, our objective is to acquire hyponyms for terms in an original vocabulary that appear in the literature but are not present in the original vocabulary.
</prevsent>
<prevsent>terms play major role in variety of natural language processing (nlp) applications, including machine translation, text understanding, automatic indexing, and information retrieval.
</prevsent>
</prevsection>
<citsent citstr=" E99-1003 ">
taking advantage of the availability of large corpora, automatic terminology acquisition methods were developed, for example, by bourigault and jacquemin (1999).<papid> E99-1003 </papid></citsent>
<aftsection>
<nextsent>word affinities generally play central role in these methods.
</nextsent>
<nextsent>grefenstette (1994) defines three association for computational linguistics.
</nextsent>
<nextsent>the biomedical domain, philadelphia, july 2002, pp.
</nextsent>
<nextsent>53-60.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y175">
<title id=" W02-0308.xml">unsupervised corpus based method for extending a biomedical terminology </title>
<section> methods.  </section>
<citcontext>
<prevsection>
<prevsent>examples of simple medline noun phrases include abdominal aneurysmal aortitis and radical aggressive tumor resection.
</prevsent>
<prevsent>out of some forty million simple noun phrases, we randomly selected subset of three million phrases to be used as our corpus, representative of the noun phrases found in medline.
</prevsent>
</prevsection>
<citsent citstr=" A00-1026 ">
the phrases in our sample were then submitted to an underspecified syntactic analysis described by rindflesch et al (2000) <papid> A00-1026 </papid>that draws on stochastic tagger (see cutting et al (1992) <papid> A92-1018 </papid>for details) as well as the specialist lexicon, large syntactic lexicon of both general and medical english that is distributed with the umls.</citsent>
<aftsection>
<nextsent>although not perfect, this combination of resources effectively addresses the phenomenon of part-of-speech ambiguity in english.
</nextsent>
<nextsent>the resulting syntactic structure identifies the head and modifiers for the noun phrase analyzed.
</nextsent>
<nextsent>each modifier is also labeled as being adjectival,adverbial, or nominal.
</nextsent>
<nextsent>although all types of modification in the simple english noun phrase were labeled, only adjectives and nouns were selected for further analysis in this study.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y176">
<title id=" W02-0308.xml">unsupervised corpus based method for extending a biomedical terminology </title>
<section> methods.  </section>
<citcontext>
<prevsection>
<prevsent>examples of simple medline noun phrases include abdominal aneurysmal aortitis and radical aggressive tumor resection.
</prevsent>
<prevsent>out of some forty million simple noun phrases, we randomly selected subset of three million phrases to be used as our corpus, representative of the noun phrases found in medline.
</prevsent>
</prevsection>
<citsent citstr=" A92-1018 ">
the phrases in our sample were then submitted to an underspecified syntactic analysis described by rindflesch et al (2000) <papid> A00-1026 </papid>that draws on stochastic tagger (see cutting et al (1992) <papid> A92-1018 </papid>for details) as well as the specialist lexicon, large syntactic lexicon of both general and medical english that is distributed with the umls.</citsent>
<aftsection>
<nextsent>although not perfect, this combination of resources effectively addresses the phenomenon of part-of-speech ambiguity in english.
</nextsent>
<nextsent>the resulting syntactic structure identifies the head and modifiers for the noun phrase analyzed.
</nextsent>
<nextsent>each modifier is also labeled as being adjectival,adverbial, or nominal.
</nextsent>
<nextsent>although all types of modification in the simple english noun phrase were labeled, only adjectives and nouns were selected for further analysis in this study.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y177">
<title id=" W01-0906.xml">verification and validation of language processing systems is it evaluation </title>
<section> evaluation of nlp systems.  </section>
<citcontext>
<prevsection>
<prevsent>this is essentially the same testing method that was used for the mycin expert system (yu, 1985) and many additional systems.
</prevsent>
<prevsent>however, within the expert systems area several methods have been developed in subsequent years that address the weaknesses of strictly functional evaluation approaches (e.g. barr, 1999; gross ner, 1993).
</prevsent>
</prevsection>
<citsent citstr=" P97-1035 ">
there are also well-known evaluation efforts such as eagles (sparck jones and galli ers, 1996) and the paradise evaluation framework (walker et al, 1997).<papid> P97-1035 </papid></citsent>
<aftsection>
<nextsent>in addition, many researchers have participated in the comparative evaluation efforts characterized by the text retrieval conferences (trec)1, the message understanding conferences (muc)2 and document understanding conferences (duc)3, the cross-language evaluation forum (clef)4, and the summarization evaluation effort (summac) (for very comprehensive list of evaluation related links, see http://www.limsi.fr/tlp/class/prj.eval.links.h tml).
</nextsent>
<nextsent>evaluation of nlp systems is aided by the fact that there is considerable test data available.
</nextsent>
<nextsent>there are substantial repositories of data, such as the trec collection that includes, among other data, associated press wire feeds; department of energy documents; federal register documents; wall street journal full texts; and sources from ziff-davis publishing.
</nextsent>
<nextsent>1 http://trec.nist.gov/ 2 http://www.itl.nist.gov/iaui/894.02/related_projects/ti pster/muc.htm 3 http://www-nlpir.nist.gov/projects/duc/index.html 4 http://www.iei.i.cnr.it/delos/clef/ it is important to note that the darpa/arpa sponsored conferences (muc, tipster, and trec, for example), while making considerable data available, promote functional testing by stressing black-box performance of system.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y178">
<title id=" W02-0108.xml">using gate as an environment for teaching nlp </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in addition, these environments offer support and automation of common tasks, e.g., user interfaces can be designed easily by assembling them visually from components like menus and windows.
</prevsent>
<prevsent>similarly, nlp and cl students can benefit from the existence of graphical development environment, which allows them to gethands-on experience in every aspect of developing and evaluating language processing modules.
</prevsent>
</prevsection>
<citsent citstr=" P02-1022 ">
in addition, such tool would enable students to see clearly the practical relevance and need for language processing, by allowing them to experiment easily with building nlp-powered (web) applications.this paper shows how an existing infrastructure for language engineering research ? gate (cunningham et al, 2002<papid> P02-1022 </papid>a; cunningham, 2002)?</citsent>
<aftsection>
<nextsent>has been used successfully as an nlp teaching environment, in addition to being successful vehicle for building nlp applications and reusable components (maynard et al, 2002; maynard et al, 2001).
</nextsent>
<nextsent>the key features of gate which make it particularly suitable for teaching are: ? the system is designed to separate cleanly low-level tasks such as data storage, data visualisation, location and loading of components and execution of processes from thedata structures and algorithms that actually process human language.
</nextsent>
<nextsent>in this way, the students can concentrate on studyingand/or modifying the nlp data and algorithms, while leaving the mundane tasks to gate.
</nextsent>
<nextsent>july 2002, pp.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y188">
<title id=" W02-0108.xml">using gate as an environment for teaching nlp </title>
<section> gate from teaching.  </section>
<citcontext>
<prevsection>
<prevsent>the annotation can be done completely manually or it can be boot strapped by running someof gates processing resources over the corpus and then correcting/adding new annotations manually.
</prevsent>
<prevsent>these facilities can also be used in courses and assignments where the students need to learn how to create data for quantitative evaluation of nlp systems.
</prevsent>
</prevsection>
<citsent citstr=" W02-0109 ">
if evaluated against the requirements for teaching environments discussed in (loper and bird, 2002), <papid> W02-0109 </papid>gate covers them all quite well.</citsent>
<aftsection>
<nextsent>the graphical development environment and the jape language facilitate otherwise difficult tasks.
</nextsent>
<nextsent>inter-module consistency is achieved by using the annotations model to hold language data, while extensibility and modularity are the very reason why gate has been successfully used in many research projects (maynard et al, 2000).
</nextsent>
<nextsent>in addition, gate also offers robustness and scala bility, which allow students to experiment with big corpora, such as the british national corpus (approx.
</nextsent>
<nextsent>4gb).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y205">
<title id=" W00-1315.xml">empirical term weighting and expansion frequency </title>
<section> 0.53 0.81  </section>
<citcontext>
<prevsection>
<prevsent>3 burs iness.
</prevsent>
<prevsent>table 6 is like tables 4 but the binning rule not only uses idf, but also burst iness (b).
</prevsent>
</prevsection>
<citsent citstr=" C00-1027 ">
burst iness (church and gale, 1995)(katz, 1996)(church, 2000) <papid> C00-1027 </papid>is intended to account for the fact that some very good keywords uch as  kennedy  tend to be mentioned quite few times in document or not at all, whereas less good keywords uch as  except  tend to be mentioned about the same number of times no matter what the document tf 0 1 2 3 4+ b=0 b -0.05 -0.00 -0.61 -1.23 0.63 -0.80 -0.76 0.71 -0.05 0.00 0.69 0.23 0.68 0.71 0.75 b=i b 0.02 0.79 0.79 0.82 0.83 table 6: regression coefficients for method fit-b.</citsent>
<aftsection>
<nextsent>note that the slopes and intercepts are larger when = 1 than when = 0 (except when f = 0).
</nextsent>
<nextsent>even though usually lies between-0 and idf, we restrict to 0     idf, just to make sure.
</nextsent>
<nextsent>120 tf ef 1 0 2 0 3 0 4+ 0 1 2 3 4+ 1 2 3 4+ 2 2 2 2 1 3 2 3 3 3 4+ 3 where=d b -1.57 0.37 -3.41 0.82 -1.30 0.11 0.40 0.06 -1.84 0.87 -2.12 1.10 -0.66 0.95 0.84 0.98 -1.87 0.92 -1.77 1.12 -1.72 1.10 -3.06 1.71  -2.52 0.95 -1.81 1.02 0.45 0.85 0.38 1.22 where=e b -2.64 -2.70 -2.98 -3.35 -3.00 -2.78 -3.07 -3.25 0.68 0.71 0.74 0.78 0.86 0.85 0.93 0.79 -2.71 0.91 -2.28 0.88 -2.63 0.97 -3.66 1.14 table 7: many of the regression coefficients for method fit-e.
</nextsent>
<nextsent>(the coefficients marked with an asterisk are worrisome because the bins are too small and/or the slopes fall well outside the nor-mal range of 0 to 1.)
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y206">
<title id=" W01-1629.xml">spoken dialogue control based on a turn minimization criterion depending on the speech recognition accuracy </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, the system must include candidates for all attributes in recognition vocabulary, which cause more recognition errors.
</prevsent>
<prevsent>moreover, even though there is only one mis recognized item in confirmed items, theuser might just say coldly no?, and the system cannot know that what are correct items.
</prevsent>
</prevsection>
<citsent citstr=" C00-1073 ">
several efficient dialogue control methods have been proposed (niimi and kobayashi, 1996; litman et al, 2000).<papid> C00-1073 </papid></citsent>
<aftsection>
<nextsent>but there is no previous works that take into account multiple types of user requests and recognition accuracy during confirmation, which changes what to be confirmed without domain-specific rules or training.to prevent needlessly long confirmation dialogues even if the system can accepts multiple types of user request, our method estimates the expected number of turns to certain use request type and the approximated probability distribution of user request types.
</nextsent>
<nextsent>the expected number of turns can be derived from the required vocabulary for confirmation and base recognition accuracy under certain vocabulary size.
</nextsent>
<nextsent>overview first, we describe about system to which we assume this method will be applied.
</nextsent>
<nextsent>the system has belief state which is represented by the set of attributes, their values, and the certainty of the values.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y207">
<title id=" W00-1103.xml">use of dependency tree structures for the micro context extraction </title>
<section> transformation the complements..  </section>
<citcontext>
<prevsection>
<prevsent>krovetz and croft (1992) demonstrated that wsd can improve text retrieval performance.
</prevsent>
<prevsent>later, schiitze and pedersen (1995) found noticeable improvement in precision using sense-based retrieval and word sense discrimination.
</prevsent>
</prevsection>
<citsent citstr=" J98-1005 ">
towell and voorhees (1998) <papid> J98-1005 </papid>showed that, given accurate wsd, the lexical relations encoded in lexicons such as wordnet can be exploited to improve the effectiveness of ir systems.</citsent>
<aftsection>
<nextsent>schiitze (1998) introduced an interesting method: word sense discrimination.
</nextsent>
<nextsent>this technique is easier than full disambiguation since it only determines which occurrences of given word have the same meaning and not what the ? meaning actually is. moreover, while other disambiguation algorithms employ various sources of information, this method ispenses of an outside source of knowledge for defining senses.
</nextsent>
<nextsent>for many problems in information access, it is sufficient to solve the discrimination problem only.
</nextsent>
<nextsent>schiitze and pedersen (1995) measured ocument-query similarity based on word senses rather on words and achieved considerable improvement in ranking relevant documents.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y208">
<title id=" W00-1103.xml">use of dependency tree structures for the micro context extraction </title>
<section> transformation the complements..  </section>
<citcontext>
<prevsection>
<prevsent>in the literature, we can meet different definitions of collocation (cf.
</prevsent>
<prevsent>ide and vdronis, 1998).
</prevsent>
</prevsection>
<citsent citstr=" H93-1052 ">
following yarowsky (1993), <papid> H93-1052 </papid>who explicitly addresses the use of collocations in the wsd work, we adopt his definition, adapted to our purpose: collocation is co-occurrence of two words in defined relation.</citsent>
<aftsection>
<nextsent>dependency micro contexts and collocations can be treated as mutually equivalent concepts in the sense that collocations can be derived from the knowledge of micro contexts and vice versa.
</nextsent>
<nextsent>in order to separate significant collocations from word pairs which occurred merely by coincidence, we can compute the measure of col locating of word and an mce as the mutual information of the probability of their occurrence.
</nextsent>
<nextsent>we also use the knowledge of collocations for computing the context similarity measure of two words.
</nextsent>
<nextsent>assuming the  strong contextual hypothesis , the context similarity of words implies their semantic similarity, too.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y209">
<title id=" W02-0201.xml">synchronization in an asynchronous agent based architecture for dialogue systems </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we present this architecture and the synchronization issues we encountered in building truly distributed, agent based dialogue architecture.
</prevsent>
<prevsent>more and more people are building dialogue systems.
</prevsent>
</prevsection>
<citsent citstr=" P99-1026 ">
architecturally, these systems tend to fall into two camps: those with pipe lined architectures (e.g., (lamel et al, 1998; nakano et al, 1999)), <papid> P99-1026 </papid>and those with agent-based architectures (e.g., (seneff et al, 1999; stent et al, 1999; <papid> P99-1024 </papid>rudnicky et al,1999)).</citsent>
<aftsection>
<nextsent>agent-based architectures are advantageous because they free up system components to potentially act in more asynchronous manner.
</nextsent>
<nextsent>however, in practice, most dialogue systems built on an agent-based architecture pass messages such that they are basically functioning in terms of pipe lined flow-of-information.
</nextsent>
<nextsent>our original implementation of the trips spoken dialogue system (ferguson and allen, 1998) was such an agent-based, pipelinedflow-of-information system.
</nextsent>
<nextsent>recently, how ever, we made changes to the system (allenet al, 2001a) which allow it to take advantage of the distributed nature of an agent based system.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y210">
<title id=" W02-0201.xml">synchronization in an asynchronous agent based architecture for dialogue systems </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we present this architecture and the synchronization issues we encountered in building truly distributed, agent based dialogue architecture.
</prevsent>
<prevsent>more and more people are building dialogue systems.
</prevsent>
</prevsection>
<citsent citstr=" P99-1024 ">
architecturally, these systems tend to fall into two camps: those with pipe lined architectures (e.g., (lamel et al, 1998; nakano et al, 1999)), <papid> P99-1026 </papid>and those with agent-based architectures (e.g., (seneff et al, 1999; stent et al, 1999; <papid> P99-1024 </papid>rudnicky et al,1999)).</citsent>
<aftsection>
<nextsent>agent-based architectures are advantageous because they free up system components to potentially act in more asynchronous manner.
</nextsent>
<nextsent>however, in practice, most dialogue systems built on an agent-based architecture pass messages such that they are basically functioning in terms of pipe lined flow-of-information.
</nextsent>
<nextsent>our original implementation of the trips spoken dialogue system (ferguson and allen, 1998) was such an agent-based, pipelinedflow-of-information system.
</nextsent>
<nextsent>recently, how ever, we made changes to the system (allenet al, 2001a) which allow it to take advantage of the distributed nature of an agent based system.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y211">
<title id=" W02-0201.xml">synchronization in an asynchronous agent based architecture for dialogue systems </title>
<section> trips architecture.  </section>
<citcontext>
<prevsection>
<prevsent>3this is selective broadcasts to the components which have registered for such messages.
</prevsent>
<prevsent>2.2.2 discourse level the discourse level4 describes information which is not directly related to the task at hand, but rather is linguistic in nature.
</prevsent>
</prevsection>
<citsent citstr=" P94-1001 ">
this information is represented as salience information (for reference) and discourse obligations (traum and allen, 1994).<papid> P94-1001 </papid>when the user makes an utterance, the in put passes (as detailed above) through the speech recognizer, to the parser, and then to the im, which calls reference to do resolution.</citsent>
<aftsection>
<nextsent>based on this reference resolved form, the im computes any discourse obligations which the utterance entails (e.g., if the utterance was question, to address or answer it, also, to acknowledge that it heard the question).at this point, the im broadcasts an system heard?
</nextsent>
<nextsent>message, which includes incurred discourse obligations and changes in salience.upon receipt of this message, discourse context updates its discourse obligations and reference updates its salience information.
</nextsent>
<nextsent>the gm learns of new discourse obligations from the discourse context and begins to try to fulfill them, regardless of whether or notit has heard from the ba about the problem solving side of things.
</nextsent>
<nextsent>however, there are some obligations it will be unable to fulfill without knowledge of what is happening at the problem solving level ? answering or addressing the question, for example.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y212">
<title id=" W00-0905.xml">verb subcategorization frequency differences between business news and balanced corpora the role of verb sense </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we show that all or nearly all of these shifts in subcategorization are realised via (often subtle) word sense differences.
</prevsent>
<prevsent>this is an interesting observation itself, and also suggests that stable cross corpus subcategorization frequencies may be found when verb sense is adequately controlled.
</prevsent>
</prevsection>
<citsent citstr=" P96-1025 ">
verb subcategorizafion probabilities play an important role in both computational linguistic applications (e.g. carroll, minnen, and briscoe 1998, charniak 1997, collins 1996/<papid> P96-1025 </papid>1997, joshi and srinivas 1994, <papid> C94-1024 </papid>kim, srinivas, and tme swell 1997, stolcke et al 1997) and psycholinguisfic models of language processing (e.g. boland 1997, clifton et al 1984, ferreira &amp; mcclure 1997, fodor 1978, garnsey et al 1997, jurafsky 1996, macdonald 1994, mitchell &amp; holmes 1985, tanenhaus et al 1990, true swell et al 1993).</citsent>
<aftsection>
<nextsent>previous research, however, has shown that subcategorization probabilities vary widely in different corpora.
</nextsent>
<nextsent>studies such as merlo (1994), gibson et al (1996), and roland &amp; jurafsky (1997) have found subcategorization frequency differences between traditional corpus data and data from psychological experiments.
</nextsent>
<nextsent>biber (1993) <papid> J93-2001 </papid>and biber et al (1998) have shown that that word frequency, word sense (as defined by collocates), the distribution of synonymous words and the use of syntactic structures varies with corpus genre.</nextsent>
<nextsent>roland &amp; jurafsky (1998, <papid> P98-2184 </papid>2000 in press) showed that there were subcategorization frequency differences between various written and spoken corpora, and furthermore showed that that these subcategorization frequency differences are caused by variation in word sense as well as genre and discourse type differences among the corpora.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y213">
<title id=" W00-0905.xml">verb subcategorization frequency differences between business news and balanced corpora the role of verb sense </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we show that all or nearly all of these shifts in subcategorization are realised via (often subtle) word sense differences.
</prevsent>
<prevsent>this is an interesting observation itself, and also suggests that stable cross corpus subcategorization frequencies may be found when verb sense is adequately controlled.
</prevsent>
</prevsection>
<citsent citstr=" C94-1024 ">
verb subcategorizafion probabilities play an important role in both computational linguistic applications (e.g. carroll, minnen, and briscoe 1998, charniak 1997, collins 1996/<papid> P96-1025 </papid>1997, joshi and srinivas 1994, <papid> C94-1024 </papid>kim, srinivas, and tme swell 1997, stolcke et al 1997) and psycholinguisfic models of language processing (e.g. boland 1997, clifton et al 1984, ferreira &amp; mcclure 1997, fodor 1978, garnsey et al 1997, jurafsky 1996, macdonald 1994, mitchell &amp; holmes 1985, tanenhaus et al 1990, true swell et al 1993).</citsent>
<aftsection>
<nextsent>previous research, however, has shown that subcategorization probabilities vary widely in different corpora.
</nextsent>
<nextsent>studies such as merlo (1994), gibson et al (1996), and roland &amp; jurafsky (1997) have found subcategorization frequency differences between traditional corpus data and data from psychological experiments.
</nextsent>
<nextsent>biber (1993) <papid> J93-2001 </papid>and biber et al (1998) have shown that that word frequency, word sense (as defined by collocates), the distribution of synonymous words and the use of syntactic structures varies with corpus genre.</nextsent>
<nextsent>roland &amp; jurafsky (1998, <papid> P98-2184 </papid>2000 in press) showed that there were subcategorization frequency differences between various written and spoken corpora, and furthermore showed that that these subcategorization frequency differences are caused by variation in word sense as well as genre and discourse type differences among the corpora.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y214">
<title id=" W00-0905.xml">verb subcategorization frequency differences between business news and balanced corpora the role of verb sense </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>previous research, however, has shown that subcategorization probabilities vary widely in different corpora.
</prevsent>
<prevsent>studies such as merlo (1994), gibson et al (1996), and roland &amp; jurafsky (1997) have found subcategorization frequency differences between traditional corpus data and data from psychological experiments.
</prevsent>
</prevsection>
<citsent citstr=" J93-2001 ">
biber (1993) <papid> J93-2001 </papid>and biber et al (1998) have shown that that word frequency, word sense (as defined by collocates), the distribution of synonymous words and the use of syntactic structures varies with corpus genre.</citsent>
<aftsection>
<nextsent>roland &amp; jurafsky (1998, <papid> P98-2184 </papid>2000 in press) showed that there were subcategorization frequency differences between various written and spoken corpora, and furthermore showed that that these subcategorization frequency differences are caused by variation in word sense as well as genre and discourse type differences among the corpora.</nextsent>
<nextsent>while the subcategorization probabilities in computational nguage model can be adjusted to match particular corpus, cross corpus differences in such probabilities pose an important problem when using corpora for norming psychological experiments.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y215">
<title id=" W00-0905.xml">verb subcategorization frequency differences between business news and balanced corpora the role of verb sense </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>studies such as merlo (1994), gibson et al (1996), and roland &amp; jurafsky (1997) have found subcategorization frequency differences between traditional corpus data and data from psychological experiments.
</prevsent>
<prevsent>biber (1993) <papid> J93-2001 </papid>and biber et al (1998) have shown that that word frequency, word sense (as defined by collocates), the distribution of synonymous words and the use of syntactic structures varies with corpus genre.</prevsent>
</prevsection>
<citsent citstr=" P98-2184 ">
roland &amp; jurafsky (1998, <papid> P98-2184 </papid>2000 in press) showed that there were subcategorization frequency differences between various written and spoken corpora, and furthermore showed that that these subcategorization frequency differences are caused by variation in word sense as well as genre and discourse type differences among the corpora.</citsent>
<aftsection>
<nextsent>while the subcategorization probabilities in computational nguage model can be adjusted to match particular corpus, cross corpus differences in such probabilities pose an important problem when using corpora for norming psychological experiments.
</nextsent>
<nextsent>if each corpus generates separate set of probabilities, which probabilities are the correct ones to use as model of human language processing?
</nextsent>
<nextsent>in an attempt to use corpora to provide norming data for 64 verbs for experimental purposes, we investigate in detail how verb frequencies and verb subcategorization frequencies differ among three corpora: the british national corpus 28 (bnc), the wall street journal corpus (wsj), and the brown corpus (brown).
</nextsent>
<nextsent>for the 64 verbs, we randomly selected set of sentences from each corpus and hand-coded them for transit ivity, passive versus active voice, and whether the selected usage was an instance of the most common sense of the verb.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y216">
<title id=" W00-0905.xml">verb subcategorization frequency differences between business news and balanced corpora the role of verb sense </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>even after controlfing for the broad sense of the verb, we found subcategorization differences caused by the  micro-differences  in sense, including quite specific arguments othe verb.
</prevsent>
<prevsent>1 data.
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
data for 64 verbs (shown in table 1) was collected from three corpora; the british national corpus (bnc) (http j/info.ox.ac.uk/bnc/index.html), he penn tree hank parsed version of the brown corpus (brown), and the penn treebank wall street journal corpas (wsj) (marcus et al 1993).<papid> J93-2004 </papid></citsent>
<aftsection>
<nextsent>the 64 verbs were chosen on the basis of the requirements of separate psychological experiments including having single dominant sense, being easily ima gable  and participating in one of several subcategorization alternations.
</nextsent>
<nextsent>a random sample of 100 examples of each verb was selected from each of the three corpora.
</nextsent>
<nextsent>when the corpus contained less than 100 tokens of the verb, as was frequently the case in the brown and wsj corpora, the entire available data was used.
</nextsent>
<nextsent>this data was coded for several properties: transitive/intransitive  active/passive  and whether the example involved the major sense of the verb or not.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y217">
<title id=" W02-0216.xml">multitasking and collaborative activities in dialogue systems </title>
<section> message generation.  </section>
<citcontext>
<prevsection>
<prevsent>for this reason, generation of large pieces of text is not appropriate, especially since the user is able to interrupt the system.
</prevsent>
<prevsent>other differences abound, for example that aggregation rules must be sensitive to incremental aspects of message generation.as well as the general problems of message selection and aggregation in dialogue systems, this particular type of application domain presents specific problems in comparison with, say, travel-planningdialogue systems ? e.g.
</prevsent>
</prevsection>
<citsent citstr=" H91-1070 ">
(seneff et al, 1991).<papid> H91-1070 </papid></citsent>
<aftsection>
<nextsent>an autonomous device will, in general, need to communicate about,   its perceptions of changing environment,   progress towards user-specified goals,   execution status of activities or tasks,   its own internal state changes,   the progress of the dialogue itself.for these reasons, the message selection and generation component of such system needs to beof wider coverage and more flexible than template based approaches, while remaining in real, or near real, time (stent, 1999).
</nextsent>
<nextsent>as well as this, the system must potentially be able to deal with large bandwidth stream of communications from the robot, and so must be able to intelligently filter them for relevance?
</nextsent>
<nextsent>so that the user is not overloaded with unimportant information, or repetitious utterances.
</nextsent>
<nextsent>in general, the system should appear as natural?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y218">
<title id=" W02-0216.xml">multitasking and collaborative activities in dialogue systems </title>
<section> message generation.  </section>
<citcontext>
<prevsection>
<prevsent>system should also exhibit variability?
</prevsent>
<prevsent>in that it can convey the same content in variety of ways.
</prevsent>
</prevsection>
<citsent citstr=" J90-1004 ">
a further desirable feature is that the systems generated utterances should be in the cover age of the dialogue systems speech recognizer, so that system-generated utterances effectively prime the user to speak in-grammar.consequently we attempted to implement the following features in message selection and generation:relevance filtering; recency filtering; echoing; variability; aggregation; symmetry; real-time genera tion.our general method is to take as inputs to the process various communicative goals of the system, expressed as logical forms, and use them to construct asingle new logical form to be input to geminis semantic head-driven generation algorithm (shieber et al, 1990), <papid> J90-1004 </papid>which produces strings for festival speech synthesis.</citsent>
<aftsection>
<nextsent>we now describe how to use complex dialogue context to produce natural generation in multitasking contexts.
</nextsent>
<nextsent>6.1 message selection - filtering.
</nextsent>
<nextsent>inputs to the selection and generation module areconcept?
</nextsent>
<nextsent>logical forms (lfs) describing the communicative goals of the system.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y220">
<title id=" W01-1407.xml">toward hierarchical models for statistical machine translation of inflected languages </title>
<section> statistical machine translation.  </section>
<citcontext>
<prevsection>
<prevsent>every english string is considered as possible translation for the input.
</prevsent>
<prevsent>if we assign probability      to each pair of strings       , then according to bayes?
</prevsent>
</prevsection>
<citsent citstr=" P97-1047 ">
decision rule, we have to choose the english string that maximizes the product of the english language model     and the string translation model   ff fi . many existing systems for statistical machine translation (wang and waibel, 1997; <papid> P97-1047 </papid>nieen et al., 1998; och and weber, 1998) <papid> P98-2162 </papid>make use of special way of structuring the string translation model like proposed by (brown et al, 1993): <papid> J93-2003 </papid>the correspondence between the words in the source and the target string is described by alignments which assign one target word position to each source word position.</citsent>
<aftsection>
<nextsent>the lexicon probability fl    of certain english word is assumed to depend basically only on the source word  aligned to it.the overall architecture of the statistical translation approach is depicted in figure 1.
</nextsent>
<nextsent>in this figure we already anticipate the fact that we can transform the source strings in certain manner.
</nextsent>
<nextsent>the parameters of the statistical knowledge sources mentioned above are trained on bilingual source language text lexicon model language model global search: target language text over pr(f1 |e1i ) pr( e1i ) pr(f1 |e1i ) pr( e1i ) e1 f1 maximize alignment model transformation transformation morpho-syntactic analysis figure 1: architecture of the translation approach based on bayes?
</nextsent>
<nextsent>decision rule.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y221">
<title id=" W01-1407.xml">toward hierarchical models for statistical machine translation of inflected languages </title>
<section> statistical machine translation.  </section>
<citcontext>
<prevsection>
<prevsent>every english string is considered as possible translation for the input.
</prevsent>
<prevsent>if we assign probability      to each pair of strings       , then according to bayes?
</prevsent>
</prevsection>
<citsent citstr=" P98-2162 ">
decision rule, we have to choose the english string that maximizes the product of the english language model     and the string translation model   ff fi . many existing systems for statistical machine translation (wang and waibel, 1997; <papid> P97-1047 </papid>nieen et al., 1998; och and weber, 1998) <papid> P98-2162 </papid>make use of special way of structuring the string translation model like proposed by (brown et al, 1993): <papid> J93-2003 </papid>the correspondence between the words in the source and the target string is described by alignments which assign one target word position to each source word position.</citsent>
<aftsection>
<nextsent>the lexicon probability fl    of certain english word is assumed to depend basically only on the source word  aligned to it.the overall architecture of the statistical translation approach is depicted in figure 1.
</nextsent>
<nextsent>in this figure we already anticipate the fact that we can transform the source strings in certain manner.
</nextsent>
<nextsent>the parameters of the statistical knowledge sources mentioned above are trained on bilingual source language text lexicon model language model global search: target language text over pr(f1 |e1i ) pr( e1i ) pr(f1 |e1i ) pr( e1i ) e1 f1 maximize alignment model transformation transformation morpho-syntactic analysis figure 1: architecture of the translation approach based on bayes?
</nextsent>
<nextsent>decision rule.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y222">
<title id=" W01-1407.xml">toward hierarchical models for statistical machine translation of inflected languages </title>
<section> statistical machine translation.  </section>
<citcontext>
<prevsection>
<prevsent>every english string is considered as possible translation for the input.
</prevsent>
<prevsent>if we assign probability      to each pair of strings       , then according to bayes?
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
decision rule, we have to choose the english string that maximizes the product of the english language model     and the string translation model   ff fi . many existing systems for statistical machine translation (wang and waibel, 1997; <papid> P97-1047 </papid>nieen et al., 1998; och and weber, 1998) <papid> P98-2162 </papid>make use of special way of structuring the string translation model like proposed by (brown et al, 1993): <papid> J93-2003 </papid>the correspondence between the words in the source and the target string is described by alignments which assign one target word position to each source word position.</citsent>
<aftsection>
<nextsent>the lexicon probability fl    of certain english word is assumed to depend basically only on the source word  aligned to it.the overall architecture of the statistical translation approach is depicted in figure 1.
</nextsent>
<nextsent>in this figure we already anticipate the fact that we can transform the source strings in certain manner.
</nextsent>
<nextsent>the parameters of the statistical knowledge sources mentioned above are trained on bilingual source language text lexicon model language model global search: target language text over pr(f1 |e1i ) pr( e1i ) pr(f1 |e1i ) pr( e1i ) e1 f1 maximize alignment model transformation transformation morpho-syntactic analysis figure 1: architecture of the translation approach based on bayes?
</nextsent>
<nextsent>decision rule.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y224">
<title id=" W01-1407.xml">toward hierarchical models for statistical machine translation of inflected languages </title>
<section> output representation after.  </section>
<citcontext>
<prevsection>
<prevsent>as consequence of these considerations, we aim at taking into account the interdependencies between the different derivatives of the same base form.
</prevsent>
<prevsent>morpho-syntactic analysis we use gercg, constraint grammar parser for german for lexical analysis and morphological and syntactic disambiguation.
</prevsent>
</prevsection>
<citsent citstr=" C90-3030 ">
for description of the constraint grammar approach we refer the reader to (karlsson, 1990).<papid> C90-3030 </papid></citsent>
<aftsection>
<nextsent>figure 2 gives an example of the information provided by this tool.
</nextsent>
<nextsent>input: wir wollen nach dem essen nach essen aufbrechen   *wir    wir  * pron pers pl1 nom   wollen    wollen  ind pras pl1   nach    nach  pre prap dat   dem    das  art def sg dat neutr   *essen    *essen  neutr sg dat   nach    nach  pre prap dat   *essen    *essen  eigen neutr sg dat  *esse  fem pl dat  *essen  neutr pl dat  *essen  neutr sg dat   aufbrechen    aufbrechen  inf figure 2: sample analysis of german sentencea full word form is represented by the information provided by the morpho-syntactic analysis: from the interpretation gehen-v-ind pras-sg1?, i.e. the lemma plus part of speech plus the other tags the word form gehe?
</nextsent>
<nextsent>can be restored.
</nextsent>
<nextsent>from figure 2 we see that the toolcan quite reliably disambiguate between different readings: it infers for instance that the word wollen?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y225">
<title id=" W01-1407.xml">toward hierarchical models for statistical machine translation of inflected languages </title>
<section> equivalence classes of words with.  </section>
<citcontext>
<prevsection>
<prevsent>we found that the impact on the end result due to different choices of features to be ignored was not large enough to serve as reliable criterion.
</prevsent>
<prevsent>instead, we could think of defining likelihood criterion ona held-out corpus for this purpose.
</prevsent>
</prevsection>
<citsent citstr=" P98-1117 ">
another possibility is to assess the impact on the alignment quality after training, which can be evaluated automatically (langlais et al, 1998; <papid> P98-1117 </papid>och and ney, 2000), <papid> P00-1056 </papid>but as we found that the alignment quality on the verb mobil data is consistently very high, and extremely robust against manipulation of the training data, we abandoned this approach.</citsent>
<aftsection>
<nextsent>we resorted to detecting candidates from the probabilistic lexica trained for translation from german to english.
</nextsent>
<nextsent>for this, we focussed on those derivatives of the same base form, which resulted in the same translation.
</nextsent>
<nextsent>for each set of tags, we counted how often an additional tag could be replaced by certain other tag without effect on the translation.
</nextsent>
<nextsent>table 1 gives some ofthe most frequently identified candidates to be ignored while translating: the gender of nouns is irrelevant for their translation (which is straight forward, because the gender is unambiguous for certain noun) and the case, i.e. nominative, dative, accusative.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y226">
<title id=" W01-1407.xml">toward hierarchical models for statistical machine translation of inflected languages </title>
<section> equivalence classes of words with.  </section>
<citcontext>
<prevsection>
<prevsent>we found that the impact on the end result due to different choices of features to be ignored was not large enough to serve as reliable criterion.
</prevsent>
<prevsent>instead, we could think of defining likelihood criterion ona held-out corpus for this purpose.
</prevsent>
</prevsection>
<citsent citstr=" P00-1056 ">
another possibility is to assess the impact on the alignment quality after training, which can be evaluated automatically (langlais et al, 1998; <papid> P98-1117 </papid>och and ney, 2000), <papid> P00-1056 </papid>but as we found that the alignment quality on the verb mobil data is consistently very high, and extremely robust against manipulation of the training data, we abandoned this approach.</citsent>
<aftsection>
<nextsent>we resorted to detecting candidates from the probabilistic lexica trained for translation from german to english.
</nextsent>
<nextsent>for this, we focussed on those derivatives of the same base form, which resulted in the same translation.
</nextsent>
<nextsent>for each set of tags, we counted how often an additional tag could be replaced by certain other tag without effect on the translation.
</nextsent>
<nextsent>table 1 gives some ofthe most frequently identified candidates to be ignored while translating: the gender of nouns is irrelevant for their translation (which is straight forward, because the gender is unambiguous for certain noun) and the case, i.e. nominative, dative, accusative.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y229">
<title id=" W01-1407.xml">toward hierarchical models for statistical machine translation of inflected languages </title>
<section> translation experiments.  </section>
<citcontext>
<prevsection>
<prevsent>of word forms 385 no.
</prevsent>
<prevsent>of unknown word forms 25we used translation system called single word based approach?
</prevsent>
</prevsection>
<citsent citstr=" C00-2123 ">
described in (tillmann and ney, 2000) <papid> C00-2123 </papid>and compared to other approaches in (ney et al, 2000).</citsent>
<aftsection>
<nextsent>7.3.1 lexicon combination so far we have performed experiments with hierarchical lexica, where two levels are combined, i.e.   in equation (2) is set to 1.
</nextsent>
<nextsent>  &amp; and    are set to dc and fl   43&amp;  is modeled as uniform distribution over all derivations of the lemma 4*&amp; occurring in the training data plus the base form itself, in case it is not contained.
</nextsent>
<nextsent>the process of lemmatization is unique in the majority of cases, and as consequence, the sum in equation (1) is not needed for two-level lexicon combination of full word forms and lemmata.
</nextsent>
<nextsent>as the results summarized in table 4 show, the combined lexicon outperforms the conventional one-level lexicon.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y230">
<title id=" W01-1402.xml">overcoming the customization bottleneck using example based mt </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>one of the goals of data-driven mt research has been to overcome this customization bottleneck through automated or semi-automated extraction of translation knowledge from bilingual corpora.
</prevsent>
<prevsent>to address this bottleneck, variety of example based machine translation (ebmt) systems have been created and described in the literature.
</prevsent>
</prevsection>
<citsent citstr=" C00-1078 ">
some of these employ parsers to produce dependency structures for the sentence pairs in aligned bilingual corpora, which are then aligned to obtain transfer rules or examples (meyers et al 2000; <papid> C00-1078 </papid>watanabe et al 2000).<papid> C00-2131 </papid></citsent>
<aftsection>
<nextsent>other systems extract and use examples that are represented as linear patterns of varying complexity (brown 1999; watanabe and takeda 1998; <papid> P98-2223 </papid>turcato et al 1999).</nextsent>
<nextsent>for some ebmt systems, substantial collections of examples are also manually crafted or at least reviewed for correctness after being identified automatically (watanabe et al 2000; <papid> C00-2131 </papid>brown 1999; franz et al 2000).<papid> C00-2152 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y232">
<title id=" W01-1402.xml">overcoming the customization bottleneck using example based mt </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>one of the goals of data-driven mt research has been to overcome this customization bottleneck through automated or semi-automated extraction of translation knowledge from bilingual corpora.
</prevsent>
<prevsent>to address this bottleneck, variety of example based machine translation (ebmt) systems have been created and described in the literature.
</prevsent>
</prevsection>
<citsent citstr=" C00-2131 ">
some of these employ parsers to produce dependency structures for the sentence pairs in aligned bilingual corpora, which are then aligned to obtain transfer rules or examples (meyers et al 2000; <papid> C00-1078 </papid>watanabe et al 2000).<papid> C00-2131 </papid></citsent>
<aftsection>
<nextsent>other systems extract and use examples that are represented as linear patterns of varying complexity (brown 1999; watanabe and takeda 1998; <papid> P98-2223 </papid>turcato et al 1999).</nextsent>
<nextsent>for some ebmt systems, substantial collections of examples are also manually crafted or at least reviewed for correctness after being identified automatically (watanabe et al 2000; <papid> C00-2131 </papid>brown 1999; franz et al 2000).<papid> C00-2152 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y233">
<title id=" W01-1402.xml">overcoming the customization bottleneck using example based mt </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>to address this bottleneck, variety of example based machine translation (ebmt) systems have been created and described in the literature.
</prevsent>
<prevsent>some of these employ parsers to produce dependency structures for the sentence pairs in aligned bilingual corpora, which are then aligned to obtain transfer rules or examples (meyers et al 2000; <papid> C00-1078 </papid>watanabe et al 2000).<papid> C00-2131 </papid></prevsent>
</prevsection>
<citsent citstr=" P98-2223 ">
other systems extract and use examples that are represented as linear patterns of varying complexity (brown 1999; watanabe and takeda 1998; <papid> P98-2223 </papid>turcato et al 1999).</citsent>
<aftsection>
<nextsent>for some ebmt systems, substantial collections of examples are also manually crafted or at least reviewed for correctness after being identified automatically (watanabe et al 2000; <papid> C00-2131 </papid>brown 1999; franz et al 2000).<papid> C00-2152 </papid></nextsent>
<nextsent>the efforts that report accuracy results for fully automatic example extraction (meyers et al 2000; <papid> C00-1078 </papid>watanabe et al 2000) <papid> C00-2131 </papid>do so for very modest amounts of training data (a few thousand sentence pairs).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y235">
<title id=" W01-1402.xml">overcoming the customization bottleneck using example based mt </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>some of these employ parsers to produce dependency structures for the sentence pairs in aligned bilingual corpora, which are then aligned to obtain transfer rules or examples (meyers et al 2000; <papid> C00-1078 </papid>watanabe et al 2000).<papid> C00-2131 </papid></prevsent>
<prevsent>other systems extract and use examples that are represented as linear patterns of varying complexity (brown 1999; watanabe and takeda 1998; <papid> P98-2223 </papid>turcato et al 1999).</prevsent>
</prevsection>
<citsent citstr=" C00-2152 ">
for some ebmt systems, substantial collections of examples are also manually crafted or at least reviewed for correctness after being identified automatically (watanabe et al 2000; <papid> C00-2131 </papid>brown 1999; franz et al 2000).<papid> C00-2152 </papid></citsent>
<aftsection>
<nextsent>the efforts that report accuracy results for fully automatic example extraction (meyers et al 2000; <papid> C00-1078 </papid>watanabe et al 2000) <papid> C00-2131 </papid>do so for very modest amounts of training data (a few thousand sentence pairs).</nextsent>
<nextsent>previous work in this area thus raises the possibility that manual review or crafting is required to obtain example bases of sufficient coverage and accuracy to be truly useful.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y239">
<title id=" W01-1402.xml">overcoming the customization bottleneck using example based mt </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the efforts that report accuracy results for fully automatic example extraction (meyers et al 2000; <papid> C00-1078 </papid>watanabe et al 2000) <papid> C00-2131 </papid>do so for very modest amounts of training data (a few thousand sentence pairs).</prevsent>
<prevsent>previous work in this area thus raises the possibility that manual review or crafting is required to obtain example bases of sufficient coverage and accuracy to be truly useful.</prevsent>
</prevsection>
<citsent citstr=" C92-3161 ">
other variations of ebmt systems are hybrids that integrate an ebmt component as one of multiple sources of transfer knowledge (in addition to other transfer rule or knowledge based components) used during translation (frederking et al 1994; takeda et al 1992).<papid> C92-3161 </papid></citsent>
<aftsection>
<nextsent>to our knowledge, commercial quality mt has so far been achieved only through years of effort in creating hand-coded transfer rules.
</nextsent>
<nextsent>systems whose primary source of translation knowledge comes from an automatically created example base have not been shown capable of matching or exceeding the quality of commercial systems.
</nextsent>
<nextsent>this paper reports on msr-mt, an mt system that attempts to break the customization bottleneck by exploiting example-based (and some statistical) techniques to automatically acquire its primary translation knowledge from bilingual corpus of several million words.
</nextsent>
<nextsent>the system leverages the linguistic generality of existing rule-based parsers to enable broad coverage and to overcome some of the limitations on locality of context characteristic of data-driven approaches.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y240">
<title id=" W01-1402.xml">overcoming the customization bottleneck using example based mt </title>
<section> training msr-mt.  </section>
<citcontext>
<prevsection>
<prevsent>both single word and multiword associations are iteratively hypothesized and scored by the algorithm under certain constraints until reliable set of each is obtained.
</prevsent>
<prevsent>over the english/spanish bilingual corpus used for the present work, 9,563 single word and 4,884 multi-word associations not already known to our system were identified using this method.
</prevsent>
</prevsection>
<citsent citstr=" W01-1411 ">
moore (2001) <papid> W01-1411 </papid>describes this technique in detail, while pinkham &amp; corston-oliver (2001) describes its integration with msr-mt and investigates its effect on translation quality.</citsent>
<aftsection>
<nextsent>4.2 logical form alignment.
</nextsent>
<nextsent>as described in section 2, msr-mt acquires transfer mappings by aligning pairs of lfs obtained from parsing sentence pairs in bilingual corpus.
</nextsent>
<nextsent>the lf alignment algorithm first establishes tentative lexical correspondences between nodes in the source and target lfs using translation pairs from bilingual lexicon.
</nextsent>
<nextsent>our english/spanish lexicon presently contains 88,500 translation pairs, which are then augmented with single word translations acquired using the statistical method described in section 4.1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y241">
<title id=" W01-1402.xml">overcoming the customization bottleneck using example based mt </title>
<section> training msr-mt.  </section>
<citcontext>
<prevsection>
<prevsent>after establishing possible correspondences, the algorithm uses small set of alignment grammar rules to align lf nodes according to both lexical and structural considerations and to create lf transfer mappings.
</prevsent>
<prevsent>the final step is to filter the mappings based on the frequency of their source and target sides.
</prevsent>
</prevsection>
<citsent citstr=" W01-1406 ">
menezes &amp; richardson (2001) <papid> W01-1406 </papid>provides further details and an evaluation of the lf alignment algorithm.</citsent>
<aftsection>
<nextsent>the english/spanish bilingual training corpus, consisting largely of microsoft manuals and help text, averaged 14.1 words per english sentence.
</nextsent>
<nextsent>a 2.5 million word sample of english data contained almost 40k unique word forms.
</nextsent>
<nextsent>the data was arbitrarily split in two for use in our spanish-english and english-spanish systems.
</nextsent>
<nextsent>the first sub-corpus contains over 208,000 sentence pairs and the second over 183,000 sentence pairs.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y242">
<title id=" W01-1402.xml">overcoming the customization bottleneck using example based mt </title>
<section> training msr-mt.  </section>
<citcontext>
<prevsection>
<prevsent>english/spanish transfer mappings from lf alignment 4.3 mindnet.
</prevsent>
<prevsent>the repository into which transfer mappings from lf alignment are stored is known as mindnet.
</prevsent>
</prevsection>
<citsent citstr=" P98-2180 ">
richardson et al (1998) <papid> P98-2180 </papid>describes how mindnet began as lexical knowledge base containing lf-like structures that were produced automatically from the definitions and example sentences in machine-readable dictionaries.</citsent>
<aftsection>
<nextsent>later, mindnet was generalized, becoming an architecture for class of repositories that can store and access lfs produced for variety of expository texts, including but not limited to dictionaries, encyclopedias, and technical manuals.
</nextsent>
<nextsent>for msr-mt, mindnet serves as the optimal example base, specifically designed to store and retrieve the linked source and target lf segments comprising the transfer mappings extracted during lf alignment.
</nextsent>
<nextsent>as part of daily regression testing for msr-mt, all the sentence pairs in the combined english/spanish corpus are parsed, the resulting spanning lfs are aligned, and separate mindnet for each of the two directed language pairs is built from the lf transfer mappings obtained.
</nextsent>
<nextsent>these mind nets are about 7mb each in size and take roughly 6.5 hours each to create on 550 mhz pc.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y243">
<title id=" W00-1108.xml">a text categorization based on a summarization extraction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the task of summarization is to identify informative vidence from given document, which are most relevant to its content and create shorter version of smnmary of the document from this information.
</prevsent>
<prevsent>the informative evidence associated with techniques used in summarization may also provide clues for text categorization to determine the appropriate category of the document.
</prevsent>
</prevsection>
<citsent citstr=" W97-0704 ">
several techniques for text summarization have been reported in the literature, including methods based on position (edmundson, 1969; hovy and lin, 1997; <papid> W97-0704 </papid>teufel and moens, 1997), cue phrase (mckeown and radev, 1995; mahesh, 1997), word frequency (teufel and moens, 1997), and discourse segmentation (boguraev and kennedy, 1997).</citsent>
<aftsection>
<nextsent>79 of the above approaches, both word frequency and position methods are easy to implement.
</nextsent>
<nextsent>in this research we combine these two approaches to investigate the efforts for categorization..
</nextsent>
<nextsent>in regard to the position method, hovy and lin (1997) <papid> W97-0704 </papid>considered the title is the most likely to bear topics.</nextsent>
<nextsent>they claim words in titles are positively relevant summarization.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y245">
<title id=" W02-0709.xml">interactive chinesetoenglish speech translation based on dialogue management </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, to develop the knowledge base with dependence relation of keywords and to match an input with all examples in the knowledge base are sometimes difficult.
</prevsent>
<prevsent>wakita et al (1997) proposed robust translation method which locally extracts only reliable parts, i.e., those within the semantic distance threshold and over some word length.
</prevsent>
</prevsection>
<citsent citstr=" P98-1070 ">
this technique, however, does not split input into units globally, or sometimes does not output any translation result (furuse et al 1998).<papid> P98-1070 </papid></citsent>
<aftsection>
<nextsent>in addition, the method closely lies on the semantic computation, and sometimes it is hard to compute the semantic distance for the spoken utterances.
</nextsent>
<nextsent>in summary, both mainstream mt methods and non-mainstream methods have been practiced in many experimental s2s translation systems.
</nextsent>
<nextsent>however, all methods mentioned above are unilateral and based on user own wishful thinking.
</nextsent>
<nextsent>the system is passive and blind in some extent.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y246">
<title id=" W01-1603.xml">development of a machine learn able discourse tagging tool </title>
<section> multiple level tagging scheme.  </section>
<citcontext>
<prevsection>
<prevsent>[1:the large-sized meeting room: ] 41 a: dai-kaigishitsu ga tukae masu (you can use the large meeting room.)
</prevsent>
<prevsent>[1: room for lecture: return] 42 b: {d soreja} dai-kaigishitsu de onegai shimasu (ok. please book the large meeting room.)--------------------------------------- [tbi:topic name:segment relation]figure 5: an example dialogue with the dialogue segment tags 2.3 dialogue segment.
</prevsent>
</prevsection>
<citsent citstr=" J86-3001 ">
dialogue segment of jdtag indicates boundary of discourse segment introduced in (grosz and sidner, 1986).<papid> J86-3001 </papid></citsent>
<aftsection>
<nextsent>a dialogue segment is identified based on the exchange structure explained above.
</nextsent>
<nextsent>a dialogue segment tag is first inserted before each initiating utterance.
</nextsent>
<nextsent>after that, topic break index, topic name, and segment relation are identified.
</nextsent>
<nextsent>topic break index (tbi) takes the value of1 or 2: the boundary with tbi=2 is less continuous than the one with tbi=1 with regard to the topic.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y247">
<title id=" W01-1603.xml">development of a machine learn able discourse tagging tool </title>
<section> dialogue act tagger.  </section>
<citcontext>
<prevsection>
<prevsent>this rule isadded to the current rule set and this iteration is continued until no improvement is observed.
</prevsent>
<prevsent>in the previous research, tbl showed successful performance in many annotation task, e.g.
</prevsent>
</prevsection>
<citsent citstr=" J95-4004 ">
(brill, 1995), (<papid> J95-4004 </papid>samuel et al, 1998).<papid> P98-2188 </papid></citsent>
<aftsection>
<nextsent>in our experiment, the selected features in the conditional part of the rule are words (the notation in the rule is include), sentence length (length) and previous dialogue act tag (prev).
</nextsent>
<nextsent>although each feature is not enough to use as clue in determining dialogue act, the combination of these features works well.
</nextsent>
<nextsent>we used four types of combinations, that is, include + include, include + length, include + prev and length + prev.the result of the learning process is sequence of rules.
</nextsent>
<nextsent>for example, in dialogue act tagging, acquired rules in scheduling domain are shown in figure 7.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y248">
<title id=" W01-1603.xml">development of a machine learn able discourse tagging tool </title>
<section> dialogue act tagger.  </section>
<citcontext>
<prevsection>
<prevsent>this rule isadded to the current rule set and this iteration is continued until no improvement is observed.
</prevsent>
<prevsent>in the previous research, tbl showed successful performance in many annotation task, e.g.
</prevsent>
</prevsection>
<citsent citstr=" P98-2188 ">
(brill, 1995), (<papid> J95-4004 </papid>samuel et al, 1998).<papid> P98-2188 </papid></citsent>
<aftsection>
<nextsent>in our experiment, the selected features in the conditional part of the rule are words (the notation in the rule is include), sentence length (length) and previous dialogue act tag (prev).
</nextsent>
<nextsent>although each feature is not enough to use as clue in determining dialogue act, the combination of these features works well.
</nextsent>
<nextsent>we used four types of combinations, that is, include + include, include + length, include + prev and length + prev.the result of the learning process is sequence of rules.
</nextsent>
<nextsent>for example, in dialogue act tagging, acquired rules in scheduling domain are shown in figure 7.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y249">
<title id=" W00-0605.xml">a question answering system developed as a project in a natural language processing course </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this was the first exposure to this material for all but one student, and so much of the semester was spent learning about and constructing the tools that would be needed to attack this comprehen-sive problem.
</prevsent>
<prevsent>the course was structured around the project of building question answering system following the humsent evaluation as used by the deep read system (hirschman eta\]., 1999).
</prevsent>
</prevsection>
<citsent citstr=" P99-1042 ">
the deep read reading comprehension prototype system (hirschman et al , 1999) <papid> P99-1042 </papid>achieves level of 36% of the answers correct using bag-of-words approach together with limited linguistic processing.</citsent>
<aftsection>
<nextsent>since the average number of sentences per passage is 19.41, this performance is much better than chance (i.e., 5%).
</nextsent>
<nextsent>we hypothesized that by using combina-tion of syntactic and semantic features and machine learning techniques, we could improve the accuracy of question answering on the test set of the remedia corpus over these reported levels.
</nextsent>
<nextsent>the overall architecture of our system is depicted in figure 1.
</nextsent>
<nextsent>the story sentences and its five ques-tions (who, what, where, when, and why) are first preprocessed and tagged by the brill part-of-speech * we would ike to thank the deep read group forgiving us   access to the r test bed.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y250">
<title id=" W00-0605.xml">a question answering system developed as a project in a natural language processing course </title>
<section> tagged text.  </section>
<citcontext>
<prevsection>
<prevsent>overall, there are 3,007 unique words across both training and testing.
</prevsent>
<prevsent>one of our hypotheses was that by creating lex-icon with rich set of features, we would improve the accuracy of question answering.
</prevsent>
</prevsection>
<citsent citstr=" A00-2014 ">
the entries in the lexicon were constructed using the conventions adopted for the parsec parser (harper and helzer-man, 1995; harper et al , 1995; harper et al , 2000).<papid> A00-2014 </papid></citsent>
<aftsection>
<nextsent>each word entry contains information about its root word (if there is one), its lexical category (or cate- gories) along with corresponding set of allowable features and their corresponding values.
</nextsent>
<nextsent>lexical cat-egories include noun, verb, pronoun, proper noun, adjective, adverb, preposition, particle, conjunction, determiner, cardinal, ordinal, pre determiner, noun modifier, and month.
</nextsent>
<nextsent>feature types used in the lexicon include subcat, gender, agr, case, vtype (e.g., progressive), mood, gap, inverted , voice, behavior (e.g., mass), type (e.g., interrogative, rel- ative), semtype, and con type (e.g., noun-type, verb-type, etc.).
</nextsent>
<nextsent>we hypothesized that semtype should play significant role in improving question answering performance, but the choice of semantic granularity is difficult problem.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y251">
<title id=" W01-0815.xml">evaluating text quality judging output texts without a clear source </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the answers to these questions say something important about how good the target texts are and ? perhaps more to the point ? how good the system that generated them is. there is no priori reason why the target texts should be any better or worse when they result from natural language generation (nlg) or from machine translation (mt): indeed, they could result from the same language generator.
</prevsent>
<prevsent>given this, it may be natural to assume that nlg could appropriately adopt evaluation methods developed for its more mature sister, mt. however, while this holds true for issues related to intelligibility (the second critical question), it does not apply as readily to issues of fidelity (the first question).
</prevsent>
</prevsection>
<citsent citstr=" C00-1069 ">
we go beyond our recent experience of evaluating the agile system for producing multilingual versions of software user manuals (hartley, scott et al, 2000; kruijff et al, 2000) <papid> C00-1069 </papid>and raise some open questions about how best to evaluate the faithfulness of an output text with respect to its input specification.</citsent>
<aftsection>
<nextsent>the use of rating scales to assess the intelligibility of mt output has been widespread since the early days in the field.
</nextsent>
<nextsent>typically, monolingual raters assign score to each sentence in the output text.
</nextsent>
<nextsent>however, this does not amount to an agreed methodology, since the number of points on the scale and their definition have varied considerably.
</nextsent>
<nextsent>for example, carroll (1966) used nine-point scale where point 1 was defined as hopelessly unintelligible?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y252">
<title id=" W01-0815.xml">evaluating text quality judging output texts without a clear source </title>
<section> evaluating intelligibility.  </section>
<citcontext>
<prevsection>
<prevsent>however, this does not amount to an agreed methodology, since the number of points on the scale and their definition have varied considerably.
</prevsent>
<prevsent>for example, carroll (1966) used nine-point scale where point 1 was defined as hopelessly unintelligible?
</prevsent>
</prevsection>
<citsent citstr=" J85-2001 ">
and point 9 as perfectly clear and intelligible?; nagao and colleagues (nagao et al, 1985), <papid> J85-2001 </papid>in contrast, used five-point scale, while arnold and his colleagues (arnold et al, 1994) suggest four-point discrimination.</citsent>
<aftsection>
<nextsent>in evaluating the intelligibility of the agile output, we asked professional translators and authors who were native speakers of the languages concerned bulgarian, czech and russian to score individual text fragments on four-point scale.
</nextsent>
<nextsent>the evaluators were also asked to give summative assessment of the outputs suitability as the first draft of manual.
</nextsent>
<nextsent>in single pass, agile is capable of generating several types of text, each anthony hartley and donia scott information technology research institute, university of brighton uk {firstname.lastname}@itri.bton.ac.uk constituting section of typical software user manuali.e., overview, short instructions, full instructions, and functional description sand appearing in one of two styles (personal/direct or impersonal/indirect).
</nextsent>
<nextsent>we evaluated all of these text types using the same method.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y254">
<title id=" W01-0815.xml">evaluating text quality judging output texts without a clear source </title>
<section> evaluating fidelity.  </section>
<citcontext>
<prevsection>
<prevsent>yet, whatever its origins, directly comparing this intermediate representation to the output text is problematic.
</prevsent>
<prevsent>a recent survey of complete nlg systems (cahill et al, 1999) found that half of the 18 systems examined accepted input directly from another system1.
</prevsent>
</prevsection>
<citsent citstr=" J98-3004 ">
a typical example is the caption generation system (mittal et al, 1998), <papid> J98-3004 </papid>which produces paragraph-sized captions to accompany the complex graphics generated by sage (roth et al, 1994).</citsent>
<aftsection>
<nextsent>the input to generation includes definitions of the graphical constituents that are used to by sage to convey information: spaces (e.g., charts, maps, tables), graphemes (e.g., labels, marks, bars), their properties (e.g., color, shape) and encodersthe frames of reference that enable their properties to be interpreted/translated back to data values (e.g., axes, graphical keys).2 for obvious reasons, this does not readily lend itself to direct comparison with the generated text caption.
</nextsent>
<nextsent>in the remaining half of the systems covered, the domain model is constructed by the user (usually domain expert) through technique that has come to be known as symbolic authoring: the author?
</nextsent>
<nextsent>uses specially-built knowledge editor to construct the symbolic source of the target text.
</nextsent>
<nextsent>these editors are interfaces that allow authors to build the domain model using representation that is more natural?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y256">
<title id=" W01-0815.xml">evaluating text quality judging output texts without a clear source </title>
<section> evaluating fidelity.  </section>
<citcontext>
<prevsection>
<prevsent>domain model.
</prevsent>
<prevsent>as such, they are obvious candidates as the standard against which to measure the content of the texts that are generated from them.
</prevsent>
</prevsection>
<citsent citstr=" P98-2173 ">
we first consider the case of feedback presented in graphical mode, and then the option of textual feedback, using the wysiwym technology (power and scott, 1998; <papid> P98-2173 </papid>scott, power and evans, 1998).</citsent>
<aftsection>
<nextsent>we go on to make recommendations concerning the desirable properties of the feedback text.
</nextsent>
<nextsent>symbolic authoring systems typically make use of graphical representations of the content of the domain model for example, conceptual graphs (caldwell and korelsky, 1994).<papid> A94-1001 </papid></nextsent>
<nextsent>once trained in the language of the interface, the domain specialist uses standard text-editing devices such as menu selection and navigation with cursor, together with standard text-editing actions (e.g., select, copy, paste, delete) to create and edit the content specification of the text to be generated in one or several selected languages.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y257">
<title id=" W01-0815.xml">evaluating text quality judging output texts without a clear source </title>
<section> graphical representations of content.  </section>
<citcontext>
<prevsection>
<prevsent>we first consider the case of feedback presented in graphical mode, and then the option of textual feedback, using the wysiwym technology (power and scott, 1998; <papid> P98-2173 </papid>scott, power and evans, 1998).</prevsent>
<prevsent>we go on to make recommendations concerning the desirable properties of the feedback text.</prevsent>
</prevsection>
<citsent citstr=" A94-1001 ">
symbolic authoring systems typically make use of graphical representations of the content of the domain model for example, conceptual graphs (caldwell and korelsky, 1994).<papid> A94-1001 </papid></citsent>
<aftsection>
<nextsent>once trained in the language of the interface, the domain specialist uses standard text-editing devices such as menu selection and navigation with cursor, together with standard text-editing actions (e.g., select, copy, paste, delete) to create and edit the content specification of the text to be generated in one or several selected languages.
</nextsent>
<nextsent>the user of agile, conceived to be specialist in the domain of the particular software for which the manual is required (i.e., cad/cam), models the procedures for how to use the software.
</nextsent>
<nextsent>agiles graphical user interface (hartley, power et al, 2000) closely resembles the interface that was developed for an earlier system, drafter, which generates software manuals in english and french (paris et al, 1995).
</nextsent>
<nextsent>the design of the interface represents the components of the procedures (e.g., goals, methods, preconditions, sub-steps, side-effects) as differently coloured boxes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y258">
<title id=" W01-0815.xml">evaluating text quality judging output texts without a clear source </title>
<section> forex.  </section>
<citcontext>
<prevsection>
<prevsent>we focused on (a), which was of course mediated by (c); that is, we focused on the issue of creating an accurate model.
</prevsent>
<prevsent>this is an easier issue than that of the fidelity of the output text to the model (b), while the representations in (d) are too remote from one another to permit useful comparison.
</prevsent>
</prevsection>
<citsent citstr=" C00-1007 ">
to measure the correspondence between the actual models and the desired/target models, we adopted the generation string accuracy (gsa) metric (bangalore, rambow and whittaker, 2000; bangalore and rambow, 2000) <papid> C00-1007 </papid>used in evaluating the output of nlg system.</citsent>
<aftsection>
<nextsent>it extends the simple word accuracy metric suggested in the mt literature (alshawi et al, 1998), <papid> P98-1006 </papid>based on the string edit distance between some reference text and the output of the system.</nextsent>
<nextsent>as it stands, this metric fails to account for some of the special properties of the text generation task, which involves ordering word tokens.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y259">
<title id=" W01-0815.xml">evaluating text quality judging output texts without a clear source </title>
<section> forex.  </section>
<citcontext>
<prevsection>
<prevsent>this is an easier issue than that of the fidelity of the output text to the model (b), while the representations in (d) are too remote from one another to permit useful comparison.
</prevsent>
<prevsent>to measure the correspondence between the actual models and the desired/target models, we adopted the generation string accuracy (gsa) metric (bangalore, rambow and whittaker, 2000; bangalore and rambow, 2000) <papid> C00-1007 </papid>used in evaluating the output of nlg system.</prevsent>
</prevsection>
<citsent citstr=" P98-1006 ">
it extends the simple word accuracy metric suggested in the mt literature (alshawi et al, 1998), <papid> P98-1006 </papid>based on the string edit distance between some reference text and the output of the system.</citsent>
<aftsection>
<nextsent>as it stands, this metric fails to account for some of the special properties of the text generation task, which involves ordering word tokens.
</nextsent>
<nextsent>thus, corrections may involve reordering tokens.
</nextsent>
<nextsent>in order not to penalise misplaced constituent twiceas both deletion and an insertion the generation accuracy metric treats the deletion (d) of token from one location and its insertion (i) at another location as single movement (m).
</nextsent>
<nextsent>the remaining deletions, insertions, and substitutions (s) are counted separately.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y260">
<title id=" W02-0211.xml">discourse processing for explanatory essays in tutorial applications </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>indeed, one propositions entire proof may be sub proof of the next proposition.
</prevsent>
<prevsent>moreover, subtle misconceptions such as impetus are revealed when they must be used to prove proposition.
</prevsent>
</prevsection>
<citsent citstr=" J95-4001 ">
abductive inference has long history in plan recognition, text understanding and discourse processing (appelt and pollack, 1992; charniak, 1986;hobbs et al, 1993; mcroy and hirst, 1995; <papid> J95-4001 </papid>lascarides and asher, 1991; <papid> P91-1008 </papid>rayner and alshawi, 1992).<papid> A92-1001 </papid></citsent>
<aftsection>
<nextsent>we are using an extended version of sris tacitus-lite weighted abductive inference engine (hobbs et al, 1993) as our main tool for building abductive proofs.
</nextsent>
<nextsent>we had to extend it in order to useit for domain as well as language reasoning.
</nextsent>
<nextsent>as advised in (appelt and pollack, 1992), abductive inference requires some application specific engineering to become practical technique.
</nextsent>
<nextsent>in this paper we describe how the system creates and utilizes proof-based representation of student essays.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y261">
<title id=" W02-0211.xml">discourse processing for explanatory essays in tutorial applications </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>indeed, one propositions entire proof may be sub proof of the next proposition.
</prevsent>
<prevsent>moreover, subtle misconceptions such as impetus are revealed when they must be used to prove proposition.
</prevsent>
</prevsection>
<citsent citstr=" P91-1008 ">
abductive inference has long history in plan recognition, text understanding and discourse processing (appelt and pollack, 1992; charniak, 1986;hobbs et al, 1993; mcroy and hirst, 1995; <papid> J95-4001 </papid>lascarides and asher, 1991; <papid> P91-1008 </papid>rayner and alshawi, 1992).<papid> A92-1001 </papid></citsent>
<aftsection>
<nextsent>we are using an extended version of sris tacitus-lite weighted abductive inference engine (hobbs et al, 1993) as our main tool for building abductive proofs.
</nextsent>
<nextsent>we had to extend it in order to useit for domain as well as language reasoning.
</nextsent>
<nextsent>as advised in (appelt and pollack, 1992), abductive inference requires some application specific engineering to become practical technique.
</nextsent>
<nextsent>in this paper we describe how the system creates and utilizes proof-based representation of student essays.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y262">
<title id=" W02-0211.xml">discourse processing for explanatory essays in tutorial applications </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>indeed, one propositions entire proof may be sub proof of the next proposition.
</prevsent>
<prevsent>moreover, subtle misconceptions such as impetus are revealed when they must be used to prove proposition.
</prevsent>
</prevsection>
<citsent citstr=" A92-1001 ">
abductive inference has long history in plan recognition, text understanding and discourse processing (appelt and pollack, 1992; charniak, 1986;hobbs et al, 1993; mcroy and hirst, 1995; <papid> J95-4001 </papid>lascarides and asher, 1991; <papid> P91-1008 </papid>rayner and alshawi, 1992).<papid> A92-1001 </papid></citsent>
<aftsection>
<nextsent>we are using an extended version of sris tacitus-lite weighted abductive inference engine (hobbs et al, 1993) as our main tool for building abductive proofs.
</nextsent>
<nextsent>we had to extend it in order to useit for domain as well as language reasoning.
</nextsent>
<nextsent>as advised in (appelt and pollack, 1992), abductive inference requires some application specific engineering to become practical technique.
</nextsent>
<nextsent>in this paper we describe how the system creates and utilizes proof-based representation of student essays.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y263">
<title id=" W02-0211.xml">discourse processing for explanatory essays in tutorial applications </title>
<section> overview of the why-atlas tutoring.  </section>
<citcontext>
<prevsection>
<prevsent>et al, 2001).
</prevsent>
<prevsent>the other communicative goals, disambiguating terminology and clarifying the essay, are addressed by the discourse manager as directives for the student to modify the essay.
</prevsent>
</prevsection>
<citsent citstr=" A97-1039 ">
it passes propositions and goal to the sentence-level realization module which uses templates to build the deep syntactic structures required by the realpro realizer (lavoie and rambow, 1997) <papid> A97-1039 </papid>for generating string that communicates the goal.</citsent>
<aftsection>
<nextsent>when the discourse manager is ready to end itsturn in the dialogue, it passes the accumulated natural language strings to the user interface.
</nextsent>
<nextsent>this out put may also include transitions between the goals selected for the turn.while dialogue is in progress, the discourse level understanding and tutorial strategist modules are bypassed until the essay is revised.
</nextsent>
<nextsent>once the student revises his essay, it is reanalyzed and the cycle repeats until no additional communicative goals arise from the systems analysis of the essay.
</nextsent>
<nextsent>although the overall architecture of the system is pipeline, there is feedback to earlier modules viathe history.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y264">
<title id=" W02-0211.xml">discourse processing for explanatory essays in tutorial applications </title>
<section> background on weighted abduction and.  </section>
<citcontext>
<prevsection>
<prevsent>most of the heuristics in why atlas are specific to the domain and application.sris release of tacitus-lite was subsequently extended by the first author of this paper for there search project described in (thomason et al, 1996).
</prevsent>
<prevsent>it was named tacitus-lite+ at that time.
</prevsent>
</prevsection>
<citsent citstr=" P88-1012 ">
two main extensions from that work that we are making use of are: 1) proofs falling below user defined cost threshold halt the search 2) simple variable typing system reduces the number of axioms written and the size of the search space (hobbs et al, 1988, <papid> P88-1012 </papid>pg 102).</citsent>
<aftsection>
<nextsent>unlike the earlier applications of tacitus-lite+, why-atlas uses it for both shallow qualitative physics reasoning and discourse-level language reasoning.
</nextsent>
<nextsent>to support qualitative physics reasoning weve made number of general inference engine extensions, such as improved consistency checking, detecting and avoiding reasoning loops and allowing the axiom author to express both good and bad axioms in the same axiom set.
</nextsent>
<nextsent>these recent extensions are described further in (jordan et al, 2002).
</nextsent>
<nextsent>the discourse-level understanding module uses language axioms and the tacitus-lite+ abductive inference engine to resolve pronominal and temporal anaphora and make other discourse-level language related inferences.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y266">
<title id=" W02-0211.xml">discourse processing for explanatory essays in tutorial applications </title>
<section> building an abductive proof.  </section>
<citcontext>
<prevsection>
<prevsent>(5) a. the man catches the pumpkin.
</prevsent>
<prevsent>b. this is because they had the same velocity when he threw it.
</prevsent>
</prevsection>
<citsent citstr=" J88-2006 ">
otherwise, the language axioms use information about tense and aspect and default orderings relative to these to guide inferences about temporal relationships ((kamp, 1993; dowty, 1986; partee, 1984; webber, 1988) <papid> J88-2006 </papid>inter alia).</citsent>
<aftsection>
<nextsent>embedded relationships.
</nextsent>
<nextsent>in the physics essays we are addressing, there is tendency to express multiple relations within single sentence as in (6).here the equal?
</nextsent>
<nextsent>and opposite?
</nextsent>
<nextsent>relations are embedded in temporal when?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y267">
<title id=" W00-1326.xml">one sense per collocation and genre topic variations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in fact, we demonstrate hat when two independent corpora share related genre/topic, the word sense disambiguation results would be better.
</prevsent>
<prevsent>future work on word sense disambiguation will have to take into account genre and topic as important parameters on their models.
</prevsent>
</prevsection>
<citsent citstr=" H93-1052 ">
in the early nineties two famous papers claimed that the behavior of word senses in texts adhered to two principles: one sense per discourse (gale et al, 1992) and one sense per collocation (yarowsky, 1993).<papid> H93-1052 </papid></citsent>
<aftsection>
<nextsent>these hypotheses were shown to hold for some particular corpora (totaling 380 mwords) on words with 2-way ambiguity.
</nextsent>
<nextsent>the word sense distinctions came from different sources (translations into french, homo phones, homo graphs, pseudo-words, etc.), but no dictionary or lexical resource was linked to them.
</nextsent>
<nextsent>in the case of the one sense per collocation paper, several corpora were used, but nothing is said on whether the collocations hold across corpora.
</nextsent>
<nextsent>since the papers were published, word sense disambiguation has moved to deal with fine- grained sense distinctions from widely recognized semantic lexical resources; ontologies like sensus, cyc, edr, wordnet, euro wordnet, etc. or machine-readable dictionaries like oaldc, webster s, ldoce, etc. this is due, in part, to the availability of public hand-tagged material, e.g. semcor (miller et al, 1993) <papid> H93-1061 </papid>and the dso collection (ng &amp; lee, 1996).<papid> P96-1006 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y268">
<title id=" W00-1326.xml">one sense per collocation and genre topic variations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the word sense distinctions came from different sources (translations into french, homo phones, homo graphs, pseudo-words, etc.), but no dictionary or lexical resource was linked to them.
</prevsent>
<prevsent>in the case of the one sense per collocation paper, several corpora were used, but nothing is said on whether the collocations hold across corpora.
</prevsent>
</prevsection>
<citsent citstr=" H93-1061 ">
since the papers were published, word sense disambiguation has moved to deal with fine- grained sense distinctions from widely recognized semantic lexical resources; ontologies like sensus, cyc, edr, wordnet, euro wordnet, etc. or machine-readable dictionaries like oaldc, webster s, ldoce, etc. this is due, in part, to the availability of public hand-tagged material, e.g. semcor (miller et al, 1993) <papid> H93-1061 </papid>and the dso collection (ng &amp; lee, 1996).<papid> P96-1006 </papid></citsent>
<aftsection>
<nextsent>we think that the old hypotheses should be tested under the conditions of this newly available data.
</nextsent>
<nextsent>this paper focuses on the dso collection, which was tagged with wordnet senses (miller et al 1990) and comprises sentences extracted from two different corpora: the balanced brown corpus and the wall street journal corpus.
</nextsent>
<nextsent>krovetz (1998) has shown that the one sense per discourse hypothesis does not hold for fine- grained senses in semcor and dso.
</nextsent>
<nextsent>his results have been confirmed in our own experiments.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y269">
<title id=" W00-1326.xml">one sense per collocation and genre topic variations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the word sense distinctions came from different sources (translations into french, homo phones, homo graphs, pseudo-words, etc.), but no dictionary or lexical resource was linked to them.
</prevsent>
<prevsent>in the case of the one sense per collocation paper, several corpora were used, but nothing is said on whether the collocations hold across corpora.
</prevsent>
</prevsection>
<citsent citstr=" P96-1006 ">
since the papers were published, word sense disambiguation has moved to deal with fine- grained sense distinctions from widely recognized semantic lexical resources; ontologies like sensus, cyc, edr, wordnet, euro wordnet, etc. or machine-readable dictionaries like oaldc, webster s, ldoce, etc. this is due, in part, to the availability of public hand-tagged material, e.g. semcor (miller et al, 1993) <papid> H93-1061 </papid>and the dso collection (ng &amp; lee, 1996).<papid> P96-1006 </papid></citsent>
<aftsection>
<nextsent>we think that the old hypotheses should be tested under the conditions of this newly available data.
</nextsent>
<nextsent>this paper focuses on the dso collection, which was tagged with wordnet senses (miller et al 1990) and comprises sentences extracted from two different corpora: the balanced brown corpus and the wall street journal corpus.
</nextsent>
<nextsent>krovetz (1998) has shown that the one sense per discourse hypothesis does not hold for fine- grained senses in semcor and dso.
</nextsent>
<nextsent>his results have been confirmed in our own experiments.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y270">
<title id=" W00-1326.xml">one sense per collocation and genre topic variations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>207 this study has special significance at this point of word sense disambiguation research.
</prevsent>
<prevsent>a recent study (agirre &amp; martinez, 2000) concludes that, for currently available hand- tagged data, the precision is limited to around 70% when tagging all words in running text.
</prevsent>
</prevsection>
<citsent citstr=" W99-0502 ">
in the course of extending available data, the efforts to use corpora tagged by independent teams of researchers have been shown to fail (ng et al, 1999), <papid> W99-0502 </papid>as have failed some tuning experiments (escudero et al, 2000), and an attempt to use examples automatically acquired from the internet (agirre &amp; martinez, 2000).</citsent>
<aftsection>
<nextsent>all these studies obviated the fact that the examples come from different genre and topics.
</nextsent>
<nextsent>future work that takes into account he conclusions drawn in this paper will perhaps be able to automatically extend the number of examples available and tackle the acquisition problem.
</nextsent>
<nextsent>the paper is organized as follows.
</nextsent>
<nextsent>the resources used and the experimental settings are presented first.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y272">
<title id=" W00-1326.xml">one sense per collocation and genre topic variations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>table 1).
</prevsent>
<prevsent>the examples from the brown corpus comprise 78,080 occurrences of word senses, and the examples from the wsj 114,794 occurrences.
</prevsent>
</prevsection>
<citsent citstr=" A00-1031 ">
the sentences in the dso collection were tagged with parts of speech using tnt (brants, 2000) <papid> A00-1031 </papid>trained on the brown corpus itself.</citsent>
<aftsection>
<nextsent>a. press: reportage b. press: editorial c. press: reviews (theatre, books, music, dance) d. religion e. skills and hobbies f. popular lore g. belles lettres, biography, memoirs, etc. h. miscellaneous j. learned k. general fiction l. mystery and detective fiction m. science fiction n. adventure and western fiction p. romance and love story r. humor table 1: list of categories of texts from the brown corpus, divided into informative prose (top) and imaginative prose (bottom).
</nextsent>
<nextsent>1.1 categories in the brown corpus.
</nextsent>
<nextsent>and genre/topic variation the brown corpus manual (francis &amp; kucera, 1964) does not detail the criteria followed to set the categories in table 1: the samples represent wide range of styles and varieties of prose...
</nextsent>
<nextsent>the list of main categories and their subdivisions was drawn up at conference held at brown university in february 1963.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y278">
<title id=" W00-1326.xml">one sense per collocation and genre topic variations </title>
<section> collocations considered.  </section>
<citcontext>
<prevsection>
<prevsent>for instance, governing body and governing bodies are different collocations for the sake of this paper.
</prevsent>
<prevsent>ambiguities decision lists as defined in (yarowsky, 1993; <papid> H93-1052 </papid>1994) are simple means to solve ambiguity problems.</prevsent>
</prevsection>
<citsent citstr=" P94-1013 ">
they have been successfully applied to accent restoration, word  sense disambiguation 209 and homo graph disambiguation (yarowsky, 1994; <papid> P94-1013 </papid>1995; 1996).</citsent>
<aftsection>
<nextsent>in order to build decision lists the training examples are processed to extract he features (each feature corresponds to kind of collocation), which are weighted with log-likelihood measure.
</nextsent>
<nextsent>the list of all features ordered by log-likelihood values constitutes the decision list.
</nextsent>
<nextsent>we adapted the original formula in order to accommodate ambiguities higher than two: . , pr(sense i features) , weight(sensei , feature , ) = ~ogt- ) pr(sense~ feature , ) ,i=i when testing, the decision list is checked in order and the feature with highest weight hat is present in the test sentence selects the winning word sense.
</nextsent>
<nextsent>for this work we also considered negative weights, which were not possible on two-way ambiguities.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y285">
<title id=" W01-0512.xml">the unknown word problem a morphological analysis of japanese using maximum entropy aided by a dictionary </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a morpheme is minimal grammatical unit, such as word or sux, and morphological analysis is the process segmenting given sentence intoa row of morphemes and assigning to each morpheme grammatical attributes such as part of-speech (pos) and an in ection type.
</prevsent>
<prevsent>one of the most important problems in morphological analysis is that posed by unknown words, which are words found in neither dictionary nor at raining corpus, and there have been two statistical approaches to this problem.
</prevsent>
</prevsection>
<citsent citstr=" C96-2202 ">
one is to acquire unknown words from corpora and put them into dictionary (e.g., (mori and nagao, 1996)), <papid> C96-2202 </papid>and the other is to estimate model that can identify unknown words correctly (e.g., (kashioka et al, 1997; nagata, 1999)).<papid> P99-1036 </papid></citsent>
<aftsection>
<nextsent>we would like to be able to make good use of both approaches.
</nextsent>
<nextsent>if words acquired by the former method could be added to dictionary and model developed by the latter method could consult the amended dictionary, then the model could be the best statistical model which has the potential to overcome the unknown wordproblem.
</nextsent>
<nextsent>mori and nagao proposed statistical model that can consult dictionary (moriand nagao, 1998).
</nextsent>
<nextsent>in their model the probability that string of letters or characters is morpheme is augmented when the string is found in dictionary.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y286">
<title id=" W01-0512.xml">the unknown word problem a morphological analysis of japanese using maximum entropy aided by a dictionary </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a morpheme is minimal grammatical unit, such as word or sux, and morphological analysis is the process segmenting given sentence intoa row of morphemes and assigning to each morpheme grammatical attributes such as part of-speech (pos) and an in ection type.
</prevsent>
<prevsent>one of the most important problems in morphological analysis is that posed by unknown words, which are words found in neither dictionary nor at raining corpus, and there have been two statistical approaches to this problem.
</prevsent>
</prevsection>
<citsent citstr=" P99-1036 ">
one is to acquire unknown words from corpora and put them into dictionary (e.g., (mori and nagao, 1996)), <papid> C96-2202 </papid>and the other is to estimate model that can identify unknown words correctly (e.g., (kashioka et al, 1997; nagata, 1999)).<papid> P99-1036 </papid></citsent>
<aftsection>
<nextsent>we would like to be able to make good use of both approaches.
</nextsent>
<nextsent>if words acquired by the former method could be added to dictionary and model developed by the latter method could consult the amended dictionary, then the model could be the best statistical model which has the potential to overcome the unknown wordproblem.
</nextsent>
<nextsent>mori and nagao proposed statistical model that can consult dictionary (moriand nagao, 1998).
</nextsent>
<nextsent>in their model the probability that string of letters or characters is morpheme is augmented when the string is found in dictionary.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y288">
<title id=" W01-0512.xml">the unknown word problem a morphological analysis of japanese using maximum entropy aided by a dictionary </title>
<section> a morpheme model </section>
<citcontext>
<prevsection>
<prevsent>for instance, one of our features is g(h; f) = 8       : 1 : if has(h; x) = true; = \pos( 1)(major) : verb; 00 &amp; = 1 0 : otherwise: (1)here \has(h,x)  is binary function that returns true if the history has feature x. in our experiments, we focused on such information as whether or not string is found in dictionary, the length of the string, what types of characters are used in the string, and the part-of-speech of the adjacent morpheme.
</prevsent>
<prevsent>given set of features and some training data, the m.e. estimation process produces model in which every feature ihas an associated parameter  . this enables us to compute.
</prevsent>
</prevsection>
<citsent citstr=" J96-1002 ">
the conditional probability as follows (berger et al., 1996): (<papid> J96-1002 </papid>f jh) = i  i (h;f) z  (h) (2)  (h) = f i  i (h;f) : (3) the m.e. estimation process guarantees that for every feature i , the expected value of iaccord ing to the m.e. model will equal the empirical expectation of i in the training corpus.</citsent>
<aftsection>
<nextsent>in other words, h;f ~ (h; f)
</nextsent>
<nextsent>g (h; f) = h ~ (h)
</nextsent>
<nextsent>x p m:e: (f jh)
</nextsent>
<nextsent>g (h; f): (4) here ~ is an empirical probability and m:e: is the probability assigned by the model.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y289">
<title id=" W01-0512.xml">the unknown word problem a morphological analysis of japanese using maximum entropy aided by a dictionary </title>
<section> a morpheme model </section>
<citcontext>
<prevsection>
<prevsent>(2), in which can be one of (n + 1) tags from 0 to n. given sentence is divided into morphemes, and grammatical attribute is assigned to each morpheme so as to maximize the sentence probability estimated by our morpheme model.
</prevsent>
<prevsent>sentence probability is de ned as the product of the probabilities estimated for particular division of morphemes in sentence.
</prevsent>
</prevsection>
<citsent citstr=" C94-1032 ">
we use the viterbi algorithm to nd the optimal set of morphemes in sentence and we use the method proposed by nagata (nagata, 1994) <papid> C94-1032 </papid>to search for the best sets.</citsent>
<aftsection>
<nextsent>3.1 experimental conditions.
</nextsent>
<nextsent>the part-of-speech categories that we used follow those of juman (kurohashi and nagao,1999).
</nextsent>
<nextsent>there are 53 categories covering all possible combinations of major and minor categories as de ned in juman.
</nextsent>
<nextsent>the number of grammatical attributes is 106 if we include the detection of whether or not the left side of morpheme is bunsetsu boundary.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y290">
<title id=" W01-0512.xml">the unknown word problem a morphological analysis of japanese using maximum entropy aided by a dictionary </title>
<section> experiments and discussion.  </section>
<citcontext>
<prevsection>
<prevsent>the accuracy in segmentation and major pos tagging obtained with our method and that obtained with juman were about 3% worse than that obtained with juman plus knp.
</prevsent>
<prevsent>we think the main reason for this wasan insucient amount of training data and feature sets and the inconsistency of the corpus.the number of sentences in the training corpus was only about 8,000, and we did not use as many combined features as were proposed in ref.
</prevsent>
</prevsection>
<citsent citstr=" E99-1026 ">
(uchimoto et al, 1999).<papid> E99-1026 </papid></citsent>
<aftsection>
<nextsent>we were unable touse more training data or more feature sets because every string consisting of ve or less characters in our training corpus was used to train our model, so the amount of tokenized training data would have become too large and the training would not have been completed on the available machine if we had used more training data or more feature sets.
</nextsent>
<nextsent>the inconsistency of the corpus was due to the way the corpus was made.
</nextsent>
<nextsent>the kyoto university corpus wasmade by manually correcting the output of ju man plus knp, and it is dicult to manually correct all of the inconsistencies in the output.the use of juman plus knp thus has an advantage over the use of our method when we evaluate system accuracy by using the kyoto university corpus.
</nextsent>
<nextsent>for example, the number of morphemes whose rightmost character is \?  was 153 in the test corpus, and they were all the same as those in the output of juman plus knp.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y291">
<title id=" W01-0512.xml">the unknown word problem a morphological analysis of japanese using maximum entropy aided by a dictionary </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>these results show that our model can eciently learn the characteristics of unknown words, especially those of proper nouns such as the names of people, organizations, and locations.
</prevsent>
<prevsent>several methods based on statistical model shave been proposed for the morphological analysis of japanese sentences.
</prevsent>
</prevsection>
<citsent citstr=" P97-1030 ">
an f-measure of about 96% was achieved by method based on hidden markov model (hmm) (takeuchi and matsumoto, 1997) and by one based on variable-memory markov model (haruno and matsumoto, 1997; <papid> P97-1030 </papid>kitauchi et al, 1999).</citsent>
<aftsection>
<nextsent>although the accuracy obtained with these methods was better than that obtained with ours, their accuracy cannot be compared directly with that of our method because their part of-speech categories dier from ours.
</nextsent>
<nextsent>and an advantage of our model is that it can handle unknown words, whereas their models do not handle unknown words well.
</nextsent>
<nextsent>in their models, unknown words are divided into combination of word consisting of one character and known words.
</nextsent>
<nextsent>haruno and matsumoto (haruno and matsumoto, 1997) <papid> P97-1030 </papid>achieved recall of about 96% when using trigram or greater information,but achieved recall of only 94% when using bigram information.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y297">
<title id=" W01-0512.xml">the unknown word problem a morphological analysis of japanese using maximum entropy aided by a dictionary </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>their slight improvement inaccuracy by using dictionary information resulted in an f-measure of about 0.2, while our improvement was about 1.7.
</prevsent>
<prevsent>their accuracy of 95% when using the kyoto university corpus is similar to ours, but they added to their dictionary all of the words appearing in the training corpus.
</prevsent>
</prevsection>
<citsent citstr=" A92-1018 ">
therefore, their experiment had to deal with fewer unknown words than ours did.with regard to the morphological analysis of english sentences, methods for part-of speech tagging based on an hmm (cutting et al., 1992), <papid> A92-1018 </papid>variable-memory markov model (schutze and singer, 1994), decision tree model (daelemans et al, 1996), <papid> W96-0102 </papid>an m.e. model (ratnaparkhi, 1996), <papid> W96-0213 </papid>neural network model (schmid, 1994), <papid> C94-1027 </papid>and transformation-based error-driven learning model (brill, 1995) <papid> J95-4004 </papid>have been proposed, as well as combined method (marquez and padro, 1997; van halteren et al, 1998).</citsent>
<aftsection>
<nextsent>on available machines, however, these models cannot handle large amount of lexical information.
</nextsent>
<nextsent>we think that our model, which can not only consult dictionary with large amount of lexical information, but canalso identify unknown words by learning certain characteristics, has the potential to achieve good accuracy for part-of-speech tagging in english.
</nextsent>
<nextsent>we plan to apply our model to english sentences.
</nextsent>
<nextsent>this paper described method for morphological analysis based on maximum entropy (m.e.) model.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y298">
<title id=" W01-0512.xml">the unknown word problem a morphological analysis of japanese using maximum entropy aided by a dictionary </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>their slight improvement inaccuracy by using dictionary information resulted in an f-measure of about 0.2, while our improvement was about 1.7.
</prevsent>
<prevsent>their accuracy of 95% when using the kyoto university corpus is similar to ours, but they added to their dictionary all of the words appearing in the training corpus.
</prevsent>
</prevsection>
<citsent citstr=" W96-0102 ">
therefore, their experiment had to deal with fewer unknown words than ours did.with regard to the morphological analysis of english sentences, methods for part-of speech tagging based on an hmm (cutting et al., 1992), <papid> A92-1018 </papid>variable-memory markov model (schutze and singer, 1994), decision tree model (daelemans et al, 1996), <papid> W96-0102 </papid>an m.e. model (ratnaparkhi, 1996), <papid> W96-0213 </papid>neural network model (schmid, 1994), <papid> C94-1027 </papid>and transformation-based error-driven learning model (brill, 1995) <papid> J95-4004 </papid>have been proposed, as well as combined method (marquez and padro, 1997; van halteren et al, 1998).</citsent>
<aftsection>
<nextsent>on available machines, however, these models cannot handle large amount of lexical information.
</nextsent>
<nextsent>we think that our model, which can not only consult dictionary with large amount of lexical information, but canalso identify unknown words by learning certain characteristics, has the potential to achieve good accuracy for part-of-speech tagging in english.
</nextsent>
<nextsent>we plan to apply our model to english sentences.
</nextsent>
<nextsent>this paper described method for morphological analysis based on maximum entropy (m.e.) model.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y299">
<title id=" W01-0512.xml">the unknown word problem a morphological analysis of japanese using maximum entropy aided by a dictionary </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>their slight improvement inaccuracy by using dictionary information resulted in an f-measure of about 0.2, while our improvement was about 1.7.
</prevsent>
<prevsent>their accuracy of 95% when using the kyoto university corpus is similar to ours, but they added to their dictionary all of the words appearing in the training corpus.
</prevsent>
</prevsection>
<citsent citstr=" W96-0213 ">
therefore, their experiment had to deal with fewer unknown words than ours did.with regard to the morphological analysis of english sentences, methods for part-of speech tagging based on an hmm (cutting et al., 1992), <papid> A92-1018 </papid>variable-memory markov model (schutze and singer, 1994), decision tree model (daelemans et al, 1996), <papid> W96-0102 </papid>an m.e. model (ratnaparkhi, 1996), <papid> W96-0213 </papid>neural network model (schmid, 1994), <papid> C94-1027 </papid>and transformation-based error-driven learning model (brill, 1995) <papid> J95-4004 </papid>have been proposed, as well as combined method (marquez and padro, 1997; van halteren et al, 1998).</citsent>
<aftsection>
<nextsent>on available machines, however, these models cannot handle large amount of lexical information.
</nextsent>
<nextsent>we think that our model, which can not only consult dictionary with large amount of lexical information, but canalso identify unknown words by learning certain characteristics, has the potential to achieve good accuracy for part-of-speech tagging in english.
</nextsent>
<nextsent>we plan to apply our model to english sentences.
</nextsent>
<nextsent>this paper described method for morphological analysis based on maximum entropy (m.e.) model.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y300">
<title id=" W01-0512.xml">the unknown word problem a morphological analysis of japanese using maximum entropy aided by a dictionary </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>their slight improvement inaccuracy by using dictionary information resulted in an f-measure of about 0.2, while our improvement was about 1.7.
</prevsent>
<prevsent>their accuracy of 95% when using the kyoto university corpus is similar to ours, but they added to their dictionary all of the words appearing in the training corpus.
</prevsent>
</prevsection>
<citsent citstr=" C94-1027 ">
therefore, their experiment had to deal with fewer unknown words than ours did.with regard to the morphological analysis of english sentences, methods for part-of speech tagging based on an hmm (cutting et al., 1992), <papid> A92-1018 </papid>variable-memory markov model (schutze and singer, 1994), decision tree model (daelemans et al, 1996), <papid> W96-0102 </papid>an m.e. model (ratnaparkhi, 1996), <papid> W96-0213 </papid>neural network model (schmid, 1994), <papid> C94-1027 </papid>and transformation-based error-driven learning model (brill, 1995) <papid> J95-4004 </papid>have been proposed, as well as combined method (marquez and padro, 1997; van halteren et al, 1998).</citsent>
<aftsection>
<nextsent>on available machines, however, these models cannot handle large amount of lexical information.
</nextsent>
<nextsent>we think that our model, which can not only consult dictionary with large amount of lexical information, but canalso identify unknown words by learning certain characteristics, has the potential to achieve good accuracy for part-of-speech tagging in english.
</nextsent>
<nextsent>we plan to apply our model to english sentences.
</nextsent>
<nextsent>this paper described method for morphological analysis based on maximum entropy (m.e.) model.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y301">
<title id=" W01-0512.xml">the unknown word problem a morphological analysis of japanese using maximum entropy aided by a dictionary </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>their slight improvement inaccuracy by using dictionary information resulted in an f-measure of about 0.2, while our improvement was about 1.7.
</prevsent>
<prevsent>their accuracy of 95% when using the kyoto university corpus is similar to ours, but they added to their dictionary all of the words appearing in the training corpus.
</prevsent>
</prevsection>
<citsent citstr=" J95-4004 ">
therefore, their experiment had to deal with fewer unknown words than ours did.with regard to the morphological analysis of english sentences, methods for part-of speech tagging based on an hmm (cutting et al., 1992), <papid> A92-1018 </papid>variable-memory markov model (schutze and singer, 1994), decision tree model (daelemans et al, 1996), <papid> W96-0102 </papid>an m.e. model (ratnaparkhi, 1996), <papid> W96-0213 </papid>neural network model (schmid, 1994), <papid> C94-1027 </papid>and transformation-based error-driven learning model (brill, 1995) <papid> J95-4004 </papid>have been proposed, as well as combined method (marquez and padro, 1997; van halteren et al, 1998).</citsent>
<aftsection>
<nextsent>on available machines, however, these models cannot handle large amount of lexical information.
</nextsent>
<nextsent>we think that our model, which can not only consult dictionary with large amount of lexical information, but canalso identify unknown words by learning certain characteristics, has the potential to achieve good accuracy for part-of-speech tagging in english.
</nextsent>
<nextsent>we plan to apply our model to english sentences.
</nextsent>
<nextsent>this paper described method for morphological analysis based on maximum entropy (m.e.) model.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y302">
<title id=" W02-0306.xml">a transformational based learner for dependency grammars in discharge summaries </title>
<section> dependency grammars.  </section>
<citcontext>
<prevsection>
<prevsent>figure 1 shows an example of sentence with dependency grammar parse.
</prevsent>
<prevsent>there has been interest in learning dependency grammars from corpora.
</prevsent>
</prevsection>
<citsent citstr=" P96-1025 ">
collins (collins, 1996) <papid> P96-1025 </papid>used dependencies as the backbone for his probabilistic parser and there has been work on learning both probabilistic (carroll, 1992; lee, 1999; paskin, 2001) and transformation based dependency grammars (hajic, 1997).</citsent>
<aftsection>
<nextsent>there are number of attributes of dependency grammars which make them ideal for our goal of investigating medical sublanguage.
</nextsent>
<nextsent>first, the semantics of word are often defined by feature space of related words.
</nextsent>
<nextsent>the head-dependent relationships generated by dependency parse can be used as the relationship for acquisition.
</nextsent>
<nextsent>second, dependency grammars may be better fit for parsing medical text.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y303">
<title id=" W02-0103.xml">a web based instructional platform for contraintbased grammar formalisms and parsing </title>
<section> integration of the framework.  </section>
<citcontext>
<prevsection>
<prevsent>compared to the lkb system2, which as mentioned in section 5.2 has also been used successfully in teaching grammar development, the greater range of formal expressive devices available to our parsing system, called trale, allows for more readable and compact grammars, which we believe to be of central importance in teaching context.
</prevsent>
<prevsent>to illustrate this, we are currently porting the lingo3 english resource grammar (erg) from the lkb (on which the erg was designed) to the trale system.given the scope of our web-based training framework as including an integrated module on parsing, it is also relevant that the trale system itself canbe relatively compact and transparent at the source code level since it exploits its close affinity to the underlying prolog on which it is implemented.
</prevsent>
</prevsection>
<citsent citstr=" W01-1512 ">
this contrasts with the perspective of copestake et al (2001), <papid> W01-1512 </papid>who concede that the lkb is unsuitable for teaching parsing.</citsent>
<aftsection>
<nextsent>2http://www-csli.stanford.edu/aac/lkb.html 3http://lingo.stanford.edu/csli/ 4.2 the use of hyperlinks.
</nextsent>
<nextsent>several different varieties of links are distinguished within the course material, giving first-class representation to the transfer of knowledge between the linguistic, computational and mathematical sources that inform this interdisciplinary area.
</nextsent>
<nextsent>we intend to distinguish the following kinds of links: conceptual/taxonomical: connecting instances of key concepts and terms used throughout the course material with their definitions and prove nience;empirical context: connecting instances of design decisions, algorithms and formal definitions to encyclopedic discussions of their linguistic motivation and empirical significance;denotational: connecting instances of constructional terms and issues within linguistics as well as correctness conditions of algorithms to the mathematical definitions that formalize them within the foundations of constraint-based linguistics;operational: connecting mathematical definitions and instances of related linguistic discussions to computational instructional material describing the algorithms used to construct, refute or transform the formal objects representing them in practical system;implementational: connecting discussions of algorithms to the actual annotated system source code in the trale system used to implement them, and mathematical definitions and discussions of linguistic constructions to the actual annotated grammar source code used to represent them in typical implementation.
</nextsent>
<nextsent>the idea behind this classification is that when more course material is added to the web-basedtraining framework we are proposing, the new material will take into account these distinctions to obtaina conceptually coherent use of hyper links through out the framework.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y304">
<title id=" W02-0103.xml">a web based instructional platform for contraintbased grammar formalisms and parsing </title>
<section> from seminar-style courses to.  </section>
<citcontext>
<prevsection>
<prevsent>5http://www.cs.toronto.edu/gpenn/ale.html 6http://grid.let.rug.nl/vannoord/hdrug/ 7http://www.ltg.hcrc.ed.ac.uk/projects/ledtools/ale-hpsg/ 8http://www.ltg.ed.ac.uk/projects/ledtools/ale-ra/ 9http://nl.ijs.si/et/thesis/ale-ra/ 5.2 constraint-based grammar.
</prevsent>
<prevsent>implementation over the past five years, we have held another course on constraint-based grammar implementation ina variety of settings, from summer schools to regular curriculum courses.10 it offers hands-on experience to linguists interested in the formalization of linguistic knowledge in constraint-based grammar formalism.
</prevsent>
</prevsection>
<citsent citstr=" W97-1506 ">
the course is taught in an interactive fashion in computer laboratory and combines background lectures with practical exercises on howto specify grammars in controll11 (gotz and meurers, 1997), <papid> W97-1506 </papid>processing system for constraint-based grammars intended to process with hpsg theories directly from the form in which they are constructed by linguists.</citsent>
<aftsection>
<nextsent>the background lectures of the constraint-basedgrammar implementation courses introduce the relevant mathematical and computational knowledge and focus on the main ingredients of constraint based grammars: highly structured lexical representations, constituent structures, and the encoding ofwell-formedness constraints on grammatical representations.
</nextsent>
<nextsent>in the lab, students work on exercises exploring the theoretical concepts covered in the lectures.
</nextsent>
<nextsent>in later part of the course, they are given the opportunity to undertake individualized grammar projects for modeling theoretically and empirically significant syntactic constructions of their native language.
</nextsent>
<nextsent>this course was the first hands-on computational syntax course at the european summer school in language, logic, and information (esslli, 1997: aix-en-provence), and was also offered at thelsa linguistic institute (1999: university of illinois, urbana-champaign)12 and the computational linguistics and represented knowledge (clark) summer school (1999: eberhard-karls universitat,tubingen)13.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y305">
<title id=" W01-0706.xml">exploring evidence for shallow parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this research is supported by nsf grants iis-9801638, itr-iis-0085836 and an onr muri award.
</prevsent>
<prevsent>to ] [np only $ 1.8 billion ] [pp in ] [np september] .while earlier work in this direction concentrated on manual construction of rules, most ofthe recent work has been motivated by the observation that shallow syntactic information can be extracted using local information ? by examining the pattern itself, its nearby context and the local part-of-speech information.
</prevsent>
</prevsection>
<citsent citstr=" P97-1003 ">
thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers (collins, 1997; <papid> P97-1003 </papid>charniak, 1997a; charniak, 1997b; ratnaparkhi, 1997), <papid> W97-0301 </papid>significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns ? syntactic phrases or words that participate in syntactic relationship (church, 1988; <papid> A88-1019 </papid>ramshaw and marcus, 1995; <papid> W95-0107 </papid>argamon et al., 1998; <papid> P98-1010 </papid>cardie and pierce, 1998; <papid> P98-1034 </papid>munoz et al, 1999; <papid> W99-0621 </papid>punyakanok and roth, 2001; buchholz et al., 1999; <papid> W99-0629 </papid>tjong kim sang and buchholz, 2000).</citsent>
<aftsection>
<nextsent>research on shallow parsing was inspired by psycho linguistics arguments (gee and grosjean, 1983) that suggest that in many scenarios (e.g.,conversational) full parsing is not realistic strategy for sentence processing and analysis, and was further motivated by several arguments from natural language engineering viewpoint.first, it has been noted that in many natural language applications it is sufficient to use shallow parsing information; information such as noun phrases (nps) and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization (grishman, 1995; <papid> M95-1014 </papid>appelt et al, 1993).</nextsent>
<nextsent>second, while training full parser requires collection of fully parsed sentences as training corpus, it is possible to train ash allow parser incrementally.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y307">
<title id=" W01-0706.xml">exploring evidence for shallow parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this research is supported by nsf grants iis-9801638, itr-iis-0085836 and an onr muri award.
</prevsent>
<prevsent>to ] [np only $ 1.8 billion ] [pp in ] [np september] .while earlier work in this direction concentrated on manual construction of rules, most ofthe recent work has been motivated by the observation that shallow syntactic information can be extracted using local information ? by examining the pattern itself, its nearby context and the local part-of-speech information.
</prevsent>
</prevsection>
<citsent citstr=" W97-0301 ">
thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers (collins, 1997; <papid> P97-1003 </papid>charniak, 1997a; charniak, 1997b; ratnaparkhi, 1997), <papid> W97-0301 </papid>significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns ? syntactic phrases or words that participate in syntactic relationship (church, 1988; <papid> A88-1019 </papid>ramshaw and marcus, 1995; <papid> W95-0107 </papid>argamon et al., 1998; <papid> P98-1010 </papid>cardie and pierce, 1998; <papid> P98-1034 </papid>munoz et al, 1999; <papid> W99-0621 </papid>punyakanok and roth, 2001; buchholz et al., 1999; <papid> W99-0629 </papid>tjong kim sang and buchholz, 2000).</citsent>
<aftsection>
<nextsent>research on shallow parsing was inspired by psycho linguistics arguments (gee and grosjean, 1983) that suggest that in many scenarios (e.g.,conversational) full parsing is not realistic strategy for sentence processing and analysis, and was further motivated by several arguments from natural language engineering viewpoint.first, it has been noted that in many natural language applications it is sufficient to use shallow parsing information; information such as noun phrases (nps) and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization (grishman, 1995; <papid> M95-1014 </papid>appelt et al, 1993).</nextsent>
<nextsent>second, while training full parser requires collection of fully parsed sentences as training corpus, it is possible to train ash allow parser incrementally.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y308">
<title id=" W01-0706.xml">exploring evidence for shallow parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this research is supported by nsf grants iis-9801638, itr-iis-0085836 and an onr muri award.
</prevsent>
<prevsent>to ] [np only $ 1.8 billion ] [pp in ] [np september] .while earlier work in this direction concentrated on manual construction of rules, most ofthe recent work has been motivated by the observation that shallow syntactic information can be extracted using local information ? by examining the pattern itself, its nearby context and the local part-of-speech information.
</prevsent>
</prevsection>
<citsent citstr=" A88-1019 ">
thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers (collins, 1997; <papid> P97-1003 </papid>charniak, 1997a; charniak, 1997b; ratnaparkhi, 1997), <papid> W97-0301 </papid>significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns ? syntactic phrases or words that participate in syntactic relationship (church, 1988; <papid> A88-1019 </papid>ramshaw and marcus, 1995; <papid> W95-0107 </papid>argamon et al., 1998; <papid> P98-1010 </papid>cardie and pierce, 1998; <papid> P98-1034 </papid>munoz et al, 1999; <papid> W99-0621 </papid>punyakanok and roth, 2001; buchholz et al., 1999; <papid> W99-0629 </papid>tjong kim sang and buchholz, 2000).</citsent>
<aftsection>
<nextsent>research on shallow parsing was inspired by psycho linguistics arguments (gee and grosjean, 1983) that suggest that in many scenarios (e.g.,conversational) full parsing is not realistic strategy for sentence processing and analysis, and was further motivated by several arguments from natural language engineering viewpoint.first, it has been noted that in many natural language applications it is sufficient to use shallow parsing information; information such as noun phrases (nps) and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization (grishman, 1995; <papid> M95-1014 </papid>appelt et al, 1993).</nextsent>
<nextsent>second, while training full parser requires collection of fully parsed sentences as training corpus, it is possible to train ash allow parser incrementally.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y309">
<title id=" W01-0706.xml">exploring evidence for shallow parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this research is supported by nsf grants iis-9801638, itr-iis-0085836 and an onr muri award.
</prevsent>
<prevsent>to ] [np only $ 1.8 billion ] [pp in ] [np september] .while earlier work in this direction concentrated on manual construction of rules, most ofthe recent work has been motivated by the observation that shallow syntactic information can be extracted using local information ? by examining the pattern itself, its nearby context and the local part-of-speech information.
</prevsent>
</prevsection>
<citsent citstr=" W95-0107 ">
thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers (collins, 1997; <papid> P97-1003 </papid>charniak, 1997a; charniak, 1997b; ratnaparkhi, 1997), <papid> W97-0301 </papid>significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns ? syntactic phrases or words that participate in syntactic relationship (church, 1988; <papid> A88-1019 </papid>ramshaw and marcus, 1995; <papid> W95-0107 </papid>argamon et al., 1998; <papid> P98-1010 </papid>cardie and pierce, 1998; <papid> P98-1034 </papid>munoz et al, 1999; <papid> W99-0621 </papid>punyakanok and roth, 2001; buchholz et al., 1999; <papid> W99-0629 </papid>tjong kim sang and buchholz, 2000).</citsent>
<aftsection>
<nextsent>research on shallow parsing was inspired by psycho linguistics arguments (gee and grosjean, 1983) that suggest that in many scenarios (e.g.,conversational) full parsing is not realistic strategy for sentence processing and analysis, and was further motivated by several arguments from natural language engineering viewpoint.first, it has been noted that in many natural language applications it is sufficient to use shallow parsing information; information such as noun phrases (nps) and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization (grishman, 1995; <papid> M95-1014 </papid>appelt et al, 1993).</nextsent>
<nextsent>second, while training full parser requires collection of fully parsed sentences as training corpus, it is possible to train ash allow parser incrementally.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y310">
<title id=" W01-0706.xml">exploring evidence for shallow parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this research is supported by nsf grants iis-9801638, itr-iis-0085836 and an onr muri award.
</prevsent>
<prevsent>to ] [np only $ 1.8 billion ] [pp in ] [np september] .while earlier work in this direction concentrated on manual construction of rules, most ofthe recent work has been motivated by the observation that shallow syntactic information can be extracted using local information ? by examining the pattern itself, its nearby context and the local part-of-speech information.
</prevsent>
</prevsection>
<citsent citstr=" P98-1010 ">
thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers (collins, 1997; <papid> P97-1003 </papid>charniak, 1997a; charniak, 1997b; ratnaparkhi, 1997), <papid> W97-0301 </papid>significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns ? syntactic phrases or words that participate in syntactic relationship (church, 1988; <papid> A88-1019 </papid>ramshaw and marcus, 1995; <papid> W95-0107 </papid>argamon et al., 1998; <papid> P98-1010 </papid>cardie and pierce, 1998; <papid> P98-1034 </papid>munoz et al, 1999; <papid> W99-0621 </papid>punyakanok and roth, 2001; buchholz et al., 1999; <papid> W99-0629 </papid>tjong kim sang and buchholz, 2000).</citsent>
<aftsection>
<nextsent>research on shallow parsing was inspired by psycho linguistics arguments (gee and grosjean, 1983) that suggest that in many scenarios (e.g.,conversational) full parsing is not realistic strategy for sentence processing and analysis, and was further motivated by several arguments from natural language engineering viewpoint.first, it has been noted that in many natural language applications it is sufficient to use shallow parsing information; information such as noun phrases (nps) and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization (grishman, 1995; <papid> M95-1014 </papid>appelt et al, 1993).</nextsent>
<nextsent>second, while training full parser requires collection of fully parsed sentences as training corpus, it is possible to train ash allow parser incrementally.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y311">
<title id=" W01-0706.xml">exploring evidence for shallow parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this research is supported by nsf grants iis-9801638, itr-iis-0085836 and an onr muri award.
</prevsent>
<prevsent>to ] [np only $ 1.8 billion ] [pp in ] [np september] .while earlier work in this direction concentrated on manual construction of rules, most ofthe recent work has been motivated by the observation that shallow syntactic information can be extracted using local information ? by examining the pattern itself, its nearby context and the local part-of-speech information.
</prevsent>
</prevsection>
<citsent citstr=" P98-1034 ">
thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers (collins, 1997; <papid> P97-1003 </papid>charniak, 1997a; charniak, 1997b; ratnaparkhi, 1997), <papid> W97-0301 </papid>significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns ? syntactic phrases or words that participate in syntactic relationship (church, 1988; <papid> A88-1019 </papid>ramshaw and marcus, 1995; <papid> W95-0107 </papid>argamon et al., 1998; <papid> P98-1010 </papid>cardie and pierce, 1998; <papid> P98-1034 </papid>munoz et al, 1999; <papid> W99-0621 </papid>punyakanok and roth, 2001; buchholz et al., 1999; <papid> W99-0629 </papid>tjong kim sang and buchholz, 2000).</citsent>
<aftsection>
<nextsent>research on shallow parsing was inspired by psycho linguistics arguments (gee and grosjean, 1983) that suggest that in many scenarios (e.g.,conversational) full parsing is not realistic strategy for sentence processing and analysis, and was further motivated by several arguments from natural language engineering viewpoint.first, it has been noted that in many natural language applications it is sufficient to use shallow parsing information; information such as noun phrases (nps) and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization (grishman, 1995; <papid> M95-1014 </papid>appelt et al, 1993).</nextsent>
<nextsent>second, while training full parser requires collection of fully parsed sentences as training corpus, it is possible to train ash allow parser incrementally.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y312">
<title id=" W01-0706.xml">exploring evidence for shallow parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this research is supported by nsf grants iis-9801638, itr-iis-0085836 and an onr muri award.
</prevsent>
<prevsent>to ] [np only $ 1.8 billion ] [pp in ] [np september] .while earlier work in this direction concentrated on manual construction of rules, most ofthe recent work has been motivated by the observation that shallow syntactic information can be extracted using local information ? by examining the pattern itself, its nearby context and the local part-of-speech information.
</prevsent>
</prevsection>
<citsent citstr=" W99-0621 ">
thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers (collins, 1997; <papid> P97-1003 </papid>charniak, 1997a; charniak, 1997b; ratnaparkhi, 1997), <papid> W97-0301 </papid>significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns ? syntactic phrases or words that participate in syntactic relationship (church, 1988; <papid> A88-1019 </papid>ramshaw and marcus, 1995; <papid> W95-0107 </papid>argamon et al., 1998; <papid> P98-1010 </papid>cardie and pierce, 1998; <papid> P98-1034 </papid>munoz et al, 1999; <papid> W99-0621 </papid>punyakanok and roth, 2001; buchholz et al., 1999; <papid> W99-0629 </papid>tjong kim sang and buchholz, 2000).</citsent>
<aftsection>
<nextsent>research on shallow parsing was inspired by psycho linguistics arguments (gee and grosjean, 1983) that suggest that in many scenarios (e.g.,conversational) full parsing is not realistic strategy for sentence processing and analysis, and was further motivated by several arguments from natural language engineering viewpoint.first, it has been noted that in many natural language applications it is sufficient to use shallow parsing information; information such as noun phrases (nps) and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization (grishman, 1995; <papid> M95-1014 </papid>appelt et al, 1993).</nextsent>
<nextsent>second, while training full parser requires collection of fully parsed sentences as training corpus, it is possible to train ash allow parser incrementally.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y313">
<title id=" W01-0706.xml">exploring evidence for shallow parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this research is supported by nsf grants iis-9801638, itr-iis-0085836 and an onr muri award.
</prevsent>
<prevsent>to ] [np only $ 1.8 billion ] [pp in ] [np september] .while earlier work in this direction concentrated on manual construction of rules, most ofthe recent work has been motivated by the observation that shallow syntactic information can be extracted using local information ? by examining the pattern itself, its nearby context and the local part-of-speech information.
</prevsent>
</prevsection>
<citsent citstr=" W99-0629 ">
thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers (collins, 1997; <papid> P97-1003 </papid>charniak, 1997a; charniak, 1997b; ratnaparkhi, 1997), <papid> W97-0301 </papid>significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns ? syntactic phrases or words that participate in syntactic relationship (church, 1988; <papid> A88-1019 </papid>ramshaw and marcus, 1995; <papid> W95-0107 </papid>argamon et al., 1998; <papid> P98-1010 </papid>cardie and pierce, 1998; <papid> P98-1034 </papid>munoz et al, 1999; <papid> W99-0621 </papid>punyakanok and roth, 2001; buchholz et al., 1999; <papid> W99-0629 </papid>tjong kim sang and buchholz, 2000).</citsent>
<aftsection>
<nextsent>research on shallow parsing was inspired by psycho linguistics arguments (gee and grosjean, 1983) that suggest that in many scenarios (e.g.,conversational) full parsing is not realistic strategy for sentence processing and analysis, and was further motivated by several arguments from natural language engineering viewpoint.first, it has been noted that in many natural language applications it is sufficient to use shallow parsing information; information such as noun phrases (nps) and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization (grishman, 1995; <papid> M95-1014 </papid>appelt et al, 1993).</nextsent>
<nextsent>second, while training full parser requires collection of fully parsed sentences as training corpus, it is possible to train ash allow parser incrementally.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y314">
<title id=" W01-0706.xml">exploring evidence for shallow parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>to ] [np only $ 1.8 billion ] [pp in ] [np september] .while earlier work in this direction concentrated on manual construction of rules, most ofthe recent work has been motivated by the observation that shallow syntactic information can be extracted using local information ? by examining the pattern itself, its nearby context and the local part-of-speech information.
</prevsent>
<prevsent>thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers (collins, 1997; <papid> P97-1003 </papid>charniak, 1997a; charniak, 1997b; ratnaparkhi, 1997), <papid> W97-0301 </papid>significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns ? syntactic phrases or words that participate in syntactic relationship (church, 1988; <papid> A88-1019 </papid>ramshaw and marcus, 1995; <papid> W95-0107 </papid>argamon et al., 1998; <papid> P98-1010 </papid>cardie and pierce, 1998; <papid> P98-1034 </papid>munoz et al, 1999; <papid> W99-0621 </papid>punyakanok and roth, 2001; buchholz et al., 1999; <papid> W99-0629 </papid>tjong kim sang and buchholz, 2000).</prevsent>
</prevsection>
<citsent citstr=" M95-1014 ">
research on shallow parsing was inspired by psycho linguistics arguments (gee and grosjean, 1983) that suggest that in many scenarios (e.g.,conversational) full parsing is not realistic strategy for sentence processing and analysis, and was further motivated by several arguments from natural language engineering viewpoint.first, it has been noted that in many natural language applications it is sufficient to use shallow parsing information; information such as noun phrases (nps) and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization (grishman, 1995; <papid> M95-1014 </papid>appelt et al, 1993).</citsent>
<aftsection>
<nextsent>second, while training full parser requires collection of fully parsed sentences as training corpus, it is possible to train ash allow parser incrementally.
</nextsent>
<nextsent>if all that is available is collection of sentences annotated fornps, it can be used to produce this level of analysis.
</nextsent>
<nextsent>this can be augmented later if more information is available.
</nextsent>
<nextsent>finally, the hope behind this research direction was that this incremental and modular processing might result in more robust parsing decisions, especially in cases of spoken language or other cases in which the quality of the natural language inputs is low ? sentences which may have repeated words, missing words, or any other lexical and syntactic mistakes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y318">
<title id=" W01-0706.xml">exploring evidence for shallow parsing </title>
<section> experimental design.  </section>
<citcontext>
<prevsection>
<prevsent>2.1 parsers.
</prevsent>
<prevsent>we perform our comparison using two state-of the-art parsers.
</prevsent>
</prevsection>
<citsent citstr=" P96-1025 ">
for the full parser, we use the one developed by michael collins (collins, 1996; <papid> P96-1025 </papid>collins, 1997) ? <papid> P97-1003 </papid>one of the most accurate full parsers around.</citsent>
<aftsection>
<nextsent>it represents full parse tree as set of basic phrases and set of dependency relationships between them.
</nextsent>
<nextsent>statistical learning techniques are used to compute the probabilities of these phrases and of candidate dependency relations occurring in that sentence.
</nextsent>
<nextsent>after that, itwill choose the candidate parse tree with the highest probability as output.
</nextsent>
<nextsent>the experiments usethe version that was trained (by collins) on sections 02-21 of the penn treebank.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y323">
<title id=" W01-0706.xml">exploring evidence for shallow parsing </title>
<section> experimental design.  </section>
<citcontext>
<prevsection>
<prevsent>see (tjong kim sang and buchholz, 2000) for details.
</prevsent>
<prevsent>parsers precision( ) recall(  )  (  )  km00 93.45 93.51 93.48  hal00 93.13 93.51 93.32  cscl * 93.41 92.64 93.02  tks00 94.04 91.00 92.50  zst00 91.99 92.25 92.12  dej00 91.87 91.31 92.09  koe00 92.08 91.86 91.97  osb00 91.65 92.23 91.94  vb00 91.05 92.03 91.54  pmp00 90.63 89.65 90.14  joh00 86.24 88.25 87.23  vd00 88.82 82.91 85.76 baseline 72.58 82.14 77.07 2.2 data.
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
training was done on the penn treebank (mar cus et al, 1993) <papid> J93-2004 </papid>wall street journal data, sections 02-21.</citsent>
<aftsection>
<nextsent>to train the cscl shallow parser we had first to convert the wsj data to flat format that directly provides the phrase annotations.
</nextsent>
<nextsent>this is done using the chunklink?
</nextsent>
<nextsent>program provided for conll-2000 (tjong kim sang and buchholz, 2000).
</nextsent>
<nextsent>testing was done on two types of data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y324">
<title id=" W01-1414.xml">adding domain specificity to an mt system </title>
<section> domain specificity.  </section>
<citcontext>
<prevsection>
<prevsent>these files, of size 30,000 and 2600 respectively, added to the quality of the alignments and to overall translation quality.
</prevsent>
<prevsent>2.1 word-association list.
</prevsent>
</prevsection>
<citsent citstr=" W01-1411 ">
moore (2001) <papid> W01-1411 </papid>describes method for learning translation relationship between words from bilingual corpora.</citsent>
<aftsection>
<nextsent>the five step process is restated here: 1.
</nextsent>
<nextsent>extract word lemmas from the logical.
</nextsent>
<nextsent>form created by parsing the raw training data.
</nextsent>
<nextsent>2.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y327">
<title id=" W02-0706.xml">architectures for speechtospeech translation using finite state models </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this integrated architecture?
</prevsent>
<prevsent>can be compared with the more conventional serial architecture?, where the hmms, along with suitable source language model, are used as front-end to recognize sequence of source-language words which is then processed by the translation model.
</prevsent>
</prevsection>
<citsent citstr=" W00-0508 ">
a related approach has been proposed in (bangalore and ricardi, 2000; <papid> W00-0508 </papid>bangalore and ricardi, 2001).<papid> N01-1018 </papid></citsent>
<aftsection>
<nextsent>in any case, pure pattern-recognition approach can be followed to build the required systems.acoustic models can be trained from sufficiently large source-language speech training set, in the very same way as in speech recognition.on the other hand, using adequate learning algorithms (casacuberta, 2000; vilar, 2000), the translation model can also be learned from sufficiently large training set consisting of source-target parallel text.in this paper, we comment the results obtained using this approach in eutrans, five-year joint effort of four european institutions, partially funded by the european union.
</nextsent>
<nextsent>association for computational linguistics.
</nextsent>
<nextsent>algorithms and systems, philadelphia, july 2002, pp.
</nextsent>
<nextsent>39-44.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y328">
<title id=" W02-0706.xml">architectures for speechtospeech translation using finite state models </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this integrated architecture?
</prevsent>
<prevsent>can be compared with the more conventional serial architecture?, where the hmms, along with suitable source language model, are used as front-end to recognize sequence of source-language words which is then processed by the translation model.
</prevsent>
</prevsection>
<citsent citstr=" N01-1018 ">
a related approach has been proposed in (bangalore and ricardi, 2000; <papid> W00-0508 </papid>bangalore and ricardi, 2001).<papid> N01-1018 </papid></citsent>
<aftsection>
<nextsent>in any case, pure pattern-recognition approach can be followed to build the required systems.acoustic models can be trained from sufficiently large source-language speech training set, in the very same way as in speech recognition.on the other hand, using adequate learning algorithms (casacuberta, 2000; vilar, 2000), the translation model can also be learned from sufficiently large training set consisting of source-target parallel text.in this paper, we comment the results obtained using this approach in eutrans, five-year joint effort of four european institutions, partially funded by the european union.
</nextsent>
<nextsent>association for computational linguistics.
</nextsent>
<nextsent>algorithms and systems, philadelphia, july 2002, pp.
</nextsent>
<nextsent>39-44.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y329">
<title id=" W01-1413.xml">using the web as a bilingual dictionary </title>
<section> discussion and related works.  </section>
<citcontext>
<prevsection>
<prevsent>we think this is useful toolfor human translators, and it could provide useful resource for statistical machine translation and cross language information retrieval.
</prevsent>
<prevsent>previous studies on bilingual text mainly focused on either parallel texts, non-parallel texts, or comparable texts, in which pair of texts are written in two different languages (veronis, 2000).
</prevsent>
</prevsection>
<citsent citstr=" P99-1068 ">
how ever, except for governmental documents from canada (english/french) and hongkong (chi nese/english), bilingual texts are usually subject to such limitations as licensing conditions, usage fees, domains, language pairs, etc. one approach that partially overcomes these limitations is to collect parallel texts from the web (nie et al, 1999; resnik, 1999).<papid> P99-1068 </papid>to provide better coverage with fewer restrictions, we focused on partially bilingual text.</citsent>
<aftsection>
<nextsent>considering the enormous volume of such texts and the variety of fields covered, we believe they arethe best resource to mine for mt-related applications that involve english and asian languages.the current system for extracting the translation of given term is more similar to the information extraction system for term descriptions(fujii and ishikawa, 2000) <papid> P00-1062 </papid>than any other machine translation systems.</nextsent>
<nextsent>in order to collect descriptions for technical term x, such as data min ing?, (fujii and ishikawa, 2000) <papid> P00-1062 </papid>collected phrases like is y?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y330">
<title id=" W01-1413.xml">using the web as a bilingual dictionary </title>
<section> discussion and related works.  </section>
<citcontext>
<prevsection>
<prevsent>previous studies on bilingual text mainly focused on either parallel texts, non-parallel texts, or comparable texts, in which pair of texts are written in two different languages (veronis, 2000).
</prevsent>
<prevsent>how ever, except for governmental documents from canada (english/french) and hongkong (chi nese/english), bilingual texts are usually subject to such limitations as licensing conditions, usage fees, domains, language pairs, etc. one approach that partially overcomes these limitations is to collect parallel texts from the web (nie et al, 1999; resnik, 1999).<papid> P99-1068 </papid>to provide better coverage with fewer restrictions, we focused on partially bilingual text.</prevsent>
</prevsection>
<citsent citstr=" P00-1062 ">
considering the enormous volume of such texts and the variety of fields covered, we believe they arethe best resource to mine for mt-related applications that involve english and asian languages.the current system for extracting the translation of given term is more similar to the information extraction system for term descriptions(fujii and ishikawa, 2000) <papid> P00-1062 </papid>than any other machine translation systems.</citsent>
<aftsection>
<nextsent>in order to collect descriptions for technical term x, such as data min ing?, (fujii and ishikawa, 2000) <papid> P00-1062 </papid>collected phrases like is y?</nextsent>
<nextsent>and is defined as y?, from the web.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y334">
<title id=" W01-1413.xml">using the web as a bilingual dictionary </title>
<section> conclusion.  </section>
<citcontext>
<prevsection>
<prevsent>since the search engine is an uncontrollable factor, one possible solution is to make your own search engine.
</prevsent>
<prevsent>we are very interested in combining such ideas as focused crawling (chakrabarti et al, 1999) and domain-specific internet portals (mccallum et al, 2000) with the proposed term translation extractor to develop domain-specific on-line dictionary service.
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
we investigated the possibility of using the webas bilingual dictionary, and reported the preliminary results of an experiment on extracting the english translations of given japanese technical terms from the web.one interesting approach to extending the current system is to introduce statistical translation model (brown et al, 1993) <papid> J93-2003 </papid>to filter out irrelevant translation candidates and to extract the most appropriate subpart from long english sequence as the translation by locally aligning the japanese and english sequences.</citsent>
<aftsection>
<nextsent>unlike ordinary machine translation which generates english sentences from japanese sentences, this is recognition-type application which identifies whether or not japanese term and an english term are translations of each other.considering the fact that what the statistical translation model provides is the joint probability of japanese and english phrases, this could be amore natural and prospective application of statistical translation model than sentence-to-sentence translation.
</nextsent>



</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y335">
<title id=" W00-1310.xml">non local language modeling based on context cooccurrence vectors </title>
<section> 1  </section>
<citcontext>
<prevsection>
<prevsent>in the reduction of matrix, angles of two row-vectors in the original matrlx should be maintained in the reduced matrlx.
</prevsent>
<prevsent>figure 2: ~vord-word co-occurrence matrix.
</prevsent>
</prevsection>
<citsent citstr=" X96-1031 ">
as such matrix reduction, we utilized learning method developed by hnc software (ilgen and rushall, 1996).<papid> X96-1031 </papid></citsent>
<aftsection>
<nextsent>1 1.
</nextsent>
<nextsent>not the word-docmnent co-occurrence.
</nextsent>
<nextsent>matrix is constructed from tile learning corpus, but word-word co-occurrence matrix.
</nextsent>
<nextsent>in this matrix: the rows and colunms correspond to words and the i- th diagonal element denotes the number of documents in which the word wl ap-pears, f(wi).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y336">
<title id=" W00-1310.xml">non local language modeling based on context cooccurrence vectors </title>
<section> re la ted  work.  </section>
<citcontext>
<prevsection>
<prevsent>trigger models (lau et al, 1993), even more general, try to capture the co-occurrences be-tween words.
</prevsent>
<prevsent>while the basic idea of our model is similar to trigger models, they handle co-occurrences of word pairs independently and do not use representation of the whole context.
</prevsent>
</prevsection>
<citsent citstr=" P94-1013 ">
this omission is also done in ap-plications such as word sense dismnbiguation (yarowsky: 1994; <papid> P94-1013 </papid>fung et al, 1999).<papid> P99-1043 </papid></citsent>
<aftsection>
<nextsent>our model is the most related to coccaro mad jurafsky (1998), in that reduced vec-tor space approach was taken and context is represented by the accumulation of word co-occurrence vectors.
</nextsent>
<nextsent>their model was reported to decrease the test set perplexity by 12%, compared to the bigram nmdel.
</nextsent>
<nextsent>the major differences are: 1.
</nextsent>
<nextsent>svd (singular value decomposition).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y337">
<title id=" W00-1310.xml">non local language modeling based on context cooccurrence vectors </title>
<section> re la ted  work.  </section>
<citcontext>
<prevsection>
<prevsent>trigger models (lau et al, 1993), even more general, try to capture the co-occurrences be-tween words.
</prevsent>
<prevsent>while the basic idea of our model is similar to trigger models, they handle co-occurrences of word pairs independently and do not use representation of the whole context.
</prevsent>
</prevsection>
<citsent citstr=" P99-1043 ">
this omission is also done in ap-plications such as word sense dismnbiguation (yarowsky: 1994; <papid> P94-1013 </papid>fung et al, 1999).<papid> P99-1043 </papid></citsent>
<aftsection>
<nextsent>our model is the most related to coccaro mad jurafsky (1998), in that reduced vec-tor space approach was taken and context is represented by the accumulation of word co-occurrence vectors.
</nextsent>
<nextsent>their model was reported to decrease the test set perplexity by 12%, compared to the bigram nmdel.
</nextsent>
<nextsent>the major differences are: 1.
</nextsent>
<nextsent>svd (singular value decomposition).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y338">
<title id=" W00-1310.xml">non local language modeling based on context cooccurrence vectors </title>
<section> re la ted  work.  </section>
<citcontext>
<prevsection>
<prevsent>we judged the distinction of con-tent words and function words is good enough for that purpose, and developed their trigram- based distinction as shown in figure 4.
</prevsent>
<prevsent>several topic-based models have been pro-posed based on the observation that certain words tend to have different probability dis-tributions in different topics.
</prevsent>
</prevsection>
<citsent citstr=" P99-1022 ">
for example, florian and yarowsky (1999) <papid> P99-1022 </papid>proposed the fol-lowing model: (9) where denotes topic id. topics are obtained by hierarchical clustering from training corpus, and topic-specific language model, pt, is learned from the clustered ocu- ments.</citsent>
<aftsection>
<nextsent>reductions in perplexity relative to bigrmn model were 10.5% for the entire text and 33.5% for the target vocabulary.
</nextsent>
<nextsent>topic-based models capture long-distance lexical dependencies via intermediate topics.
</nextsent>
<nextsent>in other words, the estimated istribution of topics, p(t\]w~), is the representation a con-text.
</nextsent>
<nextsent>our model does not use such interme-diate topics, but accesses word cg-occurrence information directly aald represents context as the accumulation of this information.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y339">
<title id=" W02-0508.xml">a morphologicalsyntacticand semantic search engine for hebrew texts </title>
<section> having perfected the.  </section>
<citcontext>
<prevsection>
<prevsent>analysis by means of computer program; for instance, wintner and ornan, 1995.
</prevsent>
<prevsent>herz and rimon, 1992, also deal mainly with syntactic problems.
</prevsent>
</prevsection>
<citsent citstr=" J95-3004 ">
levinger et al , 1995 <papid> J95-3004 </papid>demonstrate methods of eliminating syntactically incorrect morphological readings.</citsent>
<aftsection>
<nextsent>see, too, levinger, 1992.
</nextsent>
<nextsent>first, the program looks for verb.
</nextsent>
<nextsent>when verb is identified, the program checks possible nouns that can be the syntactic subject.
</nextsent>
<nextsent>it then checks other nps and pps, possible adjectives and adverbs.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y340">
<title id=" W02-0702.xml">topic detection based on dialogue history </title>
<section> abstract </section>
<citcontext>
<prevsection>


</prevsection>
<citsent citstr=" W02-0712 ">
in this paper, we propose topic detection method using dialogue history for selecting scene in the automatic interpretation system (ikeda et al, 2002).<papid> W02-0712 </papid></citsent>
<aftsection>
<nextsent>the method uses k-nearest neighbor method for the algorithm, automatically clusters target topics into smaller topics grouped by similarity, and incorporates dialogue history weighted in terms of time to detect and track topics on spoken phrases.
</nextsent>
<nextsent>from the evaluation of detection performance using test corpus comprised of realistic spoken dialogue, the method has shown to perform better with clustering incorporated, and combined with time-weighted dialogue history of three sentences, gives detection accuracy of 77.0%.
</nextsent>
<nextsent>in recent years, speech-to-speech translation systems have been developed that integrate three components: speech recognition, machine translation, and speech synthesis (watanabe et al., 2000).
</nextsent>
<nextsent>however, these systems cannot guarantee accurate translation because the individual components do not always provide correct results.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y344">
<title id=" W00-1402.xml">a task based framework to evaluate evaluative arguments </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>figure 1 sample by decomposing abstract communicative actions into primitive ones and by imposing appropriate ordering constraints among communicative actions.
</prevsent>
<prevsent>two knowledge sources are involved in this process: - complex model of the user preferences based on multi attribute utilility theory (maut)(clemen 1996).
</prevsent>
</prevsection>
<citsent citstr=" W00-1407 ">
- set of plan operators, implementing guidelines for content selection and organ(sat(on from argumentation theory (carenini and moore 2000).<papid> W00-1407 </papid></citsent>
<aftsection>
<nextsent>by using these two knowledge sources, the discourse planner produces text plan for an argument whose content and organization are tailored to the user according to argumentation theory.
</nextsent>
<nextsent>next, the text plan is passed to the microptanner which performs aggregation, pronominal ization and makes decisions about cue phrases.
</nextsent>
<nextsent>aggregation is performed according to heuristics similar to the ones proposed in (shaw 1998).<papid> W98-1415 </papid></nextsent>
<nextsent>for pronominal ization, simple rules based on centering are applied (grosz.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y345">
<title id=" W00-1402.xml">a task based framework to evaluate evaluative arguments </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>by using these two knowledge sources, the discourse planner produces text plan for an argument whose content and organization are tailored to the user according to argumentation theory.
</prevsent>
<prevsent>next, the text plan is passed to the microptanner which performs aggregation, pronominal ization and makes decisions about cue phrases.
</prevsent>
</prevsection>
<citsent citstr=" W98-1415 ">
aggregation is performed according to heuristics similar to the ones proposed in (shaw 1998).<papid> W98-1415 </papid></citsent>
<aftsection>
<nextsent>for pronominal ization, simple rules based on centering are applied (grosz.
</nextsent>
<nextsent>josh( et al 1995).
</nextsent>
<nextsent>arguments in order of decreasing expected effectiveness for the target user sub j-26 finally, decisions about cue phrases are made according to decision tree based on suggestions from (knott 1996; di eugenio, moore et el.
</nextsent>
<nextsent>1997) . the sentence realizer extends previous work on realizing evaluative statements (elhadad 1995).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y346">
<title id=" W01-1503.xml">the telri tool catalogue structure and prospects </title>
<section> catalogue contents.  </section>
<citcontext>
<prevsection>
<prevsent>figure 2 contains sample html output of one item in the catalogue.
</prevsent>
<prevsent>in summary, figure 3 gives graphical overview of the data processing of the telri catalogue items.
</prevsent>
</prevsection>
<citsent citstr=" A00-1031 ">
the catalogue currently contains only few sample entries, which, nevertheless, exemplify the kinds of software that are to be most relevant for inclusion into the catalogue: tools that at least one telri partner has experience in using and that the partner is willing to support for new users  tools that are available free of cost, at least for academic purposes and, preferably, are open source  tools that are language independent or adapt easily to new languages tools that are primarily meant for corpus processing at present, the catalogue lists the following tools:  the morpho-syntactic tagger tnt (brants, 2000)<papid> A00-1031 </papid>a robust and very efficient statistical partof-speech tagger that is trainable on different languages and on virtually any tagset.</citsent>
<aftsection>
<nextsent>it is available by license agreement which is free of charge for non-commercial purposes.
</nextsent>
<nextsent>distribution is available, in bin aries only, for linux and sunos/solaris.
</nextsent>
<nextsent>figure 1: the telri catalogue html form *name = name of product *task = task of product author = name(s) of author(s) affiliation = name of company street = address of company city country version = version number language = language(s) *description licres = license conditions for research purposes liccom = license conditions for commercial purposes restrict = license restrictions source url = url of source code binary url = url of binary files platform = supported hardware os = supper ted operating system(s) impl = language of implementation interface = user interface homepage = url of homepage doc url = url of documentation doc lang = language of documentation *helpline = telri helpline table 2: full list of fields of the catalogue html form figure 2: sample output page of one catalogue item user input userreadableformats: html, pdf etc. docbook xml perl/cgi xslthtmlform figure 3: overview of the catalogue data processing  the ims corpus workbench concordancer (christ, 1994)comprises powerful corpus query processor and graphical user interface.
</nextsent>
<nextsent>it is available by license agreement which is free of charge for non-commercial purposes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y347">
<title id=" W01-1503.xml">the telri tool catalogue structure and prospects </title>
<section> catalogue contents.  </section>
<citcontext>
<prevsection>
<prevsent>the translations are ranked according to computed confidence.
</prevsent>
<prevsent>the system uses statistical measures and works for single words (to kens) only.
</prevsent>
</prevsection>
<citsent citstr=" P98-1004 ">
it is available under the gnu general public license and is written in c.  plug word aligner (ahrenberg et al, 1998) <papid> P98-1004 </papid>the system integrates set of modules forknowledge-lite approaches to word alignment, with various possibilities to change configuration and to adapt the system to other language pairs and text types.</citsent>
<aftsection>
<nextsent>the system takes parallel sentence aligned corpus as input and produces list of word and phrase correspondences in the text (link in stances) and additionally bilingual lexicon from these instances (type links).
</nextsent>
<nextsent>it is available by license agreement which is free of charge for non-commercial purposes.
</nextsent>
<nextsent>distribution is available, in binary form only, for linux and ms windows.
</nextsent>
<nextsent>the paper reported on the set-up of the telricorpus-tool catalogue, concentrating on the technical issues involved in its creation (form inter face), storage (docbook) and display (xslt).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y348">
<title id=" W01-0802.xml">a two staged model for content determination </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>also, algorithms and architectures for content-determination seem to often be based on the intuitions of system developers, instead of on empirical observations, although detailed content determination rules are often based on corpus analysis and interaction with experts.
</prevsent>
<prevsent>in this paper we propose general architecture for content determination in data summarisation systems which assumes that content determination happens in two stages: first qualitative overview of the data is formed, and second the content of the actual summaries is decided upon.
</prevsent>
</prevsection>
<citsent citstr=" W00-1429 ">
this model is based on extensive knowledge acquisition (ka) activ ies that we have carried out in the sumtime project (sripada, 2001), and also matches observations made during ka activities carried out in the stop project (reiter et al 2000).<papid> W00-1429 </papid></citsent>
<aftsection>
<nextsent>we have not yet implemented this model, and indeed one of the issues that we need to think about is to what degree content-determination strategy used by human experts is also an appropriate one for computer nlg system.
</nextsent>
<nextsent>content determination is the task of deciding on the information content of generated text.
</nextsent>
<nextsent>in the three-stage pipeline model of reiter and dale (2000), content determination is part of the first stage, document planning, along with document structuring (determining the textual and rhetorical structure of text).
</nextsent>
<nextsent>content determination is extremely important to end users; in most applications users probably prefer text which poorly expresses appropriate content to text which nicely expresses inappropriate content.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y349">
<title id=" W01-0802.xml">a two staged model for content determination </title>
<section> content determination.  </section>
<citcontext>
<prevsection>
<prevsent>content determination is extremely important to end users; in most applications users probably prefer text which poorly expresses appropriate content to text which nicely expresses inappropriate content.
</prevsent>
<prevsent>from theoretical perspective content determination should probably be based on deep reasoning about the system communicative goal, the user intentions, and the current context (allen and perrault 1980), but this requires an enormous amount of knowledge and reasoning, and is difficult to do robustly in real applications.
</prevsent>
</prevsection>
<citsent citstr=" J98-3004 ">
in recent years many new content determination strategies have been proposed, ranging from the use of sophisticated signal-processing techniques (boyd 1997) to complex planning algorithms (mittal et al 1998) <papid> J98-3004 </papid>to systems which exploit cognitive models of the user (fiedler 1998).<papid> W98-1410 </papid></citsent>
<aftsection>
<nextsent>however, most of these strategies have only been demonstrated in one application.
</nextsent>
<nextsent>furthermore, as far as we can tell these strategies are usually based on the intuition and experiences of the developers.
</nextsent>
<nextsent>while realisation, micro planning, and document structuring techniques are increasingly based on analyses of how humans perform these tasks (including corpus analysis, psycho linguistic studies, and ka activities), most papers on content determination make little reference to how human experts determine the content of text.
</nextsent>
<nextsent>human experts are often consulted with regard to the details of content rules, especially when schemas are used for content determination (goldberg et al 1994, mckeown et al 1994, <papid> A94-1002 </papid>reiter et al 2000); <papid> W00-1429 </papid>but they rarely seem to be consulted (as far as we can tell) when deciding on the general algorithm or strategy to use for content determination.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y350">
<title id=" W01-0802.xml">a two staged model for content determination </title>
<section> content determination.  </section>
<citcontext>
<prevsection>
<prevsent>content determination is extremely important to end users; in most applications users probably prefer text which poorly expresses appropriate content to text which nicely expresses inappropriate content.
</prevsent>
<prevsent>from theoretical perspective content determination should probably be based on deep reasoning about the system communicative goal, the user intentions, and the current context (allen and perrault 1980), but this requires an enormous amount of knowledge and reasoning, and is difficult to do robustly in real applications.
</prevsent>
</prevsection>
<citsent citstr=" W98-1410 ">
in recent years many new content determination strategies have been proposed, ranging from the use of sophisticated signal-processing techniques (boyd 1997) to complex planning algorithms (mittal et al 1998) <papid> J98-3004 </papid>to systems which exploit cognitive models of the user (fiedler 1998).<papid> W98-1410 </papid></citsent>
<aftsection>
<nextsent>however, most of these strategies have only been demonstrated in one application.
</nextsent>
<nextsent>furthermore, as far as we can tell these strategies are usually based on the intuition and experiences of the developers.
</nextsent>
<nextsent>while realisation, micro planning, and document structuring techniques are increasingly based on analyses of how humans perform these tasks (including corpus analysis, psycho linguistic studies, and ka activities), most papers on content determination make little reference to how human experts determine the content of text.
</nextsent>
<nextsent>human experts are often consulted with regard to the details of content rules, especially when schemas are used for content determination (goldberg et al 1994, mckeown et al 1994, <papid> A94-1002 </papid>reiter et al 2000); <papid> W00-1429 </papid>but they rarely seem to be consulted (as far as we can tell) when deciding on the general algorithm or strategy to use for content determination.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y351">
<title id=" W01-0802.xml">a two staged model for content determination </title>
<section> content determination.  </section>
<citcontext>
<prevsection>
<prevsent>furthermore, as far as we can tell these strategies are usually based on the intuition and experiences of the developers.
</prevsent>
<prevsent>while realisation, micro planning, and document structuring techniques are increasingly based on analyses of how humans perform these tasks (including corpus analysis, psycho linguistic studies, and ka activities), most papers on content determination make little reference to how human experts determine the content of text.
</prevsent>
</prevsection>
<citsent citstr=" A94-1002 ">
human experts are often consulted with regard to the details of content rules, especially when schemas are used for content determination (goldberg et al 1994, mckeown et al 1994, <papid> A94-1002 </papid>reiter et al 2000); <papid> W00-1429 </papid>but they rarely seem to be consulted (as far as we can tell) when deciding on the general algorithm or strategy to use for content determination.</citsent>
<aftsection>
<nextsent>3.1 text summaries of time-series data.
</nextsent>
<nextsent>time-series data is collection of values of set of parameters over time.
</nextsent>
<nextsent>such data is very common in the modern world, with its proliferation of databases and sensors, and humans frequently need to examine and make inferences from time-series data.
</nextsent>
<nextsent>currently, human examination of time-series data is generally done either by direct inspection of the data (for small data sets), by graphical visualisation, or by statistical analyses.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y354">
<title id=" W01-0802.xml">a two staged model for content determination </title>
<section> two-stage model for content.  </section>
<citcontext>
<prevsection>
<prevsent>two stage model for content determination communication reasoner (cr) system generates the final content specification taking into account the influence of the user constraints (uc) and other pragmatic factors.
</prevsent>
<prevsent>this content is then sent to subsequent nlg modules (not shown), such as micro planning and surface realisation.
</prevsent>
</prevsection>
<citsent citstr=" W98-1409 ">
our model has some similarities to the one proposed by barzilay et al (1998), <papid> W98-1409 </papid>in that the domain reasoner uses general domain knowledge similar to their rdk, while the communication reasoner uses communication knowledge similar to their cdk and dck.</citsent>
<aftsection>
<nextsent>the central feature of the above model is the idea of data overview and its effect on content selection.
</nextsent>
<nextsent>one possible use of overviews is to trigger context-dependent content rules.
</nextsent>
<nextsent>the time-series analysis part of sumtime is largely based on shahar model (1997), which makes heavy use of such rules.
</nextsent>
<nextsent>in shahar model contexts are inferred by separate mechanisms; we believe that these should be incorporated into the overview, but this needs further investigation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y355">
<title id=" W01-1404.xml">approximating context free by rational transduction for example based mt </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the advantage is increased processing speed, which benefits real time applications involving spoken language.
</prevsent>
<prevsent>several studies have investigated automatic or partly automatic learning of transductions for machine translation.
</prevsent>
</prevsection>
<citsent citstr=" J00-1004 ">
some of these studies have concentrated on finite-state or extended finite-statemachinery, such as (vilar and others, 1999), others have chosen models closer to context-free grammars and context-free transduction, such as(alshawi et al , 2000; <papid> J00-1004 </papid>watanabe et al , 2000; <papid> C00-2131 </papid>yamamoto and matsumoto, 2000), <papid> C00-2135 </papid>and yet other studies cannot be comfortably assigned to either of these two frameworks, such as (brown and others, 1990) and (tillmann and ney, 2000).<papid> C00-2123 </papid>in this paper we will investigate both context free and finite-state models.</citsent>
<aftsection>
<nextsent>the basis for our study is context-free transduction since that is powerful model of translation, which can in many cases adequately describe the changes of word
</nextsent>
<nextsent>the second address is the current contact address; supported by the royal netherlands academy of arts and sci ences; current secondary affiliation is the german research center for artificial intelligence (dfki).
</nextsent>
<nextsent>order between two languages, and the selection of appropriate lexical items.
</nextsent>
<nextsent>furthermore, for limited domains, automatic learning of weighted context-free transductions from examples seems to be reasonably successful.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y356">
<title id=" W01-1404.xml">approximating context free by rational transduction for example based mt </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the advantage is increased processing speed, which benefits real time applications involving spoken language.
</prevsent>
<prevsent>several studies have investigated automatic or partly automatic learning of transductions for machine translation.
</prevsent>
</prevsection>
<citsent citstr=" C00-2131 ">
some of these studies have concentrated on finite-state or extended finite-statemachinery, such as (vilar and others, 1999), others have chosen models closer to context-free grammars and context-free transduction, such as(alshawi et al , 2000; <papid> J00-1004 </papid>watanabe et al , 2000; <papid> C00-2131 </papid>yamamoto and matsumoto, 2000), <papid> C00-2135 </papid>and yet other studies cannot be comfortably assigned to either of these two frameworks, such as (brown and others, 1990) and (tillmann and ney, 2000).<papid> C00-2123 </papid>in this paper we will investigate both context free and finite-state models.</citsent>
<aftsection>
<nextsent>the basis for our study is context-free transduction since that is powerful model of translation, which can in many cases adequately describe the changes of word
</nextsent>
<nextsent>the second address is the current contact address; supported by the royal netherlands academy of arts and sci ences; current secondary affiliation is the german research center for artificial intelligence (dfki).
</nextsent>
<nextsent>order between two languages, and the selection of appropriate lexical items.
</nextsent>
<nextsent>furthermore, for limited domains, automatic learning of weighted context-free transductions from examples seems to be reasonably successful.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y357">
<title id=" W01-1404.xml">approximating context free by rational transduction for example based mt </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the advantage is increased processing speed, which benefits real time applications involving spoken language.
</prevsent>
<prevsent>several studies have investigated automatic or partly automatic learning of transductions for machine translation.
</prevsent>
</prevsection>
<citsent citstr=" C00-2135 ">
some of these studies have concentrated on finite-state or extended finite-statemachinery, such as (vilar and others, 1999), others have chosen models closer to context-free grammars and context-free transduction, such as(alshawi et al , 2000; <papid> J00-1004 </papid>watanabe et al , 2000; <papid> C00-2131 </papid>yamamoto and matsumoto, 2000), <papid> C00-2135 </papid>and yet other studies cannot be comfortably assigned to either of these two frameworks, such as (brown and others, 1990) and (tillmann and ney, 2000).<papid> C00-2123 </papid>in this paper we will investigate both context free and finite-state models.</citsent>
<aftsection>
<nextsent>the basis for our study is context-free transduction since that is powerful model of translation, which can in many cases adequately describe the changes of word
</nextsent>
<nextsent>the second address is the current contact address; supported by the royal netherlands academy of arts and sci ences; current secondary affiliation is the german research center for artificial intelligence (dfki).
</nextsent>
<nextsent>order between two languages, and the selection of appropriate lexical items.
</nextsent>
<nextsent>furthermore, for limited domains, automatic learning of weighted context-free transductions from examples seems to be reasonably successful.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y358">
<title id=" W01-1404.xml">approximating context free by rational transduction for example based mt </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the advantage is increased processing speed, which benefits real time applications involving spoken language.
</prevsent>
<prevsent>several studies have investigated automatic or partly automatic learning of transductions for machine translation.
</prevsent>
</prevsection>
<citsent citstr=" C00-2123 ">
some of these studies have concentrated on finite-state or extended finite-statemachinery, such as (vilar and others, 1999), others have chosen models closer to context-free grammars and context-free transduction, such as(alshawi et al , 2000; <papid> J00-1004 </papid>watanabe et al , 2000; <papid> C00-2131 </papid>yamamoto and matsumoto, 2000), <papid> C00-2135 </papid>and yet other studies cannot be comfortably assigned to either of these two frameworks, such as (brown and others, 1990) and (tillmann and ney, 2000).<papid> C00-2123 </papid>in this paper we will investigate both context free and finite-state models.</citsent>
<aftsection>
<nextsent>the basis for our study is context-free transduction since that is powerful model of translation, which can in many cases adequately describe the changes of word
</nextsent>
<nextsent>the second address is the current contact address; supported by the royal netherlands academy of arts and sci ences; current secondary affiliation is the german research center for artificial intelligence (dfki).
</nextsent>
<nextsent>order between two languages, and the selection of appropriate lexical items.
</nextsent>
<nextsent>furthermore, for limited domains, automatic learning of weighted context-free transductions from examples seems to be reasonably successful.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y359">
<title id=" W01-1404.xml">approximating context free by rational transduction for example based mt </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>furthermore, for limited domains, automatic learning of weighted context-free transductions from examples seems to be reasonably successful.
</prevsent>
<prevsent>however, practical algorithms for computing the most likely context-free derivation have cubic time complexity, in terms of the length ofthe input string, or in the case of graph output by speech recognizer, in terms of the number of nodes in the graph.
</prevsent>
</prevsection>
<citsent citstr=" P99-1059 ">
for certain lexicalized context-free models we even obtain higher time complexities when the size of the grammar is not to be considered as parameter (eisner and satta, 1999).<papid> P99-1059 </papid></citsent>
<aftsection>
<nextsent>this may pose problems, especially for real-time speech systems.
</nextsent>
<nextsent>therefore, we have investigated approximation of weighted context-free transduction by means of weighted rational transduction.
</nextsent>
<nextsent>the finite-state machinery for implementing the latter kind of transduction in general allows faster processing.
</nextsent>
<nextsent>we can also more easily obtain robustness.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y368">
<title id=" W01-1404.xml">approximating context free by rational transduction for example based mt </title>
<section> finite-state approximation.  </section>
<citcontext>
<prevsection>
<prevsent>both the reorder operators and the symbols ofthe target al habet will here be marked by horizontal line to distinguish them from the source alphabet.
</prevsent>
<prevsent>for example, the two productions 0  a 1k  0  a 1ffj 0 | | 1  0 | | 1j 0  a 1  0  a 1k   a  from the transduction grammar are represented by the following two context-free productions: 0  a 1k 0 0  a 1  0 | | 1  0  a 1k g ain the first production, the rhs nonterminals occur in the same order as in the left half of the original production, but reorder operators have been added to indicate that, after parsing, some sub strings of the output string are to be reordered.our reorder operators are similar to the two operators ? and ? from (vilar and others, 1999), but the former are more powerful, since the latter allow only single words to be moved instead of whole phrases.
</prevsent>
</prevsection>
<citsent citstr=" J00-1003 ">
there are several methods to approximate context-free grammars by regular languages (nederhof, 2000).<papid> J00-1003 </papid></citsent>
<aftsection>
<nextsent>we will consider here only theso called rtn method, which is applied in simplified form.3 3as opposed to (nederhof, 2000), <papid> J00-1003 </papid>we assume here that all nonterminals are mutually recursive, and the grammar contains self-embedding.</nextsent>
<nextsent>we have observed that typical grammars that we obtain in the context of this article indeed have the property that almost all nonterminals belong to the same mutually recursive set.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y370">
<title id=" W01-1404.xml">approximating context free by rational transduction for example based mt </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>that it restarts at the initial state when it gets stuck at some input word, and when necessary, that input word is deleted.
</prevsent>
<prevsent>the out put string with the lowest weight obtained so far (preferably attached to final states, orto other states with outgoing transitions labelled by input symbols) is then concate nated with the output string resulting from processing subsequent input.
</prevsent>
</prevsection>
<citsent citstr=" N01-1018 ">
we have investigated corpus of en glish/japanese sentence pairs, related by hierarchical alignment (see also (bangalore and riccardi, 2001)).<papid> N01-1018 </papid></citsent>
<aftsection>
<nextsent>we have taken the first 500, 1000, 1500, . . .
</nextsent>
<nextsent>aligned sentence pairs from this corpus to act as training corpora of varying sizes; we have taken 300 other sentence pairs to act as test corpus.
</nextsent>
<nextsent>we have constructed bilexical transduction grammar from each training corpus, in the form of context-free grammar, and this grammar was approximated by finite automaton.
</nextsent>
<nextsent>the input sentences from the test corpus were then processed by context-free and finite-state machinery (in the sequel referred to by cfg and fa, respectively).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y372">
<title id=" W01-1404.xml">approximating context free by rational transduction for example based mt </title>
<section> conclusions.  </section>
<citcontext>
<prevsection>
<prevsent>for our application, context-free transduction has relatively high accuracy, but it also has high time consumption, and it may be difficult to obtain robustness without further increasing the time costs.
</prevsent>
<prevsent>these are two major obstacles for use in spoken language systems.
</prevsent>
</prevsection>
<citsent citstr=" P94-1017 ">
we have tried to obtain rational transduction that approximates 4it uses trie to represent productions (similar to elr parsing (nederhof, 1994)), <papid> P94-1017 </papid>postponing generation of output for production until all nonterminals and all input symbols from the right-hand side have been found.</citsent>
<aftsection>
<nextsent>0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 1000 2000 3000 4000 5000 6000 7000 8000 o rd cc ur ac ? training corpus size cfg2 cfg fa2 fa bigram robust_fa2 robust_fa figure 1: average word accuracy for transduced sentences.
</nextsent>
<nextsent>0 0.2 0.4 0.6 0.8 1 0 1000 2000 3000 4000 5000 6000 7000 8000 cc pt ed ? training corpus size fa fa2 cfg cfg2 figure 2: fraction of the sentences that were transduced.
</nextsent>
<nextsent>context-free transduction, preserving some of its accuracy.our experiments show that the automata we obtain become very large for training corpora of increasing sizes.
</nextsent>
<nextsent>this poses problem for deter minization.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y373">
<title id=" W02-0302.xml">tagging gene and protein names in full text articles </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>proceedings of the workshop on natural language processing in chromosomal locations and disease loci to consider.
</prevsent>
<prevsent>the domain-specific irregularities and ambiguities just described are superimposed upon the ambiguities in the natural language itself, so it is not surprising that the identification of gene and protein names in biomedical text remains difficult and challenging task.
</prevsent>
</prevsection>
<citsent citstr=" C00-1030 ">
the methodologies applied to this fundamental problem include rule-based and/or pattern matching methods (fukuda et al , 1998) (thomas et al , 2000) (yoshida et al , 2000) (jenssen et al , 2001) (ono et al , 2001) (yu at al, 2002) (bunescu et al , 2002), modified blast algorithm (krauthammer et al ., 2000), hidden markov models (hmms) (collier et al , 2000) (<papid> C00-1030 </papid>proux et al , 1998), naive bayes and decision trees (nobata et al , 1999), underspecified parsing with knowledge sources (rindflesch et al  2000), and context-free grammars (gaizauskas, 2000).</citsent>
<aftsection>
<nextsent>in this paper, we evaluate the application of gene and protein name tagger trained on medline abstracts (abgene) (tanabe and wilbur, 2002) to randomly selected set of 1,000 pubmedcentral?
</nextsent>
<nextsent>(pmc) articles.
</nextsent>
<nextsent>pmc is digital archive of full text peer reviewed biomedical articles launched in february 2000 by the national center for biotechnology information (ncbi) and the u.s. national library of medicine (nlm?)
</nextsent>
<nextsent>(roberts et al , 2001).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y374">
<title id=" W01-1412.xml">a comparative study on translation units for bilingual lexicon extraction </title>
<section> models of translation units.  </section>
<citcontext>
<prevsection>
<prevsent>finally, section 6 concludes the paper.
</prevsent>
<prevsent>the main objective of this paper is to determine suitable translation units for the automatic acquisition of translation pairs.
</prevsent>
</prevsection>
<citsent citstr=" J00-2004 ">
a word-to-word correspondence is often assumed in the pioneering works, and recently melamed argues that one-to one assumption is not restrictive as it may appear in (melamed, 2000).<papid> J00-2004 </papid></citsent>
<aftsection>
<nextsent>however, we question his claim, since the tokenization of words for non segmented languages such as japanese is, by nature, ambiguous, and thus his one-to-one assumption is difficult to hold.
</nextsent>
<nextsent>we address this ambiguity problem by allowing overlaps?
</nextsent>
<nextsent>in generation of translation units and obtain single- and multiword correspondences simultaneously.
</nextsent>
<nextsent>previous works that focus on multi-word pierre vinken , 61 years old , will join the board as non executive director nov. 28 . figure 1: sample sentence pierre pierre-vinken pierre-vinken-years pierre-vinken-years-old pierre-vinken-years-old-join vinken vinken-years vinken-years-old vinken-years-old-join vinken-years-old-join-board years years-old years-old-join years-old-join-board years-old-join-board-nonexecutive old old-join old-join-board old-join-board-nonexecutive old-join-board-nonexecutive-director join join-board join-board-nonexecutive join-board-nonexecutive-director join-board-nonexecutive-director-nov. board board-nonexecutive board-nonexecutive-director board-nonexecutive-director-nov non executive nonexecutive-director nonexecutive-director-nov director director-nov nov figure 2: bound-length n-gram correspondences include (kupiec, 1993) <papid> P93-1003 </papid>where np recognizers are used to extract translation units and (smadja et al, 1996) <papid> J96-1001 </papid>which uses th extract system to extract collocations.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y375">
<title id=" W01-1412.xml">a comparative study on translation units for bilingual lexicon extraction </title>
<section> models of translation units.  </section>
<citcontext>
<prevsection>
<prevsent>we address this ambiguity problem by allowing overlaps?
</prevsent>
<prevsent>in generation of translation units and obtain single- and multiword correspondences simultaneously.
</prevsent>
</prevsection>
<citsent citstr=" P93-1003 ">
previous works that focus on multi-word pierre vinken , 61 years old , will join the board as non executive director nov. 28 . figure 1: sample sentence pierre pierre-vinken pierre-vinken-years pierre-vinken-years-old pierre-vinken-years-old-join vinken vinken-years vinken-years-old vinken-years-old-join vinken-years-old-join-board years years-old years-old-join years-old-join-board years-old-join-board-nonexecutive old old-join old-join-board old-join-board-nonexecutive old-join-board-nonexecutive-director join join-board join-board-nonexecutive join-board-nonexecutive-director join-board-nonexecutive-director-nov. board board-nonexecutive board-nonexecutive-director board-nonexecutive-director-nov non executive nonexecutive-director nonexecutive-director-nov director director-nov nov figure 2: bound-length n-gram correspondences include (kupiec, 1993) <papid> P93-1003 </papid>where np recognizers are used to extract translation units and (smadja et al, 1996) <papid> J96-1001 </papid>which uses th extract system to extract collocations.</citsent>
<aftsection>
<nextsent>more over, (kitamura and matsumoto, 1996) <papid> W96-0107 </papid>extracts an arbitrary length of word correspondences and (haruno et al, 1996) <papid> C96-1089 </papid>identifies collocations through word-level sorting.in this paper, we compare three n-gram models of translation units, namely bound-length ngram, chunk-bound n-gram, and dependency linked n-gram.</nextsent>
<nextsent>our approach of extracting bilingual lexicon is two-staged.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y376">
<title id=" W01-1412.xml">a comparative study on translation units for bilingual lexicon extraction </title>
<section> models of translation units.  </section>
<citcontext>
<prevsection>
<prevsent>we address this ambiguity problem by allowing overlaps?
</prevsent>
<prevsent>in generation of translation units and obtain single- and multiword correspondences simultaneously.
</prevsent>
</prevsection>
<citsent citstr=" J96-1001 ">
previous works that focus on multi-word pierre vinken , 61 years old , will join the board as non executive director nov. 28 . figure 1: sample sentence pierre pierre-vinken pierre-vinken-years pierre-vinken-years-old pierre-vinken-years-old-join vinken vinken-years vinken-years-old vinken-years-old-join vinken-years-old-join-board years years-old years-old-join years-old-join-board years-old-join-board-nonexecutive old old-join old-join-board old-join-board-nonexecutive old-join-board-nonexecutive-director join join-board join-board-nonexecutive join-board-nonexecutive-director join-board-nonexecutive-director-nov. board board-nonexecutive board-nonexecutive-director board-nonexecutive-director-nov non executive nonexecutive-director nonexecutive-director-nov director director-nov nov figure 2: bound-length n-gram correspondences include (kupiec, 1993) <papid> P93-1003 </papid>where np recognizers are used to extract translation units and (smadja et al, 1996) <papid> J96-1001 </papid>which uses th extract system to extract collocations.</citsent>
<aftsection>
<nextsent>more over, (kitamura and matsumoto, 1996) <papid> W96-0107 </papid>extracts an arbitrary length of word correspondences and (haruno et al, 1996) <papid> C96-1089 </papid>identifies collocations through word-level sorting.in this paper, we compare three n-gram models of translation units, namely bound-length ngram, chunk-bound n-gram, and dependency linked n-gram.</nextsent>
<nextsent>our approach of extracting bilingual lexicon is two-staged.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y377">
<title id=" W01-1412.xml">a comparative study on translation units for bilingual lexicon extraction </title>
<section> models of translation units.  </section>
<citcontext>
<prevsection>
<prevsent>in generation of translation units and obtain single- and multiword correspondences simultaneously.
</prevsent>
<prevsent>previous works that focus on multi-word pierre vinken , 61 years old , will join the board as non executive director nov. 28 . figure 1: sample sentence pierre pierre-vinken pierre-vinken-years pierre-vinken-years-old pierre-vinken-years-old-join vinken vinken-years vinken-years-old vinken-years-old-join vinken-years-old-join-board years years-old years-old-join years-old-join-board years-old-join-board-nonexecutive old old-join old-join-board old-join-board-nonexecutive old-join-board-nonexecutive-director join join-board join-board-nonexecutive join-board-nonexecutive-director join-board-nonexecutive-director-nov. board board-nonexecutive board-nonexecutive-director board-nonexecutive-director-nov non executive nonexecutive-director nonexecutive-director-nov director director-nov nov figure 2: bound-length n-gram correspondences include (kupiec, 1993) <papid> P93-1003 </papid>where np recognizers are used to extract translation units and (smadja et al, 1996) <papid> J96-1001 </papid>which uses th extract system to extract collocations.</prevsent>
</prevsection>
<citsent citstr=" W96-0107 ">
more over, (kitamura and matsumoto, 1996) <papid> W96-0107 </papid>extracts an arbitrary length of word correspondences and (haruno et al, 1996) <papid> C96-1089 </papid>identifies collocations through word-level sorting.in this paper, we compare three n-gram models of translation units, namely bound-length ngram, chunk-bound n-gram, and dependency linked n-gram.</citsent>
<aftsection>
<nextsent>our approach of extracting bilingual lexicon is two-staged.
</nextsent>
<nextsent>we first prepare ngrams independently for each language in the parallel corpora and then find corresponding translation pairs from both sets of translation unitsin greedy manner.
</nextsent>
<nextsent>the essence of our algorithm is that we allow some overlapping translation units to accommodate ambiguity in the firststage.
</nextsent>
<nextsent>once translation pairs are detected during the process, they are decisively selected, and the translation units that overlaps with the found translation pairs are gradually ruled out.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y378">
<title id=" W01-1412.xml">a comparative study on translation units for bilingual lexicon extraction </title>
<section> models of translation units.  </section>
<citcontext>
<prevsection>
<prevsent>in generation of translation units and obtain single- and multiword correspondences simultaneously.
</prevsent>
<prevsent>previous works that focus on multi-word pierre vinken , 61 years old , will join the board as non executive director nov. 28 . figure 1: sample sentence pierre pierre-vinken pierre-vinken-years pierre-vinken-years-old pierre-vinken-years-old-join vinken vinken-years vinken-years-old vinken-years-old-join vinken-years-old-join-board years years-old years-old-join years-old-join-board years-old-join-board-nonexecutive old old-join old-join-board old-join-board-nonexecutive old-join-board-nonexecutive-director join join-board join-board-nonexecutive join-board-nonexecutive-director join-board-nonexecutive-director-nov. board board-nonexecutive board-nonexecutive-director board-nonexecutive-director-nov non executive nonexecutive-director nonexecutive-director-nov director director-nov nov figure 2: bound-length n-gram correspondences include (kupiec, 1993) <papid> P93-1003 </papid>where np recognizers are used to extract translation units and (smadja et al, 1996) <papid> J96-1001 </papid>which uses th extract system to extract collocations.</prevsent>
</prevsection>
<citsent citstr=" C96-1089 ">
more over, (kitamura and matsumoto, 1996) <papid> W96-0107 </papid>extracts an arbitrary length of word correspondences and (haruno et al, 1996) <papid> C96-1089 </papid>identifies collocations through word-level sorting.in this paper, we compare three n-gram models of translation units, namely bound-length ngram, chunk-bound n-gram, and dependency linked n-gram.</citsent>
<aftsection>
<nextsent>our approach of extracting bilingual lexicon is two-staged.
</nextsent>
<nextsent>we first prepare ngrams independently for each language in the parallel corpora and then find corresponding translation pairs from both sets of translation unitsin greedy manner.
</nextsent>
<nextsent>the essence of our algorithm is that we allow some overlapping translation units to accommodate ambiguity in the firststage.
</nextsent>
<nextsent>once translation pairs are detected during the process, they are decisively selected, and the translation units that overlaps with the found translation pairs are gradually ruled out.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y380">
<title id=" W01-1412.xml">a comparative study on translation units for bilingual lexicon extraction </title>
<section> models of translation units.  </section>
<citcontext>
<prevsection>
<prevsent>pierre pierre-vinken pierre-vinken-join vinken vinken-join years years-old old pierre-vinken-old join board join-board non executive nonexecutive-director director nov. join-nov figure 4: dependency-linked n-gram extended boundaries.
</prevsent>
<prevsent>therefore, translation units generated by dependency-linked n-gram (figure 4) become the superset of the units generated by chunk-bound n-gram (figure 3).the distinct characteristics of dependency linked n-gram from previous works are two-fold.
</prevsent>
</prevsection>
<citsent citstr=" C00-2135 ">
first, (yamamoto and matsumoto, 2000) <papid> C00-2135 </papid>also uses dependency relations in the generation of translation units.</citsent>
<aftsection>
<nextsent>however, it suffers from data sparseness (and thus low coverage), since the entire chunk is treated as translation unit, which is too coarse.
</nextsent>
<nextsent>dependency-linked n-gram, on the other hand, uses more fine-grained n-grams as translation units in order to avoid sparseness.
</nextsent>
<nextsent>second, dependency-linked n-gram includes flexible?
</nextsent>
<nextsent>or non-contiguous collocations if dependency links are distant in sentence.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y384">
<title id=" W01-1412.xml">a comparative study on translation units for bilingual lexicon extraction </title>
<section> experiment and result.  </section>
<citcontext>
<prevsection>
<prevsent>of the correct translation pairs for unseen test data.
</prevsent>
<prevsent>it is the number of tokens matched by the correct translation pairs over the number of tokens in the unseen test data.
</prevsent>
</prevsection>
<citsent citstr=" W95-0115 ">
acuracy and coverage roughly correspond to mela meds precision and percent correct respectively (melamed, 1995).<papid> W95-0115 </papid></citsent>
<aftsection>
<nextsent>accuracy is calculated on the training data (8000 sentences) manually, whereas coverage is calculated on the test data (2000 sentences) automatically.
</nextsent>
<nextsent>4.2 accuracy.
</nextsent>
<nextsent>stepwise accuracy for each model is listed in table 2, table 3, and table 4.
</nextsent>
<nextsent>? dfngo7pp ? indicates the threshold, i.e. stages in the algorithm.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y385">
<title id=" W01-1412.xml">a comparative study on translation units for bilingual lexicon extraction </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>that is how to determine translation pairs which co-occur only once.
</prevsent>
<prevsent>one simple approach is to use machine-readable bilingual dictionary.
</prevsent>
</prevsection>
<citsent citstr=" C00-2131 ">
however, more fundamental solution may lie in the partial structural matching of parallel sentences (watanabe et al, 2000).<papid> C00-2131 </papid></citsent>
<aftsection>
<nextsent>we intend to incorporate these techniques to improve the overall coverage.
</nextsent>
<nextsent>this paper reports on-going research on extracting bilingual lexicon from english-japanese parallel corpora.
</nextsent>
<nextsent>three models including previously proposed one in (kitamura and matsumoto,1996) <papid> W96-0107 </papid>are compared in this paper.</nextsent>
<nextsent>through preliminary experiments with 10000 bilingual sentences, we obtain that our new models (chunk bound n-gram and dependency-linked n-gram)gain approximately 13% improvement in accuracy and 5-9% improvement in coverage from the baseline model (bound-length n-gram).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y387">
<title id=" W02-0105.xml">a non programming introduction to computer science via nlp ir and ai </title>
<section> course content.  </section>
<citcontext>
<prevsection>
<prevsent>to describe this structure, we formally defined context-freegrammars.
</prevsent>
<prevsent>we then showed how (a tiny fragment of) x-bar theory can be modeled by context-free grammar and, using its structural assignments and the notion of heads of constituents, accounted for some of the ambiguities and non-ambiguities in the linguistic examples we previously examined.the discussion of context-free grammars naturally led us to push down automata (which provided nice contrast to the turing machine swe studied earlier in the course).
</prevsent>
</prevsection>
<citsent citstr=" J86-3001 ">
and, having thus introduced stacks, we then investigated the grosz and sidner (1986) <papid> J86-3001 </papid>stack-based theory of discourse structure, showing that language structures exist at granularities beyond the sentence level.</citsent>
<aftsection>
<nextsent>statistical language processing [6 lectures]we began this unit by considering word frequency distributions, and in particular, zipfs law ? note that our having studied power-law distributions in the web unit greatly facilitated this discussion.
</nextsent>
<nextsent>in fact, because we had previously investigated generative models for the web, it was natural to consider millers (1957) monkeys?
</nextsent>
<nextsent>model which demonstrates that very simple generative models can account for zipfslaw.
</nextsent>
<nextsent>next, we looked at methods taking advantage of statistical regularities, including the ibm candide statistical machine translation system, following knights (1999) tutorial and treating probabilities as weights.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y388">
<title id=" W02-0105.xml">a non programming introduction to computer science via nlp ir and ai </title>
<section> course content.  </section>
<citcontext>
<prevsection>
<prevsent>next, we looked at methods taking advantage of statistical regularities, including the ibm candide statistical machine translation system, following knights (1999) tutorial and treating probabilities as weights.
</prevsent>
<prevsent>it was interesting to point out parallels with the hubs and authorities algorithm ? both are iterative update procedures with auxiliary information (alignments in one case, hubs in the other).
</prevsent>
</prevsection>
<citsent citstr=" A00-2032 ">
we also discus sedan intuitive algorithm for japanese segmentation drawn from one of my own recent research collaborations (ando and lee, 2000), <papid> A00-2032 </papid>and how word statistics were applied to determining the authorship of the federalist papers (mostellerand wallace, 1984).</citsent>
<aftsection>
<nextsent>we concluded with an examination of human statistical learning, focusing on recent evidence indicating that human infants can use statistics when learning to segment continuous speech into words (saffran et al., 1996).
</nextsent>
<nextsent>the turing test [2 lectures] finally, weended the course with consideration of intelligence in the large.
</nextsent>
<nextsent>in particular, we focused on turings (1950) proposal of the imitation game?, which can be interpreted as one of the first appearances of the claim that natural language processing is ai-complete?, and searles(1980) chinese room?
</nextsent>
<nextsent>rebuttal that fluent language behavior is not sufficient indication ofintelligence.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y389">
<title id=" W01-1206.xml">answer mining from online documents </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, given that the expected information is recognized by inspecting text snippets of relatively small size, the trec q/a task took step closer to information retrieval rather than document retrieval.
</prevsent>
<prevsent>moreover, the techniques developed to extract text snippets where the answers might lie paved the way to unified model for answer mining.
</prevsent>
</prevsection>
<citsent citstr=" A00-1041 ">
to find the answer to question several steps must be taken, as reported in (abney et al, 2000) (<papid> A00-1041 </papid>moldovan et al, 2000) (<papid> P00-1071 </papid>srihari and li, 2000): first, the question semantics needs to be cap tured.</citsent>
<aftsection>
<nextsent>this translates into identifying (i) the expected answer type and (ii) the question keywords that can be used to retrieve text passages where the answer may be found. secondly, the index of the document collection must be used to identify the text passages of interest.
</nextsent>
<nextsent>the retrieval method either employs special operators or simply modifies boolean or vector retrieval.
</nextsent>
<nextsent>since the expected answer type is known at the time workshops organized by the national institute of standard sand technology (nist), designed to advance the state-of the-art in information retrieval (ir)of the retrieval, the quality of the text passages is greatly improved by filtering out those passages where concepts of the same category as the answer type are not present.
</nextsent>
<nextsent> thirdly, answer extraction takes place by combining several features that take into account the expected answer type.since the expected answer type is the only information used in all the phases of textual q/a,its recognition and usage is central to the performance of answer mining.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y390">
<title id=" W01-1206.xml">answer mining from online documents </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, given that the expected information is recognized by inspecting text snippets of relatively small size, the trec q/a task took step closer to information retrieval rather than document retrieval.
</prevsent>
<prevsent>moreover, the techniques developed to extract text snippets where the answers might lie paved the way to unified model for answer mining.
</prevsent>
</prevsection>
<citsent citstr=" P00-1071 ">
to find the answer to question several steps must be taken, as reported in (abney et al, 2000) (<papid> A00-1041 </papid>moldovan et al, 2000) (<papid> P00-1071 </papid>srihari and li, 2000): first, the question semantics needs to be cap tured.</citsent>
<aftsection>
<nextsent>this translates into identifying (i) the expected answer type and (ii) the question keywords that can be used to retrieve text passages where the answer may be found. secondly, the index of the document collection must be used to identify the text passages of interest.
</nextsent>
<nextsent>the retrieval method either employs special operators or simply modifies boolean or vector retrieval.
</nextsent>
<nextsent>since the expected answer type is known at the time workshops organized by the national institute of standard sand technology (nist), designed to advance the state-of the-art in information retrieval (ir)of the retrieval, the quality of the text passages is greatly improved by filtering out those passages where concepts of the same category as the answer type are not present.
</nextsent>
<nextsent> thirdly, answer extraction takes place by combining several features that take into account the expected answer type.since the expected answer type is the only information used in all the phases of textual q/a,its recognition and usage is central to the performance of answer mining.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y391">
<title id=" W01-1206.xml">answer mining from online documents </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>for example, if the answer type of the trec question qt: how many dogs pull sled in the iditarod?
</prevsent>
<prevsent>is known to be number, we also need to be aware that this number must quantify the dogs harnessed to sled in the iditarod games and not the number of participants in the games.
</prevsent>
</prevsection>
<citsent citstr=" P96-1025 ">
capturing question or answer dependencies can be cast as straightforward process of mapping syntactic trees to sets of binary head modifier relationships, as first noted in (collins, 1996).<papid> P96-1025 </papid></citsent>
<aftsection>
<nextsent>given parse tree, the head-child of each syntactic constituent can be identified based on simple set of rules used to train syntactic parsers,cf.
</nextsent>
<nextsent>(collins, 1996).<papid> P96-1025 </papid></nextsent>
<nextsent>dependency relations are established between each leaf corresponding to thehead child and the leaves of its constituent sib vp sq wp pp what do most tourists visit in question et1: reims rbsvbp nns vb in nnp npnpwhnp sbarqparse: what visit reimstouristsmost question dependecies (et1): what do most tourists visit in reims?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y395">
<title id=" W01-1206.xml">answer mining from online documents </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>(a) (b) figure 1: example of trec test question lings that are not stop words, as illustrated by the mapping of figure 1(a) into figure 1(b).
</prevsent>
<prevsent>unlike in ir systems, question stems are considered content words.
</prevsent>
</prevsection>
<citsent citstr=" C00-1043 ">
when question dependencies are known (harabagiu et al, 2000) <papid> C00-1043 </papid>proposed technique of identifying the answer type based on the semantic category of the question stem and eventually of its most connected dependent concept.</citsent>
<aftsection>
<nextsent>for example, in the case of question et1, illustrated in figure 1, the answer type is determined by the ambiguous question stem what and the verb visit.
</nextsent>
<nextsent>the answer type is the object of the verb visit, which is placeof attraction or entertainment, defined by these mantic category landmark.
</nextsent>
<nextsent>the answer type replaces the question stem, generating the following dependency graph, that can be later unified with the answer dependency graph: most landmark tourists visit reims however syntactic dependencies vary across question reformulations or equivalent answers made possible by the productive nature of natural language.
</nextsent>
<nextsent>for example, the dependency structure of et2, reformulation of question et1 differs from the dependency structure of et1:due to the fact that verbs see and visit are synonyms (cf.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y397">
<title id=" W01-1206.xml">answer mining from online documents </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>the event of interest is complex event, like terrorism in latin america, joint ventures or management successions.
</prevsent>
<prevsent>an example of template-modeled question is: what management succes sions occurred at ibm in 1999in addition, questions may also ask about developments of events or trends that are usually answered by text summary.
</prevsent>
</prevsection>
<citsent citstr=" J98-3005 ">
since data producing these summaries can be sourced in different documents, summary fusion techniques as proposed in (radev and mckeown, 1998) <papid> J98-3005 </papid>can be employed.</citsent>
<aftsection>
<nextsent>template-based questions and summary asking inquiries cover most of the classes of question complexity proposed in (moldovan et al, 2000).<papid> P00-1071 </papid></nextsent>
<nextsent>although the topic of natural language open-domain question complexity needs further study, we consider herein the following classes of questions:  class 1: questions inquiring about entities, events, entity attributes (including number),event themes, event manners, event conditions and event consequences.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y402">
<title id=" W02-0608.xml">probabilistic context free grammars for phonology </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the second resource is manually written context-free grammar describing german and english syllable structure.
</prevsent>
<prevsent>we code the assumptions (similar to goldsmith (1995)) that the phonological material that can occur in the onsets or codas might differ depending on the syllable positions: word-initial, word-final, word-medial, versus monosyllabic words.
</prevsent>
</prevsection>
<citsent citstr=" W01-0518 ">
we train the context-free grammar for german on the transcribed and syllabified training corpus with simple supervised training method (muller, 2001<papid> W01-0518 </papid>a).</citsent>
<aftsection>
<nextsent>the main idea of the training method is that after grammar transformation step, the grammar together with parser can predict syllable boundaries of unknown phoneme strings.
</nextsent>
<nextsent>the trained model is evaluated on syllabification task showing high precision on test corpus.
</nextsent>
<nextsent>we exemplify that the method can be easily transferred to related languages (here english) by adding rules for missing phonemes tothe grammar.
</nextsent>
<nextsent>in an qualitative evaluation, we compare german and english syllable structure by interpreting the probability weights of the pre terminal july 2002, pp.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y476">
<title id=" W02-0608.xml">probabilistic context free grammars for phonology </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>for instance, kiraz and mobius (1998) develop multilingual syllabification models on the basis of pronunciation dictionary.
</prevsent>
<prevsent>partial english syllable structure is described by pierrehumbert (1994), who also used dictionary.
</prevsent>
</prevsection>
<citsent citstr=" P00-1029 ">
a more general model was introduced by muller et al (2000), <papid> P00-1029 </papid>who used clustering algorithm to induce english and german syllable classes.</citsent>
<aftsection>
<nextsent>however, the approach treats the onsets and codas as one string.
</nextsent>
<nextsent>in our method, we describe in more detail the internal structure of onsets and codas.
</nextsent>
<nextsent>our model has several advantages.
</nextsent>
<nextsent>(i) we believe that the syllable structure of all words occurring in certain language can be described by an elaborated context-free grammar.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y477">
<title id=" W02-0708.xml">balancing expressiveness and simplicity in an interlingua for task based dialogue </title>
<section> the c-star ii domain, database,.  </section>
<citcontext>
<prevsection>
<prevsent>the linguistic justi cation for an interlingua based on domain-actions is that many travel do main utterances contain xed, formulaic phrases (e.g., can you tell me; was wondering; how about; would you mind, etc.) that signal domain actions, but either do not translate literally into other languages or have meaning that is suciently indirect that the literal meaning is irrelevant for translation.
</prevsent>
<prevsent>to take two examples, how about as signal of suggestion does not translate into other languages with the words corresponding to how and about . also, would youmind might translate literally into some european languages as way of signaling request,but the literal meaning of minding is not relevant to the translation, only the fact that it signals politeness.
</prevsent>
</prevsection>
<citsent citstr=" W00-0203 ">
the measure of success for the domain-action based interlingua (as described in (levin et al,2000<papid> W00-0203 </papid>a)) is that (1) it covers the data in the star ii database with less than 8% no-tag rate, (2) inter-coder agreement across research sites is reasonably high: 82% for speech acts, 88% for concepts, and 65% for domain actions, and(3) end-to-end translation results using an analyzer and generator written at dierent sites were about the same as end-to-end translation results using an analyzer and generator written at the same site.</citsent>
<aftsection>
<nextsent>and interlingua the nespole interlingua has been underdevelopment for the last two years as part of the nespole project (http://nespole.itc.it).
</nextsent>
<nextsent>fig would like to make hotel reservation for the fourth through the seventh of july c:request-action+reservation+temporal+hotel (time=(start-time=md4, end-time=(md7, july))) figure 2: example of c-star ii interlingua representation ure 3 shows nespole dialogue.
</nextsent>
<nextsent>the ne spole domain does not include reservations and payments, but includes more detailed inquiries about hotels and facilities for ski vacations and summer vacations in val di fiemme, italy.
</nextsent>
<nextsent>(the tourism board of the trentino area is partner of the nespole project.)
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y478">
<title id=" W02-0708.xml">balancing expressiveness and simplicity in an interlingua for task based dialogue </title>
<section> more general domain actions replac-.  </section>
<citcontext>
<prevsection>
<prevsent>the interlingua was extended to cover colord, descriptions of two dimensional objects, and actions of showing.
</prevsent>
<prevsent>net: the nespole interlingua includes conventions for making new concept names based on wordnet synsets.
</prevsent>
</prevsection>
<citsent citstr=" H01-1041 ">
ing speci ones: for example, replacing hotel with accommodation.interlinguas based on domain actions contrast with interlinguas based on lexical semantics (dorr, 1993; lee et al, 2001; <papid> H01-1041 </papid>goodman and nirenburg, 1991).</citsent>
<aftsection>
<nextsent>a lexical-semantic interlingua includes representation of predicates and their arguments.
</nextsent>
<nextsent>for example, the sentence want to take vacation has predicate want with two arguments and to take vacation, which in turn has predicate take and two arguments, and vacation.
</nextsent>
<nextsent>of course, predicates like take may be represented as word senses that are less language-dependent like participate-in.
</nextsent>
<nextsent>the strength and weakness of the lexical-semantic approach is that it is less domain dependent than the domain-action approach.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y481">
<title id=" W02-0708.xml">balancing expressiveness and simplicity in an interlingua for task based dialogue </title>
<section> more general domain actions replac-.  </section>
<citcontext>
<prevsection>
<prevsent>(see table 2.)
</prevsent>
<prevsent>we conclude from this that the nespole interlingua is more expressive in that it covers more data.
</prevsent>
</prevsection>
<citsent citstr=" J94-4004 ">
language-independence of the nespole interlingua: we do not have numerical measure of language-independence, but we note that interlinguas based on domain actions are particularly suitable for avoiding translation mismatches (dorr, 1994), <papid> J94-4004 </papid>particularly head switching mismatches (e.g., just arrived and je vient dar river where the meaning of recent past is expressed by an adverb just or syntactic verb vient (venir).)</citsent>
<aftsection>
<nextsent>interlinguas basedon domain actions resolve head-switching mismatches by identifying the types of meanings that are often involved in mismatches | modality, evidentiality, disposition, and so on | and assigning them representation that abstracts away from predicate argument structure.
</nextsent>
<nextsent>interlinguas based on domain actions also neutralize the dierent ways of expressing indirect speech acts within and across languages (for example, would you mind..., was wondering ifyou could...., and please.... as ways of requesting an action).
</nextsent>
<nextsent>although nespole domain actions are more general than c-star ii domain actions, they maintain language independence by abstracting away from predicate-argument structure.
</nextsent>
<nextsent>simplicity and cross-site reliability of thenespole interlingua: simplicity of an interlingua is measured by cross-site reliability in would like to make hotel reservation for the fourth through the seventh of july c-star ii interlingua: c:request-action+reservation+temporal+hotel (time=(start-time=md4, end-time=(md7, july))) nespole interlingua: c:give-information+disposition+reservation+accommodation (disposition=(who=i, desire), reservation-spec=(reservation, identifiability=no), accommodation-spec=hotel, object-time=(start-time=(md=4), end-time=(md=7, month=7, incl-excl=inclusive)))} figure 4: example of nespole interlingua representationinter-coder agreement and end-to-end translation performance.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y482">
<title id=" W02-0708.xml">balancing expressiveness and simplicity in an interlingua for task based dialogue </title>
<section> more general domain actions replac-.  </section>
<citcontext>
<prevsection>
<prevsent>simplicity and cross-site reliability of thenespole interlingua: simplicity of an interlingua is measured by cross-site reliability in would like to make hotel reservation for the fourth through the seventh of july c-star ii interlingua: c:request-action+reservation+temporal+hotel (time=(start-time=md4, end-time=(md7, july))) nespole interlingua: c:give-information+disposition+reservation+accommodation (disposition=(who=i, desire), reservation-spec=(reservation, identifiability=no), accommodation-spec=hotel, object-time=(start-time=(md=4), end-time=(md=7, month=7, incl-excl=inclusive)))} figure 4: example of nespole interlingua representationinter-coder agreement and end-to-end translation performance.
</prevsent>
<prevsent>at the time of writing this paper we have not conducted cross-site inter-coderagreement experiments using the nespole interlingua.
</prevsent>
</prevsection>
<citsent citstr=" W02-0717 ">
we have, however, conducted cross site evaluations (lavie et al, 2002), <papid> W02-0717 </papid>in which the analyzer and generator were written at dier ent sites.</citsent>
<aftsection>
<nextsent>experiments at the end of c-star iishowed that cross-site evaluations were comparable to intra-site evaluations (analyzer and generator written at the same site) (levin et al, 2000<papid> W00-0203 </papid>b).</nextsent>
<nextsent>nespole evaluations so far show loss of cross-site reliability: intra-site evaluations are noticeably better than cross-site evaluations, as reported in (lavie et al, 2002).<papid> W02-0717 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y486">
<title id=" W01-1621.xml">reconciling initiative and discourse structure </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the question is, what should the dialogue manager pay attention to in order to accomplish this?
</prevsent>
<prevsent>two areas of research have shaped our understanding of what happens in dialogue: research in dialogue structure and in mixed initiative.
</prevsent>
</prevsection>
<citsent citstr=" J86-3001 ">
grosz and sidner (1986) <papid> J86-3001 </papid>proposed theory of discourse structure to account for why an utterance was said and what was meant by it.their theory has three components: linguistic structure, intentional structure and atten tional state.</citsent>
<aftsection>
<nextsent>intentions are key to accounting for discourse structure, de ning discourse coherence, and \providing coherent conceptualization of the term `discourse  itself.
</nextsent>
<nextsent>the intentional structure describes the purpose ofthe discourse as whole, and the relationship of the purpose of each discourse segment to the main discourse purpose or other discourse segment purposes.
</nextsent>
<nextsent>all utterances within segment contribute to the purpose of that segment.
</nextsent>
<nextsent>this theory, however, does not comment on initiative within the segment.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y488">
<title id=" W01-1621.xml">reconciling initiative and discourse structure </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>hence, it underspeci es what speakers can do in dialogue.
</prevsent>
<prevsent>research in initiative works to account for which speaker is driving the conversation atany given point.
</prevsent>
</prevsection>
<citsent citstr=" P88-1015 ">
for example, in question answer pair, the speaker asking the question is said to have the initiative (whittaker and stenton, 1988; <papid> P88-1015 </papid>walker and whittaker, 1990; <papid> P90-1010 </papid>novick and sutton, 1997).</citsent>
<aftsection>
<nextsent>whittaker and stenton segmented dialogues where initiative shifts from one speaker to the other.
</nextsent>
<nextsent>they found that initiative  did not alternate from speaker to speaker on turn by turn basis, but that there were long sequences of turns in which [initiative] remained with one speaker.
</nextsent>
<nextsent>in mixed initiative system, the dialogue manager needs to track initiative in order to know when the system should add signi cant content, and when it should let the user take over.
</nextsent>
<nextsent>however, no theory has oered good account of why speaker would want to take the initiative, or keep it once they have it.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y491">
<title id=" W01-1621.xml">reconciling initiative and discourse structure </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>hence, it underspeci es what speakers can do in dialogue.
</prevsent>
<prevsent>research in initiative works to account for which speaker is driving the conversation atany given point.
</prevsent>
</prevsection>
<citsent citstr=" P90-1010 ">
for example, in question answer pair, the speaker asking the question is said to have the initiative (whittaker and stenton, 1988; <papid> P88-1015 </papid>walker and whittaker, 1990; <papid> P90-1010 </papid>novick and sutton, 1997).</citsent>
<aftsection>
<nextsent>whittaker and stenton segmented dialogues where initiative shifts from one speaker to the other.
</nextsent>
<nextsent>they found that initiative  did not alternate from speaker to speaker on turn by turn basis, but that there were long sequences of turns in which [initiative] remained with one speaker.
</nextsent>
<nextsent>in mixed initiative system, the dialogue manager needs to track initiative in order to know when the system should add signi cant content, and when it should let the user take over.
</nextsent>
<nextsent>however, no theory has oered good account of why speaker would want to take the initiative, or keep it once they have it.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y501">
<title id=" W01-1621.xml">reconciling initiative and discourse structure </title>
<section> discourse structure and.  </section>
<citcontext>
<prevsection>
<prevsent>each discourse segment has purpose, and the purpose of each segment contributes to the purpose of its parent.
</prevsent>
<prevsent>intentional structure is key to understanding what the discourse is about and explains its coherency.
</prevsent>
</prevsection>
<citsent citstr=" J97-1006 ">
sub dialogue coding: in our study, the rst author segmented dialogues into sub dialogues based on the purpose of the utterance (smith and gordon, 1997; <papid> J97-1006 </papid>traum and hinkelman, 1992).</citsent>
<aftsection>
<nextsent>we established two classes of subdialogues: task sub dialogues, segments that describe subtasks in the dialogue, and clari cation sub dialogues, local segments that clarify gap in understanding, either to request missing information or to supply missing information to the other speaker.
</nextsent>
<nextsent>the segment initiator gives the rst utterance in the segment and establishes its purpose.
</nextsent>
<nextsent>1 the left side of figure 1 gives an example of discourse segment (or subdialogue) with two embedded clari cation sub dialogues, and the segment initiator for each subdialogue.
</nextsent>
<nextsent>generally, we found the dialogue structure in the dialogues we analyzed to be quite at, with few embedded structures.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y524">
<title id=" W01-1621.xml">reconciling initiative and discourse structure </title>
<section> future work.  </section>
<citcontext>
<prevsection>
<prevsent>this scheme annotates the forward and backward-looking functions of each utterance, from which initiative can be derived.
</prevsent>
<prevsent>reliable interco der agreement has been obtained with this coding scheme.
</prevsent>
</prevsection>
<citsent citstr=" J97-1005 ">
for coding discourse structure, several schemes have been proposed (passonneau and litman, 1997; <papid> J97-1005 </papid>flammia, 1998; nakatani et al, 1995;traum and nakatani, 1999) <papid> W99-0313 </papid>ranging from coding at segmentation on monologues to hierarchical segmentation of dialogues.</citsent>
<aftsection>
<nextsent>we will use these annotation schemes as foundation, and monitor our annotation results to ensure we achieve good interco der reliability.our theory necessitates that we better understand the structure of discourse, how it isbuilt, and the actions and rules that discourse manager can use to aect the discourse structure.
</nextsent>
<nextsent>we also need to understand the reasoning process that determines whether participant will make an other-contribution or start new subdialogue.
</nextsent>
<nextsent>since dialogue is collaborative eort (cohen and levesque, 1994; clark and wilkes-gibbs, 1986), we also need to explore how the participants collaborate on the discourse structure.
</nextsent>
<nextsent>the authors gratefully acknowledge funding from the intel research council.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y525">
<title id=" W01-1621.xml">reconciling initiative and discourse structure </title>
<section> future work.  </section>
<citcontext>
<prevsection>
<prevsent>this scheme annotates the forward and backward-looking functions of each utterance, from which initiative can be derived.
</prevsent>
<prevsent>reliable interco der agreement has been obtained with this coding scheme.
</prevsent>
</prevsection>
<citsent citstr=" W99-0313 ">
for coding discourse structure, several schemes have been proposed (passonneau and litman, 1997; <papid> J97-1005 </papid>flammia, 1998; nakatani et al, 1995;traum and nakatani, 1999) <papid> W99-0313 </papid>ranging from coding at segmentation on monologues to hierarchical segmentation of dialogues.</citsent>
<aftsection>
<nextsent>we will use these annotation schemes as foundation, and monitor our annotation results to ensure we achieve good interco der reliability.our theory necessitates that we better understand the structure of discourse, how it isbuilt, and the actions and rules that discourse manager can use to aect the discourse structure.
</nextsent>
<nextsent>we also need to understand the reasoning process that determines whether participant will make an other-contribution or start new subdialogue.
</nextsent>
<nextsent>since dialogue is collaborative eort (cohen and levesque, 1994; clark and wilkes-gibbs, 1986), we also need to explore how the participants collaborate on the discourse structure.
</nextsent>
<nextsent>the authors gratefully acknowledge funding from the intel research council.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y526">
<title id=" W00-0310.xml">using dialogue representations for concepttospeech generation </title>
<section> theoretical foundations.  </section>
<citcontext>
<prevsection>
<prevsent>further, it embodies and extends theoretical work on into national meaning in more general, robust and rigorous way than earlier cts systems, in an architecture that reflects compositional aspects of dialogue and intonation interpretation.
</prevsent>
<prevsent>in this work, we implement and extend the com-positional theory of into national meaning proposed by pierrehumbert and hirschberg (1986), pierrehumbert and hirschberg (1990), who sought identify correspondences between the bell laboratories, lucent technologies 600 mountain avenue.
</prevsent>
</prevsection>
<citsent citstr=" J86-3001 ">
murray hill, nj 07974 usa {chn j encc}research, bell-labs, com grosz and sidner (1986) <papid> J86-3001 </papid>computational model of dis-course interpretation and pierrehumbert prosodic grammar for american english (1980).</citsent>
<aftsection>
<nextsent>in the present work, certain aspects of the orig-inal theories are modified and adapted to the ar-chitecture of the dialogue system in which the cts component is embedded.
</nextsent>
<nextsent>below, we present he im-portant fundamental definitions and principles of in-tonation underlying our cts system.
</nextsent>
<nextsent>2.1 intonat ional system.
</nextsent>
<nextsent>in our cts system, the prosodic elements that are computed are based on the into national system of pierrehumbert (1980), who defined formal lan-guage for describing american english intonation using the following regular grammar: inton phrase ---~ (interm phrase) + bndry tone interm phrase ~ (pitch acc)+ phrase ace major phrases, or inlonational phrases, are made up of one or more minor phrases, or inlermediale phrases.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y527">
<title id=" W00-0310.xml">using dialogue representations for concepttospeech generation </title>
<section> theoretical foundations.  </section>
<citcontext>
<prevsection>
<prevsent>2.2 intonat ional meaning.
</prevsent>
<prevsent>theoretical work on into national meaning has at-tempted to relate the grammatical ements of pier-rehumbert system - - pitch accent, phrase accent and boundary tone, to interpretive processes at dif-ferent levels of discourse and dialogue structure.
</prevsent>
</prevsection>
<citsent citstr=" P86-1021 ">
hirschberg and pierrehumbert (1986) <papid> P86-1021 </papid>conjectured that the absence or presence of accentuation conveys discourse focus status, while the tonal properties of the accent itself (i.e. pitch accent ype) convey se-mantic focus information.</citsent>
<aftsection>
<nextsent>48 mimic: user: mimic: user: mimic: hello this is mimic the movie information system how can help you where in hoboken is october sky playing october sky is playing at hoboken cinema in hoboken can help you with anything else when is it playing there october sky is playing at hoboken cinema in hoboken at 3:45pm, 5:50pm, 7:lopm, and lopm can help you with anything else figure 1: mimic dialogue.
</nextsent>
<nextsent>in later work, pitch accent type was said to express whether the accented information was in-tended by the speaker to be  predicated  or not by the hearer (pierrehumbert and hirschberg, 1990).
</nextsent>
<nextsent>nompredicated-~forernation was said to bear low- star accentuation (l*-, l*+h, h+l*), while predi-cated information would be marked by high-star ac-cents (h*, l+h*, h*+l).
</nextsent>
<nextsent>the theory further stated that l*+h conveys uncertainty or lack of speaker commitment to the expressed propositional content, while l+h* marks correction or contrast.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y528">
<title id=" W00-0310.xml">using dialogue representations for concepttospeech generation </title>
<section> systems foundations.  </section>
<citcontext>
<prevsection>
<prevsent>our task is to improve the communicative compe-tence of spoken dialogue agent, by making re-course to our knowledge of into national meaning, di-alogue processing and relations between the two.
</prevsent>
<prevsent>of course, worthwhile cts system must also outper-form out-of-the-box text-to-speech (tts) systems that may determine prosodic mark-up in linguisti-cally sophisticated ways.
</prevsent>
</prevsection>
<citsent citstr=" P98-2155 ">
as in (nakatani, 1998), <papid> P98-2155 </papid>we take the prosodic output of an advanced research system that implements he pierrehumbert theory of intonation, namely the bell labs tts system, as our baseline xperimental system to be enhanced by cts algorithms.</citsent>
<aftsection>
<nextsent>we embed the cts system in mimic, working spoken dialogue system repre-senting state-of-the-art dialogue management prac-tices, to develop cts algorithms that can be eventu-ally realistically evaluated using task-based perfor-mance metrics.
</nextsent>
<nextsent>3.1 dialogue system: mixed-init iat ive.
</nextsent>
<nextsent>movie information consultant (mimic) the dialogue system whose baseline speech gen-eration capabilities we enhance is the mixed- initiative movie information consultant (mimic) (chu-carroll, 2000).
</nextsent>
<nextsent>mimic  provides movie list-ing information involving knowledge about towns, theaters, movies and showtimes, as demonstrated in figure 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y529">
<title id=" W00-0310.xml">using dialogue representations for concepttospeech generation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>together, mimic-cts computation of accentuation, pitch accent ype and dialogue prosody constitutes the most general and complete implementation a compositional theory of into national meaning in cts system to date.
</prevsent>
<prevsent>nevertheless, elements of handful of previ-ous cts systems support the approaches taken in mimic-cts toward conveying semantic, task and dialogue level meaning.
</prevsent>
</prevsection>
<citsent citstr=" P88-1023 ">
for example, the di-rection assistant system (davis and hirschberg, 1988) <papid> P88-1023 </papid>mapped hand-crafted route grammar to discourse structure for generated irections.</citsent>
<aftsection>
<nextsent>the discourse structure determined accentuation, with deaccenting ofdiscourse-old entities realized (by lex- ically identical morphs) in the current or previous discourse segment.
</nextsent>
<nextsent>other material was assigned ac-centuation based on lexical category information, with the exception that certain contrastive cases of accenting, such as left versus right, were stipulated for the domain.
</nextsent>
<nextsent>accent assignment in the sundial travel infor-mation system (house and yond, 1990) also relied on discourse and task models.
</nextsent>
<nextsent>mutually known en-tities, said to be in negative focus, were deaccented; entities in the current task space, in referring focus, received (possibly contrastive) accenting; and enti-ties of the same type as previously mentioned ob-ject, were classified-as in either referring or emphatic focus, depending on the dialogue act~ in the cases of corrective situations or repeated system-intitiated queries, the contrasting or corrective items were em-phatically accented.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y530">
<title id=" W01-1017.xml">the automatic generation of formal annotations in a multimedia indexing and searching environment </title>
<section> formal annotations for the soccer.  </section>
<citcontext>
<prevsection>
<prevsent>the prototype will also be tested by tv professionals and sport journalists, who will report on its practicability for the creation and management of their programme and information material.
</prevsent>
<prevsent>the principles and methods derived from this domain can be applied to other as well.
</prevsent>
</prevsection>
<citsent citstr=" A00-1033 ">
this has been shown already in the context of text-basedinformation extraction (ie), for which methodologies for fast adaptation to new domains have been developed (see the muc conferences and (neumann et al, 2000)).<papid> A00-1033 </papid></citsent>
<aftsection>
<nextsent>and generally speaking the use of ie for automatic annotation of multimedia document has the advantage of providing, besides the results of the (shallow)syntactic processing, accurate semantic (or con tent/conceptual) information (and thus potential annotation) for specific predefined domains, sincea mapping from the linguistically analyzed relevant text parts can be mapped onto an unambiguous conceptual description6 . thus in sense it can be assumed that ie is supporting the word sense disambiguation task.it is also commonly assumed (see among others (cunningham, 1999)) that ie occupies an intermediate place between information retrieval (with few linguistic knowledge involved) andtext understanding (involving the full deep linguistic analysis and being still not realized for the time being.).
</nextsent>
<nextsent>ie being robust but offering only partial (but mostly accurate) syntactic and content analysis, it can be said that this language technology is actually filling the gap between availablelow-level annotated/indexed documents and corpora and the desirable full content annotation of those documents and corpora.
</nextsent>
<nextsent>this is the reason why mumis has chosen this technology for providing automatic annotation (at distinct linguistic and domain-specific levels) of multimedia material, allowing thus to add query able content in formation?
</nextsent>
<nextsent>to this material.75we would like to thank at this place the various institutions making available various textual, audio and video data.6this topic has already been object of workshop discussing the relations between ie and corpus linguistics (mcnaught, 2000).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y531">
<title id=" W02-0701.xml">corpus centered computation </title>
<section> statistical machine translation (smt): smt.  </section>
<citcontext>
<prevsection>
<prevsent>learns models for translation from corpora and dictionaries and searches at run-time for the best translation according to the models (brown et al, 1993; knight, 1997; ney et al, 2000).
</prevsent>
<prevsent>we have developed two ebmt systems and one smt system.
</prevsent>
</prevsection>
<citsent citstr=" W01-1401 ">
an ebmt, d3 sumita (2001) <papid> W01-1401 </papid>proposed d3 (dp-match driven transducer).</citsent>
<aftsection>
<nextsent>the characteristics of d3 are different from previous ebmt approaches: a) most ebmt proposals assume syntactic parsing and bilingual tree-banks, but d3 does not; b) most ebmt proposals divide the translation process in two, i.e. learning of translation patterns in advance and application of the translation patterns, but d3 generates translation patterns on the fly according to the input and the retrieved translation examples as needed.
</nextsent>
<nextsent>as shown in figure 1, our language resources are [i] bilingual corpus, in which sentences are aligned beforehand; [ii] bilingual dictionary, which is used for generating target sentences; and [iii] thesauri of both languages, which are used for incorporating the semantic distance between words into the distance between word sequences.
</nextsent>
<nextsent>furthermore, [ii] and [iii] are also used for word alignment.
</nextsent>
<nextsent>generate select substitute retrieve aligned bilingual corpus [i] sentence [ii] bilingual dictionary [iii] thesauri input sentence target sentence figure 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y532">
<title id=" W02-0701.xml">corpus centered computation </title>
<section> the toeic (test of english for international.  </section>
<citcontext>
<prevsection>
<prevsent>ebmts on the same corpus rank(s) a+b a+b+c d3(je) 47% 66% 77% hpat(ej) 50% 61% 71% finally, it became clear that word-based smt, revival of the direct method of the 50s, is suitable for pairs of european languages but not for japanese and english.
</prevsent>
<prevsent>this is because word-based smt cannot capture the major differences such as word order between japanese and english.
</prevsent>
</prevsection>
<citsent citstr=" J00-1004 ">
several organizations (yamada et al, 2001; alshawi et al, 2000) <papid> J00-1004 </papid>are pursuing syntax-based smt.</citsent>
<aftsection>
<nextsent>we plan to join the race.
</nextsent>
<nextsent>which is suitable for japanese and english, syntax-based smt or ebmt?
</nextsent>
<nextsent>5 5.1 combination of evaluation and translation we are researching automatic evaluation of machine translation outputs and multiple paradigms for machine translation simultaneously.
</nextsent>
<nextsent>together, they have synegistic effects as explained below.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y533">
<title id=" W02-0701.xml">corpus centered computation </title>
<section> the toeic (test of english for international.  </section>
<citcontext>
<prevsection>
<prevsent>subsets of input sentences whose translation is a-ranked we are investigating methods to utilize techniques of automatic evaluation for selector (figure 5).
</prevsent>
<prevsent>mt1 mt2 mt3 selector figure 5.
</prevsent>
</prevsection>
<citsent citstr=" C02-1076 ">
selector for multi-engine mt in our pilot experiment, our selectors (akiba et al., 2002; <papid> C02-1076 </papid>yasuda et al, 2002) outperformed not only the component systems but also conventional selector using n-gram (callison-burch et al, 2001).</citsent>
<aftsection>
<nextsent>combination of paraphrasing and translation 6 6.1 we are automating extraction of paraphrase knowledge from bilingual corpus.
</nextsent>
<nextsent>in this section, we introduce its application to improve the performance of corpus-based translation by using smt as touchstone.
</nextsent>
<nextsent>extraction of synonymous expressions we propose an automatic paraphrasing method that exploits knowledge from bilingual corpora (shimohata et al, 2002).
</nextsent>
<nextsent>synonymous expressions are defined as sequence of variant words with surrounding common words.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y534">
<title id=" W01-0909.xml">a cross comparison of two clustering methods </title>
<section> semantic domain learning.  </section>
<citcontext>
<prevsection>
<prevsent>text segmentation is based on the use of the collocation network.
</prevsent>
<prevsent>a topic is detected by computing cohesion value for each word resulting from the relations found in the network between these words and their neighbors in text.
</prevsent>
</prevsection>
<citsent citstr=" P93-1041 ">
as inkozimas work (kozima, 1993), <papid> P93-1041 </papid>this computation operates on words belonging to focus window that is moved all over the text.the cohesion values lead to build graph and by successive transformations applied to it, texts are automatically divided in discourse segments.</citsent>
<aftsection>
<nextsent>such method leads to delimit small segments, whose size is equivalent to paragraph, i. e. capable of retrieving topic variations in short texts,as new swires for example.
</nextsent>
<nextsent>table 1 shows an extract of the words belonging to cohesive segment about dedication of book.
</nextsent>
<nextsent>2.2 semantic domain learning in.
</nextsent>
<nextsent>segapsithlearning semantic domain consists of aggregating all the most cohesive thematic units, tus, that are related to same subject, i. e. same kind of situation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y535">
<title id=" W02-0102.xml">an interactive spreadsheet for teaching the forward backward algorithm </title>
<section> the subject matter.  </section>
<citcontext>
<prevsection>
<prevsent>among topics in natural language processing, the forward-backward or baum-welch algorithm (baum, 1972) is particularly difficult to teach.
</prevsent>
<prevsent>the algorithm estimates the parameters of ahidden markov model (hmm) by expectation maximization (em), using dynamic programming to carry out the expectation steps efficiently.hmms have long been central in speech recognition (rabiner, 1989).
</prevsent>
</prevsection>
<citsent citstr=" A88-1019 ">
their application to part of-speech tagging (church, 1988; <papid> A88-1019 </papid>derose, 1988) <papid> J88-1003 </papid>kicked off the era of statistical nlp, and they have found additional nlp applications to phrase chunking, text segmentation, word-sense disambiguation, and information extraction.the algorithm is also important to teach for pedagogical reasons, as the entry point to family ofem algorithms for unsupervised parameter estima tion.</citsent>
<aftsection>
<nextsent>indeed, it is an instructive special case of (1)the inside-outside algorithm for estimation of probabilistic context-free grammars; (2) belief propagation for training singly-connected bayesian networks and junction trees (pearl, 1988; lauritzen,1995); (3) algorithms for learning alignment models such as weighted edit distance; (4) general finite state parameter estimation (eisner, 2002).<papid> P02-1001 </papid></nextsent>
<nextsent>before studying the algorithm, students should first have worked with some if not all of the key ideas in simpler settings.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y536">
<title id=" W02-0102.xml">an interactive spreadsheet for teaching the forward backward algorithm </title>
<section> the subject matter.  </section>
<citcontext>
<prevsection>
<prevsent>among topics in natural language processing, the forward-backward or baum-welch algorithm (baum, 1972) is particularly difficult to teach.
</prevsent>
<prevsent>the algorithm estimates the parameters of ahidden markov model (hmm) by expectation maximization (em), using dynamic programming to carry out the expectation steps efficiently.hmms have long been central in speech recognition (rabiner, 1989).
</prevsent>
</prevsection>
<citsent citstr=" J88-1003 ">
their application to part of-speech tagging (church, 1988; <papid> A88-1019 </papid>derose, 1988) <papid> J88-1003 </papid>kicked off the era of statistical nlp, and they have found additional nlp applications to phrase chunking, text segmentation, word-sense disambiguation, and information extraction.the algorithm is also important to teach for pedagogical reasons, as the entry point to family ofem algorithms for unsupervised parameter estima tion.</citsent>
<aftsection>
<nextsent>indeed, it is an instructive special case of (1)the inside-outside algorithm for estimation of probabilistic context-free grammars; (2) belief propagation for training singly-connected bayesian networks and junction trees (pearl, 1988; lauritzen,1995); (3) algorithms for learning alignment models such as weighted edit distance; (4) general finite state parameter estimation (eisner, 2002).<papid> P02-1001 </papid></nextsent>
<nextsent>before studying the algorithm, students should first have worked with some if not all of the key ideas in simpler settings.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y537">
<title id=" W02-0102.xml">an interactive spreadsheet for teaching the forward backward algorithm </title>
<section> the subject matter.  </section>
<citcontext>
<prevsection>
<prevsent>the algorithm estimates the parameters of ahidden markov model (hmm) by expectation maximization (em), using dynamic programming to carry out the expectation steps efficiently.hmms have long been central in speech recognition (rabiner, 1989).
</prevsent>
<prevsent>their application to part of-speech tagging (church, 1988; <papid> A88-1019 </papid>derose, 1988) <papid> J88-1003 </papid>kicked off the era of statistical nlp, and they have found additional nlp applications to phrase chunking, text segmentation, word-sense disambiguation, and information extraction.the algorithm is also important to teach for pedagogical reasons, as the entry point to family ofem algorithms for unsupervised parameter estima tion.</prevsent>
</prevsection>
<citsent citstr=" P02-1001 ">
indeed, it is an instructive special case of (1)the inside-outside algorithm for estimation of probabilistic context-free grammars; (2) belief propagation for training singly-connected bayesian networks and junction trees (pearl, 1988; lauritzen,1995); (3) algorithms for learning alignment models such as weighted edit distance; (4) general finite state parameter estimation (eisner, 2002).<papid> P02-1001 </papid></citsent>
<aftsection>
<nextsent>before studying the algorithm, students should first have worked with some if not all of the key ideas in simpler settings.
</nextsent>
<nextsent>markov models can be introduced through n-gram models or probabilistic finite-state automata.
</nextsent>
<nextsent>em can be introduced through simpler tasks such as soft clustering.
</nextsent>
<nextsent>global optimization through dynamic programming can be introduced in other contexts such as probabilistic cky parsing or edit distance.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y538">
<title id=" W02-0102.xml">an interactive spreadsheet for teaching the forward backward algorithm </title>
<section> follow-up assignment.  </section>
<citcontext>
<prevsection>
<prevsent>37a: use more states.
</prevsent>
<prevsent>four states would suffice to distinguish hot/2, cold/2, hot/not2, and cold/not2 days.38a: there is no guarantee that and will continue to distinguish nouns and verbs after reestimation.
</prevsent>
</prevsection>
<citsent citstr=" A00-1031 ">
they will evolve to make whatever distinctions help to predict the word sequence.39advanced students might also want to read about modern supervised trigram tagger (brants, 2000), <papid> A00-1031 </papid>or the mixed results when one actually trains trigram taggers by em (merialdo, 1994).<papid> J94-2001 </papid></citsent>
<aftsection>
<nextsent>furthermore, students could check their ice cream output against the spreadsheet, and track down basic bugs by comparing their intermediate results to the spreadsheets. they reported this to be very useful.
</nextsent>
<nextsent>presumably it helps learning when students actually find their bugs before handing in the assignment, and when they are able to isolate their misconceptions on their own.
</nextsent>
<nextsent>it also made office hours and grading much easier for the teaching assistant.
</nextsent>
<nextsent>the spreadsheet (in microsoft excel) and assignment are available at http://www.cs.jhu.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y539">
<title id=" W02-0102.xml">an interactive spreadsheet for teaching the forward backward algorithm </title>
<section> follow-up assignment.  </section>
<citcontext>
<prevsection>
<prevsent>37a: use more states.
</prevsent>
<prevsent>four states would suffice to distinguish hot/2, cold/2, hot/not2, and cold/not2 days.38a: there is no guarantee that and will continue to distinguish nouns and verbs after reestimation.
</prevsent>
</prevsection>
<citsent citstr=" J94-2001 ">
they will evolve to make whatever distinctions help to predict the word sequence.39advanced students might also want to read about modern supervised trigram tagger (brants, 2000), <papid> A00-1031 </papid>or the mixed results when one actually trains trigram taggers by em (merialdo, 1994).<papid> J94-2001 </papid></citsent>
<aftsection>
<nextsent>furthermore, students could check their ice cream output against the spreadsheet, and track down basic bugs by comparing their intermediate results to the spreadsheets. they reported this to be very useful.
</nextsent>
<nextsent>presumably it helps learning when students actually find their bugs before handing in the assignment, and when they are able to isolate their misconceptions on their own.
</nextsent>
<nextsent>it also made office hours and grading much easier for the teaching assistant.
</nextsent>
<nextsent>the spreadsheet (in microsoft excel) and assignment are available at http://www.cs.jhu.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y540">
<title id=" W01-0810.xml">linear order as higher level decision information structure in strategic and tactical generation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>information structure is means that speaker employs to indicate that some parts of sentence meaning are context-dependent (given?),and that others are context-affecting (new?).
</prevsent>
<prevsent>information structure is therefore an inherent aspect of sentence meaning, and it contributes in an important way to the overall coherence of text.
</prevsent>
</prevsection>
<citsent citstr=" A97-1039 ">
while it is commonly accepted that information structuring is major source of constraints for the organization of given content in particular linear order in many languages, there is very little work in natural language generation that explicitly models this relation.from practical perspective, in the most commonly employed generation systems such as kpml, fuf (elhadad, 1993; elhadad and robin, 1997) or realpro (lavoie and rambow, 1997), <papid> A97-1039 </papid>linear ordering comes as by-product of other grammatical choices.</citsent>
<aftsection>
<nextsent>this is fine for tactical generation components and it is sufficient for languages with grammatically determined wo(fixed?
</nextsent>
<nextsent>wo languages), such as english or chinese.
</nextsent>
<nextsent>however, most languages have some wovariability and this variation usually reflects information structure.
</nextsent>
<nextsent>when languages in which linear order is primarily pragmatically determined are involved, such as the slavonic languages we have dealt with, number of problems become immediately apparent.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y541">
<title id=" W01-0810.xml">linear order as higher level decision information structure in strategic and tactical generation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a comprehensive account of wo variation for natural language generation that is reusable across languages is thus required.
</prevsent>
<prevsent>such an account needsto represent linearization as an explicit decisionmaking process that involves both the representation of the language-specific linear ordering possibilites and the representation of the language specific (and possibly cross-linguistically valid) motivations for particular linearizations.
</prevsent>
</prevsection>
<citsent citstr=" W94-0314 ">
again, while the former is catered for in most tactical generation systems, only selected aspects of the latter have been dealt with and only for selected languages (e.g., (hoffman, 1994; <papid> W94-0314 </papid>hoffman, 1995; <papid> E95-1034 </papid>hakkani et al, 1996)).<papid> W96-0409 </papid></citsent>
<aftsection>
<nextsent>for example, (hoffman, 1994) <papid> W94-0314 </papid>proposes treatment of wo in turkish using categorial grammar framework (ccg, (steedman, 2000)) and relating this to steed mans (earlier) account of information structure (steedman, 1991).</nextsent>
<nextsent>how ever, the most important issue, that of providing an integrated account of how information structure guides the choice of (or, is realized by) linear ordering, is left unsolved (kruijff, 2001).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y542">
<title id=" W01-0810.xml">linear order as higher level decision information structure in strategic and tactical generation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a comprehensive account of wo variation for natural language generation that is reusable across languages is thus required.
</prevsent>
<prevsent>such an account needsto represent linearization as an explicit decisionmaking process that involves both the representation of the language-specific linear ordering possibilites and the representation of the language specific (and possibly cross-linguistically valid) motivations for particular linearizations.
</prevsent>
</prevsection>
<citsent citstr=" E95-1034 ">
again, while the former is catered for in most tactical generation systems, only selected aspects of the latter have been dealt with and only for selected languages (e.g., (hoffman, 1994; <papid> W94-0314 </papid>hoffman, 1995; <papid> E95-1034 </papid>hakkani et al, 1996)).<papid> W96-0409 </papid></citsent>
<aftsection>
<nextsent>for example, (hoffman, 1994) <papid> W94-0314 </papid>proposes treatment of wo in turkish using categorial grammar framework (ccg, (steedman, 2000)) and relating this to steed mans (earlier) account of information structure (steedman, 1991).</nextsent>
<nextsent>how ever, the most important issue, that of providing an integrated account of how information structure guides the choice of (or, is realized by) linear ordering, is left unsolved (kruijff, 2001).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y543">
<title id=" W01-0810.xml">linear order as higher level decision information structure in strategic and tactical generation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a comprehensive account of wo variation for natural language generation that is reusable across languages is thus required.
</prevsent>
<prevsent>such an account needsto represent linearization as an explicit decisionmaking process that involves both the representation of the language-specific linear ordering possibilites and the representation of the language specific (and possibly cross-linguistically valid) motivations for particular linearizations.
</prevsent>
</prevsection>
<citsent citstr=" W96-0409 ">
again, while the former is catered for in most tactical generation systems, only selected aspects of the latter have been dealt with and only for selected languages (e.g., (hoffman, 1994; <papid> W94-0314 </papid>hoffman, 1995; <papid> E95-1034 </papid>hakkani et al, 1996)).<papid> W96-0409 </papid></citsent>
<aftsection>
<nextsent>for example, (hoffman, 1994) <papid> W94-0314 </papid>proposes treatment of wo in turkish using categorial grammar framework (ccg, (steedman, 2000)) and relating this to steed mans (earlier) account of information structure (steedman, 1991).</nextsent>
<nextsent>how ever, the most important issue, that of providing an integrated account of how information structure guides the choice of (or, is realized by) linear ordering, is left unsolved (kruijff, 2001).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y545">
<title id=" W01-0810.xml">linear order as higher level decision information structure in strategic and tactical generation </title>
<section> information structure and strategic.  </section>
<citcontext>
<prevsection>
<prevsent>while construct inga text plan, the text planner constructs (rudimentary) discourse model that keeps track of the discourse entities introduced.text planning results in text plan and discourse model that serve as input to the sentence planner.
</prevsent>
<prevsent>the text plan is hierarchical structure, organizing the content into more linear fashion (see figure 3.2).
</prevsent>
</prevsection>
<citsent citstr=" H89-1022 ">
the sentence planner creates the input to the tactical generation phase as formulas of the sentence planning language (spl, (kasper, 1989)).<papid> H89-1022 </papid></citsent>
<aftsection>
<nextsent>the spl formulas express the bits of content identified by the text plans leaves, and can also group one or more leaves together (aggregation) depending on decisions taken by the text planner concerning discourse relations.
</nextsent>
<nextsent>most importantly, during this phase of planning what content is to be realized by sentence, the underlying information structure of that content is determined: whenever the sentence planner encounters piece of content that the discourse model notes as previously used, it marks the corresponding item in the spl formula as contextually bound (note that we are hereby making simplifying assumption that in the current version ofthe sentence planner we equate contextual bound ness with previous mention).
</nextsent>
<nextsent>the text planner can also choose particular textual organization and determine the element which should become the theme.
</nextsent>
<nextsent>if no particular element is chosen as the theme, the grammar chooses some element as the default theme.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y546">
<title id=" W01-0810.xml">linear order as higher level decision information structure in strategic and tactical generation </title>
<section> realizing information structure.  </section>
<citcontext>
<prevsection>
<prevsent>5.2 algorithm application.
</prevsent>
<prevsent>the algorithm described above has been implemented and used for generation of czech and english instructional texts.
</prevsent>
</prevsection>
<citsent citstr=" C00-1069 ">
the czech grammar resources used in tactical generation have been built up along with bulgarian and russian grammar resources as described in (kruijff et al, 2000), <papid> C00-1069 </papid>reusing the nigel grammar for english.</citsent>
<aftsection>
<nextsent>the original nigel grammar itself already combines the specification of ordering constraints in the grammar with the application of defaults.
</nextsent>
<nextsent>if an ordering is underspecified by the grammar, the defaults are applied.
</nextsent>
<nextsent>the defaults are static?, i.e. specified once and for all.
</nextsent>
<nextsent>the algorithm we have described replaces these static?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y547">
<title id=" W02-0401.xml">using maximum entropy for sentence extraction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this situation may be acceptable when the features used to model sentence extraction are simple.
</prevsent>
<prevsent>however, itwill rapidly become unacceptable when more sophisticated heuristics, with complicated interactions, are brought to bear upon the problem.
</prevsent>
</prevsection>
<citsent citstr=" C00-1012 ">
for example, boguraev and ne (2000<papid> C00-1012 </papid>a) argue that the quality of summarisation can be increased if lexical cohesion factors (rhetorical devices which help achieve cohesion between related document utterances) are modelled by sentence extraction system.</citsent>
<aftsection>
<nextsent>clearly such devices (for example, lexical repetition, ellipsis, coreference and so on) all contribute towards the general discourse structure of some text and furthermore are related to each other in non-obvious ways.
</nextsent>
<nextsent>maximum entropy (log-linear) models, on the other hand, do not make unnecessary independenceassumptions.
</nextsent>
<nextsent>within the maximum entropy framework, we are able to optimally integrate together whatever sources of knowledge we believe potentially to be useful for the task.
</nextsent>
<nextsent>should we use features that are bene cial, then the model will be able to exploit this fact.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y548">
<title id=" W02-0401.xml">using maximum entropy for sentence extraction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>should we use features that are bene cial, then the model will be able to exploit this fact.
</prevsent>
<prevsent>should we use features that are irrelevant, then again, the model will be able to notice this, and eectively ignore them.
</prevsent>
</prevsection>
<citsent citstr=" W96-0213 ">
models based on maximum entropy are therefore well suited to the sentence extraction task, and furthermore, yield competitive results on variety of language tasks (ratnaparkhi, 1996; <papid> W96-0213 </papid>berger et al, 1996; <papid> J96-1002 </papid>charniak, 1999; nigam et al., 1999).</citsent>
<aftsection>
<nextsent>in this paper, we outline conditional maximum philadelphia, july 2002, pp.
</nextsent>
<nextsent>1-8.
</nextsent>
<nextsent>association for computational linguistics.
</nextsent>
<nextsent>proceedings of the workshop on automatic summarization (including duc 2002), entropy classi cation model for sentence extraction.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y549">
<title id=" W02-0401.xml">using maximum entropy for sentence extraction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>should we use features that are bene cial, then the model will be able to exploit this fact.
</prevsent>
<prevsent>should we use features that are irrelevant, then again, the model will be able to notice this, and eectively ignore them.
</prevsent>
</prevsection>
<citsent citstr=" J96-1002 ">
models based on maximum entropy are therefore well suited to the sentence extraction task, and furthermore, yield competitive results on variety of language tasks (ratnaparkhi, 1996; <papid> W96-0213 </papid>berger et al, 1996; <papid> J96-1002 </papid>charniak, 1999; nigam et al., 1999).</citsent>
<aftsection>
<nextsent>in this paper, we outline conditional maximum philadelphia, july 2002, pp.
</nextsent>
<nextsent>1-8.
</nextsent>
<nextsent>association for computational linguistics.
</nextsent>
<nextsent>proceedings of the workshop on automatic summarization (including duc 2002), entropy classi cation model for sentence extraction.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y551">
<title id=" W01-1009.xml">gathering knowledge for a question answering system from heterogeneous information sources </title>
<section> initial prototype.  </section>
<citcontext>
<prevsection>
<prevsent>supervised use of natural language annotation falls short of the grandiose goal of accessing the entire world wide web, but is the practical and useful way to apply nl annotation until the transformational rule problem can be solved for unlimited domains.
</prevsent>
<prevsent>webnotator is prototype test-bed to evaluate the practicality of nl-based annotation and retrieval through web-based collaboration.
</prevsent>
</prevsection>
<citsent citstr=" P93-1016 ">
it provides ecient facilities for retrieving answers already stored within the knowledge base and scalable framework for ordinary users to contribute knowledge.the system analyzes natural language annotations to produce ternary expressions by postprocessing the results of minipar (lin, 1993; <papid> P93-1016 </papid>lin, 1994), <papid> C94-1079 </papid>fast and robust functional dependency parser that is freely available for non-commercial purposes.</citsent>
<aftsection>
<nextsent>the quality of the representational structures depends ultimately on the quality of whatever parser webnotator is made to access.
</nextsent>
<nextsent>in the current implementation, ternary expressions are not embedded, elements of ternary expressions are not indexed, and coreference is not detected.
</nextsent>
<nextsent>words are stemmed to their root form and morphological information is discarded.
</nextsent>
<nextsent>the system also implements version of transformational rules described above as simple forward-chaining rule-based system.using relational database, webnotator implements knowledge base that stores ternary expressions derived from annotations and their associated information segments.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y552">
<title id=" W01-1009.xml">gathering knowledge for a question answering system from heterogeneous information sources </title>
<section> initial prototype.  </section>
<citcontext>
<prevsection>
<prevsent>supervised use of natural language annotation falls short of the grandiose goal of accessing the entire world wide web, but is the practical and useful way to apply nl annotation until the transformational rule problem can be solved for unlimited domains.
</prevsent>
<prevsent>webnotator is prototype test-bed to evaluate the practicality of nl-based annotation and retrieval through web-based collaboration.
</prevsent>
</prevsection>
<citsent citstr=" C94-1079 ">
it provides ecient facilities for retrieving answers already stored within the knowledge base and scalable framework for ordinary users to contribute knowledge.the system analyzes natural language annotations to produce ternary expressions by postprocessing the results of minipar (lin, 1993; <papid> P93-1016 </papid>lin, 1994), <papid> C94-1079 </papid>fast and robust functional dependency parser that is freely available for non-commercial purposes.</citsent>
<aftsection>
<nextsent>the quality of the representational structures depends ultimately on the quality of whatever parser webnotator is made to access.
</nextsent>
<nextsent>in the current implementation, ternary expressions are not embedded, elements of ternary expressions are not indexed, and coreference is not detected.
</nextsent>
<nextsent>words are stemmed to their root form and morphological information is discarded.
</nextsent>
<nextsent>the system also implements version of transformational rules described above as simple forward-chaining rule-based system.using relational database, webnotator implements knowledge base that stores ternary expressions derived from annotations and their associated information segments.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y553">
<title id=" W00-1426.xml">can text structure be incompatible with rhetorical structure </title>
<section> introduction.  </section>
<citcontext>
<prevsection>
<prevsent>in example 10, the injunction you should not stop taking elixir obviously expresses the main intention of the author.
</prevsent>
<prevsent>however, the fact that the subordinated concession is placed after its main clause makes it available for further expansion.
</prevsent>
</prevsection>
<citsent citstr=" J92-4007 ">
the sometimes compet-ing informational nd intentional roles of dis-course segments have been at the centre of the debate over the nucleus-satellite distinction (moore and pollack, 1992; <papid> J92-4007 </papid>moser and moore, 1996; <papid> J96-3006 </papid>bateman and rondhius, 1997); the acces-sibility of discourse segments on the right fron-tier of discourse structure is phenomenon that has already been discussed by several re-searchers (webber, 1991; asher, 1993).</citsent>
<aftsection>
<nextsent>extra- position provides useful and sometimes im-portant means of rearranging complex material in an abstract discourse representation in order to satisfy the constraints posed by linear isation into text.
</nextsent>



</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y554">
<title id=" W00-1426.xml">can text structure be incompatible with rhetorical structure </title>
<section> introduction.  </section>
<citcontext>
<prevsection>
<prevsent>in example 10, the injunction you should not stop taking elixir obviously expresses the main intention of the author.
</prevsent>
<prevsent>however, the fact that the subordinated concession is placed after its main clause makes it available for further expansion.
</prevsent>
</prevsection>
<citsent citstr=" J96-3006 ">
the sometimes compet-ing informational nd intentional roles of dis-course segments have been at the centre of the debate over the nucleus-satellite distinction (moore and pollack, 1992; <papid> J92-4007 </papid>moser and moore, 1996; <papid> J96-3006 </papid>bateman and rondhius, 1997); the acces-sibility of discourse segments on the right fron-tier of discourse structure is phenomenon that has already been discussed by several re-searchers (webber, 1991; asher, 1993).</citsent>
<aftsection>
<nextsent>extra- position provides useful and sometimes im-portant means of rearranging complex material in an abstract discourse representation in order to satisfy the constraints posed by linear isation into text.
</nextsent>



</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y555">
<title id=" W00-1311.xml">detection of language model errors </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>various ways to improve language models were reported.
</prevsent>
<prevsent>first, the model has been extended with longer dependencies ( .g. trigram) (jelinek, 1991) and using non-contiguous dependencies, like trigger pairs (rosenfeid, 1994) or long distance n-gram language models (huang et al, 1993).
</prevsent>
</prevsection>
<citsent citstr=" J92-4003 ">
for better probability estimation, the model was extended to work with (hidden) word classes (brown et al, 1992, <papid> J92-4003 </papid>ward and issar, 1996).</citsent>
<aftsection>
<nextsent>a more error-driven approach is the use of hybrid language models, in which some detection mechanism (e.g. perplexity measures \[keene and kane, 1996\] or topic detection \[mahajan et al, 1999\]) selects or combines with more appropriate language model.
</nextsent>
<nextsent>for asian languages (e.g. chinese, japanese and korean) represented by ideographic haracters, language models are widely used in computer entry because these asian languages have large set of characters (in thousands) that the conventional keyboard is not designed for.
</nextsent>
<nextsent>apart from using speech and handwriting recognition for computer entry, language models for asian languages can be used for sentence-based keyboard input (e.g. lochovsky and chung, 1997), as well as detecting improper writing (e.g. dialect- specific words or expressions).
</nextsent>
<nextsent>unlike indo-european languages, words in these asian languages are not delimited by space and conventional approximate string matching techniques (wagner and fisher, 1974; oommen and zhang, 1974) in handwriting recognition are seldom used in asian language models.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y556">
<title id=" W02-0221.xml">training a dialogue act tagger for human human and human computer travel dialogues </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>wetrain and test the date tagger on various combinations of the darpa communicator june-2000 and october-2001 human-computer corpora, and the cmuhuman-human corpus in the travel planning domain.
</prevsent>
<prevsent>our results show that wecan achieve high accuracies on the human computer data, and surprisingly, that the human-computer data improves accuracy on the human-human data, when only small amounts of human-human training data are available.
</prevsent>
</prevsection>
<citsent citstr=" P98-1052 ">
recent research on dialogue is based on the assumption that dialogue acts provide useful wayof characterizing dialogue behaviors in both human human (hh) and human-computer (hc) dialogue (isard and carletta, 1995; shriberg et al, 2000; di eugenio et al, 1998; <papid> P98-1052 </papid>cattoni et al, 2001).</citsent>
<aftsection>
<nextsent>previous research has used dialogue act tagging for tasks such as improving recognition performance (shriberg et al., 2000), identifying important parts of dialogue(finke et al, 1998), evaluating and comparing spoken dialogue systems (walker et al, 2001<papid> P01-1066 </papid>c; cattoni et al, 2001; hastie et al, 2002), as constraint on nominal expression generation (jordan, 2000), and for comparing hh to hc dialogues (doran et al, 2001).<papid> W01-1607 </papid>our work builds directly on the previous application of the date (dialogue act tagging for evaluation) tagging scheme to the evaluation and comparison of darpa communicator dialogues.</nextsent>
<nextsent>the hypothesis underlying the use of dialogue act tagging in spoken dialogue evaluation is that systems dialogue behaviors have strong effect on its usabil ity.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y557">
<title id=" W02-0221.xml">training a dialogue act tagger for human human and human computer travel dialogues </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our results show that wecan achieve high accuracies on the human computer data, and surprisingly, that the human-computer data improves accuracy on the human-human data, when only small amounts of human-human training data are available.
</prevsent>
<prevsent>recent research on dialogue is based on the assumption that dialogue acts provide useful wayof characterizing dialogue behaviors in both human human (hh) and human-computer (hc) dialogue (isard and carletta, 1995; shriberg et al, 2000; di eugenio et al, 1998; <papid> P98-1052 </papid>cattoni et al, 2001).</prevsent>
</prevsection>
<citsent citstr=" P01-1066 ">
previous research has used dialogue act tagging for tasks such as improving recognition performance (shriberg et al., 2000), identifying important parts of dialogue(finke et al, 1998), evaluating and comparing spoken dialogue systems (walker et al, 2001<papid> P01-1066 </papid>c; cattoni et al, 2001; hastie et al, 2002), as constraint on nominal expression generation (jordan, 2000), and for comparing hh to hc dialogues (doran et al, 2001).<papid> W01-1607 </papid>our work builds directly on the previous application of the date (dialogue act tagging for evaluation) tagging scheme to the evaluation and comparison of darpa communicator dialogues.</citsent>
<aftsection>
<nextsent>the hypothesis underlying the use of dialogue act tagging in spoken dialogue evaluation is that systems dialogue behaviors have strong effect on its usability.
</nextsent>
<nextsent>because communicator systems have unique dialogue strategies, and unique way of representing and achieving particular communicative goals, date was developed to consistently label dialogue behaviors across systems so that the potential utility of dialogue act tagging could be explored.
</nextsent>
<nextsent>in previous work, walker and passonneau defined the date scheme, and labelled the system utterances in the june 2000 data collection of 663 dialogues from nine participating communicator systems (walker et al, 2001<papid> P01-1066 </papid>c; walker et al, 2001<papid> P01-1066 </papid>a).</nextsent>
<nextsent>they then derived dialogue act metrics from the date tags and showed that when these metrics were used in the paradise evaluation framework (walker et al, 1997) <papid> P97-1035 </papid>that they improved models of user satisfaction by an absolute 5   , and that the new metrics could beused to understand which systems dialogue strategies were most effective.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y569">
<title id=" W02-0221.xml">training a dialogue act tagger for human human and human computer travel dialogues </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our results show that wecan achieve high accuracies on the human computer data, and surprisingly, that the human-computer data improves accuracy on the human-human data, when only small amounts of human-human training data are available.
</prevsent>
<prevsent>recent research on dialogue is based on the assumption that dialogue acts provide useful wayof characterizing dialogue behaviors in both human human (hh) and human-computer (hc) dialogue (isard and carletta, 1995; shriberg et al, 2000; di eugenio et al, 1998; <papid> P98-1052 </papid>cattoni et al, 2001).</prevsent>
</prevsection>
<citsent citstr=" W01-1607 ">
previous research has used dialogue act tagging for tasks such as improving recognition performance (shriberg et al., 2000), identifying important parts of dialogue(finke et al, 1998), evaluating and comparing spoken dialogue systems (walker et al, 2001<papid> P01-1066 </papid>c; cattoni et al, 2001; hastie et al, 2002), as constraint on nominal expression generation (jordan, 2000), and for comparing hh to hc dialogues (doran et al, 2001).<papid> W01-1607 </papid>our work builds directly on the previous application of the date (dialogue act tagging for evaluation) tagging scheme to the evaluation and comparison of darpa communicator dialogues.</citsent>
<aftsection>
<nextsent>the hypothesis underlying the use of dialogue act tagging in spoken dialogue evaluation is that systems dialogue behaviors have strong effect on its usability.
</nextsent>
<nextsent>because communicator systems have unique dialogue strategies, and unique way of representing and achieving particular communicative goals, date was developed to consistently label dialogue behaviors across systems so that the potential utility of dialogue act tagging could be explored.
</nextsent>
<nextsent>in previous work, walker and passonneau defined the date scheme, and labelled the system utterances in the june 2000 data collection of 663 dialogues from nine participating communicator systems (walker et al, 2001<papid> P01-1066 </papid>c; walker et al, 2001<papid> P01-1066 </papid>a).</nextsent>
<nextsent>they then derived dialogue act metrics from the date tags and showed that when these metrics were used in the paradise evaluation framework (walker et al, 1997) <papid> P97-1035 </papid>that they improved models of user satisfaction by an absolute 5   , and that the new metrics could beused to understand which systems dialogue strategies were most effective.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y596">
<title id=" W02-0221.xml">training a dialogue act tagger for human human and human computer travel dialogues </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>because communicator systems have unique dialogue strategies, and unique way of representing and achieving particular communicative goals, date was developed to consistently label dialogue behaviors across systems so that the potential utility of dialogue act tagging could be explored.
</prevsent>
<prevsent>in previous work, walker and passonneau defined the date scheme, and labelled the system utterances in the june 2000 data collection of 663 dialogues from nine participating communicator systems (walker et al, 2001<papid> P01-1066 </papid>c; walker et al, 2001<papid> P01-1066 </papid>a).</prevsent>
</prevsection>
<citsent citstr=" P97-1035 ">
they then derived dialogue act metrics from the date tags and showed that when these metrics were used in the paradise evaluation framework (walker et al, 1997) <papid> P97-1035 </papid>that they improved models of user satisfaction by an absolute 5   , and that the new metrics could beused to understand which systems dialogue strategies were most effective.</citsent>
<aftsection>
<nextsent>philadelphia, july 2002, pp.
</nextsent>
<nextsent>162-173.
</nextsent>
<nextsent>association for computational linguistics.
</nextsent>
<nextsent>proceedings of the third sigdial workshop on discourse and dialogue, major part of evaluation effort using dialogue act tagging, however, is to actually label the dialogues with the dialogue act tags.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y627">
<title id=" W02-0221.xml">training a dialogue act tagger for human human and human computer travel dialogues </title>
<section> discussion and future work.  </section>
<citcontext>
<prevsection>
<prevsent>while this is not surprising, there is also significant overlap in the features and values used in the rules.
</prevsent>
<prevsent>for example, the utterance string feature utilizes words such as select, ight, do, okay, ne, these in both rule sets.
</prevsent>
</prevsection>
<citsent citstr=" P98-2188 ">
in summary our results show that: (1) it is possible to assign date dialogue act tags to system utterances in hc dialogues from many different systems for the same domain with high accuracy; (2) date tagger trained on data from an earlier version of the system only achieves moderate accuracy on later version of the system without small amount labelled training data from that later version; (3) labelled training data from hc dialogues can improve the performance of date tagger for hh dialogue when only small amount of hh training data is available.previous work has also reported results for dialogue act taggers, using similar features to those we use, with accuracies ranging from 62   to 75   (reithinger and klesen, 1997; shriberg et al, 2000; samuel et al, 1998).<papid> P98-2188 </papid></citsent>
<aftsection>
<nextsent>our best accuracy for the hcdata is 98   . the best performance for the hh corpus is 76   accuracy for the cross-validation study using only hh data.
</nextsent>
<nextsent>however, accuracies reported for previous work are not directly comparable to ours for several reasons.
</nextsent>
<nextsent>first, some of our results concern labelling the system side of utterances inhc dialogues for the purpose of automatic evaluation of system performance.
</nextsent>
<nextsent>it is much easier to develop high accuracy tagger for hc dialogue than it is for hh dialogue.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y640">
<title id=" W00-1309.xml">error driven hmm based chunk tagger with context dependent lexicon </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>ratnaparkhi(1998) used maximum entropy to recognise arbitrary chunk as part of tagging task.
</prevsent>
<prevsent>on the rule-based approaches, bourigaut(1992) used some heuristics and grammar to extract  terminology noun phrases  from french text.
</prevsent>
</prevsection>
<citsent citstr=" W93-0306 ">
voutilainen(1993) <papid> W93-0306 </papid>used similar method to detect english noun phrases.</citsent>
<aftsection>
<nextsent>kupiec(1993) <papid> P93-1003 </papid>applied.</nextsent>
<nextsent>finite state transducer in his noun phrases recogniser for both english and french.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y641">
<title id=" W00-1309.xml">error driven hmm based chunk tagger with context dependent lexicon </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>on the rule-based approaches, bourigaut(1992) used some heuristics and grammar to extract  terminology noun phrases  from french text.
</prevsent>
<prevsent>voutilainen(1993) <papid> W93-0306 </papid>used similar method to detect english noun phrases.</prevsent>
</prevsection>
<citsent citstr=" P93-1003 ">
kupiec(1993) <papid> P93-1003 </papid>applied.</citsent>
<aftsection>
<nextsent>finite state transducer in his noun phrases recogniser for both english and french.
</nextsent>
<nextsent>ramshaw and marcus(1995) <papid> W95-0107 </papid>used transformation-based arning, an error-driven learning technique introduced by eric bn11(1993), to locate chunks in the tagged corpus.</nextsent>
<nextsent>grefenstette(1996) applied finite state transducers to fred noun phrases and verb phrases.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y642">
<title id=" W00-1309.xml">error driven hmm based chunk tagger with context dependent lexicon </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>kupiec(1993) <papid> P93-1003 </papid>applied.</prevsent>
<prevsent>finite state transducer in his noun phrases recogniser for both english and french.</prevsent>
</prevsection>
<citsent citstr=" W95-0107 ">
ramshaw and marcus(1995) <papid> W95-0107 </papid>used transformation-based arning, an error-driven learning technique introduced by eric bn11(1993), to locate chunks in the tagged corpus.</citsent>
<aftsection>
<nextsent>grefenstette(1996) applied finite state transducers to fred noun phrases and verb phrases.
</nextsent>
<nextsent>in this paper, we will focus on statistics- based methods.
</nextsent>
<nextsent>the structure of this paper is as follows: in section 1, we will briefly describe the new error-driven hmm-based chunk tagger with context-dependent lexicon in principle.
</nextsent>
<nextsent>in section 2, baseline system which only includes the current part-of-speech in the lexicon is given.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y643">
<title id=" W00-1309.xml">error driven hmm based chunk tagger with context dependent lexicon </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in section 4, an error=driven learning method is used to decrease memory requirement of the lexicon by keeping only positive lexical 71 entries and make it possible to further improve the accuracy by merging different context- dependent lexicons into one after automatic analysis of the chunking errors.
</prevsent>
<prevsent>finally, the conclusion is given.
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
the data used for all our experiments extracted from the penn  wsj treebank (marcus et al 1993) <papid> J93-2004 </papid>by the program provided by sabine buchholz from tilbug university.</citsent>
<aftsection>
<nextsent>we use sections 00-19 as the training data and 20-24 as test data.
</nextsent>
<nextsent>therefore, the performance is on large scale task instead of small scale task on conll-2000 with the same evaluation program.
</nextsent>
<nextsent>for evaluation of our results, we use the precision and recall measures.
</nextsent>
<nextsent>precision is the percentage of predicted chunks that are actually correct while the recall is the percentage of correct chunks that are actually found.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y644">
<title id=" W00-1309.xml">error driven hmm based chunk tagger with context dependent lexicon </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>/3 2.
</prevsent>
<prevsent>precision + recall 1 hmm-based chunk tagger.
</prevsent>
</prevsection>
<citsent citstr=" A88-1019 ">
the idea of using statistics for chunking goes back to church(1988), <papid> A88-1019 </papid>who used corpus frequencies to determine the boundaries of simple non-recursive noun phrases.</citsent>
<aftsection>
<nextsent>skut and brants(1998) modified church approach in way permitting efficient and reliable recognition of structures of limited depth and encoded the structure in such way that it can be recognised by viterbi tagger.
</nextsent>
<nextsent>this makes the process run in time linear to the length of the input string.
</nextsent>
<nextsent>our approach follows skut and brants  way by employing hmm-based tagging method to model the chunking process.
</nextsent>
<nextsent>given token sequence g~ = g~g2   g, , the goal is to fred stochastic optimal tag sequence tin = tlt2...t which maximizes log p(t~  of ) : e(:q ,g?) log p(ti \[ g?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y646">
<title id=" W01-1502.xml">introduction extending nlp tools repositories for the interaction with language data resource repositories </title>
<section> the acl natural language software.  </section>
<citcontext>
<prevsection>
<prevsent>in order to know in which sections product is to be found, the user can submit standard query to the registry database.the underlying classification of the actual version of the acl registry is largely based on thebook (varile and zampolli, 1996).
</prevsent>
<prevsent>but this taxonomy will probably have to be further specialized and extended in order to satisfy the majority ofthe visitors of the nlsr.
</prevsent>
</prevsection>
<citsent citstr=" W01-1506 ">
therefore the classification can be enriched by the products submitted 1as (bird and simons, 2001) <papid> W01-1506 </papid>names it.</citsent>
<aftsection>
<nextsent>2see http://registry.dfki.de/and/or by comments made by the visitors, introducing thus bottom-up, developer and/or user oriented classification.
</nextsent>
<nextsent>a general goal of the most recent editions ofthe nlsr was the simplification of the registration procedure, providing short form to be filled by the customer.
</nextsent>
<nextsent>we do not request anymore an exhaustive description of the submitted product, but concentrate on few points providing guiding for the visitor, who will have to consult the homepage of the institutions or authors having submitted their product for getting more detailed information.
</nextsent>
<nextsent>in accordance with this simplification of the registration procedure, institutes or companies submitting their nlp products to the acl natural language software registry are required to give their url.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y649">
<title id=" W01-1502.xml">introduction extending nlp tools repositories for the interaction with language data resource repositories </title>
<section> meta data for nlp tools.  </section>
<citcontext>
<prevsection>
<prevsent>as we saw above, the sole conformance to standards (xml) for document description and interchange is not enough in the context of olac.
</prevsent>
<prevsent>but the use of meta data descriptions for tools seems to make sense not only for such initiatives.
</prevsent>
</prevsection>
<citsent citstr=" W01-1505 ">
(lavelliet al, 2001) <papid> W01-1505 </papid>show the use of meta data description for tools in the context of an infrastructure fornlp application development.</citsent>
<aftsection>
<nextsent>the role of meta data there is to specify the level of analysis accomplished by the source processor?.
</nextsent>
<nextsent>thus themetadata descriptions are useful for the communication between processes within an nlp chain, and also allow to mark and identify the document produced by such process.
</nextsent>
<nextsent>in any cases, the use of meta data description for tools (or processes triggered by those tools) is probably key-issue in the modular design of complex nlp environment.
</nextsent>
<nextsent>and one can see in the sissa approach to meta data descriptions for nlp processes, maybeas side effect, proposition for sharing annotations for processes and documents (resources) that can be handled.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y650">
<title id=" W01-1502.xml">introduction extending nlp tools repositories for the interaction with language data resource repositories </title>
<section> connection with.  </section>
<citcontext>
<prevsection>
<prevsent>metadata-descriptions for (multimedia/mltimodal) language resources catalogue and repositories for natural language data resources have already been working on the topic of meta data description for their entries (see for example ldc and elra).
</prevsent>
<prevsent>one can see olac as natural extension of the ldc, enlarging the resources catalogue to real infrastructure for language resource identification.
</prevsent>
</prevsection>
<citsent citstr=" W01-1507 ">
from the side of the language engineering there are initiatives for describing standards and(calzolari et al, 2001) <papid> W01-1507 </papid>present such an initiative, the isle project, which is the continuation of the eagles initiative.</citsent>
<aftsection>
<nextsent>the main objective of isle is to promote widely agreed and urgently demanded standards and guidelines for infrastructural language resources ..., tools that exploit them and le products?.
</nextsent>
<nextsent>the ongoing discussions within this project are thus important for the intended extension of nlp tools repositories.
</nextsent>
<nextsent>while (calzolari et al, 2001) <papid> W01-1507 </papid>concentrate onthe description of the task of the isle computational lexicon working group and address the topic of meta data for encoding multilingual lexical resources, (broeder and wittenburg, 2001) <papid> W01-1508 </papid>presents the work of the isle meta data initiative (imdi), which is directly relevant for the topic addressed here.</nextsent>
<nextsent>(broeder and wittenburg, 2001) <papid> W01-1508 </papid>give good overview of meta data initiatives for language resources and propose contrastive description of olac and imdi, where the main distinction can be seen in the top-down versusbottom-up approach.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y654">
<title id=" W01-1502.xml">introduction extending nlp tools repositories for the interaction with language data resource repositories </title>
<section> connection with.  </section>
<citcontext>
<prevsection>
<prevsent>the main objective of isle is to promote widely agreed and urgently demanded standards and guidelines for infrastructural language resources ..., tools that exploit them and le products?.
</prevsent>
<prevsent>the ongoing discussions within this project are thus important for the intended extension of nlp tools repositories.
</prevsent>
</prevsection>
<citsent citstr=" W01-1508 ">
while (calzolari et al, 2001) <papid> W01-1507 </papid>concentrate onthe description of the task of the isle computational lexicon working group and address the topic of meta data for encoding multilingual lexical resources, (broeder and wittenburg, 2001) <papid> W01-1508 </papid>presents the work of the isle meta data initiative (imdi), which is directly relevant for the topic addressed here.</citsent>
<aftsection>
<nextsent>(broeder and wittenburg, 2001) <papid> W01-1508 </papid>give good overview of meta data initiatives for language resources and propose contrastive description of olac and imdi, where the main distinction can be seen in the top-down versusbottom-up approach.</nextsent>
<nextsent>the top-down approach followed by olac allows an easy conformance tothe dublin core set, whereas the bottow-up approach requires the definition of more narrow and specialized categorization schemes?.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y663">
<title id=" W00-0206.xml">an application of the interlingua system iss for spanish english pronominal anaphora generation </title>
<section> generation module.  </section>
<citcontext>
<prevsection>
<prevsent>the basic task of syntactic generation is to order constituents in the correct sequence for the target language.
</prevsent>
<prevsent>however, the aim of this work is only the generation of pronominal anaphora into the target language, so we have only focused on the differences between the spanish and english languages in the generation of the pronoun.
</prevsent>
</prevsection>
<citsent citstr=" W99-0210 ">
these differences are what we have named discrepancies (a study of spanish-english- spanish discrepancies is showed in peral et al (1999)).<papid> W99-0210 </papid></citsent>
<aftsection>
<nextsent>in syntactic generation the following discrepancies can be found: syntactic discrepancies and spanish elliptical zero-subject constructions.
</nextsent>
<nextsent>4.1.1 syntactic discrepancies this discrepancy is due to the fact that the surface structures of the spanish sentences are more flexible than the english ones.
</nextsent>
<nextsent>the constituents of the spanish sentences can appear without specific order in the sentence.
</nextsent>
<nextsent>in order to carry out correct generation to english, we must firstly reorganize the spanish sentence.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y664">
<title id=" W01-1008.xml">document fusion for comprehensive event description </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>typical users include journalists and intelligence analysts, for whom compiling and fusing information is an integral part of their work (carbonell et al, 2000).
</prevsent>
<prevsent>obviously, if done manually, this process can be rather laborious as it involves numerous comparisons, depending on the number and length of the documents.
</prevsent>
</prevsection>
<citsent citstr=" P99-1071 ">
the aim of this paper is to describe an approach automatizing this process by fusing information stemming from different documents to generate single comprehensive document, containing the information of all original documents without repeating information which is conveyed by two or more documents.the work described in this paper is closely related to the area of multi-document summarization (barzilay et al, 1999; <papid> P99-1071 </papid>mani and bloedorn, 1999; mckeown and radev, 1995; radev, 2000),<papid> W00-1009 </papid>where related documents are analyzed to use frequently occurring segments for identifying relevant information that has to be included in the summary.</citsent>
<aftsection>
<nextsent>our work differs from the work on multi-document summarization as we focus on document fusion disregarding summarization.
</nextsent>
<nextsent>on the contrary, we are not aiming for the shortest description containing the most relevant information, but for the shortest description containing all information.
</nextsent>
<nextsent>for instance, even historic background information is included, as long as it allows the reader to get more comprehensive description of an event.
</nextsent>
<nextsent>although the techniques that are used formulti-document fusion and multi-document summarization are similar, the task of fusion is complementary to the summarization task.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y665">
<title id=" W01-1008.xml">document fusion for comprehensive event description </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>typical users include journalists and intelligence analysts, for whom compiling and fusing information is an integral part of their work (carbonell et al, 2000).
</prevsent>
<prevsent>obviously, if done manually, this process can be rather laborious as it involves numerous comparisons, depending on the number and length of the documents.
</prevsent>
</prevsection>
<citsent citstr=" W00-1009 ">
the aim of this paper is to describe an approach automatizing this process by fusing information stemming from different documents to generate single comprehensive document, containing the information of all original documents without repeating information which is conveyed by two or more documents.the work described in this paper is closely related to the area of multi-document summarization (barzilay et al, 1999; <papid> P99-1071 </papid>mani and bloedorn, 1999; mckeown and radev, 1995; radev, 2000),<papid> W00-1009 </papid>where related documents are analyzed to use frequently occurring segments for identifying relevant information that has to be included in the summary.</citsent>
<aftsection>
<nextsent>our work differs from the work on multi-document summarization as we focus on document fusion disregarding summarization.
</nextsent>
<nextsent>on the contrary, we are not aiming for the shortest description containing the most relevant information, but for the shortest description containing all information.
</nextsent>
<nextsent>for instance, even historic background information is included, as long as it allows the reader to get more comprehensive description of an event.
</nextsent>
<nextsent>although the techniques that are used formulti-document fusion and multi-document summarization are similar, the task of fusion is complementary to the summarization task.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y666">
<title id=" W01-1008.xml">document fusion for comprehensive event description </title>
<section> fusing documents.  </section>
<citcontext>
<prevsection>
<prevsent>in general, this problem does not only holdfor pronouns but for all kind of anaphoric expressions such as pronouns, definite noun phrases (e.g., the negotiations) and anaphoric adverbials (e.g., later).
</prevsent>
<prevsent>to cope with this problem simple segmentation is applied as pre-processing step where paragraphs that contain pronouns or simple definite noun phrases are attached to the preceding paragraph.
</prevsent>
</prevsection>
<citsent citstr=" J97-1003 ">
a more sophisticated approach to text segmentation is described in (hearst, 1997).<papid> J97-1003 </papid>obviously, it would be better to use an automatic anaphora resolution component to cope with this problem, see, e.g., (kennedy and boguraev, 1996; <papid> C96-1021 </papid>kameyama, 1997), <papid> W97-1307 </papid>where anaphoric expressions are replaced by their antecedents, butat the moment, the integration of such component remains future work.</citsent>
<aftsection>
<nextsent>2.2 informativity.
</nextsent>
<nextsent>(radev, 2000) <papid> W00-1009 </papid>describes 24 cross-document relations that can hold between their segments, one of which is the subsumption (or entailment) relation.</nextsent>
<nextsent>in the context of document fusion, we focus on the entailment relation and how it can be formally defined; unfortunately, (radev, 2000) <papid> W00-1009 </papid>provides no formal definition for any of the relations.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y667">
<title id=" W01-1008.xml">document fusion for comprehensive event description </title>
<section> fusing documents.  </section>
<citcontext>
<prevsection>
<prevsent>in general, this problem does not only holdfor pronouns but for all kind of anaphoric expressions such as pronouns, definite noun phrases (e.g., the negotiations) and anaphoric adverbials (e.g., later).
</prevsent>
<prevsent>to cope with this problem simple segmentation is applied as pre-processing step where paragraphs that contain pronouns or simple definite noun phrases are attached to the preceding paragraph.
</prevsent>
</prevsection>
<citsent citstr=" C96-1021 ">
a more sophisticated approach to text segmentation is described in (hearst, 1997).<papid> J97-1003 </papid>obviously, it would be better to use an automatic anaphora resolution component to cope with this problem, see, e.g., (kennedy and boguraev, 1996; <papid> C96-1021 </papid>kameyama, 1997), <papid> W97-1307 </papid>where anaphoric expressions are replaced by their antecedents, butat the moment, the integration of such component remains future work.</citsent>
<aftsection>
<nextsent>2.2 informativity.
</nextsent>
<nextsent>(radev, 2000) <papid> W00-1009 </papid>describes 24 cross-document relations that can hold between their segments, one of which is the subsumption (or entailment) relation.</nextsent>
<nextsent>in the context of document fusion, we focus on the entailment relation and how it can be formally defined; unfortunately, (radev, 2000) <papid> W00-1009 </papid>provides no formal definition for any of the relations.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y668">
<title id=" W01-1008.xml">document fusion for comprehensive event description </title>
<section> fusing documents.  </section>
<citcontext>
<prevsection>
<prevsent>in general, this problem does not only holdfor pronouns but for all kind of anaphoric expressions such as pronouns, definite noun phrases (e.g., the negotiations) and anaphoric adverbials (e.g., later).
</prevsent>
<prevsent>to cope with this problem simple segmentation is applied as pre-processing step where paragraphs that contain pronouns or simple definite noun phrases are attached to the preceding paragraph.
</prevsent>
</prevsection>
<citsent citstr=" W97-1307 ">
a more sophisticated approach to text segmentation is described in (hearst, 1997).<papid> J97-1003 </papid>obviously, it would be better to use an automatic anaphora resolution component to cope with this problem, see, e.g., (kennedy and boguraev, 1996; <papid> C96-1021 </papid>kameyama, 1997), <papid> W97-1307 </papid>where anaphoric expressions are replaced by their antecedents, butat the moment, the integration of such component remains future work.</citsent>
<aftsection>
<nextsent>2.2 informativity.
</nextsent>
<nextsent>(radev, 2000) <papid> W00-1009 </papid>describes 24 cross-document relations that can hold between their segments, one of which is the subsumption (or entailment) relation.</nextsent>
<nextsent>in the context of document fusion, we focus on the entailment relation and how it can be formally defined; unfortunately, (radev, 2000) <papid> W00-1009 </papid>provides no formal definition for any of the relations.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y673">
<title id=" W01-1008.xml">document fusion for comprehensive event description </title>
<section> conclusions.  </section>
<citcontext>
<prevsection>
<prevsent>a question for future research is towhat extent shallow parsing techniques can im prove the entailment scores.
</prevsent>
<prevsent>in particular, does considering the relational structure of sentence improve computing entailment relations?
</prevsent>
</prevsection>
<citsent citstr=" C00-1043 ">
thishas shown to be successful in inference-based approaches to question-answering, see (harabagiuet al, 2000), <papid> C00-1043 </papid>and document fusion might also benefit from representations that are bit deeper than the one discussed in this paper.</citsent>
<aftsection>
<nextsent>another open issue at this point is the need for standards for evaluating the quality of document fusion.
</nextsent>
<nextsent>we think that this can be done by using standard ir measures like miss and false alarm.
</nextsent>
<nextsent>although miss can be approximated extrinsic ally, it is unclear whether this also possible for falsealarm.
</nextsent>
<nextsent>obviously, intrinsic evaluation is more reliable, but it remains an extremely laborious process, where inter-judge disagreement is still an issue, see (radev et al, 2000).<papid> W00-0403 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y674">
<title id=" W01-1008.xml">document fusion for comprehensive event description </title>
<section> conclusions.  </section>
<citcontext>
<prevsection>
<prevsent>we think that this can be done by using standard ir measures like miss and false alarm.
</prevsent>
<prevsent>although miss can be approximated extrinsic ally, it is unclear whether this also possible for falsealarm.
</prevsent>
</prevsection>
<citsent citstr=" W00-0403 ">
obviously, intrinsic evaluation is more reliable, but it remains an extremely laborious process, where inter-judge disagreement is still an issue, see (radev et al, 2000).<papid> W00-0403 </papid></citsent>
<aftsection>
<nextsent>acknowledgments the author would like to thank maarten de rijke for providing the entailment judgments.
</nextsent>
<nextsent>this work was supported by the physical sciences council with financial support from the netherlands organization for scientific research (nwo), project 612-13-001.
</nextsent>


</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y675">
<title id=" W01-0808.xml">multilingual sentence generation </title>
<section> syntactic generation component.  </section>
<citcontext>
<prevsection>
<prevsent>transferred spanish lf: transferred japanese lf: transferred chinese lf: figure 2 the transferred lf is the input to the generation component, which we will discuss in detail below.
</prevsent>
<prevsent>the different language generation modules in our system are syntactic realization components that take as input an lf characteristic of the language to be generated and produce syntactic tree and surface string for that language.
</prevsent>
</prevsection>
<citsent citstr=" A97-1039 ">
in this sense, they are functionally similar to the realpro system (lavoie and rambow, 1997).<papid> A97-1039 </papid></citsent>
<aftsection>
<nextsent>1 english gloss is provided in figure 2 for readability.
</nextsent>
<nextsent>purposes only.
</nextsent>
<nextsent>the generation modules are not designed specifically for mt, but rather are application independent.
</nextsent>
<nextsent>they can take as input an lf produced by dialog application, critiquing application, database query application, an mt application, etc. they only require monolingual dictionary for the language being generated and an input lf that is characteristic of that language.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y676">
<title id=" W01-0804.xml">logical form equivalence the case of referring expressions generation </title>
<section> logic in gre.  </section>
<citcontext>
<prevsection>


</prevsection>
<citsent citstr=" T87-1042 ">
a key question regarding the foundations of natural language generation (nlg) is the problem of logical form equivalence (appelt 1987).<papid> T87-1042 </papid></citsent>
<aftsection>
<nextsent>the problem goes as follows.
</nextsent>
<nextsent>nlg systems take semantic expressions as input, usually formulated in some logical language.
</nextsent>
<nextsent>these expressions are governed by rules determining which of them count as equivalent?.
</nextsent>
<nextsent>if two expressions are equivalent then, ideally, the nlg program should verbalize them in the same ways.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y677">
<title id=" W01-0804.xml">logical form equivalence the case of referring expressions generation </title>
<section> logic in gre.  </section>
<citcontext>
<prevsection>
<prevsent> is logically equivalent with
</prevsent>
<prevsent>   , yet ? so the argument goes ? an nlg system should word the two formulas differently.
</prevsent>
</prevsection>
<citsent citstr=" J93-1008 ">
shieber (1993) <papid> J93-1008 </papid>suggested that some more sophisticated notion of equivalence is needed, which would count fewer semantic expressions as equivalent.1 in the present paper, different response to the problem is explored, which keeps the notion of equivalence classical and prevents the generator from distinguishing between inputs that are logically equivalent (i.e., inputs that have the same truth condi tions).</citsent>
<aftsection>
<nextsent>pragmatic constraints determine whichof all the logically equivalent semantic expressions is put into words by the nlg program.
</nextsent>
<nextsent>whereas this programme, which might be called logic-oriented?
</nextsent>
<nextsent>generation, would constitute fairly radical departure from current practice if applied to all of nlg (krahmer &amp; van deemter (forthcoming); power 2000 <papid> C00-2093 </papid>for related work), the main aim of the present paper is modest: to show that logic-oriented generation is standard practice in connection with the generation of referring expressions (gre).</nextsent>
<nextsent>more specifically, we show the semantics of current gre algorithms to be guided by surprisingly simple principle of co extensivity, while their pragmatics is guided by gricean brevity.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y678">
<title id=" W01-0804.xml">logical form equivalence the case of referring expressions generation </title>
<section> logic in gre.  </section>
<citcontext>
<prevsection>
<prevsent>pragmatic constraints determine whichof all the logically equivalent semantic expressions is put into words by the nlg program.
</prevsent>
<prevsent>whereas this programme, which might be called logic-oriented?
</prevsent>
</prevsection>
<citsent citstr=" C00-2093 ">
generation, would constitute fairly radical departure from current practice if applied to all of nlg (krahmer &amp; van deemter (forthcoming); power 2000 <papid> C00-2093 </papid>for related work), the main aim of the present paper is modest: to show that logic-oriented generation is standard practice in connection with the generation of referring expressions (gre).</citsent>
<aftsection>
<nextsent>more specifically, we show the semantics of current gre algorithms to be guided by surprisingly simple principle of co extensivity, while their pragmatics is guided by gricean brevity.
</nextsent>
<nextsent>our game plan is as follows.
</nextsent>
<nextsent>in section 2, we illustrate the collaboration between brevity andco-extensivity, focussing on simple?
</nextsent>
<nextsent>referring expressions, which intersect atomic properties (e.g.,dog?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y679">
<title id=" W01-0804.xml">logical form equivalence the case of referring expressions generation </title>
<section> intersec tive reference to sets of.  </section>
<citcontext>
<prevsection>
<prevsent>we will restrict attention to the problem of determining the semantic content of description, leaving linguistic realization aside.
</prevsent>
<prevsent>(cf.
</prevsent>
</prevsection>
<citsent citstr=" W98-1419 ">
stone and webber 1998, <papid> W98-1419 </papid>krahmer and theune 1999, which inter leave linguistic realization andgeneration.)</citsent>
<aftsection>
<nextsent>accordingly, generation of refer ring expressions?
</nextsent>
<nextsent>(gre) will refer specifically to content determination.
</nextsent>
<nextsent>we will call gre algorithm complete if it is successful whenever an individuating description exists.
</nextsent>
<nextsent>most gre algorithms are limited to individual target objects (for an exception, stone 2000), <papid> W00-1416 </papid>but we will present ones that refer to sets of objects (van deemter2000); reference to an individual  will equal reference to the singleton set  . 2.1 the incremental algorithm.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y680">
<title id=" W01-0804.xml">logical form equivalence the case of referring expressions generation </title>
<section> intersec tive reference to sets of.  </section>
<citcontext>
<prevsection>
<prevsent>(gre) will refer specifically to content determination.
</prevsent>
<prevsent>we will call gre algorithm complete if it is successful whenever an individuating description exists.
</prevsent>
</prevsection>
<citsent citstr=" W00-1416 ">
most gre algorithms are limited to individual target objects (for an exception, stone 2000), <papid> W00-1416 </papid>but we will present ones that refer to sets of objects (van deemter2000); reference to an individual  will equal reference to the singleton set  . 2.1 the incremental algorithm.</citsent>
<aftsection>
<nextsent>dale and reiter (1995) proposed an algorithm that takes shared kb as its input and delivers set of properties which jointly identify the target.
</nextsent>
<nextsent>descriptions produced by the algorithm full fill the criterion of co-extensivity.
</nextsent>
<nextsent>according to this principle, description is semantically correct if it has the target as its referent (i.e., its extension).
</nextsent>
<nextsent>the authors observed that semantically correct description can still be unnatural, but that naturalness is not always easy to achieve.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y681">
<title id=" W01-0804.xml">logical form equivalence the case of referring expressions generation </title>
<section> reference using boolean descriptions.  </section>
<citcontext>
<prevsection>
<prevsent>suppose brevity of descriptions is defined as follows: 4  is less brief than 4  if either 4  contains only atomic properties while 4 contains non-atomic properties as well, or 4  contains more boolean operators than 4  . then the intractability of full brevity for intersections of atomic properties logically implies that of the new algorithm: proof: suppose an algorithm, bool, produc eda maximally brief boolean description when ever one exists.
</prevsent>
<prevsent>then whenever target set + can be described as an intersection of atomic properties, bool( + ) would be maximally brief intersection of atomic properties, and this is inconsistent with the intractability of full brevity for intersections of atomic properties.
</prevsent>
</prevsection>
<citsent citstr=" P99-1017 ">
3compare bateman (1999), <papid> P99-1017 </papid>where kb is compiled into format that brings out the commonalities between objects before the content of referring expression is determined.</citsent>
<aftsection>
<nextsent>this negative result gives rise to the question whether full brevity may be approximated, perhaps in the spirit of reiter (1990)<papid> P90-1013 </papid>s localbrevity?</nextsent>
<nextsent>algorithm which takes given intersec tive description and tests whether any set of properties in it may be replaced by one other property.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y682">
<title id=" W01-0804.xml">logical form equivalence the case of referring expressions generation </title>
<section> reference using boolean descriptions.  </section>
<citcontext>
<prevsection>
<prevsent>then whenever target set + can be described as an intersection of atomic properties, bool( + ) would be maximally brief intersection of atomic properties, and this is inconsistent with the intractability of full brevity for intersections of atomic properties.
</prevsent>
<prevsent>3compare bateman (1999), <papid> P99-1017 </papid>where kb is compiled into format that brings out the commonalities between objects before the content of referring expression is determined.</prevsent>
</prevsection>
<citsent citstr=" P90-1013 ">
this negative result gives rise to the question whether full brevity may be approximated, perhaps in the spirit of reiter (1990)<papid> P90-1013 </papid>s localbrevity?</citsent>
<aftsection>
<nextsent>algorithm which takes given intersec tive description and tests whether any set of properties in it may be replaced by one other property.
</nextsent>
<nextsent>unfortunately, however, simplification is much harder in the boolean setting.
</nextsent>
<nextsent>suppose, for example, one wanted to use the quine-mccluskey algorithm (mccluskey 1965), known from its applications to electronic circuits, to reduce the number of boolean operators in the description.
</nextsent>
<nextsent>this would go only small part of the way, since quine-mccluskey assumes logical independence of all the properties involved.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y684">
<title id=" W02-0219.xml">a new taxonomy for the quality of telephone services based on spoken dialogue systems </title>
<section> quality of service taxonomy.  </section>
<citcontext>
<prevsection>
<prevsent>this fact has tentatively been illustrated by the gray cans on the upper side of the taxonomy, but will not be further addressed in this paper.
</prevsent>
<prevsent>the remaining categories are discussed in the following.
</prevsent>
</prevsection>
<citsent citstr=" P97-1035 ">
walker et al (1997) <papid> P97-1035 </papid>identified three factors which carry an influence on the performance of sdss, and which therefore are thought to contribute to its quality perceived by the user: agent factors (mainly related to the dialogue and the system itself), task factors (related to how the sds captures the task it has been developed for) and environmental factors (e.g. factors related to the acoustic environment and the transmission channel).</citsent>
<aftsection>
<nextsent>because the taxonomy refers to the service as whole, fourth point is added here, namely contextual factors such as costs, type of access, or the availability.
</nextsent>
<nextsent>all four types of factors subsume quality elements which can be expected to carry an influence on the quality perceived by theuser.
</nextsent>
<nextsent>the corresponding quality features are summarized into aspects and categories in the following quality of service environmental factors agent factors task factors contextual factors speechi/o quality dialogue cooperativity dialogue symmetry communication efficiency comfort task efficiency usability service efficiency economical benefit user satisfaction utility acceptability attitude experi-enceemotions flexibility motivation,goals task/domain knowledge transm.channel backgr.noise room acoustics task coverage domain cov . task flexibility taskdif ficulty costs availability opening hours access intelligibility naturalness listening-effort systemunderst.
</nextsent>
<nextsent>informative ness truth&evidence; relevance manner backgr.know . meta-comm.handl.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y688">
<title id=" W01-0703.xml">learning classtoclass selectional preferences </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>previous literature on selectional preference has usually learned preferences for words in the form of classes, e.g., the object of eat is an edible entity.
</prevsent>
<prevsent>this paper extends previous statistical models to classes of verbs, yielding relation between classes in hierarchy, as opposed to relation between word and class.
</prevsent>
</prevsection>
<citsent citstr=" H93-1061 ">
the model is trained using subject-verb and object-verb associations extracted from semcor, corpus (miller et al, 1993) <papid> H93-1061 </papid>tagged with wordnet word-senses (miller et al, 1990).</citsent>
<aftsection>
<nextsent>the syntactic relations were extracted using the minipar parser (lin, 1993).<papid> P93-1016 </papid></nextsent>
<nextsent>a peculiarity of this exercise is the use of small sense disambiguated corpus, in contrast to using large corpus of ambiguous words.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y689">
<title id=" W01-0703.xml">learning classtoclass selectional preferences </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this paper extends previous statistical models to classes of verbs, yielding relation between classes in hierarchy, as opposed to relation between word and class.
</prevsent>
<prevsent>the model is trained using subject-verb and object-verb associations extracted from semcor, corpus (miller et al, 1993) <papid> H93-1061 </papid>tagged with wordnet word-senses (miller et al, 1990).</prevsent>
</prevsection>
<citsent citstr=" P93-1016 ">
the syntactic relations were extracted using the minipar parser (lin, 1993).<papid> P93-1016 </papid></citsent>
<aftsection>
<nextsent>a peculiarity of this exercise is the use of small sense disambiguated corpus, in contrast to using large corpus of ambiguous words.
</nextsent>
<nextsent>we think that two factors can help alleviate the scarcity of data: the fact that using disambiguated words provides purer data, and the ability to use classes of verbs in the preferences.
</nextsent>
<nextsent>nevertheless, the approach can be easily extended to larger, non disambiguated corpora.
</nextsent>
<nextsent>we have defined word sense disambiguation exercise in order to evaluate the extracted preferences, using sample of words and sample of documents, both from semcor.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y690">
<title id=" W01-0703.xml">learning classtoclass selectional preferences </title>
<section> selectional preference learning.  </section>
<citcontext>
<prevsection>
<prevsent>selectional preferences get more complex than it might seem: (2) the acid ate the metal?, (3) this car eats lot of gas?, (4) we ate our savings?, etc. corpus-based approaches for selectional preference learning extract number of (e.g. verb/subject) relations from large corpora and use an algorithm to generalize from the set of nouns for each verb separately.
</prevsent>
<prevsent>usually, nouns are generalized using classes (concepts) from lexical knowledge base (e.g. wordnet).
</prevsent>
</prevsection>
<citsent citstr=" P92-1053 ">
resnik (1992), <papid> P92-1053 </papid>resnik (1997) <papid> W97-0209 </papid>defines an information theoretic measure of the association between verb and nominal wordnet classes: selectional association.</citsent>
<aftsection>
<nextsent>he uses verb-argument pairs from brown.
</nextsent>
<nextsent>evaluation is performed applying intuition and wsd.
</nextsent>
<nextsent>our measure follows in part from his formalization.
</nextsent>
<nextsent>abe and li (1995) follow similar approach, but they employ different information theoretic measure (the minimum description length principle) to select the set of concepts in hierarchy that generalize best the selectional preferences for verb.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y691">
<title id=" W01-0703.xml">learning classtoclass selectional preferences </title>
<section> selectional preference learning.  </section>
<citcontext>
<prevsection>
<prevsent>selectional preferences get more complex than it might seem: (2) the acid ate the metal?, (3) this car eats lot of gas?, (4) we ate our savings?, etc. corpus-based approaches for selectional preference learning extract number of (e.g. verb/subject) relations from large corpora and use an algorithm to generalize from the set of nouns for each verb separately.
</prevsent>
<prevsent>usually, nouns are generalized using classes (concepts) from lexical knowledge base (e.g. wordnet).
</prevsent>
</prevsection>
<citsent citstr=" W97-0209 ">
resnik (1992), <papid> P92-1053 </papid>resnik (1997) <papid> W97-0209 </papid>defines an information theoretic measure of the association between verb and nominal wordnet classes: selectional association.</citsent>
<aftsection>
<nextsent>he uses verb-argument pairs from brown.
</nextsent>
<nextsent>evaluation is performed applying intuition and wsd.
</nextsent>
<nextsent>our measure follows in part from his formalization.
</nextsent>
<nextsent>abe and li (1995) follow similar approach, but they employ different information theoretic measure (the minimum description length principle) to select the set of concepts in hierarchy that generalize best the selectional preferences for verb.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y692">
<title id=" W01-0703.xml">learning classtoclass selectional preferences </title>
<section> selectional preference learning.  </section>
<citcontext>
<prevsection>
<prevsent>abe and li (1995) follow similar approach, but they employ different information theoretic measure (the minimum description length principle) to select the set of concepts in hierarchy that generalize best the selectional preferences for verb.
</prevsent>
<prevsent>the argument pairs are extracted from the wsj corpus, and evaluation is performed using intuition and pp-attachment resolution.
</prevsent>
</prevsection>
<citsent citstr=" W98-0701 ">
stetina et al (1998) <papid> W98-0701 </papid>extract word-arg-word triples for all possible combinations, and use measure of relational probability?</citsent>
<aftsection>
<nextsent>based on frequency and similarity.
</nextsent>
<nextsent>they provide an algorithm to disambiguate all words in sentence.
</nextsent>
<nextsent>it is directly applied to wsd with good results.
</nextsent>
<nextsent>the model explored in this paper emerges as result of the following observations: ? distinguishing verb senses can be useful.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y693">
<title id=" W00-0707.xml">incorporating position information into a maximum entropy minimum divergence translation model </title>
<section> introduction.  </section>
<citcontext>
<prevsection>
<prevsent>in particular, if hi is known, finding the best word at the current position requires only straightforward search through the target 1this ignores the issue of normalization over target texts of all possible lengths, which can be easily enforced when desired by using stop token or prior distribution over lengths.
</prevsent>
<prevsent>vocabulary, and efficient dynamic-programming based heuristics can be used to extend this to sequences of words.
</prevsent>
</prevsection>
<citsent citstr=" A00-1019 ">
this is very important for applications uch as trans type (foster et al, 1997; langlais et al, 2000), <papid> A00-1019 </papid>where the task is to make real-time predictions of the text hu-man translator will type next, based on the source text under translation and some prefix of the target text that has already been typed.</citsent>
<aftsection>
<nextsent>the standard  noisy channel  approach used in smt, where p(tls )  p(t)p(slt), is generally too expensive for such applications because it does not permit direct calculation of the probabil-ity of word or sequence of words beginning at the current position.
</nextsent>
<nextsent>complex and expensive search strategies are required to find the best target text in this approach (garcfa-varea et al., 1998; niessen et al, 1998; ochet al, 1999; <papid> W99-0604 </papid>wang and waibel, 1998).</nextsent>
<nextsent>the challenge in modeling p(wlhi,s ) is to combine two disparate sources of conditioning information in an effective way.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y694">
<title id=" W00-0707.xml">incorporating position information into a maximum entropy minimum divergence translation model </title>
<section> introduction.  </section>
<citcontext>
<prevsection>
<prevsent>this is very important for applications uch as trans type (foster et al, 1997; langlais et al, 2000), <papid> A00-1019 </papid>where the task is to make real-time predictions of the text hu-man translator will type next, based on the source text under translation and some prefix of the target text that has already been typed.</prevsent>
<prevsent>the standard  noisy channel  approach used in smt, where p(tls )  p(t)p(slt), is generally too expensive for such applications because it does not permit direct calculation of the probabil-ity of word or sequence of words beginning at the current position.</prevsent>
</prevsection>
<citsent citstr=" W99-0604 ">
complex and expensive search strategies are required to find the best target text in this approach (garcfa-varea et al., 1998; niessen et al, 1998; ochet al, 1999; <papid> W99-0604 </papid>wang and waibel, 1998).</citsent>
<aftsection>
<nextsent>the challenge in modeling p(wlhi,s ) is to combine two disparate sources of conditioning information in an effective way.
</nextsent>
<nextsent>one obvious strategy is to use linear combination of sep-arate language and translation components, of the form: p(w\[hi, s) -- ap(w\[hi) + (1 - a)p(w\[i, s).
</nextsent>
<nextsent>(2) where p(w\[hi) is language model, p(wli , s) is translation model, and e \[0, 1\] is com-bining weight.
</nextsent>
<nextsent>however, this appears to be weak technique (langlais and foster, 2000), <papid> P00-1006 </papid>even when is allowed to depend on various features of the context (hi, s).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y695">
<title id=" W00-0707.xml">incorporating position information into a maximum entropy minimum divergence translation model </title>
<section> introduction.  </section>
<citcontext>
<prevsection>
<prevsent>one obvious strategy is to use linear combination of sep-arate language and translation components, of the form: p(w\[hi, s) -- ap(w\[hi) + (1 - a)p(w\[i, s).
</prevsent>
<prevsent>(2) where p(w\[hi) is language model, p(wli , s) is translation model, and e \[0, 1\] is com-bining weight.
</prevsent>
</prevsection>
<citsent citstr=" P00-1006 ">
however, this appears to be weak technique (langlais and foster, 2000), <papid> P00-1006 </papid>even when is allowed to depend on various features of the context (hi, s).</citsent>
<aftsection>
<nextsent>in previous work (foster, 2000), <papid> P00-1006 </papid>de-scribed maximum entropy/minimum diver-gence (memd) model (berger et al, 1996) <papid> J96-1002 </papid>for p(w\[hi, s) which incorporates trigram lan-guage model and translation component which is an analog of the well-known ibm transla-tion model 1 (brown et al, 1993).<papid> J93-2003 </papid></nextsent>
<nextsent>this model 37 significantly outperforms an equivalent linear combination of trigram and model 1 in test- corpus perplexity, despite using several orders of magnitude fewer translation parameters.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y701">
<title id=" W00-0707.xml">incorporating position information into a maximum entropy minimum divergence translation model </title>
<section> introduction.  </section>
<citcontext>
<prevsection>
<prevsent>(2) where p(w\[hi) is language model, p(wli , s) is translation model, and e \[0, 1\] is com-bining weight.
</prevsent>
<prevsent>however, this appears to be weak technique (langlais and foster, 2000), <papid> P00-1006 </papid>even when is allowed to depend on various features of the context (hi, s).</prevsent>
</prevsection>
<citsent citstr=" J96-1002 ">
in previous work (foster, 2000), <papid> P00-1006 </papid>de-scribed maximum entropy/minimum diver-gence (memd) model (berger et al, 1996) <papid> J96-1002 </papid>for p(w\[hi, s) which incorporates trigram lan-guage model and translation component which is an analog of the well-known ibm transla-tion model 1 (brown et al, 1993).<papid> J93-2003 </papid></citsent>
<aftsection>
<nextsent>this model 37 significantly outperforms an equivalent linear combination of trigram and model 1 in test- corpus perplexity, despite using several orders of magnitude fewer translation parameters.
</nextsent>
<nextsent>like model 1, its translation component is based only on the occurrences in of words which are po-tential translations for w, and does not take into account the positions of these words rel-ative to w. an obvious enhancement is to in-corporate such positional information into the memd model, thereby making its translation component analogous to the ibm model 2.
</nextsent>
<nextsent>this is the problem address in this paper.
</nextsent>
<nextsent>2.1 inear mode as baseline for comparison used linear com-bination as in (2) of standard interpolated tri-gram language model and the ibm translation model 2 (ibm2), with the combining weight optimized using the em algorithm.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y703">
<title id=" W00-0707.xml">incorporating position information into a maximum entropy minimum divergence translation model </title>
<section> introduction.  </section>
<citcontext>
<prevsection>
<prevsent>(2) where p(w\[hi) is language model, p(wli , s) is translation model, and e \[0, 1\] is com-bining weight.
</prevsent>
<prevsent>however, this appears to be weak technique (langlais and foster, 2000), <papid> P00-1006 </papid>even when is allowed to depend on various features of the context (hi, s).</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
in previous work (foster, 2000), <papid> P00-1006 </papid>de-scribed maximum entropy/minimum diver-gence (memd) model (berger et al, 1996) <papid> J96-1002 </papid>for p(w\[hi, s) which incorporates trigram lan-guage model and translation component which is an analog of the well-known ibm transla-tion model 1 (brown et al, 1993).<papid> J93-2003 </papid></citsent>
<aftsection>
<nextsent>this model 37 significantly outperforms an equivalent linear combination of trigram and model 1 in test- corpus perplexity, despite using several orders of magnitude fewer translation parameters.
</nextsent>
<nextsent>like model 1, its translation component is based only on the occurrences in of words which are po-tential translations for w, and does not take into account the positions of these words rel-ative to w. an obvious enhancement is to in-corporate such positional information into the memd model, thereby making its translation component analogous to the ibm model 2.
</nextsent>
<nextsent>this is the problem address in this paper.
</nextsent>
<nextsent>2.1 inear mode as baseline for comparison used linear com-bination as in (2) of standard interpolated tri-gram language model and the ibm translation model 2 (ibm2), with the combining weight optimized using the em algorithm.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y714">
<title id=" W01-0713.xml">unsupervised induction of stochastic context free grammars using distributional clustering </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>previous research on completely unsupervised learning has produced poor results, and as result researchers have resorted to mild forms of supervision.
</prevsent>
<prevsent>magerman and marcus(1990) use dis tituent grammar to eliminate undesirable rules.
</prevsent>
</prevsection>
<citsent citstr=" P92-1017 ">
pereira and schabes(1992) <papid> P92-1017 </papid>use partially bracketed corpora and carroll and charniak(1992) restrict the set of non-terminals that may appear on the right hand side of rules with given left hand side.</citsent>
<aftsection>
<nextsent>the work of van zaanen (2000) does not have this problem, and appears to perform well on small datasets, but it is not clear whether it will scale up to large datasets.
</nextsent>
<nextsent>adriaans et al (2000) presents another algorithm but its performance on authentic natural language data appears to be very limited.
</nextsent>
<nextsent>the work presented here can be seen as onemore attempt to implement zellig harriss distributional analysis (harris, 1954), the first such attempt being (lamb, 1961).
</nextsent>
<nextsent>the rest of the paper is arranged as follows:section 2 introduces the technique of distributional clustering and presents the results of preliminary experiment.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y715">
<title id=" W01-0713.xml">unsupervised induction of stochastic context free grammars using distributional clustering </title>
<section> mathematical justification.  </section>
<citcontext>
<prevsection>
<prevsent>we can gain some insight into the significance ofthe mi criterion by analysing it within the framework of scfgs.
</prevsent>
<prevsent>we are interested in lookin gat the properties of the two-dimensional distributions of each non-terminal.
</prevsent>
</prevsection>
<citsent citstr=" J95-2002 ">
the terminals are the part of speech tags of which there are  . for each terminal or non-terminal symbol  we define four distributions,         , over  or equivalently  -dimensional vectors.two of these,   and   are just the prefix and suffix probability distributions for the symbol(stolcke, 1995): <papid> J95-2002 </papid>the probabilities that the string derived from  begins (or ends) with particular tag.</citsent>
<aftsection>
<nextsent>the other two     for left distribution and right distribution, are the distributions of the symbols before and after the nonterminal.
</nextsent>
<nextsent>clearly if  is terminal symbol, the strings derived from it are all of length 1, and thus begin and end with  , giving   and   very simple form.
</nextsent>
<nextsent>if we consider each non-terminal  in scfg, we can associate with it two random variables which we can call the internal and external variables.
</nextsent>
<nextsent>the internal random variable is the more familiar and ranges over the set of rules expanding that non-terminal.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y716">
<title id=" W01-0713.xml">unsupervised induction of stochastic context free grammars using distributional clustering </title>
<section> minimum description length.  </section>
<citcontext>
<prevsection>
<prevsent>the mi criterion allows us to find the right places to cut the sentences up; we look for sequences where there are interesting long-range dependencies.
</prevsent>
<prevsent>given these potential sequences, we can then hypothe sise sets of rules with the same right hand side.
</prevsent>
</prevsection>
<citsent citstr=" P95-1031 ">
this naturally suggests minimum description length (mdl) or bayesian approach (stolcke, 1994; chen, 1995).<papid> P95-1031 </papid></citsent>
<aftsection>
<nextsent>starting with the maximum likelihood grammar, which has one rule for each sentence type in the corpus, and single nonterminal, at each iteration we cluster all frequent strings, and filter according to the mi criterion discussed above.
</nextsent>
<nextsent>we then greedily select the cluster that will give the best immediate reduction in description length, calculated according to theoretically optimal code.
</nextsent>
<nextsent>we add new non-terminal with rules for each sequence in the cluster.
</nextsent>
<nextsent>ifthere is sequence of length 1 with nonterminal in it, then instead of adding new nonterminal, we add rules expanding that old nonterminal.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y718">
<title id=" W01-0713.xml">unsupervised induction of stochastic context free grammars using distributional clustering </title>
<section> future work.  </section>
<citcontext>
<prevsection>
<prevsent>the comparison presented here allows only tentative conclusions for these reasons: first, there are minor differences in the test sets used; secondly, the cdc algorithm is not completely unsupervised at the moment as it runs on tagged text, whereas abl and emile run on raw text, though since the atis corpus has very little lexical ambiguity the difference is probably quite minor; thirdly, it is worth reiterating that the cdc algorithm was trained on radically different and much more complex dataset.
</prevsent>
<prevsent>however, we can conclude that the cdc algorithm compares favourably to other unsupervised algorithms.
</prevsent>
</prevsection>
<citsent citstr=" W00-0717 ">
preliminary experiments with tags derived automatically using distributional clustering (clark, 2000), <papid> W00-0717 </papid>have shown essentially the same results.</citsent>
<aftsection>
<nextsent>it appears that for the simple constituents that are being constructed in the work presented here, theyare sufficiently accurate.
</nextsent>
<nextsent>this makes the algorithm completely unsupervised.
</nextsent>
<nextsent>i have so far used the simplest possible metric and clustering algorithm; there are much more sophisticated hierarchical clustering algorithms that might perform better.
</nextsent>
<nextsent>in addition, will explore the use of lexicalised formalism.this algorithm uses exclusively bottom-up information; the standard estimation and parsing algorithms use the interaction between bottom-up and top-down information, or inside and outside probabilities to direct the search.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y719">
<title id=" W02-0210.xml">adaptive dialogue systems  interaction with interact </title>
<section> natural language capabilities.  </section>
<citcontext>
<prevsection>
<prevsent>at all stages the response specification is xml-based, including the final speech markup language which is passed to the speech synthesizer.the system architecture allows multiple generators to be used.
</prevsent>
<prevsent>in addition to the xml based pipeline components we have some pre generated outputs, such as greetings at the start and end of the dialogue or meta-acts such aswait-requests and thanking.
</prevsent>
</prevsection>
<citsent citstr=" W01-1611 ">
we are also exploiting the agent-based architecture to increase the systems adapt ivity in response generation, using the level of communicative confidence as described by jokinen and wilcock (2001).<papid> W01-1611 </papid></citsent>
<aftsection>
<nextsent>one of the important aspects of the systems adapt ivity is that it can recognize the correct topic that the user wants to talk about.
</nextsent>
<nextsent>by topic?
</nextsent>
<nextsent>we refer to the general subject matter that dialogue is about, such as bus timetables?
</nextsent>
<nextsent>and bus tickets?, realized by particular words inthe utterances.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y720">
<title id=" W02-0210.xml">adaptive dialogue systems  interaction with interact </title>
<section> recognition of discussion topic.  </section>
<citcontext>
<prevsection>
<prevsent>and tickets?.
</prevsent>
<prevsent>one can then estimate the probability of named topic class for new dialogue segment by construing probability model defined on top of the map.
</prevsent>
</prevsection>
<citsent citstr=" W02-0214 ">
a detailed description of the experiments as well as results can be found in (lagus and kuusisto, 2002).<papid> W02-0214 </papid></citsent>
<aftsection>
<nextsent>4.2 topic recognition module.
</nextsent>
<nextsent>the topical semantic representation, i.e. the map coordinates, can be used as input for the dialogue manager, as one of the values of the current dialogue state.
</nextsent>
<nextsent>the system architecture thus integrates special topic recognition module that outputs the utterance topic in the information storage.
</nextsent>
<nextsent>forgiven text segment,say, the recognition result from the speech recognizer, the module returns the coordinates of the best-matching dialogue map unit as well as the most probable prior topic category (if prior categorization was used in labeling the map).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y721">
<title id=" W02-0210.xml">adaptive dialogue systems  interaction with interact </title>
<section> dialogue management.  </section>
<citcontext>
<prevsection>
<prevsent>the system could learn suitable interaction strategies from its interaction with the user, showing adaptation to various user habits and situations.
</prevsent>
<prevsent>5.1 constructive dialogue model.
</prevsent>
</prevsection>
<citsent citstr=" C96-2101 ">
a uniform basis for dialogue management canbe found in the communicative principles related to human rational and coordinated interaction (allwood et al, 2000; jokinen, 1996).<papid> C96-2101 </papid>the speakers are engaged in particular activity, they have certain role in that activity, and their actions are constrained by communicativeobligations.</citsent>
<aftsection>
<nextsent>they act by exchanging new information and constructing shared context inwhich to resolve the underlying task satisfactorily.
</nextsent>
<nextsent>the model consists of set of dialogue states,defined with the help of dialogue acts, observations of the context, and reinforcement values.
</nextsent>
<nextsent>each action results in new dialogue state.
</nextsent>
<nextsent>the dialogue act, dact, describes the actthat the speaker performs by particular utterance, while the topic top and new informationnewinfo denote the semantic content of the utterance and are related to the task domain.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y722">
<title id=" W01-1626.xml">a corpus study of evaluative and speculative language </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>analyses of annotator agreement and of characteristics of subjective language areperformed.
</prevsent>
<prevsent>this study yields knowledge needed to design eective machine learning systems for identifying subjective language.
</prevsent>
</prevsection>
<citsent citstr=" J94-2004 ">
subjectivity in natural language refers to aspects of language used to express opinions and evaluations (ban eld, 1982; wiebe, 1994).<papid> J94-2004 </papid></citsent>
<aftsection>
<nextsent>subjectivity tagging is distinguishing sentences used to present opinions and other forms of subjectivity (subjec tive sentences) from sentences used to objectively present factual information (objective sentences).this task is especially relevant for news reporting and internet forums, in which opinions of various agents are expressed.
</nextsent>
<nextsent>there are numerous applications for which subjectivity tagging is relevant.
</nextsent>
<nextsent>two are information retrieval and information extraction.
</nextsent>
<nextsent>current extraction and retrieval technology focuses almost exclusively on the subject matter of documents.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y724">
<title id=" W01-1626.xml">a corpus study of evaluative and speculative language </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>two are information retrieval and information extraction.
</prevsent>
<prevsent>current extraction and retrieval technology focuses almost exclusively on the subject matter of documents.
</prevsent>
</prevsection>
<citsent citstr=" P97-1005 ">
however, additional aspects of document in uence its relevance, including, e.g., the evidential status of the material presented, and the attitudes expressed about the topic (kessler et al, 1997).<papid> P97-1005 </papid></citsent>
<aftsection>
<nextsent>knowledge of subjective language would also be useful in ame recognition (spertus, 1997; kaufer, 2000), email classi cation (aone et al, 2000), intellectual attribution in text (teufel and moens, 2000), <papid> W00-1302 </papid>recognizing speaker role in radio broadcasts (barzilay et al., 2000), review mining (terveen et al, 1997),generation and style (hovy, 1987), clustering documents by ideological point of view (sack, 1995), and any other application that would bene from knowledge of how opinionated the language is, and whether or not the writer purports to objectively present factual material.</nextsent>
<nextsent>to use subjectivity tagging in applications, good linguistic clues must be found.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y725">
<title id=" W01-1626.xml">a corpus study of evaluative and speculative language </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>current extraction and retrieval technology focuses almost exclusively on the subject matter of documents.
</prevsent>
<prevsent>however, additional aspects of document in uence its relevance, including, e.g., the evidential status of the material presented, and the attitudes expressed about the topic (kessler et al, 1997).<papid> P97-1005 </papid></prevsent>
</prevsection>
<citsent citstr=" W00-1302 ">
knowledge of subjective language would also be useful in ame recognition (spertus, 1997; kaufer, 2000), email classi cation (aone et al, 2000), intellectual attribution in text (teufel and moens, 2000), <papid> W00-1302 </papid>recognizing speaker role in radio broadcasts (barzilay et al., 2000), review mining (terveen et al, 1997),generation and style (hovy, 1987), clustering documents by ideological point of view (sack, 1995), and any other application that would bene from knowledge of how opinionated the language is, and whether or not the writer purports to objectively present factual material.</citsent>
<aftsection>
<nextsent>to use subjectivity tagging in applications, good linguistic clues must be found.
</nextsent>
<nextsent>as with many pragmatic and discourse distinctions, existing lexical resources are not comprehensively coded forsubjectivity.
</nextsent>
<nextsent>the goal of our current work is learning subjectivity clues from corpora.
</nextsent>
<nextsent>this paper contributes to this goal by empirically examining subjectivity.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y729">
<title id=" W01-1626.xml">a corpus study of evaluative and speculative language </title>
<section> previous work on subjectivity.  </section>
<citcontext>
<prevsection>
<prevsent>there is great variety of such expressions, including many studied under the rubric of idioms(see, for example, (nunberg et al, 1994)).
</prevsent>
<prevsent>we address learning such expressions in another project.
</prevsent>
</prevsection>
<citsent citstr=" P99-1032 ">
tagging in previous work (wiebe et al, 1999; <papid> P99-1032 </papid>bruce and wiebe, 1999), corpus of sentences from the wall street journal treebank corpus (marcus et al, 1993) <papid> J93-2004 </papid>was manually annotated with subjectivity classi cations by multiple judges.</citsent>
<aftsection>
<nextsent>the judges were instructed to consider sentence to be subjective if they perceived any signi cant expression of subjectivity (of any source) in the sentence, and to consider the sentence to be objective, otherwise.
</nextsent>
<nextsent>agreement was summarized in terms of cohen s (cohen, 1960), which compares the total probability of agreement to that expected if the taggers  classi cations were statistically independent (i.e.,\chance agreement ).
</nextsent>
<nextsent>after two rounds of tagging by three judges, an average pairwise  valueof .69 was achieved on test set.
</nextsent>
<nextsent>the em learning algorithm was used to produce corrected tags representing the consensus opinions of the taggers (goodman, 1974; dawid and skene, 1979).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y731">
<title id=" W01-1626.xml">a corpus study of evaluative and speculative language </title>
<section> previous work on subjectivity.  </section>
<citcontext>
<prevsection>
<prevsent>there is great variety of such expressions, including many studied under the rubric of idioms(see, for example, (nunberg et al, 1994)).
</prevsent>
<prevsent>we address learning such expressions in another project.
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
tagging in previous work (wiebe et al, 1999; <papid> P99-1032 </papid>bruce and wiebe, 1999), corpus of sentences from the wall street journal treebank corpus (marcus et al, 1993) <papid> J93-2004 </papid>was manually annotated with subjectivity classi cations by multiple judges.</citsent>
<aftsection>
<nextsent>the judges were instructed to consider sentence to be subjective if they perceived any signi cant expression of subjectivity (of any source) in the sentence, and to consider the sentence to be objective, otherwise.
</nextsent>
<nextsent>agreement was summarized in terms of cohen s (cohen, 1960), which compares the total probability of agreement to that expected if the taggers  classi cations were statistically independent (i.e.,\chance agreement ).
</nextsent>
<nextsent>after two rounds of tagging by three judges, an average pairwise  valueof .69 was achieved on test set.
</nextsent>
<nextsent>the em learning algorithm was used to produce corrected tags representing the consensus opinions of the taggers (goodman, 1974; dawid and skene, 1979).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y732">
<title id=" W01-1626.xml">a corpus study of evaluative and speculative language </title>
<section> previous work on subjectivity.  </section>
<citcontext>
<prevsection>
<prevsent>in 10-fold cross validation experiments, probabilistic classier obtained an average accuracy on subjectivity tagging of 72.17%, more than 20 percentage points higher than baseline accuracy obtained by always choosing the more frequent class.
</prevsent>
<prevsent>five part-of-speech features, two lexical features, and paragraph feature were used.
</prevsent>
</prevsection>
<citsent citstr=" P98-2127 ">
to identify richer features, (wiebe, 2000) usedlin (1998) <papid> P98-2127 </papid>method for clustering words according to distributional similarity, seeded by small amount of detailed manual annotation, to automatically identify adjective pses.</citsent>
<aftsection>
<nextsent>there are two parameters of this process, neither of which was varied in (wiebe, 2000): c, the cluster size considered, and ft , ltering threshold, such that, if the seed word and the words in its cluster have, as set, lower precision than the ltering threshold on the training data, the entire cluster, including the seed word, is ltered out.
</nextsent>
<nextsent>this process is adapted for use in the current paper, as described in section 7.
</nextsent>
<nextsent>in expression-level annotation, the judges rst identify the sentences they believe are subjective.
</nextsent>
<nextsent>they next identify the subjective elements in the sentence, i.e., the expressions they feel are responsible for the subjective classi cation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y738">
<title id=" W01-1626.xml">a corpus study of evaluative and speculative language </title>
<section> subjective-element annotation.  </section>
<citcontext>
<prevsection>
<prevsent>in datasets wsj-se and ng-se, the taggers were also asked to specify one of ve subjective element types: e+ (positive evaluative), e  (negative evaluative), e?
</prevsent>
<prevsent>(some other type of evalua tion), (uncertainty), and (none of the above),with the option to assign multiple types to an instance.
</prevsent>
</prevsection>
<citsent citstr=" C92-3145 ">
all corpora were stemmed (karp et al, 1992) <papid> C92-3145 </papid>and part-of-speech tagged (brill, 1992).<papid> A92-1021 </papid></citsent>
<aftsection>
<nextsent>6.2 agreement among taggers.
</nextsent>
<nextsent>there are techniques for analyzing agreement when annotations involve segment boundaries (litman and passonneau, 1995; <papid> P95-1015 </papid>marcu et al, 1999), but our focus in this paper is on words.</nextsent>
<nextsent>thus, our analyses are at the word level: each word is classi ed as either appearing in subjective element or not.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y739">
<title id=" W01-1626.xml">a corpus study of evaluative and speculative language </title>
<section> subjective-element annotation.  </section>
<citcontext>
<prevsection>
<prevsent>in datasets wsj-se and ng-se, the taggers were also asked to specify one of ve subjective element types: e+ (positive evaluative), e  (negative evaluative), e?
</prevsent>
<prevsent>(some other type of evalua tion), (uncertainty), and (none of the above),with the option to assign multiple types to an instance.
</prevsent>
</prevsection>
<citsent citstr=" A92-1021 ">
all corpora were stemmed (karp et al, 1992) <papid> C92-3145 </papid>and part-of-speech tagged (brill, 1992).<papid> A92-1021 </papid></citsent>
<aftsection>
<nextsent>6.2 agreement among taggers.
</nextsent>
<nextsent>there are techniques for analyzing agreement when annotations involve segment boundaries (litman and passonneau, 1995; <papid> P95-1015 </papid>marcu et al, 1999), but our focus in this paper is on words.</nextsent>
<nextsent>thus, our analyses are at the word level: each word is classi ed as either appearing in subjective element or not.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y740">
<title id=" W01-1626.xml">a corpus study of evaluative and speculative language </title>
<section> subjective-element annotation.  </section>
<citcontext>
<prevsection>
<prevsent>all corpora were stemmed (karp et al, 1992) <papid> C92-3145 </papid>and part-of-speech tagged (brill, 1992).<papid> A92-1021 </papid></prevsent>
<prevsent>6.2 agreement among taggers.</prevsent>
</prevsection>
<citsent citstr=" P95-1015 ">
there are techniques for analyzing agreement when annotations involve segment boundaries (litman and passonneau, 1995; <papid> P95-1015 </papid>marcu et al, 1999), but our focus in this paper is on words.</citsent>
<aftsection>
<nextsent>thus, our analyses are at the word level: each word is classi ed as either appearing in subjective element or not.
</nextsent>
<nextsent>punctuation is excluded from our analyses.
</nextsent>
<nextsent>the wsj data is divided into two subsets in this section, exp1 and exp2.
</nextsent>
<nextsent>as mentioned above, in wsj-se exp1 andexp2, the taggers also classi ed subjective elements with respect to the type of subjectivity being expressed.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y743">
<title id=" W01-1626.xml">a corpus study of evaluative and speculative language </title>
<section> subjective-element annotation.  </section>
<citcontext>
<prevsection>
<prevsent>additionally, these ndings suggest that it may be possible to automatically correct the type biases expressed by the taggers using the technique described in(bruce and wiebe, 1999), topic that will be investigated in future work.
</prevsent>
<prevsent>6.3 uniqueness.
</prevsent>
</prevsection>
<citsent citstr=" W98-1126 ">
based on previous work (wiebe et al, 1998), <papid> W98-1126 </papid>we hypothesized that low-frequency words are associated with subjectivity.</citsent>
<aftsection>
<nextsent>table 6 provides evidence that the number of unique words (words that appear just once) in subjective elements is higher than expected.
</nextsent>
<nextsent>the rst row gives information for all words and the second gives information for words that appear just once.
</nextsent>
<nextsent>the gures in the num columns are total counts, and the gures in the columns give the proportion that appear in subjective elements.
</nextsent>
<nextsent>the agree columns give in all words nouns verbs modals adj adverbs det exp1 full match 0:4216 0:4228 0.2933 0:1422 0:5919 0:1207 0:5000 partial match 0:5156 0:4570 0.4447 0:3011 0:6607 0:3305 0:5000 exp2 full match 0:3041 0:2353 0.2765 0:1429 0:5794 0:1207 0:0000 partial match 0:4209 0:2353 0.3994 0:3494 0:6719 0:4439 0:1429 table 3:  values for type agreement using all types in the wsj data all words nouns verbs modals adj adverbs det exp1 full match 0:3377 0:0440 0.1648 0:1968 0:5443 0:3810 0:0000 partial match 0:5287 0:1637 0.3765 0:4903 0:8125 0:3810 0:0000 exp2 full match 0:2569 0:0000 0.1923 0:1509 0:4783 0:1707 0:1429 partial match 0:4789 0:0000 0.4167 0:4000 0:8056 0:7671 0:4000 table 4:  values for type agreement using e,o,u in the wsj data sym.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y744">
<title id=" W02-0301.xml">tuning support vector machines for biomedical named entity recognition </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>conceptually, named entity recognition consists of two tasks: identification, which finds the region of named entity ina text, and classification, which determines the semantic class of that named entity.
</prevsent>
<prevsent>the following illustrates biomedical named entity recognition.thus, ciitaprotein not only activates the expression of class ii genes dna but recruits another cell-specific co activator to increase transcript ional activity of class ii promoters dna in cellscelltype.?
</prevsent>
</prevsection>
<citsent citstr=" C00-1030 ">
machine learning approach has been applied to biomedical named entity recognition (nobata et al, 1999; collier et al, 2000; <papid> C00-1030 </papid>yamada et al, 2000; shimpuku, 2002).</citsent>
<aftsection>
<nextsent>however, no work has achieved sufficient recognition accuracy.
</nextsent>
<nextsent>one reason is the lack of annotated corpora for training as is often the case of new domain.
</nextsent>
<nextsent>nobata et al (1999) and collier et al (2000) <papid> C00-1030 </papid>trained their model with only 100 annotated paper abstracts from the medline database (national library of medicine, 1999), and yamada et al (2000) used only 77 annotated paper abstracts.</nextsent>
<nextsent>in addition, it is difficult to compare the techniques used in each study because they used closed and different corpus.to overcome such situation, the genia corpus (ohta et al, 2002) has been developed, and atthis time it is the largest biomedical annotated corpus available to public, containing 670 annotated abstracts of the medline database.another reason for low accuracies is that biomedical named entities are essentially hard to recognize using standard feature sets compared with the named entities in newswire articles (nobata et al, 2000).<papid> W00-0904 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y746">
<title id=" W02-0301.xml">tuning support vector machines for biomedical named entity recognition </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>one reason is the lack of annotated corpora for training as is often the case of new domain.
</prevsent>
<prevsent>nobata et al (1999) and collier et al (2000) <papid> C00-1030 </papid>trained their model with only 100 annotated paper abstracts from the medline database (national library of medicine, 1999), and yamada et al (2000) used only 77 annotated paper abstracts.</prevsent>
</prevsection>
<citsent citstr=" W00-0904 ">
in addition, it is difficult to compare the techniques used in each study because they used closed and different corpus.to overcome such situation, the genia corpus (ohta et al, 2002) has been developed, and atthis time it is the largest biomedical annotated corpus available to public, containing 670 annotated abstracts of the medline database.another reason for low accuracies is that biomedical named entities are essentially hard to recognize using standard feature sets compared with the named entities in newswire articles (nobata et al, 2000).<papid> W00-0904 </papid></citsent>
<aftsection>
<nextsent>thus, we need to employ powerful machine learning techniques which can incorporate various and complex features inconsistent way.
</nextsent>
<nextsent>support vector machines (svms) (vapnik, 1995) and maximum entropy (me) method (berger et al, 1996) <papid> J96-1002 </papid>are powerful learning methods that satisfy such requirements, and are applied successfully toother nlp tasks (kudo and matsumoto, 2000; <papid> W00-0730 </papid>nakagawa et al, 2001; ratnaparkhi, 1996).<papid> W96-0213 </papid></nextsent>
<nextsent>in this paper, we apply support vector machines to biomedical named entity recognition and train them with association for computational linguistics.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y747">
<title id=" W02-0301.xml">tuning support vector machines for biomedical named entity recognition </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in addition, it is difficult to compare the techniques used in each study because they used closed and different corpus.to overcome such situation, the genia corpus (ohta et al, 2002) has been developed, and atthis time it is the largest biomedical annotated corpus available to public, containing 670 annotated abstracts of the medline database.another reason for low accuracies is that biomedical named entities are essentially hard to recognize using standard feature sets compared with the named entities in newswire articles (nobata et al, 2000).<papid> W00-0904 </papid></prevsent>
<prevsent>thus, we need to employ powerful machine learning techniques which can incorporate various and complex features inconsistent way.</prevsent>
</prevsection>
<citsent citstr=" J96-1002 ">
support vector machines (svms) (vapnik, 1995) and maximum entropy (me) method (berger et al, 1996) <papid> J96-1002 </papid>are powerful learning methods that satisfy such requirements, and are applied successfully toother nlp tasks (kudo and matsumoto, 2000; <papid> W00-0730 </papid>nakagawa et al, 2001; ratnaparkhi, 1996).<papid> W96-0213 </papid></citsent>
<aftsection>
<nextsent>in this paper, we apply support vector machines to biomedical named entity recognition and train them with association for computational linguistics.
</nextsent>
<nextsent>the biomedical domain, philadelphia, july 2002, pp.
</nextsent>
<nextsent>1-8.
</nextsent>
<nextsent>proceedings of the workshop on natural language processing in the genia corpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y748">
<title id=" W02-0301.xml">tuning support vector machines for biomedical named entity recognition </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in addition, it is difficult to compare the techniques used in each study because they used closed and different corpus.to overcome such situation, the genia corpus (ohta et al, 2002) has been developed, and atthis time it is the largest biomedical annotated corpus available to public, containing 670 annotated abstracts of the medline database.another reason for low accuracies is that biomedical named entities are essentially hard to recognize using standard feature sets compared with the named entities in newswire articles (nobata et al, 2000).<papid> W00-0904 </papid></prevsent>
<prevsent>thus, we need to employ powerful machine learning techniques which can incorporate various and complex features inconsistent way.</prevsent>
</prevsection>
<citsent citstr=" W00-0730 ">
support vector machines (svms) (vapnik, 1995) and maximum entropy (me) method (berger et al, 1996) <papid> J96-1002 </papid>are powerful learning methods that satisfy such requirements, and are applied successfully toother nlp tasks (kudo and matsumoto, 2000; <papid> W00-0730 </papid>nakagawa et al, 2001; ratnaparkhi, 1996).<papid> W96-0213 </papid></citsent>
<aftsection>
<nextsent>in this paper, we apply support vector machines to biomedical named entity recognition and train them with association for computational linguistics.
</nextsent>
<nextsent>the biomedical domain, philadelphia, july 2002, pp.
</nextsent>
<nextsent>1-8.
</nextsent>
<nextsent>proceedings of the workshop on natural language processing in the genia corpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y750">
<title id=" W02-0301.xml">tuning support vector machines for biomedical named entity recognition </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in addition, it is difficult to compare the techniques used in each study because they used closed and different corpus.to overcome such situation, the genia corpus (ohta et al, 2002) has been developed, and atthis time it is the largest biomedical annotated corpus available to public, containing 670 annotated abstracts of the medline database.another reason for low accuracies is that biomedical named entities are essentially hard to recognize using standard feature sets compared with the named entities in newswire articles (nobata et al, 2000).<papid> W00-0904 </papid></prevsent>
<prevsent>thus, we need to employ powerful machine learning techniques which can incorporate various and complex features inconsistent way.</prevsent>
</prevsection>
<citsent citstr=" W96-0213 ">
support vector machines (svms) (vapnik, 1995) and maximum entropy (me) method (berger et al, 1996) <papid> J96-1002 </papid>are powerful learning methods that satisfy such requirements, and are applied successfully toother nlp tasks (kudo and matsumoto, 2000; <papid> W00-0730 </papid>nakagawa et al, 2001; ratnaparkhi, 1996).<papid> W96-0213 </papid></citsent>
<aftsection>
<nextsent>in this paper, we apply support vector machines to biomedical named entity recognition and train them with association for computational linguistics.
</nextsent>
<nextsent>the biomedical domain, philadelphia, july 2002, pp.
</nextsent>
<nextsent>1-8.
</nextsent>
<nextsent>proceedings of the workshop on natural language processing in the genia corpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y752">
<title id=" W02-0301.xml">tuning support vector machines for biomedical named entity recognition </title>
<section> named entity recognition using svms.  </section>
<citcontext>
<prevsection>
<prevsent>3.1 named entity recognition as classification.
</prevsent>
<prevsent>we formulate the named entity task as the classification of each word with context to one of the classes that represent region information and named entitys semantic class.
</prevsent>
</prevsection>
<citsent citstr=" W95-0107 ">
several representations to encode region information are proposed and examined (ramshaw and marcus, 1995; <papid> W95-0107 </papid>uchimoto et al, 2000; <papid> P00-1042 </papid>kudo and matsumoto, 2001).<papid> N01-1025 </papid></citsent>
<aftsection>
<nextsent>in this paper, we employ the simplest bio representation, which is also used in (yamada et al, 2000).
</nextsent>
<nextsent>we modify this representation in section 5.1 in order to accelerate the svm training.
</nextsent>
<nextsent>in the bio representation, the region information is represented as the class prefixes b-?
</nextsent>
<nextsent>and i-?, and class o?.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y753">
<title id=" W02-0301.xml">tuning support vector machines for biomedical named entity recognition </title>
<section> named entity recognition using svms.  </section>
<citcontext>
<prevsection>
<prevsent>3.1 named entity recognition as classification.
</prevsent>
<prevsent>we formulate the named entity task as the classification of each word with context to one of the classes that represent region information and named entitys semantic class.
</prevsent>
</prevsection>
<citsent citstr=" P00-1042 ">
several representations to encode region information are proposed and examined (ramshaw and marcus, 1995; <papid> W95-0107 </papid>uchimoto et al, 2000; <papid> P00-1042 </papid>kudo and matsumoto, 2001).<papid> N01-1025 </papid></citsent>
<aftsection>
<nextsent>in this paper, we employ the simplest bio representation, which is also used in (yamada et al, 2000).
</nextsent>
<nextsent>we modify this representation in section 5.1 in order to accelerate the svm training.
</nextsent>
<nextsent>in the bio representation, the region information is represented as the class prefixes b-?
</nextsent>
<nextsent>and i-?, and class o?.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y754">
<title id=" W02-0301.xml">tuning support vector machines for biomedical named entity recognition </title>
<section> named entity recognition using svms.  </section>
<citcontext>
<prevsection>
<prevsent>3.1 named entity recognition as classification.
</prevsent>
<prevsent>we formulate the named entity task as the classification of each word with context to one of the classes that represent region information and named entitys semantic class.
</prevsent>
</prevsection>
<citsent citstr=" N01-1025 ">
several representations to encode region information are proposed and examined (ramshaw and marcus, 1995; <papid> W95-0107 </papid>uchimoto et al, 2000; <papid> P00-1042 </papid>kudo and matsumoto, 2001).<papid> N01-1025 </papid></citsent>
<aftsection>
<nextsent>in this paper, we employ the simplest bio representation, which is also used in (yamada et al, 2000).
</nextsent>
<nextsent>we modify this representation in section 5.1 in order to accelerate the svm training.
</nextsent>
<nextsent>in the bio representation, the region information is represented as the class prefixes b-?
</nextsent>
<nextsent>and i-?, and class o?.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y760">
<title id=" W01-0707.xml">probabilistic models for ppattachment resolution and np analysis </title>
<section> nuclei and sequences of nuclei.  </section>
<citcontext>
<prevsection>
<prevsent>for example, the base np the white horse?
</prevsent>
<prevsent>is nucleus, since the attachments of both the determiner and the adjective to the nounare straightforward.
</prevsent>
</prevsection>
<citsent citstr=" C00-2099 ">
the segmentation into nuclei relies on manually built chunker, similar to the one described in (ait-mokhtar and chanod, 1997), andre sembles the one proposed in (samuelsson, 2000).<papid> C00-2099 </papid></citsent>
<aftsection>
<nextsent>the motivation for this assumption is twofold.
</nextsent>
<nextsent>first, the amount of grammatical information carried by individual words varies greatly across language families.
</nextsent>
<nextsent>grammatical information carried by function wordsin non-agglutinative languages, for instance, is realized morphologically in agglutinative languages.
</nextsent>
<nextsent>a model manipulating dependencies at the word level only would be constrained to the specific amount of grammatical and lexical information associated with words in given language.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y761">
<title id=" W01-0707.xml">probabilistic models for ppattachment resolution and np analysis </title>
<section> attachment model.  </section>
<citcontext>
<prevsection>
<prevsent>assuming that the dependencies are built by processing the chain in linear order, we have: fl ffi !
</prevsent>
<prevsent>#   $&amp;% fffi  $&amp;% ffi  fl &ffi;  $(% )  *  %,+-+-+    $&amp;%  . /10 % flfi / ffi  / $(%2)  (1) / differs from  / $(% only in that it additionally specifies particular attachment site    for fi3  such that nocycle nor crossing dependencies are produced.
</prevsent>
</prevsection>
<citsent citstr=" C96-1058 ">
in order to avoid sparse data problems, we make the simplifying assumption (similar to the one presented in (eisner, 1996)) <papid> C96-1058 </papid>that the attachment of nucleus fi3  to nucleus    depends only on the set of indices of the preceding dependency relations (in order to avoid cycles and crossing dependencies) and on the three nuclei fi3  ,   and 4 /  , where 5 /  denotes the last nucleus being attached to    . 5 /  is thus the closest sibling of fi3  . conditioning attachment on it the attachment of.</citsent>
<aftsection>
<nextsent>fi3  allows capturing the fact that the object of verb may depend on its subject, that the indirect object may depend on the direct object, and other similar indirect dependencies.
</nextsent>
<nextsent>in order to focus on the probabilities of interest, we use the following simplified notation: fffi / ffi  / $&amp;%1) !6 fl78fi / 9: fl;fi3  ffi     5 /  ;78fi / ; (2) where 78fi /  represents the graph produced by the dependencies generated so far.
</nextsent>
<nextsent>if this graph contains cycles or crossing links, the associated probability is 0.
</nextsent>
<nextsent>making explicit the different elements of nucleus,.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y762">
<title id=" W01-0707.xml">probabilistic models for ppattachment resolution and np analysis </title>
<section> comparison with other models.  </section>
<citcontext>
<prevsection>
<prevsent>the probabilistic model described in (lauer and dras, 1994), addresses the problem of parsing english nominal compounds.
</prevsent>
<prevsent>a comparison with this model is of interest to us since the sequences we are interested in contain both verbal and nominal phrases in french.
</prevsent>
</prevsection>
<citsent citstr=" P98-2177 ">
a second model relevant to our discussion is the one proposed in (ratnaparkhi,1998), <papid> P98-2177 </papid>addressing the problem of unsupervised learning for pp attachment resolution in verb noun pp sequences.</citsent>
<aftsection>
<nextsent>lastly, the third model, even though used in supervised setting, addresses the more complex problem of probabilistic dependency parsing on complete sentences 2.
</nextsent>
<nextsent>in the model proposed in (lauer and dras, 1994), that we will refer to as model l, the quantity denoted as z[5\ /^] \ 1ffi _`bac` ] \  is the same as the quantity defined by our equation (8).
</nextsent>
<nextsent>the quantity fl4d  in model is the same as our quantity fl78fi / 9 . there is no equivalent for probabilities involved in equations (9) to (11) in model l, since there is no need for them in analysing english nominal compounds.
</nextsent>
<nextsent>lastly, our probability to generate pq1r / depends only on inmodel (the dependency on the pos category is obvious since only nouns are considered).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y768">
<title id=" W01-0707.xml">probabilistic models for ppattachment resolution and np analysis </title>
<section> comparison with other models.  </section>
<citcontext>
<prevsection>
<prevsent>we present below the results we obtained with this model.from the models proposed in (eisner, 1996), <papid> C96-1058 </papid>we retain only the model referred to as model in this work, since the best results were obtained with it.</prevsent>
<prevsent>model does not make use of semantic information, nor does it relyon nuclei.</prevsent>
</prevsection>
<citsent citstr=" W95-0103 ">
so the sequence with fork, which corresponds to only one nucleus is treated as three word sequence in model c. apart from this difference, model directly relies on combination of equations (10) and (12), namely conditioning by pq1r , j=ak9 and j=ak9=v , both the probability of generating j=ak / and the one of generating pq1r / . thus, model uses reduced version of equation (12) and an extended version of 2other models, as (collins and brooks, 1995; <papid> W95-0103 </papid>merlo et al., 1998) for pp-attachment resolution, or (collins, 1997; <papid> P97-1003 </papid>samuelsson, 2000) <papid> C00-2099 </papid>for probabilistic parsing, are somewhat related, but their supervised nature makes any direct comparison impossible.</citsent>
<aftsection>
<nextsent>equation (10).
</nextsent>
<nextsent>this extension could be used in our casetoo, but, since the input to our processing chain consists of tagged words (unless the input of the stochastic dependency parser of (eisner, 1996)), <papid> C96-1058 </papid>we do not think it necessary.furthermore, by marginalizing the counts for the estimates of our general model, we can derive the probabilities used in other models.</nextsent>
<nextsent>we thus view our model as generalization of the previous ones.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y769">
<title id=" W01-0707.xml">probabilistic models for ppattachment resolution and np analysis </title>
<section> comparison with other models.  </section>
<citcontext>
<prevsection>
<prevsent>we present below the results we obtained with this model.from the models proposed in (eisner, 1996), <papid> C96-1058 </papid>we retain only the model referred to as model in this work, since the best results were obtained with it.</prevsent>
<prevsent>model does not make use of semantic information, nor does it relyon nuclei.</prevsent>
</prevsection>
<citsent citstr=" P97-1003 ">
so the sequence with fork, which corresponds to only one nucleus is treated as three word sequence in model c. apart from this difference, model directly relies on combination of equations (10) and (12), namely conditioning by pq1r , j=ak9 and j=ak9=v , both the probability of generating j=ak / and the one of generating pq1r / . thus, model uses reduced version of equation (12) and an extended version of 2other models, as (collins and brooks, 1995; <papid> W95-0103 </papid>merlo et al., 1998) for pp-attachment resolution, or (collins, 1997; <papid> P97-1003 </papid>samuelsson, 2000) <papid> C00-2099 </papid>for probabilistic parsing, are somewhat related, but their supervised nature makes any direct comparison impossible.</citsent>
<aftsection>
<nextsent>equation (10).
</nextsent>
<nextsent>this extension could be used in our casetoo, but, since the input to our processing chain consists of tagged words (unless the input of the stochastic dependency parser of (eisner, 1996)), <papid> C96-1058 </papid>we do not think it necessary.furthermore, by marginalizing the counts for the estimates of our general model, we can derive the probabilities used in other models.</nextsent>
<nextsent>we thus view our model as generalization of the previous ones.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y773">
<title id=" W01-0707.xml">probabilistic models for ppattachment resolution and np analysis </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>q 0 0.2 0.4 0.6 0.8 1 precision 0.83 0.85 0.83 0.81 0.8 0.78 table 2: influence of these results first show that the accurate information is sufficient to derive good estimates.
</prevsent>
<prevsent>furthermore, discounting part of the less accurate information seems to be essential, since the worst results are obtained when  . we can also notice that the best results are well above the baseline obtained by relying only on information present in our lexicon, thus justifying ama chine learning approach to the problem of pp attachment resolution.
</prevsent>
</prevsection>
<citsent citstr=" J93-1005 ">
lastly, the results we obtained are similar to the ones obtained by different authors on similar task, as (ratnaparkhi, 1998; <papid> P98-2177 </papid>hindle and rooth, 1993; <papid> J93-1005 </papid>brill and resnik, 1994) <papid> C94-2195 </papid>for example.</citsent>
<aftsection>
<nextsent>6.2 evaluation of our general model.
</nextsent>
<nextsent>the model described in section 3 was tested against 900 manually annotated sequences of nuclei from the newspaper le monde?, randomly selected from portion of the corpus which was held out from training.
</nextsent>
<nextsent>the average length of sequences was of 3.33 nuclei.
</nextsent>
<nextsent>the trivial method consisting in linking every nucleus to the preceding one achieves an accuracy of 72.08%.the proposed model was used to assign probability estimates to dependency links between nuclei in our own implementation of the parser described in(eisner, 1996).<papid> C96-1058 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y774">
<title id=" W01-0707.xml">probabilistic models for ppattachment resolution and np analysis </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>q 0 0.2 0.4 0.6 0.8 1 precision 0.83 0.85 0.83 0.81 0.8 0.78 table 2: influence of these results first show that the accurate information is sufficient to derive good estimates.
</prevsent>
<prevsent>furthermore, discounting part of the less accurate information seems to be essential, since the worst results are obtained when  . we can also notice that the best results are well above the baseline obtained by relying only on information present in our lexicon, thus justifying ama chine learning approach to the problem of pp attachment resolution.
</prevsent>
</prevsection>
<citsent citstr=" C94-2195 ">
lastly, the results we obtained are similar to the ones obtained by different authors on similar task, as (ratnaparkhi, 1998; <papid> P98-2177 </papid>hindle and rooth, 1993; <papid> J93-1005 </papid>brill and resnik, 1994) <papid> C94-2195 </papid>for example.</citsent>
<aftsection>
<nextsent>6.2 evaluation of our general model.
</nextsent>
<nextsent>the model described in section 3 was tested against 900 manually annotated sequences of nuclei from the newspaper le monde?, randomly selected from portion of the corpus which was held out from training.
</nextsent>
<nextsent>the average length of sequences was of 3.33 nuclei.
</nextsent>
<nextsent>the trivial method consisting in linking every nucleus to the preceding one achieves an accuracy of 72.08%.the proposed model was used to assign probability estimates to dependency links between nuclei in our own implementation of the parser described in(eisner, 1996).<papid> C96-1058 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y787">
<title id=" W02-0109.xml">nltk the natural language toolkit </title>
<section> other approaches.  </section>
<citcontext>
<prevsection>
<prevsent>other researchers and developers.
</prevsent>
<prevsent>a variety of tool kits have been created for research or r&d; purposes.
</prevsent>
</prevsection>
<citsent citstr=" W02-0108 ">
examples include the cmu-cambridge statistical language modeling toolkit (clarkson and rosenfeld, 1997), the emu speech database system (harrington and cassidy, 1999), the general architecture for text engineering (bontcheva et al, 2002), <papid> W02-0108 </papid>the maxent package for maximum entropy models (baldridge et al, 2002b), and the annotation graph toolkit (maeda et al, 2002).</citsent>
<aftsection>
<nextsent>although not originally motivated by pedagogical needs, all of these tool kits have pedagogical applications and many have already been used in teaching.
</nextsent>
<nextsent>nltk provides simple, extensible, uniform framework for assignments, projects, and class demonstrations.
</nextsent>
<nextsent>it is well documented, easy to learn, and simple to use.
</nextsent>
<nextsent>we hope that nltk will allow computational linguistics classes to include more hands-on experience with using and building nlp components and systems.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y788">
<title id=" W00-1423.xml">coordination and context dependence in the generation of embodied conversation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this argument akes on particular importance given that users repeat hem- selves needlessly, mistake when it is their turn to speak, and so forth when interacting with voice di-alogue systems (oviatt, 1995): tn -life; noisy situa-tions like these provoke the non-verbal modalities to come into play (rogers, 1978).
</prevsent>
<prevsent>in this paper, we describe the generation of com-municative actions in an implemented embodied conversational gent.
</prevsent>
</prevsection>
<citsent citstr=" P97-1026 ">
our generation framework adopts goal-directed view of generation and casts knowledge about communicative action in the form of grammar that specifies how forms combine, what interpretive ffects they impart and in what contexts they are appropriate (appelt, 1985; moore, 1994; dale, 1992; stone and doran, 1997).<papid> P97-1026 </papid></citsent>
<aftsection>
<nextsent>we ex-pand this framework to take into account findings, by ourselves and others, on the relationship between spontaneous co verbal hand gestures and speech.
</nextsent>
<nextsent>in particular, our agent plans each utterance so that multiple communicative goals may be realized op-portunistically by composite action including not only speech but also co verbal gesture.
</nextsent>
<nextsent>by describing gesture declaratively in terms of its discourse func-tion, semantics and synch rony with speech, we en-sure that co verbal gesture fits the context and the on- going speech in ways representative of natural hu-man conversation.
</nextsent>
<nextsent>the result is streamlined imple-mentation that instantiates important theoretical in-sights into the relationship between speech and ges-ture in human-human conversation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y789">
<title id=" W00-1423.xml">coordination and context dependence in the generation of embodied conversation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>pragmatically, speech and ges-ture mark information about his meaning as advanc-ing the purposes of the conversation a consistent way.
</prevsent>
<prevsent>indeed, gesture often emphasizes information that is also focused pragmatically by mechanisms like prosody in speech (cassell, 2000b).
</prevsent>
</prevsection>
<citsent citstr=" W98-0210 ">
the seman-tic and pragmatic ompatibility seen in the gesture- speech relationship recalls the interaction of words and graphics in multimodal presentations (feiner and mckeown, 1991; green et al, 1998; <papid> W98-0210 </papid>wahlster et al, 1991 ).</citsent>
<aftsection>
<nextsent>in fact, some suggest (mcneill, 1992), that gesture and speech arise together om an under- lying representation that has both visual and linguis-tic aspects, and so the relationship between gesture and speech is essential to the production of meaning and to its comprehension.
</nextsent>
<nextsent>this theoretical perspective on speech and gesture involves two key claims with computational import: that gesture and speech ref lectacommon concep-tual source; and that the content and form of ges-ture is tuned to the communicative context and the 172 actor communicative intentions.
</nextsent>
<nextsent>we believe that these characteristics of the use of gesture are uni-versal, and see the key contribution of this work as providing agen eral framework for building dialogue systems in accord with them.
</nextsent>
<nextsent>however, concrete !mplementationrequires   more thanjustgeneralities behind its operation; we also need an understanding of the precise ways gesture and speech are used to-gether in particular task and setting.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y790">
<title id=" W00-1423.xml">coordination and context dependence in the generation of embodied conversation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for example, vari-ety of changing features determine whether marked forms in speech and gesture are appropriate in the context.
</prevsent>
<prevsent>rea dialogue manager acks the chang-ing status of such features as: attentionalprominence, presented (as usual in natural language generation) by setting up context set for each entity (dale, 1992).
</prevsent>
</prevsection>
<citsent citstr=" P98-2204 ">
our model of prominence is simple local one sim-ilar to (strube, 1998).<papid> P98-2204 </papid></citsent>
<aftsection>
<nextsent>o cognitive status, including whether an entity is hearer-old or hearer-new (prince, 1992), and whether an entity is in-focus or not (gundel et al, 1993).
</nextsent>
<nextsent>we can assume that houses and their rooms are hearer-new until rea describes them; and that just those entities mentioned in the prior sentence are in-focus.
</nextsent>
<nextsent>information structure, including the open propositions or, following (steedman, 1991 ), themes, which describe the salient questions currently at issue in the discourse (prince, 1986).
</nextsent>
<nextsent>in rea dialogue, open questions are always general questions about some entity raised by recent urn; although in principle such an open question ought be formalized as theme(xp.pe), rea can use the simpler theme(e).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y791">
<title id=" W00-1423.xml">coordination and context dependence in the generation of embodied conversation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the knowledge base kinds of information provide mechanism for spec-ifying and reasoning about such functions.
</prevsent>
<prevsent>the knowledge base is structured to describe the rela-tionship between the system private information and the questions of interest hat that information can be used to settle.
</prevsent>
</prevsection>
<citsent citstr=" P94-1001 ">
once the user words have been interpreted, layer of production rules con-structs obligations for response (traum and allen, 1994); <papid> P94-1001 </papid>then, second layer plans to meet hese obli-gations by deciding to present specified kind of information about specified object.</citsent>
<aftsection>
<nextsent>this deter-mines some concrete communicative goals--facts of this kind that contribution to dialogue could make.
</nextsent>
<nextsent>both speech and gesture can access the whole structured database in realizing these concrete communicative goals.
</nextsent>
<nextsent>for example, variety of facts that bear on where residence is--which city, which neighborhood or, if appropriate, where in building--all provide the same kind of information, and would therefore fit the obligation to specify the location of residence.
</nextsent>
<nextsent>or, to implement the rule for presentation described in connection with ( 1 ), we can associate an obligation of presentation with cluster of facts describing an object type, its loca-tion in house, and its size, shape or quality.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y793">
<title id=" W00-1423.xml">coordination and context dependence in the generation of embodied conversation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>such representa-tions have been explored in research on combining linguistic and graphical interaction.
</prevsent>
<prevsent>for example, multimodal managers have been described to allo-cate an underlying content representation for gen-eration of text and graphics (wahlster et al, 1991; green et al, 1998).<papid> W98-0210 </papid></prevsent>
</prevsection>
<citsent citstr=" P97-1036 ">
meanwhile, (johnston et al, 1997; <papid> P97-1036 </papid>johnston, 1998) <papid> P98-1102 </papid>describe formalism for tightly-coupled interpretation which uses gram-mar and semantic onstraints oanalyze input from speech and pen.</citsent>
<aftsection>
<nextsent>while many insights from these formalisms are relevant in embodied conversation, spontaneous gesture requires adistinct analysis with different emphasis:for example;-we need some no-tion of discourse pragmatics that would allow us to predict where gesture occurs with respect to speech, conclusion . . .
</nextsent>
<nextsent>research on the robustness of human conversation suggests that dialogue agent capable of acting as conversational partner would provide for effi-cient and natural collaborative dialogue.
</nextsent>
<nextsent>but human conversational partners display gestures that derive from the same underlying conceptual source as their speech, and which relate appropriately to their com-municative intent.
</nextsent>
<nextsent>in this paper, we have summa-rized the evidence for this view of human conver-sation, and shown how it informs the generation of communicative action in our artificial embodied conversational agent, rea.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y794">
<title id=" W00-1423.xml">coordination and context dependence in the generation of embodied conversation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>such representa-tions have been explored in research on combining linguistic and graphical interaction.
</prevsent>
<prevsent>for example, multimodal managers have been described to allo-cate an underlying content representation for gen-eration of text and graphics (wahlster et al, 1991; green et al, 1998).<papid> W98-0210 </papid></prevsent>
</prevsection>
<citsent citstr=" P98-1102 ">
meanwhile, (johnston et al, 1997; <papid> P97-1036 </papid>johnston, 1998) <papid> P98-1102 </papid>describe formalism for tightly-coupled interpretation which uses gram-mar and semantic onstraints oanalyze input from speech and pen.</citsent>
<aftsection>
<nextsent>while many insights from these formalisms are relevant in embodied conversation, spontaneous gesture requires adistinct analysis with different emphasis:for example;-we need some no-tion of discourse pragmatics that would allow us to predict where gesture occurs with respect to speech, conclusion . . .
</nextsent>
<nextsent>research on the robustness of human conversation suggests that dialogue agent capable of acting as conversational partner would provide for effi-cient and natural collaborative dialogue.
</nextsent>
<nextsent>but human conversational partners display gestures that derive from the same underlying conceptual source as their speech, and which relate appropriately to their com-municative intent.
</nextsent>
<nextsent>in this paper, we have summa-rized the evidence for this view of human conver-sation, and shown how it informs the generation of communicative action in our artificial embodied conversational agent, rea.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y795">
<title id=" W01-0904.xml">translating treebank annotation for evaluation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this case the learned artefact is used to annotate the examples, which can thenbe compared against the correctly annotated version.
</prevsent>
<prevsent>hence, correctly annotated corpora are vital for the evaluation of very large number of nlp tasks.unfortunately, there are often no suitably annotated corpora forgiven task.
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
for example, the penn treebank (marcus et al, 1993; <papid> J93-2004 </papid>marcus etal., 1994; <papid> H94-1020 </papid>bies et al, 1994) provides large corpus of syntactically annotated examples mostly from the wall street journal.</citsent>
<aftsection>
<nextsent>it is an excellent resource for tasks dealing with the syntax of written english.
</nextsent>
<nextsent>however, if the annotation formalism (a phrase-structure grammar with some simple features) does not match that of ones nlp system, it is of very little use.
</nextsent>
<nextsent>for example, suppose parser using categorial grammar (wood, 1993; steedman, 1993) is developed and applied to the examples in the corpus.
</nextsent>
<nextsent>while the bracketing of the examples will bear strong relationship to the bracketing of the treebank, the labelling of the lexical items and the inner nodes of the tree will be entirely different and no labelling evaluation will be possible.however, intuitively, plenty of syntactic information is available.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y797">
<title id=" W01-0904.xml">translating treebank annotation for evaluation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this case the learned artefact is used to annotate the examples, which can thenbe compared against the correctly annotated version.
</prevsent>
<prevsent>hence, correctly annotated corpora are vital for the evaluation of very large number of nlp tasks.unfortunately, there are often no suitably annotated corpora forgiven task.
</prevsent>
</prevsection>
<citsent citstr=" H94-1020 ">
for example, the penn treebank (marcus et al, 1993; <papid> J93-2004 </papid>marcus etal., 1994; <papid> H94-1020 </papid>bies et al, 1994) provides large corpus of syntactically annotated examples mostly from the wall street journal.</citsent>
<aftsection>
<nextsent>it is an excellent resource for tasks dealing with the syntax of written english.
</nextsent>
<nextsent>however, if the annotation formalism (a phrase-structure grammar with some simple features) does not match that of ones nlp system, it is of very little use.
</nextsent>
<nextsent>for example, suppose parser using categorial grammar (wood, 1993; steedman, 1993) is developed and applied to the examples in the corpus.
</nextsent>
<nextsent>while the bracketing of the examples will bear strong relationship to the bracketing of the treebank, the labelling of the lexical items and the inner nodes of the tree will be entirely different and no labelling evaluation will be possible.however, intuitively, plenty of syntactic information is available.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y798">
<title id=" W01-0904.xml">translating treebank annotation for evaluation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>here, we present system that translates the annotation of the penn treebank from the standard phrase structure annotation to categorial grammar (cg) annotation and in the process induces large scale cg lexicons.
</prevsent>
<prevsent>it is data-driven multi-pass system that uses both predefined rules and machine learning techniques to translate the trees and in the process induce large scale cg lexicon.
</prevsent>
</prevsection>
<citsent citstr=" W01-0720 ">
the system was designed to produce the lexical annotations for the sentences without null elements (i.e. without movement) from the penn treebank, so that these could be used to evaluate the results produced by an unsupervised cg lexicon learner (watkinson and manandhar, 2000; watkinson and manandhar, 2001).<papid> W01-0720 </papid></citsent>
<aftsection>
<nextsent>the system has four major features.
</nextsent>
<nextsent>firstly, there is significant control over how the treebank is annotated.
</nextsent>
<nextsent>this is vital if the results are to beused for evaluation.
</nextsent>
<nextsent>secondly, the system prevents propagation of translation errors throughout the trees by being data-driven.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y799">
<title id=" W01-0904.xml">translating treebank annotation for evaluation </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>each approach involves process that splits up the annotated trees in the treebank into set of subtrees that define the grammar.
</prevsent>
<prevsent>these approaches still continue to work with the syntactic data in the same form as it is found in the corpora.
</prevsent>
</prevsection>
<citsent citstr=" P98-1115 ">
a slightly different approach has been followed by krotov et al(krotov et al, 1998), <papid> P98-1115 </papid>where they extract the grammar from the penn treebank like charniak, but then compact it.</citsent>
<aftsection>
<nextsent>this provides smaller grammar of similar quality to grammar that has not been compacted, when linguistically motivated compaction is used.
</nextsent>
<nextsent>however, the formalism remains unchanged.
</nextsent>
<nextsent>similarly, johnson (johnson, 1998) <papid> J98-4004 </papid>modifies the labelling of thepenn treebank, but remains within cfg frame work.</nextsent>
<nextsent>hockenmaier et al(hockenmaier et al, 2000), although to some extent following the approach of xia (xia, 1999) where ltags are extracted,have pursued an alternative by extracting com bina tory categorial grammar (ccg) (steedman,1993; wood, 1993) lexicons from the penn tree bank.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y800">
<title id=" W01-0904.xml">translating treebank annotation for evaluation </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>this provides smaller grammar of similar quality to grammar that has not been compacted, when linguistically motivated compaction is used.
</prevsent>
<prevsent>however, the formalism remains unchanged.
</prevsent>
</prevsection>
<citsent citstr=" J98-4004 ">
similarly, johnson (johnson, 1998) <papid> J98-4004 </papid>modifies the labelling of thepenn treebank, but remains within cfg frame work.</citsent>
<aftsection>
<nextsent>hockenmaier et al(hockenmaier et al, 2000), although to some extent following the approach of xia (xia, 1999) where ltags are extracted,have pursued an alternative by extracting com bina tory categorial grammar (ccg) (steedman,1993; wood, 1993) lexicons from the penn tree bank.
</nextsent>
<nextsent>in this case the data in the treebank is truly translated into another formalism providing an entire ccg annotation for the corpus based on top-down algorithm.
</nextsent>
<nextsent>the lexicon is built by reading off the lexical assignments made for each tree.
</nextsent>
<nextsent>this is the most closely related work to this research, especially as it translates into formalism very closely related to cg.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y810">
<title id=" W02-0208.xml">dialogue view  an annotation tool for dialogue </title>
<section> word view.  </section>
<citcontext>
<prevsection>
<prevsent>3.4 speech repairs.
</prevsent>
<prevsent>word view also allows users to annotate speech repairs.
</prevsent>
</prevsection>
<citsent citstr=" J99-4003 ">
a speech repair is where user goes back and repeats or changes something that was just said (heeman and allen, 1999).<papid> J99-4003 </papid></citsent>
<aftsection>
<nextsent>below we give an example of speech repair and show its principle components: reparandum, interruption point, and editing term.
</nextsent>
<nextsent>example 1 why don we take |{z} reparandum   ip um |{z} et take two boxcars the reparandum is the speech that is being replaced, the interruption point is the end of the reparandum, and the editing term consists of words such as \um , \uh , \okay , \let see  that help signal the repair.to annotate repair, the user highlights sequence of words and then tags it as reparandum or an editing term of repair.
</nextsent>
<nextsent>the user can also specify the type of repair.
</nextsent>
<nextsent>figure 2 shows how speech repairs are displayed in wordview.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y812">
<title id=" W02-0208.xml">dialogue view  an annotation tool for dialogue </title>
<section> utterance view.  </section>
<citcontext>
<prevsection>
<prevsent>4.5 annotating blocks of utterances.
</prevsent>
<prevsent>in the utterance view, the user can also annotate hierarchical groupings of utterances.
</prevsent>
</prevsection>
<citsent citstr=" J86-3001 ">
3 we use the utterance blocks to annotate discourse structure (grosz and sidner, 1986).<papid> J86-3001 </papid></citsent>
<aftsection>
<nextsent>this is similar to what flammia tool allows (flammia, 1995).
</nextsent>
<nextsent>rather than showing it with indentation and color, we draw boxes around segments.
</nextsent>
<nextsent>figure 3 shows adia logue excerpt with three utterance blocks in side of larger block.
</nextsent>
<nextsent>to create segment, the user highlights sequence of utterances and then presses the \make segment  button.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y813">
<title id=" W02-0208.xml">dialogue view  an annotation tool for dialogue </title>
<section> utterance view.  </section>
<citcontext>
<prevsection>
<prevsent>we are experimenting with special type of dialogue block.
</prevsent>
<prevsent>consider the example from the previous paragraph, in which the conversant stake seven utterances to jointly make suggestion.
</prevsent>
</prevsection>
<citsent citstr=" W99-0313 ">
this is related to the shared turns of schirin (1987), the co-operative completions of linell (1998), and the grounding units of traum and nakatani (1999).<papid> W99-0313 </papid></citsent>
<aftsection>
<nextsent>we are experimenting withhow to support the annotation of such phenomena.
</nextsent>
<nextsent>we have added tag to indicate whether the utterances in the block are being used to build single contribution.
</nextsent>
<nextsent>for these single contributions, we also supply concise paraphrase of what was said.
</nextsent>
<nextsent>we have found that this paraphrase can be built from sequential subset of the words in the utterances of the block.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y814">
<title id=" W01-1510.xml">resource sharing amongst hpsg and ltag communities by a method of grammar conversion between fbltag and hpsg </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our concern is, however, not limited to the sharing of grammars and lexicons.
</prevsent>
<prevsent>strongly equivalent grammars enable the sharing of ideas developed in each formalism.
</prevsent>
</prevsection>
<citsent citstr=" P00-1058 ">
there have been many studies on parsing techniques (poller and becker, 1998; flickinger etal., 2000), ones on disambiguation models (chi ang, 2000; <papid> P00-1058 </papid>kanayama et al, 2000), <papid> C00-1060 </papid>and oneson programming/grammar-development environ1in this paper, we use the term ltag to refer to fb ltag, if not confusing.</citsent>
<aftsection>
<nextsent>ltag resources grammar: elementary tree templates lexicon type hierarchy extractor tree converter lexicon converter rental system hpsg resources grammar: lexical entry templates lexicon ltag parsers hpsg parsers derivation trees parse trees derivation translator ltag-based application hpsg-based application figure 1: the rental system: overview ment (sarkar and wintner, 1999; doran et al,2000; makino et al, 1998)<papid> P98-2132 </papid></nextsent>
<nextsent>these works are restricted to each closed community, and the relation between them is not well discussed.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y815">
<title id=" W01-1510.xml">resource sharing amongst hpsg and ltag communities by a method of grammar conversion between fbltag and hpsg </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our concern is, however, not limited to the sharing of grammars and lexicons.
</prevsent>
<prevsent>strongly equivalent grammars enable the sharing of ideas developed in each formalism.
</prevsent>
</prevsection>
<citsent citstr=" C00-1060 ">
there have been many studies on parsing techniques (poller and becker, 1998; flickinger etal., 2000), ones on disambiguation models (chi ang, 2000; <papid> P00-1058 </papid>kanayama et al, 2000), <papid> C00-1060 </papid>and oneson programming/grammar-development environ1in this paper, we use the term ltag to refer to fb ltag, if not confusing.</citsent>
<aftsection>
<nextsent>ltag resources grammar: elementary tree templates lexicon type hierarchy extractor tree converter lexicon converter rental system hpsg resources grammar: lexical entry templates lexicon ltag parsers hpsg parsers derivation trees parse trees derivation translator ltag-based application hpsg-based application figure 1: the rental system: overview ment (sarkar and wintner, 1999; doran et al,2000; makino et al, 1998)<papid> P98-2132 </papid></nextsent>
<nextsent>these works are restricted to each closed community, and the relation between them is not well discussed.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y816">
<title id=" W01-1510.xml">resource sharing amongst hpsg and ltag communities by a method of grammar conversion between fbltag and hpsg </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>strongly equivalent grammars enable the sharing of ideas developed in each formalism.
</prevsent>
<prevsent>there have been many studies on parsing techniques (poller and becker, 1998; flickinger etal., 2000), ones on disambiguation models (chi ang, 2000; <papid> P00-1058 </papid>kanayama et al, 2000), <papid> C00-1060 </papid>and oneson programming/grammar-development environ1in this paper, we use the term ltag to refer to fb ltag, if not confusing.</prevsent>
</prevsection>
<citsent citstr=" P98-2132 ">
ltag resources grammar: elementary tree templates lexicon type hierarchy extractor tree converter lexicon converter rental system hpsg resources grammar: lexical entry templates lexicon ltag parsers hpsg parsers derivation trees parse trees derivation translator ltag-based application hpsg-based application figure 1: the rental system: overview ment (sarkar and wintner, 1999; doran et al,2000; makino et al, 1998)<papid> P98-2132 </papid></citsent>
<aftsection>
<nextsent>these works are restricted to each closed community, and the relation between them is not well discussed.
</nextsent>
<nextsent>investigating the relation will be apparently valuable for both communities.in this paper, we show that the strongly equivalent grammars enable the sharing of parsingtechniques?, which are dependent on each computational framework and have never been shared among hpsg and ltag communities.
</nextsent>
<nextsent>we apply our system to the latest version of the xtag english grammar (the xtag research group,2001), which is large-scale fb-ltag grammar. parsing experiment shows that an efficient hpsg parser with the obtained grammar achieved significant speed-up against an existing ltagparser (yoshinaga et al, 2001).
</nextsent>
<nextsent>this result implies that parsing techniques for hpsg are also beneficial for ltag parsing.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y818">
<title id=" W01-1510.xml">resource sharing amongst hpsg and ltag communities by a method of grammar conversion between fbltag and hpsg </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>thus the translation was manual and grammar dependent.
</prevsent>
<prevsent>the manual translation demanded considerable efforts from the translator, and obscures the equivalence between the original and obtained grammars.
</prevsent>
</prevsection>
<citsent citstr=" P95-1013 ">
other works (kasper et al, 1995; <papid> P95-1013 </papid>becker and lopez, 2000) convert hpsg grammars intoltag grammars.</citsent>
<aftsection>
<nextsent>however, given the greater expressive power of hpsg, it is impossible to convert an arbitrary hpsg grammar into an ltag grammar.
</nextsent>
<nextsent>therefore, conversion from hpsg into ltag often requires some restrictions on thehpsg grammar to suppress its generative capacity.
</nextsent>
<nextsent>thus, the conversion loses the equivalence ofthe grammars, and we cannot gain the above ad vantages.section 2 reviews the source and the target grammar formalisms of the conversion algorithm.
</nextsent>
<nextsent>section 3 describes the conversion algorithm which the core module in the rental system uses.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y819">
<title id=" W01-1510.xml">resource sharing amongst hpsg and ltag communities by a method of grammar conversion between fbltag and hpsg </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>section 5 concludes this study and addresses future works.
</prevsent>
<prevsent>2.1 feature-based lexicalized tree.
</prevsent>
</prevsection>
<citsent citstr=" C88-2121 ">
adjoining grammar (fb-ltag)ltag (schabes et al, 1988) <papid> C88-2121 </papid>is grammar formalism that provides syntactic analyses for sentence by composing elementary trees with two opera arg : we can run id grammar rule unify sym : np arg : sym : vp arg : vp sym : vp arg : np arg : sym : arg : 2 3 2 unify 3 unify id grammar rule we can run sym : np arg : sym : vp arg : vp sym : vp arg : np arg : np sym : arg : arg : 1 1 | 2 arg : 2 unify we can run sym : np arg : sym : vp arg : vp sym : vp arg : np arg : np arg : figure 6: parsing with an hpsg grammar np vp run np we substitution 1 2 np vp run we figure 3: substitution vp vp can * adjunction 1 np vp run we np vp vp can we run figure 4: adjunction tions called substitution and adjunction.</citsent>
<aftsection>
<nextsent>elementary trees are classified into two types, initial trees and auxiliary trees (figure 2).
</nextsent>
<nextsent>an elementary tree has at least one leaf node labeled with terminal symbol called an anchor (marked with ).
</nextsent>
<nextsent>in an auxiliary tree, one leaf node is labeled with the same symbol as the root node and is specially marked as foot node (marked with ).
</nextsent>
<nextsent>in an elementary tree, leaf nodes with the exception of anchors and the foot node are called substitution nodes (marked with #).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y820">
<title id=" W01-1510.xml">resource sharing amongst hpsg and ltag communities by a method of grammar conversion between fbltag and hpsg </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>stanford university is developing the english resource grammar, an hpsg grammar for english, as part of the linguistic grammars online (lingo)project (flickinger, 2000).
</prevsent>
<prevsent>in practical context, german, english, and japanese hpsg-basedgrammars are developed and used in the verb mobil project (kay et al, 1994).
</prevsent>
</prevsection>
<citsent citstr=" P98-2144 ">
our group has developed wide-coverage hpsg grammar for japanese (mitsuishi et al, 1998), <papid> P98-2144 </papid>which isused in high-accuracy japanese dependency analyzer (kanayama et al, 2000).<papid> C00-1060 </papid></citsent>
<aftsection>
<nextsent>the grammar conversion from ltag to hpsg (yoshinaga and miyao, 2001) is the core portion of the rental system.
</nextsent>
<nextsent>the conversion algorithm consists of: 1.
</nextsent>
<nextsent>conversion of canonical elementary trees to.
</nextsent>
<nextsent>hpsg lexical entries.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y825">
<title id=" W02-0304.xml">accenting unknown words in a specialized language </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>we took obvious measures to reduce the number of unknown words: we filtered out the words that can be found in accented lexicons and corpora.
</prevsent>
<prevsent>but this technique is limited by the size of the corpus that would be necessary for such rare?
</prevsent>
</prevsection>
<citsent citstr=" J95-4004 ">
words to occur, and by the lack of availability of specialized french lexicons for the medical domain.we then designed two methods that can learn accenting rules for the remaining unknown words: (i)adapting pos-tagging method (brill, 1995) (<papid> J95-4004 </papid>section 3.3); (ii) adapting method designed for learning morphological rules (theron and cloete, 1997) (<papid> A97-1016 </papid>section 3.4).</citsent>
<aftsection>
<nextsent>3.1 filtering out know words.
</nextsent>
<nextsent>the french mesh was briefly presented in the in troduction; we work with the 2001 version.
</nextsent>
<nextsent>the part which was accented and converted into mixed case by the cismef team is that of november 2001.
</nextsent>
<nextsent>as more resources are added to cismef on regular basis, larger number of these accented terms mustnow be available.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y826">
<title id=" W02-0304.xml">accenting unknown words in a specialized language </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>we took obvious measures to reduce the number of unknown words: we filtered out the words that can be found in accented lexicons and corpora.
</prevsent>
<prevsent>but this technique is limited by the size of the corpus that would be necessary for such rare?
</prevsent>
</prevsection>
<citsent citstr=" A97-1016 ">
words to occur, and by the lack of availability of specialized french lexicons for the medical domain.we then designed two methods that can learn accenting rules for the remaining unknown words: (i)adapting pos-tagging method (brill, 1995) (<papid> J95-4004 </papid>section 3.3); (ii) adapting method designed for learning morphological rules (theron and cloete, 1997) (<papid> A97-1016 </papid>section 3.4).</citsent>
<aftsection>
<nextsent>3.1 filtering out know words.
</nextsent>
<nextsent>the french mesh was briefly presented in the in troduction; we work with the 2001 version.
</nextsent>
<nextsent>the part which was accented and converted into mixed case by the cismef team is that of november 2001.
</nextsent>
<nextsent>as more resources are added to cismef on regular basis, larger number of these accented terms mustnow be available.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y828">
<title id=" W00-1202.xml">sense tagging chinese corpus </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>many pos-tagged corpora are available.
</prevsent>
<prevsent>the accuracy for pos-tagging is in the range of 95% to 97% 1 . in contrast, although the researches on word sense disambiguation (wsd) are also very early (kelly and stone, 1975), large-scale sense-tagged corpus is relatively few.
</prevsent>
</prevsection>
<citsent citstr=" P96-1006 ">
in english, only some sense-tagged corpora such as hector (atkins, 1993), dso (ng and lee, 1996), <papid> P96-1006 </papid>semcor (fellbaum, 1997), and senseval (kilgarriff, 1998) are available.</citsent>
<aftsection>
<nextsent>for evaluating word sense disarnbiguation systems, the first senseval (kilgarriff and rosenzweig, 2000) reports that the performance for fine-grained word sense disambiguation task is at around 75 %.
</nextsent>
<nextsent>1 the pelrforlnancg includes tagging wnzmbiguous.
</nextsent>
<nextsent>words.
</nextsent>
<nextsent>marslmll (1987) reported that the performance of claws tagger is 94%.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y829">
<title id=" W01-1606.xml">an empirical study of speech recognition errors in a task oriented dialogue system </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the system interface.
</prevsent>
<prevsent>related applications, i.e. dialogue systems involving the choice of film or tv programme have been also described in [hagen, 2000] [ludwig et al, 2000].
</prevsent>
</prevsection>
<citsent citstr=" P94-1015 ">
our system is mixed initiative conversational interface organised around human-like character with which the user communicates through speech recognition [nagao and takeuchi, 1994] [<papid> P94-1015 </papid>beskow and mcglashan, 1997].</citsent>
<aftsection>
<nextsent>the interface is based on the microsoft agent?
</nextsent>
<nextsent>system with set of animated bitmaps acquired from real human subject (figure 1).
</nextsent>
<nextsent>an example dialogue illustrating the system capabilities is presented below (this example has been obtained with keyboard input only).
</nextsent>
<nextsent>after greetings by the system, the user opens the dialogue with first request (u1).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y830">
<title id=" W01-1606.xml">an empirical study of speech recognition errors in a task oriented dialogue system </title>
<section> from speech recognition errors to speech.  </section>
<citcontext>
<prevsection>
<prevsent>then the impact of errors at critical points of dialogue can be tested on the system using textual input, entering the utterances comprising speech recognition errors.
</prevsent>
<prevsent>this also makes possible to explore the consequences of set of errors at various stages of the dialogue process (see below).
</prevsent>
</prevsection>
<citsent citstr=" P97-1035 ">
in doing so, we are essentially exploring system behaviour in glass box approach: this does not constitute an evaluation method for dialogue performance [walker et al, 1997].<papid> P97-1035 </papid></citsent>
<aftsection>
<nextsent>the only global metric that can be used in our case is dialogue length, as measure of the extra dialogue turns required to repair the consequences of speech recognition error.
</nextsent>
<nextsent>5.1 fatal?
</nextsent>
<nextsent>speech recognition errors obviously, the worst errors are those which terminate the dialogue by selecting programme that the user intended to reject.
</nextsent>
<nextsent>this happens when the semantic structure produced from the mis-recognised utterance can be interpreted as an acceptance speech act.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y831">
<title id=" W01-1607.xml">comparing several aspects of human computer and human human dialogues </title>
<section> our data.  </section>
<citcontext>
<prevsection>
<prevsent>2.2 initiative tagging.
</prevsent>
<prevsent>there is not clearly agreed upon definition of initiative in the literature on dialogue analysis (but see e.g., (chu-carroll and brown, 1998; jordan and di eugenio, 1997; flammia and zue, 1997)),despite the fact the terms initiative and mixed initiative are widely used.
</prevsent>
</prevsection>
<citsent citstr=" P90-1010 ">
intuitively, it seems that control rests with the participant who is moving conversation ahead at given point, or selecting new topics for conversation.after experimenting with several tagging methods, we concluded that the approach presented in walker and whittaker (1990) <papid> P90-1010 </papid>adopted from (whittaker and stenton, 1988) <papid> P88-1015 </papid>best captured the aspects of the dialogue we were interested in and, as with the das, could be tagged reliably on our data.</citsent>
<aftsection>
<nextsent>each turn is tagged with which participant has control at the end of that turn, based on the utterance type.
</nextsent>
<nextsent>again, we did not tag turns composed entirely of non-speech annotation, and we also excluded conventional openings and closings, following walker and whittaker.
</nextsent>
<nextsent>below, we list the rules for tagging each utterance type; prompt is an utterance which did not express propositional content, such as yeah, okay, uh-huh, . . .
</nextsent>
<nextsent>.?(op cit, p. 3) the classification refers to the il locutionary force of the item, rather than to its particular syntactic form.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y832">
<title id=" W01-1607.xml">comparing several aspects of human computer and human human dialogues </title>
<section> our data.  </section>
<citcontext>
<prevsection>
<prevsent>2.2 initiative tagging.
</prevsent>
<prevsent>there is not clearly agreed upon definition of initiative in the literature on dialogue analysis (but see e.g., (chu-carroll and brown, 1998; jordan and di eugenio, 1997; flammia and zue, 1997)),despite the fact the terms initiative and mixed initiative are widely used.
</prevsent>
</prevsection>
<citsent citstr=" P88-1015 ">
intuitively, it seems that control rests with the participant who is moving conversation ahead at given point, or selecting new topics for conversation.after experimenting with several tagging methods, we concluded that the approach presented in walker and whittaker (1990) <papid> P90-1010 </papid>adopted from (whittaker and stenton, 1988) <papid> P88-1015 </papid>best captured the aspects of the dialogue we were interested in and, as with the das, could be tagged reliably on our data.</citsent>
<aftsection>
<nextsent>each turn is tagged with which participant has control at the end of that turn, based on the utterance type.
</nextsent>
<nextsent>again, we did not tag turns composed entirely of non-speech annotation, and we also excluded conventional openings and closings, following walker and whittaker.
</nextsent>
<nextsent>below, we list the rules for tagging each utterance type; prompt is an utterance which did not express propositional content, such as yeah, okay, uh-huh, . . .
</nextsent>
<nextsent>.?(op cit, p. 3) the classification refers to the il locutionary force of the item, rather than to its particular syntactic form.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y833">
<title id=" W01-1607.xml">comparing several aspects of human computer and human human dialogues </title>
<section> analysis.  </section>
<citcontext>
<prevsection>
<prevsent>we graphed both the average total number of system turns aswell as the average number of turns minus repetitions.
</prevsent>
<prevsent>hc responds almost immediately to 10this issue may also be related to where in the dialogue errors occur.
</prevsent>
</prevsection>
<citsent citstr=" H01-1028 ">
we are pursuing another line of research which looks at automatic error detection, described in (aberdeen et al, 2001).<papid> H01-1028 </papid></citsent>
<aftsection>
<nextsent>we believe wemay also be able to detect unsolicited information automatically, as well as to see whether it is likely to trigger errors by the system.
</nextsent>
<nextsent>figure 4: unsolicited fields vs. success rate of incorporation unsolicited information while hcs and take more turns to respond.
</nextsent>
<nextsent>hc has trouble understanding the unsolicited information, and either keeps asking for clarification or continues to ignore the human and prompts for some other piece of information multiple times.figure 5: variation of system response to unsolicited information figure 6 shows the different rates at which systems acknowledge unsolicited information for different fields.
</nextsent>
<nextsent>for example, departure city is recognized and validated almost immediately.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y835">
<title id=" W01-0812.xml">reusing a statistical language model for generation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we propose method where sentence realization is carried out using simplified (context free) version of large analysis grammar, combined with statistical language model from the full (context sensitive) version of the same grammar.
</prevsent>
<prevsent>the statistical model provides measure of the probability of syntactic substructures, derived from the analysis of corpus with the full grammar, and is used to guide both subsequent analysis and generation.
</prevsent>
</prevsection>
<citsent citstr=" P95-1034 ">
to date, only limited use of statistically-derived resources has been made for realization in natural language generation, notably knight &amp; hatzivassiloglou (1995), <papid> P95-1034 </papid>langkilde &amp; knight (1998) <papid> P98-1116 </papid>and bangalore &amp; rambow (2000).<papid> C00-1007 </papid></citsent>
<aftsection>
<nextsent>this paper reports on new work in that direction, but with an emphasis on reusing resources originally produced for analysis purposes.
</nextsent>
<nextsent>in particular, generation grammar is derived from an extensive analysis grammar in such way as to retain the statistical language model built using the analysis grammar.
</nextsent>
<nextsent>work to date on using statistical knowledge for generation has mainly focused on the sub-task of surface (in fact, sentence) realization: the production of grammatically correct string from an abstract semantic/logical representation of linguistic content.
</nextsent>
<nextsent>this assumes the existence of separate higher-level process to produce such representation, following the canonical pipeline architecture of full generation system (reiter, 1994).<papid> W94-0319 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y836">
<title id=" W01-0812.xml">reusing a statistical language model for generation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we propose method where sentence realization is carried out using simplified (context free) version of large analysis grammar, combined with statistical language model from the full (context sensitive) version of the same grammar.
</prevsent>
<prevsent>the statistical model provides measure of the probability of syntactic substructures, derived from the analysis of corpus with the full grammar, and is used to guide both subsequent analysis and generation.
</prevsent>
</prevsection>
<citsent citstr=" P98-1116 ">
to date, only limited use of statistically-derived resources has been made for realization in natural language generation, notably knight &amp; hatzivassiloglou (1995), <papid> P95-1034 </papid>langkilde &amp; knight (1998) <papid> P98-1116 </papid>and bangalore &amp; rambow (2000).<papid> C00-1007 </papid></citsent>
<aftsection>
<nextsent>this paper reports on new work in that direction, but with an emphasis on reusing resources originally produced for analysis purposes.
</nextsent>
<nextsent>in particular, generation grammar is derived from an extensive analysis grammar in such way as to retain the statistical language model built using the analysis grammar.
</nextsent>
<nextsent>work to date on using statistical knowledge for generation has mainly focused on the sub-task of surface (in fact, sentence) realization: the production of grammatically correct string from an abstract semantic/logical representation of linguistic content.
</nextsent>
<nextsent>this assumes the existence of separate higher-level process to produce such representation, following the canonical pipeline architecture of full generation system (reiter, 1994).<papid> W94-0319 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y837">
<title id=" W01-0812.xml">reusing a statistical language model for generation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we propose method where sentence realization is carried out using simplified (context free) version of large analysis grammar, combined with statistical language model from the full (context sensitive) version of the same grammar.
</prevsent>
<prevsent>the statistical model provides measure of the probability of syntactic substructures, derived from the analysis of corpus with the full grammar, and is used to guide both subsequent analysis and generation.
</prevsent>
</prevsection>
<citsent citstr=" C00-1007 ">
to date, only limited use of statistically-derived resources has been made for realization in natural language generation, notably knight &amp; hatzivassiloglou (1995), <papid> P95-1034 </papid>langkilde &amp; knight (1998) <papid> P98-1116 </papid>and bangalore &amp; rambow (2000).<papid> C00-1007 </papid></citsent>
<aftsection>
<nextsent>this paper reports on new work in that direction, but with an emphasis on reusing resources originally produced for analysis purposes.
</nextsent>
<nextsent>in particular, generation grammar is derived from an extensive analysis grammar in such way as to retain the statistical language model built using the analysis grammar.
</nextsent>
<nextsent>work to date on using statistical knowledge for generation has mainly focused on the sub-task of surface (in fact, sentence) realization: the production of grammatically correct string from an abstract semantic/logical representation of linguistic content.
</nextsent>
<nextsent>this assumes the existence of separate higher-level process to produce such representation, following the canonical pipeline architecture of full generation system (reiter, 1994).<papid> W94-0319 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y838">
<title id=" W01-0812.xml">reusing a statistical language model for generation </title>
<section> statistically-driven generation.  </section>
<citcontext>
<prevsection>
<prevsent>in particular, generation grammar is derived from an extensive analysis grammar in such way as to retain the statistical language model built using the analysis grammar.
</prevsent>
<prevsent>work to date on using statistical knowledge for generation has mainly focused on the sub-task of surface (in fact, sentence) realization: the production of grammatically correct string from an abstract semantic/logical representation of linguistic content.
</prevsent>
</prevsection>
<citsent citstr=" W94-0319 ">
this assumes the existence of separate higher-level process to produce such representation, following the canonical pipeline architecture of full generation system (reiter, 1994).<papid> W94-0319 </papid></citsent>
<aftsection>
<nextsent>the approach described here has the same focus, but attempts to more tightly integrate the statistical knowledge in the generation process, and also to avoid the need to create generation-specific resources.
</nextsent>
<nextsent>2.1 nitrogen.
</nextsent>
<nextsent>the nitrogen system (knight &amp; hatzivassiloglou, 1995; <papid> P95-1034 </papid>langkilde &amp; knight, 1998) <papid> P98-1116 </papid>made the first significant attempt to integrate statistical knowledge for surface realization.</nextsent>
<nextsent>it uses an extremely simple generation-specific grammar and generates lattice representing all possible strings that the grammar allows for particular semantic input.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y844">
<title id=" W01-0812.xml">reusing a statistical language model for generation </title>
<section> a reusable language model  </section>
<citcontext>
<prevsection>
<prevsent>the analysis system is built around broad coverage, manually-constructed grammar (a descendant of that described in jensen et al, 1993).
</prevsent>
<prevsent>the grammar can be viewed as context-free backbone of binary phrase-structure rules, together with an extensive set of detailed, potentially context-sensitive, conditions on each rule, referring to lexical, morphological, syntactic and semantic features.
</prevsent>
</prevsection>
<citsent citstr=" P97-1003 ">
a statistical language model ? lexicalized pcfg (similar to that of collins, 1997) ? <papid> P97-1003 </papid>is derived from the analysis grammar by processing corpus using the same grammar with no statistical model and recording frequencies of substructures built by each rule.</citsent>
<aftsection>
<nextsent>the sensitivity of the model can be tuned to include any of the features referred to by rule conditions, including neighboring or descendant nodes.
</nextsent>
<nextsent>the training phase for the model requires no manual annotation of the corpus, although some manual filtering was done to attempt to exclude any particularly bad parses.
</nextsent>
<nextsent>for the approximately 200 rule grammar, corpus of 25,000 sentences was used for training, selected from variety of sources and genres.
</nextsent>
<nextsent>the model is then used in subsequent analysis with the same grammar to guide bottom-up rule applications to build the most probable substructures first, acting to direct the search through the structures licensed by the grammar.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y845">
<title id=" W01-0812.xml">reusing a statistical language model for generation </title>
<section> the generation system.  </section>
<citcontext>
<prevsection>
<prevsent>if substructure generated at lower level has higher probability than the substructure generated at the current level, discard the substructure at the current level.
</prevsent>
<prevsent>b. add the substructure generated at the current level with the highest probability to the current syntactic tree.
</prevsent>
</prevsection>
<citsent citstr=" J90-1004 ">
if no substructures exist at the current level (no applicable rules or all discarded), step down one level (apply null rule) and repeat from 2.a. the algorithm in fact follows head-driven node expansion, or search through the grammar, (as in shieber et al, 1990), <papid> J90-1004 </papid>with the head of the most recently expanded node being selected for the next expansion (in step 2 of the algorithm above), until leaf node is produced.</citsent>
<aftsection>
<nextsent>however, the nature of the grammar is such that no rule expansion will have side effects on any node other than the head of its substructure, and so any search strategy will produce the same final tree, though more alternative paths may be considered.
</nextsent>
<nextsent>the look-ahead?
</nextsent>
<nextsent>in the search (step 2.a.iii), to find other rules expressing the same features as current rule, means that, although the rule probabilities obtained from the language model are based entirely on local rule substructures, the overall path chosen through the grammar is globally optimal.
</nextsent>
<nextsent>the current implementation of the look-ahead is not optimal, however, with duplicate substructures being created and evaluated for the same rules along equivalent paths, and an obvious extension would be the addition of simple caching mechanism for substructures and their probabilities.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y851">
<title id=" W01-1011.xml">gistit combining linguistic and machine learning techniques for email summarization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>email messages and web documents raise several challenges to automatic text processing, and the summarization task addresses most of them: they are free-style text, not always syntactically or grammatically well-formed, domain and genre independent, of variable length and on multiple topics.
</prevsent>
<prevsent>furthermore, due to the lack of well-formed syntactic and grammatical structures, the granularity of document extracts presents another level of complexity.
</prevsent>
</prevsection>
<citsent citstr=" C00-2127 ">
in our work, we address the extraction problem at phrase-level [ueda et al(2000), <papid> C00-2127 </papid>wacholder et al(2000)], identifying salient information that is spread across multiple sentences and paragraphs.</citsent>
<aftsection>
<nextsent>our novel approach first extracts simple noun phrases as candidate units for representing document meaning and then uses machine learning algorithms to select the most prominent ones.
</nextsent>
<nextsent>this combined method allows us to generate an informative, generic, at-a-glance?
</nextsent>
<nextsent>summary.
</nextsent>
<nextsent>in this paper, we show: (a) the efficiency of the linguistic approach for phrase extraction in comparing results with and without filtering techniques, (b) the usefulness of vector representation in determining proper features to identify content ful information, (c) the benefit of using new measure of tf*idf for the noun phrase and its constituents, (d) the power of machine learning systems in evaluating several classifiers in order to select the one performing the best for this task.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y852">
<title id=" W01-1011.xml">gistit combining linguistic and machine learning techniques for email summarization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these approaches select set of candidate phrases (sequence of one, two or three consecutive stemmed, non-stop words) and then apply machine learning techniques to classify them as key phrases or not.
</prevsent>
<prevsent>but dealing only with n-grams does not always provide good output in terms of summary (see discussion in section 5.4).
</prevsent>
</prevsection>
<citsent citstr=" W98-0610 ">
wacholder (1998) <papid> W98-0610 </papid>proposes linguistically motivated method for the representation of the document aboutness: head clustering?.</citsent>
<aftsection>
<nextsent>a list of simple noun phrases is first extracted, clustered by head and then ranked by the frequency of the head.
</nextsent>
<nextsent>klavans et al(2000) report on the evaluation of usefulness?
</nextsent>
<nextsent>of head clustering in the context of browsing applications, in terms of quality and coverage.
</nextsent>
<nextsent>other researchers have used noun-phrases quite successfully for information retrieval task [strzalkowski et al(1999), sparck-jones (1999)].
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y854">
<title id=" W01-1011.xml">gistit combining linguistic and machine learning techniques for email summarization </title>
<section> system architecture.  </section>
<citcontext>
<prevsection>
<prevsent>filtering unit this module performs shallow text processing for extraction and filtering of simple np candidates, consisting of pipeline of three modules: text tokenization, np extraction, and np filtering.
</prevsent>
<prevsent>since the tool was created to pre process email for speech output, some of the text tokenization suitable for speech is not accurate for text processing and some modifications needed to be implemented (e.g. email pre processor splits acronyms like dli2 into dli 2).
</prevsent>
</prevsection>
<citsent citstr=" A92-1021 ">
the noun phrase extraction module uses brill pos tagger [brill (1992)]<papid> A92-1021 </papid>and base np chunker [ramshaw and marcus (1995)].<papid> W95-0107 </papid></citsent>
<aftsection>
<nextsent>after analyzing some of these errors, we augmented the tagger lexicon from our training data and we added lexical and contextual rules to deal mainly with incorrect tagging of gerund endings.
</nextsent>
<nextsent>in order to improve the accuracy of classifiers we perform linguistic filtering, as discussed in detail in section 3.1.2.
</nextsent>
<nextsent>2.3 machine learning unit.
</nextsent>
<nextsent>the first component of the ml unit is the feature selection module to compute np vectors.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y855">
<title id=" W01-1011.xml">gistit combining linguistic and machine learning techniques for email summarization </title>
<section> system architecture.  </section>
<citcontext>
<prevsection>
<prevsent>filtering unit this module performs shallow text processing for extraction and filtering of simple np candidates, consisting of pipeline of three modules: text tokenization, np extraction, and np filtering.
</prevsent>
<prevsent>since the tool was created to pre process email for speech output, some of the text tokenization suitable for speech is not accurate for text processing and some modifications needed to be implemented (e.g. email pre processor splits acronyms like dli2 into dli 2).
</prevsent>
</prevsection>
<citsent citstr=" W95-0107 ">
the noun phrase extraction module uses brill pos tagger [brill (1992)]<papid> A92-1021 </papid>and base np chunker [ramshaw and marcus (1995)].<papid> W95-0107 </papid></citsent>
<aftsection>
<nextsent>after analyzing some of these errors, we augmented the tagger lexicon from our training data and we added lexical and contextual rules to deal mainly with incorrect tagging of gerund endings.
</nextsent>
<nextsent>in order to improve the accuracy of classifiers we perform linguistic filtering, as discussed in detail in section 3.1.2.
</nextsent>
<nextsent>2.3 machine learning unit.
</nextsent>
<nextsent>the first component of the ml unit is the feature selection module to compute np vectors.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y857">
<title id=" W01-1011.xml">gistit combining linguistic and machine learning techniques for email summarization </title>
<section> combining linguistic knowledge and.  </section>
<citcontext>
<prevsection>
<prevsent>the focus of this paper is on extracting content with gist it, although presentation is topic for future research.
</prevsent>
<prevsent>machine learning for email gisting we combine symbolic machine learning and linguistic processing in order to extract the salient phrases of document.
</prevsent>
</prevsection>
<citsent citstr=" P98-1112 ">
out of the large syntactic constituents of sentence, e.g. noun phrases, verb phrases, and prepositional phrases, we assume that noun phrases (nps) carry the most content ful information about the document, even if sometimes the verbs are important too, as reported in the work by [klavans and kan (1998)].<papid> P98-1112 </papid></citsent>
<aftsection>
<nextsent>the problem is that no matter the size of document, the number of informative noun phrases is very small comparing with the number of all noun phrases, making selection necessity.
</nextsent>
<nextsent>indeed, in the context of gisting, generating and presenting the list of all noun phrases, even with adequate linguistic filtering, may be overwhelming.
</nextsent>
<nextsent>thus, we define the extraction of important noun phrases as classification task, applying machine learning techniques to determine which features associated with the candidate nps classify them as salient vs. non-salient.
</nextsent>
<nextsent>we represent the document -- in this case an email message -- as set of candidate nps, each of them associated with feature vector used in the classification model.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y862">
<title id=" W02-0214.xml">topic identification in natural language dialogues using neural networks </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>moreover, the utilization of information regarding the position of the word in the utterance is found to improve the results.
</prevsent>
<prevsent>the analysis of the topic of sentence or adocument is an important task for many natural language applications.
</prevsent>
</prevsection>
<citsent citstr=" W02-0210 ">
for example, in interactive dialogue systems that attempt tocarry out and answer requests made by customers, the response strategy employed may depend on the topic of the request (jokinen etal., 2002).<papid> W02-0210 </papid></citsent>
<aftsection>
<nextsent>in large vocabulary speech recognition knowledge of the topic can, in general, be utilized for adjusting the language model used (see, e.g., (iyer and ostendorf, 1999)).
</nextsent>
<nextsent>we describe two approaches to analyzing the topical information, namely the use of topically ordered document maps for analyzing the overall topic of dialogue segments, and identification of topic and focus words in an utterance for sentence-level analysis and identification of topically relevant specific information in short contexts.
</nextsent>
<nextsent>1.1 document map as topically.
</nextsent>
<nextsent>ordered semantic space the self-organizing map (kohonen, 1982; kohonen, 1995) is an unsupervised neural network method suitable for ordering and visualization of complex datasets.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y864">
<title id=" W02-0214.xml">topic identification in natural language dialogues using neural networks </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the sentence, or its semantic or syntactic-semantic representation, is divided into two segments, usually at the location of the main verb, and the words or semantical concepts in the first segment are regarded as topicwords/concepts and those in the second as fo cus?
</prevsent>
<prevsent>words/concepts.
</prevsent>
</prevsection>
<citsent citstr=" W96-0204 ">
for example in (meteer and iyer, 1996), <papid> W96-0204 </papid>the division point is placed before the first strong verb, or, in the absence of such verb, behind the last weak verb ofthe sentence.</citsent>
<aftsection>
<nextsent>similar division is also the starting point for the algorithm for topic focus identification introduced in (hajicova?
</nextsent>
<nextsent>et al, 1995).
</nextsent>
<nextsent>the initial division is then modified according to the verbs position and meaning, the subjects definite ness or indefinite ness andthe number, type and order of the other sentence constituents.in language modeling for speech recognition improvements in perplexity and word error rate have been observed on english corpora when using language models trained separately for the topic and the focus part of the sentence (meteer and iyer, 1996; <papid> W96-0204 </papid>ma et al, 1998).</nextsent>
<nextsent>identification of these concepts is likely to be important also for sentence comprehension and dialogue strategy selection.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y867">
<title id=" W01-1610.xml">labeling corrections and aware sites in spoken dialogue systems </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in addition, users  corrections may miss their goal, because corrections themselves are moredicult for the system to recognize and interpret correctly, which may lead to so-called cyclic (or spiral) errors.
</prevsent>
<prevsent>that corrections are dicult for asr systems is generally explained by the fact that they tend to be hyper articulated | higher, louder, longer . . .
</prevsent>
</prevsection>
<citsent citstr=" P98-1122 ">
than other turns (wade et al, 1992; oviatt et al, 1996; levow, 1998; <papid> P98-1122 </papid>bell and gustafson, 1999; shimojima et al, 1999), where asr models are not well adapted to handle this special speaking style.the current paper focuses on user corrections, and looks at places where people rst become aware of system problem (\aware sites ).</citsent>
<aftsection>
<nextsent>in other papers (swerts et al, 2000; hirschberg et al, 2001; <papid> N01-1027 </papid>litman et al, 2001),<papid> P01-1048 </papid>we have already given some descriptive statistics on corrections and aware sites and wehave been looking at methods to automatically predict these two utterance categories.</nextsent>
<nextsent>one of our major ndings is that prosody, which had already been shown to be good predictor of mis recognitions (litman et al, 2000; <papid> A00-2029 </papid>hirschberg et al, 2000), is also useful to correctly classify corrections and aware sites.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y868">
<title id=" W01-1610.xml">labeling corrections and aware sites in spoken dialogue systems </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>that corrections are dicult for asr systems is generally explained by the fact that they tend to be hyper articulated | higher, louder, longer . . .
</prevsent>
<prevsent>than other turns (wade et al, 1992; oviatt et al, 1996; levow, 1998; <papid> P98-1122 </papid>bell and gustafson, 1999; shimojima et al, 1999), where asr models are not well adapted to handle this special speaking style.the current paper focuses on user corrections, and looks at places where people rst become aware of system problem (\aware sites ).</prevsent>
</prevsection>
<citsent citstr=" N01-1027 ">
in other papers (swerts et al, 2000; hirschberg et al, 2001; <papid> N01-1027 </papid>litman et al, 2001),<papid> P01-1048 </papid>we have already given some descriptive statistics on corrections and aware sites and wehave been looking at methods to automatically predict these two utterance categories.</citsent>
<aftsection>
<nextsent>one of our major ndings is that prosody, which had already been shown to be good predictor of mis recognitions (litman et al, 2000; <papid> A00-2029 </papid>hirschberg et al, 2000), is also useful to correctly classify corrections and aware sites.</nextsent>
<nextsent>in this paper, we will elaborate more on the exact labeling scheme we used, and add further descriptive statistics.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y869">
<title id=" W01-1610.xml">labeling corrections and aware sites in spoken dialogue systems </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>that corrections are dicult for asr systems is generally explained by the fact that they tend to be hyper articulated | higher, louder, longer . . .
</prevsent>
<prevsent>than other turns (wade et al, 1992; oviatt et al, 1996; levow, 1998; <papid> P98-1122 </papid>bell and gustafson, 1999; shimojima et al, 1999), where asr models are not well adapted to handle this special speaking style.the current paper focuses on user corrections, and looks at places where people rst become aware of system problem (\aware sites ).</prevsent>
</prevsection>
<citsent citstr=" P01-1048 ">
in other papers (swerts et al, 2000; hirschberg et al, 2001; <papid> N01-1027 </papid>litman et al, 2001),<papid> P01-1048 </papid>we have already given some descriptive statistics on corrections and aware sites and wehave been looking at methods to automatically predict these two utterance categories.</citsent>
<aftsection>
<nextsent>one of our major ndings is that prosody, which had already been shown to be good predictor of mis recognitions (litman et al, 2000; <papid> A00-2029 </papid>hirschberg et al, 2000), is also useful to correctly classify corrections and aware sites.</nextsent>
<nextsent>in this paper, we will elaborate more on the exact labeling scheme we used, and add further descriptive statistics.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y870">
<title id=" W01-1610.xml">labeling corrections and aware sites in spoken dialogue systems </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>than other turns (wade et al, 1992; oviatt et al, 1996; levow, 1998; <papid> P98-1122 </papid>bell and gustafson, 1999; shimojima et al, 1999), where asr models are not well adapted to handle this special speaking style.the current paper focuses on user corrections, and looks at places where people rst become aware of system problem (\aware sites ).</prevsent>
<prevsent>in other papers (swerts et al, 2000; hirschberg et al, 2001; <papid> N01-1027 </papid>litman et al, 2001),<papid> P01-1048 </papid>we have already given some descriptive statistics on corrections and aware sites and wehave been looking at methods to automatically predict these two utterance categories.</prevsent>
</prevsection>
<citsent citstr=" A00-2029 ">
one of our major ndings is that prosody, which had already been shown to be good predictor of mis recognitions (litman et al, 2000; <papid> A00-2029 </papid>hirschberg et al, 2000), is also useful to correctly classify corrections and aware sites.</citsent>
<aftsection>
<nextsent>in this paper, we will elaborate more on the exact labeling scheme we used, and add further descriptive statistics.
</nextsent>
<nextsent>more in particular, we address the question whether there is much variance in the way people react to system errors, and if so, to what extent this variance can be explained on the basis of particular properties of the dialogue system.
</nextsent>
<nextsent>in the following section we rst provide details on the toot corpus that we used for our analyses.
</nextsent>
<nextsent>then we give information on the labels for corrections and aware sites, and on the actual labeling procedure.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y878">
<title id=" W01-1616.xml">on the means for clarification in dialogue </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>resolution of these elliptical forms is achieved by allowing conversational participant to coerce clari cation question onto the list of questions under discussion (qud) inthe current dialogue context.
</prevsent>
<prevsent>this allows ellipsis resolution in the manner of of (ginzburg et al, 2001a) to give essentially the same reading as reprise questions.
</prevsent>
</prevsection>
<citsent citstr=" P01-1031 ">
(ginzburg and cooper, 2001) (<papid> P01-1031 </papid>hereafter g&c;) give more detailed analysis for the bare fragment form (therein described as clari cation ellipsis) and also give further reading for this form.</citsent>
<aftsection>
<nextsent>they call this reading the constituent reading to distinguish it from theclausal reading described above.
</nextsent>
<nextsent>this constituent reading involves querying the content of constituent which the cr initiator has been unable to ground in context (see (traum, 1994; clark, 1996)), and is along the lines of \what/who/(etc.) is the reference of your utterance x? .
</nextsent>
<nextsent>a possible lexical identi cation reading is also discussed, but no analysis given.
</nextsent>
<nextsent>they also raise the issue of whether these speci readings really exist or could be subsumed by single vague reading, but give evidence that this is not the case: they cite examples of cr misunderstanding leading to repeated attempts to elicit the desired clari cat ional information, showing that speci reading was intended; they also point out that some readings involve dierent parallelism conditions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y879">
<title id=" W01-1616.xml">on the means for clarification in dialogue </title>
<section> corpus analysis.  </section>
<citcontext>
<prevsection>
<prevsent>for this experiment, sub-portion of the dialogue transcripts was used consisting of c. 150,000 words.
</prevsent>
<prevsent>to maintain spread across dialogue domain, region, speaker age etc., this sub portion was created by taking 200-speaker turn section from 59 transcripts.all crs within this sub-corpus were iden ti ed and tagged, using the markup scheme and decision process described in 4.2 and 4.3 below.
</prevsent>
</prevsection>
<citsent citstr=" J96-2004 ">
at time of writing this process has been performed by only one (expert) user {our intention is to con rm results by comparing with those obtained by naive users, using e.g. the kappa statistic (carletta, 1996) <papid> J96-2004 </papid>to assess reliability.</citsent>
<aftsection>
<nextsent>initial identi cation of crs was performed using score (purver, 2001), search engine developed speci cally for this purpose (in particular, to allow searches for repeated words between speaker turns, and to display dialogue in an intuitive manner).
</nextsent>
<nextsent>however, inorder to ensure that all clari cat ional phenomena were captured, the nal search and markup were performed manually.
</nextsent>
<nextsent>4.2 markup scheme.
</nextsent>
<nextsent>the markup scheme used evolved during the markup process as new cr mechanisms wereidenti ed, and the nal scheme was as described here.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y880">
<title id=" W01-1616.xml">on the means for clarification in dialogue </title>
<section> conclusions.  </section>
<citcontext>
<prevsection>
<prevsent>of the remaining forms, we believe that two(non-reprise and conventional) can be accommodated relatively smoothly within our current hpsg framework.
</prevsent>
<prevsent>gaps and llers, how ever, present signi cant challenge and will be the subject of future research.
</prevsent>
</prevsection>
<citsent citstr=" W00-0302 ">
the measurements of css distance show that an utterance record with length of the order of ten sentences would be sucient to allow dialogue system to process the vast majority of crs.we are in the process of implementing our existing analyses for the cr forms and readings described above within hpsg/trindikit-based dialogue system which incorporates the ellipsis resolution capability of shards (ginzburg et al, 2001a)and the dialogue move engine of godis (larsson et al, 2000).<papid> W00-0302 </papid></citsent>
<aftsection>
<nextsent>at time of writing, the system can successfully produce both clausal and constituent readings.
</nextsent>
<nextsent>as result of there search outlined in this paper, lexical reading is currently being implemented.
</nextsent>
<nextsent>our results also suggest that investigation into disambiguation of reading, possibly on the basis of dialogue information state and/or intonation, will be required.
</nextsent>

</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y881">
<title id=" W00-0311.xml">a compact architecture for dialogue management based on scripts and meta outputs </title>
<section> introduction 1.  </section>
<citcontext>
<prevsection>
<prevsent>more recent work on spo-ken language interfaces to semi-autonomous robots include sri flakey robot (konolige et al, 1993) and ncarai inter bot project (perzanowski et al., 1998; perzanowski et al, 1999).
</prevsent>
<prevsent>a number of other systems have addressed part of the task.
</prevsent>
</prevsection>
<citsent citstr=" A97-1001 ">
com- mandtalk (moore et al, 1997), <papid> A97-1001 </papid>circuit fix-it shop (smith, 1997) <papid> A97-1008 </papid>and tl:tains-96 (traum and allen, 1994; <papid> P94-1001 </papid>traum and andersen, 1999) are spoken lan-guage systems but they interface to simulation or help facilities rather than semi-autonomous agents.</citsent>
<aftsection>
<nextsent>jack moose lodge (badler et al, 1999) takes text rather than speech as natural anguage input and the avatars being controlled are not semi-autonomous.
</nextsent>
<nextsent>other researchers have considered particular aspects of the problem such as accounting for various aspects of actions (webber, 1995; pym et al, 1995).
</nextsent>
<nextsent>in most of this and other related work the treatment is some variant of the following.
</nextsent>
<nextsent>if there is speech inter-face, the input speech signal is converted into text.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y882">
<title id=" W00-0311.xml">a compact architecture for dialogue management based on scripts and meta outputs </title>
<section> introduction 1.  </section>
<citcontext>
<prevsection>
<prevsent>more recent work on spo-ken language interfaces to semi-autonomous robots include sri flakey robot (konolige et al, 1993) and ncarai inter bot project (perzanowski et al., 1998; perzanowski et al, 1999).
</prevsent>
<prevsent>a number of other systems have addressed part of the task.
</prevsent>
</prevsection>
<citsent citstr=" A97-1008 ">
com- mandtalk (moore et al, 1997), <papid> A97-1001 </papid>circuit fix-it shop (smith, 1997) <papid> A97-1008 </papid>and tl:tains-96 (traum and allen, 1994; <papid> P94-1001 </papid>traum and andersen, 1999) are spoken lan-guage systems but they interface to simulation or help facilities rather than semi-autonomous agents.</citsent>
<aftsection>
<nextsent>jack moose lodge (badler et al, 1999) takes text rather than speech as natural anguage input and the avatars being controlled are not semi-autonomous.
</nextsent>
<nextsent>other researchers have considered particular aspects of the problem such as accounting for various aspects of actions (webber, 1995; pym et al, 1995).
</nextsent>
<nextsent>in most of this and other related work the treatment is some variant of the following.
</nextsent>
<nextsent>if there is speech inter-face, the input speech signal is converted into text.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y883">
<title id=" W00-0311.xml">a compact architecture for dialogue management based on scripts and meta outputs </title>
<section> introduction 1.  </section>
<citcontext>
<prevsection>
<prevsent>more recent work on spo-ken language interfaces to semi-autonomous robots include sri flakey robot (konolige et al, 1993) and ncarai inter bot project (perzanowski et al., 1998; perzanowski et al, 1999).
</prevsent>
<prevsent>a number of other systems have addressed part of the task.
</prevsent>
</prevsection>
<citsent citstr=" P94-1001 ">
com- mandtalk (moore et al, 1997), <papid> A97-1001 </papid>circuit fix-it shop (smith, 1997) <papid> A97-1008 </papid>and tl:tains-96 (traum and allen, 1994; <papid> P94-1001 </papid>traum and andersen, 1999) are spoken lan-guage systems but they interface to simulation or help facilities rather than semi-autonomous agents.</citsent>
<aftsection>
<nextsent>jack moose lodge (badler et al, 1999) takes text rather than speech as natural anguage input and the avatars being controlled are not semi-autonomous.
</nextsent>
<nextsent>other researchers have considered particular aspects of the problem such as accounting for various aspects of actions (webber, 1995; pym et al, 1995).
</nextsent>
<nextsent>in most of this and other related work the treatment is some variant of the following.
</nextsent>
<nextsent>if there is speech inter-face, the input speech signal is converted into text.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y885">
<title id=" W00-0311.xml">a compact architecture for dialogue management based on scripts and meta outputs </title>
<section> a prototype implementation  </section>
<citcontext>
<prevsection>
<prevsent>state parame-ters include the psa current position, some envi-ronmental variables uch as local temperature, pres-sure and carbon dioxide levels, and the status of the shuttle doors (open/closed).
</prevsent>
<prevsent>a visual display gives direct feedback on some of these parameters.
</prevsent>
</prevsection>
<citsent citstr=" P99-1024 ">
the speech and language processing architecture is based on that of the sri command talk sys-tem (moore et al, 1997; <papid> A97-1001 </papid>stent et al, 1999).<papid> P99-1024 </papid></citsent>
<aftsection>
<nextsent>the sys-tem comprises suite of about 20 agents, connected together using the sri open agent architecture (oaa; (martin et al, 1998)).
</nextsent>
<nextsent>speech recognition is performed using version of the nuance recog-nizer (nuance, 2000).
</nextsent>
<nextsent>initial language processing is carried out using the sri gemini system (dowding et al, 1993), <papid> P93-1008 </papid>using domain-independent ification.</nextsent>
<nextsent>grammar and domain-specific lexicon.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y886">
<title id=" W00-0311.xml">a compact architecture for dialogue management based on scripts and meta outputs </title>
<section> a prototype implementation  </section>
<citcontext>
<prevsection>
<prevsent>the sys-tem comprises suite of about 20 agents, connected together using the sri open agent architecture (oaa; (martin et al, 1998)).
</prevsent>
<prevsent>speech recognition is performed using version of the nuance recog-nizer (nuance, 2000).
</prevsent>
</prevsection>
<citsent citstr=" P93-1008 ">
initial language processing is carried out using the sri gemini system (dowding et al, 1993), <papid> P93-1008 </papid>using domain-independent ification.</citsent>
<aftsection>
<nextsent>grammar and domain-specific lexicon.
</nextsent>
<nextsent>the lan-guage processing rammar is compiled into recog-nition grammar using the methods of (moore et al, 1997); <papid> A97-1001 </papid>the n~ resnlt is that only grammatically well- formed utterances gan be recognized.</nextsent>
<nextsent>output from the initial language-processing step is represented inversion of quasi logical form (van eijck and moore, 1992), and passed in that form to the dia-logue manager.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y888">
<title id=" W01-0905.xml">two levels of valuation in a complex nl system </title>
<section> system architecture.  </section>
<citcontext>
<prevsection>
<prevsent>2.2 selection of relevant documents.
</prevsent>
<prevsent>the second module is classic search engine, giving, for each question, ranked list of documents, each of which could contain the answer.
</prevsent>
</prevsection>
<citsent citstr=" P99-1044 ">
this set of documents is then processed by third module, made of fastr (jacquemin, 1999), <papid> P99-1044 </papid>shallow transformational natural language analyser and of ranker.</citsent>
<aftsection>
<nextsent>this module can select, among documents found by the search engine, subset that satisfies more refined criteria.
</nextsent>
<nextsent>fastr improves things because it indexes documents with set of terms, including not only the (simple or compound) words of the initial question, but also their morphological, syntactic and semantic variants.
</nextsent>
<nextsent>each index is given weight all the higher as it is close to the original word in the question, or as it is significant.
</nextsent>
<nextsent>for instance, original terms are considered more reliable than semantic variants, and proper names are considered more significant than nouns.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y890">
<title id=" W01-0905.xml">two levels of valuation in a complex nl system </title>
<section> question-sentence pairing evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>answers to other questions (why questions for instance, or some sort of what questions), however, will consist of noun or sentence.
</prevsent>
<prevsent>finding its type is more complex, and is not done very often.
</prevsent>
</prevsection>
<citsent citstr=" C00-1043 ">
some systems, like falcon (harabagiu et al  2000) <papid> C00-1043 </papid>use wordnet word class hierarchies to assign types to answers.</citsent>
<aftsection>
<nextsent>among 682 answers in trec9, 57.5% were analysed by our system as named-entity questions, while others received no type assignment.
</nextsent>
<nextsent>among answers from our best 250-character run, 62.7% were about named entities.
</nextsent>
<nextsent>however, our run for shorter answers, yielding more modest score, gives 84% of named-entities answers.
</nextsent>
<nextsent>in our system answer type assignment is of surprisingly small import, where longer answers are concerned.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y891">
<title id=" W01-0908.xml">using the distribution of performance for studying statistical nlp systems and corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in particular, about sensitivity to features in the training data, and transferability.
</prevsent>
<prevsent>these properties can be different even when similar performance is reported.
</prevsent>
</prevsection>
<citsent citstr=" C00-2137 ">
a statistical treatment of question 1 is presented by yeh (2000).<papid> C00-2137 </papid></citsent>
<aftsection>
<nextsent>he tests for the significance of performance differences on fixed training and test datasets.
</nextsent>
<nextsent>in other related works, martin and hirschberg (1996) provides an overview of significance tests of error differences in small samples, and dietterich (1998) discusses results of number of tests.
</nextsent>
<nextsent>questions 2 and 3 have been frequently raised in nlp, but not explicitly addressed,since the prevailing evaluation methods provide no means of addressing them.
</nextsent>
<nextsent>in this paper we propose addressing all three questions with single experimental methodology, which uses the distribution of recall.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y893">
<title id=" W01-0908.xml">using the distribution of performance for studying statistical nlp systems and corpora </title>
<section> experimental setup.  </section>
<citcontext>
<prevsection>
<prevsent>section 4.4 motivates the use of recall and describes the experiments.
</prevsent>
<prevsent>4.1 data.
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
we used penn-treebank (marcus et al, 1993)<papid> J93-2004 </papid>data, presented in table 1.</citsent>
<aftsection>
<nextsent>wall-street journal (wsj) sections 15-18 and 20 were used by ramshaw and marcus (1995) <papid> W95-0107 </papid>as training and test data respectively for evaluating theirbase-np chunker.</nextsent>
<nextsent>these data have since become standard for evaluating base-np sys tems.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y894">
<title id=" W01-0908.xml">using the distribution of performance for studying statistical nlp systems and corpora </title>
<section> experimental setup.  </section>
<citcontext>
<prevsection>
<prevsent>4.1 data.
</prevsent>
<prevsent>we used penn-treebank (marcus et al, 1993)<papid> J93-2004 </papid>data, presented in table 1.</prevsent>
</prevsection>
<citsent citstr=" W95-0107 ">
wall-street journal (wsj) sections 15-18 and 20 were used by ramshaw and marcus (1995) <papid> W95-0107 </papid>as training and test data respectively for evaluating theirbase-np chunker.</citsent>
<aftsection>
<nextsent>these data have since become standard for evaluating base-np systems.
</nextsent>
<nextsent>the wsj texts are economic newspaper reports, which often include elaborated sentences containing about six base-nps on the source sentences words base nps wsj 15-18 8936 229598 54760 wsj 20 2012 51401 12335 atis 190 2046 613 wsj 20a 100 2479 614 wsj 20b 93 2661 619 table 1: data sources average.
</nextsent>
<nextsent>the atis data, on the other hand, are collection of customer requests related to flight schedules.
</nextsent>
<nextsent>these typically include short sentences which contain only three base-nps on the average.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y895">
<title id=" W01-0908.xml">using the distribution of performance for studying statistical nlp systems and corpora </title>
<section> experimental setup.  </section>
<citcontext>
<prevsection>
<prevsent>there is slight difference in size because sentences were kept complete, as explained section 4.3.
</prevsent>
<prevsent>4.2 learning algorithms.
</prevsent>
</prevsection>
<citsent citstr=" W99-0621 ">
we evaluated base-np learning systems based on two algorithms: mbsl (argamon et al, 1999) and snow (munoz et al, 1999).<papid> W99-0621 </papid></citsent>
<aftsection>
<nextsent>mbsl is memory-based system which records, for each pos sequence containing border (left, right, or both) of base-np, the number of times it appears with that border vs. the number of times it appears without it.
</nextsent>
<nextsent>it is possible to set an upper limit on the length of the pos sequences.
</nextsent>
<nextsent>given sentence, represented by sequence of pos tags, the system examines each sub sequence for being base-np.
</nextsent>
<nextsent>this is done by attempting to tile it using pos sequences that appeared in the training data with the base-np borders at the same locations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y896">
<title id=" W01-0908.xml">using the distribution of performance for studying statistical nlp systems and corpora </title>
<section> summary and further research.  </section>
<citcontext>
<prevsection>
<prevsent>by using mbsl with different context sizes, our results provide insights into the relation between training and test datasets, in termsof general and specific features.
</prevsent>
<prevsent>that issue becomes important when one plans to use system trained on certain dataset for analysing an arbitrary text.
</prevsent>
</prevsection>
<citsent citstr=" W01-0521 ">
another approach to this topic, examining the effect of using lexical bigram information, which is very corpus specific, appears in (gildea, 2001).<papid> W01-0521 </papid></citsent>
<aftsection>
<nextsent>in our experiments with systems trained onwsj data, there was clear difference between their behaviour on other wsj data and on the atis dataset, in which the structure of base-nps is different.
</nextsent>
<nextsent>that difference was observed with correlations and standard deviations.
</nextsent>
<nextsent>this shows that re sampling the training data is essential for noticing these structure differences.
</nextsent>
<nextsent>to control the effect of small size of the atis dataset, we provided two equally-small wsj datasets.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y897">
<title id=" W02-0404.xml">revisions that improve cohesion in multi document summaries a preliminary study </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>not only do writers have their own styles, they have the overarching structure of the article in mind when producing it.
</prevsent>
<prevsent>as result, in mds we are more likely to encounter text that is not cohesive.
</prevsent>
</prevsection>
<citsent citstr=" A00-2024 ">
previous research has addressed revision in single-document summaries [jing &amp; mckeown, 2000] <papid> A00-2024 </papid>[<papid> A00-2024 </papid>mani et al  1999] <papid> P99-1072 </papid>and has suggested that revising summaries can make them more informative and correct errors.</citsent>
<aftsection>
<nextsent>we believe that generate and-revise strategy might also be used in creating better multiple-document summaries, within the framework of current extractive summarization systems.
</nextsent>
<nextsent>however, as mentioned previously, there is reason to believe that multi-document summaries suffer from many different coherence problems and that such problems occur more often than in single-document summaries.
</nextsent>
<nextsent>therefore, an important preliminary step in determining how we might revise such summaries is to closely examine the cohesion problems that occur in multi document summaries.
</nextsent>
<nextsent>in the current paper we analyze small corpus of manually revised multi-document summaries.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y899">
<title id=" W02-0404.xml">revisions that improve cohesion in multi document summaries a preliminary study </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>not only do writers have their own styles, they have the overarching structure of the article in mind when producing it.
</prevsent>
<prevsent>as result, in mds we are more likely to encounter text that is not cohesive.
</prevsent>
</prevsection>
<citsent citstr=" P99-1072 ">
previous research has addressed revision in single-document summaries [jing &amp; mckeown, 2000] <papid> A00-2024 </papid>[<papid> A00-2024 </papid>mani et al  1999] <papid> P99-1072 </papid>and has suggested that revising summaries can make them more informative and correct errors.</citsent>
<aftsection>
<nextsent>we believe that generate and-revise strategy might also be used in creating better multiple-document summaries, within the framework of current extractive summarization systems.
</nextsent>
<nextsent>however, as mentioned previously, there is reason to believe that multi-document summaries suffer from many different coherence problems and that such problems occur more often than in single-document summaries.
</nextsent>
<nextsent>therefore, an important preliminary step in determining how we might revise such summaries is to closely examine the cohesion problems that occur in multi document summaries.
</nextsent>
<nextsent>in the current paper we analyze small corpus of manually revised multi-document summaries.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y902">
<title id=" W02-0404.xml">revisions that improve cohesion in multi document summaries a preliminary study </title>
<section> background and previous work.  </section>
<citcontext>
<prevsection>
<prevsent>rst describes the coherence nature of text and is based on the assumption that the elementary textual units are non-overlapping text spans.
</prevsent>
<prevsent>the central concept of rst is the rhetorical relation, which indicates the relationship between two spans.
</prevsent>
</prevsection>
<citsent citstr=" W97-0713 ">
rst can be used in sentence selection for single document summarization [marcu, 1997].<papid> W97-0713 </papid></citsent>
<aftsection>
<nextsent>however, it cannot be applied to mds.
</nextsent>
<nextsent>in rst, text coherence is achieved because the writer intentionally establishes relationships between the phrases in the text.
</nextsent>
<nextsent>this is not the case in mds, where sentences are extracted from different source articles, written by various authors.
</nextsent>
<nextsent>inspired by rst, [radev, 2000] <papid> W00-1009 </papid>endeavored to establish cross-document structure theory (cst) that is more appropriate for mds.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y903">
<title id=" W02-0404.xml">revisions that improve cohesion in multi document summaries a preliminary study </title>
<section> background and previous work.  </section>
<citcontext>
<prevsection>
<prevsent>in rst, text coherence is achieved because the writer intentionally establishes relationships between the phrases in the text.
</prevsent>
<prevsent>this is not the case in mds, where sentences are extracted from different source articles, written by various authors.
</prevsent>
</prevsection>
<citsent citstr=" W00-1009 ">
inspired by rst, [radev, 2000] <papid> W00-1009 </papid>endeavored to establish cross-document structure theory (cst) that is more appropriate for mds.</citsent>
<aftsection>
<nextsent>cst focuses on the relationships between sentences that come from multiple documents, which vary substantially from those between sentences in the same text.
</nextsent>
<nextsent>such relationships include identity, paraphrase and subsumption (one sentence contains more information than the other).
</nextsent>
<nextsent>2.2 computational models of text coherence.
</nextsent>
<nextsent>based on rst, [marcu, 2000] established rhetorical parser.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y908">
<title id=" W02-0404.xml">revisions that improve cohesion in multi document summaries a preliminary study </title>
<section> background and previous work.  </section>
<citcontext>
<prevsection>
<prevsent>they identified six operations and used them to implement an automatic revision module.
</prevsent>
<prevsent>2.4 temporal ordering of events.
</prevsent>
</prevsection>
<citsent citstr=" W01-1313 ">
[filatova &amp; hovy, 2001] <papid> W01-1313 </papid>addressed the issue of resolving temporal references in news stories.</citsent>
<aftsection>
<nextsent>although events in articles are not always presented in chronological order, readers must be able to reconstruct the timeline of events in order to comprehend the story.
</nextsent>
<nextsent>they endeavored to develop module that could automatically assign time stamp to each clause in document.
</nextsent>
<nextsent>using syntactic parser, patterns were discovered as to which syntactic phrases tend to indicate the occurrence of new event.
</nextsent>
<nextsent>in mds, the correct temporal relationships between events described in the extracted sentences often needs to be reestablished, since they may be incorrect or unclear.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y909">
<title id=" W02-0404.xml">revisions that improve cohesion in multi document summaries a preliminary study </title>
<section> background and previous work.  </section>
<citcontext>
<prevsection>
<prevsent>using syntactic parser, patterns were discovered as to which syntactic phrases tend to indicate the occurrence of new event.
</prevsent>
<prevsent>in mds, the correct temporal relationships between events described in the extracted sentences often needs to be reestablished, since they may be incorrect or unclear.
</prevsent>
</prevsection>
<citsent citstr=" H01-1065 ">
[barzilay et al  2001] <papid> H01-1065 </papid>evaluated three algorithms for sentence ordering in multi-document summaries.</citsent>
<aftsection>
<nextsent>one algorithm implemented was the chronological ordering algorithm.
</nextsent>
<nextsent>however, the resulting summaries often suffered from abrupt changes in topic.
</nextsent>
<nextsent>after conducting an experiment in which they studied how humans manually ordered sentences in summary, they concluded that topically related sentences should be grouped together.
</nextsent>
<nextsent>the chronological ordering algorithm was augmented by introducing cohesion constraint.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y910">
<title id=" W02-0404.xml">revisions that improve cohesion in multi document summaries a preliminary study </title>
<section> revision-based system architecture.  </section>
<citcontext>
<prevsection>
<prevsent>the output of this module is the revised, enhanced summary.
</prevsent>
<prevsent>3.1 the mead summarizer.
</prevsent>
</prevsection>
<citsent citstr=" W00-0403 ">
the mead summarizer [radev et al  2000] [<papid> W00-0403 </papid>radev et al 2002] is based on sentence extraction and uses linear combination of three features to rank the sentences in the source documents.</citsent>
<aftsection>
<nextsent>the first of the three features is the centro id score, which quantifies the centrality of sentence to the overall cluster of documents.
</nextsent>
<nextsent>the second is the position score, which assigns higher scores to sentences that are closer to the beginning of the document.
</nextsent>
<nextsent>the third feature, length, gives higher score to longer sentences.
</nextsent>
<nextsent>using linear combination of the three features, sentences are ranked by score and added to the summary until the desired length is attained.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y914">
<title id=" W00-1011.xml">dynamic user level and utility measurement for adaptive dialog in a help desk system </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>close-world and static approaches have tremendous limitations and often fail when the task becomes complex and the application environment and knowledge changes.
</prevsent>
<prevsent>thus, the learning capability of dialog system has become an important issue.
</prevsent>
</prevsection>
<citsent citstr=" P98-2129 ">
it has been addressed in many different aspects including dynamic construction of mutual knowledge (andersen et al 1999), learning of speech acts (stolcker et al 1998), learning optimal strategies (litman et al 1998; <papid> P98-2129 </papid>litman et al 1999; walker et al 1998), collaborative agent in plan recognition (lesh et al 1999), etc. this paper addresses the dynamic user modeling and dialog-goal utility measurement to facilitate adaptive dialog behavior.</citsent>
<aftsection>
<nextsent>for any dialog system dealing with technical domain, such as repair support (weis, 1997), help-desk support, etc, it is crucial for the system not only to pay attention to the user knowledge and experience level and dialog goals, but more important, to have certain mechanisms that adapt he system behavior in terms of action planning, content selection, and content realization to user cognitive limitations.
</nextsent>
<nextsent>dialog strategies and management should be adjusted to the evolving state of the user.
</nextsent>
<nextsent>thus better understanding and modeling of user cognitive process and human perception is desirable.
</nextsent>
<nextsent>in this paper, we propose methodology that automatically learns user experience levels based on sub-goal utilities and characteristics observed during the interaction.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y915">
<title id=" W02-0225.xml">rare dialogue acts in oncology consultations </title>
<section> background </section>
<citcontext>
<prevsection>
<prevsent>dialogue acts (das) which explicitly ensure mutual understanding are frequent in dialogues between cancer patients and health professionals.
</prevsent>
<prevsent>we present examples, and argue that this arises from the health- critical nature of these dialogues.
</prevsent>
</prevsection>
<citsent citstr=" W01-1627 ">
we have described elsewhere (wood, 2001; <papid> W01-1627 </papid>wood and craggs, 2002) the use of dialogue analysis in communication skills training for health professionals working with cancer patients.</citsent>
<aftsection>
<nextsent>our initial corpus arises from study of macmillan cancer care nurses undertaken by the psychological medicine group, university of manchester, funded by the cancer research campaign.
</nextsent>
<nextsent>it consists of 37 dialogues between nurses and patients, each comprising 200-1200 utterances (mostly 300-600).
</nextsent>
<nextsent>the nurses goal is to learn as much as possible about the pa tients?
</nextsent>
<nextsent>condition, both physical and mental, and to inform the patients about their condition and treatment.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y916">
<title id=" W02-0225.xml">rare dialogue acts in oncology consultations </title>
<section> rare dialogue acts.  </section>
<citcontext>
<prevsection>
<prevsent>protracted closing sequences are characteristic, and tend to have elements of both.
</prevsent>
<prevsent>we interpret these patterns as direct responses to the goal-directed and potentially health-critical nature of these dialogues.
</prevsent>
</prevsection>
<citsent citstr=" W98-0319 ">
we take as our point of comparison the corpus of some 200,000+ utterances from the switchboard corpus tagged with the swbd-damsl tagset (ju rafsky et al 1998).<papid> W98-0319 </papid></citsent>
<aftsection>
<nextsent>of these, 36% are statements?, 19% continuers?, and 13% opinions?, giving atotal of 68% of all utterances in the three most common categories.
</nextsent>
<nextsent>at the other end of the scale, an original tagset of 220 was reduced to 42 because the rarity of many made statistical analysis impossible.even of these 42, 32 occur with less than 1% frequency, 25 less than 0.5%.
</nextsent>
<nextsent>four of the five das we will discuss here are among these last 25.
</nextsent>
<nextsent>2.1 mutual understanding.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y917">
<title id=" W01-1409.xml">building a statistical machine translation system from scratch how much bang for the buck can we expect </title>
<section> evaluation experiments.  </section>
<citcontext>
<prevsection>
<prevsent>some of the parallel data was with held for system evaluation; the augmented training corpus (berkeley and tamil net corpus; short b+tn)had size of 85k tokens on the tamil side.
</prevsent>
<prevsent>the augmented training corpus had text coverage of 81% (seen at least once; 75% without augmentation), and67% (seen at least 5 times; 60% without augmenta tion), respectively, for sri lankan tamil.
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
we trained ibm translation model 4 (brown et al, 1993) <papid> J93-2003 </papid>both on our corpus alone and on the augmented corpus, using the egypt toolkit (knight et al, 1999; al-onaizan etal., 1999), and then translated number of texts using different translation models and different transfer methods, namely glossing (replacing each tamil word by the most likely candidate from the translation tables created with the egypt toolkit) and model 4 decoding (brown et al, 1995; germann et al, 2001).<papid> P01-1030 </papid></citsent>
<aftsection>
<nextsent>figure 3 shows the output of the different systems in comparison with the human translation.
</nextsent>
<nextsent>we then conducted the following experiments.
</nextsent>
<nextsent>3.2.1 document classification task seven human subjects without any knowledge of tamil were given translations of set of 15 texts (all from the berkeley corpus) and asked to categorize them according to the following topic hierarchy:  news about sri lanka  reports about clashes between the sri lankan army and the liberation tigers  sri lankan security-related news (arrests, arms deals, etc.)  sri lankan political news (strikes, transport, telecom)  concerns sri lanka but doesnt fit any of the above  news about pakistan/india nuclear tests in pakistan and india, including their aftermath (international reactions, etc.)  corruption investigation against benazir buto  news about pakistan/india but none of the above  international news  disasters, accidents  nelson mandelas birthday  other international news  impossible to tell except for one duplicate set, each subject received adifferent set of translations.
</nextsent>
<nextsent>the sets differed in training parameters and the translation method used.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y918">
<title id=" W01-1409.xml">building a statistical machine translation system from scratch how much bang for the buck can we expect </title>
<section> evaluation experiments.  </section>
<citcontext>
<prevsection>
<prevsent>some of the parallel data was with held for system evaluation; the augmented training corpus (berkeley and tamil net corpus; short b+tn)had size of 85k tokens on the tamil side.
</prevsent>
<prevsent>the augmented training corpus had text coverage of 81% (seen at least once; 75% without augmentation), and67% (seen at least 5 times; 60% without augmenta tion), respectively, for sri lankan tamil.
</prevsent>
</prevsection>
<citsent citstr=" P01-1030 ">
we trained ibm translation model 4 (brown et al, 1993) <papid> J93-2003 </papid>both on our corpus alone and on the augmented corpus, using the egypt toolkit (knight et al, 1999; al-onaizan etal., 1999), and then translated number of texts using different translation models and different transfer methods, namely glossing (replacing each tamil word by the most likely candidate from the translation tables created with the egypt toolkit) and model 4 decoding (brown et al, 1995; germann et al, 2001).<papid> P01-1030 </papid></citsent>
<aftsection>
<nextsent>figure 3 shows the output of the different systems in comparison with the human translation.
</nextsent>
<nextsent>we then conducted the following experiments.
</nextsent>
<nextsent>3.2.1 document classification task seven human subjects without any knowledge of tamil were given translations of set of 15 texts (all from the berkeley corpus) and asked to categorize them according to the following topic hierarchy:  news about sri lanka  reports about clashes between the sri lankan army and the liberation tigers  sri lankan security-related news (arrests, arms deals, etc.)  sri lankan political news (strikes, transport, telecom)  concerns sri lanka but doesnt fit any of the above  news about pakistan/india nuclear tests in pakistan and india, including their aftermath (international reactions, etc.)  corruption investigation against benazir buto  news about pakistan/india but none of the above  international news  disasters, accidents  nelson mandelas birthday  other international news  impossible to tell except for one duplicate set, each subject received adifferent set of translations.
</nextsent>
<nextsent>the sets differed in training parameters and the translation method used.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y921">
<title id=" W02-0305.xml">mplus a probabilistic medical language understanding system </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>because of the off-the-cuff nature of radiology dictation, report will frequently contain text that is telegraphic or otherwise not well formed grammatically.
</prevsent>
<prevsent>our desire was not only to take advantage of phrasal structure to discover semantic patterns in text, but also to be able to infer those patterns from lexical and contextual cues when necessary.
</prevsent>
</prevsection>
<citsent citstr=" P89-1005 ">
most nlp systems capable of semantic analysis employ representational formalisms with ties to classical logic, including semantic grammars (friedman et al , 1994), unification based semantics (moore, 1989), <papid> P89-1005 </papid>and description logics (romacker and hahn, 2000).<papid> A00-2043 </papid></citsent>
<aftsection>
<nextsent>m+ and its predecessors employ bayesian networks (pearl, 1988), methodology outside this tradition.
</nextsent>
<nextsent>this study discusses the philosophy and implementation of m+, and attempts to show how bayesian networks can be useful in medical text analysis.
</nextsent>
<nextsent>the m+ semantic model 2.1 semantic bayesian networks.
</nextsent>
<nextsent>m+ uses bayesian networks (bns) to represent the basic semantic types and relations within medical domain such as chest radiology reports.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y922">
<title id=" W02-0305.xml">mplus a probabilistic medical language understanding system </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>because of the off-the-cuff nature of radiology dictation, report will frequently contain text that is telegraphic or otherwise not well formed grammatically.
</prevsent>
<prevsent>our desire was not only to take advantage of phrasal structure to discover semantic patterns in text, but also to be able to infer those patterns from lexical and contextual cues when necessary.
</prevsent>
</prevsection>
<citsent citstr=" A00-2043 ">
most nlp systems capable of semantic analysis employ representational formalisms with ties to classical logic, including semantic grammars (friedman et al , 1994), unification based semantics (moore, 1989), <papid> P89-1005 </papid>and description logics (romacker and hahn, 2000).<papid> A00-2043 </papid></citsent>
<aftsection>
<nextsent>m+ and its predecessors employ bayesian networks (pearl, 1988), methodology outside this tradition.
</nextsent>
<nextsent>this study discusses the philosophy and implementation of m+, and attempts to show how bayesian networks can be useful in medical text analysis.
</nextsent>
<nextsent>the m+ semantic model 2.1 semantic bayesian networks.
</nextsent>
<nextsent>m+ uses bayesian networks (bns) to represent the basic semantic types and relations within medical domain such as chest radiology reports.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y923">
<title id=" W00-1102.xml">exploiting lexical expansions and boolean compositions for web querying </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the use of query expansions for text retrieval is debated topic.
</prevsent>
<prevsent>voorhees (1998) argues that wordnet derived query expansions are effective for very short queries, while they do not bring any improvements for long queries.
</prevsent>
</prevsection>
<citsent citstr=" W98-0704 ">
from number of experiments (mandala et al, 1998) <papid> W98-0704 </papid>conclude that wordnet query expansions can increase recall but degrade precision performances.</citsent>
<aftsection>
<nextsent>three reasons are suggested to explain this behavior: (i) the lack of relations among terms of different parts of speech in wordnet; (ii) many semantic relations are not present in wordnet; (iii) proper names are not included in wordnet.
</nextsent>
<nextsent>(gonzalo et al., 1998) <papid> W98-0705 </papid>pointed out some more weaknesses of wordnet for information retrieval purposes, in particular the lack of domain information and the fact that sense distinctions are excessively fine-grained for the task.</nextsent>
<nextsent>a related topic of query expansion is query i~anslation, which is performed in cross-language information retrieval (verdejo et al 2000).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y924">
<title id=" W00-1102.xml">exploiting lexical expansions and boolean compositions for web querying </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>from number of experiments (mandala et al, 1998) <papid> W98-0704 </papid>conclude that wordnet query expansions can increase recall but degrade precision performances.</prevsent>
<prevsent>three reasons are suggested to explain this behavior: (i) the lack of relations among terms of different parts of speech in wordnet; (ii) many semantic relations are not present in wordnet; (iii) proper names are not included in wordnet.</prevsent>
</prevsection>
<citsent citstr=" W98-0705 ">
(gonzalo et al., 1998) <papid> W98-0705 </papid>pointed out some more weaknesses of wordnet for information retrieval purposes, in particular the lack of domain information and the fact that sense distinctions are excessively fine-grained for the task.</citsent>
<aftsection>
<nextsent>a related topic of query expansion is query i~anslation, which is performed in cross-language information retrieval (verdejo et al 2000).
</nextsent>
<nextsent>this work brings additional elements in favor of the thesis that using linguistic expansions can improve ir in web search scenario.
</nextsent>
<nextsent>in addition we argue that, to be effective, query expansion has to be combined with proper search modalities.
</nextsent>
<nextsent>the evaluation experiment we carried out, even within the limitations due to time and budget constraints, was designed to take into account the indications that came out at the recent trec workshop on question answering (voorhees, 2000).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y925">
<title id=" W00-0103.xml">reducing lexical semantic complexity with systematic polysemous classes and under specification </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in many applications, for instance in document categorization, information retrieval, and information extraction, it may be sufficient to know if given word belongs to certain class (underspecified sense) rather than to know which of its (related) senses exactly to pick.
</prevsent>
<prevsent>the approach for finding systematic polysemous classes is based on that of (buitelaar 1998a, buitelaar 1998b), while addressing some previous hortcomings.
</prevsent>
</prevsection>
<citsent citstr=" W97-0802 ">
this paper presents an algorithm for finding systematic polysemous classes in wordnet (miller et al 1990) and germanet (hamp and feldweg 1997) -- <papid> W97-0802 </papid>semantic database for german similar to wordnet.</citsent>
<aftsection>
<nextsent>the introduction of such classes can reduce the amount of lexical semantic processing, because the number of disambiguation decisions can be restricted more clearly to those cases that involve real ambiguity (homonymy).
</nextsent>
<nextsent>different than with homonyms, systematically polysemous words need not always be disambiguated, because such words have several related senses that are shared in systematic way by group of similar words.
</nextsent>
<nextsent>in many applications then, for instance in document categorization and other areas of information retrieval, it may be sufficient know if given word belongs to this grou rather than to know which of its (related) senses exactly to pick.
</nextsent>
<nextsent>in other words, it will suffice to assign more coarse grained sense that leaves several related senses underspecified, but which can be further specified on demand 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y926">
<title id=" W00-0103.xml">reducing lexical semantic complexity with systematic polysemous classes and under specification </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in many applications then, for instance in document categorization and other areas of information retrieval, it may be sufficient know if given word belongs to this grou rather than to know which of its (related) senses exactly to pick.
</prevsent>
<prevsent>in other words, it will suffice to assign more coarse grained sense that leaves several related senses underspecified, but which can be further specified on demand 1.
</prevsent>
</prevsection>
<citsent citstr=" W98-0717 ">
the approach for finding systematic polysemous classes is based on that of (buitelaar 1998a, buitelaar 1998b), but takes into account some shortcomings as pointed out in (krymolowski and roth 1998) (<papid> W98-0717 </papid>peters, peters and vossen 1998) (tomuro 1998).<papid> W98-0715 </papid></citsent>
<aftsection>
<nextsent>whereas the original approach identified small set of top-level synsets for grouping together lexical items, as pointed out in (wilks 99), earlier work in ai on  polaroid words  (hirst 87) and  word experts  (small 81) advocated similar, incremental approach to sense representation and interpretation.
</nextsent>
<nextsent>in line with this, the corelex approach discussed here provides large scale inventory of systematically polysemous lexical items with underspecified presentations that can be incrementally refined.
</nextsent>
<nextsent>14 the new approach compares lexical items according to all of their synsets on all hierarchy levels.
</nextsent>
<nextsent>in addition, the new approach is both more flexible and precise by using clustering algorithm for comparing meaning distributions between lexical items.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y928">
<title id=" W00-0103.xml">reducing lexical semantic complexity with systematic polysemous classes and under specification </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in many applications then, for instance in document categorization and other areas of information retrieval, it may be sufficient know if given word belongs to this grou rather than to know which of its (related) senses exactly to pick.
</prevsent>
<prevsent>in other words, it will suffice to assign more coarse grained sense that leaves several related senses underspecified, but which can be further specified on demand 1.
</prevsent>
</prevsection>
<citsent citstr=" W98-0715 ">
the approach for finding systematic polysemous classes is based on that of (buitelaar 1998a, buitelaar 1998b), but takes into account some shortcomings as pointed out in (krymolowski and roth 1998) (<papid> W98-0717 </papid>peters, peters and vossen 1998) (tomuro 1998).<papid> W98-0715 </papid></citsent>
<aftsection>
<nextsent>whereas the original approach identified small set of top-level synsets for grouping together lexical items, as pointed out in (wilks 99), earlier work in ai on  polaroid words  (hirst 87) and  word experts  (small 81) advocated similar, incremental approach to sense representation and interpretation.
</nextsent>
<nextsent>in line with this, the corelex approach discussed here provides large scale inventory of systematically polysemous lexical items with underspecified presentations that can be incrementally refined.
</nextsent>
<nextsent>14 the new approach compares lexical items according to all of their synsets on all hierarchy levels.
</nextsent>
<nextsent>in addition, the new approach is both more flexible and precise by using clustering algorithm for comparing meaning distributions between lexical items.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y932">
<title id=" W00-0103.xml">reducing lexical semantic complexity with systematic polysemous classes and under specification </title>
<section> corelex-ii.  </section>
<citcontext>
<prevsection>
<prevsent>instead, threshold on similarity can be set that constraints clustering algorithm for automatically grouping together words into systematic polysemous classes.
</prevsent>
<prevsent>(no human intervention to further group together resulting classes is required.)
</prevsent>
</prevsection>
<citsent citstr=" C94-2113 ">
this approach took inspiration from the pioneering work by (dolan 1994), <papid> C94-2113 </papid>but it is also fundamentally different, because instead of grouping similar senses together, the corelex approach groups together words according to all of their senses.</citsent>
<aftsection>
<nextsent>1.
</nextsent>
<nextsent>foreach noun 2.
</nextsent>
<nextsent>get al  levell synsets (senses) 3.
</nextsent>
<nextsent>if number of level1 synsets   1 then put noun in list 4.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y934">
<title id=" W00-1017.xml">wit a toolkit for building robust and real time spoken dialogu systems </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>wit features an incremental under-standing mechanism that enables ro-bust utterance understanding and real- time responses.
</prevsent>
<prevsent>wit ability to com-pile domain-dependent system specifi-cations into internal knowledge sources makes building spoken dialogue sys-tems much easier than :it is from scratch.
</prevsent>
</prevsection>
<citsent citstr=" P96-1009 ">
the recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems (aust et al., 1995; allen et al, 1996; <papid> P96-1009 </papid>zue et al, 2000; walker et al, 2000).<papid> A00-2028 </papid></citsent>
<aftsection>
<nextsent>one of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain.
</nextsent>
<nextsent>to this end, several tool kits for building spo-ken dialogue systems have been developed (bar- nett and singh, 1997; sasa jima et al, 1999).
</nextsent>
<nextsent>one is the cslu toolkit (sutton et al, 1998), which enables rapid prototyping of spoken di-alogue system that incorporates finite-state dia-logue model.
</nextsent>
<nextsent>it decreases the amount of the ef-fort required in building spoken dialogue sys-tem in user-defined task domain.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y935">
<title id=" W00-1017.xml">wit a toolkit for building robust and real time spoken dialogu systems </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>wit features an incremental under-standing mechanism that enables ro-bust utterance understanding and real- time responses.
</prevsent>
<prevsent>wit ability to com-pile domain-dependent system specifi-cations into internal knowledge sources makes building spoken dialogue sys-tems much easier than :it is from scratch.
</prevsent>
</prevsection>
<citsent citstr=" A00-2028 ">
the recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems (aust et al., 1995; allen et al, 1996; <papid> P96-1009 </papid>zue et al, 2000; walker et al, 2000).<papid> A00-2028 </papid></citsent>
<aftsection>
<nextsent>one of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain.
</nextsent>
<nextsent>to this end, several tool kits for building spo-ken dialogue systems have been developed (bar- nett and singh, 1997; sasa jima et al, 1999).
</nextsent>
<nextsent>one is the cslu toolkit (sutton et al, 1998), which enables rapid prototyping of spoken di-alogue system that incorporates finite-state dia-logue model.
</nextsent>
<nextsent>it decreases the amount of the ef-fort required in building spoken dialogue sys-tem in user-defined task domain.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y936">
<title id=" W00-1017.xml">wit a toolkit for building robust and real time spoken dialogu systems </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>previous tool kits, however, do not allow us to achieve these features, or do not provide mecha-nisms that achieve these features without requir-ing excessive fforts by the developers.
</prevsent>
<prevsent>this paper presents wit 1, which is toolkit iwit is an acronym of workable spoken dialogue lnter- 150 for building spoken dialogue systems that inte-grate speech recognition, language understanding and generation, and speech output.
</prevsent>
</prevsection>
<citsent citstr=" P99-1026 ">
wit features an incremental understanding method (nakano et al., 1999<papid> P99-1026 </papid>b) that makes it possible to build robust and real-time system.</citsent>
<aftsection>
<nextsent>in addition, wit compiles domain-dependent system specifications into in-ternal knowledge sources that building systems is easier.
</nextsent>
<nextsent>although wit requires more domain- dependent specifications than finite-state-model- based tool kits, wit-based systems are capable of taking full advantage of language processing technology.
</nextsent>
<nextsent>wit has been implemented and used to build several spoken dialogue systems.
</nextsent>
<nextsent>in what follows, we overview wit, explain its architecture, domain-dependent system specifica-tions, and implementation, and then discuss its advantages and problems.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y942">
<title id=" W00-1017.xml">wit a toolkit for building robust and real time spoken dialogu systems </title>
<section> architecture of  wit-based spoken.  </section>
<citcontext>
<prevsection>
<prevsent>isss also makes it possible for the language generation module to respond in real time because it can out- put partial result of understanding at any point in time.
</prevsent>
<prevsent>the domain-dependent knowledge used in this module consists of unification-based lexicon and phrase structure rules.
</prevsent>
</prevsection>
<citsent citstr=" P91-1040 ">
disjunctive feature descriptions are also possible; wit incorporates an efficient method for handling dis junctions (nakano, 1991).<papid> P91-1040 </papid></citsent>
<aftsection>
<nextsent>when phrase boundary is de-tected, the feature structure for phrase is com-puted using some built-in rules from the feature structure rules for the words in the phrase.
</nextsent>
<nextsent>the phrase structure rules specify what kind of phrase sequences can be considered as sentences, and they also enable computing the semantic repre-sentation for found sentences.
</nextsent>
<nextsent>two kinds of sen- tenees can be considered; domain-related ones that express the user intention about he reser- 152 vafion and dialogue-related ones that express the user attitude with respect to the progress of the dialogue, such as confirmation and denial.
</nextsent>
<nextsent>con-sidering the meeting room reservation system, ex-amples of domain-related sentences are  need to book room 2 on wednesday , need to book room 2 , and  room 2  and dialogue-related ones are  yes ,  no , and  okay .
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y963">
<title id=" W00-1017.xml">wit a toolkit for building robust and real time spoken dialogu systems </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>recording some dialogue history is also possible.
</prevsent>
<prevsent>since the language understanding mod-ule utilizes unification, wide variety of lin-guistic phenomena can be covered.
</prevsent>
</prevsection>
<citsent citstr=" C94-2164 ">
for exam-ple, speech repairs, particle omission, and fillers can be dealt with in the framework of unifica-tion grammar (nakano et al, 1994; <papid> C94-2164 </papid>nakano and shimazu, 1999).</citsent>
<aftsection>
<nextsent>the language generation mod-ule features common lisp functions, so there is no limitation on the description.
</nextsent>
<nextsent>some of the systems we have developed feature generation method based on hierarchical planning (dohsaka and shirnazu, 1997).
</nextsent>
<nextsent>it is also possible to build simple finite-state-model-based dialogue system using wit.
</nextsent>
<nextsent>states can be represented by dialogue phases in wit.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y964">
<title id=" W00-1321.xml">reducing parsing complexity by intra sentence segmentation based on maximum entropy model </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the parsing efficiency is im-proved by 77% in time and 71% in space.
</prevsent>
<prevsent>long sentence analysis has been critical problem in machine translation because of high complexity.
</prevsent>
</prevsection>
<citsent citstr=" C94-1014 ">
in ebmt (example-based machine translation), the longer sentence is, the less possible it is that the sentence has an exact match in the translation archive, and the less flexible an ebmt system will be (cranias et al, 1994).<papid> C94-1014 </papid></citsent>
<aftsection>
<nextsent>in idiom-based ma-chine translation (lee, 1993), long sentence parsing is difficult because more resources are spent during idiom recognition phase as sen-tence length increases.
</nextsent>
<nextsent>a parser is often un-able to analyze long sentences owing to their complexity, though they have no grammatical errors (nasukawa, 1995).<papid> P95-1006 </papid></nextsent>
<nextsent>in english-korean machine translation, idiom-based approach is adopted to overcome the structural differences between two lan-guages and to get more accurate translation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y965">
<title id=" W00-1321.xml">reducing parsing complexity by intra sentence segmentation based on maximum entropy model </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in ebmt (example-based machine translation), the longer sentence is, the less possible it is that the sentence has an exact match in the translation archive, and the less flexible an ebmt system will be (cranias et al, 1994).<papid> C94-1014 </papid></prevsent>
<prevsent>in idiom-based ma-chine translation (lee, 1993), long sentence parsing is difficult because more resources are spent during idiom recognition phase as sen-tence length increases.</prevsent>
</prevsection>
<citsent citstr=" P95-1006 ">
a parser is often un-able to analyze long sentences owing to their complexity, though they have no grammatical errors (nasukawa, 1995).<papid> P95-1006 </papid></citsent>
<aftsection>
<nextsent>in english-korean machine translation, idiom-based approach is adopted to overcome the structural differences between two lan-guages and to get more accurate translation.
</nextsent>
<nextsent>the parser is chart parser with capabil-ity of idiom recognition and translation, which is adapted to english-korean machine trana- lation.
</nextsent>
<nextsent>idioms are recognized prior to syn-tactic analysis and the part of sentence for an idiom takes an edge in chart (winograd, 1983).
</nextsent>
<nextsent>when parsing long sentences, an am-biguity of an idiom range may cause more edges than the number of words included in the idiom (yoon, 1994), which increases pars-ing complexity much.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y966">
<title id=" W00-1321.xml">reducing parsing complexity by intra sentence segmentation based on maximum entropy model </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our work is moti-vated by the fact that parsing becomes more efficient, if becomes horter.
</prevsent>
<prevsent>this paper deals with the problem of parsing complex-ity by way of reducing the length of sentence to be analyzed.
</prevsent>
</prevsection>
<citsent citstr=" W97-0304 ">
this reduction is achieved by in ra -sentence segmentation, which is distinguished from inter--sentence gmen- tation that is used for text categorization (beeferman et al, 1997) <papid> W97-0304 </papid>or sentence boundary identification (palmer and hearst, 1997) (<papid> J97-2002 </papid>rey- nar and ratnaparkhi, 1997).<papid> A97-1004 </papid></citsent>
<aftsection>
<nextsent>intra-sentence segmentation plays role as preliminary step to chart-based, context-free parser in english-korean machine translation.
</nextsent>
<nextsent>there have been several methods for reducing parsing complexities by intra- sentence segmentation.
</nextsent>
<nextsent>in (lyon and frank, 1995)(lyon and dickerson, 1997), they took advantage of the fact that the declarative sentences almost always consist of three seg- ments: \[pre-subject : subject:predicate\] . the complexity could be reduced by decom-posing sentence into three sections.
</nextsent>
<nextsent>pattern rules (li et al, 1990) <papid> C90-3088 </papid>and sentence patterns (kim and khn, 1995) were used to segment long english sentences.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y967">
<title id=" W00-1321.xml">reducing parsing complexity by intra sentence segmentation based on maximum entropy model </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our work is moti-vated by the fact that parsing becomes more efficient, if becomes horter.
</prevsent>
<prevsent>this paper deals with the problem of parsing complex-ity by way of reducing the length of sentence to be analyzed.
</prevsent>
</prevsection>
<citsent citstr=" J97-2002 ">
this reduction is achieved by in ra -sentence segmentation, which is distinguished from inter--sentence gmen- tation that is used for text categorization (beeferman et al, 1997) <papid> W97-0304 </papid>or sentence boundary identification (palmer and hearst, 1997) (<papid> J97-2002 </papid>rey- nar and ratnaparkhi, 1997).<papid> A97-1004 </papid></citsent>
<aftsection>
<nextsent>intra-sentence segmentation plays role as preliminary step to chart-based, context-free parser in english-korean machine translation.
</nextsent>
<nextsent>there have been several methods for reducing parsing complexities by intra- sentence segmentation.
</nextsent>
<nextsent>in (lyon and frank, 1995)(lyon and dickerson, 1997), they took advantage of the fact that the declarative sentences almost always consist of three seg- ments: \[pre-subject : subject:predicate\] . the complexity could be reduced by decom-posing sentence into three sections.
</nextsent>
<nextsent>pattern rules (li et al, 1990) <papid> C90-3088 </papid>and sentence patterns (kim and khn, 1995) were used to segment long english sentences.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y968">
<title id=" W00-1321.xml">reducing parsing complexity by intra sentence segmentation based on maximum entropy model </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our work is moti-vated by the fact that parsing becomes more efficient, if becomes horter.
</prevsent>
<prevsent>this paper deals with the problem of parsing complex-ity by way of reducing the length of sentence to be analyzed.
</prevsent>
</prevsection>
<citsent citstr=" A97-1004 ">
this reduction is achieved by in ra -sentence segmentation, which is distinguished from inter--sentence gmen- tation that is used for text categorization (beeferman et al, 1997) <papid> W97-0304 </papid>or sentence boundary identification (palmer and hearst, 1997) (<papid> J97-2002 </papid>rey- nar and ratnaparkhi, 1997).<papid> A97-1004 </papid></citsent>
<aftsection>
<nextsent>intra-sentence segmentation plays role as preliminary step to chart-based, context-free parser in english-korean machine translation.
</nextsent>
<nextsent>there have been several methods for reducing parsing complexities by intra- sentence segmentation.
</nextsent>
<nextsent>in (lyon and frank, 1995)(lyon and dickerson, 1997), they took advantage of the fact that the declarative sentences almost always consist of three seg- ments: \[pre-subject : subject:predicate\] . the complexity could be reduced by decom-posing sentence into three sections.
</nextsent>
<nextsent>pattern rules (li et al, 1990) <papid> C90-3088 </papid>and sentence patterns (kim and khn, 1995) were used to segment long english sentences.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y969">
<title id=" W00-1321.xml">reducing parsing complexity by intra sentence segmentation based on maximum entropy model </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>there have been several methods for reducing parsing complexities by intra- sentence segmentation.
</prevsent>
<prevsent>in (lyon and frank, 1995)(lyon and dickerson, 1997), they took advantage of the fact that the declarative sentences almost always consist of three seg- ments: \[pre-subject : subject:predicate\] . the complexity could be reduced by decom-posing sentence into three sections.
</prevsent>
</prevsection>
<citsent citstr=" C90-3088 ">
pattern rules (li et al, 1990) <papid> C90-3088 </papid>and sentence patterns (kim and khn, 1995) were used to segment long english sentences.</citsent>
<aftsection>
<nextsent>they showed low seg-mentation coverage, which means that many of long sentences are not segmented by the pattern rules or sentence patterns.
</nextsent>
<nextsent>and they require much human efforts to construct pat-tern rules or collect sentence patterns.
</nextsent>
<nextsent>these factors may prevent hem being applicable to practical machine translation systems.
</nextsent>
<nextsent>this paper presents trainable model for identifying potential segmentation positions 164 in sentence and determining appropriate segmentation positions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y973">
<title id=" W00-0504.xml">mandarin english information mei investigating trans lingual speech retrieval </title>
<section> multi seale embedded translation.  </section>
<citcontext>
<prevsection>
<prevsent>in query translation, english text queries are transformed into mandarin and then used to retrieve mandarin documents.
</prevsent>
<prevsent>for document translation, mandarin documents are translated into english before they are indexed and then matched with english queries.
</prevsent>
</prevsection>
<citsent citstr=" P99-1027 ">
mccarley has reported improved effectiveness from techniques that couple the two techniques \[mccarley, 1999\], <papid> P99-1027 </papid>but time constraints may limit us to explonng only the query translation strategy dunng the six-week workshop.</citsent>
<aftsection>
<nextsent>4,1 word translation while we make use of sub-word transcription to smooth out-of-vocabulary(oov) problems in speech recognition as described above, and to alleviate the oov problem :for translation as we discuss in the next section, accurate translation generally relies on the additional information available at the word and phrase levels.
</nextsent>
<nextsent>since the  bag of words  information retrieval techniques do not incorporate any meaningful degree of language understanding to assess similarity between queries and documents, word-for-word (or, more generally, term-for-term) embedded translation approach can achieve useful level of effectiveness for many trans lingual retrieval applications \[oard and diekema, 1998\].
</nextsent>
<nextsent>we have developed such technique for the tdt-3 topic tracking evaluation \[levow and oard, 2000\].
</nextsent>
<nextsent>for that work we extracted an enriched bilingual mandarin-english term list by combining two term lists: (i) list assembled by the linguistic data consortium from freely available on-line resources; and (ii) entries from the ceta file (sometimes referred to as  optilex ).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y974">
<title id=" W00-1207.xml">statistically enhanced new word identification in a rule based chinese system </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in languages like chinese, where no word boundary exists in written texts, this is by no means an easy job.
</prevsent>
<prevsent>in many cases the machine will not even realize that there is an unfound word in the sentence since most single chinese characters can be words by themselves.
</prevsent>
</prevsection>
<citsent citstr=" J96-3004 ">
purely statistical methods of word segmentation (e.g. de marcken 1996, sproat et al 1996, <papid> J96-3004 </papid>tung and lee 1994, lin et al(1993), chiang et al(1992), lua, huang et al etc.) often fail to identify those words because of the sparse data problem, as the likelihood for those words to appear in the training texts is extremely ow.</citsent>
<aftsection>
<nextsent>there are also hybrid approaches such as (nie dt al 1995) where statistical approaches and heuristic rules are combined to identify new words.
</nextsent>
<nextsent>they generally perform better than purely statistical segment ers, but the new words they are able to recognize are usually proper names and other elat ively frequent words.
</nextsent>
<nextsent>they require reasonably big training corpus and the performance is often domain-specific depending on the training corpus used.
</nextsent>
<nextsent>many word segment ers ignore low-frequency new words and treat heir component characters as independent words, since they are often of 46 little significance in applications where the structure of sentences is not taken into consideration.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y975">
<title id=" W00-1212.xml">a block based robust dependency parser for unrestricted chinese text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this is mainly due to the fact that the structure of the chinese language is quite different from english.
</prevsent>
<prevsent>therefore the computational model in processing english may not be directly applied to the chinese language.
</prevsent>
</prevsection>
<citsent citstr=" J91-4001 ">
lin-shan lee et al(1991) <papid> J91-4001 </papid>proposed chinese natural language processing system with special consideration of some typical phenomena of chinese.</citsent>
<aftsection>
<nextsent>jinye zhou et al(1986) presented deterministic chinese parsing methodology using formal semantics to combine syntactic and semantic analysis.
</nextsent>
<nextsent>however, most of the proposed approaches were realized on small-scale lexicon and rule base (usually thousands words and tens or hundreds rules).
</nextsent>
<nextsent>it is still an open issue whether these models will work on real texts containing various ungrammatical phenomena.
</nextsent>
<nextsent>a parser capable of handling real text should have not only large lexicon and big rule base, but also high robustness in coping with different kinds of ungrammatical phenomena.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y976">
<title id=" W02-0504.xml">an hmm approach to vowel restoration in arabic and hebrew </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in both hebrew and in arabic, modern written texts are composed in script that leaves out most of the vowels of the words.
</prevsent>
<prevsent>because many words that have different vowel patterns may appear identical in vowel-less setting, considerable ambiguity exists at the word level.
</prevsent>
</prevsection>
<citsent citstr=" J95-3004 ">
in hebrew, levinger et al (1995) <papid> J95-3004 </papid>computed that 55% out of 40,000 word tokens taken froma corpus of the israeli daily haaretz were am biguous.</citsent>
<aftsection>
<nextsent>for example, the non-voweled hebrew word , written in latin transliteration as spr, may represent the noun book?
</nextsent>
<nextsent>(pronounced /sepher/), the third person singular form of the verb to count?
</nextsent>
<nextsent>(pronounced /saphar/) or at least four other possible interpretations.
</nextsent>
<nextsent>in arabic, there are almost five possible morphological analyses per word on average (beesley 1998).take, for example, the arabic word , written in latin transliteration as ktaab.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y977">
<title id=" W00-1205.xml">sinica treebank design criteria annotation guidelines and online interface </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>one of the important design decisions following these criteria is the encoding of thematic role information.
</prevsent>
<prevsent>an on-line interface facilitating empirical studies of chinese phrase structure is also described.
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
the penn treebank (marcus et al 1993) <papid> J93-2004 </papid>initiated new paradigm in corpus-based research.</citsent>
<aftsection>
<nextsent>the english.
</nextsent>
<nextsent>penn treebank has enabled and motivated corpus and computational linguistic research based on information extractable from structurally annotated corpora.
</nextsent>
<nextsent>recently, the research has focused on the following two issues: first, when and how can structurally annotated corpus of language be built?
</nextsent>
<nextsent>second, what information should or can be annotated?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y978">
<title id=" W01-1627.xml">dialogue tagsets in oncology </title>
<section> motivation </section>
<citcontext>
<prevsection>
<prevsent>this in turn can form basis for more effective training in communication skills.the psychological medicine group at manchester (pmg), funded by the cancer research campaign (crc), is leading group in dialogue analysis in oncology.
</prevsent>
<prevsent>this paper describes the parameters and tagsets (analogous to dialogue act?
</prevsent>
</prevsection>
<citsent citstr=" J00-3003 ">
tagging (stolcke et al 2000)), <papid> J00-3003 </papid>which they and three other groups have developed for this highly specialised domain.</citsent>
<aftsection>
<nextsent>this domain offers an interesting contrast to the instructional?
</nextsent>
<nextsent>or service?
</nextsent>
<nextsent>dialogues commonly studied.
</nextsent>
<nextsent>the health professional is the expert?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y983">
<title id=" W01-0702.xml">combining a self organising map with memory based learning </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the method is tested on the identification of base noun-phrases in the wall street journal corpus, using sections 15 to 18 for training and section 20 for testing.
</prevsent>
<prevsent>currently, there is considerable interest in machine learning methods for corpus-based language learning.
</prevsent>
</prevsection>
<citsent citstr=" W99-0707 ">
a promising technique here is memory-based learning1 (mbl) (daelemans etal., 1999<papid> W99-0707 </papid>a), where task is re described as classification problem.</citsent>
<aftsection>
<nextsent>the classification is performed by matching an input item to the most similar ofa set of training items and choosing the most frequent classification of the closest item(s).
</nextsent>
<nextsent>similarity is computed using an explicit similarity metric.
</nextsent>
<nextsent>mbl performs well by bringing all the training data to bear on the task.
</nextsent>
<nextsent>this is done at the cost, in the worst case, of comparing novel items to all of the training items to find the closest match.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y984">
<title id=" W01-0702.xml">combining a self organising map with memory based learning </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this is done at the cost, in the worst case, of comparing novel items to all of the training items to find the closest match.
</prevsent>
<prevsent>thereis thus some interest in developing memory editing techniques to select subset of the items for comparison.this paper investigates whether self organising map (som) can be used to perform memory editing without reducing performance.
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
the system is tested on base noun-phrase (np) chunking using the wall street journal corpus (marcus et al , 1993).<papid> J93-2004 </papid></citsent>
<aftsection>
<nextsent>the som was developed originally by kohonen (1990) and has found wide range of uses from classification to storing lexicon.
</nextsent>
<nextsent>it operates as follows (see figure 1).
</nextsent>
<nextsent>the som consists of two layers, an input layer and the map.
</nextsent>
<nextsent>each unit in the map has vector of weights associated with it that is the same size as that of the input layer.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y986">
<title id=" W01-0702.xml">combining a self organising map with memory based learning </title>
<section> the self-organising map (som).  </section>
<citcontext>
<prevsection>
<prevsent>this system is referred to as sommbl.
</prevsent>
<prevsent>5 the task: base np chunking.
</prevsent>
</prevsection>
<citsent citstr=" W95-0107 ">
the task is base np chunking on section 20 of the wall street journal corpus, using sections 15 to 18 of the corpus as training data as in (ramshaw and marcus, 1995).<papid> W95-0107 </papid></citsent>
<aftsection>
<nextsent>for each word in sentence,the pos tag is presented to the system which outputs whether the word is inside or outside base np, or on the boundary between 2 base nps.
</nextsent>
<nextsent>training items consist of the part of speech (pos) tag for the current word, varying amounts of left and right context (pos tags only) and the classification frequencies for that combination oftags.
</nextsent>
<nextsent>the tags were represented by set of vectors.
</nextsent>
<nextsent>2 sets of vectors were used for comparison.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1003">
<title id=" W02-0402.xml">selecting sentences for multi document summaries using randomized local search </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in comparative evaluation against two duc-like baselines and three simpler versions of our system, we found that our randomized local search method provided substantial improvements in both content and intelligibility, while the use of the ie groups also appeared to contribute small further improvement in content.
</prevsent>
<prevsent>improving the intellibility of multi document summaries remains significant challenge.while most previous approaches to multi document summarization have addressed the problem of reducing repetition, less attention hasbeen paid to problems of coherence and cohesion.
</prevsent>
</prevsection>
<citsent citstr=" W00-0405 ">
in typical extractive system (e.g. goldstein et al (2000)), <papid> W00-0405 </papid>sentences are selected for inclusion in the summary one at time, with later choices sensitive to their similarity to earlier ones; the selected sentences are then ordered either chronologically or by relevance.the resulting summaries often jump incoherently from topic to topic, and contain broken cohesive links, such as dangling anaphors or unmet presuppositions.</citsent>
<aftsection>
<nextsent>barzilay et al (2001) <papid> H01-1065 </papid>present an improved method of ordering sentences in the context of multi gen, multi document summarizer that identifies sets of similar sentences, termed themes, and re formulates their common phrases as new text.</nextsent>
<nextsent>in their approach, topically related themes are identified and kept together in the resulting summary, in order to help improve cohesion and reduce topic switching.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1004">
<title id=" W02-0402.xml">selecting sentences for multi document summaries using randomized local search </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>improving the intellibility of multi document summaries remains significant challenge.while most previous approaches to multi document summarization have addressed the problem of reducing repetition, less attention hasbeen paid to problems of coherence and cohesion.
</prevsent>
<prevsent>in typical extractive system (e.g. goldstein et al (2000)), <papid> W00-0405 </papid>sentences are selected for inclusion in the summary one at time, with later choices sensitive to their similarity to earlier ones; the selected sentences are then ordered either chronologically or by relevance.the resulting summaries often jump incoherently from topic to topic, and contain broken cohesive links, such as dangling anaphors or unmet presuppositions.</prevsent>
</prevsection>
<citsent citstr=" H01-1065 ">
barzilay et al (2001) <papid> H01-1065 </papid>present an improved method of ordering sentences in the context of multi gen, multi document summarizer that identifies sets of similar sentences, termed themes, and re formulates their common phrases as new text.</citsent>
<aftsection>
<nextsent>in their approach, topically related themes are identified and kept together in the resulting summary, in order to help improve cohesion and reduce topic switching.
</nextsent>
<nextsent>in this paper, we pursue related but simpler idea in an extractive context, namely to favor the selection of blocks of adjacent sentences in constructing multi document summary.
</nextsent>
<nextsent>here,the challenge is to improve intelligibility without unduly sacrificing informativeness; for example, selecting the beginning of the most recent article in document set will usually produce highly intelligible text, but one that is not very representative of the document set as whole.
</nextsent>
<nextsent>to manage this tradeoff, we have developed randomized local search procedure (cf.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1005">
<title id=" W02-0402.xml">selecting sentences for multi document summaries using randomized local search </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>the use of the ie groups also appeared to contribute small further improvement in content when used with our selection search.
</prevsent>
<prevsent>we discuss these results in greater detail in the final section of the paper.
</prevsent>
</prevsection>
<citsent citstr=" H01-1054 ">
we have implemented our randomized local search method for sentence selection as partof the rip tides (white et al, 2001) <papid> H01-1054 </papid>system.</citsent>
<aftsection>
<nextsent>rip tides combines information extraction (ie) in the domain of natural disasters andmultidocument summarization to produce hypertext summaries.
</nextsent>
<nextsent>the hypertext summaries include high-level textual overview; tables of all comparable numeric estimates, organized to highlight discrepancies; and targeted access to supporting information from the original articles.
</nextsent>
<nextsent>in white et al (2002), we showed that the hypertext summaries can help to identify dis repancies in numeric estimates, and provide asignificantly more complete picture of the available information than the latest article.
</nextsent>
<nextsent>thenext subsection walks through sample hypertext summary; it is followed by descriptions of the ie and summarizer system components.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1006">
<title id=" W02-0402.xml">selecting sentences for multi document summaries using randomized local search </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>the tables also provide links to the original articles, allowing the user to quickly and directly determine the accuracy of any estimate in the table.
</prevsent>
<prevsent>2.2 ie system.
</prevsent>
</prevsection>
<citsent citstr=" A97-1029 ">
the ie system combines existing language technology components (bikel et al, 1997; <papid> A97-1029 </papid>charniak, 1999; day et al, 1997; <papid> A97-1051 </papid>fellbaum, 1998) ina traditional ie architecture (cardie, 1997; grishman, 1996).</citsent>
<aftsection>
<nextsent>unique features of the system include weakly supervised extraction pattern learning component, autoslog-xml, which isbased on autoslog-ts (riloff, 1996), but operates in an xml framework and acquires patterns for extracting text elements beyond noun phrases (e.g. verb groups, adjectives, adverbs, and single-noun modifiers).
</nextsent>
<nextsent>in addition, figure 1: hypertext summary overview figure 2: tables of death toll estimates heuristic-based clustering algorithm organizes the extracted concepts into output templates specifically designed to support multi-documentsummarization: the ie system, for example, distinguishes different reports or views of the same event from multiple sources (white et al, 2001).<papid> H01-1054 </papid></nextsent>
<nextsent>output templates from the ie system for each text to be covered in the multi-document summary are provided as input to the summarization component along with all linguistic annotations accrued in the ie phase.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1007">
<title id=" W02-0402.xml">selecting sentences for multi document summaries using randomized local search </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>the tables also provide links to the original articles, allowing the user to quickly and directly determine the accuracy of any estimate in the table.
</prevsent>
<prevsent>2.2 ie system.
</prevsent>
</prevsection>
<citsent citstr=" A97-1051 ">
the ie system combines existing language technology components (bikel et al, 1997; <papid> A97-1029 </papid>charniak, 1999; day et al, 1997; <papid> A97-1051 </papid>fellbaum, 1998) ina traditional ie architecture (cardie, 1997; grishman, 1996).</citsent>
<aftsection>
<nextsent>unique features of the system include weakly supervised extraction pattern learning component, autoslog-xml, which isbased on autoslog-ts (riloff, 1996), but operates in an xml framework and acquires patterns for extracting text elements beyond noun phrases (e.g. verb groups, adjectives, adverbs, and single-noun modifiers).
</nextsent>
<nextsent>in addition, figure 1: hypertext summary overview figure 2: tables of death toll estimates heuristic-based clustering algorithm organizes the extracted concepts into output templates specifically designed to support multi-documentsummarization: the ie system, for example, distinguishes different reports or views of the same event from multiple sources (white et al, 2001).<papid> H01-1054 </papid></nextsent>
<nextsent>output templates from the ie system for each text to be covered in the multi-document summary are provided as input to the summarization component along with all linguistic annotations accrued in the ie phase.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1013">
<title id=" W00-0405.xml">multi document summarization by sentence extraction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>i however, large- scale ir and summarization have not yet been truly in-tegrated, and the functionality challenges on summa-rization system are greater in true ir or topic-detection context (yang et al, 1998; allan et al, 1998).
</prevsent>
<prevsent>consider the situation where the user issues search query, for instance on news topic, and the retrieval sys-tem finds hundreds of closely-ranked documents in re-sponse.
</prevsent>
</prevsection>
<citsent citstr=" P95-1053 ">
many of these documents are likely to repeat much the same information, while differing in certain most of these were based on statistical techniques applied tovar- ious document entities; examples include frait, 1983; kupiec et al, 1995; paice, 1990, klavans and shaw, 1995; <papid> P95-1053 </papid>mekeown et al, 1995; shaw, 1995; <papid> P95-1053 </papid>aon?</citsent>
<aftsection>
<nextsent>et al, 1997; boguraev and kennedy, 1997; hovy and lin, 1997; <papid> W97-0704 </papid>mitra et al, 1997; <papid> W97-0707 </papid>teufel and moens, 1997; barzilay and elhadad, 1997; <papid> W97-0703 </papid>carbonell and goldstein, 1998; baldwin and mor- tbn, 1998; radev and mckeown, 1998; <papid> J98-3005 </papid>strzalkowski etal., 1998).</nextsent>
<nextsent>parts.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1019">
<title id=" W00-0405.xml">multi document summarization by sentence extraction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>consider the situation where the user issues search query, for instance on news topic, and the retrieval sys-tem finds hundreds of closely-ranked documents in re-sponse.
</prevsent>
<prevsent>many of these documents are likely to repeat much the same information, while differing in certain most of these were based on statistical techniques applied tovar- ious document entities; examples include frait, 1983; kupiec et al, 1995; paice, 1990, klavans and shaw, 1995; <papid> P95-1053 </papid>mekeown et al, 1995; shaw, 1995; <papid> P95-1053 </papid>aon?</prevsent>
</prevsection>
<citsent citstr=" W97-0704 ">
et al, 1997; boguraev and kennedy, 1997; hovy and lin, 1997; <papid> W97-0704 </papid>mitra et al, 1997; <papid> W97-0707 </papid>teufel and moens, 1997; barzilay and elhadad, 1997; <papid> W97-0703 </papid>carbonell and goldstein, 1998; baldwin and mor- tbn, 1998; radev and mckeown, 1998; <papid> J98-3005 </papid>strzalkowski etal., 1998).</citsent>
<aftsection>
<nextsent>parts.
</nextsent>
<nextsent>summaries of the individual documents would help, but are likely to be very similar to each other, un-less the summarization system takes into account other summaries that have already been generated.
</nextsent>
<nextsent>multi- document summarization - capable of summarizing ei-ther complete documents sets, or single documents in the context of previously summarized ones - are likely to be essential in such situations.
</nextsent>
<nextsent>ideally, multi-document summaries should contain the key shared relevant infor-mation among all the documents only once, plus other information unique to some of the individual documents that are directly relevant to the user query.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1020">
<title id=" W00-0405.xml">multi document summarization by sentence extraction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>consider the situation where the user issues search query, for instance on news topic, and the retrieval sys-tem finds hundreds of closely-ranked documents in re-sponse.
</prevsent>
<prevsent>many of these documents are likely to repeat much the same information, while differing in certain most of these were based on statistical techniques applied tovar- ious document entities; examples include frait, 1983; kupiec et al, 1995; paice, 1990, klavans and shaw, 1995; <papid> P95-1053 </papid>mekeown et al, 1995; shaw, 1995; <papid> P95-1053 </papid>aon?</prevsent>
</prevsection>
<citsent citstr=" W97-0707 ">
et al, 1997; boguraev and kennedy, 1997; hovy and lin, 1997; <papid> W97-0704 </papid>mitra et al, 1997; <papid> W97-0707 </papid>teufel and moens, 1997; barzilay and elhadad, 1997; <papid> W97-0703 </papid>carbonell and goldstein, 1998; baldwin and mor- tbn, 1998; radev and mckeown, 1998; <papid> J98-3005 </papid>strzalkowski etal., 1998).</citsent>
<aftsection>
<nextsent>parts.
</nextsent>
<nextsent>summaries of the individual documents would help, but are likely to be very similar to each other, un-less the summarization system takes into account other summaries that have already been generated.
</nextsent>
<nextsent>multi- document summarization - capable of summarizing ei-ther complete documents sets, or single documents in the context of previously summarized ones - are likely to be essential in such situations.
</nextsent>
<nextsent>ideally, multi-document summaries should contain the key shared relevant infor-mation among all the documents only once, plus other information unique to some of the individual documents that are directly relevant to the user query.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1021">
<title id=" W00-0405.xml">multi document summarization by sentence extraction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>consider the situation where the user issues search query, for instance on news topic, and the retrieval sys-tem finds hundreds of closely-ranked documents in re-sponse.
</prevsent>
<prevsent>many of these documents are likely to repeat much the same information, while differing in certain most of these were based on statistical techniques applied tovar- ious document entities; examples include frait, 1983; kupiec et al, 1995; paice, 1990, klavans and shaw, 1995; <papid> P95-1053 </papid>mekeown et al, 1995; shaw, 1995; <papid> P95-1053 </papid>aon?</prevsent>
</prevsection>
<citsent citstr=" W97-0703 ">
et al, 1997; boguraev and kennedy, 1997; hovy and lin, 1997; <papid> W97-0704 </papid>mitra et al, 1997; <papid> W97-0707 </papid>teufel and moens, 1997; barzilay and elhadad, 1997; <papid> W97-0703 </papid>carbonell and goldstein, 1998; baldwin and mor- tbn, 1998; radev and mckeown, 1998; <papid> J98-3005 </papid>strzalkowski etal., 1998).</citsent>
<aftsection>
<nextsent>parts.
</nextsent>
<nextsent>summaries of the individual documents would help, but are likely to be very similar to each other, un-less the summarization system takes into account other summaries that have already been generated.
</nextsent>
<nextsent>multi- document summarization - capable of summarizing ei-ther complete documents sets, or single documents in the context of previously summarized ones - are likely to be essential in such situations.
</nextsent>
<nextsent>ideally, multi-document summaries should contain the key shared relevant infor-mation among all the documents only once, plus other information unique to some of the individual documents that are directly relevant to the user query.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1023">
<title id=" W00-0405.xml">multi document summarization by sentence extraction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>consider the situation where the user issues search query, for instance on news topic, and the retrieval sys-tem finds hundreds of closely-ranked documents in re-sponse.
</prevsent>
<prevsent>many of these documents are likely to repeat much the same information, while differing in certain most of these were based on statistical techniques applied tovar- ious document entities; examples include frait, 1983; kupiec et al, 1995; paice, 1990, klavans and shaw, 1995; <papid> P95-1053 </papid>mekeown et al, 1995; shaw, 1995; <papid> P95-1053 </papid>aon?</prevsent>
</prevsection>
<citsent citstr=" J98-3005 ">
et al, 1997; boguraev and kennedy, 1997; hovy and lin, 1997; <papid> W97-0704 </papid>mitra et al, 1997; <papid> W97-0707 </papid>teufel and moens, 1997; barzilay and elhadad, 1997; <papid> W97-0703 </papid>carbonell and goldstein, 1998; baldwin and mor- tbn, 1998; radev and mckeown, 1998; <papid> J98-3005 </papid>strzalkowski etal., 1998).</citsent>
<aftsection>
<nextsent>parts.
</nextsent>
<nextsent>summaries of the individual documents would help, but are likely to be very similar to each other, un-less the summarization system takes into account other summaries that have already been generated.
</nextsent>
<nextsent>multi- document summarization - capable of summarizing ei-ther complete documents sets, or single documents in the context of previously summarized ones - are likely to be essential in such situations.
</nextsent>
<nextsent>ideally, multi-document summaries should contain the key shared relevant infor-mation among all the documents only once, plus other information unique to some of the individual documents that are directly relevant to the user query.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1025">
<title id=" W00-0405.xml">multi document summarization by sentence extraction </title>
<section> background and related work.  </section>
<citcontext>
<prevsection>
<prevsent>with text-span deletion the system attempts delete  less im-portant  spans of text from the original document; the text that remains is deemed summary.
</prevsent>
<prevsent>work on auto-mated document summarization by text span extraction dates back at least to work at ibm in the fifties (luhn, 1958).
</prevsent>
</prevsection>
<citsent citstr=" W97-0713 ">
most of the work in sentence xtraction applied statistical techniques (frequency analysis, variance anal-ysis, etc.) to linguistic units such as tokens, names, anaphora, etc. more recently, other approaches have investigated the utility of discourse structure (marcu, 1997), <papid> W97-0713 </papid>the combination of information extraction and language generation (klavans and shaw, 1995; <papid> P95-1053 </papid>mcke-own et al, 1995), and using machine learning to find patterns in text (teufel and moens, 1997; barzilay and elhadad, 1997; <papid> W97-0703 </papid>strzalkowski et al, 1998).</citsent>
<aftsection>
<nextsent>some of these approaches to single document summa-rization have been extended to deal with multi-document summarization (mani and bloedern, 1997; goldstein and carbonell, 1998; tipster, 1998b; radev and mcke-own, 1998; mani and bloedorn, 1999; mckeown et al, .!999; stein et al, 1999).
</nextsent>
<nextsent>these include comparing tem-plates filled in by extracting information - using special-ized, domain specific knowledge sources - from the doc-  ument, and then generating natural language summaries from the templates (radev and mckeown, 1998), <papid> J98-3005 </papid>com-- ? paring named-entities - extracted using specialized lists - between documents and selecting the most relevant section (tipster, 1998b), finding co-reference chains in the document set to identify common sections of inter-est (tipster, 1998b), or building activation etworks of related lexical items (identity mappings, synonyms, hypernyms, etc.) to extract text spans from the document set (mani and bloedern, 1997).</nextsent>
<nextsent>another system (stein et al., 1999) creates multi-document summary from mul-tiple single document summaries, an approach that can be sub-optimal in some cases, due to the fact that the process of generating the final multi-document summary takes as input he individual summaries and not the com-plete documents.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1034">
<title id=" W00-0201.xml">an interlingualbased approach to reference resolution </title>
<section> background </section>
<citcontext>
<prevsection>
<prevsent>since the system itself is in the early stages of development, descriptions of performance are projected rather than actual.
</prevsent>
<prevsent>current approaches to reference resolution focus on surface forms (see sundheim &amp; grishman, 1995, and muc-7, 1998, for general introduction).
</prevsent>
</prevsection>
<citsent citstr=" P98-1012 ">
they include string match algorithms for proper noun phrases (pns) and common oun phrases (nps) (e.g., bagga &amp; baldwin, 1998) <papid> P98-1012 </papid>and syntactically constrained morphological matching for pronominals and deictic nps (lappin &amp; leass, 1994).<papid> J94-4002 </papid></citsent>
<aftsection>
<nextsent>in some cases, they include constraints related to clause structure or larger textual units (e.g., grosz, joshi, and weinstein, 1995).
</nextsent>
<nextsent>1 dod contract 66001-99-1-8915.
</nextsent>
<nextsent>they do not generally consider implicit references or, in the case of spanish (e.g., ferrhndez et al 1998), references to contextually clear possessors using determiners rather than possessive adjectives.
</nextsent>
<nextsent>all such approaches are supplemented, if not entirely determined, by heuristics which, more recently, have been induced statistically from corpora (e.g., hirsehman et al 1998, popescu- belis, 1998).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1035">
<title id=" W00-0201.xml">an interlingualbased approach to reference resolution </title>
<section> background </section>
<citcontext>
<prevsection>
<prevsent>since the system itself is in the early stages of development, descriptions of performance are projected rather than actual.
</prevsent>
<prevsent>current approaches to reference resolution focus on surface forms (see sundheim &amp; grishman, 1995, and muc-7, 1998, for general introduction).
</prevsent>
</prevsection>
<citsent citstr=" J94-4002 ">
they include string match algorithms for proper noun phrases (pns) and common oun phrases (nps) (e.g., bagga &amp; baldwin, 1998) <papid> P98-1012 </papid>and syntactically constrained morphological matching for pronominals and deictic nps (lappin &amp; leass, 1994).<papid> J94-4002 </papid></citsent>
<aftsection>
<nextsent>in some cases, they include constraints related to clause structure or larger textual units (e.g., grosz, joshi, and weinstein, 1995).
</nextsent>
<nextsent>1 dod contract 66001-99-1-8915.
</nextsent>
<nextsent>they do not generally consider implicit references or, in the case of spanish (e.g., ferrhndez et al 1998), references to contextually clear possessors using determiners rather than possessive adjectives.
</nextsent>
<nextsent>all such approaches are supplemented, if not entirely determined, by heuristics which, more recently, have been induced statistically from corpora (e.g., hirsehman et al 1998, popescu- belis, 1998).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1036">
<title id=" W01-0805.xml">a meta algorithm for the generation of referring expressions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the resulting graph algorithm can be seen as meta-algorithm in the sense that defining cost functions in different ways allows us to mimic and even improve?
</prevsent>
<prevsent>a number of well known algorithms.
</prevsent>
</prevsection>
<citsent citstr=" P90-1013 ">
the generation of referring expressions is one of the most common tasks in natural language generation, and has been addressed by many researchers in the past two decades (including appelt 1985, dale 1992, reiter 1990, <papid> P90-1013 </papid>dale &amp; haddock 1991, <papid> E91-1028 </papid>dale &amp; reiter 1995, horacek 1997, <papid> P97-1027 </papid>stone &amp; webber 1998, <papid> W98-1419 </papid>krahmer &amp; theune 1999 and van deemter 2000).</citsent>
<aftsection>
<nextsent>as result, there are many different algorithms for the generation of referring expressions, each with its own object ives: some aim at producing the shortest possible description, others focus on efficiency or realistic output.
</nextsent>
<nextsent>the degree of detail in which the various algorithms are described differs considerably, andas result it is often difficult to compare the various proposals.
</nextsent>
<nextsent>in addition, most of the algorithms are primarily concerned with the generation of descriptions only using properties of the target object.
</nextsent>
<nextsent>consequently, the problem of generating relational descriptions (i.e., descriptions which incorporate references to other objects to single out the target object) has not received the attention it deserves.in this paper, we describe general, graph theoretic approach to the generation of referring expressions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1037">
<title id=" W01-0805.xml">a meta algorithm for the generation of referring expressions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the resulting graph algorithm can be seen as meta-algorithm in the sense that defining cost functions in different ways allows us to mimic and even improve?
</prevsent>
<prevsent>a number of well known algorithms.
</prevsent>
</prevsection>
<citsent citstr=" E91-1028 ">
the generation of referring expressions is one of the most common tasks in natural language generation, and has been addressed by many researchers in the past two decades (including appelt 1985, dale 1992, reiter 1990, <papid> P90-1013 </papid>dale &amp; haddock 1991, <papid> E91-1028 </papid>dale &amp; reiter 1995, horacek 1997, <papid> P97-1027 </papid>stone &amp; webber 1998, <papid> W98-1419 </papid>krahmer &amp; theune 1999 and van deemter 2000).</citsent>
<aftsection>
<nextsent>as result, there are many different algorithms for the generation of referring expressions, each with its own object ives: some aim at producing the shortest possible description, others focus on efficiency or realistic output.
</nextsent>
<nextsent>the degree of detail in which the various algorithms are described differs considerably, andas result it is often difficult to compare the various proposals.
</nextsent>
<nextsent>in addition, most of the algorithms are primarily concerned with the generation of descriptions only using properties of the target object.
</nextsent>
<nextsent>consequently, the problem of generating relational descriptions (i.e., descriptions which incorporate references to other objects to single out the target object) has not received the attention it deserves.in this paper, we describe general, graph theoretic approach to the generation of referring expressions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1038">
<title id=" W01-0805.xml">a meta algorithm for the generation of referring expressions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the resulting graph algorithm can be seen as meta-algorithm in the sense that defining cost functions in different ways allows us to mimic and even improve?
</prevsent>
<prevsent>a number of well known algorithms.
</prevsent>
</prevsection>
<citsent citstr=" P97-1027 ">
the generation of referring expressions is one of the most common tasks in natural language generation, and has been addressed by many researchers in the past two decades (including appelt 1985, dale 1992, reiter 1990, <papid> P90-1013 </papid>dale &amp; haddock 1991, <papid> E91-1028 </papid>dale &amp; reiter 1995, horacek 1997, <papid> P97-1027 </papid>stone &amp; webber 1998, <papid> W98-1419 </papid>krahmer &amp; theune 1999 and van deemter 2000).</citsent>
<aftsection>
<nextsent>as result, there are many different algorithms for the generation of referring expressions, each with its own object ives: some aim at producing the shortest possible description, others focus on efficiency or realistic output.
</nextsent>
<nextsent>the degree of detail in which the various algorithms are described differs considerably, andas result it is often difficult to compare the various proposals.
</nextsent>
<nextsent>in addition, most of the algorithms are primarily concerned with the generation of descriptions only using properties of the target object.
</nextsent>
<nextsent>consequently, the problem of generating relational descriptions (i.e., descriptions which incorporate references to other objects to single out the target object) has not received the attention it deserves.in this paper, we describe general, graph theoretic approach to the generation of referring expressions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1039">
<title id=" W01-0805.xml">a meta algorithm for the generation of referring expressions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the resulting graph algorithm can be seen as meta-algorithm in the sense that defining cost functions in different ways allows us to mimic and even improve?
</prevsent>
<prevsent>a number of well known algorithms.
</prevsent>
</prevsection>
<citsent citstr=" W98-1419 ">
the generation of referring expressions is one of the most common tasks in natural language generation, and has been addressed by many researchers in the past two decades (including appelt 1985, dale 1992, reiter 1990, <papid> P90-1013 </papid>dale &amp; haddock 1991, <papid> E91-1028 </papid>dale &amp; reiter 1995, horacek 1997, <papid> P97-1027 </papid>stone &amp; webber 1998, <papid> W98-1419 </papid>krahmer &amp; theune 1999 and van deemter 2000).</citsent>
<aftsection>
<nextsent>as result, there are many different algorithms for the generation of referring expressions, each with its own object ives: some aim at producing the shortest possible description, others focus on efficiency or realistic output.
</nextsent>
<nextsent>the degree of detail in which the various algorithms are described differs considerably, andas result it is often difficult to compare the various proposals.
</nextsent>
<nextsent>in addition, most of the algorithms are primarily concerned with the generation of descriptions only using properties of the target object.
</nextsent>
<nextsent>consequently, the problem of generating relational descriptions (i.e., descriptions which incorporate references to other objects to single out the target object) has not received the attention it deserves.in this paper, we describe general, graph theoretic approach to the generation of referring expressions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1040">
<title id=" W01-0805.xml">a meta algorithm for the generation of referring expressions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>arguably, the proposed algorithm is meta algorithm, in the sense that by defining the cost function in different ways, we can mimic variouswell-known algorithms for the generation of referring expressions.
</prevsent>
<prevsent>a second advantage of the graph-theoretical framework is that it does not run into problems with relational descriptions, due tothe fact that properties and relations are formalized in the same way, namely as edges in graph.
</prevsent>
</prevsection>
<citsent citstr=" W98-1426 ">
the third advantage is that the combined usage of graphs and cost-functions paves the way fora natural integration of traditional rule-based approaches to generation with more recent statistical approaches (e.g., langkilde &amp; knight 1998, <papid> W98-1426 </papid>malouf 2000) <papid> P00-1012 </papid>in single algorithm.the outline of this paper is as follows.</citsent>
<aftsection>
<nextsent>in section 2, we describe how scenes can be described as labeled directed graphs and show how content selection can be formalized as subgraph construction problem.
</nextsent>
<nextsent>section 3 contains sketch of the branch and bound algorithm, which is illustrated with worked example.
</nextsent>
<nextsent>in section 4 it is argued that by defining cost functions in different ways, we can mimic various well-known algorithms for the generation of referring expressions.
</nextsent>
<nextsent>we end with some concluding remarks in section 5.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1042">
<title id=" W01-0805.xml">a meta algorithm for the generation of referring expressions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>arguably, the proposed algorithm is meta algorithm, in the sense that by defining the cost function in different ways, we can mimic variouswell-known algorithms for the generation of referring expressions.
</prevsent>
<prevsent>a second advantage of the graph-theoretical framework is that it does not run into problems with relational descriptions, due tothe fact that properties and relations are formalized in the same way, namely as edges in graph.
</prevsent>
</prevsection>
<citsent citstr=" P00-1012 ">
the third advantage is that the combined usage of graphs and cost-functions paves the way fora natural integration of traditional rule-based approaches to generation with more recent statistical approaches (e.g., langkilde &amp; knight 1998, <papid> W98-1426 </papid>malouf 2000) <papid> P00-1012 </papid>in single algorithm.the outline of this paper is as follows.</citsent>
<aftsection>
<nextsent>in section 2, we describe how scenes can be described as labeled directed graphs and show how content selection can be formalized as subgraph construction problem.
</nextsent>
<nextsent>section 3 contains sketch of the branch and bound algorithm, which is illustrated with worked example.
</nextsent>
<nextsent>in section 4 it is argued that by defining cost functions in different ways, we can mimic various well-known algorithms for the generation of referring expressions.
</nextsent>
<nextsent>we end with some concluding remarks in section 5.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1048">
<title id=" W01-0805.xml">a meta algorithm for the generation of referring expressions </title>
<section> concluding remarks.  </section>
<citcontext>
<prevsection>
<prevsent>krahmer &amp; theune(1999) present an extension of the incremental algorithm which takes context into account.
</prevsent>
<prevsent>they argue that an object which has been mentioned in the recent context is somehow salient, and hence can be referred to using fewer properties.
</prevsent>
</prevsection>
<citsent citstr=" J95-2003 ">
thisis modelled by assigning salience weights to objects (basically using version of centering theory (grosz et al 1995) <papid> J95-2003 </papid>augmented with recency effect), and by defining the set of dis tractors as the set of objects with salience weight higher or equal than that of the target object.</citsent>
<aftsection>
<nextsent>in terms of thegraph-theoretical framework, one can easily imagine assigning salience weights to the nodes in the scene graph, and restricting the dis tractor set essentially as krahmer &amp; theune do.
</nextsent>
<nextsent>in this way,distinguishing graphs for salient objects will generally be smaller than those of non-salient objects.
</nextsent>
<nextsent>acknowledgements thanks are due to alexander koller, kees van deemter, paul piwek, mariet theune and two anonymous referees for discussions and comments on an earlier version of this paper.
</nextsent>

</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1049">
<title id=" W02-0226.xml">bridging the gap between dialogue management and dialogue models </title>
<section> the gap.  </section>
<citcontext>
<prevsection>
<prevsent>so his latter two correspond to our plan based approach in general.patten-based approach models recurrent interaction patterns or regularities in dialogues at the illo cutionary force level of speech acts (austin, 1962;searle, 1969) in terms of dialogue grammar (sin clair and coulthard, 1975), dialogue/conversational game (carlson, 1983; kowtko et al, 1992; mann, 2001), or adjacency pairs (sacks et al, 1974).
</prevsent>
<prevsent>it benefits lot from the insights of discourse analysis (sinclair and coulthard, 1975; coulthard, 1992; brown and yule, 1983) and conversation analysis (levinson, 1983).plan-based approach relates speech acts performed in utterances to plans and complex mental states (cohen and perrault, 1979; allen and perrault,1980; lochbaum et al, 2000) and uses ai planning techniques (fikes and nilsson, 1971).
</prevsent>
</prevsection>
<citsent citstr=" P91-1007 ">
later developments of plan-based dialogue models include multilevel plan extension (litman and allen, 1987; litman and allen, 1990; carberry, 1990; lambert and carberry, 1991), <papid> P91-1007 </papid>theories of joint action (co hen and levesque, 1991) and shared plan (grosz and sidner, 1990; grosz and kraus, 1996).pattern-based dialogue model describes what happens in dialogues at the speech act level and cares little about why.</citsent>
<aftsection>
<nextsent>plan-based dialogue model explains why agents act in dialogues, but at the expense of complex representation and reasoning.
</nextsent>
<nextsent>inother words, the former is shallow and descriptive and the latter is deep and explanatory.
</nextsent>
<nextsent>hul stijn (2000) argues for the complementary aspects of the two approaches and claims that dialogue games are recipes for joint action?.
</nextsent>
<nextsent>since, on the one hand, our target tasks belong to the class of simple service, like information-seekingand simple transactions, which are relatively well structured and well-defined and not too complex for pattern-based dialogue models, on the other hand,there are some significant problems in using plan based models in practical sdss ? those of knowledge representation, knowledge engineering, computational complexity, and noisy input?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1050">
<title id=" W02-0226.xml">bridging the gap between dialogue management and dialogue models </title>
<section> the gap.  </section>
<citcontext>
<prevsection>
<prevsent>it is neither flexible nor natural, but simple and efficient.
</prevsent>
<prevsent>its suitable for simple and well-structured tasks similar to automated services over atms or telephones with dtmf input.
</prevsent>
</prevsection>
<citsent citstr=" W00-0309 ">
dite or frame-based, with no explicit dialogue model, but task is explicitly represented as frame or form (goddeau et al, 1996), task description table (lin et al, 1998), topic forest (wu et al, 2000), or an agenda (xu and rudnicky, 2000), <papid> W00-0309 </papid>etc. both system and usermay take the initiative.</citsent>
<aftsection>
<nextsent>topic flow is not predetermined.
</nextsent>
<nextsent>its more flexible than that of diti, but still far from naturalness and friendliness,since it makes no explicit use of dialogue models.
</nextsent>
<nextsent>most working sdss adopt this way of dialogue management.
</nextsent>
<nextsent>deti there is no practical dialogue management3for more comprehensive discussion on dialogue management (and sdss), see (mctear, 2002).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1051">
<title id=" W02-0226.xml">bridging the gap between dialogue management and dialogue models </title>
<section> the gap.  </section>
<citcontext>
<prevsection>
<prevsent>2.3 the causes behind the gap.
</prevsent>
<prevsent>from the analysis above we can see the surface gap between (dite) dialogue management in most working sdss and (pattern-based) dialogue models is mainly due to deep one, i.e., the one between dialogue models and the underlying tasks.
</prevsent>
</prevsection>
<citsent citstr=" J97-1002 ">
there is another important cause ? the interaction patterns are described at the level of speech act or dialogue act.4 to link dialogue acts to utterances, three problems5 must be addressed at the same time:  dialogue act classification scheme and its reliability in coding corpus, (carletta et al, 1997; <papid> J97-1002 </papid>allen and core, 1997; traum, 1999);  choice of features/cues that can support automatic dialogue act identification, including lexical, syntactic, prosodic, collocational, and discourse cues;   model that correlates dialogue acts with those features.</citsent>
<aftsection>
<nextsent>some of the problems are discussed in (jurafsky et al., 1998; stolcke et al, 2000; <papid> J00-3003 </papid>jurafsky and martin,2000; jurafsky, 2002).</nextsent>
<nextsent>the empirical work on dialogue act classification and recognition did not begin until some dialogue corpora (like map task, verb mobil, trains, and our nlpr-ti) were available.but how could dialogue act recognition be successfully applied to practical dialogue management remains to be seen.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1053">
<title id=" W02-0226.xml">bridging the gap between dialogue management and dialogue models </title>
<section> the gap.  </section>
<citcontext>
<prevsection>
<prevsent>from the analysis above we can see the surface gap between (dite) dialogue management in most working sdss and (pattern-based) dialogue models is mainly due to deep one, i.e., the one between dialogue models and the underlying tasks.
</prevsent>
<prevsent>there is another important cause ? the interaction patterns are described at the level of speech act or dialogue act.4 to link dialogue acts to utterances, three problems5 must be addressed at the same time:  dialogue act classification scheme and its reliability in coding corpus, (carletta et al, 1997; <papid> J97-1002 </papid>allen and core, 1997; traum, 1999);  choice of features/cues that can support automatic dialogue act identification, including lexical, syntactic, prosodic, collocational, and discourse cues;   model that correlates dialogue acts with those features.</prevsent>
</prevsection>
<citsent citstr=" J00-3003 ">
some of the problems are discussed in (jurafsky et al., 1998; stolcke et al, 2000; <papid> J00-3003 </papid>jurafsky and martin,2000; jurafsky, 2002).</citsent>
<aftsection>
<nextsent>the empirical work on dialogue act classification and recognition did not begin until some dialogue corpora (like map task, verb mobil, trains, and our nlpr-ti) were available.but how could dialogue act recognition be successfully applied to practical dialogue management remains to be seen.
</nextsent>
<nextsent>so we choose higher level4following jurafsky (2002), we will adopt the term dialogue act, which captures the illocutionary force or commuca tive function of speech act.
</nextsent>
<nextsent>though there are some arguments in (levinson, 1983) and others against using dialogue act tomodel dialogues, and there are indeed some unresolved problems in linking dialogue acts to utterances, it will be our choice for the time being.
</nextsent>
<nextsent>5we extend webbers (2001) idea by splitting feature choice out.construct (ut-3, see section 3.1.3) to describe interaction patterns instead.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1054">
<title id=" W02-0226.xml">bridging the gap between dialogue management and dialogue models </title>
<section> the bridge ? gdm.  </section>
<citcontext>
<prevsection>
<prevsent>we first refine the concept of dialogue strategies.
</prevsent>
<prevsent>from the view of gdm, the strategies dialogue agent may choose can also be classified into three levels, i.e., micro-level strategies how to realize information structure, anaphora, ellipsis, and others, in utterances, meso-level strategies what to say regarding current group status, so as to complete ongoing group more friendly, macro-level strategies how to choose discourse topic regarding current task status, so as to complete the underlying task more efficiently.
</prevsent>
</prevsection>
<citsent citstr=" J86-3001 ">
6grosz and sidner (1986) <papid> J86-3001 </papid>proposed tripartite discourse model consisting of attentional state, intentional structure, and linguistic structure.</citsent>
<aftsection>
<nextsent>it is influential and covers both dialogue and text.
</nextsent>
<nextsent>but their intentional structure fails to capture the distinction between global level and local level structure.
</nextsent>
<nextsent>their discourse unit ? discourse segment ? is used without noticing that there are different ranks of discourse unit in dialogues.
</nextsent>
<nextsent>this is partly due to that they looked more at the similarities between dialogue and text and less at the differences between them.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1055">
<title id=" W02-0226.xml">bridging the gap between dialogue management and dialogue models </title>
<section> utterance groups in gdm-is.  </section>
<citcontext>
<prevsection>
<prevsent>4.2.2 manual segmentation subject was given the basic ideas about gdm and utterance groups in gdm-is and segmented two dialogues with an experts guide before starting the work.
</prevsent>
<prevsent>to test the reliability of group segmentation within gdm-is, we calculate the kappa coefficient(
</prevsent>
</prevsection>
<citsent citstr=" J96-2004 ">
)8 (carletta, 1996; <papid> J96-2004 </papid>carletta et al, 1997; <papid> J97-1002 </papid>flam mia, 1998) to measure pairwise agreement between the subject and the expert.</citsent>
<aftsection>
<nextsent>two coders segmented the first 20 dialogues (totally 845 utterances).
</nextsent>
<nextsent>theyreached
</nextsent>
<nextsent>, which shows high reliability.
</nextsent>
<nextsent>using the experts segmentation as reference, wealso measure the subjects segmentation using information retrieval metrics ? precision (p), recall (r), and f-measure9 (see table 3 for the result).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1059">
<title id=" W02-0226.xml">bridging the gap between dialogue management and dialogue models </title>
<section> utterance groups in gdm-is.  </section>
<citcontext>
<prevsection>
<prevsent>from (carletta, 1996) <papid> J96-2004 </papid>9combined metric   </prevsent>
<prevsent> , from (jurafsky and martin, 2000, p.578),    .10that we adopt such deep features in discourse segmentation is mainly due to our target application ? dialogue manage ment.</prevsent>
</prevsection>
<citsent citstr=" J97-1005 ">
this makes it different from others using surface features like (passonneau and litman, 1997).<papid> J97-1005 </papid></citsent>
<aftsection>
<nextsent>i. using topic only for segmentation if topic is new then ut-3 = initiative else ut-3 = non-initiative ii.
</nextsent>
<nextsent>using ut-1 only for segmentation if ut-1
</nextsent>
<nextsent>interrogatives then ut-3 = initiative else ut-3 = non-initiative iii.
</nextsent>
<nextsent>using both for segmentation if topic is new  ut-1
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1060">
<title id=" W01-1612.xml">annotating anaphoric and bridging relations with mmax </title>
<section> definition.  </section>
<citcontext>
<prevsection>
<prevsent>in general, anaphoric as well as bridging relations hold between specifying expressions.
</prevsent>
<prevsent>these are those expressions that specify (i.e. are used to refer to) particular extra-linguistic entity.
</prevsent>
</prevsection>
<citsent citstr=" J00-4003 ">
in what follows, we briefly discuss the approach of (vieira &amp; poesio, 2000)<papid> J00-4003 </papid>and present our own definition.</citsent>
<aftsection>
<nextsent>since (vieira &amp; poe sio, 2000) <papid> J00-4003 </papid>address the problem of bridging annotation, they try to find an operational and easily applicable definition.</nextsent>
<nextsent>this is the main motivation for choosing (vieira &amp; poesio, 2000)<papid> J00-4003 </papid> (and not e.g.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1063">
<title id=" W01-1612.xml">annotating anaphoric and bridging relations with mmax </title>
<section> definition.  </section>
<citcontext>
<prevsection>
<prevsent>since (vieira &amp; poe sio, 2000) <papid> J00-4003 </papid>address the problem of bridging annotation, they try to find an operational and easily applicable definition.</prevsent>
<prevsent>this is the main motivation for choosing (vieira &amp; poesio, 2000)<papid> J00-4003 </papid> (and not e.g.</prevsent>
</prevsection>
<citsent citstr=" T75-2034 ">
(clark, 1975), <papid> T75-2034 </papid>who introduced the term bridging) as the background of our discussion.</citsent>
<aftsection>
<nextsent>in the following discussion, two features of pairs of specifying expressions will be important.
</nextsent>
<nextsent>the first one is co specification (sidner, 1983), also known as coreference, relation holding between two or more specifying expressions which specify thesame extra-linguistic entity.
</nextsent>
<nextsent>the second important feature is identity of the head noun.
</nextsent>
<nextsent>this feature is applicable to full nps only and simply states that in pair of nps the head of each is realized by the same noun.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1067">
<title id=" W01-1612.xml">annotating anaphoric and bridging relations with mmax </title>
<section> annotation scheme.  </section>
<citcontext>
<prevsection>
<prevsent>by separating the annotation of anaphoric relations in this way, the concept antecedent becomes free to be used only in those cases where it is both relevant and unambiguously decidable.
</prevsent>
<prevsent>it is important to note that no relevant information appears to be lost here: supplied that the linear order of markableswithin the text is preserved, it should be possible to establish an antecedent to any anaphoric expression from set of co specifying expressions annotated within the scheme described above.
</prevsent>
</prevsection>
<citsent citstr=" M95-1005 ">
moreover, the important task of evaluating the annotation scheme is not affected either, because common evaluation algorithms for anaphor annotations (vilain et al, 1995) <papid> M95-1005 </papid>do not depend on antecedence information, but treat anaphoric expressions as co specifying equivalence classes.</citsent>
<aftsection>
<nextsent>what is even more important is that by the same means we can render optional the explicit specification of bridging antecedents as well.
</nextsent>
<nextsent>two cases can be distinguished here: whenever only single candidate for antecedence exists, specifying it is trivial.
</nextsent>
<nextsent>thus,the only cases where uncertainty as to the correct antecedent of bridging expression can arise appear to be those in which multiple co specifying candidates are available.
</nextsent>
<nextsent>since bridging (as we define it) is relation not between lexical items, but between extra-linguisticentities, and since co specification is transitive relation, bridging relation can be sufficiently expressed by specifying any of the candidates.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1068">
<title id=" W01-1612.xml">annotating anaphoric and bridging relations with mmax </title>
<section> annotation tool.  </section>
<citcontext>
<prevsection>
<prevsent>the tool (like the other two mentioned before) processes sgml files, into which annotation tags are inserted directly during annotation.
</prevsent>
<prevsent>we regard this approach to annotation as drawback,because it mixes the basic data (i.e. the texts to be an notated) with the annotation itself.
</prevsent>
</prevsection>
<citsent citstr=" W99-0107 ">
this can give riseto problems, e.g. in cases where alternative annotations of the same data are to be compared.referee, tcl/tk program for coreference annotation (decristofaro et al, 1999), <papid> W99-0107 </papid>is better in this res pectin that it writes the annotations to separate file, leaving the annotated text itself unaltered.</citsent>
<aftsection>
<nextsent>the format of this annotation file, however, is highly idiosyncratic, rendering very difficult the subsequent analysis of theannotation.
</nextsent>
<nextsent>moreover, this tool also represents co specification in terms of antecedence only, making it im possible to annotate the former without specifying the latter.
</nextsent>
<nextsent>on the other hand, referee directly supports the definition of user-definable attributes.finally, the mate workbench5 is the most ambitious tool that we considered for the implementation of our annotation scheme.
</nextsent>
<nextsent>it has been developed in java as highly customizable tool for the xml-based annotation of arbitrary and possibly non-hierarchical levels of linguistic description.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1069">
<title id=" W00-0306.xml">stochastic language generation for spoken dialogue systems </title>
<section> stochastic surface realization.  </section>
<citcontext>
<prevsection>
<prevsent>m tl a*.
</prevsent>
<prevsent>= arg max ~ p(bk ) \ [   p(al bk) k=l i=1 29 although this independence assumption is an oversimplification, this simple model is good starting point for our initial implementation this approach.
</prevsent>
</prevsection>
<citsent citstr=" W98-1425 ">
we follow busemann and horacek (1998) <papid> W98-1425 </papid>in designing our generation engine with  different levels of granularity. </citsent>
<aftsection>
<nextsent>the different levels contribute to the specific needs of the various utterance classes.
</nextsent>
<nextsent>for example, at the beginning of the dialogue, system greeting can be simply generated by  canned  expression.
</nextsent>
<nextsent>other short, simple utterances can be generated efficiently by templates.
</nextsent>
<nextsent>in busemann and horacek (1998), <papid> W98-1425 </papid>the remaining output is generated by grammar rules.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1071">
<title id=" W01-1405.xml">stochastic modelling from pattern classification to language translation </title>
<section> language translation as pattern.  </section>
<citcontext>
<prevsection>
<prevsent>to classify an observation vector into one out of several classes c, the bayes decision rule is: c?
</prevsent>
<prevsent>= argmaxc {pr(c|y)} = argmaxc {pr(c) ? pr(y|c)} . for language translation, the starting point is the observed sequence of source symbols = fj1 = f1...fj , i.e. the sequence of source words, for which the target word sequence = ei1 = e1...ei has to be determined.
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
in order to minimize the number of decision errors at the sentence level, we have to choose the sequence of target words ei1 according to the equation (brown et al 1993): <papid> J93-2003 </papid>ei1 = argmaxei1 { pr(ei1|fj1 ) } = argmax ei1 { pr(ei1) ? pr(fj1 |ei1) } . here, the posterior probability pr(ei1|fj1 ) is decomposed into the language model probability pr(ej1 ) and the string translation probability pr(fj1 |ei1).</citsent>
<aftsection>
<nextsent>due to this factor ization, we have two separate probability distributions which can be modelled and trained independently of each other.
</nextsent>
<nextsent>fig.1 shows the architecture that results from the bayes decision theory.
</nextsent>
<nextsent>here we have already taken into account that, in order to implement the string translation model, we will decompose it into so-called alignment model and lexicon model.
</nextsent>
<nextsent>as also shown in this figure, we explicitly allow for optional transformations to make the translation task simpler for the algorithm.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1074">
<title id=" W01-1405.xml">stochastic modelling from pattern classification to language translation </title>
<section> experimental results.  </section>
<citcontext>
<prevsection>
<prevsent>these methods are only the time-honoured methods and successful methods of today.
</prevsent>
<prevsent>the characteristic property lies in the systematic use of probabilistic framework for the construction of models, in the statistical training of the free parameters of these models and in the explicit use of global scoring criterion for the decisionmaking process.
</prevsent>
</prevsection>
<citsent citstr=" H94-1028 ">
whereas stochastic modelling is widely used in speech recognition, there are so far only few research groups that apply stochastic modelling to language translation (berger et al 1994; <papid> H94-1028 </papid>brown et al. 1993; <papid> J93-2003 </papid>knight 1999).<papid> J99-4005 </papid></citsent>
<aftsection>
<nextsent>the presentation here is based on work carried out in the framework of the eutrans project (casacuberta et al 2001) and the verb mobil project (wahlster 2000).
</nextsent>
<nextsent>we will consider the experimental results obtained in the verb mobil project.
</nextsent>
<nextsent>the goal of the verb mobil project is the translation of spoken dialogues in the domains of appointment scheduling and travel planning.
</nextsent>
<nextsent>the languages are german and english.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1076">
<title id=" W01-1405.xml">stochastic modelling from pattern classification to language translation </title>
<section> experimental results.  </section>
<citcontext>
<prevsection>
<prevsent>these methods are only the time-honoured methods and successful methods of today.
</prevsent>
<prevsent>the characteristic property lies in the systematic use of probabilistic framework for the construction of models, in the statistical training of the free parameters of these models and in the explicit use of global scoring criterion for the decisionmaking process.
</prevsent>
</prevsection>
<citsent citstr=" J99-4005 ">
whereas stochastic modelling is widely used in speech recognition, there are so far only few research groups that apply stochastic modelling to language translation (berger et al 1994; <papid> H94-1028 </papid>brown et al. 1993; <papid> J93-2003 </papid>knight 1999).<papid> J99-4005 </papid></citsent>
<aftsection>
<nextsent>the presentation here is based on work carried out in the framework of the eutrans project (casacuberta et al 2001) and the verb mobil project (wahlster 2000).
</nextsent>
<nextsent>we will consider the experimental results obtained in the verb mobil project.
</nextsent>
<nextsent>the goal of the verb mobil project is the translation of spoken dialogues in the domains of appointment scheduling and travel planning.
</nextsent>
<nextsent>the languages are german and english.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1077">
<title id=" W02-0205.xml">mup  the uic standoff markup tool </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this paper we present mup,a coding tool for standoff markup which is sophisticated enough to allow for variety of different markings to be applied, but which is also simple enough to use that it does not require sizable set up effort.
</prevsent>
<prevsent>other coding tools have been developed, and some of them do in fact target discourse phenomena.
</prevsent>
</prevsection>
<citsent citstr=" A97-1051 ">
tools specifically developed to code for discourse phenomena include nb (flammia and zue, 1995), dat (allen and core, 1997), mate (mckelvie et al., 2001), and the alembic workbench (day et al, 1997).<papid> A97-1051 </papid></citsent>
<aftsection>
<nextsent>mup differs from all of them because it is standoff (contrary to nb and dat), allows tagging of discontinuous constituents (contrary to nb), and is simple to set up and use (contrary to mate).
</nextsent>
<nextsent>we developed mup within the diag-nlp project (di eugenio et al, 2002), which is grafting an nlg component onto tutorial program written in the vivids (munro, 1994) and diag (towne, 1997) its authoring environment.
</nextsent>
<nextsent>mup is targeted to written or transcribed text.
</nextsent>
<nextsent>phenomena such as intonation contours and overlapping speech have no opportunity to occur in our transcripts.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1078">
<title id=" W02-0205.xml">mup  the uic standoff markup tool </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>being annotated as an indicator via pop-up dialogue.
</prevsent>
<prevsent>discussion we believe simplified easy-to-configure and run tool will have wider applicability beyond our own project.
</prevsent>
</prevsection>
<citsent citstr=" W99-0307 ">
other projects that manually code large quantities of typed text, e.g. the rst dialogue markup project (marcu et al, 1999), <papid> W99-0307 </papid>have found it desirable to create their own markup tools.</citsent>
<aftsection>
<nextsent>the circsim-tutor project, with well over hundred transcripts of typed dialogue averaging an hour each, has been coding in sgml (freedman et al, 1998; kim, 1999) with general-purpose text editors.
</nextsent>
<nextsent>the mate workbench (mckelvie et al, 2001) is full-featured dialogue markup tool, however we found it to be complex and difficult to use.
</nextsent>
<nextsent>we saw an opportunity to borrow some of the ideas from mate and realize them with simpler annotation tool.
</nextsent>
<nextsent>mate envisions three levels of user: coders, researchers for whom the coding task is performed and who need to view and manipulate the results, and experts who are able to configure the software (carletta and isard, 1999).<papid> W99-0302 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1079">
<title id=" W02-0205.xml">mup  the uic standoff markup tool </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the mate workbench (mckelvie et al, 2001) is full-featured dialogue markup tool, however we found it to be complex and difficult to use.
</prevsent>
<prevsent>we saw an opportunity to borrow some of the ideas from mate and realize them with simpler annotation tool.
</prevsent>
</prevsection>
<citsent citstr=" W99-0302 ">
mate envisions three levels of user: coders, researchers for whom the coding task is performed and who need to view and manipulate the results, and experts who are able to configure the software (carletta and isard, 1999).<papid> W99-0302 </papid></citsent>
<aftsection>
<nextsent>it is this last group that can perform the manipulations necessary for adding new tags to the tag set and controlling how theyare displayed.
</nextsent>
<nextsent>mate permits programmatic control over the coding interface by means of an xsl style sheet customized for particular application.it is possible to split windows, intercept cursor operations, provide linking operations between text indifferent windows, and so on.
</nextsent>
<nextsent>this kind of flexibility is useful in annotated speech, for example in separately displaying and linking two speech stream sand having several related windows update simultaneously in response to coder actions.
</nextsent>
<nextsent>in our experience the mate style sheets were quite difficult to write and debug, and for our application we did not need the flexibility, so we dispensed with them and created our own, simpler, mechanism to control the display of text.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1080">
<title id=" W02-0605.xml">using eigenvectors of the bigram graph to infer morpheme identity </title>
<section> identifying syntactic behavior of.  </section>
<citcontext>
<prevsection>
<prevsent>automatically identified suffixes interesting as they are, the representations we have seen are not capable of specifying membership in grammatical categories in an absolute sense.
</prevsent>
<prevsent>in this section, we explore the application of this representation to text which has been morphologically analyzed by language-neutral morphological analyzer.
</prevsent>
</prevsection>
<citsent citstr=" J01-2001 ">
for this purpose, we employ the algorithm described in goldsmith (2001), <papid> J01-2001 </papid>which takes an unanalyzed corpus and provides an analysis of the words into stems and suffixes.</citsent>
<aftsection>
<nextsent>what is useful about that algorithm for our purposes is that it shares the same commitment to analysis based only on raw (untreated) natural text, and neither hand coding nor prior linguistic knowledge.
</nextsent>
<nextsent>the algorithm in goldsmith (2001) <papid> J01-2001 </papid>links each stem in the corpus to the set of suffixes (called its signature) with which it appears in the corpus.</nextsent>
<nextsent>thus the stem jump might appear with the three suffixes ed-ing-s in given corpus.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1082">
<title id=" W02-0603.xml">unsupervised discovery of morphemes </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>1.1 previous work on unsupervised.
</prevsent>
<prevsent>segmentation many existing morphology discovery algorithms concentrate on identifying prefixes, suffixes and stems, i.e., assume rather simple inflectional morphology.
</prevsent>
</prevsection>
<citsent citstr=" W98-1239 ">
dejean (1998) <papid> W98-1239 </papid>concentrates on the problem of finding the list of frequent affixes for language rather than attempting to produce morphological analysis of each word.</citsent>
<aftsection>
<nextsent>following the work of zellig harris he identifies possible morpheme boundaries by looking at the number of possible letters following given sequence of letters, and then utilizes frequency limits for accepting morphemes.
</nextsent>
<nextsent>1for comprehensive view of finnish morphology, see (karlsson, 1987).goldsmith (2000) concentrates on stem+suffix languages, in particular indo-european languages, and tries to produce output that would match as closely as possible with the analysis given by human morphologist.
</nextsent>
<nextsent>he further assumes that stems form groups that he calls signatures, and each signature shares set of possible affixes.
</nextsent>
<nextsent>he applies an mdl criterion for model optimization.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1083">
<title id=" W02-0603.xml">unsupervised discovery of morphemes </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>he applies an mdl criterion for model optimization.
</prevsent>
<prevsent>the previously discussed approaches consider only individual words without regard to their contexts, or to their semantic content.
</prevsent>
</prevsection>
<citsent citstr=" W00-0712 ">
in different approach, schone and jurafsky (2000) <papid> W00-0712 </papid>utilize the context of each term to obtain semantic representation for it using lsa.</citsent>
<aftsection>
<nextsent>the division to morphemes is then accepted only when the stem and stem+affix are sufficiently similar semantically.
</nextsent>
<nextsent>their method is shown to improve on the performance of goldsmiths linguist ica on celex, morphologically analyzed english corpus.
</nextsent>
<nextsent>in the related field of text segmentation, one can sometimes obtain morphemes.
</nextsent>
<nextsent>some of the approaches remove spaces from text and try to identify word boundaries utilizing e.g. entropy-based measures, as in (redlich, 1993).word induction from natural language text with out word boundaries is also studied in (deligne and bimbot, 1997; hua, 2000), where mdl-based model optimization measures are used.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1084">
<title id=" W01-1406.xml">a best first alignment algorithm for automatic extraction of transfer mappings from bilingual corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this system acquires transfer mappings by aligning pairs of logical form structures (lfs) similar to those described by jensen (1993).
</prevsent>
<prevsent>these lfs are obtained by parsing sentence pairs from sentence-aligned bilingual corpus.
</prevsent>
</prevsection>
<citsent citstr=" P93-1002 ">
(the problem of aligning parallel corpora at the sentence level has been addressed by meyers (1998b) chen (1993) <papid> P93-1002 </papid>and others and is beyond the scope of this paper).</citsent>
<aftsection>
<nextsent>we show that alignment using best-first strategy in conjunction with small alignment grammar improves the alignment and the quality of the acquired transfer mappings.
</nextsent>
<nextsent>hacer usted informacin hipervnculo hipervnculo clic direc cin click hyperlink_information you address hyper link de de en en under dsub dobj mod dsub dobj figure 1a: lexical correspondences figure 1b: alignment mappings hacer usted informacin hipervnculo hipervnculo clic direc cin click hyperlink_information you address hyper link de de en en under dsub dobj mod dsub dobj
</nextsent>
<nextsent>a logical form (lf) is an unordered graph representing the relations among the most meaningful elements of sentence.
</nextsent>
<nextsent>nodes are identified by the lemma of content word and directed, labeled arcs indicate the underlying semantic relations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1085">
<title id=" W01-1406.xml">a best first alignment algorithm for automatic extraction of transfer mappings from bilingual corpora </title>
<section> alignment.  </section>
<citcontext>
<prevsection>
<prevsent>correspondences we use bilingual lexicon that merges data from several sources (cup, 1995), (softart, 1995), (langenscheidt, 1997), and inverts target-to-source dictionaries to improve coverage.
</prevsent>
<prevsent>our spanish-english lexicon contains 88,500 translation pairs.
</prevsent>
</prevsection>
<citsent citstr=" W01-1411 ">
we augment this with 19,762 translation correspondences acquired using statistical techniques described by moore (2001).<papid> W01-1411 </papid></citsent>
<aftsection>
<nextsent>like watanabe (2000) and meyers (2000), we use lexicon to establish initial tentative word correspondences.
</nextsent>
<nextsent>however, we have found that even relatively large bilingual dictionary has only moderately good coverage for our purposes.
</nextsent>
<nextsent>hence, we pursue an aggressive matching strategy for establishing tentative word correspondences.
</nextsent>
<nextsent>using the bilingual dictionary together with the derivational morphology component in our system (pentheroudakis, 1993), we find direct translations, translations of morphological bases and derivations, and base and derived forms of translations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1086">
<title id=" W01-1408.xml">an efficient a search algorithm for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we are given source string fj1 = f1...fj ...fj , which is to be translated into target string ei1 = e1...ei...ei . among all possible target strings, we will choose the string with the highest probability: ei1 = argmaxei1 { pr(ej1 |f i1 ) } = argmax ei1 { pr(ei1) ? pr(fj1 |ei1) }the argmax operation denotes the search problem, i.e. the generation of the output sentence in the target language.
</prevsent>
<prevsent>pr(ei1) is the language model of the target language, whereas pr(fj1 |ei1) denotes the translation model.
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
many statistical translation models (brown et al., 1993; <papid> J93-2003 </papid>vogel et al, 1996; <papid> C96-2141 </papid>och and ney, 2000<papid> P00-1056 </papid>b)try to model word-to-word correspondences between source and target words.</citsent>
<aftsection>
<nextsent>these correspondences are called an alignment.
</nextsent>
<nextsent>the model is often further restricted in way such that each source word is assigned exactly one target word.
</nextsent>
<nextsent>the alignment mapping is ? = aj from source position to target position = aj . the alignment aj1 may contain alignments aj = 0 with the empty?
</nextsent>
<nextsent>word e0 to account for source words that are not aligned to any target word.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1087">
<title id=" W01-1408.xml">an efficient a search algorithm for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we are given source string fj1 = f1...fj ...fj , which is to be translated into target string ei1 = e1...ei...ei . among all possible target strings, we will choose the string with the highest probability: ei1 = argmaxei1 { pr(ej1 |f i1 ) } = argmax ei1 { pr(ei1) ? pr(fj1 |ei1) }the argmax operation denotes the search problem, i.e. the generation of the output sentence in the target language.
</prevsent>
<prevsent>pr(ei1) is the language model of the target language, whereas pr(fj1 |ei1) denotes the translation model.
</prevsent>
</prevsection>
<citsent citstr=" C96-2141 ">
many statistical translation models (brown et al., 1993; <papid> J93-2003 </papid>vogel et al, 1996; <papid> C96-2141 </papid>och and ney, 2000<papid> P00-1056 </papid>b)try to model word-to-word correspondences between source and target words.</citsent>
<aftsection>
<nextsent>these correspondences are called an alignment.
</nextsent>
<nextsent>the model is often further restricted in way such that each source word is assigned exactly one target word.
</nextsent>
<nextsent>the alignment mapping is ? = aj from source position to target position = aj . the alignment aj1 may contain alignments aj = 0 with the empty?
</nextsent>
<nextsent>word e0 to account for source words that are not aligned to any target word.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1089">
<title id=" W01-1408.xml">an efficient a search algorithm for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we are given source string fj1 = f1...fj ...fj , which is to be translated into target string ei1 = e1...ei...ei . among all possible target strings, we will choose the string with the highest probability: ei1 = argmaxei1 { pr(ej1 |f i1 ) } = argmax ei1 { pr(ei1) ? pr(fj1 |ei1) }the argmax operation denotes the search problem, i.e. the generation of the output sentence in the target language.
</prevsent>
<prevsent>pr(ei1) is the language model of the target language, whereas pr(fj1 |ei1) denotes the translation model.
</prevsent>
</prevsection>
<citsent citstr=" P00-1056 ">
many statistical translation models (brown et al., 1993; <papid> J93-2003 </papid>vogel et al, 1996; <papid> C96-2141 </papid>och and ney, 2000<papid> P00-1056 </papid>b)try to model word-to-word correspondences between source and target words.</citsent>
<aftsection>
<nextsent>these correspondences are called an alignment.
</nextsent>
<nextsent>the model is often further restricted in way such that each source word is assigned exactly one target word.
</nextsent>
<nextsent>the alignment mapping is ? = aj from source position to target position = aj . the alignment aj1 may contain alignments aj = 0 with the empty?
</nextsent>
<nextsent>word e0 to account for source words that are not aligned to any target word.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1115">
<title id=" W01-1408.xml">an efficient a search algorithm for statistical machine translation </title>
<section> search problem.  </section>
<citcontext>
<prevsection>
<prevsent>efficient language model recombination the recombination procedure which is described above can be improved by taking into account the backing-off structure of the language model.
</prevsent>
<prevsent>the trigram language model we use has the property that if the count of the bigram n(u, v) = 0, then the probability (w|u, v) depends only on v. in this case the recombination can be significantly improved by re combining all nodes whose language model state has the property n(u, v) = 0 only with respect to v. obviously, this could be generalized to other types of language models as well.experiments have shown that by using this efficient recombination, the number of needed hypotheses can be reduced by about factor of 4.
</prevsent>
</prevsection>
<citsent citstr=" C00-2123 ">
search algorithms we evaluate the following two search algorithms: ? beam search algorithm (bs): (tillmann, 2001; tillmann and ney, 2000) <papid> C00-2123 </papid>in this algorithm the search space is explored in breadth-first manner.</citsent>
<aftsection>
<nextsent>the search algorithm is based on dynamic programming approach and applies various pruning techniques in order to restrict the number of considered hypotheses.
</nextsent>
<nextsent>for more details see (tillmann, 2001).
</nextsent>
<nextsent>a* search algorithm: in a*, all search hypotheses are managed ina priority queue.
</nextsent>
<nextsent>the basic a* search (nils son, 1971) can be described as follows: 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1116">
<title id=" W01-1408.xml">an efficient a search algorithm for statistical machine translation </title>
<section> admissible heuristic function.  </section>
<citcontext>
<prevsection>
<prevsent>the a* search algorithm corresponds to the dijkstra algorithm if the heuristic function is equal to zero.
</prevsent>
<prevsent>in order to perform an efficient search with the a* search algorithm it is crucial to use good heuristic function.
</prevsent>
</prevsection>
<citsent citstr=" P97-1047 ">
we only know of the work by (wang and waibel, 1997) <papid> P97-1047 </papid>dealing with heuristic functions for search in statistical machine translation.</citsent>
<aftsection>
<nextsent>they developed simple heuristic function for model 2 from (brown et al, 1993) <papid> J93-2003 </papid>which was non admissible.</nextsent>
<nextsent>in the following we develop guaranteed admissible heuristic function for model 4 taking into account distortion probabilities and the coupling of lexicon, fertility, and language model probabilities.the basic idea for developing heuristic function for the alignment models is the fact that all source sentence positions which have not been covered so far still have to be translated in order to complete the sentence.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1118">
<title id=" W00-0407.xml">evaluation of phrase representation summarization based on information retrieval task </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>summaries are often used to select relevant documents from information retrieval results.
</prevsent>
<prevsent>the goal of summarization for such  indicative  use is to serve fast and accurate judgement.
</prevsent>
</prevsection>
<citsent citstr=" C00-2127 ">
we have developed the concept of the  at-a-glance  summary, and its realization in the japanese language -  phrase-representation summa riza tiola  - to achieve this goal (ueda, et al 2000).<papid> C00-2127 </papid></citsent>
<aftsection>
<nextsent>we have conducted an evaluation experiment to verify the effectiveness of this summarization method.
</nextsent>
<nextsent>there are two strategies for evaluating summarization systems: intrinsic and extrinsic (jing, et al 1998).
</nextsent>
<nextsent>intrinsic methods measure system quality mainly by comparing the system output with an  ideal  summary.
</nextsent>
<nextsent>extrinsic methods measure system perfor-mance in particular task.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1119">
<title id=" W00-0407.xml">evaluation of phrase representation summarization based on information retrieval task </title>
<section> evaluation method.  </section>
<citcontext>
<prevsection>
<prevsent>(a) leading fixed-length characters: extract the first 80 characters of the document body.
</prevsent>
<prevsent>(b) sentence xtraction summarization: select important sentences from document.
</prevsent>
</prevsection>
<citsent citstr=" C96-2166 ">
the importance score of each sentence is calculated from the simple sum of the im-portance scores of the words in sentence (zechner 1996).<papid> C96-2166 </papid></citsent>
<aftsection>
<nextsent>(c) phrase-representation summarization: described in chapter 1.
</nextsent>
<nextsent>(d) keyword enumeration summarization: list up important words or compound nouns.
</nextsent>
<nextsent>http://www, fujixerox.co.jp/headlinej2000/0308__nton e,r_biz,hlml (in english) this phrase lacks the subject because the original sentence lacks it.
</nextsent>
<nextsent>cases are usually omitted in japanese if they can be easily inferred.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1120">
<title id=" W00-0801.xml">an unsupervised method for multilingual word sense tagging using parallel corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the availability of large scale, accurately, sense tagged data should help alleviate the problem.
</prevsent>
<prevsent>it has been acknowledged that best way to acquire sense tags for words in corpus is manually, which has proven to be very expensive and labor intensive ndeavor.
</prevsent>
</prevsection>
<citsent citstr=" W97-0209 ">
in an attempt to approximate the human effort, both supervised \[bruce &amp; weibe, 1994; lin, 1999;etc.\] and unsupervised methods \[resnik 1997; <papid> W97-0209 </papid>yarowsky, 1992  <papid> C92-2070 </papid>etc.\] have been proposed to solve the problem automatically.</citsent>
<aftsection>
<nextsent>on average supervised methods report higher accuracy rates, but they are faced with the problem of requiring large amounts of sense tagged data as training material.
</nextsent>
<nextsent>most of the methods, to date, aim at solving the problem for one language, namely the language with the most available linguistic resources.
</nextsent>
<nextsent>moreover, most of the proposed approaches report results on handful of the data, rendering them solutions for small scale of the data.
</nextsent>
<nextsent>many researchers in the field have looked at language translations as source for sense distinctions \[dagan &amp; itai, 1994; <papid> J94-4003 </papid>dyvik, 1998; ide, in press; resnik &amp; yarowsky, 1999; etc.\].</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1121">
<title id=" W00-0801.xml">an unsupervised method for multilingual word sense tagging using parallel corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the availability of large scale, accurately, sense tagged data should help alleviate the problem.
</prevsent>
<prevsent>it has been acknowledged that best way to acquire sense tags for words in corpus is manually, which has proven to be very expensive and labor intensive ndeavor.
</prevsent>
</prevsection>
<citsent citstr=" C92-2070 ">
in an attempt to approximate the human effort, both supervised \[bruce &amp; weibe, 1994; lin, 1999;etc.\] and unsupervised methods \[resnik 1997; <papid> W97-0209 </papid>yarowsky, 1992  <papid> C92-2070 </papid>etc.\] have been proposed to solve the problem automatically.</citsent>
<aftsection>
<nextsent>on average supervised methods report higher accuracy rates, but they are faced with the problem of requiring large amounts of sense tagged data as training material.
</nextsent>
<nextsent>most of the methods, to date, aim at solving the problem for one language, namely the language with the most available linguistic resources.
</nextsent>
<nextsent>moreover, most of the proposed approaches report results on handful of the data, rendering them solutions for small scale of the data.
</nextsent>
<nextsent>many researchers in the field have looked at language translations as source for sense distinctions \[dagan &amp; itai, 1994; <papid> J94-4003 </papid>dyvik, 1998; ide, in press; resnik &amp; yarowsky, 1999; etc.\].</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1122">
<title id=" W00-0801.xml">an unsupervised method for multilingual word sense tagging using parallel corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>most of the methods, to date, aim at solving the problem for one language, namely the language with the most available linguistic resources.
</prevsent>
<prevsent>moreover, most of the proposed approaches report results on handful of the data, rendering them solutions for small scale of the data.
</prevsent>
</prevsection>
<citsent citstr=" J94-4003 ">
many researchers in the field have looked at language translations as source for sense distinctions \[dagan &amp; itai, 1994; <papid> J94-4003 </papid>dyvik, 1998; ide, in press; resnik &amp; yarowsky, 1999; etc.\].</citsent>
<aftsection>
<nextsent>the idea is that polysemons words in one language can be translated as distinct words in different language.
</nextsent>
<nextsent>the problem has always been the availability of large corpora in translation, i.e. parallel corpora.
</nextsent>
<nextsent>resnik \[1999\] <papid> P99-1068 </papid>proposed method for facilitating the acquisition of parallel corpora from the www.</nextsent>
<nextsent>potentially, we can have parallel corpora in myriad of languages, yet the downside is the scarcity of linguistic knowledge resources and processing tools for less widely represented/studied languages.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1123">
<title id=" W00-0801.xml">an unsupervised method for multilingual word sense tagging using parallel corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the idea is that polysemons words in one language can be translated as distinct words in different language.
</prevsent>
<prevsent>the problem has always been the availability of large corpora in translation, i.e. parallel corpora.
</prevsent>
</prevsection>
<citsent citstr=" P99-1068 ">
resnik \[1999\] <papid> P99-1068 </papid>proposed method for facilitating the acquisition of parallel corpora from the www.</citsent>
<aftsection>
<nextsent>potentially, we can have parallel corpora in myriad of languages, yet the downside is the scarcity of linguistic knowledge resources and processing tools for less widely represented/studied languages.
</nextsent>
<nextsent>consequently, we decided to bootstrap the process of word sense tagging for both languages in parallel corpus using the translations a source of word sense distinction.
</nextsent>
<nextsent>thereby, attaining sense tagged ata for languages with scarce resources as well as creating supply of large-scale, automatically sense tagged data for the language with more knowledge resources -albeit noisy - to be utilized by supervised algorithms.
</nextsent>
<nextsent>in this paper, we propose an unsupervised method for word sense tagging of  both corpora automatically.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1124">
<title id=" W00-0801.xml">an unsupervised method for multilingual word sense tagging using parallel corpora </title>
<section> proposed method.  </section>
<citcontext>
<prevsection>
<prevsent>for each source and corresponding target sentence, find the best token level alignments.
</prevsent>
<prevsent>methods for automating this process have been proposed in the literature.
</prevsent>
</prevsection>
<citsent citstr=" J00-2004 ">
\[a10naizan et al, 1999; melamed, 2000; <papid> J00-2004 </papid>etc.\] ? for each source language token, create alist of its alignments target language tokens, target set ? using the taxonomy, calculate the distance between the senses of the tokens in the target set; assign the appropriate sense(s) to each of the tokens in the target set based on an opt imitation function over the entire set of target token senses * propagate the assigned senses back to both target and source corpora tokens, effectively, creating two tag sets, one for each the target and source corpus ? evaluate the resnlting tag sets against ahand tagged test set.</citsent>
<aftsection>
<nextsent>3.1.
</nextsent>
<nextsent>materials.
</nextsent>
<nextsent>we chose the brown corpus of american english \[francis &amp; kutera, 1982\] - of one million words - as our target language corpus.
</nextsent>
<nextsent>it is balanced corpus and it has more than 200k words that are manually sense tagged as product of the semantic oncordance (semcor) effort using wordnet \[miller et al 1994\].<papid> H94-1046 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1125">
<title id=" W00-0801.xml">an unsupervised method for multilingual word sense tagging using parallel corpora </title>
<section> preliminary evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>materials.
</prevsent>
<prevsent>we chose the brown corpus of american english \[francis &amp; kutera, 1982\] - of one million words - as our target language corpus.
</prevsent>
</prevsection>
<citsent citstr=" H94-1046 ">
it is balanced corpus and it has more than 200k words that are manually sense tagged as product of the semantic oncordance (semcor) effort using wordnet \[miller et al 1994\].<papid> H94-1046 </papid></citsent>
<aftsection>
<nextsent>the semcor data is tagged in lamning text - words of varying parts of speech are tagged in context - using wordnet 1.6.
</nextsent>
<nextsent>hence, we used wordnet 1.6 taxonomy as the linguistic knowledge resource.
</nextsent>
<nextsent>\[fellbaum, 1998\] for purposes of this preliminary investigation, we only explored nouns in the corpus, yet there are no inherent restrictions inthe method for applying it to other parts of speech.
</nextsent>
<nextsent>accordingly, we used part of speech tags that were available in the penn tree bank for the brown corpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1126">
<title id=" W00-0801.xml">an unsupervised method for multilingual word sense tagging using parallel corpora </title>
<section> preliminary evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>artificially created, therefore there was one to one token level alignments, we used the giza program \[al onaizan et al 1999\]\[.
</prevsent>
<prevsent>giza is an intermediate program in statistical machine translation system, egypt.
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
it is an implementation models 1-4 of brown et al \[1993\], <papid> J93-2003 </papid>where each of these models produces viterbi alignment.</citsent>
<aftsection>
<nextsent>the models are trained in succession where the final paraaneter values from one model are used as the starting parameters for the next model.
</nextsent>
<nextsent>we trained each model for i0 iterations.
</nextsent>
<nextsent>given source and target pair of afigned sentences, giza produces the most probable token-level alignments.
</nextsent>
<nextsent>multiple token alignments are allowed on the target language side, i.e. token in english could align with multiple tokens :in the foreign language.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1130">
<title id=" W02-0307.xml">enhanced natural language access to anatomically indexed data </title>
<section> increasing recall.  </section>
<citcontext>
<prevsection>
<prevsent>this left 451 nps, of which 82 were found to be exact matches for component terms and eight, for the synonyms in the nomenclature.
</prevsent>
<prevsent>these were also removed, leaving 361 possible anatomical terms not found in the nomenclature.
</prevsent>
</prevsection>
<citsent citstr=" W02-0308 ">
we then used common technique to reduce this set by only considering nps headed by or modified by frequent head or modifier from within the set of component terms (bodenreider et al, 2002).<papid> W02-0308 </papid></citsent>
<aftsection>
<nextsent>here, frequent meant   3 times.
</nextsent>
<nextsent>for example, carotid,fibrous and endocardial are frequent modifiers, while artery, septum and tissue are frequent head nouns.
</nextsent>
<nextsent>of the 361 remaining nps from the heart chapter, 115 shared high frequency head noun with terms already in the nomenclature, while 105 shared high frequency modifier with terms in the nomenclature.
</nextsent>
<nextsent>we considered these 220 nps probable anatomical terms, with the remaining 141 being possible anatomical terms.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1131">
<title id=" W01-1605.xml">building a discourse tagged corpus in the framework of rhetorical structure theory </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these corpora, now also common in many languages, have accelerated development efforts and energized the community.
</prevsent>
<prevsent>annotation ranges from broad characterization of document-level information, such as topic or relevance judgments (voorhees and harman, 1999; wayne, 2000) to discrete analysis of wide range of linguistic phenomena.
</prevsent>
</prevsection>
<citsent citstr=" J86-3001 ">
however, rich theoretical approaches to discourse/text analysis (van dijk and kintsch, 1983; meyer, 1985; grosz and sidner, 1986; <papid> J86-3001 </papid>mann and thompson, 1988) have yet to be applied on large scale.</citsent>
<aftsection>
<nextsent>so far, the annotation of discourse structure of documents has been applied primarily to identifying topical segments (hearst, 1997), <papid> J97-1003 </papid>inter-sentential relations (nomoto and matsumoto, 1999; <papid> W99-0620 </papid>tsou et al , 2000), <papid> W00-1206 </papid>and hierarchical analyses of small corpora (moser and moore, 1995; <papid> P95-1018 </papid>marcu et al , 1999).<papid> W99-0307 </papid></nextsent>
<nextsent>in this paper, we recount our experience in developing large resource with discourse-level annotation for nlp research.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1132">
<title id=" W01-1605.xml">building a discourse tagged corpus in the framework of rhetorical structure theory </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>annotation ranges from broad characterization of document-level information, such as topic or relevance judgments (voorhees and harman, 1999; wayne, 2000) to discrete analysis of wide range of linguistic phenomena.
</prevsent>
<prevsent>however, rich theoretical approaches to discourse/text analysis (van dijk and kintsch, 1983; meyer, 1985; grosz and sidner, 1986; <papid> J86-3001 </papid>mann and thompson, 1988) have yet to be applied on large scale.</prevsent>
</prevsection>
<citsent citstr=" J97-1003 ">
so far, the annotation of discourse structure of documents has been applied primarily to identifying topical segments (hearst, 1997), <papid> J97-1003 </papid>inter-sentential relations (nomoto and matsumoto, 1999; <papid> W99-0620 </papid>tsou et al , 2000), <papid> W00-1206 </papid>and hierarchical analyses of small corpora (moser and moore, 1995; <papid> P95-1018 </papid>marcu et al , 1999).<papid> W99-0307 </papid></citsent>
<aftsection>
<nextsent>in this paper, we recount our experience in developing large resource with discourse-level annotation for nlp research.
</nextsent>
<nextsent>our main goal in undertaking this effort was to create reference corpus for community-wide use.
</nextsent>
<nextsent>two essential considerations from the outset were that the corpus needed to be consistently annotated, and that it would be made publicly available through the linguistic data consortium for nominal fee to cover distribution costs.
</nextsent>
<nextsent>the paper describes the challenges we faced in building corpus of this level of complexity and scope ? including selection of theoretical approach, annotation methodology, training, and quality assurance.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1134">
<title id=" W01-1605.xml">building a discourse tagged corpus in the framework of rhetorical structure theory </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>annotation ranges from broad characterization of document-level information, such as topic or relevance judgments (voorhees and harman, 1999; wayne, 2000) to discrete analysis of wide range of linguistic phenomena.
</prevsent>
<prevsent>however, rich theoretical approaches to discourse/text analysis (van dijk and kintsch, 1983; meyer, 1985; grosz and sidner, 1986; <papid> J86-3001 </papid>mann and thompson, 1988) have yet to be applied on large scale.</prevsent>
</prevsection>
<citsent citstr=" W99-0620 ">
so far, the annotation of discourse structure of documents has been applied primarily to identifying topical segments (hearst, 1997), <papid> J97-1003 </papid>inter-sentential relations (nomoto and matsumoto, 1999; <papid> W99-0620 </papid>tsou et al , 2000), <papid> W00-1206 </papid>and hierarchical analyses of small corpora (moser and moore, 1995; <papid> P95-1018 </papid>marcu et al , 1999).<papid> W99-0307 </papid></citsent>
<aftsection>
<nextsent>in this paper, we recount our experience in developing large resource with discourse-level annotation for nlp research.
</nextsent>
<nextsent>our main goal in undertaking this effort was to create reference corpus for community-wide use.
</nextsent>
<nextsent>two essential considerations from the outset were that the corpus needed to be consistently annotated, and that it would be made publicly available through the linguistic data consortium for nominal fee to cover distribution costs.
</nextsent>
<nextsent>the paper describes the challenges we faced in building corpus of this level of complexity and scope ? including selection of theoretical approach, annotation methodology, training, and quality assurance.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1136">
<title id=" W01-1605.xml">building a discourse tagged corpus in the framework of rhetorical structure theory </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>annotation ranges from broad characterization of document-level information, such as topic or relevance judgments (voorhees and harman, 1999; wayne, 2000) to discrete analysis of wide range of linguistic phenomena.
</prevsent>
<prevsent>however, rich theoretical approaches to discourse/text analysis (van dijk and kintsch, 1983; meyer, 1985; grosz and sidner, 1986; <papid> J86-3001 </papid>mann and thompson, 1988) have yet to be applied on large scale.</prevsent>
</prevsection>
<citsent citstr=" W00-1206 ">
so far, the annotation of discourse structure of documents has been applied primarily to identifying topical segments (hearst, 1997), <papid> J97-1003 </papid>inter-sentential relations (nomoto and matsumoto, 1999; <papid> W99-0620 </papid>tsou et al , 2000), <papid> W00-1206 </papid>and hierarchical analyses of small corpora (moser and moore, 1995; <papid> P95-1018 </papid>marcu et al , 1999).<papid> W99-0307 </papid></citsent>
<aftsection>
<nextsent>in this paper, we recount our experience in developing large resource with discourse-level annotation for nlp research.
</nextsent>
<nextsent>our main goal in undertaking this effort was to create reference corpus for community-wide use.
</nextsent>
<nextsent>two essential considerations from the outset were that the corpus needed to be consistently annotated, and that it would be made publicly available through the linguistic data consortium for nominal fee to cover distribution costs.
</nextsent>
<nextsent>the paper describes the challenges we faced in building corpus of this level of complexity and scope ? including selection of theoretical approach, annotation methodology, training, and quality assurance.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1137">
<title id=" W01-1605.xml">building a discourse tagged corpus in the framework of rhetorical structure theory </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>annotation ranges from broad characterization of document-level information, such as topic or relevance judgments (voorhees and harman, 1999; wayne, 2000) to discrete analysis of wide range of linguistic phenomena.
</prevsent>
<prevsent>however, rich theoretical approaches to discourse/text analysis (van dijk and kintsch, 1983; meyer, 1985; grosz and sidner, 1986; <papid> J86-3001 </papid>mann and thompson, 1988) have yet to be applied on large scale.</prevsent>
</prevsection>
<citsent citstr=" P95-1018 ">
so far, the annotation of discourse structure of documents has been applied primarily to identifying topical segments (hearst, 1997), <papid> J97-1003 </papid>inter-sentential relations (nomoto and matsumoto, 1999; <papid> W99-0620 </papid>tsou et al , 2000), <papid> W00-1206 </papid>and hierarchical analyses of small corpora (moser and moore, 1995; <papid> P95-1018 </papid>marcu et al , 1999).<papid> W99-0307 </papid></citsent>
<aftsection>
<nextsent>in this paper, we recount our experience in developing large resource with discourse-level annotation for nlp research.
</nextsent>
<nextsent>our main goal in undertaking this effort was to create reference corpus for community-wide use.
</nextsent>
<nextsent>two essential considerations from the outset were that the corpus needed to be consistently annotated, and that it would be made publicly available through the linguistic data consortium for nominal fee to cover distribution costs.
</nextsent>
<nextsent>the paper describes the challenges we faced in building corpus of this level of complexity and scope ? including selection of theoretical approach, annotation methodology, training, and quality assurance.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1139">
<title id=" W01-1605.xml">building a discourse tagged corpus in the framework of rhetorical structure theory </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>annotation ranges from broad characterization of document-level information, such as topic or relevance judgments (voorhees and harman, 1999; wayne, 2000) to discrete analysis of wide range of linguistic phenomena.
</prevsent>
<prevsent>however, rich theoretical approaches to discourse/text analysis (van dijk and kintsch, 1983; meyer, 1985; grosz and sidner, 1986; <papid> J86-3001 </papid>mann and thompson, 1988) have yet to be applied on large scale.</prevsent>
</prevsection>
<citsent citstr=" W99-0307 ">
so far, the annotation of discourse structure of documents has been applied primarily to identifying topical segments (hearst, 1997), <papid> J97-1003 </papid>inter-sentential relations (nomoto and matsumoto, 1999; <papid> W99-0620 </papid>tsou et al , 2000), <papid> W00-1206 </papid>and hierarchical analyses of small corpora (moser and moore, 1995; <papid> P95-1018 </papid>marcu et al , 1999).<papid> W99-0307 </papid></citsent>
<aftsection>
<nextsent>in this paper, we recount our experience in developing large resource with discourse-level annotation for nlp research.
</nextsent>
<nextsent>our main goal in undertaking this effort was to create reference corpus for community-wide use.
</nextsent>
<nextsent>two essential considerations from the outset were that the corpus needed to be consistently annotated, and that it would be made publicly available through the linguistic data consortium for nominal fee to cover distribution costs.
</nextsent>
<nextsent>the paper describes the challenges we faced in building corpus of this level of complexity and scope ? including selection of theoretical approach, annotation methodology, training, and quality assurance.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1141">
<title id=" W01-1605.xml">building a discourse tagged corpus in the framework of rhetorical structure theory </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>two essential considerations from the outset were that the corpus needed to be consistently annotated, and that it would be made publicly available through the linguistic data consortium for nominal fee to cover distribution costs.
</prevsent>
<prevsent>the paper describes the challenges we faced in building corpus of this level of complexity and scope ? including selection of theoretical approach, annotation methodology, training, and quality assurance.
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
the resulting corpus contains 385 documents of american english selected from the penn treebank (marcus et al , 1993), <papid> J93-2004 </papid>annotated in the framework of rhetorical structure theory.</citsent>
<aftsection>
<nextsent>we believe this resource holds great promise as rich new source of text level information to support multiple lines of research for language understanding applications.
</nextsent>
<nextsent>two principle goals underpin the creation of this discourse-tagged corpus: 1) the corpus should be grounded in particular theoretical approach, and 2) it should be sufficiently large enough to offer potential for wide-scale use ? including linguistic analysis, training of statistical models of discourse, and other computational linguistic applications.
</nextsent>
<nextsent>these goals necessitated number of constraints to our approach.
</nextsent>
<nextsent>the theoretical framework had to be practical and repeatable over large set of documents in reasonable amount of time, with significant level of consistency across annotators.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1143">
<title id=" W01-1605.xml">building a discourse tagged corpus in the framework of rhetorical structure theory </title>
<section> framework.  </section>
<citcontext>
<prevsection>
<prevsent>previous research on annotating texts with rhetorical structure trees (marcu et al , 1999) <papid> W99-0307 </papid>has shown that texts can be annotated by multiple judges at relatively high levels of agreement.</prevsent>
<prevsent>we aimed to produce annotation protocols that would yield even higher agreement figures.</prevsent>
</prevsection>
<citsent citstr=" A00-2002 ">
previous research has shown that rst trees can play crucial role in building natural language generation systems (hovy, 1993; moore and paris, 1993; moore, 1995) and text summarization systems (marcu, 2000); can be used to increase the naturalness of machine translation outputs (marcu et al 2000); <papid> A00-2002 </papid>and can be used to build essay scoring systems that provide students with discourse-based feedback (burstein et al , 2001).<papid> P01-1014 </papid></citsent>
<aftsection>
<nextsent>we suspect that rst trees can be exploited successfully in the context of other applications as well.
</nextsent>
<nextsent>in the rst framework, the discourse structure of text can be represented as tree defined in terms of four aspects: ? the leaves of the tree correspond to text fragments that represent the minimal units of the discourse, called elementary discourse units ? the internal nodes of the tree correspond to contiguous text spans ? each node is characterized by its nuclearity ? nucleus indicates more essential unit of information, while satellite indicates supporting or background unit of information.
</nextsent>
<nextsent>each node is characterized by rhetorical relation that holds between two or more non-overlapping, adjacent text spans.
</nextsent>
<nextsent>relations can be of intentional, semantic, or textual nature.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1144">
<title id=" W01-1605.xml">building a discourse tagged corpus in the framework of rhetorical structure theory </title>
<section> framework.  </section>
<citcontext>
<prevsection>
<prevsent>previous research on annotating texts with rhetorical structure trees (marcu et al , 1999) <papid> W99-0307 </papid>has shown that texts can be annotated by multiple judges at relatively high levels of agreement.</prevsent>
<prevsent>we aimed to produce annotation protocols that would yield even higher agreement figures.</prevsent>
</prevsection>
<citsent citstr=" P01-1014 ">
previous research has shown that rst trees can play crucial role in building natural language generation systems (hovy, 1993; moore and paris, 1993; moore, 1995) and text summarization systems (marcu, 2000); can be used to increase the naturalness of machine translation outputs (marcu et al 2000); <papid> A00-2002 </papid>and can be used to build essay scoring systems that provide students with discourse-based feedback (burstein et al , 2001).<papid> P01-1014 </papid></citsent>
<aftsection>
<nextsent>we suspect that rst trees can be exploited successfully in the context of other applications as well.
</nextsent>
<nextsent>in the rst framework, the discourse structure of text can be represented as tree defined in terms of four aspects: ? the leaves of the tree correspond to text fragments that represent the minimal units of the discourse, called elementary discourse units ? the internal nodes of the tree correspond to contiguous text spans ? each node is characterized by its nuclearity ? nucleus indicates more essential unit of information, while satellite indicates supporting or background unit of information.
</nextsent>
<nextsent>each node is characterized by rhetorical relation that holds between two or more non-overlapping, adjacent text spans.
</nextsent>
<nextsent>relations can be of intentional, semantic, or textual nature.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1145">
<title id=" W01-1605.xml">building a discourse tagged corpus in the framework of rhetorical structure theory </title>
<section> framework.  </section>
<citcontext>
<prevsection>
<prevsent>it is inevitable that any decision on how to bracket elementary discourse units necessarily involves some compromises.
</prevsent>
<prevsent>reseachers in the field have proposed number of competing hypotheses about what constitutes an elementary discourse unit.
</prevsent>
</prevsection>
<citsent citstr=" J93-3003 ">
while some take the elementary units to be clauses (grimes, 1975; givon, 1983; long acre, 1983), others take them to be prosodic units (hirschberg and litman, 1993), <papid> J93-3003 </papid>turns of talk (sacks, 1974), sentences (polanyi, 1988), intentionally defined discourse segments (grosz and sidner, 1986), <papid> J86-3001 </papid>or the contextually indexed representation of information conveyed by semiotic gesture, asserting single state of affairs or partial state of affairs in discourse world,?</citsent>
<aftsection>
<nextsent>(polanyi, 1996, p.5).
</nextsent>
<nextsent>regardless of their theoretical stance, all agree that the elementary discourse units are non-overlapping spans of text.
</nextsent>
<nextsent>our goal was to find balance between granularity of tagging and ability to identify units consistently on large scale.
</nextsent>
<nextsent>in the end, we chose the clause as the elementary unit of discourse, using lexical and syntactic clues to help determine boundaries: 5.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1149">
<title id=" W01-1605.xml">building a discourse tagged corpus in the framework of rhetorical structure theory </title>
<section> discourse annotation task.  </section>
<citcontext>
<prevsection>
<prevsent>to reinforce new rules, annotators re-tagged the document.during this process, we regularly tracked inter annotator agreement (see section 4.2).
</prevsent>
<prevsent>in the final phase, the annotation team concentrated on ways to reduce differences by adopting some heuristics for handling higher levels of the discourse structure.
</prevsent>
</prevsection>
<citsent citstr=" P99-1032 ">
wiebe et al  (1999) <papid> P99-1032 </papid>present method for automatically formulating single best tag when multiple judges disagree on selecting between binary features.</citsent>
<aftsection>
<nextsent>because our annotators had to select among multiple choices at each stage of the discourse annotation process, and because decisions made at one stage influenced the decisions made during subsequent stages, we could not apply wiebe et al .s method.
</nextsent>
<nextsent>our methodology for determining the best?
</nextsent>
<nextsent>guidelines was much more of consensus-building process, taking into consideration multiple factors at each step.
</nextsent>
<nextsent>the final tagging manual, over 80 pages in length, contains extensive examples from the corpus to illustrate text segmentation, nuclearity, selection of relations, and discourse cues.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1151">
<title id=" W01-1605.xml">building a discourse tagged corpus in the framework of rhetorical structure theory </title>
<section> quality assurance.  </section>
<citcontext>
<prevsection>
<prevsent>4.2 measuring consistency.
</prevsent>
<prevsent>we tracked inter-annotator agreement during each phase of the project, using method developed by marcu et al  (1999) <papid> W99-0307 </papid>for computing kappa statistics over hierarchical structures.</prevsent>
</prevsection>
<citsent citstr=" J97-1002 ">
the kappa coefficient (siegel and castel lan, 1988) has been used extensively in previous empirical studies of discourse (carletta et al , 1997; <papid> J97-1002 </papid>flammia and zue, 1995; passonneau and litman, 1997).<papid> J97-1005 </papid></citsent>
<aftsection>
<nextsent>it measures pairwise agreement among set of coders who make category judgments, correcting for chance expected agreement.
</nextsent>
<nextsent>the method described in marcu et al .
</nextsent>
<nextsent>(1999) maps hierarchical structures into sets of units that are labeled with categorial judgments.
</nextsent>
<nextsent>the strengths and shortcomings of the approach are also discussed in detail there.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1152">
<title id=" W01-1605.xml">building a discourse tagged corpus in the framework of rhetorical structure theory </title>
<section> quality assurance.  </section>
<citcontext>
<prevsection>
<prevsent>4.2 measuring consistency.
</prevsent>
<prevsent>we tracked inter-annotator agreement during each phase of the project, using method developed by marcu et al  (1999) <papid> W99-0307 </papid>for computing kappa statistics over hierarchical structures.</prevsent>
</prevsection>
<citsent citstr=" J97-1005 ">
the kappa coefficient (siegel and castel lan, 1988) has been used extensively in previous empirical studies of discourse (carletta et al , 1997; <papid> J97-1002 </papid>flammia and zue, 1995; passonneau and litman, 1997).<papid> J97-1005 </papid></citsent>
<aftsection>
<nextsent>it measures pairwise agreement among set of coders who make category judgments, correcting for chance expected agreement.
</nextsent>
<nextsent>the method described in marcu et al .
</nextsent>
<nextsent>(1999) maps hierarchical structures into sets of units that are labeled with categorial judgments.
</nextsent>
<nextsent>the strengths and shortcomings of the approach are also discussed in detail there.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1155">
<title id=" W01-1605.xml">building a discourse tagged corpus in the framework of rhetorical structure theory </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>a growing number of groups have developed or are developing discourse-annotated corpora for text.
</prevsent>
<prevsent>these can be characterized both in terms of the kinds of features annotated as well as by the scope of the annotation.
</prevsent>
</prevsection>
<citsent citstr=" M95-1002 ">
features may include specific discourse cues or markers, coreference links, identification of rhetorical relations, etc. the scope of the annotation refers to the levels of analysis within the document, and can be characterized as follows: ? sentential: annotation of features at the intra-sentential or inter-sentential level, at single level of depth (sundheim, 1995; <papid> M95-1002 </papid>tsou et al , 2000; <papid> W00-1206 </papid>nomoto and matsumoto, 1999; <papid> W99-0620 </papid>rebeyrolle, 2000).</citsent>
<aftsection>
<nextsent>hierarchical: annotation of features at multiple levels, building upon lower levels of analysis at the clause or sentence level (moser and moore, 1995; <papid> P95-1018 </papid>marcu, et al  1999) ? <papid> W99-0307 </papid>document-level: broad characterization of document structure such as identification of topical segments (hearst, 1997), <papid> J97-1003 </papid>linking of large text segments via specific relations (ferrari, 1998; rebeyrolle, 2000), or defining text objects with text architecture (pery-woodley and rebeyrolle, 1998).</nextsent>
<nextsent>developing corpora with these kinds of rich annotation is labor-intensive effort.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1164">
<title id=" W01-0704.xml">semantic pattern learning through maximum entropy based wsd technique </title>
<section> full parsing.  </section>
<citcontext>
<prevsection>
<prevsent>on the other hand, wsd tool must provide the correct sense in order to ensure the appropriate selection of the onto logical concept associated toeach word.
</prevsent>
<prevsent>finally, with the parsing and the correct sense of each word, the pattern extraction method will form and store onto logical pairs that define the semantic behaviour of each sentence.
</prevsent>
</prevsection>
<citsent citstr=" A97-1011 ">
the analyzer used for this work is the con exors fdg parser (pasi tapanainen and timo jarvinen,1997).<papid> A97-1011 </papid></citsent>
<aftsection>
<nextsent>this parser tries to provide build dependency tree from the sentence.
</nextsent>
<nextsent>when this is not possible, the parser tries to build partial trees that often result from unresolved ambiguity.
</nextsent>
<nextsent>one visual example of this dependency trees is shown in figure 1 where the parsing tree of sentence (1) is illustrated.
</nextsent>
<nextsent>(1) the minister gave explanations to the government.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1165">
<title id=" W01-0704.xml">semantic pattern learning through maximum entropy based wsd technique </title>
<section> wsd based on maximum entropy.  </section>
<citcontext>
<prevsection>
<prevsent>for each word, the training set is divided in 10 folds, 9 for training and 1 for evaluation; ten tests were accomplished using different fold for evaluation in each one (10-fold cross-validation).
</prevsent>
<prevsent>the accuracy results are the average accuracy on the ten tests for word.results comparison with previous work is difficult because there is different approaches to the wsd task (knowledge based methods, supervised and unsupervised statistical methods...)
</prevsent>
</prevsection>
<citsent citstr=" P99-1020 ">
(mihal cea and moldovan, 1999) <papid> P99-1020 </papid>and many of them focus on different set of words and sense definitions.furthermore, the training corpus seems to be critical to the application of the learning to specific ocurrences accuracy standard deviation age,n 48,2 0,584 0,134 art,n 38,0 0,623 0,090 car,n 136,7 0,963 0,048 child,n 105,1 0,809 0,073 church,n 35,8 0,625 0,126 cost,n 143,2 0,895 0,051 fall,v 143,7 0,759 0,242 head,n 83,3 0,714 0,125 interest,n 147,8 0,619 0,173 know,v 143,3 0,421 0,087 line,n 132,8 0,529 0,154 set,v 126,1 0,537 0,139 speak,v 51,1 0,729 0,080 take,v 138,0 0,264 0,042 work,n 118,9 0,530 0,175 overall 0,637 table 1: evaluation results from dso-wsj domain (escudero et al, 2000b).</citsent>
<aftsection>
<nextsent>in the experiment presented here, the selection of the target words and the corpus used are thesame that (escudero et al, 2000a) where boosting method is proposed.
</nextsent>
<nextsent>in this paper comparison between some wsd methods is shown.
</nextsent>
<nextsent>boosting is the most successful method with a68.1 % accuracy.
</nextsent>
<nextsent>our method obtains lower accuracy but this is first implementation and better feature selection is expected to improve our results.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1166">
<title id=" W01-0704.xml">semantic pattern learning through maximum entropy based wsd technique </title>
<section> applying the method to anaphora.  </section>
<citcontext>
<prevsection>
<prevsent>he ( was hungry?, the pronoun he is the anaphor and it refers to the antecedent john.traditionally, some of the most relevant approaches to solve anaphora have been those called poor-knowledge approaches.
</prevsent>
<prevsent>they use limited knowledge (lexical, morphological and syntactic information sources) for the detection of the correct antecedent.
</prevsent>
</prevsection>
<citsent citstr=" P98-2143 ">
these proposals have report high success rates for english (89.7%) (mitkov, 1998) <papid> P98-2143 </papid>and for spanish (83%) (ferrandez et al, 1999).taking this basis, it is possible to improve there sults of resolution method adding other sources such us semantic, pragmatic, world-knowledge or indeed statistical information.we have explored the use of semantic information extracted from an ontology and its application to the anaphora resolution proccess.</citsent>
<aftsection>
<nextsent>this additional source has give good results on restricted texts (azzam et al, 1998).
</nextsent>
<nextsent>nevertheless, its application on unrestricted texts has not been so satisfactory, mainly due to the lack of adequate and available lexical resources.
</nextsent>
<nextsent>due to this, we consider that the pattern learning can complement the semantic source in order to establish additional criteria in the antecedent selection.
</nextsent>
<nextsent>in addition, we believe that an adequate selection of pattern scan improve the success rate in anaphora resolution on unrestricted texts.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1167">
<title id=" W01-1623.xml">toward a large spontaneous mandarin dialogue corpus </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>it is now well known that what differentiates written texts from spontaneous speech most is the use of discourse particles.
</prevsent>
<prevsent>among the core words, eleven words were discourse particles, or they were used as discourse markers.
</prevsent>
</prevsection>
<citsent citstr=" J93-3003 ">
in the literature, there is still no consistent definition for discourse markers (hirschberg and litman 1993).<papid> J93-3003 </papid></citsent>
<aftsection>
<nextsent>discourse markers can be defined as follows: elements whose original semantic meaning tends to decrease and their use in spoken discourse becomes more pragmatic and indicative of discourse structuring are discourse markers.
</nextsent>
<nextsent>in addition to several adverbs and determiners, discourse particles can also be categorized as discourse markers.
</nextsent>
<nextsent>they are very often observed in mandarin spoken conversations as mentioned in tseng (2001) and clancy et al (1996).
</nextsent>
<nextsent>in tseng (2001), each subject used on average 1.6 discourse particles per turn.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1168">
<title id=" W01-1623.xml">toward a large spontaneous mandarin dialogue corpus </title>
<section> mandarin conversational dialogue corpus.  </section>
<citcontext>
<prevsection>
<prevsent>they are daily conversations with specific topics given for each stage of recording.
</prevsent>
<prevsent>since the design of scenario aims to collect natural and spontaneous conversations, limitations on the topics are reduced to minimum.
</prevsent>
</prevsection>
<citsent citstr=" H89-2017 ">
different from task-oriented dialogues such as air-planning or instruction-construction tasks (kowtko and price 1989, <papid> H89-2017 </papid>sagerer et al 1994), subjects participating in this project were told to converse as naturally as possible.</citsent>
<aftsection>
<nextsent>the scenario is similar to situation where two strangers meet at the first time, try to find common topics interested by both of them and have chat.
</nextsent>
<nextsent>figure 1: corpus domain design topic 1 what-where topic 2 topic 3 what-when topic 5 topic 4 what-who where-how what-why as illustrated in figure 1, this stage of corpus collection consists of what-where component.
</nextsent>
<nextsent>the subjects have to determine on what topic theyd like to talk.
</nextsent>
<nextsent>usually, they do not stick to only one topic.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1169">
<title id=" W01-1623.xml">toward a large spontaneous mandarin dialogue corpus </title>
<section> an extensible transcription tool.  </section>
<citcontext>
<prevsection>
<prevsent>thus, whether it is turn-taking or it is overlapping can be evaluated by means of the third parameter time (msec).
</prevsent>
<prevsent>with respect to tags added into the transcribed segments, it is optional to include or 1  1 6473  from city want go city gallery in the case we just can just you en just from our school too is same take 236 most convenient prt  /a 1 16200   1 16230  mhm  /b 1 16380   2 16530  afterwards take to too to gong guan change danshui-line afterwards you just take to approximately yuanshan-station  /a 2 22230  to exclude the annotation tags.
</prevsent>
</prevsection>
<citsent citstr=" J99-4003 ">
as shown in figure 2, these can be non-speech sounds, repairs or discourse markers (heeman and allen 1999).<papid> J99-4003 </papid></citsent>
<aftsection>
<nextsent>conclusion this paper discussed general issues on mandarin spoken dialogues and analysed components of new developed transcription and annotation tool for spoken mandarin.
</nextsent>



</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1170">
<title id=" W02-0705.xml">speech translation performance of statistical dependency transduction and semantic similarity transduction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>hybrid solutions combining these approaches have also been used in language processing generally (klavans and resnik, 1996) andmore specifically in machine translation (for example frederking et al (1994)).
</prevsent>
<prevsent>in this paper we compare the performance of two methods for speech translation.
</prevsent>
</prevsection>
<citsent citstr=" J00-1004 ">
one is the statistical dependency transduction model (alshawi and douglas, 2000; alshawi et al, 2000<papid> J00-1004 </papid>b), trainable generative statistical translation model using head transducers (alshawi, 1996).</citsent>
<aftsection>
<nextsent>the other is case-based transduction model which makes use of semantic similarity measure between words.
</nextsent>
<nextsent>both models are trained automatically using examples of translated utterances (the transcription of spoken utterance and translation of that transcription).
</nextsent>
<nextsent>the case based model makes use of additional information in the form of labels associated with source language utterances, typically one or two labels per utterance.
</nextsent>
<nextsent>this additional information, which was originally provided for separate monolingual task, is used to construct the lexical similarity measure.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1182">
<title id=" W02-0705.xml">speech translation performance of statistical dependency transduction and semantic similarity transduction </title>
<section> hierarchical alignments.  </section>
<citcontext>
<prevsection>
<prevsent>2.1 word correlation statistics.
</prevsent>
<prevsent>for each source word in the dataset, translation pairing cost   is assigned for all possible translations in the context of bitext  . here  and are usually words, but may also be the empty word  or compounds formed from contiguous words; here we restrict compounds to maximum length of two words.the assignment of these lexical translation pairing costs may be done using various statistical measures.
</prevsent>
</prevsection>
<citsent citstr=" H91-1026 ">
the main component of  is the so-called  correlation measure (see gale and church (1991))<papid> H91-1026 </papid>normalized to the range ff fiflffi!</citsent>
<aftsection>
<nextsent>with fi indicating perfect correlation.
</nextsent>
<nextsent>in the experiments described in this paper, the cost function  relating source word (orcompound)  in bitext with target word (or com pound) is   # %$    fl  &amp;)(  # where (* + is length-normalized measure of the apparent distortion in the positions of  and in the source and target strings of  . for example, if appears at the middle of the source string and appears at the middle of the target string, then the distortion is fi . we have found that, at least for our data, this pairing cost leads to better performance than the use of log probabilities of target words given source words (cf.
</nextsent>
<nextsent>brown et al (1993)).<papid> J93-2003 </papid></nextsent>
<nextsent>the value used for    is first computed from counts of the number of bitexts in the training set in which  and co-occur, in which  only appears, in which only appears, and in which neither of them appear.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1183">
<title id=" W02-0705.xml">speech translation performance of statistical dependency transduction and semantic similarity transduction </title>
<section> hierarchical alignments.  </section>
<citcontext>
<prevsection>
<prevsent>with fi indicating perfect correlation.
</prevsent>
<prevsent>in the experiments described in this paper, the cost function  relating source word (orcompound)  in bitext with target word (or com pound) is   # %$    fl  &amp;)(  # where (* + is length-normalized measure of the apparent distortion in the positions of  and in the source and target strings of  . for example, if appears at the middle of the source string and appears at the middle of the target string, then the distortion is fi . we have found that, at least for our data, this pairing cost leads to better performance than the use of log probabilities of target words given source words (cf.
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
brown et al (1993)).<papid> J93-2003 </papid></citsent>
<aftsection>
<nextsent>the value used for    is first computed from counts of the number of bitexts in the training set in which  and co-occur, in which  only appears, in which only appears, and in which neither of them appear.
</nextsent>
<nextsent>in other words, we first treat any word in the target string to be possible translation of any word in the source string.
</nextsent>
<nextsent>this value is then refined by re-estimation during the alignment optimization process.
</nextsent>
<nextsent>2.2 optimal hierarchical alignments.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1189">
<title id=" W02-0705.xml">speech translation performance of statistical dependency transduction and semantic similarity transduction </title>
<section> similarity cased-based transduction.  </section>
<citcontext>
<prevsection>
<prevsent>and x| ~{7 , are those for the probability x8y  that source string which includes word  has been assigned label  . the similarity measure yqwo-{7 is computed from the relative entropy (kullback leibler distance (kullback and leibler, 1951)) between these distributions.
</prevsent>
<prevsent>to make the similarity measure symmetrical, i.e. yq o-{7 $yq-{h o!
</prevsent>
</prevsection>
<citsent citstr=" P93-1024 ">
, we take the average of two relative entropy quantities: yq  { $ ffi7gh xa? x|  yny x|  {  ?&amp; a? x| -{7 yny x|}y  o   of course, this is one of many different possible similarity measures which could have been used (cfpereira et al (1993)), <papid> P93-1024 </papid>including ones that do not depend on additional labels.</citsent>
<aftsection>
<nextsent>however, since semantic labels had already been assigned to our training data, the distributions seemed like convenient rough proxy for the semantic similarity of words in this limited domain.
</nextsent>
<nextsent>4.2 case-based transduction procedure.
</nextsent>
<nextsent>basically, the transduction procedure (i) finds an instance vv]t8 of the translation training pairs for which the example source string provides the best?
</nextsent>
<nextsent>match to the input source string ? , and (ii)produces, as the translation output, modified version of the example target string , where the modifications reflect mismatches between and the input.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1190">
<title id=" W01-1312.xml">a multilingual approach to annotating and extracting temporal information </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our annotation scheme, described in detail in (ferro et al 2000), has several novel features, including the following: it goes well beyond the one used in the message understanding conference (muc7 1998), not only in terms of the range of expressions that are flagged, but, also, more importantly, in terms of representing and normalizing the time values that are communicated by the expressions.
</prevsent>
<prevsent>in addition to handling fully-specified time expressions (e.g., september 3rd, 1997), it also handles context-dependent expressions.
</prevsent>
</prevsection>
<citsent citstr=" P00-1010 ">
this is significant because of the ubiquity of context dependent time expressions; recent corpus study (mani and wilson 2000) <papid> P00-1010 </papid>revealed that more than two-thirds of time expressions in print and broadcast news were context-dependent ones.</citsent>
<aftsection>
<nextsent>the context can be local (within the same sentence), e.g., in 1995, the months of june and july were devilishly hot, or global (outside the sentence), e.g., the hostages were beheaded thatafternoon.
</nextsent>
<nextsent>a subclass of these context dependent expressions are indexical?
</nextsent>
<nextsent>expressions, which require knowing when the speaker is speaking to determine the intended time value, e.g., now, today, yesterday, tomorrow, next tuesday, two weeks ago, etc. the annotation scheme has been designed to meet the following criteria: ? simplicity with precision: we have tried to keep the scheme simple enough to be executed confidently by humans, and yet precise enough for use in various natural language processing tasks.
</nextsent>
<nextsent>naturalness: we assume that the annotation scheme should reflect those distinctions that human could be expected to reliably annotate, rather than reflecting an artificially-defined smaller set of distinctions that automated systems might be expected to make.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1192">
<title id=" W01-1312.xml">a multilingual approach to annotating and extracting temporal information </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent> timex2 val= 2001-w12  la semana pasada /timex2   timex2 val= 2001-w12  last week /timex2
</prevsent>
<prevsent>our scheme differs from the recent scheme of(setzer and gaizauskas 2000) in terms of our indepth focus on representations for the values of specific classes of time expressions, and in the application of our scheme to variety of different genres, including print news, broadcast news, and meeting scheduling dialogs.
</prevsent>
</prevsection>
<citsent citstr=" A97-1007 ">
others have used temporal annotation schemes for the much more constrained domain of meeting scheduling, e.g., (wiebe et al 1998), (alexandersson et al 1997), (<papid> A97-1007 </papid>busemann et al 1997).<papid> A97-1006 </papid></citsent>
<aftsection>
<nextsent>our scheme has been applied to such domains as well, our annotation of the enthusiast corpus being an example.
</nextsent>
<nextsent>in the future, we hope to extend our english annotation guidelines into set of multilingual annotation guidelines, which would include language-specific supplements specifying examples, tokenization rules, and rules for determining tag extents.
</nextsent>
<nextsent>to support development of such guidelines, we expect to develop large keyword-in-context concordances, and would like to use the time-tagger system as tool in that effort.
</nextsent>
<nextsent>our approach would be (1) to run the tagger over the desired text corpora; (2) to run the concordance creation utility over the annotated version of the same corpora, using not only timex2 tags but also lexical trigger words as input criteria; and (3) to partition the output of the creation utility into entries that are tagged as temporal expressions and entries that are not so tagged.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1193">
<title id=" W01-1312.xml">a multilingual approach to annotating and extracting temporal information </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent> timex2 val= 2001-w12  la semana pasada /timex2   timex2 val= 2001-w12  last week /timex2
</prevsent>
<prevsent>our scheme differs from the recent scheme of(setzer and gaizauskas 2000) in terms of our indepth focus on representations for the values of specific classes of time expressions, and in the application of our scheme to variety of different genres, including print news, broadcast news, and meeting scheduling dialogs.
</prevsent>
</prevsection>
<citsent citstr=" A97-1006 ">
others have used temporal annotation schemes for the much more constrained domain of meeting scheduling, e.g., (wiebe et al 1998), (alexandersson et al 1997), (<papid> A97-1007 </papid>busemann et al 1997).<papid> A97-1006 </papid></citsent>
<aftsection>
<nextsent>our scheme has been applied to such domains as well, our annotation of the enthusiast corpus being an example.
</nextsent>
<nextsent>in the future, we hope to extend our english annotation guidelines into set of multilingual annotation guidelines, which would include language-specific supplements specifying examples, tokenization rules, and rules for determining tag extents.
</nextsent>
<nextsent>to support development of such guidelines, we expect to develop large keyword-in-context concordances, and would like to use the time-tagger system as tool in that effort.
</nextsent>
<nextsent>our approach would be (1) to run the tagger over the desired text corpora; (2) to run the concordance creation utility over the annotated version of the same corpora, using not only timex2 tags but also lexical trigger words as input criteria; and (3) to partition the output of the creation utility into entries that are tagged as temporal expressions and entries that are not so tagged.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1194">
<title id=" W02-0213.xml">dialogue act recognition with bayesian networks for dutch dialogues </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>figure 1: karin in the vmc.
</prevsent>
<prevsent>an important part of our dialogue systems for natural language interaction with agents is the module for recognition of the dialogue acts performed by the human user (visitor).
</prevsent>
</prevsection>
<citsent citstr=" J00-3003 ">
this paper discusses the construction of and experiments with bayesian networks as implementation of this module.various other work has been presented on using statistical techniques for dialogue act classification (andernach, 1996; stolcke et al, 2000),<papid> J00-3003 </papid>and even some first efforts on using bayesian networks for this task (pulman, 1996; keizer, 2001).other work on using bayesian networks india logue systems aims more at interaction and user modelling (paek and horvitz, 2000) and does not specifically involve linguistic aspects.</citsent>
<aftsection>
<nextsent>the paper is organised as follows.
</nextsent>
<nextsent>section 2 philadelphia, july 2002, pp.
</nextsent>
<nextsent>88-94.
</nextsent>
<nextsent>association for computational linguistics.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1195">
<title id=" W01-1614.xml">empirical methods for evaluating dialog systems </title>
<section> purpose.  </section>
<citcontext>
<prevsection>
<prevsent>detailing its internal processing.
</prevsent>
<prevsent>if one metric fails to suffice, dialog metrics can be combined.
</prevsent>
</prevsection>
<citsent citstr=" P97-1035 ">
for example, the paradise framework allows designers to predict user satisfaction from linear combination of objective metrics such as mean recognition score and task completion (kamm et al., 1999; litman &amp; pan, 1999; walker et al, 1997).<papid> P97-1035 </papid></citsent>
<aftsection>
<nextsent>why so many metrics?
</nextsent>
<nextsent>the answer has to do with more than just the absence of agreed upon standards in the research community, notwithstanding significant efforts in that direction (gibbon et al, 1997).
</nextsent>
<nextsent>part of the reason deals with the purpose dialog metric serves.
</nextsent>
<nextsent>designers want dialog metric to address the multiple, sometimes inconsistent needs.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1196">
<title id=" W02-0111.xml">lexicalized grammar 101 </title>
<section> taglet.  </section>
<citcontext>
<prevsection>
<prevsent>for complementation, taglet adopts tags substitution operation; substitution replacesa leaf node in the head tree with the phrase structure tree associated with the complement.
</prevsent>
<prevsent>see figure 1.
</prevsent>
</prevsection>
<citsent citstr=" P95-1021 ">
for modification, taglet adopts the the sister-adjunction operation defined in (rambow et al ., 1995); <papid> P95-1021 </papid>sister-adjunction just adds the modifier subtree as child of an existing node in the headtreeeither on the left of the head (forward sister adjunction) as in figure 2, or on the right of the head (backward sister-adjunction).</citsent>
<aftsection>
<nextsent>i describe taglet formally in appendix a. taglet is equivalent in weak generative powerto context-free grammar.
</nextsent>
<nextsent>that is, any language defined by taglet al so has cfg, and any language defined by cfg also has taglet.
</nextsent>
<nextsent>on the other hand context-free languages can have derivations in which all lexical items are arbitrarily far from the root; taglet derived structures alway shave an anchor whose path to the root of the sentence has fixed length given by grammatical element.
</nextsent>
<nextsent>see appendix b. the restriction seems of little linguistic significance, since any tree-bank parse induces unique taglet grammar once you label which child of each node is the head, which are complements and which are modifiers.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1197">
<title id=" W02-0111.xml">lexicalized grammar 101 </title>
<section> taglet.  </section>
<citcontext>
<prevsection>
<prevsent>see appendix b. the restriction seems of little linguistic significance, since any tree-bank parse induces unique taglet grammar once you label which child of each node is the head, which are complements and which are modifiers.
</prevsent>
<prevsent>indeed, sincetaglet thus induces bigram dependency structures from trees, this invites the estimation of probability distributions on taglet derivations based on np chris   h np vp   h loves np np sandy vp* advp madly figure 3: parallel analysis in taglet and tag.
</prevsent>
</prevsection>
<citsent citstr=" P00-1058 ">
observed bigram dependencies; see (chiang, 2000).<papid> P00-1058 </papid></citsent>
<aftsection>
<nextsent>to implement an effective taglet generator,you can perform greedy head-first search of derivations guided by heuristic progress toward achieving communicative goals (stone et al , 2001).
</nextsent>
<nextsent>mean while, because taglet is context-free, you can easily write cky-style dynamic programming parser that stores structures recognized for spans of text in chart, and iteratively combines structures in adjacent spans until the analyses span the entire sentence.
</nextsent>
<nextsent>(more complexity would be required formultiply-anchored trees, as they induce discontinuous constituents.)
</nextsent>
<nextsent>the simple requirement that operations never apply inside complements or modifiers, and apply left-to-right within head, suffices to avoid spurious ambiguity.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1198">
<title id=" W02-0111.xml">lexicalized grammar 101 </title>
<section> taglet.  </section>
<citcontext>
<prevsection>
<prevsent>consider the trees for he knows below: np [ nm sg cs ] /he/     h h np [ nm cs ] vp [ nm ] /know/ when these trees combine, we can immediately unify the number of the verb with the pronouns singular; we can immediately unify the case of the pronoun with the nominative assigned by the verb:     h h np [ nm sg cs ] /he/ vp [ nm sg ] /know/ the feature values will be preserved by further steps of derivation.
</prevsent>
<prevsent>4.3 building on taglet.
</prevsent>
</prevsection>
<citsent citstr=" P97-1026 ">
semantics and pragmatics are crucial to nlp.taglet lets students explore meaty issues in semantics and pragmatics, using the unification-based semantics proposed in (stone and doran, 1997).<papid> P97-1026 </papid></citsent>
<aftsection>
<nextsent>we view constituents as referential, or better, indexical; we link elementary trees with constraints on these indices and conjoin the constraints in the meaning of compound structure.
</nextsent>
<nextsent>this example shows how the strategy depends on rich ontology: s:e     h h np:c chris vp:e      h h v:e loves np:s sandy advp:e madly chris(c)^ sandy(s)^ love(e,c,s)^mad(e) the example also shows how the strategy lets us quickly implement, say, the constraint-satisfactionapproaches to reference resolution or the plan recognition approaches to discourse integration described in (stone and webber, 1998).<papid> W98-1419 </papid></nextsent>
<nextsent>4.4 lectures and assignments.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1199">
<title id=" W02-0111.xml">lexicalized grammar 101 </title>
<section> taglet.  </section>
<citcontext>
<prevsection>
<prevsent>semantics and pragmatics are crucial to nlp.taglet lets students explore meaty issues in semantics and pragmatics, using the unification-based semantics proposed in (stone and doran, 1997).<papid> P97-1026 </papid></prevsent>
<prevsent>we view constituents as referential, or better, indexical; we link elementary trees with constraints on these indices and conjoin the constraints in the meaning of compound structure.</prevsent>
</prevsection>
<citsent citstr=" W98-1419 ">
this example shows how the strategy depends on rich ontology: s:e     h h np:c chris vp:e      h h v:e loves np:s sandy advp:e madly chris(c)^ sandy(s)^ love(e,c,s)^mad(e) the example also shows how the strategy lets us quickly implement, say, the constraint-satisfactionapproaches to reference resolution or the plan recognition approaches to discourse integration described in (stone and webber, 1998).<papid> W98-1419 </papid></citsent>
<aftsection>
<nextsent>4.4 lectures and assignments.
</nextsent>
<nextsent>here is plan for six-week taglet module.
</nextsent>
<nextsent>the first two weeks introduce data structures and recursive programming in prolog, with examples drawn from phrase structure trees and syntactic combi nation; and discuss dynamic-programming parsers, with an aside on convenient implementation using prolog assertion.
</nextsent>
<nextsent>as homework, students implement simple tree operations, and build up to definitions of substitution and modification for parsing and gener ation; they use these combinatory operations to write cky taglet parser.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1200">
<title id=" W02-0111.xml">lexicalized grammar 101 </title>
<section> conclusion.  </section>
<citcontext>
<prevsection>
<prevsent>versions of tag and combinatory categorial grammars (ccg) (steedman, 2000), as presented in the literature, require complex bookkeeping for effective computation.
</prevsent>
<prevsent>when wrote ccg parser as an undergraduate, it took me whole semester to get an implemented handle onthe meta theory that governs the interaction of (crossing) composition or type-raising with spurious am biguity; still have never written tag parser or ccg generator.
</prevsent>
</prevsection>
<citsent citstr=" J95-4002 ">
variants of tag like tig (schabes and waters, 1995) <papid> J95-4002 </papid>or d-tree grammars (rambow et al , 1995) <papid> P95-1021 </papid>are motivated by linguistic or formal considerations rather than pedagogical or computational ones.</citsent>
<aftsection>
<nextsent>other formalisms come with linguistic assumptions that are hard to manage.
</nextsent>
<nextsent>link grammar (sleator and temperley, 1993) and other pure dependency formalisms can make it difficult to explore rich hierarchical syntax and the flexibility of modification; hpsg (pollard and sag, 1994) comes with commitment to its complex, rather bewildering regime for formalizing linguistic information as feature structures.
</nextsent>
<nextsent>of course, you probably could refine any of these theories to simple coreand would get something very like taglet.
</nextsent>
<nextsent>i strongly believe that this distillation is worth the trouble, because lexicalization ties grammar formalisms so closely to the motivations for studying language in the first place.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1202">
<title id=" W00-1012.xml">dialogue management in the agreement negotiation process a model that involves natural reasoning </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>an information sharing dialogue is started, when the agent recognised turn of his/her partner as proposal, but does not have enough information to decide whether to accept it or not.
</prevsent>
<prevsent>a negotiation dialogue is started, when the agent concludes that the proposal is in conflict with his/her beliefs and preferences, i.e. tends to reject it.
</prevsent>
</prevsection>
<citsent citstr=" J95-3003 ">
heeman and hirst (1995) <papid> J95-3003 </papid>model cooperation by the cycle present-judge-refashion.</citsent>
<aftsection>
<nextsent>they use two levels of modelling - planning and cooperation.
</nextsent>
<nextsent>on the first level utterances are generated and interpreted, on the second level the cooperation of agents is modelled, relating it to agent mental states and planning processes.
</nextsent>
<nextsent>the shared plans cooperation model deals with planning processes in which participate multiple agents, see lochbaurn (1998).
</nextsent>
<nextsent>the model concentrates on group tasks that can be divided into separate, but interacting subtasks, and the central problem is coordination of intentions and goals of partners.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1203">
<title id=" W00-1012.xml">dialogue management in the agreement negotiation process a model that involves natural reasoning </title>
<section> dialogue management.  </section>
<citcontext>
<prevsection>
<prevsent>in the same way, some obligations (prohibitions) can be stronger than others, depending on the weight of the corresponding punishment.
</prevsent>
<prevsent>i should be mentioned that adding obligations to the standard bdi model is not new.
</prevsent>
</prevsection>
<citsent citstr=" P94-1001 ">
traum and allen (1994) <papid> P94-1001 </papid>show how discourse obligations can be used to account in natural manner for the connection between question and its answer in dialogue and how obligations can be used along with other parts of the discourse context to extend the coverage of dialogue system.</citsent>
<aftsection>
<nextsent>2.2 communicative strategies and.
</nextsent>
<nextsent>tactics knowledge about dialogue kbd, which is used by the dialogue manager, consists of two functional parts: knowledge of the regularities of dialogue, and rules of constructing and combining speech acts.
</nextsent>
<nextsent>the top level concept of dialogue rules in our model is communicative strategy.
</nextsent>
<nextsent>this concept is reserved for such basic communication types as information exchange, directive dialogue, 105 phatic communication, etc. on the more concrete level, the conversation agent can realise communicative strategy by means of several communicative tactics; this concept more closely corresponds to the: concept of communicative strategy as us~l in some other approaches, ee e.g. jokinen (1996).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1204">
<title id=" W01-0806.xml">an algorithm for efficiently generating summary paragraphs using tree adjoining grammar </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>also, it identifies an explicit mapping between input propositions and their possible surface realisations.
</prevsent>
<prevsent>summarisation of simply structured data as short natural language paragraphs has recently been focus of interest.
</prevsent>
</prevsection>
<citsent citstr=" W98-1415 ">
shaw (1998) <papid> W98-1415 </papid>and bental et al  (1999) looked at generating text from database records.</citsent>
<aftsection>
<nextsent>robin and mckeown (1996) summarised quantitative data.
</nextsent>
<nextsent>shaws examples were drawn from patient medical records; bentalet al from online resource cataloging information.
</nextsent>
<nextsent>a requirement common to all these studies has been to produce aggregated (reape and mellish, 1999) text.
</nextsent>
<nextsent>also in all these studies, the structure of the input data used was fairly flat.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1209">
<title id=" W01-0806.xml">an algorithm for efficiently generating summary paragraphs using tree adjoining grammar </title>
<section> searching for concise, coherent.  </section>
<citcontext>
<prevsection>
<prevsent>stylistic constraints include: maureen ryff wrote ...?
</prevsent>
<prevsent>is preferable to ?... was written by maureen ryff?.
</prevsent>
</prevsection>
<citsent citstr=" P97-1026 ">
we suggest, as do stone and doran (1997), <papid> P97-1026 </papid>that integrating these constraints simultaneously is more efficient thenpipelining them.</citsent>
<aftsection>
<nextsent>we additionally suggest that representing these constraints in unified form can provide further efficiency gains.constellations?
</nextsent>
<nextsent>is 4-hour lesson plan published by online provider prolog.
</nextsent>
<nextsent>maureen ryff wrote it for small group teaching.
</nextsent>
<nextsent>figure 1: paragraph which summarises the set of fields of figure 3 in an aggregated manner.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1210">
<title id=" W01-0712.xml">learning computational grammars </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>com, thollard@sfs.nphil.uni-tuebingen.de university college dublin, james.hammerton@ucd.ie university of antwerp, erikt@uia.ua.ac.bea more systematic survey to understand the relevance of many factors to the success of learning, esp. the availability of annotated data, the kind of dependencies in the data, and the availability of knowledge bases (grammars).
</prevsent>
<prevsent>we focused on syntax, esp. noun phrase (np) syntax from the beginning.
</prevsent>
</prevsection>
<citsent citstr=" W00-0702 ">
the industrial partner, xerox, focused on more immediate applications (cancedda and samuelsson, 2000).<papid> W00-0702 </papid>the network was focused not only by its scientific goal, the application and evaluation ofmachine-learning techniques as used to learn natural language syntax, and by the subarea of syntax chosen, np syntax, but also by the use of shared training and test material, in this case material drawn from the penn treebank.</citsent>
<aftsection>
<nextsent>finally, we were curious about the possibility of combining different techniques, including those from statistical and symbolic machine learning.
</nextsent>
<nextsent>the network members played an important role in the organisation of three open workshops in which several external groups participated, sharing data and test materials.
</nextsent>
<nextsent>this section starts with description of the three tasks that we have worked on in the framework ofthis project.
</nextsent>
<nextsent>after this we will describe thema chine learning algorithms applied to this data and conclude with some notes about combining different system results.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1211">
<title id=" W01-0712.xml">learning computational grammars </title>
<section> method.  </section>
<citcontext>
<prevsection>
<prevsent>the process of finding these phrases is called chunking.
</prevsent>
<prevsent>the project provided dataset for this task at the conll-2000 workshop (tjong kim sang and buchholz, 2000)1.
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
it consists of sections 15-18 of the wall street journal part of the penn treebank ii (marcus et al, 1993) <papid> J93-2004 </papid>as training data (211727 tokens) and section 20 as test data (47377 tokens).</citsent>
<aftsection>
<nextsent>a specialised version of the chunking task is np chunking or basenp identification in which the goal is to identify the base noun phrases.
</nextsent>
<nextsent>the first work on this topic was done back in the eighties (church, 1988).<papid> A88-1019 </papid></nextsent>
<nextsent>the dataset that has become standard for evaluation machine learning approaches is the one first used by ramshaw and marcus (1995).<papid> W95-0107 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1212">
<title id=" W01-0712.xml">learning computational grammars </title>
<section> method.  </section>
<citcontext>
<prevsection>
<prevsent>it consists of sections 15-18 of the wall street journal part of the penn treebank ii (marcus et al, 1993) <papid> J93-2004 </papid>as training data (211727 tokens) and section 20 as test data (47377 tokens).</prevsent>
<prevsent>a specialised version of the chunking task is np chunking or basenp identification in which the goal is to identify the base noun phrases.</prevsent>
</prevsection>
<citsent citstr=" A88-1019 ">
the first work on this topic was done back in the eighties (church, 1988).<papid> A88-1019 </papid></citsent>
<aftsection>
<nextsent>the dataset that has become standard for evaluation machine learning approaches is the one first used by ramshaw and marcus (1995).<papid> W95-0107 </papid></nextsent>
<nextsent>it consists of the same training and test data segments of the penn treebank as the chunking task (respectively sections 15-18 and section 20).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1213">
<title id=" W01-0712.xml">learning computational grammars </title>
<section> method.  </section>
<citcontext>
<prevsection>
<prevsent>a specialised version of the chunking task is np chunking or basenp identification in which the goal is to identify the base noun phrases.
</prevsent>
<prevsent>the first work on this topic was done back in the eighties (church, 1988).<papid> A88-1019 </papid></prevsent>
</prevsection>
<citsent citstr=" W95-0107 ">
the dataset that has become standard for evaluation machine learning approaches is the one first used by ramshaw and marcus (1995).<papid> W95-0107 </papid></citsent>
<aftsection>
<nextsent>it consists of the same training and test data segments of the penn treebank as the chunking task (respectively sections 15-18 and section 20).
</nextsent>
<nextsent>however, since the datasets have been generated with different software, the np boundaries in the np chunking datasets are slightly different from the np boundaries in the general chunking data.noun phrases are not restricted to the base levels of parse trees.
</nextsent>
<nextsent>for example, in the sentence in early trading in hongkong monday , gold was quoted at $ 366.50 an ounce ., the noun phrase   $ 366.50 an ounce  contains two embedded noun phrases   $ 366.50  and   an ounce  . in the np bracketing task, the goal is to find all noun phrases in sentence.
</nextsent>
<nextsent>datasets for this task were defined for conll-992.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1214">
<title id=" W01-0712.xml">learning computational grammars </title>
<section> method.  </section>
<citcontext>
<prevsection>
<prevsent>the automatic optimisation of corpus-derived lscgs is the subject of ongoing research and the results reported here for this method are therefore preliminary.
</prevsent>
<prevsent>theory refinement (allis).
</prevsent>
</prevsection>
<citsent citstr=" W00-0727 ">
allis((dejean, 2000<papid> W00-0727 </papid>b), (dejean, 2000<papid> W00-0727 </papid>c)) is inductive rule-based system using traditional general-to-specific approach (mitchell, 1997).</citsent>
<aftsection>
<nextsent>after generating default classification rule (equivalent to the n-gram model), allis tries to refine it since the accuracy of these rules is usually not high enough.
</nextsent>
<nextsent>refinement is done by adding more premises (contextual elements).
</nextsent>
<nextsent>allis uses data encoded in xml, and also learns rules in xml.
</nextsent>
<nextsent>from the perspective of the xml formalism, the initial rule can be viewed as tree with only one leaf, and refinement is done by adding adjacent leaves until the accuracy of the rule is high enough (a tuning threshold is used).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1222">
<title id=" W01-0712.xml">learning computational grammars </title>
<section> method.  </section>
<citcontext>
<prevsection>
<prevsent>during training, features are assigned weights in such away that, given the maxent principle, the training data is matched as well as possible.
</prevsent>
<prevsent>during evaluation it is tested which features are active (i.e., feature is active when the context meets the requirements given by the feature).
</prevsent>
</prevsection>
<citsent citstr=" J96-1002 ">
for every class the weights of the active features are combined and the best scoring class is chosen (berger et al, 1996).<papid> J96-1002 </papid></citsent>
<aftsection>
<nextsent>for the classifier built here we use as evidence the surrounding words, their pos tags and basenp tags predicted for the previous words.
</nextsent>
<nextsent>a mixture of simple features (consisting of oneof the mentioned information sources) and complex features (combinations thereof) were used.
</nextsent>
<nextsent>the left context never exceeded 3 words, the right context was maximally 2 words.
</nextsent>
<nextsent>the model was calculated using existing software (dehaspe, 1997).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1223">
<title id=" W01-0712.xml">learning computational grammars </title>
<section> method.  </section>
<citcontext>
<prevsection>
<prevsent>the training material well, but also iscompact.
</prevsent>
<prevsent>the basic idea is to balance the generality of model (roughly speaking, the more compact the model, the more general it is) with its specialisation to the training material.
</prevsent>
</prevsection>
<citsent citstr=" W99-0708 ">
we have applied mdl to the task of learning broad-covering definite-clause grammars from either raw text, orelse from parsed corpora (osborne, 1999<papid> W99-0708 </papid>a).</citsent>
<aftsection>
<nextsent>preliminary results have shown that learning using just raw text is worse than learning with parsed corpora, and that learning using both parsed corpora and compression-based prior is better than when learning using parsed corpora and uniformprior.
</nextsent>
<nextsent>furthermore, we have noted that our instantiation of mdl does not capture dependencies which exist either in the grammar or else in preferred parses.
</nextsent>
<nextsent>ongoing work has focused on applying random field technology (maximum entropy) to mdl-based grammar learning (see osborne (2000<papid> W00-0731 </papid>a) for some of the issues involved).finite state transducers are built by interpreting probabilistic automata as transducers.</nextsent>
<nextsent>we use probabilistic grammatical algorithm, the ddsm algorithm (thollard, 2001), for learning automata that provide the probability of an item given the previous ones.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1224">
<title id=" W01-0712.xml">learning computational grammars </title>
<section> method.  </section>
<citcontext>
<prevsection>
<prevsent>preliminary results have shown that learning using just raw text is worse than learning with parsed corpora, and that learning using both parsed corpora and compression-based prior is better than when learning using parsed corpora and uniformprior.
</prevsent>
<prevsent>furthermore, we have noted that our instantiation of mdl does not capture dependencies which exist either in the grammar or else in preferred parses.
</prevsent>
</prevsection>
<citsent citstr=" W00-0731 ">
ongoing work has focused on applying random field technology (maximum entropy) to mdl-based grammar learning (see osborne (2000<papid> W00-0731 </papid>a) for some of the issues involved).finite state transducers are built by interpreting probabilistic automata as transducers.</citsent>
<aftsection>
<nextsent>we use probabilistic grammatical algorithm, the ddsm algorithm (thollard, 2001), for learning automata that provide the probability of an item given the previous ones.
</nextsent>
<nextsent>the items are described by bigrams of the format feature:class.
</nextsent>
<nextsent>in there sulting automata we consider transition labeled feature:class as the transducer transition that takes as input the first part (feature) of the bigram and outputs the second part (class).
</nextsent>
<nextsent>by applying the viterbi algorithm on such model, we can find out the most probable set of class values given aninput set of feature values.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1228">
<title id=" W01-0712.xml">learning computational grammars </title>
<section> results.  </section>
<citcontext>
<prevsection>
<prevsent>belz (2001)used local structural context grammars for finding chunks.
</prevsent>
<prevsent>dejean (2000<papid> W00-0727 </papid>a) applied the theory refinement system allis to the shared taskdata.</prevsent>
</prevsection>
<citsent citstr=" W00-0729 ">
koeling (2000) <papid> W00-0729 </papid>evaluated maximum entropy learner while using different feature combinations (me).</citsent>
<aftsection>
<nextsent>osborne (2000<papid> W00-0731 </papid>b) used maximum entropy-based part-of-speech tagger for assigning chunk tags to words (me tag).</nextsent>
<nextsent>thollard(2001) identified chunks with finite state transducers generated by probabilistic grammar algorithm (fst).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1234">
<title id=" W01-0712.xml">learning computational grammars </title>
<section> prospects.  </section>
<citcontext>
<prevsection>
<prevsent>three of our project members will take partin the conll-2001 shared task, clausing?, hopefully with good results.
</prevsent>
<prevsent>two more have started working on the challenging task of full parsing,in particular by starting with chunker and building bottom-up arbitrary phrase recogniser on top of that.
</prevsent>
</prevsection>
<citsent citstr=" A00-2018 ">
the preliminary results are encouraging though not as good as advanced statistical parsers like those of charniak (2000) <papid> A00-2018 </papid>and collins (2000).it is fair to characterise lcgs goals as primarily technical in the sense that we sought to maximise performance rates, esp. the recognition of different levels of np structure.</citsent>
<aftsection>
<nextsent>our view in the project is certainly broader, and most project members would include learning as one of the language processes one ought to study from acomputational perspective like parsing or generation.
</nextsent>
<nextsent>this suggest several further avenues, e.g.,one might compare the learning progress of simulations to humans (mastery as function of ex perience).
</nextsent>
<nextsent>one might also be interested in the exact role of supervision, in the behaviour (and availability) of incremental learning algorithms,and also in comparing the simulations error functions to those of human learners (wrt to phrase length or construction frequency or similarity).this would add an interesting cognitive perspective to the work, along the lines begun by brent (1997), but we note it here only as prospect for future work.
</nextsent>
<nextsent>acknowledgement lcgs work has been supported by grant from the european unions programme training and mobility of researchers, erbfmrxct980237.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1235">
<title id=" W01-1005.xml">identification of relevant terms to support the construction of domain ontologies </title>
<section> text mining tools to construct a.  </section>
<citcontext>
<prevsection>
<prevsent>extensive studies suggest that statistical filters be always faced with 50-80% of non-terminological candidates.
</prevsent>
<prevsent>filtering of true terms can be done by estimating the strength of an association among words in candidate terminological expression.
</prevsent>
</prevsection>
<citsent citstr=" J96-1001 ">
commonly used association measures are the mutual information (fano, 1961) and the dice factor (smadja et al 1996).<papid> J96-1001 </papid></citsent>
<aftsection>
<nextsent>in both formulas, the denominator combines the marginal probability of each word appearing in the candidate term.
</nextsent>
<nextsent>if one of these words is particularly frequent, both measures tend to be low.
</nextsent>
<nextsent>this is indeed not desirable, because certain very prominent domain words appear in many terminological patterns.
</nextsent>
<nextsent>for example, in the tourism domain, the term visa appears both in isolation and in many multiword patterns, e.g.: business visa, extended visa, multiple entry business visa, transit visa, student visa, etc.such patterns are usually not captured by standard association measures, because of the high marginal probability of visa.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1236">
<title id=" W00-1417.xml">content aggregation in natural language hypertext summarization of olap and data mining discoveries </title>
<section> 5 3  </section>
<citcontext>
<prevsection>
<prevsent>126 in natural language generation natural language generation system is traditionally decomposed in the following subtasks: content determination, discourse-level content organization, sentence-level content organization, lexical content realization and grammatical content realization.
</prevsent>
<prevsent>the first three ......................... subtasks together ate_often=referred toas.jzontent planning, and the last two together as linguistic realization.
</prevsent>
</prevsection>
<citsent citstr=" W94-0319 ">
this separation is now fairly standard and most implementations encapsulate each task in separate module (robin 1995), (reiter 1994).<papid> W94-0319 </papid></citsent>
<aftsection>
<nextsent>another generation subtask that has recently received much attention is content aggregation.
</nextsent>
<nextsent>however, there is still no consensus on the exact scope of aggregation and on its precise relation with the five standard generation tasks listed above.
</nextsent>
<nextsent>to avoid ambiguity, we define aggregation here as: grouping several content units, sharing various semantic features, inside single linguistic structure, in such way that the shared features are maximally factored out and minimally repeated in the generated text.
</nextsent>
<nextsent>defined as above, aggregation is essentially key subtask of sentence planning.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1237">
<title id=" W00-1417.xml">content aggregation in natural language hypertext summarization of olap and data mining discoveries </title>
<section> related work in content aggregation.  </section>
<citcontext>
<prevsection>
<prevsent>fig.
</prevsent>
<prevsent>12 - fragment of life feature structure representing the discourse tree output of the sentence planner and input to the lexicalizer.
</prevsent>
</prevsection>
<citsent citstr=" P98-2199 ">
the main previous works on content aggregation are due to: (dalianis 1995, 1996), whose as trogen system generates natural language paraphrases of formal software specification for validation purposes; (huang and fiedler 1997), whose proverb system generates natural language mathematical proofs from theorem prover reasoning trace; (robin and mckeown, 1996), whose streak system generates basketball game summaries from semantic network 130 representing the key game statistics and their historical context; (shaw 1998), <papid> P98-2199 </papid>whose casper discourse and sentence planner has been used both in the plandoc system that generates telecommunication equipment installation plan documentation from an expert system trace and the magic system that generates extracted from dimensional data warehouse hypercube.</citsent>
<aftsection>
<nextsent>in contrast, he other systems all take as input either semantic network extracted from knowledge base or pre-linguistic representation the text to generate such as meteer text structure (meteer 1992) or jackendoffs semantic structure (jackendoff 1985).
</nextsent>
<nextsent>such natural language processing icu measurements.
</nextsent>
<nextsent>in this section, we briefly compare these research efforts with ours along four dimensions: (1) the definition of aggregation and the scope of the aggregation task implemented in the generator, (2) the type of representation the generator takes as input and the type of output ext that it produces, (3) the generator architecture and the localization of the aggregation task within it, and (4) the data structures and algorithms used to implement aggregation.
</nextsent>
<nextsent>3.1 definition of the aggregation task.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1238">
<title id=" W01-0720.xml">a psychologically plausible and computationally effective approach to learning syntax </title>
<section> abstract </section>
<citcontext>
<prevsection>
<prevsent>in this paper we present system cll which aims to learn natural language syntax in way that is both computationally effective and psychologically plausible.
</prevsent>
<prevsent>this theoretically plausible system can also perform the practically useful task of unsupervised learning of syntax.
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
cll has then been applied to corpus of declarative sentences from the penn treebank (marcus et al, 1993; <papid> J93-2004 </papid>marcus et al, 1994) <papid> H94-1020 </papid>on which it has been shown to perform comparatively well with respect to much less psychologically plausible systems, which are significantly more supervised and are applied to somewhat simpler problems.</citsent>
<aftsection>
<nextsent>computational learning of natural language can be considered from two common perspectives.
</nextsent>
<nextsent>firstly, there is the psychological perspective,which leads to the investigation of learning problems similar to those faced by people and the building of systems that seek to model human language learning faculties.
</nextsent>
<nextsent>secondly, there is the computational perspective, which seeks to build systems that effectively solve language learning problems that are of practical importance.in principle, there is significant overlap between these two perspectives.
</nextsent>
<nextsent>the most common language learning problems that we wish to solve computationally are frequently those that humans have to solve.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1240">
<title id=" W01-0720.xml">a psychologically plausible and computationally effective approach to learning syntax </title>
<section> abstract </section>
<citcontext>
<prevsection>
<prevsent>in this paper we present system cll which aims to learn natural language syntax in way that is both computationally effective and psychologically plausible.
</prevsent>
<prevsent>this theoretically plausible system can also perform the practically useful task of unsupervised learning of syntax.
</prevsent>
</prevsection>
<citsent citstr=" H94-1020 ">
cll has then been applied to corpus of declarative sentences from the penn treebank (marcus et al, 1993; <papid> J93-2004 </papid>marcus et al, 1994) <papid> H94-1020 </papid>on which it has been shown to perform comparatively well with respect to much less psychologically plausible systems, which are significantly more supervised and are applied to somewhat simpler problems.</citsent>
<aftsection>
<nextsent>computational learning of natural language can be considered from two common perspectives.
</nextsent>
<nextsent>firstly, there is the psychological perspective,which leads to the investigation of learning problems similar to those faced by people and the building of systems that seek to model human language learning faculties.
</nextsent>
<nextsent>secondly, there is the computational perspective, which seeks to build systems that effectively solve language learning problems that are of practical importance.in principle, there is significant overlap between these two perspectives.
</nextsent>
<nextsent>the most common language learning problems that we wish to solve computationally are frequently those that humans have to solve.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1241">
<title id=" W01-0720.xml">a psychologically plausible and computationally effective approach to learning syntax </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>however, this syntactic analysis would appear to be very expensive and the system has not been applied to largescale problems.
</prevsent>
<prevsent>the compression metric is applied with respect to the compression of the corpus, rather than the compression of syntactic information extracted from the corpus, as in cll.it seems unlikely that this simple induction algorithm would generate linguistically plausible grammars when presented with complex naturally occurring data.
</prevsent>
</prevsection>
<citsent citstr=" C94-1024 ">
joshi and srinivas (joshi and srinivas, 1994) <papid> C94-1024 </papid>have developed method called super tagging that similarly attaches complex syntactic tags (su pertags) to words.</citsent>
<aftsection>
<nextsent>the most effective learning model appears to have been combination of symbolic and stochastic techniques, like the approach presented here.
</nextsent>
<nextsent>however, full lexicon is supplied to the learner, so that the problem is reduced to one of disambiguating between the possible supertags.
</nextsent>
<nextsent>the learning appears to be supervised and occurs over parts-of-speech rather than over the actual words.
</nextsent>
<nextsent>however, some notion oflabel accuracy is supplied and this can be compared with the accuracy of our system.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1244">
<title id=" W01-0720.xml">a psychologically plausible and computationally effective approach to learning syntax </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the learning appears to be supervised and occurs over parts-of-speech rather than over the actual words.
</prevsent>
<prevsent>however, some notion oflabel accuracy is supplied and this can be compared with the accuracy of our system.
</prevsent>
</prevsection>
<citsent citstr=" W97-1010 ">
osborne and briscoe (osborne and briscoe,1997) <papid> W97-1010 </papid>present fairly supervised system for learning unusual stochastic cgs (the atomic categories far more varied than standard cg) again using part-of-speech strings rather than words.</citsent>
<aftsection>
<nextsent>while the problem solved is much simpler, this system provides suitable comparison for learning appropriate lexicons for parsing.neither joshi and srinivas (joshi and srinivas, 1994) <papid> C94-1024 </papid>nor osborne and briscoe (osborne and briscoe, 1997) <papid> W97-1010 </papid>can be considered psychologically plausible, but they are computationally effective and they do provide results for comparison.</nextsent>
<nextsent>two other approaches to learning cgs are presented by adriaans (adriaans, 1992) and solomon (solomon, 1991).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1249">
<title id=" W01-0720.xml">a psychologically plausible and computationally effective approach to learning syntax </title>
<section> results.  </section>
<citcontext>
<prevsection>
<prevsent>the results show that wide range of categories can be learned, but the current algorithm, as the author admits, is probably too naive to scale up to working on full corpora.
</prevsent>
<prevsent>no results on the coverage of the cgs learned are provided.
</prevsent>
</prevsection>
<citsent citstr=" W99-0909 ">
early results on small simple corpora with simpler version of the learner were presented in (watkinson and manandhar, 1999; <papid> W99-0909 </papid>watkinson and manandhar, 2000).</citsent>
<aftsection>
<nextsent>here, we present experiments performed using two complex corpora, c1and c2, extracted from the penn treebank (marcus et al, 1993; <papid> J93-2004 </papid>marcus et al, 1994).<papid> H94-1020 </papid></nextsent>
<nextsent>these corpora did not contain sentences with null elements (i.e. movement).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1253">
<title id=" W01-0720.xml">a psychologically plausible and computationally effective approach to learning syntax </title>
<section> results.  </section>
<citcontext>
<prevsection>
<prevsent>experiments were performed with closed-class word initial lexicon of 348 entries (lil) and asmaller closed-class word initial lexicon of 31 entries (sil) to determine the bootstrapping effect of this initial lexicon.
</prevsent>
<prevsent>the resulting lexicons are described in table 1.
</prevsent>
</prevsection>
<citsent citstr=" W01-0904 ">
these can be compared with gold standard cgannotated corpus which has been built (watkin son and manandhar, 2001), <papid> W01-0904 </papid>which has size of 15,136 lexical entries and an average ambiguity of 1.25 categories per word.</citsent>
<aftsection>
<nextsent>this corpus is only loosely gold standard, as it has been automatically constructed.
</nextsent>
<nextsent>however, it gives an indication of the effectiveness of the lexical labelling and is currently the best cg tagged resource available to us.
</nextsent>
<nextsent>the accuracy of the parsed examples both from the training and test corpora are also described in table 1.
</nextsent>
<nextsent>two measures are used to evaluate the parses: lexical accuracy, which is the percentage of correctly tagged words compared to the extracted gold standard corpus (watkin son and manandhar, 2001) <papid> W01-0904 </papid>and average crossing bracket rate (cbr) (goodman, 1996).<papid> P96-1024 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1255">
<title id=" W01-0720.xml">a psychologically plausible and computationally effective approach to learning syntax </title>
<section> results.  </section>
<citcontext>
<prevsection>
<prevsent>however, it gives an indication of the effectiveness of the lexical labelling and is currently the best cg tagged resource available to us.
</prevsent>
<prevsent>the accuracy of the parsed examples both from the training and test corpora are also described in table 1.
</prevsent>
</prevsection>
<citsent citstr=" P96-1024 ">
two measures are used to evaluate the parses: lexical accuracy, which is the percentage of correctly tagged words compared to the extracted gold standard corpus (watkin son and manandhar, 2001) <papid> W01-0904 </papid>and average crossing bracket rate (cbr) (goodman, 1996).<papid> P96-1024 </papid></citsent>
<aftsection>
<nextsent>in general the system performs better with the larger initial lexicon to bootstrap it.
</nextsent>
<nextsent>the size and ambiguity of the lexicon are close to that of the gold standard, indicating that the right level of compression has occurred.
</nextsent>
<nextsent>the best crossing bracket rate of 4.70 compares favourably with osborne and briscoe (osborne and briscoe, 1997) <papid> W97-1010 </papid>who give crossing bracket rates of around 3 for variety of systems.</nextsent>
<nextsent>considering that they are solving much simpler problem, our average crossing bracket rates seem reasonable.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1268">
<title id=" W02-0606.xml">unsupervised discovery of morphologically related words based on orthographic and semantic similarity </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>finally, in section 5 we summarize our main results, we sketch possible directions that our current work could take, and we discuss some potential uses for the output of our algorithm.
</prevsent>
<prevsent>for space reason, we discuss here only three approaches that are closely related to ours.
</prevsent>
</prevsection>
<citsent citstr=" J01-2001 ">
see, for example, goldsmith (2001) <papid> J01-2001 </papid>for very different (pos sibly complementary) approach, and for review of other relevant work.</citsent>
<aftsection>
<nextsent>2.1 jacquemin (1997).
</nextsent>
<nextsent>jacquemin (1997) presents model that automatically extracts morphologically related forms from list of english two-word medical terms and corpus from the medical domain.
</nextsent>
<nextsent>the algorithm looks for correspondences between two-word terms and ortho graphically similar pairsof words that are adjacent in the corpus.
</nextsent>
<nextsent>for example, the list contains the term artificial ventilation,and the corpus contains the phrase artificially ventilated.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1270">
<title id=" W02-0606.xml">unsupervised discovery of morphologically related words based on orthographic and semantic similarity </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>thus, we are not limited to affixal morphological patterns.
</prevsent>
<prevsent>moreover, our algorithm extracts semantic information directly fromthe input corpus, and thus it does not require pre compiled list of semantically related pairs.
</prevsent>
</prevsection>
<citsent citstr=" W00-0712 ">
2.2 schone and jurafsky (2000).<papid> W00-0712 </papid></citsent>
<aftsection>
<nextsent>schone and jurafsky (2000) <papid> W00-0712 </papid>present knowledge free unsupervised model in which orthographybased distributional cues are combined with semantic information automatically extracted from word co-occurrence patterns in the input corpus.they first look for potential suffixes by searching for frequent word-final substrings.</nextsent>
<nextsent>then, they look for potentially morphologically related pairs, i.e., pairs that end in potential suffixes and share the left substring preceding those suffixes.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1274">
<title id=" W02-0606.xml">unsupervised discovery of morphologically related words based on orthographic and semantic similarity </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>thus, we only look at the cooccurrence patterns of target words, rather than at the similarity of their contexts.
</prevsent>
<prevsent>future research should try to assess to what extent these two approaches produce significantly different results, and/or to what extent they are complementary.
</prevsent>
</prevsection>
<citsent citstr=" P00-1027 ">
2.3 yarowsky and wicentowski (2000).<papid> P00-1027 </papid></citsent>
<aftsection>
<nextsent>yarowsky and wicentowski (2000) <papid> P00-1027 </papid>propose an algorithm that extracts morphological rules relating roots and inflected forms of verbs (but the algorithm can be extended to other morphological relations).</nextsent>
<nextsent>their algorithm performs unsupervised, but not completely knowledge-free, learning.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1277">
<title id=" W02-0606.xml">unsupervised discovery of morphologically related words based on orthographic and semantic similarity </title>
<section> the current approach: morphological.  </section>
<citcontext>
<prevsection>
<prevsent>these are all pairs where one word has 9 characters, the other 9 or 8 characters, and the two differ in only one character.4for the above reasons, it is crucial that orthographic similarity is combined with an independent measure that allows us to distinguish between similarity due to morphological relatedness vs. similarity due to chance or other reasons.
</prevsent>
<prevsent>3.3 scoring the semantic similarity of word.
</prevsent>
</prevsection>
<citsent citstr=" P89-1010 ">
pairs measuring the semantic similarity of words on the basis of raw corpus data is obviously much harder task than measuring the orthographic similarity of words.mutual information (first introduced to computational linguistics by church and hanks (1989)) <papid> P89-1010 </papid>is one of many measures that seems to be roughly correlated to the degree of semantic relatedness between words.</citsent>
<aftsection>
<nextsent>the mutual information between two words and is given by: i(a,b) = log pr(a,b) pr(a)pr(b) (1) intuitively, the larger the deviation between the empirical frequency of co-occurrence of two words and the expected frequency of co-occurrence if they were independent, the more likely it is that the occurrence of one of the two words is not independent from the occurrence of the other.
</nextsent>
<nextsent>brown et al (1990) observed that when mutual information is computed in bi-directional fashion, and by counting co-occurrences of words within a4most of the pairs in this block ? 78% ? are actually morphologically related.
</nextsent>
<nextsent>however, given that all pairs contain words of length 9 and 8/9 that differ in one character only, they are bound to reflect only very small subset of the morphological processes present in german.relatively large window, but excluding close?
</nextsent>
<nextsent>cooccurrences (which would tend to capture collocations and lexicalized phrases), the measure identifies semantically related pairs.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1278">
<title id=" W02-0606.xml">unsupervised discovery of morphologically related words based on orthographic and semantic similarity </title>
<section> the current approach: morphological.  </section>
<citcontext>
<prevsection>
<prevsent>cooccurrences (which would tend to capture collocations and lexicalized phrases), the measure identifies semantically related pairs.
</prevsent>
<prevsent>it is particularly interesting for our purposes that most of the examples of english word clusters constructed on the basis of this interpretation of mutual information by brown and colleagues (reported in their table 6) include morphologically related words.
</prevsent>
</prevsection>
<citsent citstr=" C02-1096 ">
a similar pattern emerges among the examples of german words clustered in similar manner by baroni et al (2002).<papid> C02-1096 </papid></citsent>
<aftsection>
<nextsent>rosenfeld (1996) reports that morphologically related pairs are common among words with high (average) mutual information.
</nextsent>
<nextsent>we computed mutual information by considering,for each pair, only co-occurrences within maximal window of 500 words and outside minimal window of 3 words.
</nextsent>
<nextsent>given that mutual information is notoriously unreliable at low frequencies (see, for example, manning and schutze (1999), section 5.4), we only collected mutual information scores for pairs that co-occurred at least three times (withinthe relevant window) in the input corpus.
</nextsent>
<nextsent>obviously, occurrences across article boundaries were not counted.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1291">
<title id=" W02-0606.xml">unsupervised discovery of morphologically related words based on orthographic and semantic similarity </title>
<section> conclusion and future directions.  </section>
<citcontext>
<prevsection>
<prevsent>such models extract morphological generalizations in terms of correspondence patterns between whole words, rather than in terms ofaffixation rules, and are thus well suited to identify patterns involving non-concatenative morphology and/or morpho phonological changes.
</prevsent>
<prevsent>a list of related words constitutes more suitable input for them than list of words segmented into morphemes.
</prevsent>
</prevsection>
<citsent citstr=" P01-1063 ">
rules extracted in this way would have number of practical uses ? for example, they could be usedto construct stem mers for information retrieval applications, or they could be integrated into morphological analyzers.our procedure could also be used to replace the first step of algorithms, such as those of goldsmith (2001) <papid> J01-2001 </papid>and snover and brent (2001), <papid> P01-1063 </papid>where heuristic methods are employed to generate morphological hypotheses, and then an information theoretically/probabilistically motivated measure is used to evaluate or improve such hypotheses.</citsent>
<aftsection>
<nextsent>more in general, our algorithm can help reduce the size of the search space that all morphological discovery procedures must explore.last but not least, the ranked output of (an improved version of) our algorithm can be of use to the linguist analyzing the morphology of language, who can treat it as way to pre-process her/his data, while still relying on her/his analytical skills to extract the relevant morphological generalizations from the ranked pairs.
</nextsent>
<nextsent>acknowledgements we would like to thank adam albright, bruce hayes and the anonymous reviewers for helpful comments, and the austria presse agentur for kindly making the apa corpus available to us.
</nextsent>
<nextsent>this work was supported by the european union in the framework of the ist programme, project fasty (ist-2000-25420).
</nextsent>
<nextsent>financial support for ofai is provided by the austrian federal ministry of education, science and culture.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1292">
<title id=" W01-1411.xml">towards a simple and accurate statistical approach to learning translation relationships among words </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this paper is report on work in progress aimedat learning word translation relationships automatically from parallel bilingual corpora.
</prevsent>
<prevsent>our effort is distinguished by the use of simple statistical models that are easier to implement and faster to run than previous high-accuracy approaches to this problem.
</prevsent>
</prevsection>
<citsent citstr=" W01-1402 ">
our overall approach to machine translation isa deep-transfer approach in which the transfer relationships are learned from parallel bilingual corpus (richardson et al, 2001).<papid> W01-1402 </papid></citsent>
<aftsection>
<nextsent>more specifically, the transfer component is trained by parsing both sides of the corpus to produce parallel logical forms, using lexicons and analysis grammars constructed by linguists.
</nextsent>
<nextsent>the parallel logical forms are then aligned at the level of content word stems (lemmas), and logical-form transfer patterns are learned from the aligned logical form corpus.
</nextsent>
<nextsent>at run time, the source language text is parsed into logical forms employing the source language grammar and lexicon used in constructing the logical-form training corpus, andthe logical-form transfer patterns are used to construct target language logical forms.
</nextsent>
<nextsent>these logical forms are transformed into target language strings using the target-language lexicon, and generation grammar written by linguist.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1293">
<title id=" W01-1411.xml">towards a simple and accurate statistical approach to learning translation relationships among words </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>these logical forms are transformed into target language strings using the target-language lexicon, and generation grammar written by linguist.
</prevsent>
<prevsent>the principal roles played by the translation relationships derived by the methods discussed in this paper are to provide correspondences between content word lemmas in logical forms to assist in the alignment process, and to augment the lexicons used in parsing and generation, for special case described in section 4.
</prevsent>
</prevsection>
<citsent citstr=" H91-1026 ">
the most common approach to deriving translation lexicons from empirical data (catizone, russell, and warwick, 1989; gale and church, 1991; <papid> H91-1026 </papid>fung, 1995; <papid> P95-1032 </papid>kumano and hirakawa, 1994; <papid> C94-1009 </papid>wu and xia, 1994; melamed, 1995) <papid> W95-0115 </papid>is to use some variant of the following procedure:1 pick good measure of the degree of association between words in language 1 and words in language 2 in aligned sentences of parallel bilingual corpus.</citsent>
<aftsection>
<nextsent> rank order pairs consisting of word from 1 and word from 2 according to the measure of association.1the important work of brown et al (1993) <papid> J93-2003 </papid>is not directly comparable, since their globally-optimized generative probabilistic model of translation never has to make firm commitment as to what can or cannot be translation pair.</nextsent>
<nextsent>they assign some nonzero probability to every possible translation pair. choose threshold, and add to the translation lexicon all pairs of words whose degree of association is above the threshold.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1294">
<title id=" W01-1411.xml">towards a simple and accurate statistical approach to learning translation relationships among words </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>these logical forms are transformed into target language strings using the target-language lexicon, and generation grammar written by linguist.
</prevsent>
<prevsent>the principal roles played by the translation relationships derived by the methods discussed in this paper are to provide correspondences between content word lemmas in logical forms to assist in the alignment process, and to augment the lexicons used in parsing and generation, for special case described in section 4.
</prevsent>
</prevsection>
<citsent citstr=" P95-1032 ">
the most common approach to deriving translation lexicons from empirical data (catizone, russell, and warwick, 1989; gale and church, 1991; <papid> H91-1026 </papid>fung, 1995; <papid> P95-1032 </papid>kumano and hirakawa, 1994; <papid> C94-1009 </papid>wu and xia, 1994; melamed, 1995) <papid> W95-0115 </papid>is to use some variant of the following procedure:1 pick good measure of the degree of association between words in language 1 and words in language 2 in aligned sentences of parallel bilingual corpus.</citsent>
<aftsection>
<nextsent> rank order pairs consisting of word from 1 and word from 2 according to the measure of association.1the important work of brown et al (1993) <papid> J93-2003 </papid>is not directly comparable, since their globally-optimized generative probabilistic model of translation never has to make firm commitment as to what can or cannot be translation pair.</nextsent>
<nextsent>they assign some nonzero probability to every possible translation pair. choose threshold, and add to the translation lexicon all pairs of words whose degree of association is above the threshold.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1295">
<title id=" W01-1411.xml">towards a simple and accurate statistical approach to learning translation relationships among words </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>these logical forms are transformed into target language strings using the target-language lexicon, and generation grammar written by linguist.
</prevsent>
<prevsent>the principal roles played by the translation relationships derived by the methods discussed in this paper are to provide correspondences between content word lemmas in logical forms to assist in the alignment process, and to augment the lexicons used in parsing and generation, for special case described in section 4.
</prevsent>
</prevsection>
<citsent citstr=" C94-1009 ">
the most common approach to deriving translation lexicons from empirical data (catizone, russell, and warwick, 1989; gale and church, 1991; <papid> H91-1026 </papid>fung, 1995; <papid> P95-1032 </papid>kumano and hirakawa, 1994; <papid> C94-1009 </papid>wu and xia, 1994; melamed, 1995) <papid> W95-0115 </papid>is to use some variant of the following procedure:1 pick good measure of the degree of association between words in language 1 and words in language 2 in aligned sentences of parallel bilingual corpus.</citsent>
<aftsection>
<nextsent> rank order pairs consisting of word from 1 and word from 2 according to the measure of association.1the important work of brown et al (1993) <papid> J93-2003 </papid>is not directly comparable, since their globally-optimized generative probabilistic model of translation never has to make firm commitment as to what can or cannot be translation pair.</nextsent>
<nextsent>they assign some nonzero probability to every possible translation pair. choose threshold, and add to the translation lexicon all pairs of words whose degree of association is above the threshold.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1296">
<title id=" W01-1411.xml">towards a simple and accurate statistical approach to learning translation relationships among words </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>these logical forms are transformed into target language strings using the target-language lexicon, and generation grammar written by linguist.
</prevsent>
<prevsent>the principal roles played by the translation relationships derived by the methods discussed in this paper are to provide correspondences between content word lemmas in logical forms to assist in the alignment process, and to augment the lexicons used in parsing and generation, for special case described in section 4.
</prevsent>
</prevsection>
<citsent citstr=" W95-0115 ">
the most common approach to deriving translation lexicons from empirical data (catizone, russell, and warwick, 1989; gale and church, 1991; <papid> H91-1026 </papid>fung, 1995; <papid> P95-1032 </papid>kumano and hirakawa, 1994; <papid> C94-1009 </papid>wu and xia, 1994; melamed, 1995) <papid> W95-0115 </papid>is to use some variant of the following procedure:1 pick good measure of the degree of association between words in language 1 and words in language 2 in aligned sentences of parallel bilingual corpus.</citsent>
<aftsection>
<nextsent> rank order pairs consisting of word from 1 and word from 2 according to the measure of association.1the important work of brown et al (1993) <papid> J93-2003 </papid>is not directly comparable, since their globally-optimized generative probabilistic model of translation never has to make firm commitment as to what can or cannot be translation pair.</nextsent>
<nextsent>they assign some nonzero probability to every possible translation pair. choose threshold, and add to the translation lexicon all pairs of words whose degree of association is above the threshold.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1297">
<title id=" W01-1411.xml">towards a simple and accurate statistical approach to learning translation relationships among words </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>the principal roles played by the translation relationships derived by the methods discussed in this paper are to provide correspondences between content word lemmas in logical forms to assist in the alignment process, and to augment the lexicons used in parsing and generation, for special case described in section 4.
</prevsent>
<prevsent>the most common approach to deriving translation lexicons from empirical data (catizone, russell, and warwick, 1989; gale and church, 1991; <papid> H91-1026 </papid>fung, 1995; <papid> P95-1032 </papid>kumano and hirakawa, 1994; <papid> C94-1009 </papid>wu and xia, 1994; melamed, 1995) <papid> W95-0115 </papid>is to use some variant of the following procedure:1 pick good measure of the degree of association between words in language 1 and words in language 2 in aligned sentences of parallel bilingual corpus.</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
rank order pairs consisting of word from 1 and word from 2 according to the measure of association.1the important work of brown et al (1993) <papid> J93-2003 </papid>is not directly comparable, since their globally-optimized generative probabilistic model of translation never has to make firm commitment as to what can or cannot be translation pair.</citsent>
<aftsection>
<nextsent>they assign some nonzero probability to every possible translation pair. choose threshold, and add to the translation lexicon all pairs of words whose degree of association is above the threshold.
</nextsent>
<nextsent>as melamed later (1996), as melamed later (2000) pointed out,however, this technique is hampered by the existence of indirect associations between words that are not mutual translations.
</nextsent>
<nextsent>for example, in our parallel french-english corpus (consisting primarily of translated computer software manuals), two of the most strongly associated word lemma translation pairs are fichier/file and syste`me/system.
</nextsent>
<nextsent>however, because the monolingual collocations syst`eme de fichiers, fichiers syste`me, file system, and system files are so common, the spurious translation pairs fichier/system and syste`me/file also receive rather high association scores higher in fact that such true translation pairs as confiance/trust, par allelisme/parallelism, and film/movie.melameds solution to this problem is not to regard highly-associated word pairs as translation sin sentences in which there are even more highly associated pairs involving one or both of the same words.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1298">
<title id=" W01-1411.xml">towards a simple and accurate statistical approach to learning translation relationships among words </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>a second important issue regarding automatic derivation of translation relationships is the assumption implicit (or explicit) in most previous work that lexical translation relationships involve2melamed does not report computation time for the version of his approach without generation of compounds, butour approach omits number of computationally very expensive steps performed in his approach.
</prevsent>
<prevsent>only single words.
</prevsent>
</prevsection>
<citsent citstr=" W97-0311 ">
this is manifestly not the case, as is shown by the following list of translation pairs selected from our corpus: base de donnees/database mot de passe/password sauvegarder/back up annuler/roll back ouvrir session/log on some of the most sophisticated work on this aspect of problem again seems to be that of melamed (1997).<papid> W97-0311 </papid></citsent>
<aftsection>
<nextsent>our approach in this case isquite different from mela meds. it is more general in that it can propose compounds that are dis contiguous in the training text, as roll back wouldbe in phrase such as roll the failed transaction back.
</nextsent>
<nextsent>melamed does allow skipping over one or two function words, but our basic method is not limited at all by word adjacency.
</nextsent>
<nextsent>also,our approach is again much simpler computationally than mela meds and apparently runs orders of magnitude faster.3
</nextsent>
<nextsent>our basic method for deriving translation pairs consists of the following steps: 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1299">
<title id=" W01-1411.xml">towards a simple and accurate statistical approach to learning translation relationships among words </title>
<section> recompute association scores, taking into.  </section>
<citcontext>
<prevsection>
<prevsent>3.2 computing association scores.
</prevsent>
<prevsent>for step 2, we compute the degree of association between lemma l 1 and lemma l 2 in terms of the frequencies with which l 1occurs in sentences of the 1 part of the training corpus and l 2 occurs in sentences of the 2part of the training corpus, compared to the frequency with which l 1 and l 2 co-occur in aligned sentences of the training corpus.
</prevsent>
</prevsection>
<citsent citstr=" J93-1003 ">
for this purpose, we ignore multiple occurrences of lemma in single sentence.as measure of association, we use the loglikelihood-ratio statistic recommended by dunning (1993), <papid> J93-1003 </papid>which is the same statistic used by melamed to initialize his models.</citsent>
<aftsection>
<nextsent>this statistic gives measure of the likelihood that two samples are not generated by the same probability distribution.
</nextsent>
<nextsent>we use it to compare the over all frequency of l 1 in our training data to the frequency of l 1 given l 2 (i.e., the frequency with which l 1 occurs in sentences of 1 that are aligned with sentences of 2 in which l 2 occurs).
</nextsent>
<nextsent>since p(w 1 ) = p(w 1 jw 2 ) only if occurrences of l 1 and l 2 are independent, measure of the likelihood that these distributions are different is, therefore, measure of the likelihood that an observed positive association between l 1 and l 2 is not accidental.
</nextsent>
<nextsent>since this process generates association scores for huge number of lemma pairs for large training corpus, we prune the set to restrict our consideration to those pairs having at least some chance of being considered as translation pairs.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1300">
<title id=" W01-1411.xml">towards a simple and accurate statistical approach to learning translation relationships among words </title>
<section> recompute association scores, taking into.  </section>
<citcontext>
<prevsection>
<prevsent>we use the terms coverage?
</prevsent>
<prevsent>and accu racy?
</prevsent>
</prevsection>
<citsent citstr=" J00-2004 ">
in essentially the same way as melamed(1996), way as melamed(2000).<papid> J00-2004 </papid></citsent>
<aftsection>
<nextsent>type coverage?
</nextsent>
<nextsent>means the proportion of distinct lexical types in the entire training corpus, including both french and english, for which there is at least one translation given.
</nextsent>
<nextsent>as with the comparable results reported by melamed, these are predominantly single lemmas for content words, but we also include occurrences multi words as distinct types.
</nextsent>
<nextsent>mean count?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1301">
<title id=" W01-1208.xml">question answering using encyclopedic knowledge generated from the web </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>finally, extracted term descriptions are organized based on word senses anddomains.
</prevsent>
<prevsent>we also evaluate our system by way of experiments, where the japanese information-technology engineers examination is used as test collection.
</prevsent>
</prevsection>
<citsent citstr=" C00-1043 ">
motivated partially by the trec-8 qa collection (voorhees and tice, 2000), question answering has of late become one of the major topics within the natural language processing and information retrieval communities, and number of qa systems targeting the trec collection have been proposed (harabagiu et al, 2000; <papid> C00-1043 </papid>moldovan and harabagiu, 2000; prager et al, 2000).</citsent>
<aftsection>
<nextsent>although harabagiu et al (2000) <papid> C00-1043 </papid>proposed aknowledge-based qa system, most existing systems relyon conventional ir and shallow nlpmethods.</nextsent>
<nextsent>however, question answering is inherently more complicated procedure that usually requires explicit knowledge bases.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1303">
<title id=" W01-1208.xml">question answering using encyclopedic knowledge generated from the web </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this paper, we propose question answering system which uses an encyclopedia as knowledge base.
</prevsent>
<prevsent>however, since existing (published) encyclopedias usually lack technical/new terms, we generate one based on the world wide web, which includes number of technical and recent information.
</prevsent>
</prevsection>
<citsent citstr=" P00-1062 ">
for this purpose, we use modified version of our method to extract term descriptions from web pages (fujii and ishikawa, 2000).<papid> P00-1062 </papid></citsent>
<aftsection>
<nextsent>intuitively, our system answers interrogativequestions like what is x??
</nextsent>
<nextsent>in which qa system searches an encyclopedia database for one or more descriptions related to term x.the performance of qa systems can be evaluated based on coverage and accuracy.
</nextsent>
<nextsent>coverage is the ratio between the number of questions answered (disregarding their correctness) and the total number of questions.
</nextsent>
<nextsent>accuracy is the ratio between the number of correct answers and the total number of answers made by the system.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1307">
<title id=" W01-1208.xml">question answering using encyclopedic knowledge generated from the web </title>
<section> encyclopedia generation.  </section>
<citcontext>
<prevsection>
<prevsent>for the purpose of organization, we classify extracted term descriptions based on word senses and domains.although number of methods have been proposed to generate word senses (for example, one based on the vector space model (schutze, 1998)), it is still difficult to accurately identify word senses without explicit dictionaries that pre define sense candidates.
</prevsent>
<prevsent>3 dt  and  dd  are inherently provided to describe terms in html.
</prevsent>
</prevsection>
<citsent citstr=" P95-1026 ">
since word senses are often associated with domains (yarowsky, 1995), <papid> P95-1026 </papid>word senses can be consequently distinguished by way of determining the domain of each description.</citsent>
<aftsection>
<nextsent>forex ample, different senses for pipeline (processing method/transportation pipe)?
</nextsent>
<nextsent>are associated with computer and construction domains (fields), respectively.
</nextsent>
<nextsent>to sum up, the organization module classifies term descriptions based on domains, for which we use domain and description models.
</nextsent>
<nextsent>in section 5, we elaborate on the organization model.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1310">
<title id=" W01-1208.xml">question answering using encyclopedic knowledge generated from the web </title>
<section> statistical organization model.  </section>
<citcontext>
<prevsection>
<prevsent>5.2 domain model.
</prevsent>
<prevsent>the domain model quantifies the extent to which description is associated with domain c, which is fundamentally categorization task.
</prevsent>
</prevsection>
<citsent citstr=" A94-1027 ">
among number of existing categorization methods, we experimentally used one proposed by iwayama and tokunaga (1994), <papid> A94-1027 </papid>which formulates (c|d) as in equation (2).</citsent>
<aftsection>
<nextsent>p (c|d) = (c) ? ?
</nextsent>
<nextsent>t (t|c) ? (t|d) (t) (2)here, (t|d), (t|c) and (t) denote probabilities that word appears in d, and all the domains, respectively.
</nextsent>
<nextsent>we regard (c) as constant.
</nextsent>
<nextsent>while (t|d) is simply relative frequency of in d, we need predefined domains to compute (t|c) andp (t).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1311">
<title id=" W01-1208.xml">question answering using encyclopedic knowledge generated from the web </title>
<section> statistical organization model.  </section>
<citcontext>
<prevsection>
<prevsent>however, since google (i.e., the search engine we used in the retrieval module) rates the quality of pages based on hyper link information, and selectively retrieves those with higher quality (brin and page, 1998), we tentatively regarded q (d) as constant.
</prevsent>
<prevsent>thus, in practice the description model is approximated solely with the language model as in equation (4).
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
p (d) ? l (d) (4) statistical approaches to language modeling have been used in much nlp research, such as machine translation (brown et al, 1993) <papid> J93-2003 </papid>and speech recognition (bahl et al, 1983).</citsent>
<aftsection>
<nextsent>our language model is almost the same as existing models, but is different in two respects.
</nextsent>
<nextsent>first, while general language models quantify the extent to which given word sequence is linguistically acceptable, our model also quantifies the extent to which the input is acceptable as term description.
</nextsent>
<nextsent>thus, we trained the model based on an existing machine readable encyclopedia.
</nextsent>
<nextsent>we used the chasen morphological analyzer to segment the japanese cd-rom world encyclopedia (heibonsha, 1998) into words (we replaced headwords with common symbol), andthen used the cmu-cambridge toolkit (clark son and rosenfeld, 1997) to model word-based trigram.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1312">
<title id=" W01-1207.xml">terminological variants for document selection and question answer matching </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the two main activities involving terminology in nlp are term acquisition and term recognition.
</prevsent>
<prevsent>basically, terms can be viewed as particular type of lexical data.
</prevsent>
</prevsection>
<citsent citstr=" C00-1039 ">
term variation may involve structural, morphological,and semantic transformations of single or multi words terms (fabre and jacquemin, 2000).<papid> C00-1039 </papid></citsent>
<aftsection>
<nextsent>in this paper, we describe how qalc uses high level indexes, made of terms and variants, to select among documents the most relevant ones with regard to question, and then to match candidate answers with this question.
</nextsent>
<nextsent>in the selection process, the documents first retrieved by search engine, are then post filtered and ranked through weighting scheme based on high level indexes, in order to retain the top ranked ones.
</nextsent>
<nextsent>similarly, all systems that participated in trec9 have search engine component that firstly selects subset of the provided database of about one million documents.
</nextsent>
<nextsent>since search engine produces ranked list of relevant documents, systems then have to define the highest number of documents to retain.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1313">
<title id=" W01-1207.xml">terminological variants for document selection and question answer matching </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>indeed, having too many documents leads to question processing time that is too long, but conversely, having too few documents reduces the possibility of obtaining the correct answer.
</prevsent>
<prevsent>for reducing the amount of text to process, one approach consists of keeping one or more relevant text paragraphs from each document retrieved.
</prevsent>
</prevsection>
<citsent citstr=" C00-1043 ">
kwok et al (2000), for instance use an ir engine that retrieves the top 300 sub-documents of about 300-550 words and, on the other hand, the falcon system (harabagiu et al  2000) <papid> C00-1043 </papid>performs paragraph retrieval stage after the application of boolean retrieval engine.</citsent>
<aftsection>
<nextsent>these systems work on the whole database and apply bag-of-words technique to select passages whereas qalc first retains large subset of documents, among which it then selects relevant documents by applying richer criteria based on the use of the linguistic structures of the words.
</nextsent>
<nextsent>qalc indexes, used for document selection, are made of single and multi-word terms retrieved by 2-step procedure: (1)automaticterm extraction from questions through part-of speech tagging and pattern matching and (2)automatic document indexing through term recognition and variant conflation.
</nextsent>
<nextsent>as result, linguistic variation is explicitly addressed through the exploitation of word paradigms, contrarily to other approaches like the one taken in copsy (schwarz 1988) where an approximate matching technique between the query and the documents implicitly takes it into account.
</nextsent>
<nextsent>finally, terms acquired at step?(1) and indexes from step?(2) are also used by the matching procedure between question and the relevant document sentences.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1314">
<title id=" W01-1207.xml">terminological variants for document selection and question answer matching </title>
<section> system overview.  </section>
<citcontext>
<prevsection>
<prevsent>in order to select the best documents from the results given by the search engine and to locate the answers inside them, we work with terms and their variants, i.e. morphologic, syntactic and semantic equivalent expressions.
</prevsent>
<prevsent>a term extractor has been developed, based on syntactic patterns which describe complex nominal phrases and their subparts.
</prevsent>
</prevsection>
<citsent citstr=" P99-1044 ">
these terms are used by fastr (jacquemin 1999), <papid> P99-1044 </papid>shallow transformational natural language analyzer that recognizes their occurrences and their variants.</citsent>
<aftsection>
<nextsent>each occurrence or variant constitutes an index that is subsequently used in the processes of document ranking and question/document matching.
</nextsent>
<nextsent>documents are ordered according to weight computed thanks to the number and the quality of the terms and variants they contain.
</nextsent>
<nextsent>for example, original terms with proper names are considered more reliable than semantic variants.
</nextsent>
<nextsent>an analysis of the weight graph enables the system to select relevant subpart of the documents, whose size varies along the questions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1319">
<title id=" W00-1412.xml">incremental event conceptualization and natural language generation in monitoring enviro ments </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(2) incremental systems are capable of producing fluent speech, i.e. speech without ar-tificial auditory gaps.
</prevsent>
<prevsent>(3) parallelism that comes with incrementality makes better use of the avail-able resources.
</prevsent>
</prevsection>
<citsent citstr=" W94-0319 ">
furthermore, reiter (1994), <papid> W94-0319 </papid>who reviews the ? architecture of some models of natural language generation, shows that psycho linguistic and engi-neering approaches often result in systems, which are similar in crucial respects.</citsent>
<aftsection>
<nextsent>in this paper we ground on two of these common aspects, namely the distinction between what-to-say and how-to- say (de smedt, horacek &amp; zock, 1996) and the use of pipeline architecture, which divides the generation process  into multiple modules, with information flowing in  pipeline  fashion from one module to the next  (reiter, 1994).<papid> W94-0319 </papid></nextsent>
<nextsent>reiter states that these architectures do not require mod-ules to work in parallel; if parallelism is used one has an incremental model, cf.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1322">
<title id=" W01-1403.xml">inducing lexico structural transfer rules from parsed bitexts </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our approach is based on lexico-structural transfer (nasr et. al., 1997), and extends recent work reported in (han et al, 2000) about korean to english transfer in particular.
</prevsent>
<prevsent>whereas han etal.
</prevsent>
</prevsection>
<citsent citstr=" C90-3044 ">
focus on high quality domain-specific translation using handcrafted transfer rules, in this work we instead focus on automating the acquisition of such rules.our approach can be considered generalization of syntactic approaches to example-based machine translation (ebmt) such as (nagao, 1984; sato and nagao, 1990; <papid> C90-3044 </papid>maruyama and watanabe, 1992).</citsent>
<aftsection>
<nextsent>while such approaches use syntactic transfer examples during the actual transfer of source parses, our approach instead uses syntactic transfer examples to induce general transfer rules that can be compiled into transfer dictionary for use in the actual translation process.
</nextsent>
<nextsent>our approach is similar to the recent work of (meyers et al, 1998) <papid> P98-2139 </papid>where transfer rules are also derived after aligning the source and target nodes of corresponding parses.</nextsent>
<nextsent>however, it also differs from (meyers et al, 1998) <papid> P98-2139 </papid>in several important points.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1323">
<title id=" W01-1403.xml">inducing lexico structural transfer rules from parsed bitexts </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>focus on high quality domain-specific translation using handcrafted transfer rules, in this work we instead focus on automating the acquisition of such rules.our approach can be considered generalization of syntactic approaches to example-based machine translation (ebmt) such as (nagao, 1984; sato and nagao, 1990; <papid> C90-3044 </papid>maruyama and watanabe, 1992).</prevsent>
<prevsent>while such approaches use syntactic transfer examples during the actual transfer of source parses, our approach instead uses syntactic transfer examples to induce general transfer rules that can be compiled into transfer dictionary for use in the actual translation process.</prevsent>
</prevsection>
<citsent citstr=" P98-2139 ">
our approach is similar to the recent work of (meyers et al, 1998) <papid> P98-2139 </papid>where transfer rules are also derived after aligning the source and target nodes of corresponding parses.</citsent>
<aftsection>
<nextsent>however, it also differs from (meyers et al, 1998) <papid> P98-2139 </papid>in several important points.</nextsent>
<nextsent>the first difference concerns the content of parses and the resulting transfer rules; in (mey ers et al, 1998), <papid> P98-2139 </papid>parses contain only lexical label sand syntactic roles (as arc labels), while our approach uses parses containing lexical labels, syntactic roles, and any other syntactic information provided by parsers (tense, number, person, etc.).the second difference concerns the node align ment; in (meyers et al, 1998), <papid> P98-2139 </papid>the alignment of source and target nodes is designed in way that preserves node dominancy in the source and target parses, while our approach does not have such restriction.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1336">
<title id=" W01-1403.xml">inducing lexico structural transfer rules from parsed bitexts </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, it also differs from (meyers et al, 1998) <papid> P98-2139 </papid>in several important points.</prevsent>
<prevsent>the first difference concerns the content of parses and the resulting transfer rules; in (mey ers et al, 1998), <papid> P98-2139 </papid>parses contain only lexical label sand syntactic roles (as arc labels), while our approach uses parses containing lexical labels, syntactic roles, and any other syntactic information provided by parsers (tense, number, person, etc.).the second difference concerns the node align ment; in (meyers et al, 1998), <papid> P98-2139 </papid>the alignment of source and target nodes is designed in way that preserves node dominancy in the source and target parses, while our approach does not have such restriction.</prevsent>
</prevsection>
<citsent citstr=" J94-4004 ">
one of the reasons for this difference is due to the different language pairs under study; (meyers et al, 1998) <papid> P98-2139 </papid>deals with two languages that are closely related syntactically (spanish and english) while we are dealing with languages that syntactically are quite divergent, korean and english (dorr, 1994).<papid> J94-4004 </papid></citsent>
<aftsection>
<nextsent>the third difference is in the process of identification of transfer rules candi dates; in (meyers et al, 1998), <papid> P98-2139 </papid>the identification is done by using the exact tree fragments in the source and target parse that are delimited by the alignment, while we use all source and target treesub-patterns matching subset of the parse features that satisfy customizable set of alignment constraints and attribute constraints.</nextsent>
<nextsent>the fourth third difference is in the level of abstraction of transfer rules candidates; in (meyers et al, 1998), <papid> P98-2139 </papid>the source and target patterns of each transfer rule are fully lexicalized (except possibly the terminalnodes), while in our approach the nodes of transfer rules do not have to be lexicalized.section 2 describes our approach to transfer rules induction and its integration with data preparation and evaluation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1343">
<title id=" W01-1403.xml">inducing lexico structural transfer rules from parsed bitexts </title>
<section> data preparation.  </section>
<citcontext>
<prevsection>
<prevsent>3.1 parsing the bi-texts.
</prevsent>
<prevsent>in our experiments to date, we have used corpus consisting of korean dialog of 4183 sentences and their english human translations.
</prevsent>
</prevsection>
<citsent citstr=" P97-1003 ">
weran off-the-shelf parsers on each half of the corpus, namely the korean parser developed by yoon et al (1997) and the english parser developed by collins (1997).<papid> P97-1003 </papid></citsent>
<aftsection>
<nextsent>neither parser was trained on our corpus.we automatically converted the phrase structure output of the collins parser into the syntactic dependency representation used by our syntactic realizer, realpro (lavoie and rambow,1997).
</nextsent>
<nextsent>this representation is based on the deep syntactic structures (dsynts) of meaning-text theory (melcuk, 1988).
</nextsent>
<nextsent>the important features of dsynts are as follows: ? dsynts is an unordered tree with labeled nodes and labeled arcs; ? dsynts is lexicalized, meaning that the nodes are labeled with lexemes (uninflected words) from the target language; ? dsynts is dependency structure and not aphrase- structure structure: there are no nonterminal nodes, and all nodes are labeled with lexemes; ? dsynts is syntactic representation,meaning that the arcs of the tree are labeled with syntactic relations such as subject (represented in dsyntss as i), rather than conceptual or semantic relations such as agent; ? dsynts is deep syntactic representation, meaning that only meaning-bearing lexemes are represented, and not function words.since the output of the yoon parser is quite similar, with the exception of its treatment of syntactic relations, we have used its output as is.the dsynts representations for two corresponding korean1 and english sentences are illustrated in figure 1.
</nextsent>
<nextsent>in examining the outputs of the two parsers on our corpus, we found that about half of the parse pairs contained incorrect dependency assignments, incomplete lemmatization or incomplete parses.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1345">
<title id=" W00-1410.xml">reinterpretation of an existing nlg system in a generic generation architecture </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this exercise it was important choose sys-tem that had been developed by people outside the rags project.
</prevsent>
<prevsent>equally, it was important have sufficient clear information about the system in the available literature, and/or by means of personal contact with the developers.
</prevsent>
</prevsection>
<citsent citstr=" J98-3004 ">
the system chosen was the caption generation system (mittal et al, 1995; mittal et al, 1998) <papid> J98-3004 </papid>3.</citsent>
<aftsection>
<nextsent>this system was chosen be- cause, as well as fulfilling the criteria above, it ap-peared to be relatively simple pipeline, thus avoid-ing complex control issues, with individual modules performing the varied linguistic tasks that the rags data structures had been designed to handle.
</nextsent>
<nextsent>the reinterpretation exercise took the form of coming up with an account of how the interfaces to the cgs modules corresponded to the rags model and re implementing working version of each module (apart from text planning and realisa- tion) which was tested to ensure that, given appro-priate input, its output was correct (i.e. conforming to the global account) on key examples.
</nextsent>
<nextsent>naturally, given the scope of this exercise, we had to gloss over some interesting implementational issues.
</nextsent>
<nextsent>the aim was not to produce complete system or system as good as cgs, but merely to demonstrate hat the broad functionality of the system could be repro- plied 2 nlg system.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1346">
<title id=" W00-1410.xml">reinterpretation of an existing nlg system in a generic generation architecture </title>
<section> the rags data types.  </section>
<citcontext>
<prevsection>
<prevsent>finally we discuss,, the :implications:.
</prevsent>
<prevsent>-._..sentations ,are--tree-structures with,rhetorical .rela- for rags of this exercise, tions at the internal nodes and abstract rhetorical
</prevsent>
</prevsection>
<citsent citstr=" W94-0319 ">
the rags project initially set out to develop ref-erence architecture based on the three-stage pipeline suggested by reiter (reiter, 1994).<papid> W94-0319 </papid></citsent>
<aftsection>
<nextsent>however, trees or abstract semantic representations at the leaves.
</nextsent>
<nextsent>rhetorical abstract rhetorical representations are viewed as descriptions of sets of possible rhetorical representations.
</nextsent>
<nextsent>each one may be trans- detailed analysis of existing applied nlg systems formed into some subset of the possible rhetori- (cahill and reape~_~ l:998}:suggested~,that~ttch.an~ ar -~: ~ .ealreprese, nta tions by,,means ~ofa,set..o_f~.petmitted chitecture was not specific enough and not closely transformations, e.g. reversing the order of nucleus enough adhered to by the majority of the systems surveyed for this to be used as the basis of the archi-tecture.
</nextsent>
<nextsent>the abstract functionality of generation system can be specified without specific reference to pro-cessing.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1347">
<title id=" W00-1410.xml">reinterpretation of an existing nlg system in a generic generation architecture </title>
<section> the rags data types.  </section>
<citcontext>
<prevsection>
<prevsent>at this level, seman-tic predicates and roles are those used in the api to query the knowledge base and arguments are knowl-edge base entities.
</prevsent>
<prevsent>semantic (concrete) semantic representations provide complete notation for  logical forms  where there is no longer any reference to ,the knowl-edge base.
</prevsent>
</prevsection>
<citsent citstr=" H89-1022 ">
the representations are based on sys-tems such as spl (kasper, 1989) <papid> H89-1022 </papid>and drt (kamp and reyle, 1993).</citsent>
<aftsection>
<nextsent>4more details can be found in (cahill et al., 1999) and at the rags project web site: ht tp : / /www.
</nextsent>
<nextsent>i r . r ighton , ac.
</nextsent>
<nextsent>uk/rags.
</nextsent>
<nextsent>and satellite or changing the rhetorical relation to one within permitted set.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1348">
<title id=" W00-1410.xml">reinterpretation of an existing nlg system in a generic generation architecture </title>
<section> partial and mixed representations.  </section>
<citcontext>
<prevsection>
<prevsent>the input to most reusable realisation sys-tems is also best viewed as mixture of semantic and abstract syntactic information.
</prevsent>
<prevsent>the extra flexibility of having partial and mixed representations turned out to be vital in the recon-struction of the cgs system.
</prevsent>
</prevsection>
<citsent citstr=" A00-1017 ">
(mellish et al, 2000).<papid> A00-1017 </papid></citsent>
<aftsection>
<nextsent>the caption generation system (cgs) generates explanatory captions of graphical presentations (2- charts and graphs).
</nextsent>
<nextsent>its architecture is pipeline with several modules, shown in the left hand part of figure 1.
</nextsent>
<nextsent>an example of diagram and its accom-panying text are given in figure 2.
</nextsent>
<nextsent>the propositions are numbered for ease of reference throughout the paper.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1349">
<title id=" W00-1410.xml">reinterpretation of an existing nlg system in a generic generation architecture </title>
<section> the cgs system.  </section>
<citcontext>
<prevsection>
<prevsent>the ordering module receives as input the dis-course plan with links specifying the ordering re-lations between sub-trees and specifies an order for them based on heuristics uch as that the description should be done from left to right in the visual space.
</prevsent>
<prevsent>the aggregation module  only con joins pairs of contiguous propositions about the same grapheme type 5 in the same space  (mittai et al, 1999) and inserts cue phrases compatible with the propositions o ( .=.,  whereas  for contrastive ones).
</prevsent>
</prevsection>
<citsent citstr=" J95-2003 ">
the internal order of the sentence constituents determined by the centering module using an extension of the cen-tering theory of grosz and colleagues (grosz et al, 1995).<papid> J95-2003 </papid></citsent>
<aftsection>
<nextsent>the referring expression module uses date and reiter (dale and reiter, 1995) algorithm to con-struct the set of attributes that can uniquely identify referent.
</nextsent>
<nextsent>there are two, situations where the text planning module helps specifically in the generation of referring expressions: (1) when the complexity for expressing graphic demands an example and 5 graphemes are the basic building blocks for constructing pictures.
</nextsent>
<nextsent>marks, text, lines and bars are some of the different grapheme classes available in sage.
</nextsent>
<nextsent>(ivlittal et al, 1999).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1351">
<title id=" W01-0521.xml">corpus variation and parser performance </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this leads us to technique for pruning parameters to reduce the size of the parsing model.
</prevsent>
<prevsent>the past several years have seen great progress in the eld of natural language parsing, through the use of statistical methods trained using large corpora ofhand-parsed training data.
</prevsent>
</prevsection>
<citsent citstr=" P97-1003 ">
the techniques of charniak (1997), collins (1997), <papid> P97-1003 </papid>and ratnaparkhi (1997)<papid> W97-0301 </papid>achieved roughly comparable results using the same sets of training and test data.</citsent>
<aftsection>
<nextsent>in each case, the corpus used was the penn treebank hand-annotated parses of wall street journal articles.
</nextsent>
<nextsent>relatively few quantitative parsing results have been reported on other corpora (though see stolcke et al (1996) for results on switchboard, as well as collins et al.
</nextsent>
<nextsent>(1999) for results on czech and hwa (1999) <papid> P99-1010 </papid>for bootstrapping from wsj to atis).</nextsent>
<nextsent>the inclusion of parses for the brown corpus in the penn treebank allows us to compare parser performance across corpora.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1352">
<title id=" W01-0521.xml">corpus variation and parser performance </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this leads us to technique for pruning parameters to reduce the size of the parsing model.
</prevsent>
<prevsent>the past several years have seen great progress in the eld of natural language parsing, through the use of statistical methods trained using large corpora ofhand-parsed training data.
</prevsent>
</prevsection>
<citsent citstr=" W97-0301 ">
the techniques of charniak (1997), collins (1997), <papid> P97-1003 </papid>and ratnaparkhi (1997)<papid> W97-0301 </papid>achieved roughly comparable results using the same sets of training and test data.</citsent>
<aftsection>
<nextsent>in each case, the corpus used was the penn treebank hand-annotated parses of wall street journal articles.
</nextsent>
<nextsent>relatively few quantitative parsing results have been reported on other corpora (though see stolcke et al (1996) for results on switchboard, as well as collins et al.
</nextsent>
<nextsent>(1999) for results on czech and hwa (1999) <papid> P99-1010 </papid>for bootstrapping from wsj to atis).</nextsent>
<nextsent>the inclusion of parses for the brown corpus in the penn treebank allows us to compare parser performance across corpora.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1354">
<title id=" W01-0521.xml">corpus variation and parser performance </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in each case, the corpus used was the penn treebank hand-annotated parses of wall street journal articles.
</prevsent>
<prevsent>relatively few quantitative parsing results have been reported on other corpora (though see stolcke et al (1996) for results on switchboard, as well as collins et al.
</prevsent>
</prevsection>
<citsent citstr=" P99-1010 ">
(1999) for results on czech and hwa (1999) <papid> P99-1010 </papid>for bootstrapping from wsj to atis).</citsent>
<aftsection>
<nextsent>the inclusion of parses for the brown corpus in the penn treebank allows us to compare parser performance across corpora.
</nextsent>
<nextsent>in this paper we examine the following ques tions:  to what extent is the performance of statistical parsers on the wsj task due to its relatively uniform style, and how might such parsers fare on the more varied brown corpus?
</nextsent>
<nextsent> can training data from one corpus be applied to parsing another?
</nextsent>
<nextsent> what aspects of the parser probability model are particularly tuned to one corpus, and which are more general?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1355">
<title id=" W01-0521.xml">corpus variation and parser performance </title>
<section> previous comparisons of corpora.  </section>
<citcontext>
<prevsection>
<prevsent>aside from cross-corpus considerations, this is an important nding if lightweight parser is desired or memory usage is consideration.
</prevsent>
<prevsent>a great deal of work has been done outside of the parsing community analyzing the variations between corpora and dierent genres of text.
</prevsent>
</prevsection>
<citsent citstr=" J93-2001 ">
biber (1993)<papid> J93-2001 </papid>investigated variation in number syntactic features over genres, or registers, of language.</citsent>
<aftsection>
<nextsent>of particular importance to statistical parsers is the investigation of frequencies for verb subcategoriza tions such as roland and jurafsky (1998).<papid> P98-2184 </papid></nextsent>
<nextsent>roland et al (2000) <papid> W00-0905 </papid>nd that subcategorization frequencies for certain verbs vary signi cantly between the wall street journal corpus and the mixed-genre brown corpus, but that they vary less so betweengenre-balanced british and american corpora.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1356">
<title id=" W01-0521.xml">corpus variation and parser performance </title>
<section> previous comparisons of corpora.  </section>
<citcontext>
<prevsection>
<prevsent>a great deal of work has been done outside of the parsing community analyzing the variations between corpora and dierent genres of text.
</prevsent>
<prevsent>biber (1993)<papid> J93-2001 </papid>investigated variation in number syntactic features over genres, or registers, of language.</prevsent>
</prevsection>
<citsent citstr=" P98-2184 ">
of particular importance to statistical parsers is the investigation of frequencies for verb subcategoriza tions such as roland and jurafsky (1998).<papid> P98-2184 </papid></citsent>
<aftsection>
<nextsent>roland et al (2000) <papid> W00-0905 </papid>nd that subcategorization frequencies for certain verbs vary signi cantly between the wall street journal corpus and the mixed-genre brown corpus, but that they vary less so betweengenre-balanced british and american corpora.</nextsent>
<nextsent>argument structure is essentially the task that automatic parsers attempt to solve, and the frequencies of various structures in training data are re ected ina statistical parser probability model.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1357">
<title id=" W01-0521.xml">corpus variation and parser performance </title>
<section> previous comparisons of corpora.  </section>
<citcontext>
<prevsection>
<prevsent>biber (1993)<papid> J93-2001 </papid>investigated variation in number syntactic features over genres, or registers, of language.</prevsent>
<prevsent>of particular importance to statistical parsers is the investigation of frequencies for verb subcategoriza tions such as roland and jurafsky (1998).<papid> P98-2184 </papid></prevsent>
</prevsection>
<citsent citstr=" W00-0905 ">
roland et al (2000) <papid> W00-0905 </papid>nd that subcategorization frequencies for certain verbs vary signi cantly between the wall street journal corpus and the mixed-genre brown corpus, but that they vary less so betweengenre-balanced british and american corpora.</citsent>
<aftsection>
<nextsent>argument structure is essentially the task that automatic parsers attempt to solve, and the frequencies of various structures in training data are re ected ina statistical parser probability model.
</nextsent>
<nextsent>the variation in verb argument structure found by previous research caused us to wonder to what extent model trained on one corpus would be useful in parsing an other.
</nextsent>
<nextsent>the probability models of modern parsers include not only the number and syntactic type of word arguments, but lexical information about their llers.
</nextsent>
<nextsent>although we are not aware of previous comparisons of the frequencies of argument llers, we can only assume that they vary at least as much as the syntactic subcategorization frames.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1359">
<title id=" W01-0521.xml">corpus variation and parser performance </title>
<section> the parsing model.  </section>
<citcontext>
<prevsection>
<prevsent>; c; cht)for each of the three distributions, the empirical distribution of the training data is interpolated with less speci backo distributions, as we will see in section 5.
</prevsent>
<prevsent>further details of the model, including the distance features used and special handling of punctuation, conjunctions, and base noun phrases, are described in collins (1999).the fundamental features of used in the probability distributions are the lexical heads and head tags of each constituent, the co-occurrences of parent nodes and their head children, and the cooccurrences of child nodes with their head siblings and parents.
</prevsent>
</prevsection>
<citsent citstr=" P95-1037 ">
the probability models of charniak (1997), magerman (1995) <papid> P95-1037 </papid>and ratnaparkhi (1997)<papid> W97-0301 </papid>dier in their details but are based on similar fea tures.</citsent>
<aftsection>
<nextsent>models 2 and 3 of collins (1997) <papid> P97-1003 </papid>add some slightly more elaborate features to the probability model, as do the additions of charniak (2000) <papid> A00-2018 </papid>to the model of charniak (1997).</nextsent>
<nextsent>our implementation of collins  model 1 perform sat 86% precision and recall of labeled parse constituents on the standard wall street journal training and test sets.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1362">
<title id=" W01-0521.xml">corpus variation and parser performance </title>
<section> the parsing model.  </section>
<citcontext>
<prevsection>
<prevsent>further details of the model, including the distance features used and special handling of punctuation, conjunctions, and base noun phrases, are described in collins (1999).the fundamental features of used in the probability distributions are the lexical heads and head tags of each constituent, the co-occurrences of parent nodes and their head children, and the cooccurrences of child nodes with their head siblings and parents.
</prevsent>
<prevsent>the probability models of charniak (1997), magerman (1995) <papid> P95-1037 </papid>and ratnaparkhi (1997)<papid> W97-0301 </papid>dier in their details but are based on similar fea tures.</prevsent>
</prevsection>
<citsent citstr=" A00-2018 ">
models 2 and 3 of collins (1997) <papid> P97-1003 </papid>add some slightly more elaborate features to the probability model, as do the additions of charniak (2000) <papid> A00-2018 </papid>to the model of charniak (1997).</citsent>
<aftsection>
<nextsent>our implementation of collins  model 1 perform sat 86% precision and recall of labeled parse constituents on the standard wall street journal training and test sets.
</nextsent>
<nextsent>while this does notre ect the state-of-the-art performance on the wsj task achieved by the more the complex models of charniak (2000) <papid> A00-2018 </papid>and collins (2000), we regard it as reasonable baseline for the investigation of corpus eects on statistical parsing.</nextsent>
<nextsent>corpus we conducted separate experiments using wsj data, brown data, and combination of the twoas training material.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1365">
<title id=" W01-0521.xml">corpus variation and parser performance </title>
<section> the eect of lexical.  </section>
<citcontext>
<prevsection>
<prevsent>the more varied nature of the brown corpus also seems to impact results, as all the results on brown are lower than the wsj result.
</prevsent>
<prevsent>dependencies the parsers cited above all use some variety of lexical dependency feature to capture statistics on the cooccurrence of pairs of words being found in parent child relations within the parse tree.
</prevsent>
</prevsection>
<citsent citstr=" P96-1025 ">
these word pair relations, also called lexical bigrams (collins, 1996), <papid> P96-1025 </papid>are reminiscent of dependency grammars such as me  lcuk (1988) and the link grammar of sleator and temperley (1993).</citsent>
<aftsection>
<nextsent>in collins  model 1, the word pair statistics occur in the distribution cw (chwjp;h;hht;hhw;
</nextsent>
<nextsent>; c; cht) wherehhw represent the head word of parent node in the tree and chw the head word of its (non-head) child.
</nextsent>
<nextsent>(the head word of parent is the same as the head word of its head child.)
</nextsent>
<nextsent>because this is the only part of the model that involves pairs of words, it is also where the bulk of the parameters are found.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1369">
<title id=" W01-0902.xml">empirical methods for evaluating dialog systems </title>
<section> purpose.  </section>
<citcontext>
<prevsection>
<prevsent>detailing its internal processing.
</prevsent>
<prevsent>if one metric fails to suffice, dialog metrics can be combined.
</prevsent>
</prevsection>
<citsent citstr=" P97-1035 ">
for example, the paradise framework allows designers to predict user satisfaction from linear combination of objective metrics such as mean recognition score and task completion (kamm et al., 1999; litman &amp; pan, 1999; walker et al, 1997).<papid> P97-1035 </papid></citsent>
<aftsection>
<nextsent>why so many metrics?
</nextsent>
<nextsent>the answer has to do with more than just the absence of agreed upon standards in the research community, notwithstanding significant efforts in that direction (gibbon et al, 1997).
</nextsent>
<nextsent>part of the reason deals with what purpose dialog metric serves.
</nextsent>
<nextsent>designers often have multiple and sometimes inconsistent needs.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1370">
<title id=" W00-1206.xml">enhancement of a chinese discourse marker tagger with c45 </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the function of discourse analysis is to divide text into discourse segments, and to recognize and re-construct the discourse structure of the text as intended by its author.
</prevsent>
<prevsent>automatic text abstraction has received considerable attention (paice 1990).
</prevsent>
</prevsection>
<citsent citstr=" W00-0402 ">
various systems have been developed (chan et al 2000).<papid> W00-0402 </papid></citsent>
<aftsection>
<nextsent>ono et al (1994), <papid> C94-1056 </papid>sou et al (1992) <papid> C92-3162 </papid>and marcu (1997) <papid> W97-0713 </papid>focus on discourse structure in summarization using the rhetorical structure theory (rst, mann and thompson 1986).</nextsent>
<nextsent>the theory has been exploited in number of computational systems (e.g. hovy 1993).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1371">
<title id=" W00-1206.xml">enhancement of a chinese discourse marker tagger with c45 </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>automatic text abstraction has received considerable attention (paice 1990).
</prevsent>
<prevsent>various systems have been developed (chan et al 2000).<papid> W00-0402 </papid></prevsent>
</prevsection>
<citsent citstr=" C94-1056 ">
ono et al (1994), <papid> C94-1056 </papid>sou et al (1992) <papid> C92-3162 </papid>and marcu (1997) <papid> W97-0713 </papid>focus on discourse structure in summarization using the rhetorical structure theory (rst, mann and thompson 1986).</citsent>
<aftsection>
<nextsent>the theory has been exploited in number of computational systems (e.g. hovy 1993).
</nextsent>
<nextsent>the main idea is to build discourse tree where each node of the tree represents an rst relation.
</nextsent>
<nextsent>summarization is achieved by trimming lmimportant sentences on the basis of the relative saliency or rhetorical relations.
</nextsent>
<nextsent>the sifas (syntactic marker based full-text abstraction system) system has been implemented to use discourse markers in the automatic summarization of chinese (t sou et al 1999).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1372">
<title id=" W00-1206.xml">enhancement of a chinese discourse marker tagger with c45 </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>automatic text abstraction has received considerable attention (paice 1990).
</prevsent>
<prevsent>various systems have been developed (chan et al 2000).<papid> W00-0402 </papid></prevsent>
</prevsection>
<citsent citstr=" C92-3162 ">
ono et al (1994), <papid> C94-1056 </papid>sou et al (1992) <papid> C92-3162 </papid>and marcu (1997) <papid> W97-0713 </papid>focus on discourse structure in summarization using the rhetorical structure theory (rst, mann and thompson 1986).</citsent>
<aftsection>
<nextsent>the theory has been exploited in number of computational systems (e.g. hovy 1993).
</nextsent>
<nextsent>the main idea is to build discourse tree where each node of the tree represents an rst relation.
</nextsent>
<nextsent>summarization is achieved by trimming lmimportant sentences on the basis of the relative saliency or rhetorical relations.
</nextsent>
<nextsent>the sifas (syntactic marker based full-text abstraction system) system has been implemented to use discourse markers in the automatic summarization of chinese (t sou et al 1999).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1373">
<title id=" W00-1206.xml">enhancement of a chinese discourse marker tagger with c45 </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>automatic text abstraction has received considerable attention (paice 1990).
</prevsent>
<prevsent>various systems have been developed (chan et al 2000).<papid> W00-0402 </papid></prevsent>
</prevsection>
<citsent citstr=" W97-0713 ">
ono et al (1994), <papid> C94-1056 </papid>sou et al (1992) <papid> C92-3162 </papid>and marcu (1997) <papid> W97-0713 </papid>focus on discourse structure in summarization using the rhetorical structure theory (rst, mann and thompson 1986).</citsent>
<aftsection>
<nextsent>the theory has been exploited in number of computational systems (e.g. hovy 1993).
</nextsent>
<nextsent>the main idea is to build discourse tree where each node of the tree represents an rst relation.
</nextsent>
<nextsent>summarization is achieved by trimming lmimportant sentences on the basis of the relative saliency or rhetorical relations.
</nextsent>
<nextsent>the sifas (syntactic marker based full-text abstraction system) system has been implemented to use discourse markers in the automatic summarization of chinese (t sou et al 1999).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1374">
<title id=" W01-1617.xml">plug and play speech understanding </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>there are already several promising candidate platforms for achieving the necessary functionality, including universal plug and play (upnp) (microsoft, 2000) and jini (oaks and wong, 2000).
</prevsent>
<prevsent>in this paper, we address the requirements on spoken dialogue interfaces that arise from plug and play domain.
</prevsent>
</prevsection>
<citsent citstr=" N01-1030 ">
we also present the current state of our english language plug and play demonstrator for controlling lamps, dimmers and sensors, previously described in (rayner et al, 2001<papid> N01-1030 </papid>b).</citsent>
<aftsection>
<nextsent>(there is also swedish instantiation).
</nextsent>
<nextsent>first, however, we need brie to distinguish our notion from other notions of plug and play and recon gurability.
</nextsent>
<nextsent>the notion of plug and play has been used for dialogue system tool kits in which the variousdierent language processing components themselves (e.g. recognition, parsing, generation and dialogue management) can be plugged in and out.
</nextsent>
<nextsent>the most prominent instance of this is the darpa communicator architecture (goldschen and loehr, 1999), which denes interoperability standards for language processing components.the intention is simply that researchers and developers can experiment with systems containingdierent instantiations of the language processing components.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1376">
<title id=" W01-1617.xml">plug and play speech understanding </title>
<section> distributed grammar.  </section>
<citcontext>
<prevsection>
<prevsent>an approach of this kind fails however to scale up to an interface which supports complex commands, in particular commands which combine within the same utterance language referring to two or more dierent devices.
</prevsent>
<prevsent>for example, command may address several devices at once (\turn on the radio and the living room light ); alternately, several commands may be combined into single utterance (\switch on the cooker and switch o the microwave ).
</prevsent>
</prevsection>
<citsent citstr=" P00-1018 ">
our experience with practical spoken device interfaces suggests that examples like these are by no means uncommon.another architecture relatively easy to combine with plug and play is doing recognition through general large-vocabulary recogniser, and language-processing through device-speci phrase-spotting (milward, 2000).<papid> P00-1018 </papid></citsent>
<aftsection>
<nextsent>the recogniser stays the same irrespective of how many devi cesare connected, so there are by de nition no problems at the level of speech recognition, and it is in principle possible to support complex commands.
</nextsent>
<nextsent>the main drawback, however, is that recognition quality is markedly inferior compared to syst emin which recognition coverage is limited to the do main de ned by the current set of devices.
</nextsent>
<nextsent>modern speech interfaces supporting complex commands are typically speci ed using rule based grammar formalism de ned by platform like nuance (nuance communications, 1999) or speech works (inc, 2001).
</nextsent>
<nextsent>the type of grammar supported is some subset of full cfg, extended to include semantic annotations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1379">
<title id=" W02-0209.xml">a flexible framework for developing mixed initiative dialog systems </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the forms have powerful built-in capabilities, including mechanisms that trigger various types of prompts, and allow the user to specify inheritance and other relationships between tasks.
</prevsent>
<prevsent>just as in the mit framework, domain specific modules perform database queries and other backend processes; the forms call additional developer-defined modules that affect the dialog state and flow.
</prevsent>
</prevsection>
<citsent citstr=" W00-0305 ">
fdm has supported dialog systems for air travel (papineni et al 1999, axelrod 2000) <papid> W00-0305 </papid>and financial services (ibm 2001, ibm 2002).</citsent>
<aftsection>
<nextsent>the university of colorado framework also has form-based architecture (pellom et al 2001), <papid> H01-1073 </papid>while cmu and bell labs?</nextsent>
<nextsent>frameworks allow the specification of deep task hierarchies (wei and rudnicky 2000, potamianos et al 2000).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1380">
<title id=" W02-0209.xml">a flexible framework for developing mixed initiative dialog systems </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>just as in the mit framework, domain specific modules perform database queries and other backend processes; the forms call additional developer-defined modules that affect the dialog state and flow.
</prevsent>
<prevsent>fdm has supported dialog systems for air travel (papineni et al 1999, axelrod 2000) <papid> W00-0305 </papid>and financial services (ibm 2001, ibm 2002).</prevsent>
</prevsection>
<citsent citstr=" H01-1073 ">
the university of colorado framework also has form-based architecture (pellom et al 2001), <papid> H01-1073 </papid>while cmu and bell labs?</citsent>
<aftsection>
<nextsent>frameworks allow the specification of deep task hierarchies (wei and rudnicky 2000, potamianos et al 2000).
</nextsent>
<nextsent>our goal is to design framework that is both powerful, embodying much dialog functionality, and flexible, accommodating variety of dialog domains, modalities, and styles.
</nextsent>
<nextsent>our new framework goes beyond fdm in building more core functionality into its task model, yet provides variety of software tools, such as api calls and overwritable functions, for customizing tasks.
</nextsent>
<nextsent>the framework allows developers to specify wide range of relationships among tasks, and provides focus model that respects these relationships.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1381">
<title id=" W02-0209.xml">a flexible framework for developing mixed initiative dialog systems </title>
<section> the hot framework.  </section>
<citcontext>
<prevsection>
<prevsent>s status.
</prevsent>
<prevsent>the dialog history is reminiscent of bbn?
</prevsent>
</prevsection>
<citsent citstr=" A00-1010 ">
s state manager (stallard 2000), <papid> A00-1010 </papid>but the latter also includes many dialog management responsibilities that we reserve to tasks.</citsent>
<aftsection>
<nextsent>we are currently implementing the hot framework described above.
</nextsent>
<nextsent>in this section, we describe two sample applications built using this framework as implemented so far.
</nextsent>
<nextsent>3.1 mutual funds.
</nextsent>
<nextsent>we built mutual funds application, using the framework to create task hierarchy and to add task relationships such as ordering and inheritance, as described in section 2.1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1382">
<title id=" W00-0505.xml">towards trans lingual information access using portable information extraction </title>
<section> system design.  </section>
<citcontext>
<prevsection>
<prevsent>the mt component.
</prevsent>
<prevsent>the mt component (cf.
</prevsent>
</prevsection>
<citsent citstr=" A00-1009 ">
lavoie et al, 2000) <papid> A00-1009 </papid>translates the extracted korean phrases or sentences into corresponding english ones.</citsent>
<aftsection>
<nextsent>the presentation generator component.
</nextsent>
<nextsent>this component generates well-organized, easy-to-read hypertext presentations by organizing and formatting the ranked extracted information.
</nextsent>
<nextsent>it uses existing nlg components, including the exemplars text planning framework (white and caldwell, 1998) <papid> W98-1428 </papid>and the realpro syntactic realizer (lavoie and rainbow, 1997).</nextsent>
<nextsent>in our feasibility study, the majority of the effort went towards developing the pie component, described in the next section.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1383">
<title id=" W00-0505.xml">towards trans lingual information access using portable information extraction </title>
<section> system design.  </section>
<citcontext>
<prevsection>
<prevsent>the presentation generator component.
</prevsent>
<prevsent>this component generates well-organized, easy-to-read hypertext presentations by organizing and formatting the ranked extracted information.
</prevsent>
</prevsection>
<citsent citstr=" W98-1428 ">
it uses existing nlg components, including the exemplars text planning framework (white and caldwell, 1998) <papid> W98-1428 </papid>and the realpro syntactic realizer (lavoie and rainbow, 1997).</citsent>
<aftsection>
<nextsent>in our feasibility study, the majority of the effort went towards developing the pie component, described in the next section.
</nextsent>
<nextsent>this component was implemented in general way, i.e. in way that we would expect to work beyond the specific training/test corpus described below.
</nextsent>
<nextsent>in contrast, we only implemented initial versions of the user interface, ranker and presentation generator components, in order to demonstrate the system concept; that is, these initial versions were only intended.to work with our training/test corpus, and will require considerable further development prior to reaching operational status.
</nextsent>
<nextsent>for the mt component, we used an early version of the lexical transfer-based system currently underdevelopment in an ongoing sbir phase ii project (cf.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1385">
<title id=" W00-0505.xml">towards trans lingual information access using portable information extraction </title>
<section> portable information extraction.  </section>
<citcontext>
<prevsection>
<prevsent>we did not attempt normalize dates or remove appositives from nps.
</prevsent>
<prevsent>4.2 extraction pattern learning.
</prevsent>
</prevsection>
<citsent citstr=" M92-1038 ">
for our feasibility study, we chose to follow the auto slog (lehnert et al, 1992; <papid> M92-1038 </papid>riloff, 1993) approach to extraction pattern acquisition.</citsent>
<aftsection>
<nextsent>in this approach, extraction patterns are acquired 34 i. e: k:  target-np = subject   active voice verb   participant  met  target-np = subject   active voice verb   john-i  mannassta  john-nom  met 2.
</nextsent>
<nextsent>e: k:  target-np = subject   verb   infinitive   participant  agreed to meet  target-np = subject   verbl-ki- lo   verb2   john-un  manna-ki- lo hapuyhayssta  john-nom  meet-ki- lo agreed (-ki: nominal ization ending, -io: an adverbial postposition) figure 3 via one-shot general-to-specific learning algorithm designed specifically for the information extraction task.
</nextsent>
<nextsent>3 the learning algorithm is straightforward and depends only on the existence of (partial) parser and small set of general inguistic patterns that direct the creation of specific patterns.
</nextsent>
<nextsent>as training corpus, it requires set of texts with noun phrases annotated with the slot type to be extracted.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1386">
<title id=" W01-0814.xml">a paraphrase based exploration of cohesiveness criteria </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a system thus needs to know how to use such cohesive devices effectively1.
</prevsent>
<prevsent>the translation and summarization research communities are, in fact, increasingly getting concerned with the notion of cohesion, though only recently, as technologies for intra-sentential processing make progress.
</prevsent>
</prevsection>
<citsent citstr=" A00-2002 ">
for example, marcu et al (2000) <papid> A00-2002 </papid>proposes computational model for transforming rhetorical structures between source and target languages, aiming at the improvement of translation quality.</citsent>
<aftsection>
<nextsent>mani etal.
</nextsent>
<nextsent>(1999) proposes to incorporate the cohesion concerned revision process into summarization.
</nextsent>
<nextsent>what is commonly required in such tasks is technology for assessing given text according to cohesiveness.
</nextsent>
<nextsent>such technology would enable us to design, for example, translation model which would choose the best cohesive translation from more than one generated candidate.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1387">
<title id=" W01-0814.xml">a paraphrase based exploration of cohesiveness criteria </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>alternatively, it might also be feasible to design framework where system would revise an initial translation according to cohesion.
</prevsent>
<prevsent>in this paper, 1in this paper, we use the term cohesiveness to refer to the degree to which cohesive devices are properly chosen to realize cohesive relations in text, whether it is quantified or not.
</prevsent>
</prevsection>
<citsent citstr=" W96-0401 ">
it is matter of surface realization or sentence planning (wanner and hovy, 1996; <papid> W96-0401 </papid>reiter and dale, 1999).</citsent>
<aftsection>
<nextsent>here we distinguish it from the notion of coherence, which is typically used to refer to the degree to which the text contents themselves are well-organized at the conceptual level, and thus matter of content selection/organization.
</nextsent>
<nextsent>this paper concentrates its focus on the former stratum.we argue that the nlg technologies for generating certain types of structural paraphrases can be used to efficiently create training and testing data which would serve as good resource for empirical corpus-based study on cohesiveness evalua tion.to give our intuition to the reader, let us consider the following example, where (1t.1) and (1t.2) are both paraphrases of source passage (1s)2: (1s) smaland, which is located to the south-west of stockholm, is called the kingdom of glass?.
</nextsent>
<nextsent>the reason is that there are sixteen glass manufacturers in this area.
</nextsent>
<nextsent>(1t.1) smaland is located to the south-west of stockholm.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1393">
<title id=" W01-0814.xml">a paraphrase based exploration of cohesiveness criteria </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we then present our pilot case study, in which we took particular type of paraphrasing that separates relative clause from sentence, reporting the results of preliminary experiment, in section 4 and 5.
</prevsent>
<prevsent>generation one may expect that criteria or techniques for cohesiveness evaluation should be easily found in the literature on text generation for couple of reasons:  there have been quite few cohesion-related works in this field.
</prevsent>
</prevsection>
<citsent citstr=" W98-1414 ">
for example, works on discourse marker choice (scott and desouza, 1990; vander linden, 1994; grote and stede, 1998; <papid> W98-1414 </papid>oates, 1999), generation of referring expressions (dale, 1992), and clause aggregation based on discourse relation (dalianis and hovy, 1996; show, 1998)are all related to linguistic realization of cohesive relations.</citsent>
<aftsection>
<nextsent> text generation has the advantage of servin gas good device for systematically producing diverse text variants from the same input.
</nextsent>
<nextsent>imagine, for example, that nlg system could generate passages (1s), (1t.1) and(1t.2) in parallel from the same content specifications.
</nextsent>
<nextsent>if given such collection of text variants containing both cohesive and inco hesive instances in parallel, one could carryout steady explorations of cohesiveness criteria, since having negative instances besides positive ones would significantly facilitate generalization.
</nextsent>
<nextsent>unfortunately, however, there have so far been very few works in this field that have shown comprehensive and concrete criteria of cohesiveness.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1398">
<title id=" W01-0814.xml">a paraphrase based exploration of cohesiveness criteria </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this facilitates scaling up of an instance collection.
</prevsent>
<prevsent> the second advantage potentially enables us to make so-called selective sampling, which has been empirically proven to effectively accelerate learning, while reducing manual annotation costs, in many knowledge acquisition tasks, e.g.
</prevsent>
</prevsection>
<citsent citstr=" J98-4002 ">
(fujii et al, 1998).<papid> J98-4002 </papid></citsent>
<aftsection>
<nextsent>4 case studywe conducted pilot case study, taking particular type of paraphrasing which separates relative clause from given sentence as in example (1) in section 1.
</nextsent>
<nextsent>hereafter, for simplicity, weuse the term paraphras  e,ing  to refer to para phras  e,ing  of this type, as far as the present case study is concerned.
</nextsent>
<nextsent>furthermore, for convenience, we call the sentence originating from relative clause satellite sentence (or simply asatellite), and the sentence that consists of the remaining constituents of the source sentence nucleus sentence (or simply nucleus).
</nextsent>
<nextsent>the target language we have so far explored isonly japanese.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1399">
<title id=" W01-0814.xml">a paraphrase based exploration of cohesiveness criteria </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>preference 2.2 if the gap of the relative clause is not associated with the nominative case, the gap filler is preferred to be topicalized.
</prevsent>
<prevsent>5.4 anaphora and ellipsis.
</prevsent>
</prevsection>
<citsent citstr=" C00-1031 ">
several works have explored the relation between rhetorical structure and reference in english (fox, 1987; cristea et al, 2000; <papid> C00-1031 </papid>grosz and sidner, 1986; <papid> J86-3001 </papid>grosz et al, 1995).<papid> J95-2003 </papid></citsent>
<aftsection>
<nextsent>japanese reference,on the other hand, has been studied from different perspective, being associated mainly withthe linear nature of texts as in the centering theory (kameyama, 1986; <papid> P86-1031 </papid>walker et al, 1994).<papid> J94-2003 </papid></nextsent>
<nextsent>considering that choice of referring expressions isin itself quite large issue, we have been exploring it separately from this paraphrase-based exploration (hashimoto, 2001).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1400">
<title id=" W01-0814.xml">a paraphrase based exploration of cohesiveness criteria </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>preference 2.2 if the gap of the relative clause is not associated with the nominative case, the gap filler is preferred to be topicalized.
</prevsent>
<prevsent>5.4 anaphora and ellipsis.
</prevsent>
</prevsection>
<citsent citstr=" J86-3001 ">
several works have explored the relation between rhetorical structure and reference in english (fox, 1987; cristea et al, 2000; <papid> C00-1031 </papid>grosz and sidner, 1986; <papid> J86-3001 </papid>grosz et al, 1995).<papid> J95-2003 </papid></citsent>
<aftsection>
<nextsent>japanese reference,on the other hand, has been studied from different perspective, being associated mainly withthe linear nature of texts as in the centering theory (kameyama, 1986; <papid> P86-1031 </papid>walker et al, 1994).<papid> J94-2003 </papid></nextsent>
<nextsent>considering that choice of referring expressions isin itself quite large issue, we have been exploring it separately from this paraphrase-based exploration (hashimoto, 2001).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1401">
<title id=" W01-0814.xml">a paraphrase based exploration of cohesiveness criteria </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>preference 2.2 if the gap of the relative clause is not associated with the nominative case, the gap filler is preferred to be topicalized.
</prevsent>
<prevsent>5.4 anaphora and ellipsis.
</prevsent>
</prevsection>
<citsent citstr=" J95-2003 ">
several works have explored the relation between rhetorical structure and reference in english (fox, 1987; cristea et al, 2000; <papid> C00-1031 </papid>grosz and sidner, 1986; <papid> J86-3001 </papid>grosz et al, 1995).<papid> J95-2003 </papid></citsent>
<aftsection>
<nextsent>japanese reference,on the other hand, has been studied from different perspective, being associated mainly withthe linear nature of texts as in the centering theory (kameyama, 1986; <papid> P86-1031 </papid>walker et al, 1994).<papid> J94-2003 </papid></nextsent>
<nextsent>considering that choice of referring expressions isin itself quite large issue, we have been exploring it separately from this paraphrase-based exploration (hashimoto, 2001).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1402">
<title id=" W01-0814.xml">a paraphrase based exploration of cohesiveness criteria </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>5.4 anaphora and ellipsis.
</prevsent>
<prevsent>several works have explored the relation between rhetorical structure and reference in english (fox, 1987; cristea et al, 2000; <papid> C00-1031 </papid>grosz and sidner, 1986; <papid> J86-3001 </papid>grosz et al, 1995).<papid> J95-2003 </papid></prevsent>
</prevsection>
<citsent citstr=" P86-1031 ">
japanese reference,on the other hand, has been studied from different perspective, being associated mainly withthe linear nature of texts as in the centering theory (kameyama, 1986; <papid> P86-1031 </papid>walker et al, 1994).<papid> J94-2003 </papid></citsent>
<aftsection>
<nextsent>considering that choice of referring expressions isin itself quite large issue, we have been exploring it separately from this paraphrase-based exploration (hashimoto, 2001).
</nextsent>
<nextsent>we will not gointo the detail here, since we have so far implemented only the following well-known constraint and preference for the experiment.
</nextsent>
<nextsent>constraint 4.1 if two neighboring sentence shave different topics (themes), the topic of the following sentence should not be omitted.
</nextsent>
<nextsent>preference 4.1 if two neighboring sentences share the same topic (theme), the topic of the following sentence is preferred to be omitted.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1403">
<title id=" W01-0814.xml">a paraphrase based exploration of cohesiveness criteria </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>5.4 anaphora and ellipsis.
</prevsent>
<prevsent>several works have explored the relation between rhetorical structure and reference in english (fox, 1987; cristea et al, 2000; <papid> C00-1031 </papid>grosz and sidner, 1986; <papid> J86-3001 </papid>grosz et al, 1995).<papid> J95-2003 </papid></prevsent>
</prevsection>
<citsent citstr=" J94-2003 ">
japanese reference,on the other hand, has been studied from different perspective, being associated mainly withthe linear nature of texts as in the centering theory (kameyama, 1986; <papid> P86-1031 </papid>walker et al, 1994).<papid> J94-2003 </papid></citsent>
<aftsection>
<nextsent>considering that choice of referring expressions isin itself quite large issue, we have been exploring it separately from this paraphrase-based exploration (hashimoto, 2001).
</nextsent>
<nextsent>we will not gointo the detail here, since we have so far implemented only the following well-known constraint and preference for the experiment.
</nextsent>
<nextsent>constraint 4.1 if two neighboring sentence shave different topics (themes), the topic of the following sentence should not be omitted.
</nextsent>
<nextsent>preference 4.1 if two neighboring sentences share the same topic (theme), the topic of the following sentence is preferred to be omitted.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1404">
<title id=" W01-1313.xml">assigning time stamps to event clauses </title>
<section> time-stamper.  </section>
<citcontext>
<prevsection>
<prevsent>the last time interval assigned for sentence 5.2 is {1998:53:0}---{1998:71:0}, which gives an approximate range of days when the previous earthquake happened.
</prevsent>
<prevsent>but the information in sentence 5.3 is about the recent earthquake and not about the previous one of 3 months earlier, which is why it would be mistake to point monday and tuesday within that range.
</prevsent>
</prevsection>
<citsent citstr=" P00-1010 ">
mani and wilson (2000) <papid> P00-1010 </papid>point out over half of the errors [made by his time-stamper] were due to propagation of spreading of an incorrect event time to neighboring events?.</citsent>
<aftsection>
<nextsent>the rule of dropping the most recently assigned date as an anchor point when proceeding to the next sentence very often helps us to avoid this problem.
</nextsent>
<nextsent>there are however cases where dropping the most recent time as an anchor when proceeding to the next sentence causes errors: 4.8.1 but in february devastating earthquake in the same region killed 2,300 people and left thousands of people homeless.
</nextsent>
<nextsent>4.9.1 at the time international aid workers suffered through logistical nightmare to reach the snow-bound region with assistance.
</nextsent>
<nextsent>it is clear that sentence 4.9 is the continuation of sentence 4.8 and refers to the same time point (february earthquake).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1405">
<title id=" W01-1313.xml">assigning time stamps to event clauses </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>several linguistic and psycho linguistic studies deal with the problem of time-arrangement of different texts.
</prevsent>
<prevsent>the research presented in these studies highlights many problems but does not solve them.
</prevsent>
</prevsection>
<citsent citstr=" A97-1006 ">
as for computational applications of time theories, most work was done on temporal expressions that appear in scheduling dialogues (busemann et al , 1997; <papid> A97-1006 </papid>alexandresson et al , 1997).</citsent>
<aftsection>
<nextsent>there are many constraints on temporal expressions in this domain.
</nextsent>
<nextsent>the most relevant prior work is (mani and wilson, 2000), <papid> P00-1010 </papid>who implemented their system on news stories, introduced rules spreading time-stamps obtained with the help of explicit temporal expressions throughout the whole article, and invented machine learning rules for disambiguating between specific and generic use of temporal expressions (for example, whether christmas is used to denote the 25th of december or to denote some period of time around the 25th of december).</nextsent>
<nextsent>they also mention problem of disambiguating between temporal expression and proper name, as in usa today?.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1407">
<title id=" W00-1407.xml">a strategy for generating evaluative arguments </title>
<section> an operational definition for  emotionally.  </section>
<citcontext>
<prevsection>
<prevsent>the argumentation strategy has been implemented as set of plan operators.
</prevsent>
<prevsent>using these operators the longbow discourse planner (young and moore 1994) selects and arranges the content of the argument.
</prevsent>
</prevsection>
<citsent citstr=" W00-1402 ">
we have applied our strategy in system that serves as real- estate personal assistant (carenini 2000<papid> W00-1402 </papid>a).</citsent>
<aftsection>
<nextsent>the system presents information about houses available on the market in graphical format.
</nextsent>
<nextsent>the user explores this information by means of interactive techniques, and can request natural 7 the steps in the strategy are marked with the.
</nextsent>
<nextsent>guideline they are based on.
</nextsent>
<nextsent>51 argue(subject, root, argint, ) ;; assignments and content selection f subject = single-entity = then svo, = vol (e) measure-of-strength = s-compel!ingness   worth-mention?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1416">
<title id=" W00-0903.xml">comparing corpora and lexical ambiguity </title>
<section> morphological analysers, lexicons and.  </section>
<citcontext>
<prevsection>
<prevsent>2.2 morphological analysis and medical.
</prevsent>
<prevsent>morphology the morphological analysis associates every surface form with list of morpho-syntactic features.
</prevsent>
</prevsection>
<citsent citstr=" E95-1021 ">
when the surface form is not found in the lexicon, it follows two.step guessing process: the first level (oraclel) is more complex morphological nalyzer, based on the morphosemantemes, while the second level guesser (orcale2) attempts to provides set of ms features looking at the longest ending (as described in chanod and tapanainen, 1995).<papid> E95-1021 </papid></citsent>
<aftsection>
<nextsent>the importance of these two levels is not clear for pos tagging, but becomes manifest when dealing with sense tagging.
</nextsent>
<nextsent>let us consider three examples of tokens absent fxom the lexicon: allomorphiques, allomorphiquement (equivalent to allomorphic and allomorphically in eng.
</nextsent>
<nextsent>remained ambiguous arer disambiguafion, the residual ambiguity is therefore about 5.5%.
</nextsent>
<nextsent>in this sample, and before disambiguation, the number of ambiguous tokens was 150, which means an ambiguity rate of 20%.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1418">
<title id=" W02-0206.xml">an experiment to evaluate the effectiveness of cross media cues in computer media </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>9.5, you can see that ), the interpretation of graphical elements in terms of the underlying domain and data, and salient visual features of the graphic.
</prevsent>
<prevsent>furthermore, we noted that commentary-bearing and argument-bearing text may be interleaved, and that the ratio of the number of sentences of commentary to their related cmc may be many to one.
</prevsent>
</prevsection>
<citsent citstr=" J98-3004 ">
previous work in caption generation is relevant to the question of what kinds of things to say about accompanying graphics (mittal et al., 1998; <papid> J98-3004 </papid>fasciano and lapalme, 1999).</citsent>
<aftsection>
<nextsent>however, neither of those systems face the problem of integrating commentary-bearing text with text generated to achieve other presentation goals.
</nextsent>
<nextsent>2.2 human-computer interaction.
</nextsent>
<nextsent>hci research has focused on interaction techniques and features of layout that influence effectiveness.
</nextsent>
<nextsent>use of contact points, control buttons in text on web page that enable readers to control related animations (faraday and sutcliffe, 1999), is an interaction technique that, like cmcs, explicitly marks the relationship between information presented in two media.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1419">
<title id=" W02-0311.xml">utilizing text mining results the pasta web system </title>
<section> ie and its application to biomedical.  </section>
<citcontext>
<prevsection>
<prevsent>the pasta web interface compensates for the loss of information by supporting rapid, easy verification by scientists of the extracted information against the source texts.
</prevsent>
<prevsent>texts perhaps not surprisingly, the identification of biomedical terms in scientific texts has proved to bethe easiest extraction task and has demonstrated acceptable levels of performance, not too far from the best results achieved in the ne task in the muccompetitions, despite differences between the domains (i.e. names of persons, organisations etc. in muc vs. terms identifying proteins, genes, drugs etc. in biomedical domains).
</prevsent>
</prevsection>
<citsent citstr=" C00-1030 ">
the techniques used for this task vary from rule-based methods (fukuda et al, 1998; humphreys et al, 2000), to statistical methods (collier et al, 2000) <papid> C00-1030 </papid>and statistical-rule based hybrids (proux et al, 1998).</citsent>
<aftsection>
<nextsent>more complex ie tasks involving the extraction of relational information have also been addressed by the bio informatics community.
</nextsent>
<nextsent>these include protein or gene interactions (sekimizu et al, 1998; thomas et al, 2000; pustejovsky et al, 2002), relations between genes and drugs (rindflesh et al., 2000) and identification of metabolic pathways (humphreys et al, 2000).
</nextsent>
<nextsent>the range of techniques used in these systems varies considerably, but inmost cases requires the application of more sophisticated nlp methods including part-of-speech tagging, phrasal or syntactic parsing and (for some sys tems) semantic analysis and discourse processing.
</nextsent>
<nextsent>to date ie researchers working on biological texts have concentrated on building or porting systems to work in biological domains.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1420">
<title id=" W02-0311.xml">utilizing text mining results the pasta web system </title>
<section> the pasta system.  </section>
<citcontext>
<prevsection>
<prevsent>2a residue may belong to more than one region extracted for protein and species objects are their names.
</prevsent>
<prevsent>3.2 system architecture.
</prevsent>
</prevsection>
<citsent citstr=" M98-1007 ">
the pasta system has been adapted from an ie system called lasie (large scale information extraction), originally developed for participation in the muc competitions (humphreys et al, 1998).<papid> M98-1007 </papid>the pasta system is pipeline of processing components that perform the following major tasks: text preprocessing, terminological processing, syntactic and semantic analysis, discourse interpretation, and template extraction.</citsent>
<aftsection>
<nextsent>text preprocessing the text preprocessing phase aims at low-level text processing tasks including the analysis of the structure of the medline abstract sin terms of separate sections (e.g. the title, author names, abstract etc.), token isation and sentence boundary identification.
</nextsent>
<nextsent>with respect to token isation, tokens are identified at the subword level resulting in the splitting of biochemical compound terms into their constituents which need to be matched separately during the lexical lookup phase.
</nextsent>
<nextsent>forex ample, the term cys128 is split to the three-letter residue abbreviation cys and the numeral 128.terminological processing the aim of the 3 stage terminological processing phase is to identify and correctly classify instances of the term classes described above in section 3.1.1.
</nextsent>
<nextsent>during the morphological analysis stage individual tokens are analysed to see if they contain interesting biochemical affixes such as -ase or -in that indicate candidate protein names respectively.during the lexical lookup stage the previously to kenised terms are matched against terminological lexicons which have been compiled from biological databases such as cath3 and scop4 and have been augmented with terms produced by corpus processing techniques (demetriou and gaizauskas, 2000).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1421">
<title id=" W02-0311.xml">utilizing text mining results the pasta web system </title>
<section> the pasta web interface.  </section>
<citcontext>
<prevsection>
<prevsent>the evaluation metrics are the well known measures of precision and recall.
</prevsent>
<prevsent>the pasta web interface5 is aimed at providing quick access and navigation facilities through the database of the pasta tagged texts and their associated templates.
</prevsent>
</prevsection>
<citsent citstr=" H01-1040 ">
pasta web has borrowed ideas from the interface component of the trestle6system gaizauskas et al (2001) <papid> H01-1040 </papid>developed to support information workers in the pharmaceutical in dustry.</citsent>
<aftsection>
<nextsent>key characteristics of pasta web are the seamless integration between the pasta ie results and www-based browsing technology, the dynamic generation of www pages from static?
</nextsent>
<nextsent>content and the fusion of information relating to proteins and amino acid residues when found in different sources.
</nextsent>
<nextsent>4.1 pasta web architecture.
</nextsent>
<nextsent>the pasta web architecture is illustrated in fig 2.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1422">
<title id=" W02-0107.xml">a niche at the nexus situating an nlp curriculum interdisciplinarily </title>
<section> an advanced nlp course.  </section>
<citcontext>
<prevsection>
<prevsent>students from the cs department have access to cs and linguistics servers where class-related resources can be used.
</prevsent>
<prevsent>students also have access to the campus supercomputer when necessary for nlp projects, under the instructors supervision.sample non-trivialhands-on and programming assignments are given weekly.
</prevsent>
</prevsection>
<citsent citstr=" H92-1022 ">
they include such topics as: work with various corpus manipulation and annotation tools, use of various pos taggers and their comparison (brill, 1992; <papid> H92-1022 </papid>tufis and mason, 1998), development of morpho phonological rules in pc-kimmo (antworth, 1990), understanding and manipulating content from wordnet databases (fell baum, 1998), aligning bitext, using and evaluatinga machine translation system, developing phrase structure grammar for syntactic and then semantic chart parsing, experimenting with information retrieval, working with speech toolkit to develop simple application, or developing knowledge for text generation engine (tomita and nyberg, 1988).</citsent>
<aftsection>
<nextsent>tutorials are provided for for any necessary remedial work that the student might need or desire in such topics as using the emacs editor, using unix shell scripts, or writing perl or tcl scripts.
</nextsent>
<nextsent>final project: final programming project is required, similar in scope to that described above for the humanities course: close coordination with the instructor, meeting milestones, documenting and demonstrating the final product, and producing write-up of the significance and contributions of the result.
</nextsent>
<nextsent>of course, much higher standard is required of these advanced students.
</nextsent>
<nextsent>the student is free to choose any relevant project, the programming lan guage(s) to be used, and the theoretical approach to be taken.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1423">
<title id=" W01-1013.xml">multilingual authoring the namic approach </title>
<section> the namic system.  </section>
<citcontext>
<prevsection>
<prevsent>each wordnet represents uniquelanguage-internal system of lexical isa tions.
</prevsent>
<prevsent>in addition, the wordnets are linked to an inter-lingual-index (ili), based on the princeton wordnet 1.5.
</prevsent>
</prevsection>
<citsent citstr=" P00-1064 ">
wordnet 1.6 is also connected to the ilias another english wordnet (daude et al, 2000).<papid> P00-1064 </papid></citsent>
<aftsection>
<nextsent>via this index, the languages are interconnected so that it is possible to go from the words in one language to words in any other language having similar meaning.
</nextsent>
<nextsent>the index also gives access to shared top-ontology and subset of 1024 base concepts (bc).
</nextsent>
<nextsent>thebase concepts provide common semantic framework for all the languages, while language specific properties are maintained in the individual wordnets.
</nextsent>
<nextsent>the lkb can be used, among others, for monolingual and cross-lingual information retrieval, which has been demonstrated in other projects (gonzalo et al, 1998).<papid> W98-0705 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1424">
<title id=" W01-1013.xml">multilingual authoring the namic approach </title>
<section> the namic system.  </section>
<citcontext>
<prevsection>
<prevsent>the index also gives access to shared top-ontology and subset of 1024 base concepts (bc).
</prevsent>
<prevsent>thebase concepts provide common semantic framework for all the languages, while language specific properties are maintained in the individual wordnets.
</prevsent>
</prevsection>
<citsent citstr=" W98-0705 ">
the lkb can be used, among others, for monolingual and cross-lingual information retrieval, which has been demonstrated in other projects (gonzalo et al, 1998).<papid> W98-0705 </papid></citsent>
<aftsection>
<nextsent>3.3 multilingual event description.
</nextsent>
<nextsent>the traditional limitations of knowledge based information extraction system such aslasie have been the need to hand-code information for the world model - specifically relating to the event structure of the domain.
</nextsent>
<nextsent>for the namic project, we have decided to semi-automate the process of adding new event descriptions?
</nextsent>
<nextsent>to the world model.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1425">
<title id=" W00-1213.xml">annotating information structures in chinese texts using hownet </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the second one, the sinica treebank, which is derived from the sinica corpus, contains 38,725 sentences with 1000 of them released to the public 1 (ckip, 2000).
</prevsent>
<prevsent>the historical development of chinese corpus construction has shown consensus in incorporating more powerful linguistic structures into corpora.
</prevsent>
</prevsection>
<citsent citstr=" W97-0101 ">
as noted by marcus (1997), <papid> W97-0101 </papid>the more powerful inguistic structures will help in improving the accuracy of parsing.</citsent>
<aftsection>
<nextsent>this is especially true to isolating language such as chinese.
</nextsent>
<nextsent>however, there is very little work on annotating corpora with semantic information.
</nextsent>
<nextsent>to the best of our knowledge, there is only one report of this kind.
</nextsent>
<nextsent>the work by lua 2 annotated 340,000 words with semantic lass information as defined in thesaurus of synonyms (mei, 1983).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1426">
<title id=" W00-1318.xml">automatic wordnet mapping using word sense disambiguation </title>
<section> multiple heuristics for word sense.  </section>
<citcontext>
<prevsection>
<prevsent>h2(s,) = max p(s, ew) ?,n, ew~ where ewi = {ew si ~ synset(ew) } 1 ( si ewj ) -~ - - j where si ~ synset(ewj), nj = isyr et( w,)l in this formula, is the number of synsets of the translation e~t~.
</prevsent>
<prevsent>2.3 heuristic 3: sense ordering.
</prevsent>
</prevsection>
<citsent citstr=" P92-1032 ">
(gale et al, 1992) <papid> P92-1032 </papid>reports that word sense disambiguation would be at least 75% correct if system assigns the most frequently occurring sense.</citsent>
<aftsection>
<nextsent>(miller et al, 1994) <papid> H94-1046 </papid>found that automatic we use english wordnet version 1.6 - 143 assignment of polysemous words in brown corpus to senses in wordnet was 58% correct with heuristic of most frequently occurring sense.</nextsent>
<nextsent>we adopt these previous results to develop sense ordering heuristic.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1427">
<title id=" W00-1318.xml">automatic wordnet mapping using word sense disambiguation </title>
<section> multiple heuristics for word sense.  </section>
<citcontext>
<prevsection>
<prevsent>2.3 heuristic 3: sense ordering.
</prevsent>
<prevsent>(gale et al, 1992) <papid> P92-1032 </papid>reports that word sense disambiguation would be at least 75% correct if system assigns the most frequently occurring sense.</prevsent>
</prevsection>
<citsent citstr=" H94-1046 ">
(miller et al, 1994) <papid> H94-1046 </papid>found that automatic we use english wordnet version 1.6 - 143 assignment of polysemous words in brown corpus to senses in wordnet was 58% correct with heuristic of most frequently occurring sense.</citsent>
<aftsection>
<nextsent>we adopt these previous results to develop sense ordering heuristic.
</nextsent>
<nextsent>the sense ordering heuristic provides the maximum score to the most frequently used sense of ixanslation.
</nextsent>
<nextsent>the following formula explains the heuristic.
</nextsent>
<nextsent>h3(so = max so(s,,ew) eweew, where ew, = {ew si ~ synset(ew) } ot so(s,,ew) x~ where si ~ synset(ew) ^ synset(ew) is sorted by frequency ^ s, is the x- th synset in synset(ew) in this formula, refers to the sense order of in synset(ew): is 1 when s, is the most frequently used sense of ew.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1428">
<title id=" W00-1318.xml">automatic wordnet mapping using word sense disambiguation </title>
<section> related works.  </section>
<citcontext>
<prevsection>
<prevsent>several attempts have been performed to automatically produce multilingual ontologies.
</prevsent>
<prevsent>(knight &amp; luk 1994) focuses on the construction of sensus, large knowledge base for supporting the pan gloss machine translation system, merging ontologies (ontos and uppermodel) and wordnet with monolingual and bilingual dictionaries.
</prevsent>
</prevsection>
<citsent citstr=" H94-1025 ">
(okumura &amp; hovy 1994) <papid> H94-1025 </papid>describes semi-automatic method for associating japanese lexicon to an ontology using japanese/english bilingual dictionary as  bridge .</citsent>
<aftsection>
<nextsent>several exical resources and techniques are combined in (atserias et al, 1997) to map spanish words from bilingual dictionary to wordnet.
</nextsent>
<nextsent>in (farreres et al., 1998), <papid> W98-0709 </papid>use of taxonomic structure derived from monolingual mrd is proposed as an aid to the mapping process.</nextsent>
<nextsent>this research is contrasted that it utilized bilingual dictionary to build monolingual thesaurus based on the existing popular lexical resources and used the combination of multiple unsupervided wsd heuristics.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1429">
<title id=" W00-1318.xml">automatic wordnet mapping using word sense disambiguation </title>
<section> related works.  </section>
<citcontext>
<prevsection>
<prevsent>(okumura &amp; hovy 1994) <papid> H94-1025 </papid>describes semi-automatic method for associating japanese lexicon to an ontology using japanese/english bilingual dictionary as  bridge .</prevsent>
<prevsent>several exical resources and techniques are combined in (atserias et al, 1997) to map spanish words from bilingual dictionary to wordnet.</prevsent>
</prevsection>
<citsent citstr=" W98-0709 ">
in (farreres et al., 1998), <papid> W98-0709 </papid>use of taxonomic structure derived from monolingual mrd is proposed as an aid to the mapping process.</citsent>
<aftsection>
<nextsent>this research is contrasted that it utilized bilingual dictionary to build monolingual thesaurus based on the existing popular lexical resources and used the combination of multiple unsupervided wsd heuristics.
</nextsent>
<nextsent>this paper has explored the automatic construction of korean wordnet from pre-existing lexical resources - english wordnet and korean/english bilingual dictionary.
</nextsent>
<nextsent>we presented several techniques for word sense disambiguation and their application to disambiguate the translations in bilingual dictionary.
</nextsent>
<nextsent>we obtained preliminary version of the korean wordnet containing 21654 senses of 17696 korean nouns.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1430">
<title id=" W02-0602.xml">unsupervised learning of morphology using a novel directed search algorithm taking the first step </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for instance, morphology may provide information about the syntactic categories to which words belong, knowledge which could be used by parsing algorithms.
</prevsent>
<prevsent>from cognitive perspective, itis crucial to determine whether the amount of information found in pure speech is sufficient for discovering the level of morphological structure that children are able to find without any direct supervision.thus, we believe the task of automatically discovering conservative estimate of the orthographicallybased morphological structure in language independent manner is useful one.additionally, an initial description of languages morphology could provide starting point for supervised morphological models, such as the memory-based algorithm ofvan den bosch and daelemans (1999), which can not be used on languages for which annotated data is unavailable.during the last decade several minimally supervised and unsupervised algorithms that address the problem have been developed.
</prevsent>
</prevsection>
<citsent citstr=" W99-0904 ">
gaussier (1999) <papid> W99-0904 </papid>describes an explicitly probabilistic system that is based primarily on spellings.</citsent>
<aftsection>
<nextsent>it is an unsupervised algorithm, but requires the tweaking of parameters to tune it to the target language.
</nextsent>
<nextsent>brent (1993) and brent et al (1995), described minimum description length, (mdl), systems.
</nextsent>
<nextsent>one approach used only the spellings of the words; another attempted to find the set of suffixes in the language used the syntactic categories from tagged corpus as well.
</nextsent>
<nextsent>while both are unsupervised, the latter is not knowledge free and requires data that is tagged for part of speech, making it less suitable for analyzing under examined languages.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1431">
<title id=" W02-0602.xml">unsupervised learning of morphology using a novel directed search algorithm taking the first step </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>one approach used only the spellings of the words; another attempted to find the set of suffixes in the language used the syntactic categories from tagged corpus as well.
</prevsent>
<prevsent>while both are unsupervised, the latter is not knowledge free and requires data that is tagged for part of speech, making it less suitable for analyzing under examined languages.
</prevsent>
</prevsection>
<citsent citstr=" J01-2001 ">
a similar mdl approach is described by goldsmith (2001).<papid> J01-2001 </papid></citsent>
<aftsection>
<nextsent>it is ideal in being both knowledge free and unsupervised.
</nextsent>
<nextsent>the difficulty lies in goldsmiths liberal definition of morphology whichhe uses to evaluate with; more conservative approach would seem to be better hypothesis to bootstrap from.we previously, snover and brent (2001), <papid> P01-1063 </papid>presented very conservative unsupervised system, july 2002, pp.</nextsent>
<nextsent>11-20.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1432">
<title id=" W02-0602.xml">unsupervised learning of morphology using a novel directed search algorithm taking the first step </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a similar mdl approach is described by goldsmith (2001).<papid> J01-2001 </papid></prevsent>
<prevsent>it is ideal in being both knowledge free and unsupervised.</prevsent>
</prevsection>
<citsent citstr=" P01-1063 ">
the difficulty lies in goldsmiths liberal definition of morphology whichhe uses to evaluate with; more conservative approach would seem to be better hypothesis to bootstrap from.we previously, snover and brent (2001), <papid> P01-1063 </papid>presented very conservative unsupervised system, july 2002, pp.</citsent>
<aftsection>
<nextsent>11-20.
</nextsent>
<nextsent>association for computational linguistics.
</nextsent>
<nextsent>acl special interest group in computational phonology (sigphon), philadelphia, morphological and phonological learning: proceedings of the 6th workshop of the which uses generative probability model and hill climbing search.
</nextsent>
<nextsent>no quantitative studies had been conducted on it, and it appears that the hill-climbing search used limits that systems usefulness.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1442">
<title id=" W02-0602.xml">unsupervised learning of morphology using a novel directed search algorithm taking the first step </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>is not semantically related to cape?
</prevsent>
<prevsent>or cap?.
</prevsent>
</prevsection>
<citsent citstr=" W00-0712 ">
using latent semantic analysis, schone and jurafsky (2000) <papid> W00-0712 </papid>have previously demonstrated the success of using semantic information in morphological analysis.</citsent>
<aftsection>
<nextsent>preliminary results on our datasets using similar technique, co-occurrence data, which represents each word as vector of frequencies of co-occurrence with other words, indicates that much semantic, as well as morphological, information can be extracted.
</nextsent>
<nextsent>when the cosine measure of distance is used in comparing pairs of words in the corpus, the highest scoring pairs are for the most part morphologically or semantically related.
</nextsent>
<nextsent>we are currently working on correctly incorporating this information into the probability model.
</nextsent>
<nextsent>the directed search algorithm does not currently handle multiple suffix ation or any prefixation; however, some ideas for future work involve extending the model to capture these processes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1443">
<title id=" W02-0602.xml">unsupervised learning of morphology using a novel directed search algorithm taking the first step </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>while such an extension would be significant one, itwould not change the fundamental nature of the algorithm.
</prevsent>
<prevsent>furthermore, the output of the present system is potentially useful in discovering spelling change rules, which could then be boot strapped to aid in discovering further morphological structure.
</prevsent>
</prevsection>
<citsent citstr=" P00-1027 ">
yarowsky and wicentowski (2000) <papid> P00-1027 </papid>have developed system that learns such rules given preliminary morphological hypothesis and part of speech tags.</citsent>
<aftsection>
<nextsent>while the experiments reported here are based on an input lexicon of orthographic representations, there is no reason why the directed search algorithm could not be applied to phonetic ally transcribed data.
</nextsent>
<nextsent>in fact, especially in the case of the english language, where the orthography is particularly inconsistent with the phonology, our algorithm mightbe expected to perform better at discovering the internal structure of phonologically transcribed words.
</nextsent>
<nextsent>furthermore, phonetic ally transcribed data would eliminate the problems introduced by the lack of one-to-one correspondence of letters to phonemes.
</nextsent>
<nextsent>namely, the algorithm would not mistakenly treatsibilants, such as the /ch/ sound in chat?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1444">
<title id=" W01-1007.xml">the form is the substance classification of genres in text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>they also improved results by including frequencies of eight punctuation marks.
</prevsent>
<prevsent>four identified genres from the wall street journal formed the corpus, but only 40 samples per genre were used.
</prevsent>
</prevsection>
<citsent citstr=" C94-2174 ">
both works dispensed with the more complex features proposed by karlgren and cutting (1994) <papid> C94-2174 </papid>which showed promising results.</citsent>
<aftsection>
<nextsent>they report 27% error in distinguishing four genres in 500 samples from the brown corpus.
</nextsent>
<nextsent>illouz et. al.
</nextsent>
<nextsent>(2000) report successful use of coarse level partofspeech features in distinguishing section types from le monde.
</nextsent>
<nextsent>their work also showed that fine grain partof?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1445">
<title id=" W01-1201.xml">looking under the hood tools for diagnosing your question answering engine </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>when building system to perform task, the most important statistic is the performance onan end-to-end evaluation.
</prevsent>
<prevsent>for the task of open domain question answering against text collections, there have been two large-scale end-to end evaluations: (trec-8 proceedings, 1999) and (trec-9 proceedings, 2000).
</prevsent>
</prevsection>
<citsent citstr=" W00-0601 ">
in addition, number of researchers have built systems to take reading comprehension examinations designed to evaluate childrens reading levels (charniak et al, 2000; <papid> W00-0601 </papid>hirschman et al, 1999; <papid> P99-1042 </papid>ng et al, 2000; <papid> W00-1316 </papid>riloff and thelen, 2000; <papid> W00-0603 </papid>wang et al, 2000).<papid> W00-1316 </papid></citsent>
<aftsection>
<nextsent>the performance statistics have been useful for determining how well techniques work.
</nextsent>
<nextsent>however, raw performance statistics are notenough.
</nextsent>
<nextsent>if the score is low, we need to under stand what went wrong and how to fix it.
</nextsent>
<nextsent>if the score is high, it is important to understand why.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1446">
<title id=" W01-1201.xml">looking under the hood tools for diagnosing your question answering engine </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>when building system to perform task, the most important statistic is the performance onan end-to-end evaluation.
</prevsent>
<prevsent>for the task of open domain question answering against text collections, there have been two large-scale end-to end evaluations: (trec-8 proceedings, 1999) and (trec-9 proceedings, 2000).
</prevsent>
</prevsection>
<citsent citstr=" P99-1042 ">
in addition, number of researchers have built systems to take reading comprehension examinations designed to evaluate childrens reading levels (charniak et al, 2000; <papid> W00-0601 </papid>hirschman et al, 1999; <papid> P99-1042 </papid>ng et al, 2000; <papid> W00-1316 </papid>riloff and thelen, 2000; <papid> W00-0603 </papid>wang et al, 2000).<papid> W00-1316 </papid></citsent>
<aftsection>
<nextsent>the performance statistics have been useful for determining how well techniques work.
</nextsent>
<nextsent>however, raw performance statistics are notenough.
</nextsent>
<nextsent>if the score is low, we need to under stand what went wrong and how to fix it.
</nextsent>
<nextsent>if the score is high, it is important to understand why.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1447">
<title id=" W01-1201.xml">looking under the hood tools for diagnosing your question answering engine </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>when building system to perform task, the most important statistic is the performance onan end-to-end evaluation.
</prevsent>
<prevsent>for the task of open domain question answering against text collections, there have been two large-scale end-to end evaluations: (trec-8 proceedings, 1999) and (trec-9 proceedings, 2000).
</prevsent>
</prevsection>
<citsent citstr=" W00-1316 ">
in addition, number of researchers have built systems to take reading comprehension examinations designed to evaluate childrens reading levels (charniak et al, 2000; <papid> W00-0601 </papid>hirschman et al, 1999; <papid> P99-1042 </papid>ng et al, 2000; <papid> W00-1316 </papid>riloff and thelen, 2000; <papid> W00-0603 </papid>wang et al, 2000).<papid> W00-1316 </papid></citsent>
<aftsection>
<nextsent>the performance statistics have been useful for determining how well techniques work.
</nextsent>
<nextsent>however, raw performance statistics are notenough.
</nextsent>
<nextsent>if the score is low, we need to under stand what went wrong and how to fix it.
</nextsent>
<nextsent>if the score is high, it is important to understand why.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1448">
<title id=" W01-1201.xml">looking under the hood tools for diagnosing your question answering engine </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>when building system to perform task, the most important statistic is the performance onan end-to-end evaluation.
</prevsent>
<prevsent>for the task of open domain question answering against text collections, there have been two large-scale end-to end evaluations: (trec-8 proceedings, 1999) and (trec-9 proceedings, 2000).
</prevsent>
</prevsection>
<citsent citstr=" W00-0603 ">
in addition, number of researchers have built systems to take reading comprehension examinations designed to evaluate childrens reading levels (charniak et al, 2000; <papid> W00-0601 </papid>hirschman et al, 1999; <papid> P99-1042 </papid>ng et al, 2000; <papid> W00-1316 </papid>riloff and thelen, 2000; <papid> W00-0603 </papid>wang et al, 2000).<papid> W00-1316 </papid></citsent>
<aftsection>
<nextsent>the performance statistics have been useful for determining how well techniques work.
</nextsent>
<nextsent>however, raw performance statistics are notenough.
</nextsent>
<nextsent>if the score is low, we need to under stand what went wrong and how to fix it.
</nextsent>
<nextsent>if the score is high, it is important to understand why.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1452">
<title id=" W01-1609.xml">automated tutoring dialogues for training in shipboard damage control </title>
<section> implementation.  </section>
<citcontext>
<prevsection>
<prevsent>which manages message passing between number of software agents that specialize in certain tasks (e.g., speech recognition or database queries).
</prevsent>
<prevsent>our system uses oaa to coordinate the following five agents: 1.
</prevsent>
</prevsection>
<citsent citstr=" H93-1008 ">
the gemini nlp system (dowding et. al., 1993).<papid> H93-1008 </papid></citsent>
<aftsection>
<nextsent>gemini uses single unification grammar both for parsing strings of words into logical forms (lfs) and for generating sentences from lf inputs.
</nextsent>
<nextsent>2.
</nextsent>
<nextsent>a nuance speech recognition server, which converts spoken utterances to strings of words.
</nextsent>
<nextsent>the nuance server relies on language model, which is compiled directly from the gemini grammar, ensuring that every recognized utterance is assigned an lf.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1453">
<title id=" W01-1609.xml">automated tutoring dialogues for training in shipboard damage control </title>
<section> a critique planner, described below </section>
<citcontext>
<prevsection>
<prevsent>in section 4.agents 1-3 are reusable, off-the-shelf?
</prevsent>
<prevsent>dialogue system components (apart from the gemini grammar, which must be modified for each application).
</prevsent>
</prevsection>
<citsent citstr=" P99-1024 ">
we implemented agents 4 and 5 in java specifically for this application.variants of this oaa/gemini/nuance architecture have been deployed successfully inother dialogue systems, notably sris command talk (stent et al, 1999) <papid> P99-1024 </papid>and an un figure 2: screen shot of post-session tutorial dialogue system manned helicopter interface developed in our laboratory (lemon et al, 2001).</citsent>
<aftsection>
<nextsent>4 planning the dialogue.
</nextsent>
<nextsent>each student session with dc-train produces session transcript, i.e. time-stamped record of every event (both computer- and student-initiated) that occurred during the simulation.
</nextsent>
<nextsent>these transcripts serve as the input to our post-session critique planner (cp).the cp plans post-session tutorial dialogue in two steps.
</nextsent>
<nextsent>in the first step, an expert session summary (ess) is created from the session transcript.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1454">
<title id=" W02-0703.xml">spoken language parsing using phrase level grammars and trainable classifiers </title>
<section> mt system overview.  </section>
<citcontext>
<prevsection>
<prevsent>in addition to the analyzer described here, the english translation server uses the janus recognition toolkit for speech recognition, the genkit system (tomita &amp; nyberg, 1988) for generation, and the festival system (black et al, 1999) for synthesis.
</prevsent>
<prevsent>nespole!
</prevsent>
</prevsection>
<citsent citstr=" H01-1007 ">
uses client-server architecture (lavie et al, 2001) <papid> H01-1007 </papid>to enable users who are browsing the web pages of service provider (e.g. tourism bureau) to seamlessly connect to human agent who speaks different language.</citsent>
<aftsection>
<nextsent>using commercially available software such as microsoft netmeeting?, user is connected to the nespole!
</nextsent>
<nextsent>mediator, which establishes connections with the agent and with translation servers for the appropriate languages.
</nextsent>
<nextsent>during dialogue, the mediator transmits spoken input from the users to the translation servers and synthesized translations from the servers to the users.
</nextsent>
<nextsent>the interlingua used in the nespole!
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1455">
<title id=" W02-0703.xml">spoken language parsing using phrase level grammars and trainable classifiers </title>
<section> the interlingua.  </section>
<citcontext>
<prevsection>
<prevsent>during dialogue, the mediator transmits spoken input from the users to the translation servers and synthesized translations from the servers to the users.
</prevsent>
<prevsent>the interlingua used in the nespole!
</prevsent>
</prevsection>
<citsent citstr=" W00-0203 ">
system is called interchange format (if) (levin et al, 1998; levin et al, 2000).<papid> W00-0203 </papid></citsent>
<aftsection>
<nextsent>the if defines shallow semantic representation for task-oriented utterances that abstracts away from language specific syntax and idiosyncrasies while capturing the meaning of the input.
</nextsent>
<nextsent>each utterance is divided into semantic segments called semantic dialog units (sdus), and an if is assigned to each sdu.
</nextsent>
<nextsent>an if representation consists of four parts: speaker tag, speech act, an optional sequence of concepts, and an optional set of arguments.
</nextsent>
<nextsent>the representation takes the following form: speaker : speech act +concept* (argument*) the speaker tag indicates the role of the speaker in the dialogue.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1459">
<title id=" W02-0703.xml">spoken language parsing using phrase level grammars and trainable classifiers </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>lavie et al (1997) developed method for identifying sdu boundaries in speech-to-speech translation system.
</prevsent>
<prevsent>identifying sdu boundaries is also similar to sentence boundary detection.
</prevsent>
</prevsection>
<citsent citstr=" A00-1012 ">
stevenson and gaizauskas (2000) <papid> A00-1012 </papid>use timbl (daelemans et al, 2000) to identify sentence boundaries in speech recognizer output, and gotoh and renals (2000) use statistical approach to identify sentence boundaries in automatic speech recognition transcripts of broadcast speech.</citsent>
<aftsection>
<nextsent>munk (1999) attempted to combine grammars and machine learning for da classification.
</nextsent>
<nextsent>in munks salt system, two-layer hmm was used to segment and label arguments and speech acts.
</nextsent>
<nextsent>a neural network identified the concept sequences.
</nextsent>
<nextsent>finally, semantic grammars were used to parse each argument segment.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1460">
<title id=" W00-1314.xml">word alignment of english chinese bilingual corpus based on chucks </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>following this is an analysis of our experimental results.
</prevsent>
<prevsent>finally, we close our paper with discussion of future work.
</prevsent>
</prevsection>
<citsent citstr=" P91-1023 ">
there are basically two kinds of approaches on word alignment: he statistical-based approaches (brown et. al., 1990; gale &amp; church, 1991; <papid> P91-1023 </papid>dagan et. al. 1993; chang, 1994), and the lexicon-based approaches (ker &amp; chang, 1997; <papid> J97-2004 </papid>wang et. al., 1999).</citsent>
<aftsection>
<nextsent>several translation models based on word alignment are built by brown et al (1990) in order to implement the english-french statistical machine translation.
</nextsent>
<nextsent>the probabilities, such as translation probability, fertility probability, distortion probability, are estimated by em algorithm.
</nextsent>
<nextsent>the 2 measure is used by gale &amp; church (1991) <papid> P91-1023 </papid>to align partial words.</nextsent>
<nextsent>dagan (1993) uses an improved brown model to align the words for texts including ocr noise.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1461">
<title id=" W00-1314.xml">word alignment of english chinese bilingual corpus based on chucks </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>following this is an analysis of our experimental results.
</prevsent>
<prevsent>finally, we close our paper with discussion of future work.
</prevsent>
</prevsection>
<citsent citstr=" J97-2004 ">
there are basically two kinds of approaches on word alignment: he statistical-based approaches (brown et. al., 1990; gale &amp; church, 1991; <papid> P91-1023 </papid>dagan et. al. 1993; chang, 1994), and the lexicon-based approaches (ker &amp; chang, 1997; <papid> J97-2004 </papid>wang et. al., 1999).</citsent>
<aftsection>
<nextsent>several translation models based on word alignment are built by brown et al (1990) in order to implement the english-french statistical machine translation.
</nextsent>
<nextsent>the probabilities, such as translation probability, fertility probability, distortion probability, are estimated by em algorithm.
</nextsent>
<nextsent>the 2 measure is used by gale &amp; church (1991) <papid> P91-1023 </papid>to align partial words.</nextsent>
<nextsent>dagan (1993) uses an improved brown model to align the words for texts including ocr noise.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1464">
<title id=" W02-0607.xml">modeling english past tense intuitions with minimal generalization </title>
<section> description of the model.  </section>
<citcontext>
<prevsection>
<prevsent>intuitively, reliability is what makes rule trustable.
</prevsent>
<prevsent>however, reliability based on high scope (for example, 990 correct predictions out of 1000) is better than reliability based on low scope (for example, 5 out of 5).
</prevsent>
</prevsection>
<citsent citstr=" J97-3003 ">
following mikheev (1997),<papid> J97-3003 </papid>we therefore adjust reliability using lower confi 3 we believe, but have not proven, that no additional rules are.</citsent>
<aftsection>
<nextsent>discovered by comparing generalized rules against generalized rules.dence limit statistics.4 the amount of the adjustment is parameter (?), which ranges from .5   ?   1; the higher the value of ?, the more drastic the adjustment.
</nextsent>
<nextsent>the result of this adjustment value,which ranges from 0 to 1, we call confidence.
</nextsent>
<nextsent>confidence values are calculated for each generalized rule, as soon as it is discovered.
</nextsent>
<nextsent>as each new input pair is processed, it is compared against previously discovered generalized rules to see whether it adds to their hits or scope.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1465">
<title id=" W02-0607.xml">modeling english past tense intuitions with minimal generalization </title>
<section> testing the model.  </section>
<citcontext>
<prevsection>
<prevsent>a number of morphological processes involve multiple changes, as in the german past participle geschleppt dragged?, derived from schlepp- using both pre fixation and suffixation.
</prevsent>
<prevsent>our model (spe cifically, our method for detecting affixes) cannot characterize such cases as involving two simple changes, and would treat the relation as arbitrary.
</prevsent>
</prevsection>
<citsent citstr=" J01-2001 ">
two methods that might help here would be (a) to use some form of string-edit distance (kruskal1983), weighted by phonetic similarity, to determine that -schlepp- is the string shared by the two forms; (b) to adopt some method of morpheme discovery (e.g. baroni 2000; goldsmith 2001; <papid> J01-2001 </papid>neuvel, to appear; schone and jurafsky 2001; <papid> N01-1024 </papid>baroni et al 2002) <papid> W02-0606 </papid>and use its results to favor rules that prefix ge- and suffix -t.</citsent>
<aftsection>
<nextsent>summarizing, we anticipate that improvements in the model could result from better phonological representations, better methods of search, and more sophisticated forms of string matching.
</nextsent>
<nextsent>appendix: made-up verbs used in the experiments i. expected to be especially good as regular blafe [blef], bredge [brdz], chool [tsul], dape [dep], gezz [gz], nace [nes], spack [spk], stire [star], tesh [ts], wiss [ws] ii.
</nextsent>
<nextsent>expected to be especially good as irregular blig [blg], chake [tsek], drit [drt], fleep [flip], gleed [glid], glit [glt], plim [plm], queed [kwid], scride [skrad], spling [spl??], teep [tip] iii.
</nextsent>
<nextsent>expected to be good both as regular and as irregular bize [baz], dize [daz], drice [dras], flidge [fldz], fro [fro], gare [ger], glip [glp], rife [raf], stin [stn], stip [stp] iv.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1466">
<title id=" W02-0607.xml">modeling english past tense intuitions with minimal generalization </title>
<section> testing the model.  </section>
<citcontext>
<prevsection>
<prevsent>a number of morphological processes involve multiple changes, as in the german past participle geschleppt dragged?, derived from schlepp- using both pre fixation and suffixation.
</prevsent>
<prevsent>our model (spe cifically, our method for detecting affixes) cannot characterize such cases as involving two simple changes, and would treat the relation as arbitrary.
</prevsent>
</prevsection>
<citsent citstr=" N01-1024 ">
two methods that might help here would be (a) to use some form of string-edit distance (kruskal1983), weighted by phonetic similarity, to determine that -schlepp- is the string shared by the two forms; (b) to adopt some method of morpheme discovery (e.g. baroni 2000; goldsmith 2001; <papid> J01-2001 </papid>neuvel, to appear; schone and jurafsky 2001; <papid> N01-1024 </papid>baroni et al 2002) <papid> W02-0606 </papid>and use its results to favor rules that prefix ge- and suffix -t.</citsent>
<aftsection>
<nextsent>summarizing, we anticipate that improvements in the model could result from better phonological representations, better methods of search, and more sophisticated forms of string matching.
</nextsent>
<nextsent>appendix: made-up verbs used in the experiments i. expected to be especially good as regular blafe [blef], bredge [brdz], chool [tsul], dape [dep], gezz [gz], nace [nes], spack [spk], stire [star], tesh [ts], wiss [ws] ii.
</nextsent>
<nextsent>expected to be especially good as irregular blig [blg], chake [tsek], drit [drt], fleep [flip], gleed [glid], glit [glt], plim [plm], queed [kwid], scride [skrad], spling [spl??], teep [tip] iii.
</nextsent>
<nextsent>expected to be good both as regular and as irregular bize [baz], dize [daz], drice [dras], flidge [fldz], fro [fro], gare [ger], glip [glp], rife [raf], stin [stn], stip [stp] iv.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1467">
<title id=" W02-0607.xml">modeling english past tense intuitions with minimal generalization </title>
<section> testing the model.  </section>
<citcontext>
<prevsection>
<prevsent>a number of morphological processes involve multiple changes, as in the german past participle geschleppt dragged?, derived from schlepp- using both pre fixation and suffixation.
</prevsent>
<prevsent>our model (spe cifically, our method for detecting affixes) cannot characterize such cases as involving two simple changes, and would treat the relation as arbitrary.
</prevsent>
</prevsection>
<citsent citstr=" W02-0606 ">
two methods that might help here would be (a) to use some form of string-edit distance (kruskal1983), weighted by phonetic similarity, to determine that -schlepp- is the string shared by the two forms; (b) to adopt some method of morpheme discovery (e.g. baroni 2000; goldsmith 2001; <papid> J01-2001 </papid>neuvel, to appear; schone and jurafsky 2001; <papid> N01-1024 </papid>baroni et al 2002) <papid> W02-0606 </papid>and use its results to favor rules that prefix ge- and suffix -t.</citsent>
<aftsection>
<nextsent>summarizing, we anticipate that improvements in the model could result from better phonological representations, better methods of search, and more sophisticated forms of string matching.
</nextsent>
<nextsent>appendix: made-up verbs used in the experiments i. expected to be especially good as regular blafe [blef], bredge [brdz], chool [tsul], dape [dep], gezz [gz], nace [nes], spack [spk], stire [star], tesh [ts], wiss [ws] ii.
</nextsent>
<nextsent>expected to be especially good as irregular blig [blg], chake [tsek], drit [drt], fleep [flip], gleed [glid], glit [glt], plim [plm], queed [kwid], scride [skrad], spling [spl??], teep [tip] iii.
</nextsent>
<nextsent>expected to be good both as regular and as irregular bize [baz], dize [daz], drice [dras], flidge [fldz], fro [fro], gare [ger], glip [glp], rife [raf], stin [stn], stip [stp] iv.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1468">
<title id=" W01-1014.xml">automatic augmentation of translation dictionary with database terminologies in multilingual query interpretation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(not new) and nulk-ta?
</prevsent>
<prevsent>(not young) because it appears in the same table entries for sa-ta?.
</prevsent>
</prevsection>
<citsent citstr=" C00-2086 ">
since it is difficult to disambiguate the senses only with database information, we may utilize co-occurrence information between the collocated words such as (old,shoes) and (old,car) (park and cho, 2000;<papid> C00-2086 </papid>lee et al, 1999).</citsent>
<aftsection>
<nextsent>in this paper, we propose disambiguation method with the database information and cooccurrence information (park and cho, 2000;<papid> C00-2086 </papid>palmer et al, 1999) for the interpretation of natural language queries (lee and park, 2001) in multilingual query interpretation.</nextsent>
<nextsent>although we propose to construct the system without an intermediate representation language, we show that our combinatory categorial grammar (ccg) framework is compatible with the approaches with an intermediate representation (nelken and francez, 2000; <papid> C00-2161 </papid>androutsopoulos et al, 1998;klein et al, 1998).<papid> W98-0606 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1471">
<title id=" W01-1014.xml">automatic augmentation of translation dictionary with database terminologies in multilingual query interpretation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>since it is difficult to disambiguate the senses only with database information, we may utilize co-occurrence information between the collocated words such as (old,shoes) and (old,car) (park and cho, 2000;<papid> C00-2086 </papid>lee et al, 1999).</prevsent>
<prevsent>in this paper, we propose disambiguation method with the database information and cooccurrence information (park and cho, 2000;<papid> C00-2086 </papid>palmer et al, 1999) for the interpretation of natural language queries (lee and park, 2001) in multilingual query interpretation.</prevsent>
</prevsection>
<citsent citstr=" C00-2161 ">
although we propose to construct the system without an intermediate representation language, we show that our combinatory categorial grammar (ccg) framework is compatible with the approaches with an intermediate representation (nelken and francez, 2000; <papid> C00-2161 </papid>androutsopoulos et al, 1998;klein et al, 1998).<papid> W98-0606 </papid></citsent>
<aftsection>
<nextsent>we also discuss the advantages and disadvantages of these two approaches.the rest of the paper is organized as follows.
</nextsent>
<nextsent>a brief introduction to ccgs and natural language database interfaces (nldbs) will be shown in section 2.
</nextsent>
<nextsent>we show the translation process with and without an intermediate representation using ccg in section 3.
</nextsent>
<nextsent>the proposed system with multilingual translation is described in sections 4 and 5.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1473">
<title id=" W01-1014.xml">automatic augmentation of translation dictionary with database terminologies in multilingual query interpretation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>since it is difficult to disambiguate the senses only with database information, we may utilize co-occurrence information between the collocated words such as (old,shoes) and (old,car) (park and cho, 2000;<papid> C00-2086 </papid>lee et al, 1999).</prevsent>
<prevsent>in this paper, we propose disambiguation method with the database information and cooccurrence information (park and cho, 2000;<papid> C00-2086 </papid>palmer et al, 1999) for the interpretation of natural language queries (lee and park, 2001) in multilingual query interpretation.</prevsent>
</prevsection>
<citsent citstr=" W98-0606 ">
although we propose to construct the system without an intermediate representation language, we show that our combinatory categorial grammar (ccg) framework is compatible with the approaches with an intermediate representation (nelken and francez, 2000; <papid> C00-2161 </papid>androutsopoulos et al, 1998;klein et al, 1998).<papid> W98-0606 </papid></citsent>
<aftsection>
<nextsent>we also discuss the advantages and disadvantages of these two approaches.the rest of the paper is organized as follows.
</nextsent>
<nextsent>a brief introduction to ccgs and natural language database interfaces (nldbs) will be shown in section 2.
</nextsent>
<nextsent>we show the translation process with and without an intermediate representation using ccg in section 3.
</nextsent>
<nextsent>the proposed system with multilingual translation is described in sections 4 and 5.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1502">
<title id=" W00-1312.xml">cross lingual information retrieval using hidden markov models </title>
<section> impact of  lexicon size.  </section>
<citcontext>
<prevsection>
<prevsent>translation of numbers can be solved using simple rules.
</prevsent>
<prevsent>transliteration, technique that guesses the likely translations of word based on pronunciation, can be readily used in translating proper nouns.
</prevsent>
</prevsection>
<citsent citstr=" W97-0119 ">
another technique is automatic discovery of translations from parallel or non-parallel corpora (fung and mckeown, 1997).<papid> W97-0119 </papid></citsent>
<aftsection>
<nextsent>since traditional lexicons are more or less static repositories of knowledge, techniques that discover translation from newly published materials can supplement them with corpus-specific vocabularies.
</nextsent>
<nextsent>100
</nextsent>
<nextsent>in this section we estimate translation probabilities from parallel corpus rather than assuming uniform likelihood as in section 4.
</nextsent>
<nextsent>a hongkong news corpus obtained from the linguistic data consortium has 9,769 news stories in chinese with english translations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1503">
<title id=" W02-0217.xml">probabilistic dialogue modelling </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we have experience building deterministic dialogue managers (see e.g.
</prevsent>
<prevsent>(lemon et al, 1this research was (partially) funded under the wallenberg laboratory for research on information technology and autonomous systems (witas) project, linkoping university, by the wallenberg foundation, sweden.
</prevsent>
</prevsection>
<citsent citstr=" W02-0216 ">
2001; lemon et al, 2002)) <papid> W02-0216 </papid>which use deterministic context update rules.</citsent>
<aftsection>
<nextsent>this paper briefly describes our construction of abayes net modelling dialogue context.
</nextsent>
<nextsent>we will consider series of examples of increasing complexity involving anaphoric resolution in section 3.1.
</nextsent>
<nextsent>we will point out how they are to be resolved intuitively, and then discuss how our bayesian net fares.
</nextsent>
<nextsent>wewill see that many of the best insights of deterministic approaches (e.g. in the axiomatic bdi tradition and in the planning literature) can be preserved, of ten in less brittle forms, in probabilistic setting.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1512">
<title id=" W02-0507.xml">qarab a question answering system to support the arabic language </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>in addition to the above linguistic issues, there is also lack of arabic corpora, lexicons, and machine-readable dictionaries, which are essential to advance research in different areas.
</prevsent>
<prevsent>advances in natural language processing (nlp), information retrieval techniques (ir), information extraction (ie), as well as the computer industry, have given qa strong boost.
</prevsent>
</prevsection>
<citsent citstr=" C00-1043 ">
modern question answering systems have started incorporating nlp techniques to parse natural language documents, extract entities and relations between entities, resolve anaphora, and other language ambiguities [harabagiu et al, 2000; <papid> C00-1043 </papid>vicedo &amp; ferrndez, 2000].</citsent>
<aftsection>
<nextsent>research in question-answering (qa) is not new.
</nextsent>
<nextsent>the qa problem has been addressed in the literature since the beginning of computing machines.
</nextsent>
<nextsent>the ai/nlp communities initiated traditional work to address question-answering using structural methods.
</nextsent>
<nextsent>early experiments in this direction implemented systems that operate in very restricted domains (e.g. shrdlu [winogard, 1972] and lunar [woods, 1972]).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1513">
<title id=" W02-0507.xml">qarab a question answering system to support the arabic language </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>even though the bag-of-words?
</prevsent>
<prevsent>approach was commonly used in trec-8, the systems based on this approach were inadequate to handle the short string (50-byte) answers.
</prevsent>
</prevsection>
<citsent citstr=" P00-1071 ">
on the contrary, the short string (50-byte) participants (e.g. [breck et al, 1999]; [ferret et al., 1999]; [hull, 1999]; [humphreys et al, 1999]; [litkowski, 1999]; [moldovan et al, 2000]; [<papid> P00-1071 </papid>oard et al, 1999]; [singhal et al, 1999]) agreed on the importance of applying several natural language processing techniques to solve the problem.</citsent>
<aftsection>
<nextsent>among these techniques are: part-of-speech tagging, shallow parsing, query type identification and named entity recognition.
</nextsent>
<nextsent>because the number of test documents to be analyzed for each query was huge, the majority of the systems in this band used the bag-of-words?
</nextsent>
<nextsent>approach as an initial step to retrieve the relevant passages that contain the possible answer.
</nextsent>
<nextsent>another approach to the qa problem combines ir techniques with information extraction (ie) techniques for extracting named entities, e.g., [ogden et al, 1999]; [takaki, 1999]; and [srihari &amp; li, 1999].
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1514">
<title id=" W02-0507.xml">qarab a question answering system to support the arabic language </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>a detailed description of the track and the results are available at [voorhees &amp; tice, 1999].
</prevsent>
<prevsent>it is obvious from the increasing number of systems participating in trec-9 and the worldwide interest in this research area that question answering is the most promising framework for finding answers to natural language questions from huge amount of textual data.
</prevsent>
</prevsection>
<citsent citstr=" A00-1025 ">
cardie et al [2000] <papid> A00-1025 </papid>pointed out that building open-ended question answering systems that allow users to pose questions of any type and in any language, without domain restrictions, is still beyond the scope of any qa system today?</citsent>
<aftsection>
<nextsent>(p. 180).
</nextsent>
<nextsent>harabagiu et al [2000] <papid> C00-1043 </papid>indicated that advanced tools (such as dialog understanding and text mining) are essential for the success of future qa systems.</nextsent>
<nextsent>until the advanced tools are implemented, she suggested that we keep approximating the complexity of question answering with nlp enhancements of ir and ie techniques [harabagiu et al, 2000].<papid> C00-1043 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1517">
<title id=" W00-0205.xml">telicity as a cueto temporal and discourse structure in chinese english machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>syntactic divergences (whether the object precedes or follows the verb, for example) are represented in language specific lin-earization rules; lexical divergences (whether the lo-cation argument is encoded irectly in the verb, e.g. the english verb pocket or must be saturated by an exterfial argument) are stated in terms of the pieces of lcs struct-ure in the lexicon.
</prevsent>
<prevsent>sententim repre-sentations derive from saturating the arguments re-quired by the predicates in the sentence.
</prevsent>
</prevsection>
<citsent citstr=" P97-1020 ">
lcs representations also include temporal infor-mation, where available in the source language: re-cent revisions include, for example (dorr and olsen, 1997<papid> P97-1020 </papid>a) standardizing lcs representations for the as- pectual (un)boundedness ((a)telicity) of events, either lexically or sententially represented.</citsent>
<aftsection>
<nextsent>although at present he lcs encodes no supra-sentential dis-course relations, we show how the lexical aspect in-formation may be used to generate discourse co-herence in temporal structure.
</nextsent>
<nextsent>relations between clauses as constrained by temporal reference has been examined in an lcs framework by dorr and gaasterland (dorr and gaasterland, 1995).
</nextsent>
<nextsent>they explore how temporal connectives are constrained in interpretation, based on the tense of the clauses they connect.
</nextsent>
<nextsent>while overt temporal connectives are helpful when they appear, our corpus contains many sentences with neither tense markers nor tense con-nectives.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1519">
<title id=" W00-0205.xml">telicity as a cueto temporal and discourse structure in chinese english machine translation </title>
<section> use of aspect to provide.  </section>
<citcontext>
<prevsection>
<prevsent>on this analysis, the \[.-i-du- rative, +dynamic\] features of run propagate to the sentence vel in run ~o the store; the \[telic\] feature is added by the np or pp, yielding an accomplish-ment interpretation.
</prevsent>
<prevsent>the feature specification of this ompositionally derived accomplishment is here fore identical to that of sentence containing telic ac-complishment verb, such as destroy.
</prevsent>
</prevsection>
<citsent citstr=" J88-2003 ">
according to many researchers, knowledge of lex-ical aspect--how verbs denote situations as devel-oping or holding in time-=may be used to interpret event sequences in discourse (dowty, 1986; moens and steedman, 1988; <papid> J88-2003 </papid>passoneau, 1988).</citsent>
<aftsection>
<nextsent>in particu-lar, dowty suggests that, absent other cues, relic event is interpreted as completed before the next event or state, as with ran into lhe room in 4a; in contrast, atelic situations, such as run, was hungry in 4b and 4% are interpreted as contemporaneous with the following situations: fell and made pizza, respectively.
</nextsent>
<nextsent>(4) a. mary ran into the room.
</nextsent>
<nextsent>she turned on her walkman.
</nextsent>
<nextsent>b. mary ran.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1522">
<title id=" W00-0205.xml">telicity as a cueto temporal and discourse structure in chinese english machine translation </title>
<section> implementation.  </section>
<citcontext>
<prevsection>
<prevsent>real ization the amr structure is then linear ized, as described in (dorr et al, 1998), and mor-phological realization is performed.
</prevsent>
<prevsent>the result is lattice of possible realizations, represent-ing both the preserved ambiguity from previous processing phases and multiple ways of line ariz ing the sentence.
</prevsent>
</prevsection>
<citsent citstr=" W98-1426 ">
extraction the final stage uses statistical bi-gram extractor to pick an approximation of the most fluent realization (langkilde and knight, 1998<papid> W98-1426 </papid>b).</citsent>
<aftsection>
<nextsent>while there are several possible ways to address the tense and discourse connective issues mentioned above, such as modifying the lcs primitive lements and/or the composition of the lcs from the source language, we instead have been experimenting for the moment with solutions implemented within the generation component.
</nextsent>
<nextsent>the only extensions to the lcs language have been loosening of the constraint against direct modification of states and events by other states and events (thus allowing composed lc- ses to be formed from chinese with these structures, but creating challenge for fluent generation into english), and few added features to cover some of the discourse markers that are present.
</nextsent>
<nextsent>we are able to calculate telicity of clcs, using the algorithm in figure 1 and encode this information as binary te i?
</nextsent>
<nextsent>feature in the augmented meaning represen-tation (amr).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1524">
<title id=" W01-0714.xml">distributional phrase structure induction </title>
<section> overview </section>
<citcontext>
<prevsection>
<prevsent>the advantages and disadvantages of these systems are examined, including precision/recall trade-offs, error analysis, and extensibility.
</prevsent>
<prevsent>while early work showed that small, artificial context-free grammars could be induced with the em algorithm (lari and young, 1990) or with chunk-merge systems (stolcke and omohundro,1994), studies with large natural language grammars have shown that these methods of completely unsupervised acquisition are generally ineffective.for instance, charniak (1993) describes experiments running the em algorithm from random starting points, which produced widely varying grammars of extremely poor quality.
</prevsent>
</prevsection>
<citsent citstr=" P97-1003 ">
because of these kinds of results, the vast majority of statistical parsing work has focused on parsing as supervised learning problem (collins, 1997; <papid> P97-1003 </papid>charniak, 2000).<papid> A00-2018 </papid>it remains an open problem whether an entirely unsupervised method can either produce linguistically sensible grammars or accurately parse free text.</citsent>
<aftsection>
<nextsent>however, there are compelling motivations for unsupervised grammar induction.
</nextsent>
<nextsent>building supervised training data requires considerable resources,including time and linguistic expertise.
</nextsent>
<nextsent>further more, investigating unsupervised methods can shed light on linguistic phenomena which are implicitly captured within supervised parsers supervisory information, and, therefore, often not explicitly modeled in such systems.
</nextsent>
<nextsent>for example, our system and others have difficulty correctly attaching subjects to verbs above objects.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1525">
<title id=" W01-0714.xml">distributional phrase structure induction </title>
<section> overview </section>
<citcontext>
<prevsection>
<prevsent>the advantages and disadvantages of these systems are examined, including precision/recall trade-offs, error analysis, and extensibility.
</prevsent>
<prevsent>while early work showed that small, artificial context-free grammars could be induced with the em algorithm (lari and young, 1990) or with chunk-merge systems (stolcke and omohundro,1994), studies with large natural language grammars have shown that these methods of completely unsupervised acquisition are generally ineffective.for instance, charniak (1993) describes experiments running the em algorithm from random starting points, which produced widely varying grammars of extremely poor quality.
</prevsent>
</prevsection>
<citsent citstr=" A00-2018 ">
because of these kinds of results, the vast majority of statistical parsing work has focused on parsing as supervised learning problem (collins, 1997; <papid> P97-1003 </papid>charniak, 2000).<papid> A00-2018 </papid>it remains an open problem whether an entirely unsupervised method can either produce linguistically sensible grammars or accurately parse free text.</citsent>
<aftsection>
<nextsent>however, there are compelling motivations for unsupervised grammar induction.
</nextsent>
<nextsent>building supervised training data requires considerable resources,including time and linguistic expertise.
</nextsent>
<nextsent>further more, investigating unsupervised methods can shed light on linguistic phenomena which are implicitly captured within supervised parsers supervisory information, and, therefore, often not explicitly modeled in such systems.
</nextsent>
<nextsent>for example, our system and others have difficulty correctly attaching subjects to verbs above objects.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1526">
<title id=" W01-0714.xml">distributional phrase structure induction </title>
<section> systems.  </section>
<citcontext>
<prevsection>
<prevsent>to do there estimation, we must have some method of deciding which binary bracketing to prefer.
</prevsent>
<prevsent>the chance of pair   *$ being constituent is 9 s5-  *$fiff 9 sq-  9 s5-  !
</prevsent>
</prevsection>
<citsent citstr=" J97-4005 ">
9 sa and we score tree by the likelihood product of its judgements sq fi*ybc . the best tree is then djegffihd ikj ml cneoh pg  9 sq fi*ybca- fi*  as we are considering each pair independently from the rest of the parse, this model does not correspond to generative model of the kind standardly associated with pcfgs, but can be seen as random field over the possible parses, with the features being the sequences and contexts (see (abney, 1997)).<papid> J97-4005 </papid></citsent>
<aftsection>
<nextsent>how ever, note that we were primarily interested in the clustering behavior, not the parsing behavior, and that the random field parameters have not been fit to any distribution over trees.
</nextsent>
<nextsent>the parsing model isvery crude, primarily serving to eliminate systematically mutually incompatible analyses.
</nextsent>
<nextsent>4.2.1 sparsitysince this system does not postulate any nonterminal symbols, but works directly with terminal sequences, sparsity will be extremely severe for any reasonably long sequences.
</nextsent>
<nextsent>substantial smoothing was done to all terms; for the 9 sq-  estimates we interpolated the previous counts equally with uniform 9 s3 , otherwise most sequences would remain locked in their initial behaviors.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1527">
<title id=" W01-0510.xml">information extraction using the structured language model </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, the process of converting the word sequence co responding to slot into actionable canonical forms | i.e. convert half past two in the afternoon into 2:30 p.m. | may requiregrammars.
</prevsent>
<prevsent>the design of the frames | what information is relevant for taking certain action, what slot/frame tags are to be used, see (wang, 1999) | is delicate task that we will not be concerned with for the purposes of this paper.the remainder of the paper is organized as fol lows: section 2 reviews the structured language model (slm) followed by section 3 which describes in detail the training procedure and section 4 which denes the operation of the slm as constrained parser and presents the necessary modi cations tothe model.
</prevsent>
</prevsection>
<citsent citstr=" A00-2030 ">
section 5 compares our approach tooth ers in the literature, in particular that of (miller et al., 2000).<papid> A00-2030 </papid></citsent>
<aftsection>
<nextsent>section 6 presents the experiments we have carried out.
</nextsent>
<nextsent>we conclude with section 7.
</nextsent>
<nextsent>we proceed with brief review of the structured language model (slm); an extensive presentation of the slm can be found in (chelba and jelinek, 2000).
</nextsent>
<nextsent>the model assigns probability (w;t ) to every sentence and its every possible binary parse . the terminals of are the words of with postags, and the nodes of are annotated with phrase headwords and non-terminal labels.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1528">
<title id=" W01-0510.xml">information extraction using the structured language model </title>
<section> training procedure.  </section>
<citcontext>
<prevsection>
<prevsent>initialize the slm as syntactic parser for the domain we are interested in.
</prevsent>
<prevsent>a general purpose parser (such as nlpwin (heidorn, 1999)) can be used to generate syntactic treebank from which the slm parameters can be initialized.
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
another possibility for initializing the slm is to use treebank for out-of-domain data (such as the upenn treebank (marcus et al, 1993)) | <papid> J93-2004 </papid>see section 6.1.</citsent>
<aftsection>
<nextsent>2. train the slm as matched constrained parser.
</nextsent>
<nextsent>at this step the parser is going to propose set of syntactic binary parses forgiven word string (n-best parsing), all matching the constituent boundaries speci ed by the semanticparse: parse is said to match the semantic parse s, denoted 3 s, if and only if the set of un-labeled constituents that de ne is included in the set of constituents that de ne .at this time only the constituent span information in is taken into account.
</nextsent>
<nextsent>3.
</nextsent>
<nextsent>enrich the non-terminal and pre-terminal labels of the resulting parses with the semantic tags (frame and slot) present in the semantic parse, thus expanding the vocabulary of non-terminal and pre-terminal tags used by the syntactic parser to include semantic information along side the usual syntactic tags.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1536">
<title id=" W01-0510.xml">information extraction using the structured language model </title>
<section> comparison with previous work.  </section>
<citcontext>
<prevsection>
<prevsent>one possibly bene cial extension of our work suggested by (miller et al, 2000) <papid> A00-2030 </papid>would be to add semantic tags describing relations between entities (slots), in which case the semantic constraints would not be structured strictly on the two levels used in the current approach, respectively frame and slot level.</prevsent>
<prevsent>however, this would complicate the task of data annotation making it more expensive.</prevsent>
</prevsection>
<citsent citstr=" P92-1017 ">
the same constrained em variant employed for re estimating the model parameters has been used by (pereira and schabes, 1992) <papid> P92-1017 </papid>for training purely syntactic parser showing increase in likelihood but no improvement in parsing accuracy.</citsent>
<aftsection>
<nextsent>we have evaluated the model on manually annotated data for the mipad (huang et al, 2000) task.
</nextsent>
<nextsent>wehave used 2,239 sentences (27,119 words) for training and 1,101 sentences (8,652 words) for test.
</nextsent>
<nextsent>there were 2,239/5,431 semantic frames/slots in the training data and 1,101/1,698 in the test data, respectively.
</nextsent>
<nextsent>the word vocabulary size was 1,035, closed over the test data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1540">
<title id=" W01-0510.xml">information extraction using the structured language model </title>
<section> conclusions and future directions.  </section>
<citcontext>
<prevsection>
<prevsent>the performance of the baseline model could be improved with more authoring eort, although this is expensive.the big dierence in performance between training and test and the fact that we are using so little training data, makes improvements by using more training data very likely, although this may be expensive.
</prevsent>
<prevsent>a framework which utilizes the vast amounts of text data collected once such syst emis deployed would be desirable.
</prevsent>
</prevsection>
<citsent citstr=" J96-1002 ">
statistical modeling techniques that make more eective use of the training data should be used in the slm, maximum entropy (berger et al, 1996) <papid> J96-1002 </papid>being good candidate.as for using the slm as the language understanding component of speech driven application, suchas mipad, it would be interesting to evaluate the impact of incorporating the semantic constraints on the word-level accuracy of the system.</citsent>
<aftsection>
<nextsent>another possible research direction is to modify the framework such that it nds the most likely semantic parse given the acoustics | thus treating the word sequence as hidden variable.
</nextsent>



</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1541">
<title id=" W02-0215.xml">issues under negotiation </title>
<section> negotiation as discussing.  </section>
<citcontext>
<prevsection>
<prevsent>negotiation, in our view, should not in general be seen in terms of proposals and counterproposals, but in terms of proposing and choosing between several alternatives.
</prevsent>
<prevsent>alternatives in this section, we will attempt to provide amore detailed description of negotiative dialogue.
</prevsent>
</prevsection>
<citsent citstr=" P98-1052 ">
clearly, negotiation is type of problem solving (di eugenio et al, 1998).<papid> P98-1052 </papid></citsent>
<aftsection>
<nextsent>we definenegotiative dialogue more specifically to be dialogue where dps3 discuss several alternative solutions to problem (issue) before choosing one of them.
</nextsent>
<nextsent>in line with our issue-based approach to dialogue management, we propose to model negotiable problems (issues) semantically as questions and alternative solutions as alternative answers to question.
</nextsent>
<nextsent>we also propose to keep track of these issues under negotiation and the answers being considered as potential solutions to each issue in the form of stack (or ordered set) of pairs of issues 3dps = dialogue participants  today is january 6th.
</nextsent>
<nextsent>propose proposition b(alt.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1543">
<title id=" W01-0809.xml">generation of vietnamese for french vietnamese and english vietnamese machine translation </title>
<section> brief description of its3.  </section>
<citcontext>
<prevsection>
<prevsent>together with the generation module, we construct bilingual lexicons, and thus obtain prototypes of french-vietnamese and english vietnamese mt. as vietnamese is very different from european languages, the implementation of the generation module for vietnamese based on the generic mechanisms of its3 poses non-trivial problems.
</prevsent>
<prevsent>we present here some main problems and their solutions, such as the construction of vietnamese noun phrases (nps), verb phrases (vps), adverbial phrases (advps), relative clauses, etc.
</prevsent>
</prevsection>
<citsent citstr=" C92-3129 ">
its3 (wehrli, 1992; <papid> C92-3129 </papid>etchegoyhen &amp; wehrli, 1998; <papid> W98-1432 </papid>haire &amp; al, 2000) can now translate from french to english and vice versa.</citsent>
<aftsection>
<nextsent>modules for other languages such as german, italian, are underdevelopment.
</nextsent>
<nextsent>its3 is principle-based system, linguistically inspired by the government &amp; binding (gb) theory.
</nextsent>
<nextsent>(see eg.
</nextsent>
<nextsent>haegeman (1994) for an introduction to gb, berwick &amp; al (1991) for principle-based systems).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1544">
<title id=" W01-0809.xml">generation of vietnamese for french vietnamese and english vietnamese machine translation </title>
<section> brief description of its3.  </section>
<citcontext>
<prevsection>
<prevsent>together with the generation module, we construct bilingual lexicons, and thus obtain prototypes of french-vietnamese and english vietnamese mt. as vietnamese is very different from european languages, the implementation of the generation module for vietnamese based on the generic mechanisms of its3 poses non-trivial problems.
</prevsent>
<prevsent>we present here some main problems and their solutions, such as the construction of vietnamese noun phrases (nps), verb phrases (vps), adverbial phrases (advps), relative clauses, etc.
</prevsent>
</prevsection>
<citsent citstr=" W98-1432 ">
its3 (wehrli, 1992; <papid> C92-3129 </papid>etchegoyhen &amp; wehrli, 1998; <papid> W98-1432 </papid>haire &amp; al, 2000) can now translate from french to english and vice versa.</citsent>
<aftsection>
<nextsent>modules for other languages such as german, italian, are underdevelopment.
</nextsent>
<nextsent>its3 is principle-based system, linguistically inspired by the government &amp; binding (gb) theory.
</nextsent>
<nextsent>(see eg.
</nextsent>
<nextsent>haegeman (1994) for an introduction to gb, berwick &amp; al (1991) for principle-based systems).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1548">
<title id=" W01-0513.xml">is knowledge free induction of multiword unit dictionary headwords a solved problem </title>
<section> previous approaches.  </section>
<citcontext>
<prevsection>
<prevsent>a segmentation process may find that symbol stream should not be delimited even though subcomponents of the stream have been seen elsewhere.
</prevsent>
<prevsent>in such cases, these larger units may be mwus.
</prevsent>
</prevsection>
<citsent citstr=" J96-3004 ">
the principal work on segmentation has focused either on identifying words in phonetic streams (saffran, et. al, 1996; brent, 1996; de marcken, 1996) or on tokenizing asian and indian languages that do not normally include word delimiters in their orthography (sproat, et al 1996; <papid> J96-3004 </papid>ponte and croft 1996; shimohata, 1997; teahan, et al, 2000; <papid> J00-3004 </papid>and many others).</citsent>
<aftsection>
<nextsent>such efforts have employed various strategies for segmentation, including the use of hidden markov models, minimum description length, dictionary-based approaches, probabilistic automata, transformation-based learning, and text compression.
</nextsent>
<nextsent>some of these approaches require significant sources of human knowledge, though others, especially those that follow data compression or hmm schemes, do not.
</nextsent>
<nextsent>these approaches could be applied to languages where word delimiters exist (such as in european languages delimited by the space character).
</nextsent>
<nextsent>however, in such languages, it seems more prudent to simply take advantage of delimiters rather than introducing potential errors by trying to find word boundaries while ignoring knowledge of the level and identify appropriate word combinations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1549">
<title id=" W01-0513.xml">is knowledge free induction of multiword unit dictionary headwords a solved problem </title>
<section> previous approaches.  </section>
<citcontext>
<prevsection>
<prevsent>a segmentation process may find that symbol stream should not be delimited even though subcomponents of the stream have been seen elsewhere.
</prevsent>
<prevsent>in such cases, these larger units may be mwus.
</prevsent>
</prevsection>
<citsent citstr=" J00-3004 ">
the principal work on segmentation has focused either on identifying words in phonetic streams (saffran, et. al, 1996; brent, 1996; de marcken, 1996) or on tokenizing asian and indian languages that do not normally include word delimiters in their orthography (sproat, et al 1996; <papid> J96-3004 </papid>ponte and croft 1996; shimohata, 1997; teahan, et al, 2000; <papid> J00-3004 </papid>and many others).</citsent>
<aftsection>
<nextsent>such efforts have employed various strategies for segmentation, including the use of hidden markov models, minimum description length, dictionary-based approaches, probabilistic automata, transformation-based learning, and text compression.
</nextsent>
<nextsent>some of these approaches require significant sources of human knowledge, though others, especially those that follow data compression or hmm schemes, do not.
</nextsent>
<nextsent>these approaches could be applied to languages where word delimiters exist (such as in european languages delimited by the space character).
</nextsent>
<nextsent>however, in such languages, it seems more prudent to simply take advantage of delimiters rather than introducing potential errors by trying to find word boundaries while ignoring knowledge of the level and identify appropriate word combinations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1550">
<title id=" W01-0513.xml">is knowledge free induction of multiword unit dictionary headwords a solved problem </title>
<section> previous approaches.  </section>
<citcontext>
<prevsection>
<prevsent>however, in such languages, it seems more prudent to simply take advantage of delimiters rather than introducing potential errors by trying to find word boundaries while ignoring knowledge of the level and identify appropriate word combinations.
</prevsent>
<prevsent>2.2 word-based, knowledge-driven strategies.
</prevsent>
</prevsection>
<citsent citstr=" P97-1004 ">
some researchers start with words and propose mwu induction methods that make use of parts of speech, lexicons, syntax or other linguistic structure (justeson and katz, 1995; jacquemin, et al, 1997; <papid> P97-1004 </papid>daille, 1996).</citsent>
<aftsection>
<nextsent>for example, justeson and katz indicated that the patterns noun noun and adj noun are very typical of mwus.
</nextsent>
<nextsent>daille also suggests that in french, technical mwus follow patterns such as noun de noun  (1996, p. 50).
</nextsent>
<nextsent>to find word combinations that satisfy such patterns in both of these situations necessitates the use of lexicon equipped with part of speech tags.
</nextsent>
<nextsent>since we are interested in knowledge-free induction of mwus, these approaches are less directly related to our work.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1551">
<title id=" W01-0513.xml">is knowledge free induction of multiword unit dictionary headwords a solved problem </title>
<section> lexical access.  </section>
<citcontext>
<prevsection>
<prevsent>we opt to treat punctuation as words.
</prevsent>
<prevsent>once we tokenize, we use churchs (1995) suffix array approach to identify word n-grams that occur at least times (for t=10).
</prevsent>
</prevsection>
<citsent citstr=" J90-1003 ">
we then rank-order the px|ymixy mz prz|ymizy 2log [px pypx py] fy [pxypxy] fxy [pxypxy] fxy i{x,x} j{y,y} (fij ij )2 ij fxy xy xy (1 (xy/n)) fxy xy fxy (1 (fxy/n)) table 1: probabilistic approaches method formula frequency (guiliano, 1964) fxy pointwise mutual information (mi) (fano, 1961; church and hanks, 1990) <papid> J90-1003 </papid>log (p / p )2 xy y selectional association (resnik, 1996) symmetric conditional probability (ferreira and pereira, 1999) / pxy y2 dice formula (dice, 1945) 2 / (f +f )xy y log-likelihood (dunning, 1993; (<papid> J93-1003 </papid>daille, 1996).</citsent>
<aftsection>
<nextsent>since we need knowledge-poor daille, 1996) induction, we cannot use human-suggested filtering chi-squared ($ )2 (church and gale, 1991) z-score (smadja, 1993; <papid> J93-1007 </papid>fontenelle, et al, 1994) students t-score (church and hanks, 1990) <papid> J90-1003 </papid>n-gram list in accordance to each probabilistic algorithm.</nextsent>
<nextsent>this task is non-trivial since most algorithms were originally suited for finding two word collocations.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1553">
<title id=" W01-0513.xml">is knowledge free induction of multiword unit dictionary headwords a solved problem </title>
<section> lexical access.  </section>
<citcontext>
<prevsection>
<prevsent>we opt to treat punctuation as words.
</prevsent>
<prevsent>once we tokenize, we use churchs (1995) suffix array approach to identify word n-grams that occur at least times (for t=10).
</prevsent>
</prevsection>
<citsent citstr=" J93-1003 ">
we then rank-order the px|ymixy mz prz|ymizy 2log [px pypx py] fy [pxypxy] fxy [pxypxy] fxy i{x,x} j{y,y} (fij ij )2 ij fxy xy xy (1 (xy/n)) fxy xy fxy (1 (fxy/n)) table 1: probabilistic approaches method formula frequency (guiliano, 1964) fxy pointwise mutual information (mi) (fano, 1961; church and hanks, 1990) <papid> J90-1003 </papid>log (p / p )2 xy y selectional association (resnik, 1996) symmetric conditional probability (ferreira and pereira, 1999) / pxy y2 dice formula (dice, 1945) 2 / (f +f )xy y log-likelihood (dunning, 1993; (<papid> J93-1003 </papid>daille, 1996).</citsent>
<aftsection>
<nextsent>since we need knowledge-poor daille, 1996) induction, we cannot use human-suggested filtering chi-squared ($ )2 (church and gale, 1991) z-score (smadja, 1993; <papid> J93-1007 </papid>fontenelle, et al, 1994) students t-score (church and hanks, 1990) <papid> J90-1003 </papid>n-gram list in accordance to each probabilistic algorithm.</nextsent>
<nextsent>this task is non-trivial since most algorithms were originally suited for finding two word collocations.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1554">
<title id=" W01-0513.xml">is knowledge free induction of multiword unit dictionary headwords a solved problem </title>
<section> lexical access.  </section>
<citcontext>
<prevsection>
<prevsent>once we tokenize, we use churchs (1995) suffix array approach to identify word n-grams that occur at least times (for t=10).
</prevsent>
<prevsent>we then rank-order the px|ymixy mz prz|ymizy 2log [px pypx py] fy [pxypxy] fxy [pxypxy] fxy i{x,x} j{y,y} (fij ij )2 ij fxy xy xy (1 (xy/n)) fxy xy fxy (1 (fxy/n)) table 1: probabilistic approaches method formula frequency (guiliano, 1964) fxy pointwise mutual information (mi) (fano, 1961; church and hanks, 1990) <papid> J90-1003 </papid>log (p / p )2 xy y selectional association (resnik, 1996) symmetric conditional probability (ferreira and pereira, 1999) / pxy y2 dice formula (dice, 1945) 2 / (f +f )xy y log-likelihood (dunning, 1993; (<papid> J93-1003 </papid>daille, 1996).</prevsent>
</prevsection>
<citsent citstr=" J93-1007 ">
since we need knowledge-poor daille, 1996) induction, we cannot use human-suggested filtering chi-squared ($ )2 (church and gale, 1991) z-score (smadja, 1993; <papid> J93-1007 </papid>fontenelle, et al, 1994) students t-score (church and hanks, 1990) <papid> J90-1003 </papid>n-gram list in accordance to each probabilistic algorithm.</citsent>
<aftsection>
<nextsent>this task is non-trivial since most algorithms were originally suited for finding two word collocations.
</nextsent>
<nextsent>we must therefore decide how to expand the algorithms to identify general n-grams (say, c=w ...w ).
</nextsent>
<nextsent>we can either generalize or1 2 approximate.
</nextsent>
<nextsent>since generalizing requires exponential compute time and memory for several of the algorithms, approximation is an attractive alternative.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="Y1557">
<title id=" W01-0513.xml">is knowledge free induction of multiword unit dictionary headwords a solved problem </title>
<section> improvement strategies.  </section>
<citcontext>
<prevsection>
<prevsent>for every collocation c=x ..x x ..x , we1 2 i-1 i+1 ni attempt to find other similar patterns in the data, x ..x yx ..x . if and are semantically1 2 i-1 i+1 i related, chances are that is substitutable.
</prevsent>
<prevsent>since lsa excels at finding semantic correlations, we can compare and to see if isxi substitutable.
</prevsent>
</prevsection>
<citsent citstr=" W00-0712 ">
we use our earlier approach (schone and jurafsky, 2000) <papid> W00-0712 </papid>for performing the comparison; namely, for every word w, we compute cos( )w, for 200 randomly chosen words, r. this allows for computation of correlaton mean (?</citsent>
<aftsection>
<nextsent>) and standardw deviation (1 ) between and other words.
</nextsent>
<nextsent>asw before, we then compute normalized cosine score ( ) between words of interest, defined by with this set-up, we now look for substitutivity.
</nextsent>
<nextsent>note that phrases may be substitutable and still be headword if their substitute phrases are themselves mwus.
</nextsent>
<nextsent>for example, dioxide in carbon_dioxide is semantically similar to monoxide in carbon_monoxide.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
</paper>