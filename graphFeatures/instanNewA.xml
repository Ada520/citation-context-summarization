<paper>
<cited id="A0">
<title id=" A92-1008.xml">generating spatial descriptions for cross modal references </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>before you lift the lid make sure that the knob in the middle is in position c. \o \ ???
</prevsent>
<prevsent>remove the cover and pour in cold tap water.
</prevsent>
</prevsection>
<citsent citstr=" E91-1003 ">
figure 1: fragment from an instruction manual cross-modal deictic expressions, e.g.,  the lid  or  the knob in the middle,  help to establish the coreferentiality between the entities mentioned in the text and shown in the picture as well (\[wahlster et al, 1991<papid> E91-1003 </papid>b\]).</citsent>
<aftsection>
<nextsent>the use o!
</nextsent>
<nextsent>spatial relationships uch as  the knob in the middle  simplifies the generation of referring expressions that have to identify particular object in picture.
</nextsent>
<nextsent>ob-viously these spatial relationships cannot be computed in advance because they depend on the projection para-meters for the picture, e.g., the viewpoint, which in turn themselves depend on the communicative intent of the document to be planned 1.
</nextsent>
<nextsent>the localisation component described in this paper was developed in order to support the generation ot cross-modal deietic referring expressions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A1">
<title id=" C00-1077.xml">identifying terms by their family and friends </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the resulting list of ranked terms is shown to improve on that produced by traditional method- s, in terms of precision and distribution, while the information acquired in the process can also be used for variety of other applications, such as disam-biguation, lexical tuning and term clustering.
</prevsent>
<prevsent>although statistical approaches to automatic term recognition, e.g.
</prevsent>
</prevsection>
<citsent citstr=" C92-3150 ">
(bourigault, 1992; <papid> C92-3150 </papid>daille et al, 1994; <papid> C94-1084 </papid>enguehard and pantera, 1994; 3usteson and katz, 1995; lauriston, 1996), have achieved rela-tive success over the years, the addition of suitable linguistic information has the potential to enhance results still further, particularly in the case of small corpora or very specialised omains, where statis-tical information may not be so accurate.</citsent>
<aftsection>
<nextsent>one of the main reasons for the current lack of diversity in approaches to term recognition lies in the difficul-ty of extracting suitable semantic information from speeialised corpora, particularly in view of the lack of appropriate linguistic resources.
</nextsent>
<nextsent>the increasing development of electronic lexieal resources, coupled with new methods for automatically creating and fine-tuning them from corpora, has begun to pave the way for more dominant appearance of natural language processing techniques in the field of termi-nology.
</nextsent>
<nextsent>the trucks approach to term recognition (ter- recognition using combined knowledge sources) focuses on identifying relevant contextual informa-tion from variety of sources, in order to enhance traditional statistical techniques of term recognition.
</nextsent>
<nextsent>although contextual information has been previous-ly used, e.g. in general language (grefenstette, 1994) mid in the nc-value method for term recognition (frantzi, 1998; frantzi and ananiadou, 1999), only shallow syntactic information is used in these cas-es.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A3">
<title id=" C00-1077.xml">identifying terms by their family and friends </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the resulting list of ranked terms is shown to improve on that produced by traditional method- s, in terms of precision and distribution, while the information acquired in the process can also be used for variety of other applications, such as disam-biguation, lexical tuning and term clustering.
</prevsent>
<prevsent>although statistical approaches to automatic term recognition, e.g.
</prevsent>
</prevsection>
<citsent citstr=" C94-1084 ">
(bourigault, 1992; <papid> C92-3150 </papid>daille et al, 1994; <papid> C94-1084 </papid>enguehard and pantera, 1994; 3usteson and katz, 1995; lauriston, 1996), have achieved rela-tive success over the years, the addition of suitable linguistic information has the potential to enhance results still further, particularly in the case of small corpora or very specialised omains, where statis-tical information may not be so accurate.</citsent>
<aftsection>
<nextsent>one of the main reasons for the current lack of diversity in approaches to term recognition lies in the difficul-ty of extracting suitable semantic information from speeialised corpora, particularly in view of the lack of appropriate linguistic resources.
</nextsent>
<nextsent>the increasing development of electronic lexieal resources, coupled with new methods for automatically creating and fine-tuning them from corpora, has begun to pave the way for more dominant appearance of natural language processing techniques in the field of termi-nology.
</nextsent>
<nextsent>the trucks approach to term recognition (ter- recognition using combined knowledge sources) focuses on identifying relevant contextual informa-tion from variety of sources, in order to enhance traditional statistical techniques of term recognition.
</nextsent>
<nextsent>although contextual information has been previous-ly used, e.g. in general language (grefenstette, 1994) mid in the nc-value method for term recognition (frantzi, 1998; frantzi and ananiadou, 1999), only shallow syntactic information is used in these cas-es.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A6">
<title id=" C00-1077.xml">identifying terms by their family and friends </title>
<section> the nc-value method.  </section>
<citcontext>
<prevsection>
<prevsent>thirdly, we can nee that more of the top 20 terms are valid tin  tim snc-vahm than for the nc- value: 17 (851x,) as ot)t)osed to 10 (50%).
</prevsent>
<prevsent>6 eva luat ion.
</prevsent>
</prevsection>
<citsent citstr=" A92-1021 ">
the snc-value method wan initially t(;sted on eor- l)us of 800,000 eye t)athoh)gy rei)ortn , which had 1)een tagged with the brill t)art-of-nl)eeeh tagger (brill, 1992).<papid> A92-1021 </papid></citsent>
<aftsection>
<nextsent>the ca.ndidate terms we, first ex-tracted using the nc-value method (lhantzi, 1998), and the snc-value was then (:alculated.
</nextsent>
<nextsent>to exvdu- ate the results, we examined the p(. rformanee of the similarity weight alone, and the overall 1)erformance of the system.
</nextsent>
<nextsent>6.1 evaluation methods.
</nextsent>
<nextsent>the main evaluation i)rocedure was carried out with resl)ect a manual assessment of tim list of terms l)y 2 domain exi)erts.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A7">
<title id=" C00-1077.xml">identifying terms by their family and friends </title>
<section> the nc-value method.  </section>
<citcontext>
<prevsection>
<prevsent>this there-fore demonstrates two things: ? more of  the terms with the highest semantic weights are valid, and fewer of those with the lowest semmitic weights are valid; ? more valid terms have high semantic weights than non-terms, mid more non-terms have lower semantic weights than valid terms.
</prevsent>
<prevsent>we also tested the similarity measure to see whether adding sosne statistical information would improve its results, and regulate any discrepancies in tile uniformity of the hierarchy.
</prevsent>
</prevsection>
<citsent citstr=" W95-0105 ">
the method- which intuitively seem most plausible are based on information content, e.g.(resnik, 1995; <papid> W95-0105 </papid>smeaton and quigley, 1996).</citsent>
<aftsection>
<nextsent>the informatiosl content of n- ode is related to its probability of occurrence in the corpus.
</nextsent>
<nextsent>tile snore fi equently it appears, the snore likely it is to be important in terms of conveying information, and therefore the higher weighting it should receive.
</nextsent>
<nextsent>we performed experiments to cosn- pare two such methods with our similarity measure.
</nextsent>
<nextsent>the first considers the probability of the msca of the two terms (the lowest node which is an ancestor of both), whilst the second considers the probability of the nodes of the terms being colnpared.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A8">
<title id=" C00-1077.xml">identifying terms by their family and friends </title>
<section> the nc-value method.  </section>
<citcontext>
<prevsection>
<prevsent>this is largely l)ecmlse we have used very small corpus for testing.
</prevsent>
<prevsent>the contextum information acquired can also be used for mmlber of other related tasks, such as disambiguation and clustering.
</prevsent>
</prevsection>
<citsent citstr=" C96-2212 ">
at present, these- mantic information is acquired from 1)re-existing domain-slmcitic thesaurus, but there m:c 1)ossibili- tics for creating such thesaurus automatically, or entrancing an existing one, using the contextual in-formation we acquire (ushioda, 1996; <papid> C96-2212 </papid>maynam and anmfiadou, 1999b).</citsent>
<aftsection>
<nextsent>there is much scope tbr filrther extensions of this research.
</nextsent>
<nextsent>firstly, it; could be extended to other (lo- mains and larger corpora, in order to see the true benefit of such a.n apl)roach.
</nextsent>
<nextsent>secondly, the thesaurus could be tailored to the corpus, as we have men- tioncd.
</nextsent>
<nextsent>an incremental approach might be possible, whereby the similarity measure is combined with s- tatistical intbrmation to tune an existing ontology.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A9">
<title id=" A00-1021.xml">ranking suspected answers to natural language questions using predictive annotation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the results are shown in section 7.
</prevsent>
<prevsent>some techniques used by other participants in the trec evaluation are paragraph indexing, followed by abductive inference (harabagiu and maiorano, 1999) and knowledge-representation combined with information retrieval (breck et al., 1999).
</prevsent>
</prevsection>
<citsent citstr=" A97-1033 ">
some earlier systems related to our work are faq finder (kulyukin et al, 1998), murax (kupiec, 1993), which uses an encyclo-pedia as knowledge base from which to extract answers, and profile (radev and mckeown, 1997) <papid> A97-1033 </papid>which identifies named entities and noun phrases that describe them in text.</citsent>
<aftsection>
<nextsent>our system (figure 1) consists of two pieces: an ir component (guruqa) that which returns matching texts, and an answer selection compo- 150 neat (ansel/werlect) that extracts and ranks potential answers from these texts.
</nextsent>
<nextsent>this paper focuses on the process of rank-ing potential answers selected by their engine, which is itself described in (prager et al, 1999).
</nextsent>
<nextsent>~ lndexer ~ searc~x ~ guruqa \ \ rankcd ~ ansel/ ~ hit list tl st \[ \[ werlect answer selection figure 1: system architecture.
</nextsent>
<nextsent>2.1 the information retrieval.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A10">
<title id=" A00-1021.xml">ranking suspected answers to natural language questions using predictive annotation </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>some templates do not cause complete re-placement of the matched string.
</prevsent>
<prevsent>forex- ample, the pattern  what is the popula-tion  gets replaced by  numbers popu-lation .
</prevsent>
</prevsection>
<citsent citstr=" A97-1030 ">
before indexing the text, we process it with tex tract (byrd and ravin, 1998; wacholder et al, 1997), <papid> A97-1030 </papid>which performs lemmatization, and discovers proper names and technical terms.</citsent>
<aftsection>
<nextsent>we added new module (resporator) which annotates text segments with qa-tokens using pattern matching.
</nextsent>
<nextsent>thus the text  for 5 centuries  matches the durations pattern  for :cardinal _timeperiod , where :car- dinal is the label for cardinal numbers, and _timeperiod marks time expression.
</nextsent>
<nextsent>guruqa scores text passages instead of documents.
</nextsent>
<nextsent>we use simple document- and collection-independent weighting scheme: qa-tokens get weight of 400, proper nouns get 200 and any other word - 100 (stop words are removed in query processing after the pattern template matching operation).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A11">
<title id=" A97-1055.xml">automatic selection of class labels from a thesaurus for an effective semantic tagging of corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the obvious trategy to reduce this problem is to gener-alise word patterns according to some clustering tech-niques.
</prevsent>
<prevsent>in the literature, two generalisation strategies have been adopted: distributional approaches: several papers adopt distribu-tional techniques to identify clusters of words according to some defined measure of similarity.
</prevsent>
</prevsection>
<citsent citstr=" C94-2119 ">
among these, in (grishman and sterling, 1994) <papid> C94-2119 </papid>method is proposed to cluster syntactic triples, while in (pereira and tishby 1992, 1993), (dagan et al, 1994) pure bigrams are anal- ysed.</citsent>
<aftsection>
<nextsent>the most intuitive evaluation of the effectiveness of dis-tributional approaches tothe problem of word general-ization is presented in (grishman and sterling, 1994).<papid> C94-2119 </papid></nextsent>
<nextsent>in this paper it is argued that distributional (called also smoothing) techniques introduce certain degree of addi-tional error, because co-occurrences may be erroneously conflated in cluster, and some of the co-occurrences be-ing generalized are themselves incorrect.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A13">
<title id=" A97-1055.xml">automatic selection of class labels from a thesaurus for an effective semantic tagging of corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>another drawback of these methods that, since clusters have only numeric description, they are often hard to evaluate on linguistic ground.
</prevsent>
<prevsent>semantic tagging: another adopted solution is to gener- 380 alise the observed word patterns by grouping patterns in which words have the same semantic tag.
</prevsent>
</prevsection>
<citsent citstr=" J96-4006 ">
semantic tags are assigned from on-line thesaura like wordnet (basili et al 1996<papid> J96-4006 </papid>resnik, 1995), <papid> W95-0105 </papid>roget categories (yarowsky 1992) (<papid> C92-2070 </papid>chen and chen, 1996), <papid> C96-1038 </papid>the japanese bgh (utsuro et al 1993), or assigned manually (basili et al 1992) <papid> A92-1013 </papid>1.</citsent>
<aftsection>
<nextsent>the obvious advantage of semantic tags is that words are clustered according to an intuitive principle (they belong to the same concept) rather than to some probabilistic measure.
</nextsent>
<nextsent>semantic tagging has been proven useful for learning and categorising interesting relations among words, and for systematic lexical learning in sublan-guages, as shown in (basili et al 1996<papid> J96-4006 </papid>and (basili et al 1996<papid> J96-4006 </papid>b).</nextsent>
<nextsent>on the other hand, semantic tagging has serious draw- back, which is not solely due to the limited availability of on-line resources, but rather to the entangled structure of thesaura.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A16">
<title id=" A97-1055.xml">automatic selection of class labels from a thesaurus for an effective semantic tagging of corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>another drawback of these methods that, since clusters have only numeric description, they are often hard to evaluate on linguistic ground.
</prevsent>
<prevsent>semantic tagging: another adopted solution is to gener- 380 alise the observed word patterns by grouping patterns in which words have the same semantic tag.
</prevsent>
</prevsection>
<citsent citstr=" W95-0105 ">
semantic tags are assigned from on-line thesaura like wordnet (basili et al 1996<papid> J96-4006 </papid>resnik, 1995), <papid> W95-0105 </papid>roget categories (yarowsky 1992) (<papid> C92-2070 </papid>chen and chen, 1996), <papid> C96-1038 </papid>the japanese bgh (utsuro et al 1993), or assigned manually (basili et al 1992) <papid> A92-1013 </papid>1.</citsent>
<aftsection>
<nextsent>the obvious advantage of semantic tags is that words are clustered according to an intuitive principle (they belong to the same concept) rather than to some probabilistic measure.
</nextsent>
<nextsent>semantic tagging has been proven useful for learning and categorising interesting relations among words, and for systematic lexical learning in sublan-guages, as shown in (basili et al 1996<papid> J96-4006 </papid>and (basili et al 1996<papid> J96-4006 </papid>b).</nextsent>
<nextsent>on the other hand, semantic tagging has serious draw- back, which is not solely due to the limited availability of on-line resources, but rather to the entangled structure of thesaura.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A17">
<title id=" A97-1055.xml">automatic selection of class labels from a thesaurus for an effective semantic tagging of corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>another drawback of these methods that, since clusters have only numeric description, they are often hard to evaluate on linguistic ground.
</prevsent>
<prevsent>semantic tagging: another adopted solution is to gener- 380 alise the observed word patterns by grouping patterns in which words have the same semantic tag.
</prevsent>
</prevsection>
<citsent citstr=" C92-2070 ">
semantic tags are assigned from on-line thesaura like wordnet (basili et al 1996<papid> J96-4006 </papid>resnik, 1995), <papid> W95-0105 </papid>roget categories (yarowsky 1992) (<papid> C92-2070 </papid>chen and chen, 1996), <papid> C96-1038 </papid>the japanese bgh (utsuro et al 1993), or assigned manually (basili et al 1992) <papid> A92-1013 </papid>1.</citsent>
<aftsection>
<nextsent>the obvious advantage of semantic tags is that words are clustered according to an intuitive principle (they belong to the same concept) rather than to some probabilistic measure.
</nextsent>
<nextsent>semantic tagging has been proven useful for learning and categorising interesting relations among words, and for systematic lexical learning in sublan-guages, as shown in (basili et al 1996<papid> J96-4006 </papid>and (basili et al 1996<papid> J96-4006 </papid>b).</nextsent>
<nextsent>on the other hand, semantic tagging has serious draw- back, which is not solely due to the limited availability of on-line resources, but rather to the entangled structure of thesaura.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A18">
<title id=" A97-1055.xml">automatic selection of class labels from a thesaurus for an effective semantic tagging of corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>another drawback of these methods that, since clusters have only numeric description, they are often hard to evaluate on linguistic ground.
</prevsent>
<prevsent>semantic tagging: another adopted solution is to gener- 380 alise the observed word patterns by grouping patterns in which words have the same semantic tag.
</prevsent>
</prevsection>
<citsent citstr=" C96-1038 ">
semantic tags are assigned from on-line thesaura like wordnet (basili et al 1996<papid> J96-4006 </papid>resnik, 1995), <papid> W95-0105 </papid>roget categories (yarowsky 1992) (<papid> C92-2070 </papid>chen and chen, 1996), <papid> C96-1038 </papid>the japanese bgh (utsuro et al 1993), or assigned manually (basili et al 1992) <papid> A92-1013 </papid>1.</citsent>
<aftsection>
<nextsent>the obvious advantage of semantic tags is that words are clustered according to an intuitive principle (they belong to the same concept) rather than to some probabilistic measure.
</nextsent>
<nextsent>semantic tagging has been proven useful for learning and categorising interesting relations among words, and for systematic lexical learning in sublan-guages, as shown in (basili et al 1996<papid> J96-4006 </papid>and (basili et al 1996<papid> J96-4006 </papid>b).</nextsent>
<nextsent>on the other hand, semantic tagging has serious draw- back, which is not solely due to the limited availability of on-line resources, but rather to the entangled structure of thesaura.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A19">
<title id=" A97-1055.xml">automatic selection of class labels from a thesaurus for an effective semantic tagging of corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>another drawback of these methods that, since clusters have only numeric description, they are often hard to evaluate on linguistic ground.
</prevsent>
<prevsent>semantic tagging: another adopted solution is to gener- 380 alise the observed word patterns by grouping patterns in which words have the same semantic tag.
</prevsent>
</prevsection>
<citsent citstr=" A92-1013 ">
semantic tags are assigned from on-line thesaura like wordnet (basili et al 1996<papid> J96-4006 </papid>resnik, 1995), <papid> W95-0105 </papid>roget categories (yarowsky 1992) (<papid> C92-2070 </papid>chen and chen, 1996), <papid> C96-1038 </papid>the japanese bgh (utsuro et al 1993), or assigned manually (basili et al 1992) <papid> A92-1013 </papid>1.</citsent>
<aftsection>
<nextsent>the obvious advantage of semantic tags is that words are clustered according to an intuitive principle (they belong to the same concept) rather than to some probabilistic measure.
</nextsent>
<nextsent>semantic tagging has been proven useful for learning and categorising interesting relations among words, and for systematic lexical learning in sublan-guages, as shown in (basili et al 1996<papid> J96-4006 </papid>and (basili et al 1996<papid> J96-4006 </papid>b).</nextsent>
<nextsent>on the other hand, semantic tagging has serious draw- back, which is not solely due to the limited availability of on-line resources, but rather to the entangled structure of thesaura.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A23">
<title id=" A97-1055.xml">automatic selection of class labels from a thesaurus for an effective semantic tagging of corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the purpose was rather to provide the linguists with very refined, general purpose, linguistically motivated source of taxonomic knowledge.
</prevsent>
<prevsent>as consequence, in most on-line thesaura words are ex-tremely ambiguous, with very subtle distinctions among senses .
</prevsent>
</prevsection>
<citsent citstr=" C94-2113 ">
(dolan, 1994) <papid> C94-2113 </papid>and (krovetz and croft, 1992) claim that fine-grained semantic distinctions are unlikely to be of practical value for many applications.</citsent>
<aftsection>
<nextsent>our experience supports this claim: often, what matters is to be able to distinguish among contrastive (pustejowsky, 1995) ambi-guities of the bank_river bank__organisation flavour.
</nextsent>
<nextsent>high ambiguity, entangled nodes, and asymmetry have al-ready been emphasised in (hearst and shutze, 1993) as being an obstacle to the effective use of on-line thesaura in corpus linguistics.
</nextsent>
<nextsent>in most cases, the noise introduced by over ambiguity almost overrides the positive ffect of semantic lustering.
</nextsent>
<nextsent>for example, in (brill and resnik, 1994) <papid> C94-2195 </papid>clustering pp heads according to wordnet synsets produced only 1% improvement in pp disambiguation task, with respect to the non-clustered method.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A24">
<title id=" A97-1055.xml">automatic selection of class labels from a thesaurus for an effective semantic tagging of corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>high ambiguity, entangled nodes, and asymmetry have al-ready been emphasised in (hearst and shutze, 1993) as being an obstacle to the effective use of on-line thesaura in corpus linguistics.
</prevsent>
<prevsent>in most cases, the noise introduced by over ambiguity almost overrides the positive ffect of semantic lustering.
</prevsent>
</prevsection>
<citsent citstr=" C94-2195 ">
for example, in (brill and resnik, 1994) <papid> C94-2195 </papid>clustering pp heads according to wordnet synsets produced only 1% improvement in pp disambiguation task, with respect to the non-clustered method.</citsent>
<aftsection>
<nextsent>there are reported cases in which the use of wordnet worsened the performance ofan automatic indexing method.
</nextsent>
<nextsent>even context-based sense disambiguation becomes pro-hibitive task on wide-scale basis, because when words in the context of an ambiguous word are replaced by 1 manually assigning semantic tags if of course rather.
</nextsent>
<nextsent>time-consuming, however on-line thesaura re not avail-able in many languages, like italian.
</nextsent>
<nextsent>their synsets, there is multiplication of possible con-texts, rather than generalization.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A25">
<title id=" A97-1055.xml">automatic selection of class labels from a thesaurus for an effective semantic tagging of corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>time-consuming, however on-line thesaura re not avail-able in many languages, like italian.
</prevsent>
<prevsent>their synsets, there is multiplication of possible con-texts, rather than generalization.
</prevsent>
</prevsection>
<citsent citstr=" C96-1005 ">
in (agirre and rigau, 1996) <papid> C96-1005 </papid>method called conceptual distance is proposed to reduce this problem, but the reported performance in disambiguation still do not reach 50%.</citsent>
<aftsection>
<nextsent>a possible alternative is to manually select set of high- level tags from the thesaurus.
</nextsent>
<nextsent>this approach is adopted in (chen and chen, 1996) <papid> C96-1038 </papid>and in (basili et al 1996<papid> J96-4006 </papid>where only dozen categories are used.</nextsent>
<nextsent>as discussed in the latter paper, high-level tags reduce the problem of over ambiguity and allow the detection of more regular behaviours in the analysis of lexical patterns.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A31">
<title id=" A94-1003.xml">language determination natural language processing from scanned document images </title>
<section> i t roduct ion   </section>
<citcontext>
<prevsection>
<prevsent>figure 1: character code representation d character shape code representation.
</prevsent>
<prevsent>the shape-based representation a document is proving to be remarkably rich source of information.
</prevsent>
</prevsection>
<citsent citstr=" C94-2108 ">
while our initial goal has been to use it for language identification, in support of downstream ocr pro- 15 cesses, we are finding that his representation may itself be sufficient for natural language applications such as document indexing and content characterization (see nakayama (this volume), sibun &amp; farrar 1994).<papid> C94-2108 </papid></citsent>
<aftsection>
<nextsent>we fred these indications exciting because ocr is an expensive, slow, and often inaccurate process, especially inthe pres-ence of printing and scanning artifacts uch as broken or touching characters or skew or curvature of text lines.
</nextsent>
<nextsent>thus, if our technique allows natural language process-ing systems to apply ocr selectively or to side-step ocr entirely, such systems will become faster, less expensive, and more robust.
</nextsent>
<nextsent>in this paper, we first explain the background ofour system that constructs character shape codes and word shape tokens from document image.
</nextsent>
<nextsent>we next describe our method for language determination from this shape- based representation, and demonstrate our approach using only the three languages f.nglish, french, and ger-man. we then describe an automated version of this pro-cess that allows us to apply our techniques to an arbitrary set of lan~ruages and show its performance on23 roman- alphabet languages.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A32">
<title id=" C00-2098.xml">a context sensitive model for probabilistic lr parsing of spoken language with transformation based postprocessing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the input for the vcrbmobil speaker independent speech recognizers is spontaneously spoken german (vocabuhlry 10,254 word forms), english (7,534 word forms) and japanese (2,848 word forms).
</prevsent>
<prevsent>the output of the speech recognizers and tile prosody module is prosodically annotated word graph.
</prevsent>
</prevsection>
<citsent citstr=" P99-1061 ">
this word graph is sent to the integrated processing module which controls the three parsers (hpsg parser (kiefer et al, 1999), <papid> P99-1061 </papid>chunk parser (abney, 1991) and our probabilistic parser) of tile  deep  (semantics based) translation branch of verbmobil.</citsent>
<aftsection>
<nextsent>our probabilistic parser is shift-reduce parser and uses an a*-search to find the best scored path in the lattice that can be parsed by its context fi ee grammar.
</nextsent>
<nextsent>tile output of tile parser is the best scored context free analysis for this path.
</nextsent>
<nextsent>this syntax tree is passed to transformation unit that corrects known systematic errors of tile probabilistic parser to correct trees.
</nextsent>
<nextsent>the result of this process is passed to semantics construction module and processed by the other modules of the deep translation branch as shown in figure 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A33">
<title id=" C00-2098.xml">a context sensitive model for probabilistic lr parsing of spoken language with transformation based postprocessing </title>
<section> spontaneous speech parsing.  </section>
<citcontext>
<prevsection>
<prevsent>3.1.
</prevsent>
<prevsent>previous approaches.
</prevsent>
</prevsection>
<citsent citstr=" J93-1002 ">
there am several approaches (see for example (wright &amp; wrigley, 1991), (briscoe &amp; carroll, 1993/<papid> J93-1002 </papid>1996), (lavie, 1996) or (inui et al, 1997)) to probabilistic shift-reduce parsing but only lavie parser, whose probabilistic model is very similar to (briscoe &amp; carroll, 1993), <papid> J93-1002 </papid>has been tested on spontaneously spoken utterances.</citsent>
<aftsection>
<nextsent>while the model presented by (wright &amp; wrigley, 1991) was equivalent to the standard pcfg (probabilistic context-free grammar, see (charniak, 1993)) model, which is not context- sensitive and thus has certain limitations in the precision that it can achieve, later work tried to implement slight context-sensitivity (as e.g. the probability of shift/reduce-action in briscoe and carroll model depends oll the current and succeeding lr parser state and the look-ahead symbol).
</nextsent>
<nextsent>3.2.
</nextsent>
<nextsent>bringing context to probabilistie shift-.
</nextsent>
<nextsent>reduce parsing like other work oi1 probabilistic parsing our model is based on the equation p(t iw) - -v(t )  (wit)   (2) where is the analysis of word sequence and widely used approximation for p(~t) is given by p(wit)~ lq p(w, ll) , (3) w,gw where /i is the part-of speech tag for word wi in analysis t. finding realistic approximation for p(7) is very difficult but important to achieve high parsing accuracy.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A35">
<title id=" A94-1022.xml">the delphi natural language understanding system </title>
<section> grammar  and the  syntax /semant ics.  </section>
<citcontext>
<prevsection>
<prevsent>an example is: (s arg mood) -  subject: (np arg mood etc.) head: (vp agr mood etc.) in this rule, there is head vp and an np which bears the subject relation to it.
</prevsent>
<prevsent>other grammati-cal relations include the familar direct-object and indirect-object as well as the prepositions, uch as to, from, with and so on.
</prevsent>
</prevsection>
<citsent citstr=" P93-1008 ">
annotating sub-constituents with grammatical rela-tions regularizes the syntactic structure with respect particular grammatical rules, and allows  relation-to- relation  form of compositionality, as opposed to the more traditional  rule-to-rule  version that is exempli-fied by such systems as gemini (dowding et al 1993) <papid> P93-1008 </papid>and the core language engine (alshawi, 1992).</citsent>
<aftsection>
<nextsent>in relation-to-relation compositionality, each grammatical relation in the syntactic structure corresponds to se-mantic relation in parallel semantic structure we call  semantic graph .
</nextsent>
<nextsent>the terminal nodes of the seman-tic graph are the word meanings, corresponding to the lexical heads of syntactic structure.
</nextsent>
<nextsent>an example of semantic graph, representing the meaning of  what flights fly from boston to denver , may be seen in figure 2.
</nextsent>
<nextsent>the semantic graph is not fully quantified formula; rather it may be thought of as form of predicate-argument representation, with quan- tifiers in place, from which fully quantified formula can be generated.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A36">
<title id=" A94-1022.xml">the delphi natural language understanding system </title>
<section> grammar  and the  syntax /semant ics.  </section>
<citcontext>
<prevsection>
<prevsent>and sometimes the omission of an argument can have anaphoric on sequences, as in: what restrictions apply?
</prevsent>
<prevsent>which cannot be felicitously uttered except in con-text where there is something in the discourse that restriction could  apply  to.
</prevsent>
</prevsection>
<citsent citstr=" C88-2121 ">
conventional approaches to subcategorization, such as definite clause grammar (pereira and warren, 1980), categorial grammar (ades and steedman, 1982), patr-ii (shieber, 1986), and lexicalized tag (schabes et al 1988) <papid> C88-2121 </papid>all deal with complementation by includ-ing in one form or another notion of  subcategoriza-tion frame  that specifies sequence of complement phrases and constraints on them.</citsent>
<aftsection>
<nextsent>handling all the pos-sible variations in complement distribution in such for-malisms inevitably leads to an explosion in the number of such frames, and correspondingly more difficult task in porting to new domain.
</nextsent>
<nextsent>in our approach, on the other hand, it becomes pos-sible to view subcategorization a lexical item as set of constraints on the outgoing arcs of its semantic graph node.
</nextsent>
<nextsent>different ypes of constraints - order of ar-guments, optionality of arguments, emantic-class con-straints and semantic effects of arguments - can all be represented separately, instead of enumerating all pos-sible argument sequences in set of alternative subcat-egorization frames.
</nextsent>
<nextsent>133 subcategorization constraints in delphi are encoded in lexical entries using structure called  map  (stal- lard and bobrow, 1991).<papid> H91-1042 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A37">
<title id=" A94-1022.xml">the delphi natural language understanding system </title>
<section> grammar  and the  syntax /semant ics.  </section>
<citcontext>
<prevsection>
<prevsent>in our approach, on the other hand, it becomes pos-sible to view subcategorization a lexical item as set of constraints on the outgoing arcs of its semantic graph node.
</prevsent>
<prevsent>different ypes of constraints - order of ar-guments, optionality of arguments, emantic-class con-straints and semantic effects of arguments - can all be represented separately, instead of enumerating all pos-sible argument sequences in set of alternative subcat-egorization frames.
</prevsent>
</prevsection>
<citsent citstr=" H91-1042 ">
133 subcategorization constraints in delphi are encoded in lexical entries using structure called  map  (stal- lard and bobrow, 1991).<papid> H91-1042 </papid></citsent>
<aftsection>
<nextsent>below is part of the lexical entry for  fly  in the atis domain: fly sub jec : fl ight-of to : dest-of from: orig-of complet ion: (and (f l led fl ight-of) (or (f l led dest-of) (f l led orig-of) ) map entries have  translation , realization  and  com-pletion  components.
</nextsent>
<nextsent>the translation part of this entry specifies that the lexical head  fly  is to correspond to semantic-graph node labeled with event-class fly.
</nextsent>
<nextsent>the realization part of the entry specifies what grammati-cal relations the lexical item takes, and what semantic relations these correspond to, or  realize , in these- mantic graph.
</nextsent>
<nextsent>here, the entry specifies that  fly  takes subject, to, and from complements, and that these grammatical relations correspond to the semantic rela-tions flight-of, dest-of, and orig-of respec-tively.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A39">
<title id=" A94-1022.xml">the delphi natural language understanding system </title>
<section> ill-formedness handling: the.  </section>
<citcontext>
<prevsection>
<prevsent>the semantic linker may also  hallucinate  new node to bridge two fragments between whom no links can otherwise be computed.
</prevsent>
<prevsent>for example, for the utter-ance  from boston to denver , which has no explicit flight-object, flight node can be inserted be-tween the fragments to make sense of the utterance.
</prevsent>
</prevsection>
<citsent citstr=" H92-1061 ">
because the semantic linker uses the same set of realization rules as the rest of the system, when the system is ported to new domain the semantic linker can be used immediately - distinct advantage over some other approaches to fallback understanding, such as (stallard and bobrow, 1992) <papid> H92-1061 </papid>or (jackson et al 1991).<papid> H91-1034 </papid></citsent>
<aftsection>
<nextsent>informal experiments (as we discuss subsequently) the semantic linker has been show to dramatically im- prove delphi performance.
</nextsent>
<nextsent>the quantifier scoping module in delphi takes se-mantic graph and produces fully-scoped expression in the logical language fmrl.
</nextsent>
<nextsent>the basic strategy for quantifier scoping is descendant of that used in the lunar system (woods et al 1978).
</nextsent>
<nextsent>this is made possible by the use of the semantic graph ascom- mon underlying representation for both the grammatical and ill-formod parts of fragmentary utterances.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A40">
<title id=" A94-1022.xml">the delphi natural language understanding system </title>
<section> ill-formedness handling: the.  </section>
<citcontext>
<prevsection>
<prevsent>the semantic linker may also  hallucinate  new node to bridge two fragments between whom no links can otherwise be computed.
</prevsent>
<prevsent>for example, for the utter-ance  from boston to denver , which has no explicit flight-object, flight node can be inserted be-tween the fragments to make sense of the utterance.
</prevsent>
</prevsection>
<citsent citstr=" H91-1034 ">
because the semantic linker uses the same set of realization rules as the rest of the system, when the system is ported to new domain the semantic linker can be used immediately - distinct advantage over some other approaches to fallback understanding, such as (stallard and bobrow, 1992) <papid> H92-1061 </papid>or (jackson et al 1991).<papid> H91-1034 </papid></citsent>
<aftsection>
<nextsent>informal experiments (as we discuss subsequently) the semantic linker has been show to dramatically im- prove delphi performance.
</nextsent>
<nextsent>the quantifier scoping module in delphi takes se-mantic graph and produces fully-scoped expression in the logical language fmrl.
</nextsent>
<nextsent>the basic strategy for quantifier scoping is descendant of that used in the lunar system (woods et al 1978).
</nextsent>
<nextsent>this is made possible by the use of the semantic graph ascom- mon underlying representation for both the grammatical and ill-formod parts of fragmentary utterances.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A41">
<title id=" C00-1012.xml">the effects of analysing cohesion on document summarisation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>indeed, this has been the basis for an operational definition of linear dis-course segmentation, where segments in document are defined to be contiguous blocks of text, roughly  about the same thing , with segment boundaries indicative of topic shifts.
</prevsent>
<prevsent>the research reported here is just one aspect of larger study into the recognition and use of cohesive devices for content characterisation tasks.
</prevsent>
</prevsection>
<citsent citstr=" C96-1021 ">
it presupposes fine- grained methods for the identification of cohesive ties 76 between (sentence) units in text; describing the com-putational basis for developing such methods is outside of the scope of this paper (howeveb see (kennedy and boguraev, 1996), (<papid> C96-1021 </papid>fellbaum, 1999), (kelleb 1994)), as is the complete framework for lexical cohesion analysis we have developed.</citsent>
<aftsection>
<nextsent>instead, in focusing on the effects of lex-ical cohesion on summarization, we limit ourselves here on the phenomenon of simple lexical repetition; it turns out that even this can be beneficially applied to enhanc-ing summarizatkm quality.
</nextsent>
<nextsent>recent work (barzilay and elhadad, 1999) makes ex-plicit this intuition.
</nextsent>
<nextsent> lexical chains  are constructed by grouping together items related by repetition and cer-tain lexical relations derived via the woi ,dnet lexical database (fellbaum, 1999).
</nextsent>
<nextsent>a sequence of items in chain highlights discussion focused on topic related to (an) ite, m(s) in the chain; metric for scoring chains picks top-ically prominent ones; these are then taken as the basis of sentence xtractkm heuristics.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A42">
<title id=" C00-1012.xml">the effects of analysing cohesion on document summarisation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this highlights the poten-tial of purely lexical chains-based appnmch; still, barzi-lay and elhadad remain frustrated by the high degree of polysemy in wordnet (not to mention its limited cov-erage with respect more specialized omains); fortu-nately, this does not concern us here.
</prevsent>
<prevsent>1.2 discourse segmentation and summarization.
</prevsent>
</prevsection>
<citsent citstr=" E99-1011 ">
unlike barzilay and elhadad, we start with sentence- based summarizeb and are specifically seeking to im- prove upon what is already (by some measure; see section 4.1 below) good performance, judged in discipline-wide evaluation initiative (mani et al, 1999).<papid> E99-1011 </papid></citsent>
<aftsection>
<nextsent>this places certain constraints on how lexical cohesion analysis results, and in particular the identification of topically coherent segments, can be incorporated in the existing strategies and nmchanisms h~r sentence selec-tion, already deployed by the summarizer.
</nextsent>
<nextsent>making cer-tain that summary incorporates sentences from each segment intuitively seeks to ensure uniform representa-tion of all sub-stories ina document; he notion here is to avoid having inordinately large gaps between adjacent summary sentences, which would tend to lose essential inhmnation.
</nextsent>
<nextsent>moreove,, mechanism which would pick the sentence(s) in segment most representative its main topic, would also carry over into the summary  traces  of all the main topics in the original document.
</nextsent>
<nextsent>this is more than just an intuition.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A44">
<title id=" C00-1012.xml">the effects of analysing cohesion on document summarisation </title>
<section> technology  base.  </section>
<citcontext>
<prevsection>
<prevsent>all of these elements both contribute directly to the summa- rizer set of heuristics, as well as inform the discourse segmentation process.
</prevsent>
<prevsent>2.2 sa ience-dr iven summar ization.
</prevsent>
</prevsection>
<citsent citstr=" W97-0711 ">
with its set of linguistic filters, our frequency-based sum- marizer can exploit linguistic dimensions beyond single word analysis; this is not unlike the approach of (aone et al, 1997).<papid> W97-0711 </papid></citsent>
<aftsection>
<nextsent>due to the sophistication and integration of the filters (see section 2.1), we are able to exploit richer source of domain knowledge than most other frequency- based systems.
</nextsent>
<nextsent>frequency alone is poor indicator of salience, even when ignoring stop words.
</nextsent>
<nextsent>unlike early frequency-based techniques for sentence selection, we utilize the more indicative inverse document frequency measure, adapted from information retrieval, in which the relative fre-quency of an item in document is compared with its rel-ative frequency in background collection.
</nextsent>
<nextsent>the trade-off, however, for more precise term salience is the summa- rizer dependence on background collection statistics; we return to this issue below.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A45">
<title id=" C00-1012.xml">the effects of analysing cohesion on document summarisation </title>
<section> technology  base.  </section>
<citcontext>
<prevsection>
<prevsent>segmentation is document analysis function which di-rectly exploits one of tile core text cohesion factors, pat- terras of xicai repetition (see section 1.1), for  identifying some baseline data concerning tile distribution of topics in text.
</prevsent>
<prevsent>in particulal, discourse segmentation is driven by tile determination points in the narrative where per-ceptible discontinuities in the text cohesion are detected.
</prevsent>
</prevsection>
<citsent citstr=" J91-1002 ">
such discontinuities are indicative of topic shifts, t%llow- ing the original idea of lexical chains (morris and lirst, 1991), <papid> J91-1002 </papid>subsequently developed specifically for the pur-poses of segmentation expository text (hearst, 1994), <papid> P94-1002 </papid>we have adapted an algorithm for discourse segmenta-tion to our document processing environment.</citsent>
<aftsection>
<nextsent>in par- ticulab while remaining sensitive to tile distribution of  terms  across tile docunlent, and calculating similarity between adjacent ext blocks by cosine measure, our procedure differs from that in (hearst, 1994) <papid> P94-1002 </papid>in several ways.</nextsent>
<nextsent>we only take into account content words (as opposed to all terms yielded by tnkenizatkm step).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A46">
<title id=" C00-1012.xml">the effects of analysing cohesion on document summarisation </title>
<section> technology  base.  </section>
<citcontext>
<prevsection>
<prevsent>segmentation is document analysis function which di-rectly exploits one of tile core text cohesion factors, pat- terras of xicai repetition (see section 1.1), for  identifying some baseline data concerning tile distribution of topics in text.
</prevsent>
<prevsent>in particulal, discourse segmentation is driven by tile determination points in the narrative where per-ceptible discontinuities in the text cohesion are detected.
</prevsent>
</prevsection>
<citsent citstr=" P94-1002 ">
such discontinuities are indicative of topic shifts, t%llow- ing the original idea of lexical chains (morris and lirst, 1991), <papid> J91-1002 </papid>subsequently developed specifically for the pur-poses of segmentation expository text (hearst, 1994), <papid> P94-1002 </papid>we have adapted an algorithm for discourse segmenta-tion to our document processing environment.</citsent>
<aftsection>
<nextsent>in par- ticulab while remaining sensitive to tile distribution of  terms  across tile docunlent, and calculating similarity between adjacent ext blocks by cosine measure, our procedure differs from that in (hearst, 1994) <papid> P94-1002 </papid>in several ways.</nextsent>
<nextsent>we only take into account content words (as opposed to all terms yielded by tnkenizatkm step).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A48">
<title id=" C00-1012.xml">the effects of analysing cohesion on document summarisation </title>
<section> technology  base.  </section>
<citcontext>
<prevsection>
<prevsent>the function is also adjusted to reduce the noise from block comparisons where the block bnundary--and thus potential topic shift--falls at un-natural break points (such as tile middle of sentence).
</prevsent>
<prevsent>by making segmentation other component within our document processing environment, we are able to use, transparently, the results of processes such as lexical mm morl;hohnical lookup, docmnent structure identification, and cue i#trase detection.
</prevsent>
</prevsection>
<citsent citstr=" W99-0301 ">
likewise, segmentation results are naturally incorporated in an annotation superstruc-ture which records the various levels of document anal- ysis: discourse segments are just another type of  span  (annotation) over: number of sentences, logically akin to paragraph (bird and liberman, 1999).<papid> W99-0301 </papid></citsent>
<aftsection>
<nextsent>apart from the adjustments and modifications out-lined above, we use essentially tearst formnla for com-puting lexical similarity between adjacent blocks of text bland b2 (t denotes discourse lement term identified as such by prior processing, ranging over tim text span of the currently analyzed block; wt,l,n is the normalized frequency of occurrence of the term in block b~\,): sim(bl,b2) :~   twl,btwt,b~ unlike most applications ofsegmentatkm to date, which are concerned with the identification of segment bound-aries, we are primarily interested ira ieveraging the con-tent of the segments, to the extent hat it is indicative of the focus of attention, and (indirectl3; at least) points at tile topical shifts to be utilized for surnmary genera-tion.
</nextsent>
<nextsent>we use tile segmentation results (together with the name and term identificatkm and salience calculation de-livered by other functions) in order to ensure that all the base data for inferring the topic stamps, and topic shifts, ix available to the user.
</nextsent>
<nextsent>what is tile relations ldp between segmentation d sum- marizatkm: is segmentation strictly  under the covers  function for tile summarizer, or might segmentation re- suits be of any interest, and use, to tile end nser?
</nextsent>
<nextsent>we foclis ()l 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A49">
<title id=" C00-1012.xml">the effects of analysing cohesion on document summarisation </title>
<section> segmentation-assisted summaries.  </section>
<citcontext>
<prevsection>
<prevsent>for longer documents, with requisite longer summaries, the notion of salience degenerates, and the summary be-comes just an incoherent collection of sentences.
</prevsent>
<prevsent>(even if paragraphs, rather than sentences, are used to com-pose the summary--see .g.
</prevsent>
</prevsection>
<citsent citstr=" W97-0707 ">
(mitra et al, 1997)--<papid> W97-0707 </papid>the same problems of coherence degradation and topical under-representation, remain.)</citsent>
<aftsection>
<nextsent>we use segmentation to iden-tify contiguous ub-stories in long documents, which are then individually passed on to the summarizer; there- sults of sub-story summaries are  glued  together.
</nextsent>
<nextsent>for evaluating the effect of various strategies upon sum- marizer output quality, we used as baseline an evalua-tion corpus of full-length articles and their  digests , from the new york times.
</nextsent>
<nextsent>there are advantages, and disadvan-tages, to this approach.
</nextsent>
<nextsent>setting aside whether task-based evaluation is appropriate for testing strictly the effect of one technology on another (see section 4.1 below), such decision ties us to particular set of data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A52">
<title id=" C00-2126.xml">word order acquisition from corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>here  has(h,x)  is binary flmction which returns true if the history has feature z. we focus on the attributes of bunsetsu itself and on the features occurring between bunsetsus.
</prevsent>
<prevsent>given set of features and some training data, the maximum entropy estimation process produces model ill which every feature .qi has associated with it parameter ai.
</prevsent>
</prevsection>
<citsent citstr=" J96-1002 ">
this allows us to compute the con-ditional probability as follows (berger et al, 1996): <papid> J96-1002 </papid>ag~ (h .f) p( / ih ) - 1l   (2) (h) ct .</citsent>
<aftsection>
<nextsent>(3) i the maximum entropy estimation technique guaran-tees that for every feature gi, the expected value of gi according to the m.e. model will equal the empir-ical expectation of gi in the training corpus.
</nextsent>
<nextsent>in other words: p(h,/).
</nextsent>
<nextsent>mh, f) h,f = (4) / here /5 is an empirical probability and l~ ie is the 872 table l: example of estimating the probabilities of word orders.
</nextsent>
<nextsent>) i~{h (yesterday) / ?=x~ (tennis) / ~(f~l~l$ (taro) / blo ( ) t~yed.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A53">
<title id=" A00-2011.xml">wordforword glossing with contextually similar words </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for example, in cross- language information retrieval, glossing document often provides sufficient ranslation for humans to comprehend the key concepts.
</prevsent>
<prevsent>furthermore, glossing algorithm can be used for lexical selection in full-fledged machine translation (mt) system.
</prevsent>
</prevsection>
<citsent citstr=" J90-2002 ">
many corpus-based mt systems require parallel corpora (brown et al, 1990; <papid> J90-2002 </papid>brown et al., 1991; <papid> P91-1022 </papid>gale and church, 1991; <papid> P91-1023 </papid>resnik, 1999).<papid> P99-1068 </papid></citsent>
<aftsection>
<nextsent>kikui (1999) <papid> W99-0905 </papid>used word sense disambiguation algorithm and non-paralm bilingual corpus to resolve translation ambiguity.</nextsent>
<nextsent>in this paper, we present word-for-word glossing algorithm that requires only source language corpus.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A54">
<title id=" A00-2011.xml">wordforword glossing with contextually similar words </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for example, in cross- language information retrieval, glossing document often provides sufficient ranslation for humans to comprehend the key concepts.
</prevsent>
<prevsent>furthermore, glossing algorithm can be used for lexical selection in full-fledged machine translation (mt) system.
</prevsent>
</prevsection>
<citsent citstr=" P91-1022 ">
many corpus-based mt systems require parallel corpora (brown et al, 1990; <papid> J90-2002 </papid>brown et al., 1991; <papid> P91-1022 </papid>gale and church, 1991; <papid> P91-1023 </papid>resnik, 1999).<papid> P99-1068 </papid></citsent>
<aftsection>
<nextsent>kikui (1999) <papid> W99-0905 </papid>used word sense disambiguation algorithm and non-paralm bilingual corpus to resolve translation ambiguity.</nextsent>
<nextsent>in this paper, we present word-for-word glossing algorithm that requires only source language corpus.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A55">
<title id=" A00-2011.xml">wordforword glossing with contextually similar words </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for example, in cross- language information retrieval, glossing document often provides sufficient ranslation for humans to comprehend the key concepts.
</prevsent>
<prevsent>furthermore, glossing algorithm can be used for lexical selection in full-fledged machine translation (mt) system.
</prevsent>
</prevsection>
<citsent citstr=" P91-1023 ">
many corpus-based mt systems require parallel corpora (brown et al, 1990; <papid> J90-2002 </papid>brown et al., 1991; <papid> P91-1022 </papid>gale and church, 1991; <papid> P91-1023 </papid>resnik, 1999).<papid> P99-1068 </papid></citsent>
<aftsection>
<nextsent>kikui (1999) <papid> W99-0905 </papid>used word sense disambiguation algorithm and non-paralm bilingual corpus to resolve translation ambiguity.</nextsent>
<nextsent>in this paper, we present word-for-word glossing algorithm that requires only source language corpus.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A56">
<title id=" A00-2011.xml">wordforword glossing with contextually similar words </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for example, in cross- language information retrieval, glossing document often provides sufficient ranslation for humans to comprehend the key concepts.
</prevsent>
<prevsent>furthermore, glossing algorithm can be used for lexical selection in full-fledged machine translation (mt) system.
</prevsent>
</prevsection>
<citsent citstr=" P99-1068 ">
many corpus-based mt systems require parallel corpora (brown et al, 1990; <papid> J90-2002 </papid>brown et al., 1991; <papid> P91-1022 </papid>gale and church, 1991; <papid> P91-1023 </papid>resnik, 1999).<papid> P99-1068 </papid></citsent>
<aftsection>
<nextsent>kikui (1999) <papid> W99-0905 </papid>used word sense disambiguation algorithm and non-paralm bilingual corpus to resolve translation ambiguity.</nextsent>
<nextsent>in this paper, we present word-for-word glossing algorithm that requires only source language corpus.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A57">
<title id=" A00-2011.xml">wordforword glossing with contextually similar words </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>furthermore, glossing algorithm can be used for lexical selection in full-fledged machine translation (mt) system.
</prevsent>
<prevsent>many corpus-based mt systems require parallel corpora (brown et al, 1990; <papid> J90-2002 </papid>brown et al., 1991; <papid> P91-1022 </papid>gale and church, 1991; <papid> P91-1023 </papid>resnik, 1999).<papid> P99-1068 </papid></prevsent>
</prevsection>
<citsent citstr=" W99-0905 ">
kikui (1999) <papid> W99-0905 </papid>used word sense disambiguation algorithm and non-paralm bilingual corpus to resolve translation ambiguity.</citsent>
<aftsection>
<nextsent>in this paper, we present word-for-word glossing algorithm that requires only source language corpus.
</nextsent>
<nextsent>the intuitive idea behind our algorithm is the following.
</nextsent>
<nextsent>suppose is word to be translated.
</nextsent>
<nextsent>we first identify set of words similar to that occurred in the same context as in large corpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A58">
<title id=" A00-2011.xml">wordforword glossing with contextually similar words </title>
<section> resources.  </section>
<citcontext>
<prevsection>
<prevsent>section 4 presents the word-for-word glossing algorithm and section 5 describes the group similarity metric used in our algorithm.
</prevsent>
<prevsent>in section 6, we present some experimental results and finally, in section 7, we conclude with discussion of future work.
</prevsent>
</prevsection>
<citsent citstr=" P98-2127 ">
the input to our algorithm includes collocation database (lin, 1998<papid> P98-2127 </papid>b) and corpus-based thesaurus (lin, 1998<papid> P98-2127 </papid>a), which are both available on the interne0.</citsent>
<aftsection>
<nextsent>in addition, we require bilingual thesaurus.
</nextsent>
<nextsent>below, we briefly describe these resources.
</nextsent>
<nextsent>2.1.
</nextsent>
<nextsent>collocation database.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A74">
<title id=" A94-1018.xml">yet another chart based technique for parsing illformed input </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this would be useful for handling erroneous inputs from the user and for offsetting grammar and lexicon insufficiency.
</prevsent>
<prevsent>also, such capability could be applied to deal with the ungrammatical sentences and sentence fragments that frequently appear in spoken dialogs (bear, dowding and shriberg, 1992).
</prevsent>
</prevsection>
<citsent citstr=" C88-1075 ">
several efforts have been conducted to achieve this objective ((lang, 1988; <papid> C88-1075 </papid>saito and tomita, 1988), <papid> C88-2118 </papid>for example.)</citsent>
<aftsection>
<nextsent>one major decision to be made in designing this capability is whether knowledge other than purely syntactic knowledge is to be used.
</nextsent>
<nextsent>other- than syntactic knowledge includes grammar specific recovery rules such as recta-rules (weishedel and sondheimer, 1983), semantic or pragmatic knowledge which may depend on particular domain (carbonell and hayes, 1983) or the characteristics of the ill-formed utterances observed in human discourse (hindle, 1983).<papid> P83-1019 </papid></nextsent>
<nextsent>although it is obvious that the utilizing such knowledge allows us to devise more powerful strategies, we should first determine the effectiveness of using only syntactic knowledge.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A75">
<title id=" A94-1018.xml">yet another chart based technique for parsing illformed input </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this would be useful for handling erroneous inputs from the user and for offsetting grammar and lexicon insufficiency.
</prevsent>
<prevsent>also, such capability could be applied to deal with the ungrammatical sentences and sentence fragments that frequently appear in spoken dialogs (bear, dowding and shriberg, 1992).
</prevsent>
</prevsection>
<citsent citstr=" C88-2118 ">
several efforts have been conducted to achieve this objective ((lang, 1988; <papid> C88-1075 </papid>saito and tomita, 1988), <papid> C88-2118 </papid>for example.)</citsent>
<aftsection>
<nextsent>one major decision to be made in designing this capability is whether knowledge other than purely syntactic knowledge is to be used.
</nextsent>
<nextsent>other- than syntactic knowledge includes grammar specific recovery rules such as recta-rules (weishedel and sondheimer, 1983), semantic or pragmatic knowledge which may depend on particular domain (carbonell and hayes, 1983) or the characteristics of the ill-formed utterances observed in human discourse (hindle, 1983).<papid> P83-1019 </papid></nextsent>
<nextsent>although it is obvious that the utilizing such knowledge allows us to devise more powerful strategies, we should first determine the effectiveness of using only syntactic knowledge.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A76">
<title id=" A94-1018.xml">yet another chart based technique for parsing illformed input </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>several efforts have been conducted to achieve this objective ((lang, 1988; <papid> C88-1075 </papid>saito and tomita, 1988), <papid> C88-2118 </papid>for example.)</prevsent>
<prevsent>one major decision to be made in designing this capability is whether knowledge other than purely syntactic knowledge is to be used.</prevsent>
</prevsection>
<citsent citstr=" P83-1019 ">
other- than syntactic knowledge includes grammar specific recovery rules such as recta-rules (weishedel and sondheimer, 1983), semantic or pragmatic knowledge which may depend on particular domain (carbonell and hayes, 1983) or the characteristics of the ill-formed utterances observed in human discourse (hindle, 1983).<papid> P83-1019 </papid></citsent>
<aftsection>
<nextsent>although it is obvious that the utilizing such knowledge allows us to devise more powerful strategies, we should first determine the effectiveness of using only syntactic knowledge.
</nextsent>
<nextsent>moreover, the result can be applied widely, as using syntactic knowledge is base of the most of strategies.
</nextsent>
<nextsent>one significant advance in the usage of syntactic knowledge was contained in the technique proposed by mellish (1989).<papid> P89-1013 </papid></nextsent>
<nextsent>it can handle not only unknown/misspelled words, but also omitted words and extraneous words in sentences.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A77">
<title id=" A94-1018.xml">yet another chart based technique for parsing illformed input </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>although it is obvious that the utilizing such knowledge allows us to devise more powerful strategies, we should first determine the effectiveness of using only syntactic knowledge.
</prevsent>
<prevsent>moreover, the result can be applied widely, as using syntactic knowledge is base of the most of strategies.
</prevsent>
</prevsection>
<citsent citstr=" P89-1013 ">
one significant advance in the usage of syntactic knowledge was contained in the technique proposed by mellish (1989).<papid> P89-1013 </papid></citsent>
<aftsection>
<nextsent>it can handle not only unknown/misspelled words, but also omitted words and extraneous words in sentences.
</nextsent>
<nextsent>it can deal with such problems, and develop plausible explanations quickly since it utilizes the full syntactic ontext by using an active chart parser (kay, 1980; gazdar and mellish, 1989).<papid> P89-1013 </papid></nextsent>
<nextsent>one problem with his technique is that its performance heavily depends on how the search heuristics, which is implemented as score calculated from six parameters, set.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A79">
<title id=" A94-1018.xml">yet another chart based technique for parsing illformed input </title>
<section> proposed algorithm.  </section>
<citcontext>
<prevsection>
<prevsent>since parsing proceeds left to fight, an active edge is generated only when an error point exists to the fight of the found constituents.
</prevsent>
<prevsent>in the first phase, bi-directional bottom-up arsing generates all generalized edges that represent unsatisfied expectations to the right and left of constituents.
</prevsent>
</prevsection>
<citsent citstr=" P92-1006 ">
from some perspectives, the role this phase plays is similar to that of the covered bi-directional phase of the picky parser (magerman and weir, 1992), <papid> P92-1006 </papid>though the method proposed herein does not employ stochastic information at all.</citsent>
<aftsection>
<nextsent>this process can be described in three rules as shown in figure 1.
</nextsent>
<nextsent>as can be seen, this is bi-directional bottom-up arsing that uses generalized edges as the data structure.
</nextsent>
<nextsent>for simplicity, the details for avoiding duplicated edge generation have been omitted.
</nextsent>
<nextsent>it is worth noting that after this process, the needs listed in each generalized edge indicate that the expected constituents did not exist, while, before this process, need may exist just because an expectation has not been checked.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A80">
<title id=" A97-1019.xml">the nlp role in animated conversation for call </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the 1988 trieste conference on foreign language intelligent tutoring systems did not manage proceedings, few were the functioning systems using nlp and other techniques of artificial intelligence (ai).
</prevsent>
<prevsent>indeed the field of computer-assisted language learning (call) has been dominated by the work of language teachers who, tired of waiting for us, have proceeded to learn some programming orhire computer science students to make their own systems, most of them just for fairly inflexible drill-and-practice.
</prevsent>
</prevsection>
<citsent citstr=" C96-2171 ">
the 1990s have witnessed increasing interest in bringing ai to language learning systems, reflected in chanier (1994) special issue, holland anthology (1995) and zock (1996) <papid> C96-2171 </papid>panel.</citsent>
<aftsection>
<nextsent>the ai is principally of two kinds: nlp and what we will call itle, the overlapping categories of intelligent tutoring systems and interactive arning environments.
</nextsent>
<nextsent>in this paper, we report on foreign language itle that has moved from the lab into the classroom.
</nextsent>
<nextsent>after noting alternative strategies for using nlp in call (section 2), we describe our system and its nlp require-ments (section 3), as well as its pedagogical foundations (section 4).
</nextsent>
<nextsent>finally, we describe its graduation to the classroom, for use with particularly challenging kind of learner: the highly motivated but computer- unfamiliar immigrant.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A81">
<title id=" A97-1019.xml">the nlp role in animated conversation for call </title>
<section> fluent and nlp.  </section>
<citcontext>
<prevsection>
<prevsent>flexibility, extendibility and teacher involvement are achieved influent through its layered architecture, shown in figure 2.
</prevsent>
<prevsent>in this framework, independent processes work on declarative structures that have been acquired by the system through tools that do not demand knowledge of programming or linguistic theory (schoelles and hamburger, 1996).
</prevsent>
</prevsection>
<citsent citstr=" A94-1001 ">
this work shares the interface orientation of caldwell and korelsky (1994), <papid> A94-1001 </papid>but is more abstract and flexible in that user- specifications are independent of, and combinable with, domain plans.</citsent>
<aftsection>
<nextsent>on the other hand we do not follow work like traum and allen (1994), <papid> P94-1001 </papid>in pushing toward computation about more and more discourse phenomena the outer ring of figure 2 depicts the knowledge acquisition level.</nextsent>
<nextsent>it identifies existing tools for building tutorial schemas, language usage structures and the graphics of objects.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A82">
<title id=" A97-1019.xml">the nlp role in animated conversation for call </title>
<section> fluent and nlp.  </section>
<citcontext>
<prevsection>
<prevsent>in this framework, independent processes work on declarative structures that have been acquired by the system through tools that do not demand knowledge of programming or linguistic theory (schoelles and hamburger, 1996).
</prevsent>
<prevsent>this work shares the interface orientation of caldwell and korelsky (1994), <papid> A94-1001 </papid>but is more abstract and flexible in that user- specifications are independent of, and combinable with, domain plans.</prevsent>
</prevsection>
<citsent citstr=" P94-1001 ">
on the other hand we do not follow work like traum and allen (1994), <papid> P94-1001 </papid>in pushing toward computation about more and more discourse phenomena the outer ring of figure 2 depicts the knowledge acquisition level.</citsent>
<aftsection>
<nextsent>it identifies existing tools for building tutorial schemas, language usage structures and the graphics of objects.
</nextsent>
<nextsent>the tutorial schema tool lets teacher express pedagogical expertise in the form of lessons.
</nextsent>
<nextsent>the view tool allows the teacher some degree of control over the language generated by the system.
</nextsent>
<nextsent>the teacher inputs the language specifications for lesson by manipulating graphical user interface.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A83">
<title id=" A94-1017.xml">real time spoken language translation using associative processors </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this pa-per focuses on the second process, spoken language translation, which requires (1) an accurate trans-lation and (2) real-time response.
</prevsent>
<prevsent>we have al-ready proposed model that utilizes examples and translates sentence by combining pieces of trans-fer knowledge, i.e., target language xpressions that correspond to source language xpressions that cover the sentence jointly.
</prevsent>
</prevsection>
<citsent citstr=" C92-2097 ">
the model is called transfer- driven machine translation (tdmt) (furuse and iida, 1992; <papid> C92-2097 </papid>furuse et al, 1994) (see subsection 2.1 for details).</citsent>
<aftsection>
<nextsent>a prototype system of tdmt which trans-lates japanese spoken sentence into english, has performed accurate structural disambiguation and target word selection 1.
</nextsent>
<nextsent>this paper will focus on the second requirement.
</nextsent>
<nextsent>first, we will outline tdmt and analyze its com-putational cost.
</nextsent>
<nextsent>second, we will describe the con-figuration, experimental results and scala bility of tdmt on associative processors (aps).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A84">
<title id=" C00-1011.xml">parsing with the shortest derivation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this paper also contains the first publislmd experiments with dop on the wsj.
</prevsent>
<prevsent>a well-known property of stochastic grammars is their prope,lsity to assign highe, pro babil ities to shorter derivations o1  sentence (cf.
</prevsent>
</prevsection>
<citsent citstr=" H90-1053 ">
chitrao &amp; grishman 1990; <papid> H90-1053 </papid>magerman &amp; marcus 1991; <papid> E91-1004 </papid>briscoe &amp; carroll 1993; <papid> J93-1002 </papid>charniak 1996).</citsent>
<aftsection>
<nextsent>this propensity is due to the probability o1  derivation being computed as tile product of the rule pro babil ities, and thus shorter derivations involving fewer rules tend to have higher probabilities, ahnost regardless of the training data.
</nextsent>
<nextsent>while this bias may seem interesting in the light of the principle of cognitive economy, shorter derivat-ions generate smaller parse ees (consisting of fewer nodes) whiclt are not warranted by tile correct parses of sentences.
</nextsent>
<nextsent>most systems therefore redress this bias, for instance by normalizillg the derivation probability (see caraballo &amp; charniak 1998).<papid> J98-2004 </papid></nextsent>
<nextsent>however, for stochastic grammars lhat use elementary trees instead o1  context-l ree rules, the propensity to assign higher pro babil ities to shorter derivations does not necessarily lead tobias in favor of smaller parse trees, because lementary trees may differ in size and lexicalization.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A85">
<title id=" C00-1011.xml">parsing with the shortest derivation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this paper also contains the first publislmd experiments with dop on the wsj.
</prevsent>
<prevsent>a well-known property of stochastic grammars is their prope,lsity to assign highe, pro babil ities to shorter derivations o1  sentence (cf.
</prevsent>
</prevsection>
<citsent citstr=" E91-1004 ">
chitrao &amp; grishman 1990; <papid> H90-1053 </papid>magerman &amp; marcus 1991; <papid> E91-1004 </papid>briscoe &amp; carroll 1993; <papid> J93-1002 </papid>charniak 1996).</citsent>
<aftsection>
<nextsent>this propensity is due to the probability o1  derivation being computed as tile product of the rule pro babil ities, and thus shorter derivations involving fewer rules tend to have higher probabilities, ahnost regardless of the training data.
</nextsent>
<nextsent>while this bias may seem interesting in the light of the principle of cognitive economy, shorter derivat-ions generate smaller parse ees (consisting of fewer nodes) whiclt are not warranted by tile correct parses of sentences.
</nextsent>
<nextsent>most systems therefore redress this bias, for instance by normalizillg the derivation probability (see caraballo &amp; charniak 1998).<papid> J98-2004 </papid></nextsent>
<nextsent>however, for stochastic grammars lhat use elementary trees instead o1  context-l ree rules, the propensity to assign higher pro babil ities to shorter derivations does not necessarily lead tobias in favor of smaller parse trees, because lementary trees may differ in size and lexicalization.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A86">
<title id=" C00-1011.xml">parsing with the shortest derivation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this paper also contains the first publislmd experiments with dop on the wsj.
</prevsent>
<prevsent>a well-known property of stochastic grammars is their prope,lsity to assign highe, pro babil ities to shorter derivations o1  sentence (cf.
</prevsent>
</prevsection>
<citsent citstr=" J93-1002 ">
chitrao &amp; grishman 1990; <papid> H90-1053 </papid>magerman &amp; marcus 1991; <papid> E91-1004 </papid>briscoe &amp; carroll 1993; <papid> J93-1002 </papid>charniak 1996).</citsent>
<aftsection>
<nextsent>this propensity is due to the probability o1  derivation being computed as tile product of the rule pro babil ities, and thus shorter derivations involving fewer rules tend to have higher probabilities, ahnost regardless of the training data.
</nextsent>
<nextsent>while this bias may seem interesting in the light of the principle of cognitive economy, shorter derivat-ions generate smaller parse ees (consisting of fewer nodes) whiclt are not warranted by tile correct parses of sentences.
</nextsent>
<nextsent>most systems therefore redress this bias, for instance by normalizillg the derivation probability (see caraballo &amp; charniak 1998).<papid> J98-2004 </papid></nextsent>
<nextsent>however, for stochastic grammars lhat use elementary trees instead o1  context-l ree rules, the propensity to assign higher pro babil ities to shorter derivations does not necessarily lead tobias in favor of smaller parse trees, because lementary trees may differ in size and lexicalization.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A87">
<title id=" C00-1011.xml">parsing with the shortest derivation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this propensity is due to the probability o1  derivation being computed as tile product of the rule pro babil ities, and thus shorter derivations involving fewer rules tend to have higher probabilities, ahnost regardless of the training data.
</prevsent>
<prevsent>while this bias may seem interesting in the light of the principle of cognitive economy, shorter derivat-ions generate smaller parse ees (consisting of fewer nodes) whiclt are not warranted by tile correct parses of sentences.
</prevsent>
</prevsection>
<citsent citstr=" J98-2004 ">
most systems therefore redress this bias, for instance by normalizillg the derivation probability (see caraballo &amp; charniak 1998).<papid> J98-2004 </papid></citsent>
<aftsection>
<nextsent>however, for stochastic grammars lhat use elementary trees instead o1  context-l ree rules, the propensity to assign higher pro babil ities to shorter derivations does not necessarily lead tobias in favor of smaller parse trees, because lementary trees may differ in size and lexicalization.
</nextsent>
<nextsent>for stochastic tree-substitution grammars (stsg) used by data- oriented parsing (dop) models, it has been observed lhat the shortest derivation of sentence consists of the largest subtrees een in treebank thai generate that sentence (of.
</nextsent>
<nextsent>bed 1992, 98).
</nextsent>
<nextsent>we may therefore wonder whether for stsg lhe bias in favor of shorter derivations is perhaps beneficial rather than llarmful.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A88">
<title id=" C00-1011.xml">parsing with the shortest derivation </title>
<section> computational aspects.  </section>
<citcontext>
<prevsection>
<prevsent>although goodman rcductkm method does still not allow for an eff ic ient computation {51 tile most probable parse in dop (ill fact, the prol~lem of computing the most prolmble parse is np-hard -- sue sima an 1996), his method does allow for an efficient computation o1  the  nmximun~ constituents parse , i.e., the parse tree that is most likely to have the largest number of correct constitueuts (also called the  labeled recall parse ).
</prevsent>
<prevsent>goodman has shown on tile atis corpus that the nla.xinltllll constituents parse perfor,ns at least as well as lhe most probable parse if all subtl ees are used.
</prevsent>
</prevsection>
<citsent citstr=" P97-1021 ">
unfortunately, goodman reduction method remains 71 beneficial only if indeed all treebank subtrces arc used (see sima an 1999: 108), while maximum parse accuracy is typically obtained with snbtree set which is smalle,  than the total set of subtrees (this is probably due to data-sparseness effects -- see bonnema et al 1997; <papid> P97-1021 </papid>bod 1998; sima an 1999).</citsent>
<aftsection>
<nextsent>in this paper we will use bod subtree-to-rule conversion method for studying the behavior of probabi ist ic against non-probabi ist ic dop for different maximtnn subtree sizes.
</nextsent>
<nextsent>however, we will not use bod monte carlo sampling technique from complete derivation forests, as this turns out to be computationally impractical for our larger corpora.
</nextsent>
<nextsent>instead, we use viterbi n-best search and estimate the most probable parse fi mn the 1,000 most probable deriwltions, summing up tile probabilities hi  derivat-ions that generate the same tree.
</nextsent>
<nextsent>tile algorithm for computing most probable deriwttions follows straightforwardly from the algorithm which computes the most probable derivation by means of viterbi optimization (see sima an 1995, 1999).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A89">
<title id=" C00-1011.xml">parsing with the shortest derivation </title>
<section> experimental comparison.  </section>
<citcontext>
<prevsection>
<prevsent>in order to extend ()tit  results to broad- coverage domain, we tested tile two models also on tile wall street journal portion in the penn treebank (marcus et ill. 1993).
</prevsent>
<prevsent>to make our results comparable to ()tilers, we did not test on different random splits but used the now slandard division of the wsj with sec lions 2-21 for training (approx.
</prevsent>
</prevsection>
<citsent citstr=" P97-1003 ">
40,000 sentences) and section 23 for testing (see collins 1997, <papid> P97-1003 </papid>1999; charniak 1997, 2000; l~,atnalmrkhi 1999); we only tested on sentences _  40 words (2245 sentences).</citsent>
<aftsection>
<nextsent>all trees were stripped off their selllalltic lags, co-reference information and quotation marks.
</nextsent>
<nextsent>we used all training set sublrees o1  depth 1, but due to memory limitations we used subset of the subtrees larger than depth by taking for each depth random sample o1  400,000 subtrecs.
</nextsent>
<nextsent>no subtrces larger than depth 14 were used.
</nextsent>
<nextsent>this resulted into total set of 5,217,529 subtrees which were smoothed by good-turing (see bod 1996).<papid> W96-0111 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A92">
<title id=" C00-1011.xml">parsing with the shortest derivation </title>
<section> experimental comparison.  </section>
<citcontext>
<prevsection>
<prevsent>we used all training set sublrees o1  depth 1, but due to memory limitations we used subset of the subtrees larger than depth by taking for each depth random sample o1  400,000 subtrecs.
</prevsent>
<prevsent>no subtrces larger than depth 14 were used.
</prevsent>
</prevsection>
<citsent citstr=" W96-0111 ">
this resulted into total set of 5,217,529 subtrees which were smoothed by good-turing (see bod 1996).<papid> W96-0111 </papid></citsent>
<aftsection>
<nextsent>we did not employ separate part-of-speech tagger: tile test sentences were directly parsed by the training set subtrees.
</nextsent>
<nextsent>for words that were unknown in tile training set, we guessed their categories by means of the method described in weischedel et al (1993) <papid> J93-2006 </papid>which uses statistics on word-endings, hyphenation and capital izat ion.</nextsent>
<nextsent>the guessed category for each llllkllown wol was converted into depth-i subtree and assigned probability (or frequency for non- probabilistic i)op) by means of simple good-turing.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A93">
<title id=" C00-1011.xml">parsing with the shortest derivation </title>
<section> experimental comparison.  </section>
<citcontext>
<prevsection>
<prevsent>this resulted into total set of 5,217,529 subtrees which were smoothed by good-turing (see bod 1996).<papid> W96-0111 </papid></prevsent>
<prevsent>we did not employ separate part-of-speech tagger: tile test sentences were directly parsed by the training set subtrees.</prevsent>
</prevsection>
<citsent citstr=" J93-2006 ">
for words that were unknown in tile training set, we guessed their categories by means of the method described in weischedel et al (1993) <papid> J93-2006 </papid>which uses statistics on word-endings, hyphenation and capital izat ion.</citsent>
<aftsection>
<nextsent>the guessed category for each llllkllown wol was converted into depth-i subtree and assigned probability (or frequency for non- probabilistic i)op) by means of simple good-turing.
</nextsent>
<nextsent>as accuracy metric we used the standard pap, sevai, scores (black et al 1991) <papid> H91-1060 </papid>to compare proposed parse 1  with tile corresponding correct treebank parse 7  as follows: # correct constituents in l.abcled precision - # constilucnts in 1  # coi cci.</nextsent>
<nextsent>collstittlcnts ill ~ labeled recall = # constituents in a constituent in is  correct  if there exisls constituent in 7  of tile sanle label that spans the same words.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A94">
<title id=" C00-1011.xml">parsing with the shortest derivation </title>
<section> experimental comparison.  </section>
<citcontext>
<prevsection>
<prevsent>for words that were unknown in tile training set, we guessed their categories by means of the method described in weischedel et al (1993) <papid> J93-2006 </papid>which uses statistics on word-endings, hyphenation and capital izat ion.</prevsent>
<prevsent>the guessed category for each llllkllown wol was converted into depth-i subtree and assigned probability (or frequency for non- probabilistic i)op) by means of simple good-turing.</prevsent>
</prevsection>
<citsent citstr=" H91-1060 ">
as accuracy metric we used the standard pap, sevai, scores (black et al 1991) <papid> H91-1060 </papid>to compare proposed parse 1  with tile corresponding correct treebank parse 7  as follows: # correct constituents in l.abcled precision - # constilucnts in 1  # coi cci.</citsent>
<aftsection>
<nextsent>collstittlcnts ill ~ labeled recall = # constituents in a constituent in is  correct  if there exisls constituent in 7  of tile sanle label that spans the same words.
</nextsent>
<nextsent>as in other work, we collapsed ai)vp and pljl  to the same label when calculating these scores (see collins 1997; <papid> P97-1003 </papid>i~,atnaparkhi 1999; charniak 1997).</nextsent>
<nextsent>table 3 shows the labeled precision (lp) and labeled recall (lr) scores for probabilistic and non- probabilistic dop for six different maximum subtree depths.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A99">
<title id=" C00-1011.xml">parsing with the shortest derivation </title>
<section> experimental comparison.  </section>
<citcontext>
<prevsection>
<prevsent>scores on tile wsj corpus (sentences _ 40 words) 73 the table shows that probabilistic dop outperl orms non-probabilistic dop for maximum subtree depths 4 and 6, while the models yield rather similar results for maximum subtree depth 8.
</prevsent>
<prevsent>surprisingly, the scores of nonq~robabilistic dop deteriorate if the subtrees are further enlarged, while tile scores of probabilistic dop continue to grow, up to 89.5% lp and 89.3% lr.
</prevsent>
</prevsection>
<citsent citstr=" A00-2018 ">
these scores are higher than those of several other parsers (e.g. collins 1997, <papid> P97-1003 </papid>99; charniak 1997), but remain behind tim scores of charniak (2000) <papid> A00-2018 </papid>who obtains 90.1% lp and 90.1% lr for sentences _  40 words.</citsent>
<aftsection>
<nextsent>however, in bod (2000<papid> C00-1010 </papid>b) we show that even higher scores can be obtained with probabilistic dop by restricting tile number of words in the subtree frontiers to 12 and restricting the depth of unlexical- ized subtrees to 6; with these restrictions an lp of 90.8% and an lr of 90.6% is achieved.</nextsent>
<nextsent>we may raise the question as to whether we actually need these extremely large subtrees to obtain our best results.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A100">
<title id=" C00-1011.xml">parsing with the shortest derivation </title>
<section> experimental comparison.  </section>
<citcontext>
<prevsection>
<prevsent>surprisingly, the scores of nonq~robabilistic dop deteriorate if the subtrees are further enlarged, while tile scores of probabilistic dop continue to grow, up to 89.5% lp and 89.3% lr.
</prevsent>
<prevsent>these scores are higher than those of several other parsers (e.g. collins 1997, <papid> P97-1003 </papid>99; charniak 1997), but remain behind tim scores of charniak (2000) <papid> A00-2018 </papid>who obtains 90.1% lp and 90.1% lr for sentences _  40 words.</prevsent>
</prevsection>
<citsent citstr=" C00-1010 ">
however, in bod (2000<papid> C00-1010 </papid>b) we show that even higher scores can be obtained with probabilistic dop by restricting tile number of words in the subtree frontiers to 12 and restricting the depth of unlexical- ized subtrees to 6; with these restrictions an lp of 90.8% and an lr of 90.6% is achieved.</citsent>
<aftsection>
<nextsent>we may raise the question as to whether we actually need these extremely large subtrees to obtain our best results.
</nextsent>
<nextsent>one could argue that dop gain in parse accuracy with increasing subtree depth is due to tile model becoming sensitive to the int luence o1  lexical heads higher in tile lree, and that this gain could also be achieved by more compact depth-1 dop model (i.e. an scfg) which annotates the nonterminals with headwords.
</nextsent>
<nextsent>however, such head- lexicalized stochastic grammar does not capture dependencies between non headwords (such as more aud than in tile w,qj construction carry more people than cargo where neither more nor th\[lll are headwords ol  tile np-constitucnt lllore people than cargo)), whe,eas frontier-lexicalized dop model using large subtrecs does capture these dependencies ince it includes subtrees in which e.g. more and than are the only frontier words.
</nextsent>
<nextsent>in order to isolate tile contribution of non headword dependencies, we eliminated all subtrees containing two or more nonheadwnrds (where non headword of subtl ec is word which is not headword of the subtree root nonterminal -- although such non headword may be headword of one of the subtree internal nodes).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A104">
<title id=" C00-1082.xml">bunsetsu identification using category exclusive rules </title>
<section> bunsetsu identification problem.  </section>
<citcontext>
<prevsection>
<prevsent>the four types of information, (i) major pos, (ii) mi-nor pos, (iii) semmltic information, and (iv) word, mentioned in the previous section were also used as features with the decision-tree arning method.
</prevsent>
<prevsent>as shown in figure 3, the number of features is 12 (2 + 4 + 4 + 2) because we do not use (iii) semantic information and (iv) word information from the two outside morphemes.
</prevsent>
</prevsection>
<citsent citstr=" J96-1002 ">
in figure 2, for example, the value of the feature  the major pos of the far left morpheme  is  noun.  a.2 maximum-entropy method the maximum-entropy method is useful with sparse data conditions and has been used by many re-searchers (berger et al, 1996; <papid> J96-1002 </papid>ratnaparkhi, 1996; <papid> W96-0213 </papid>ratnaparkhi, 1997; <papid> W97-0301 </papid>borthwick el; al., 1998; uchi-moto et al, 1999).<papid> E99-1026 </papid></citsent>
<aftsection>
<nextsent>in our maximuln-entropy exper-iment we used ristad system (ristad, 1998).
</nextsent>
<nextsent>the analysis is performed by calculating the probability of inserting or not inserting partition mark, from the output of the system.
</nextsent>
<nextsent>whichever probability is higher is selected as the desired answer.
</nextsent>
<nextsent>in the maximum-entropy method, we use the same four types of mori)hological information, (i) major pos, (ii) minor pos, (iii) semantic information, and (iv) word, as in the decision-tree method.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A105">
<title id=" C00-1082.xml">bunsetsu identification using category exclusive rules </title>
<section> bunsetsu identification problem.  </section>
<citcontext>
<prevsection>
<prevsent>the four types of information, (i) major pos, (ii) mi-nor pos, (iii) semmltic information, and (iv) word, mentioned in the previous section were also used as features with the decision-tree arning method.
</prevsent>
<prevsent>as shown in figure 3, the number of features is 12 (2 + 4 + 4 + 2) because we do not use (iii) semantic information and (iv) word information from the two outside morphemes.
</prevsent>
</prevsection>
<citsent citstr=" W96-0213 ">
in figure 2, for example, the value of the feature  the major pos of the far left morpheme  is  noun.  a.2 maximum-entropy method the maximum-entropy method is useful with sparse data conditions and has been used by many re-searchers (berger et al, 1996; <papid> J96-1002 </papid>ratnaparkhi, 1996; <papid> W96-0213 </papid>ratnaparkhi, 1997; <papid> W97-0301 </papid>borthwick el; al., 1998; uchi-moto et al, 1999).<papid> E99-1026 </papid></citsent>
<aftsection>
<nextsent>in our maximuln-entropy exper-iment we used ristad system (ristad, 1998).
</nextsent>
<nextsent>the analysis is performed by calculating the probability of inserting or not inserting partition mark, from the output of the system.
</nextsent>
<nextsent>whichever probability is higher is selected as the desired answer.
</nextsent>
<nextsent>in the maximum-entropy method, we use the same four types of mori)hological information, (i) major pos, (ii) minor pos, (iii) semantic information, and (iv) word, as in the decision-tree method.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A106">
<title id=" C00-1082.xml">bunsetsu identification using category exclusive rules </title>
<section> bunsetsu identification problem.  </section>
<citcontext>
<prevsection>
<prevsent>the four types of information, (i) major pos, (ii) mi-nor pos, (iii) semmltic information, and (iv) word, mentioned in the previous section were also used as features with the decision-tree arning method.
</prevsent>
<prevsent>as shown in figure 3, the number of features is 12 (2 + 4 + 4 + 2) because we do not use (iii) semantic information and (iv) word information from the two outside morphemes.
</prevsent>
</prevsection>
<citsent citstr=" W97-0301 ">
in figure 2, for example, the value of the feature  the major pos of the far left morpheme  is  noun.  a.2 maximum-entropy method the maximum-entropy method is useful with sparse data conditions and has been used by many re-searchers (berger et al, 1996; <papid> J96-1002 </papid>ratnaparkhi, 1996; <papid> W96-0213 </papid>ratnaparkhi, 1997; <papid> W97-0301 </papid>borthwick el; al., 1998; uchi-moto et al, 1999).<papid> E99-1026 </papid></citsent>
<aftsection>
<nextsent>in our maximuln-entropy exper-iment we used ristad system (ristad, 1998).
</nextsent>
<nextsent>the analysis is performed by calculating the probability of inserting or not inserting partition mark, from the output of the system.
</nextsent>
<nextsent>whichever probability is higher is selected as the desired answer.
</nextsent>
<nextsent>in the maximum-entropy method, we use the same four types of mori)hological information, (i) major pos, (ii) minor pos, (iii) semantic information, and (iv) word, as in the decision-tree method.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A107">
<title id=" C00-1082.xml">bunsetsu identification using category exclusive rules </title>
<section> bunsetsu identification problem.  </section>
<citcontext>
<prevsection>
<prevsent>the four types of information, (i) major pos, (ii) mi-nor pos, (iii) semmltic information, and (iv) word, mentioned in the previous section were also used as features with the decision-tree arning method.
</prevsent>
<prevsent>as shown in figure 3, the number of features is 12 (2 + 4 + 4 + 2) because we do not use (iii) semantic information and (iv) word information from the two outside morphemes.
</prevsent>
</prevsection>
<citsent citstr=" E99-1026 ">
in figure 2, for example, the value of the feature  the major pos of the far left morpheme  is  noun.  a.2 maximum-entropy method the maximum-entropy method is useful with sparse data conditions and has been used by many re-searchers (berger et al, 1996; <papid> J96-1002 </papid>ratnaparkhi, 1996; <papid> W96-0213 </papid>ratnaparkhi, 1997; <papid> W97-0301 </papid>borthwick el; al., 1998; uchi-moto et al, 1999).<papid> E99-1026 </papid></citsent>
<aftsection>
<nextsent>in our maximuln-entropy exper-iment we used ristad system (ristad, 1998).
</nextsent>
<nextsent>the analysis is performed by calculating the probability of inserting or not inserting partition mark, from the output of the system.
</nextsent>
<nextsent>whichever probability is higher is selected as the desired answer.
</nextsent>
<nextsent>in the maximum-entropy method, we use the same four types of mori)hological information, (i) major pos, (ii) minor pos, (iii) semantic information, and (iv) word, as in the decision-tree method.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A108">
<title id=" C00-1082.xml">bunsetsu identification using category exclusive rules </title>
<section> normal noun case-particle normal form punctuation.  </section>
<citcontext>
<prevsection>
<prevsent>the decision-list method searches for rules dora tile top of the list and an-alyzes particular problem by using only the first applicable rule.
</prevsent>
<prevsent>in this study we used ill the decision-list method the same 152 types of patterns that were used in/;lie ma.ximuln-entropy method.
</prevsent>
</prevsection>
<citsent citstr=" P94-1013 ">
to determine the priority order of the rules, we re-ferred to yarowsky method (yarowsky, 1994) <papid> P94-1013 </papid>and nishiokwama method (nishiokaymna et al, 1998) and used the probability a.nd frequency of each rule as measures of this priority order.</citsent>
<aftsection>
<nextsent>when nnlltiple rifles had the same probability, the rules were ar-ranged in order of their frequency.
</nextsent>
<nextsent>suppose, for example, that pattern  noun: normal noun; particle: case-particle: none: wo; verb: normal form: 217; symhol: punctuatioif  occurs 13 times in learlfing set and that tell of the occurrences include the inserted partition inal:k. suppose also thai; pattern  noun; particle; verb; symbol  occurs 12a times in learning set and that 90 of the occurrences include the mark.
</nextsent>
<nextsent>this exmnple is recognized by the following rules: pattern ~ partition 76.9% (10/ 13), freq.
</nextsent>
<nextsent>23 pattern =  partition 73.2% (90/123), freq.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A109">
<title id=" C00-2127.xml">toward the ataglance summary phrase representation summarization method </title>
<section> eva luat ion.  </section>
<citcontext>
<prevsection>
<prevsent>,lapttrics;e dticsll require anything like relative pi+onoun+  ~  .lava and solaris are the tra(temarks of sun microsvstems.
</prevsent>
<prevsent>windows and ccleron tll the mldcmark!; of microsoft and lntel, respedively.
</prevsent>
</prevsection>
<citsent citstr=" C96-2166 ">
881 ? assume an inlbrmation need and make queiw for the information eed ? prepare simulated www search results with different ypes of summaries: (a) first 80 characters, (b) important sentence se-lection (zechner, 1996), (<papid> C96-2166 </papid>c) phrase- represented summary, (i)) keyword enu- meration.</citsent>
<aftsection>
<nextsent>the documents in the simulated search result set are selected so that the set includes an appropriate number of relevant documents and irrelevant documents.
</nextsent>
<nextsent>have subjects judge from the summaries the relevance between the search results and the given int ormation need.
</nextsent>
<nextsent>the judgement is expressed in our levels (from higher to lower: l3, l2, li, and l0, which is judged to be irrelevant).
</nextsent>
<nextsent>compare the relevance with the one that we assumed.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A110">
<title id=" C00-1046.xml">automatic refinement of a pos tagger using a reliable parser and plain text corpora </title>
<section> int roduct ion.  </section>
<citcontext>
<prevsection>

<prevsent>much research has been donc oll knowledge acquisition fiom large-scalc annotated corpora as rich source of linguistic knowledge.
</prevsent>
</prevsection>
<citsent citstr=" A88-1019 ">
m~tior works done to create english pos taggers (henceforth,  taggers ), for example, include (church 1988), (<papid> A88-1019 </papid>kupicc 1992), (brill 1992)<papid> A92-1021 </papid>and (voutilaincn et al 1992).</citsent>
<aftsection>
<nextsent>the problem with this framework, however, is that such reliable corpora are hardly awdlable duc to huge amount of the labor-intensive work required.
</nextsent>
<nextsent>in case of the acquisition of non-core knowledge, such as specific, lexically or dolnain dependent knowledge, preparation of annotated corpora becomes more serious problem.
</nextsent>
<nextsent>one viable approach then is to utilize plain text corpora instead, as in (mikheev 1996).<papid> P96-1043 </papid></nextsent>
<nextsent>but the method proposed by (mikheev 1996) <papid> P96-1043 </papid>has its own weaknesses, in that it is restricted in scope.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A111">
<title id=" C00-1046.xml">automatic refinement of a pos tagger using a reliable parser and plain text corpora </title>
<section> int roduct ion.  </section>
<citcontext>
<prevsection>

<prevsent>much research has been donc oll knowledge acquisition fiom large-scalc annotated corpora as rich source of linguistic knowledge.
</prevsent>
</prevsection>
<citsent citstr=" A92-1021 ">
m~tior works done to create english pos taggers (henceforth,  taggers ), for example, include (church 1988), (<papid> A88-1019 </papid>kupicc 1992), (brill 1992)<papid> A92-1021 </papid>and (voutilaincn et al 1992).</citsent>
<aftsection>
<nextsent>the problem with this framework, however, is that such reliable corpora are hardly awdlable duc to huge amount of the labor-intensive work required.
</nextsent>
<nextsent>in case of the acquisition of non-core knowledge, such as specific, lexically or dolnain dependent knowledge, preparation of annotated corpora becomes more serious problem.
</nextsent>
<nextsent>one viable approach then is to utilize plain text corpora instead, as in (mikheev 1996).<papid> P96-1043 </papid></nextsent>
<nextsent>but the method proposed by (mikheev 1996) <papid> P96-1043 </papid>has its own weaknesses, in that it is restricted in scope.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A112">
<title id=" C00-1046.xml">automatic refinement of a pos tagger using a reliable parser and plain text corpora </title>
<section> int roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>the problem with this framework, however, is that such reliable corpora are hardly awdlable duc to huge amount of the labor-intensive work required.
</prevsent>
<prevsent>in case of the acquisition of non-core knowledge, such as specific, lexically or dolnain dependent knowledge, preparation of annotated corpora becomes more serious problem.
</prevsent>
</prevsection>
<citsent citstr=" P96-1043 ">
one viable approach then is to utilize plain text corpora instead, as in (mikheev 1996).<papid> P96-1043 </papid></citsent>
<aftsection>
<nextsent>but the method proposed by (mikheev 1996) <papid> P96-1043 </papid>has its own weaknesses, in that it is restricted in scope.</nextsent>
<nextsent>that is, it aims to acquire rules for unknown words in corpora fi om their ending characters without looking at the context.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A114">
<title id=" C00-1046.xml">automatic refinement of a pos tagger using a reliable parser and plain text corpora </title>
<section> int roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>but the method proposed by (mikheev 1996) <papid> P96-1043 </papid>has its own weaknesses, in that it is restricted in scope.</prevsent>
<prevsent>that is, it aims to acquire rules for unknown words in corpora fi om their ending characters without looking at the context.</prevsent>
</prevsection>
<citsent citstr=" W95-0101 ">
in the meantime, (brill 1995<papid> W95-0101 </papid>a) (brill 1995<papid> W95-0101 </papid>b) proposed method to acquire contcxt-dcpendent pos disambiguation rules and created an accurate tagger, even from very small aunotated text by combining supervised and tmsupcrviscd learning.</citsent>
<aftsection>
<nextsent>the wcakness of his method is that the effect of unsupervised learning decreases as the training corpus size increases.
</nextsent>
<nextsent>the problem in using plain text corpora for knowledge acquisition is that we need human supervisor who can evaluate and sift the obtained knowledge.
</nextsent>
<nextsent>an alternative to this would be to use number of modules of well- developed nlp system which stores most of thc highly reliable general rules.
</nextsent>
<nextsent>here, one module fimctions as supervisor for other modules, since all these modules arc designed to work cooperatively and the knowledgcs tored in each module are correlated.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A122">
<title id=" C00-1046.xml">automatic refinement of a pos tagger using a reliable parser and plain text corpora </title>
<section> outline of apras.  </section>
<citcontext>
<prevsection>
<prevsent>un parsable parsable rank(verb) -+ rank(norm) : (verb)-(determiner)-$- in -(determiner) initially tagged pos parsable pos pos context figure 2: pa rule sized training corpus.
</prevsent>
<prevsent>at present we set the context size to be two words.
</prevsent>
</prevsection>
<citsent citstr=" P97-1030 ">
in choosing adequate levels of abstraction or specification of pos in the context, we grouped together those pos tags which influence the choice of pos of focus word in similar lnanner as one super-pos tag, as in (haruno &amp; matsumoto 1997).<papid> P97-1030 </papid></citsent>
<aftsection>
<nextsent>we also changed some pos tags for functional words like prepositions and words such as  be  and  have  to tags which denote their literal forms, because the choice of pos of focus word is highly dependent on the word itsclf.
</nextsent>
<nextsent>as result, we obtained 513 pos tags including 16 pos tags for nouns, 17 for verbs, 410 for prepositions and phrasal prepositions, and 70 for adjectives and adverbs.
</nextsent>
<nextsent>3.2 rule filtering module.
</nextsent>
<nextsent>this section deals with how to statistically filter out inappropriate rules from the generated pa rule candidates.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A123">
<title id=" C00-1043.xml">experiments with open domain textual question answering </title>
<section> interact ions.  </section>
<citcontext>
<prevsection>
<prevsent>(riloff and jones, 1999)).
</prevsent>
<prevsent>world knowledge axioms can also be easily derived by processing the gloss (lefinitions of wordnel; (fellbaunl 1998).
</prevsent>
</prevsection>
<citsent citstr=" P96-1025 ">
a.1 semantic and logic rans formations semantic ranstbrmat ions instead of t)roducing only phrasal parse for the question and answer, we lnake use of one of the new statistical parsers of large real-world text coverage (collins, 1996).<papid> P96-1025 </papid></citsent>
<aftsection>
<nextsent>the parse trees produced by such parser can be easily translated into sein antic repre-sentation that (1) comprises all the phrase beads and (2) captures their int,er-relationships by anonymous links.
</nextsent>
<nextsent>figure 2 illustrates both the i)arse tree and the associated semantic representation a ti{ec-8 question.
</nextsent>
<nextsent>question: why did i)avid koresh ask the fbi for word processor?
</nextsent>
<nextsent>parse: sbap.q -- - . . .
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A124">
<title id=" C00-1024.xml">a muitilingual news summarizer </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>reorganization of news is sonic sort of document summarization, which creates short version of original document.
</prevsent>
<prevsent>recently, many papers touch on single document summarization (ltovy and marcu, 1998a).
</prevsent>
</prevsection>
<citsent citstr=" J98-3005 ">
only few touch on multiple document sulnmarization (chen and huang, 1999; mani and bloedorn, 1997; radev and mckeown, 1998) <papid> J98-3005 </papid>and multilingual document summarization (hovy and marcu, 1998b).</citsent>
<aftsection>
<nextsent>for multilingual multiple news summarization, several issues have to be addressed: (1) translation among news stories in different languages the basic idea in multiple doculnent sulnmarizations to identify which paris of news articles present similar reports.
</nextsent>
<nextsent>because the news stories are in different languages, seine kind of iranslation is required, e.g., term translation.
</nextsent>
<nextsent>besides the problem of translation ambiguity, different news sites often use difl erent names to refer tile same entity.
</nextsent>
<nextsent>the translation o1  named entities, which are usually ttn known words, is another probleln.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A125">
<title id=" C00-1024.xml">a muitilingual news summarizer </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>comparatively, chinese news articles are segmented at first.
</prevsent>
<prevsent>then, several types of inforlnation fiom character, sentence and text levels are employed to extract chinese named 160 entities.
</prevsent>
</prevsection>
<citsent citstr=" C96-1039 ">
these tasks are similar to tile approaches ill tile papers (chen and lee, 1996; <papid> C96-1039 </papid>chen, el al., 1998a).</citsent>
<aftsection>
<nextsent>1.2 multilingual clustering.
</nextsent>
<nextsent>tile multilingual clusterer takes input from the lnonolingual clusterers, and determines which news clusters ill which languages talk about tile same story.
</nextsent>
<nextsent>recall that news cluster consists of several news articles reporting tile same event, and one news cluster exists lbr one event ariel monolingual clustering.
</nextsent>
<nextsent>ill this way, there is at most one corresponding news cluster ill another language.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A126">
<title id=" C00-2155.xml">an hpsgtocfg approximation of japanese </title>
<section> basic  algor thm.  </section>
<citcontext>
<prevsection>
<prevsent>firstly, the most obvious optimization applies to the function fill-daughters (line 12), where the number of unifications is reduced by avoid-ing recomputation of combinations of daugh-ters and rules that already have been checked.
</prevsent>
<prevsent>to do this in simple way, we split the set ti into ti \ t/.-1 and t/_i and fill rule with only those permutations of daughters which contain at least one element from t/ \ / _ 1 . this guaran-tees checking of only those configurations which were enabled by the last iteration.
</prevsent>
</prevsection>
<citsent citstr=" P99-1061 ">
secondly, we use techniques developed in (kiefer et al, 1999), <papid> P99-1061 </papid>namely the so-called rule filter and the quick-check method.</citsent>
<aftsection>
<nextsent>the rule fil-ter pre computes the applicability of rules into each other and thus is able to predict fail-ing unification using simple and fast table lookup.
</nextsent>
<nextsent>the quick-check method exploits the 1048 flint that unification fails snore often at cer-tain points in feature structnres than at oth-ers.
</nextsent>
<nextsent>in an off line stage, we parse test cor-pus, using special unifier that records all fail-ures instead of bailing out after the first one in order to determine the most prominent fail-ure points/paths.
</nextsent>
<nextsent>these points constitute the so-called quick-check vector.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A127">
<title id=" A00-3005.xml">corpus based syntactic error detection using syntactic patterns </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the system has been tested on corpus of real texts, containing both correct and incorrect sentences, with promising results.
</prevsent>
<prevsent>the problem of syntactic error detection and correction has been addressed since the early years of natural language processing.
</prevsent>
</prevsection>
<citsent citstr=" J83-3003 ">
different techniques have been proposed for the treatment of the significant portion of errors (typographic, phonetic, cognitive and grammatical) that result valid words (weischedel and sondheimer 1983; <papid> J83-3003 </papid>heidorn et al 1982).</citsent>
<aftsection>
<nextsent>however, although most currently used word-processors actually provide grammar checking module, little work has been done on the evaluation of results.
</nextsent>
<nextsent>there are several reasons for this: ? incomplete coverage.
</nextsent>
<nextsent>some of the best parsers at the moment can analyze only subset of the sentences in real texts.
</nextsent>
<nextsent>compared to syntactic valid structures, the set of syntactically incorrect sentences can be considered almost infinite.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A128">
<title id=" A00-3005.xml">corpus based syntactic error detection using syntactic patterns </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>she estimates that proportion of all the errors varying between 25% and over 50%, depending on the application, are valid words.
</prevsent>
<prevsent>atwell and elliott (1987) made manual study concluding that 55% of them are local syntactic errors (detectable by an examination of the local syntactic context), 18% are due to global syntactic errors (involving long-distance syntactic dependencies, which need full parse of the sentence), and 27% are semantic errors.
</prevsent>
</prevsection>
<citsent citstr=" C92-2072 ">
regarding their treatment, different approaches have been proposed: ? the relaxation of syntactic constraints (douglas and dale 1992).<papid> C92-2072 </papid></citsent>
<aftsection>
<nextsent>this grammar- based method allows the analysis of sentences \[ . . .
</nextsent>
<nextsent>, sentence i i morphological analysis and disambiguation ii chart-parser , chart (automaton) j finite-state parser , no error / error type(s) ,j . . .
</nextsent>
<nextsent>| figure 1.
</nextsent>
<nextsent>overview of the system.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A129">
<title id=" A00-3005.xml">corpus based syntactic error detection using syntactic patterns </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>that do not fulfill some of the constraints of the language by identifying rule that might have been violated, determining whether its relaxation might lead to successful parse.
</prevsent>
<prevsent>its main disadvantage is the need of full- coverage grammar, problem not solved at the moment, except for restricted environments (menzel and schroder 1999).
</prevsent>
</prevsection>
<citsent citstr=" P96-1010 ">
error patterns (kukich 1992; golding and schabes 1996; <papid> P96-1010 </papid>mangu and brill 1997), in the form of statistical information, hand-coded rules or automatically earned ones.</citsent>
<aftsection>
<nextsent>charts have been used in grammar-based systems as source of information; they can be resorted to if no complete analysis is found, so as to detect syntactic error (mellish 1989; <papid> P89-1013 </papid>min-wilson 1998).</nextsent>
<nextsent>we have used parsing system (aldezabal et al 1999, 2000) divided in three main modules (see figure 1): ? morphological analysis and disambiguation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A130">
<title id=" A00-3005.xml">corpus based syntactic error detection using syntactic patterns </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>its main disadvantage is the need of full- coverage grammar, problem not solved at the moment, except for restricted environments (menzel and schroder 1999).
</prevsent>
<prevsent>error patterns (kukich 1992; golding and schabes 1996; <papid> P96-1010 </papid>mangu and brill 1997), in the form of statistical information, hand-coded rules or automatically earned ones.</prevsent>
</prevsection>
<citsent citstr=" P89-1013 ">
charts have been used in grammar-based systems as source of information; they can be resorted to if no complete analysis is found, so as to detect syntactic error (mellish 1989; <papid> P89-1013 </papid>min-wilson 1998).</citsent>
<aftsection>
<nextsent>we have used parsing system (aldezabal et al 1999, 2000) divided in three main modules (see figure 1): ? morphological analysis and disambiguation.
</nextsent>
<nextsent>a robust morphological analyzer (alegria et al. 1996) obtains for each word its segmentation(s) into component morphemes.
</nextsent>
<nextsent>after that, morphological disambiguation (ezeiza et al 1998) <papid> P98-1063 </papid>is applied, reducing the high word-level ambiguity from 2.65 to 1.19 interpretations.</nextsent>
<nextsent>unification-based chart-parsing.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A131">
<title id=" A00-3005.xml">corpus based syntactic error detection using syntactic patterns </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we have used parsing system (aldezabal et al 1999, 2000) divided in three main modules (see figure 1): ? morphological analysis and disambiguation.
</prevsent>
<prevsent>a robust morphological analyzer (alegria et al. 1996) obtains for each word its segmentation(s) into component morphemes.
</prevsent>
</prevsection>
<citsent citstr=" P98-1063 ">
after that, morphological disambiguation (ezeiza et al 1998) <papid> P98-1063 </papid>is applied, reducing the high word-level ambiguity from 2.65 to 1.19 interpretations.</citsent>
<aftsection>
<nextsent>unification-based chart-parsing.
</nextsent>
<nextsent>after morphological nalysis and disambiguation, patr-ii unification grammar is applied bottom-up to each sentence, giving chart as result.
</nextsent>
<nextsent>the grammar is partial but it gives complete coverage of the main sentence elements, uch as noun phrases, prepositional phrases, sentential complements and simple sentences.
</nextsent>
<nextsent>the result is shallow parser (abney 1997) that can be used for subsequent processing (see figure 2).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A134">
<title id=" C00-2104.xml">experiments in automated lexicon building for text searching </title>
<section> re la ted  work.  </section>
<citcontext>
<prevsection>
<prevsent>jing and croft (1994) exanfined words and phrases in paragraph units, and found that the association data improves retrieval performance.
</prevsent>
<prevsent>callan (1994) compared paragraph units and fixed windows of text in examining passage-level retrieval.
</prevsent>
</prevsection>
<citsent citstr=" W99-0212 ">
in the question-answering context, morton (1999) <papid> W99-0212 </papid>collected document co-occurrence statistics to un-cover 1)art-whole and synonymy relationships to use in question-answering system.</citsent>
<aftsection>
<nextsent>the key differ-ence here was that co-occurrence was considered on whole-docmnent basis.
</nextsent>
<nextsent>harabagiu and maiorano (1999) argued that indexing in question answering should be based on 1)aragraphs.
</nextsent>
<nextsent>one recent al)proach to automatic lexicon build-ing has used seed words to lmild up larger sets of semmltically similar words in one or nlore categories (riloff and shepherd, 1997).<papid> W97-0313 </papid></nextsent>
<nextsent>in addition, strza-lkowski and wang (1996) used bootstrapping tech-nique to identify types of references, and riloff and jones (1999) adapted bootstrapping techniques to lexicon building targeted to information extraction.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A135">
<title id=" C00-2104.xml">experiments in automated lexicon building for text searching </title>
<section> re la ted  work.  </section>
<citcontext>
<prevsection>
<prevsent>the key differ-ence here was that co-occurrence was considered on whole-docmnent basis.
</prevsent>
<prevsent>harabagiu and maiorano (1999) argued that indexing in question answering should be based on 1)aragraphs.
</prevsent>
</prevsection>
<citsent citstr=" W97-0313 ">
one recent al)proach to automatic lexicon build-ing has used seed words to lmild up larger sets of semmltically similar words in one or nlore categories (riloff and shepherd, 1997).<papid> W97-0313 </papid></citsent>
<aftsection>
<nextsent>in addition, strza-lkowski and wang (1996) used bootstrapping tech-nique to identify types of references, and riloff and jones (1999) adapted bootstrapping techniques to lexicon building targeted to information extraction.
</nextsent>
<nextsent>in the same vein, researchers at brown univer-sity (caraballo and charniak, 1999)~ (<papid> W99-0609 </papid>berland and charniak, 1999), (caraballo, 1999) <papid> P99-1016 </papid>and (roark and charniak, 1998) <papid> P98-2182 </papid>focused on target constructions, in particular complex noun t)hrases, and searched for information ot only on identifying classes of nouns, lint also hypernyms, noun specificity and meronymy.</nextsent>
<nextsent>we have diflbrent perspective than these lines of inquiry.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A136">
<title id=" C00-2104.xml">experiments in automated lexicon building for text searching </title>
<section> re la ted  work.  </section>
<citcontext>
<prevsection>
<prevsent>one recent al)proach to automatic lexicon build-ing has used seed words to lmild up larger sets of semmltically similar words in one or nlore categories (riloff and shepherd, 1997).<papid> W97-0313 </papid></prevsent>
<prevsent>in addition, strza-lkowski and wang (1996) used bootstrapping tech-nique to identify types of references, and riloff and jones (1999) adapted bootstrapping techniques to lexicon building targeted to information extraction.</prevsent>
</prevsection>
<citsent citstr=" W99-0609 ">
in the same vein, researchers at brown univer-sity (caraballo and charniak, 1999)~ (<papid> W99-0609 </papid>berland and charniak, 1999), (caraballo, 1999) <papid> P99-1016 </papid>and (roark and charniak, 1998) <papid> P98-2182 </papid>focused on target constructions, in particular complex noun t)hrases, and searched for information ot only on identifying classes of nouns, lint also hypernyms, noun specificity and meronymy.</citsent>
<aftsection>
<nextsent>we have diflbrent perspective than these lines of inquiry.
</nextsent>
<nextsent>they were specifying various semantic rela-tionships and seeking ways to collect similar pairs.
</nextsent>
<nextsent>we.
</nextsent>
<nextsent>have less restrictive focus and are relying on surface syntactic information about clauses.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A137">
<title id=" C00-2104.xml">experiments in automated lexicon building for text searching </title>
<section> re la ted  work.  </section>
<citcontext>
<prevsection>
<prevsent>one recent al)proach to automatic lexicon build-ing has used seed words to lmild up larger sets of semmltically similar words in one or nlore categories (riloff and shepherd, 1997).<papid> W97-0313 </papid></prevsent>
<prevsent>in addition, strza-lkowski and wang (1996) used bootstrapping tech-nique to identify types of references, and riloff and jones (1999) adapted bootstrapping techniques to lexicon building targeted to information extraction.</prevsent>
</prevsection>
<citsent citstr=" P99-1016 ">
in the same vein, researchers at brown univer-sity (caraballo and charniak, 1999)~ (<papid> W99-0609 </papid>berland and charniak, 1999), (caraballo, 1999) <papid> P99-1016 </papid>and (roark and charniak, 1998) <papid> P98-2182 </papid>focused on target constructions, in particular complex noun t)hrases, and searched for information ot only on identifying classes of nouns, lint also hypernyms, noun specificity and meronymy.</citsent>
<aftsection>
<nextsent>we have diflbrent perspective than these lines of inquiry.
</nextsent>
<nextsent>they were specifying various semantic rela-tionships and seeking ways to collect similar pairs.
</nextsent>
<nextsent>we.
</nextsent>
<nextsent>have less restrictive focus and are relying on surface syntactic information about clauses.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A138">
<title id=" C00-2104.xml">experiments in automated lexicon building for text searching </title>
<section> re la ted  work.  </section>
<citcontext>
<prevsection>
<prevsent>one recent al)proach to automatic lexicon build-ing has used seed words to lmild up larger sets of semmltically similar words in one or nlore categories (riloff and shepherd, 1997).<papid> W97-0313 </papid></prevsent>
<prevsent>in addition, strza-lkowski and wang (1996) used bootstrapping tech-nique to identify types of references, and riloff and jones (1999) adapted bootstrapping techniques to lexicon building targeted to information extraction.</prevsent>
</prevsection>
<citsent citstr=" P98-2182 ">
in the same vein, researchers at brown univer-sity (caraballo and charniak, 1999)~ (<papid> W99-0609 </papid>berland and charniak, 1999), (caraballo, 1999) <papid> P99-1016 </papid>and (roark and charniak, 1998) <papid> P98-2182 </papid>focused on target constructions, in particular complex noun t)hrases, and searched for information ot only on identifying classes of nouns, lint also hypernyms, noun specificity and meronymy.</citsent>
<aftsection>
<nextsent>we have diflbrent perspective than these lines of inquiry.
</nextsent>
<nextsent>they were specifying various semantic rela-tionships and seeking ways to collect similar pairs.
</nextsent>
<nextsent>we.
</nextsent>
<nextsent>have less restrictive focus and are relying on surface syntactic information about clauses.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A139">
<title id=" C00-2104.xml">experiments in automated lexicon building for text searching </title>
<section> re la ted  work.  </section>
<citcontext>
<prevsection>
<prevsent>for more than decade, variety of statistical techniques have been developed and refilled.
</prevsent>
<prevsent>tile focus of much of this work was to develop the methods themselves.
</prevsent>
</prevsection>
<citsent citstr=" P89-1010 ">
church and hanks (1989) <papid> P89-1010 </papid>ex-plored tile use of mutual information statistics in ranking co-occurrences within five-word windows.</citsent>
<aftsection>
<nextsent>smadja (1992) gathered co-occurrences within five- word windows to find collocations, particularly in specific domains.
</nextsent>
<nextsent>hindle (1990) <papid> P90-1034 </papid>classified nouns on the basis of co-occurring patterns of subject- verb and verb-object pairs.</nextsent>
<nextsent>hatzivassiloglou and mekeown (1993) <papid> P93-1023 </papid>clustered adjectives into semantic classes, and pereira et al (1993) <papid> P93-1024 </papid>clustered nouns on their appearance ill verb-object pairs.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A140">
<title id=" C00-2104.xml">experiments in automated lexicon building for text searching </title>
<section> re la ted  work.  </section>
<citcontext>
<prevsection>
<prevsent>church and hanks (1989) <papid> P89-1010 </papid>ex-plored tile use of mutual information statistics in ranking co-occurrences within five-word windows.</prevsent>
<prevsent>smadja (1992) gathered co-occurrences within five- word windows to find collocations, particularly in specific domains.</prevsent>
</prevsection>
<citsent citstr=" P90-1034 ">
hindle (1990) <papid> P90-1034 </papid>classified nouns on the basis of co-occurring patterns of subject- verb and verb-object pairs.</citsent>
<aftsection>
<nextsent>hatzivassiloglou and mekeown (1993) <papid> P93-1023 </papid>clustered adjectives into semantic classes, and pereira et al (1993) <papid> P93-1024 </papid>clustered nouns on their appearance ill verb-object pairs.</nextsent>
<nextsent>we are try-ing to be less restrictive in learning multiple salient relationshil)s between words rather than seeldng particular elationship.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A141">
<title id=" C00-2104.xml">experiments in automated lexicon building for text searching </title>
<section> re la ted  work.  </section>
<citcontext>
<prevsection>
<prevsent>smadja (1992) gathered co-occurrences within five- word windows to find collocations, particularly in specific domains.
</prevsent>
<prevsent>hindle (1990) <papid> P90-1034 </papid>classified nouns on the basis of co-occurring patterns of subject- verb and verb-object pairs.</prevsent>
</prevsection>
<citsent citstr=" P93-1023 ">
hatzivassiloglou and mekeown (1993) <papid> P93-1023 </papid>clustered adjectives into semantic classes, and pereira et al (1993) <papid> P93-1024 </papid>clustered nouns on their appearance ill verb-object pairs.</citsent>
<aftsection>
<nextsent>we are try-ing to be less restrictive in learning multiple salient relationshil)s between words rather than seeldng particular elationship.
</nextsent>
<nextsent>ill way, our idea is the mirror image of barzilay and elhadad (1997), <papid> W97-0703 </papid>who used wordnet to identify lexical chains that would coincide with cohesive text segments.</nextsent>
<nextsent>we assunmd that documents are cohesive and that co-occurrence l)atterns call uncover word relationships.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A142">
<title id=" C00-2104.xml">experiments in automated lexicon building for text searching </title>
<section> re la ted  work.  </section>
<citcontext>
<prevsection>
<prevsent>smadja (1992) gathered co-occurrences within five- word windows to find collocations, particularly in specific domains.
</prevsent>
<prevsent>hindle (1990) <papid> P90-1034 </papid>classified nouns on the basis of co-occurring patterns of subject- verb and verb-object pairs.</prevsent>
</prevsection>
<citsent citstr=" P93-1024 ">
hatzivassiloglou and mekeown (1993) <papid> P93-1023 </papid>clustered adjectives into semantic classes, and pereira et al (1993) <papid> P93-1024 </papid>clustered nouns on their appearance ill verb-object pairs.</citsent>
<aftsection>
<nextsent>we are try-ing to be less restrictive in learning multiple salient relationshil)s between words rather than seeldng particular elationship.
</nextsent>
<nextsent>ill way, our idea is the mirror image of barzilay and elhadad (1997), <papid> W97-0703 </papid>who used wordnet to identify lexical chains that would coincide with cohesive text segments.</nextsent>
<nextsent>we assunmd that documents are cohesive and that co-occurrence l)atterns call uncover word relationships.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A143">
<title id=" C00-2104.xml">experiments in automated lexicon building for text searching </title>
<section> re la ted  work.  </section>
<citcontext>
<prevsection>
<prevsent>hatzivassiloglou and mekeown (1993) <papid> P93-1023 </papid>clustered adjectives into semantic classes, and pereira et al (1993) <papid> P93-1024 </papid>clustered nouns on their appearance ill verb-object pairs.</prevsent>
<prevsent>we are try-ing to be less restrictive in learning multiple salient relationshil)s between words rather than seeldng particular elationship.</prevsent>
</prevsection>
<citsent citstr=" W97-0703 ">
ill way, our idea is the mirror image of barzilay and elhadad (1997), <papid> W97-0703 </papid>who used wordnet to identify lexical chains that would coincide with cohesive text segments.</citsent>
<aftsection>
<nextsent>we assunmd that documents are cohesive and that co-occurrence l)atterns call uncover word relationships.
</nextsent>
<nextsent>tile focus of onr experiment was on units of text in which the constituents must fit together in order for the discourse to be coherent.
</nextsent>
<nextsent>we made the assump-tion that the documents in our corpus were coherent and reasoned that if we had enough text, covering broad range of topics, we could pick out domain- independent associations.
</nextsent>
<nextsent>for example, testimony can be about virtually anything, since anything can wind up in court dispute.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A144">
<title id=" C00-1008.xml">incremental identification of inflectional types </title>
<section> german noun-inflection classes.  </section>
<citcontext>
<prevsection>
<prevsent>morpho phonemic and morlfltographemic alterna- figure 2: schema for schwa-phn al (sehwa_pl_sort)  schwa_pl ftype ns flex ? surf atom base atom basic_is stem moph basic agr \[case-avml case |ctxt ~ daq agr kgen top hum plu gend - ,fcm ers pcr.s (;ions as shown in nominative plural zcit-cn  tinles  but gabel-n  forks  are also covered ill our descrip-tion.
</prevsent>
<prevsent>here (he real(sat(on of the plural ending - del)ends on the shape of tlm noun steul (nalnely, whether or not it ends in schwa syllable).
</prevsent>
</prevsection>
<citsent citstr=" J94-3010 ">
in agreement with (bird and klein, 1994) <papid> J94-3010 </papid>and (er\]avec, 1996), we capture such alternations declaratively in one-level model without recourse to transducers.</citsent>
<aftsection>
<nextsent>our treatment of umlaut adopts part of the tech-niques of (% ost, 1993).
</nextsent>
<nextsent>4 processing unknown words.
</nextsent>
<nextsent>in our al)proach linguistic prol)erties of unlcnown words are inferred fl om their sentential context as byproduct of parsing.
</nextsent>
<nextsent>after parsing, which re-quires only slight modification of sl;andard lexi-cal lookup, lexical entries are al)propriately updated.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A145">
<title id=" C00-1008.xml">incremental identification of inflectional types </title>
<section> german noun-inflection classes.  </section>
<citcontext>
<prevsection>
<prevsent>after parsing, which re-quires only slight modification of sl;andard lexi-cal lookup, lexical entries are al)propriately updated.
</prevsent>
<prevsent>one of our key ideas is gradual, information-based concept of  unknownness , where lexical entries are not unknown as whole, but may contain unknown, i.e. potentially revisable, pieces of information (cf.
</prevsent>
</prevsection>
<citsent citstr=" P98-1014 ">
barg and walther (1998)).<papid> P98-1014 </papid></citsent>
<aftsection>
<nextsent>this allows uniform treatment for tile full range of lexical entries from completely known to maximally unknown.
</nextsent>
<nextsent>as dis-cussed in (barg and walther, 1998), <papid> P98-1014 </papid>our system has been implemented in microcuf, derivative of the tbrmalism cuf of (dsrre and dorna, 1993).</nextsent>
<nextsent>50 figure 3: feature sl~ruci;ures for \]i lmd and hv.nd(; ba.sic_l.s -basic ftype n.5 surf ki h.uml base e\] a.q ~  moph rca.sc _avm \] case /ctxt  go.,| agr lgen r:(~.sc i iu l ,siw\] gend tt, a,sc pers third synsem ...</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A147">
<title id=" C00-1074.xml">hybrid neuro and rule based part of speech taggers </title>
<section> introduction.  </section>
<citcontext>
<prevsection>


</prevsection>
<citsent citstr=" J94-2001 ">
many pa.rt of speech (pos) tatters proposed so far (e.g., brill, 1994; meria.ldo, 1994; <papid> J94-2001 </papid>l)aele- marls, el.</citsent>
<aftsection>
<nextsent>al., 1996; and schmid, 1994) <papid> C94-1027 </papid>ha.re achieved a. high accura.ey partly because a. very large amount of dal,~ was used to 1;rain them (e.g., on the order of 1,000,000 words for \] hl- glish).</nextsent>
<nextsent>for ma.ny other la.nguages (e.g., thai, which we treat in this paper)~ however, it is not as easy to cremate \]a.rge corpora from which lm:ge amounts of tr~fining data can be extra.cted.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A148">
<title id=" C00-1074.xml">hybrid neuro and rule based part of speech taggers </title>
<section> introduction.  </section>
<citcontext>
<prevsection>

<prevsent>many pa.rt of speech (pos) tatters proposed so far (e.g., brill, 1994; meria.ldo, 1994; <papid> J94-2001 </papid>l)aele- marls, el.</prevsent>
</prevsection>
<citsent citstr=" C94-1027 ">
al., 1996; and schmid, 1994) <papid> C94-1027 </papid>ha.re achieved a. high accura.ey partly because a. very large amount of dal,~ was used to 1;rain them (e.g., on the order of 1,000,000 words for \] hl- glish).</citsent>
<aftsection>
<nextsent>for ma.ny other la.nguages (e.g., thai, which we treat in this paper)~ however, it is not as easy to cremate \]a.rge corpora from which lm:ge amounts of tr~fining data can be extra.cted.
</nextsent>
<nextsent>it is therefore desirable to construct practic;d tag-ger tha.t needs as little training a.t;a~ as possible.
</nextsent>
<nextsent>a multi-neuro tagger (ma a.nd ls~hara, 11998) and its slimmed-down version called the ela.s- tic neuro tagger (ma, el; al., 1999), which have high genera.lizing ability and therefore are good at dealing with the problems of data sp~u:se- hess, were proposed to satist~y this requh:ement.
</nextsent>
<nextsent>these taggers perform pos tagging using difl er- ent lengths of corltexts i)~.~sed on longest context prk)rity, and each element of tile input is weight-ed with information gains (quinla.n, 1993) for retlecting that tile elements of the input h~ve different rtlevances in t~gging.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A149">
<title id=" A92-1003.xml">an approach to multilevel semantics for applied systems </title>
<section> lex ica  leve l.  </section>
<citcontext>
<prevsection>
<prevsent>we define the consistency check operation such that it succeeds if selectional-restriction (i.e. the concept that represents the selectional restriction of given argument position) denotes concept that is compatible with the concept hat semantic-head (i.e. the concept associated with the constituent which has to fill such position) denotes.
</prevsent>
<prevsent>there exist several possibil ities to check the compatibility between two concepts within terminological hierarchy.
</prevsent>
</prevsection>
<citsent citstr=" P89-1024 ">
within the janus system \[weischedel, 1989\] <papid> P89-1024 </papid>the consistency is implemented by means of double subsumption check that guarantees uccess both when semantic-head is descendant of selectional-restriction and when it is an ancestor.</citsent>
<aftsection>
<nextsent>this double subsumption test does not consider the cases, sometimes relevant, in which semantic-head is brother concept of selectional-restriction (e.g.  has sculptor painted fresco? ); this case recursively extends to all the cases in which semantic-head is brother either of descendant or of an ancestor for selectional-restriction (e.g.  which object did giotto paint? ).
</nextsent>
<nextsent>this case is slightly more complex than the others.
</nextsent>
<nextsent>in fact, while it is always true that along the isa hierarchy there can be non-empty intersection between two concepts, this is not true for concepts that are brothers.
</nextsent>
<nextsent>if an explicit disjoinmess placed between two brother concepts, there cannot be common intersection and the consistency procedure must fail; otherwise it is assumed that common intersection can exist, and the consistency-test procedure will succeed.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A150">
<title id=" A92-1003.xml">an approach to multilevel semantics for applied systems </title>
<section> logical-interpretative level.  </section>
<citcontext>
<prevsection>
<prevsent>both systems have common architecture design and have been implemented in common lisp within the medley environment running on sun 4.
</prevsent>
<prevsent>the main components interacting with the semantic omponent described here are parser and hybrid knowledge representation system.
</prevsent>
</prevsection>
<citsent citstr=" J89-1001 ">
both for alfresco and maia the parser wednesday 2 is used \[stock, 1989\], <papid> J89-1001 </papid>chart-based parser for the italian language that can cope with complex sentences, idiomatic expressions, ellipsis, and so on.</citsent>
<aftsection>
<nextsent>as for knowledge representalion, alfresco the yak system \[franconi, 1990\] is used, while in maia the loom system \[mcgregor and bates, 1987\] is used.
</nextsent>
<nextsent>7.
</nextsent>
<nextsent>conclusions and future work.
</nextsent>
<nextsent>we have presented an approach to multilevel semantics that was exploited in the development of two semantic levels for dialog system architecture: the lexical level and the logical-interpretative level.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A151">
<title id=" A92-1003.xml">an approach to multilevel semantics for applied systems </title>
<section> logical-interpretative level.  </section>
<citcontext>
<prevsection>
<prevsent>further developments are connected with the use of natural anguage in domain which implies an interaction with the physical world (as happens in the maia system).
</prevsent>
<prevsent>this kind of application will also raise the need to access both information gathered from the physical environment and dynamically changing knowledge and of more complex pragmatic omponent, hereby stressing the need for clear architecture.
</prevsent>
</prevsection>
<citsent citstr=" P90-1029 ">
we are also working on the issue of integrating such expansions within the approach to multiple underlying systems (mus) as established by \[bobrow et al, 1990, <papid> P90-1029 </papid>resnik, 1989\].</citsent>
<aftsection>
<nextsent>in the mus approach, user may need to combine the capabilities of more than one system (i.e. several dbs on various domains, expert systems, information retrieval systems, interfaces to simulation packages, etc.) in order to perform general task.
</nextsent>
<nextsent>for dealing with mus, not only our semantic modules must be able to represent various levels of meaning of sentence, they must also be capable, in transparent manner, of organizing the different applications at their disposal and choosing which combination of them to use.
</nextsent>


</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A152">
<title id=" A00-2037.xml">acknowledgments in human computer interaction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>repetitions are often seen when one is con-veying complex information, such as when one copies an address or telephone number.
</prevsent>
<prevsent>neither acknowledgments or repetitions con-tribute new domain information to the conversa-tion, but they serve to assure the speaker that information has been conveyed successfully.
</prevsent>
</prevsection>
<citsent citstr=" P94-1014 ">
acknowledgments also play role in managing turn-taking in mixed-initiative dialogue; although acknowledgments may preface new contribution by the same speaker (novick &amp; sutton, 1994), <papid> P94-1014 </papid>often they occur alone as single-phrase turn that appears to serve the purpose of explicitly declining an opportunity to take turn (sacks et al 1974).</citsent>
<aftsection>
<nextsent>acknowledgments and repetitions are ubiqui-tous in many types of human-human conversation.
</nextsent>
<nextsent>in corpus of problem-solving spoken dialogues, for example, traum and heeman (1996) found that 51% of turns began with or consisted of an explicit acknowledgment.
</nextsent>
<nextsent>given this, one would expect that acknowledgments should be modeled india- logue models for spoken-language systems, and 280 indeed some research models are beginning to incorporate acknowledgments, e.g., kita et al (1996), aist, (1998), iwase &amp; ward (1998).
</nextsent>
<nextsent>typical human-computer dialogue models are structured in ways that suppress the use of acknowledgments.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A153">
<title id=" A00-1010.xml">talkntravel a conversational system for air travel planning </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this paper describes talk travel, spoken language dialogue system for making complex air travel plans over the telephone.
</prevsent>
<prevsent>talk travel is research prototype system sponsored under the darpa communicator program (mitre, 1999).
</prevsent>
</prevsection>
<citsent citstr=" W00-0303 ">
some other systems in the program are ward and pellom (1999), seneff and polifroni (2000) <papid> W00-0303 </papid>and rudnicky et al (1999).</citsent>
<aftsection>
<nextsent>the common task of this program is mixed-initiative dialogue over the telephone, in which the user plans multi-city trip by air, including all flights, hotels, and rental cars, all in conversational english over the telephone.
</nextsent>
<nextsent>the communicator common task presents special challenges.
</nextsent>
<nextsent>it is complex task with many subtasks, including the booking of each flight, hotel, and car reservation.
</nextsent>
<nextsent>because the number of legs of the trip may be arbitrary, the number of such subtasks is not known in advance.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A154">
<title id=" A00-1010.xml">talkntravel a conversational system for air travel planning </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the modules 68 interact with each other via the central hub module of the communicator common architecture.
</prevsent>
<prevsent>the speech recognizer is the byblos system (nguyen, 1995).
</prevsent>
</prevsection>
<citsent citstr=" H94-1010 ">
it uses an acoustic model trained from the macrophone telephone corpus, and bigram/trigram language model trained from -40k utterances derived from various sources, including data collected under the previous atis program (dahl et al 1994).<papid> H94-1010 </papid></citsent>
<aftsection>
<nextsent>the speech synthesizer is lucent commercial system.
</nextsent>
<nextsent>synthesizer and recognizer both interface to the telephone via dialogics telephony board.
</nextsent>
<nextsent>the database is currently frozen snapshot of actual flights between 40 different us cities (we are currently engaged in interfacing to commercial air travel website).
</nextsent>
<nextsent>the various language components are written in java.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A155">
<title id=" A00-1014.xml">mimic an adaptive mixed initiative spoken dialogue system for information queries </title>
<section> text-to-speech engine: the tts system receives.  </section>
<citcontext>
<prevsection>
<prevsent>we used the bell labs tts system (sproat, 1998), which in addition to converting plain text into speech, ac-cepts text strings annotated to override default pitch height, accent placement, speaking rate, etc. 2 3.1 semantic interpretation.
</prevsent>
<prevsent>mimic utilizes non-recursive frame-based semantic representation commonly used in spoken dialogue sys-tems (e.g.
</prevsent>
</prevsection>
<citsent citstr=" H91-1014 ">
(seneff et al, 1991; <papid> H91-1014 </papid>lamel, 1998)), which represents an utterance as set of attribute-value pairs.</citsent>
<aftsection>
<nextsent>figure 2(a) shows the frame-based semantic representa-tion for the utterance  what time is analyze this playing 2 see (nakatani and chu-carroll, 2000) for how mimic dialogue-.
</nextsent>
<nextsent>level knowledge is used to override default prosodic assignments for concept-to-speech generation.
</nextsent>
<nextsent>question-type: when movie: analyze this theater: null town: montclair (a) semantic representation question-type: when movie: mandatory theater: mandatory town: optional (b) task specification figure 2: semantic representation d task specifica-tion in montclair?
</nextsent>
<nextsent>mimic semantic representation is constructed by first extracting, for each attribute, set of keywords from the user utterance.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A156">
<title id=" A00-1014.xml">mimic an adaptive mixed initiative spoken dialogue system for information queries </title>
<section> text-to-speech engine: the tts system receives.  </section>
<citcontext>
<prevsection>
<prevsent>({speaker}) = 0.7; md . . .
</prevsent>
<prevsent>(o) = 0.3 (b)dialogue initiative figure 3: cues and bpas for modeling initiative in mimic seleet-goal(semrep): (1) ifambiguousaction detected (2) ambiguous-attr +--get-ambiguous(semrep) /* get name of ambiguous attribute */ (3) if (number-values(ambiguous-attr) == 0) /* attribute unspecified *,1 (4) instantiate(ambiguous-attr) (5) else/* more than one value specified */ (6) constrain(ambiguous-attr) (7) else if lnvalidaction detected (8) providenegativeanswer(semrep) (9) else/* well-formed query */ (10) answer +-- database-query(semrep) (11 ) provideanswer(answer) figure 4: goal selection algorithm user queries (steps 1-8) 5 (van beeket al., 1993; raskutti and zukerman, 1993; qu and beale, 1999), and 2) pro-viding answers to well-formed queries (steps 9-11).
</prevsent>
</prevsection>
<citsent citstr=" P88-1015 ">
3.2.3 strategy selection previous work has argued that initiative affects the de-gree of control an agent has in the dialogue interaction (whittaker and stenton, 1988; <papid> P88-1015 </papid>walker and whittaker, 1990; <papid> P90-1010 </papid>chu-carroll and brown, 1998).</citsent>
<aftsection>
<nextsent>thus, cooper-ative system may adopt different strategies to achieve the same goal depending on the initiative distribution.
</nextsent>
<nextsent>since task initiative models contribution to domain/problem- solving goals, while dialogue initiative affects the cur- 5an alternative strategy to step (4) is to perform adatabase lookup based on the ambiguous query and summarize the results (litman et al., 1998), <papid> P98-2129 </papid>which we leave for future work.</nextsent>
<nextsent>rent discourse goal, we developed alternative strategies for achieving the goals in figure 4 based on initiative distribution, as shown in table 1.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A157">
<title id=" A00-1014.xml">mimic an adaptive mixed initiative spoken dialogue system for information queries </title>
<section> text-to-speech engine: the tts system receives.  </section>
<citcontext>
<prevsection>
<prevsent>({speaker}) = 0.7; md . . .
</prevsent>
<prevsent>(o) = 0.3 (b)dialogue initiative figure 3: cues and bpas for modeling initiative in mimic seleet-goal(semrep): (1) ifambiguousaction detected (2) ambiguous-attr +--get-ambiguous(semrep) /* get name of ambiguous attribute */ (3) if (number-values(ambiguous-attr) == 0) /* attribute unspecified *,1 (4) instantiate(ambiguous-attr) (5) else/* more than one value specified */ (6) constrain(ambiguous-attr) (7) else if lnvalidaction detected (8) providenegativeanswer(semrep) (9) else/* well-formed query */ (10) answer +-- database-query(semrep) (11 ) provideanswer(answer) figure 4: goal selection algorithm user queries (steps 1-8) 5 (van beeket al., 1993; raskutti and zukerman, 1993; qu and beale, 1999), and 2) pro-viding answers to well-formed queries (steps 9-11).
</prevsent>
</prevsection>
<citsent citstr=" P90-1010 ">
3.2.3 strategy selection previous work has argued that initiative affects the de-gree of control an agent has in the dialogue interaction (whittaker and stenton, 1988; <papid> P88-1015 </papid>walker and whittaker, 1990; <papid> P90-1010 </papid>chu-carroll and brown, 1998).</citsent>
<aftsection>
<nextsent>thus, cooper-ative system may adopt different strategies to achieve the same goal depending on the initiative distribution.
</nextsent>
<nextsent>since task initiative models contribution to domain/problem- solving goals, while dialogue initiative affects the cur- 5an alternative strategy to step (4) is to perform adatabase lookup based on the ambiguous query and summarize the results (litman et al., 1998), <papid> P98-2129 </papid>which we leave for future work.</nextsent>
<nextsent>rent discourse goal, we developed alternative strategies for achieving the goals in figure 4 based on initiative distribution, as shown in table 1.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A158">
<title id=" A00-1014.xml">mimic an adaptive mixed initiative spoken dialogue system for information queries </title>
<section> text-to-speech engine: the tts system receives.  </section>
<citcontext>
<prevsection>
<prevsent>3.2.3 strategy selection previous work has argued that initiative affects the de-gree of control an agent has in the dialogue interaction (whittaker and stenton, 1988; <papid> P88-1015 </papid>walker and whittaker, 1990; <papid> P90-1010 </papid>chu-carroll and brown, 1998).</prevsent>
<prevsent>thus, cooper-ative system may adopt different strategies to achieve the same goal depending on the initiative distribution.</prevsent>
</prevsection>
<citsent citstr=" P98-2129 ">
since task initiative models contribution to domain/problem- solving goals, while dialogue initiative affects the cur- 5an alternative strategy to step (4) is to perform adatabase lookup based on the ambiguous query and summarize the results (litman et al., 1998), <papid> P98-2129 </papid>which we leave for future work.</citsent>
<aftsection>
<nextsent>rent discourse goal, we developed alternative strategies for achieving the goals in figure 4 based on initiative distribution, as shown in table 1.
</nextsent>
<nextsent>the strategies employed when mimic has only dia-logue initiative are similar to the mixed initiative dia-logue strategies employed by many existing spoken di-alogue systems (e.g., (bennacef et al, 1996; stent et al., 1999)).<papid> P99-1024 </papid></nextsent>
<nextsent>to instantiate an attribute, mimic adopts the lnfoseek dialogue act to solicit the missing informa- tion.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A159">
<title id=" A00-1014.xml">mimic an adaptive mixed initiative spoken dialogue system for information queries </title>
<section> text-to-speech engine: the tts system receives.  </section>
<citcontext>
<prevsection>
<prevsent>since task initiative models contribution to domain/problem- solving goals, while dialogue initiative affects the cur- 5an alternative strategy to step (4) is to perform adatabase lookup based on the ambiguous query and summarize the results (litman et al., 1998), <papid> P98-2129 </papid>which we leave for future work.</prevsent>
<prevsent>rent discourse goal, we developed alternative strategies for achieving the goals in figure 4 based on initiative distribution, as shown in table 1.</prevsent>
</prevsection>
<citsent citstr=" P99-1024 ">
the strategies employed when mimic has only dia-logue initiative are similar to the mixed initiative dia-logue strategies employed by many existing spoken di-alogue systems (e.g., (bennacef et al, 1996; stent et al., 1999)).<papid> P99-1024 </papid></citsent>
<aftsection>
<nextsent>to instantiate an attribute, mimic adopts the lnfoseek dialogue act to solicit the missing informa-tion.
</nextsent>
<nextsent>in contrast, when mimic has both initiatives, it plays more active role by presenting the user with addi-tional information comprising valid instantiations of the attribute (giveoptions).
</nextsent>
<nextsent>given an invalid query, mimic notifies the user of the failed query and provides an open- ended prompt when it only has dialogue initiative.
</nextsent>
<nextsent>when mimic has both initiatives, however, in addition to no- tifyfailure, it suggests an alternative close to the user original query and provides limited prompt.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A160">
<title id=" A00-1014.xml">mimic an adaptive mixed initiative spoken dialogue system for information queries </title>
<section> system evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>each experiment involved eight users interacting with mimic and mimic-si or mimic-mi to perform aset of tasks, each requiring the user to obtain specific movie in-formation.
</prevsent>
<prevsent>user satisfaction was assessed by asking the subjects to fill out questionnaire after interacting with each version of the system.
</prevsent>
</prevsection>
<citsent citstr=" P97-1035 ">
furthermore, number of performance atures, largely based on the paradise dialogue valuation scheme (walker et al, 1997), <papid> P97-1035 </papid>were automatically logged, derived, or manually annotated.</citsent>
<aftsection>
<nextsent>in addition, we logged the cues automatically detected in each user utterance, as well as the initiative distribution for each turn and the dialogue acts selected to generate each system response.
</nextsent>
<nextsent>the features gathered from the dialogue interactions were analyzed along three dimensions: system perfor-mance, discourse features (in terms of characteristics of the resulting dialogues, such as the cues detected in user utterances), and initiative distribution.
</nextsent>
<nextsent>our results show that mimic adaptation capabilities 1) led to bet-ter system performance in terms of user satisfaction, dia-logue efficiency (shorter dialogues), and dialogue quality (fewer asr timeouts), and 2) better matched user expec-tations (by giving up task initiative when the user intends to have control of the dialogue interaction) and more effi-ciently resolved ialogue anomalies (by taking over task initiative to provide guidance when no progress is made in the dialogue, or to constrain user utterances when asr performance is poor).
</nextsent>
<nextsent>in this paper, we discussed mimic, an adaptive mixed- initiative spoken dialogue system.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A161">
<title id=" A97-1043.xml">an automatic extraction of key paragraphs based on context dependency </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>one is knowledge-based approach with particular subject fields (reimer, 1988), (jacobs, 1990).
</prevsent>
<prevsent>this approach, based on deep knowledge of particular subject fields, is useful for restricted tasks, such as, for example, the construc-tion of  weather forecasts  summaries.
</prevsent>
</prevsection>
<citsent citstr=" C96-2166 ">
however, when unrestricted subject matter must be treated, as is often the case in practice, the passage retrieval and text summarisation methods proposed have not proven equal to the need, since deep knowledge of particular subject fields is required (paice, 1990), (zechner, 1996).<papid> C96-2166 </papid></citsent>
<aftsection>
<nextsent>the other, alternative strategy is the approach that relies mainly on corpus statistics (paice, 1990), (palce, 1993).
</nextsent>
<nextsent>the main task of this approach is the sentence scoring process.
</nextsent>
<nextsent>typically, weights are assigned to the individual words in text, and the complete sentence scores are then based on the occurrence characteristics of highly-weighted terms (keywords) in the respective sentences.
</nextsent>
<nextsent>term weighting technique has been widely inves-tigated in information retrieval and lots of tech-niques such as location heuristics (baxendale, 1958), rhetorical relations (miike, 1994), and title informa-tion (edmundson, 1969) have been proposed.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A167">
<title id=" A97-1043.xml">an automatic extraction of key paragraphs based on context dependency </title>
<section> term weighting.  </section>
<citcontext>
<prevsection>
<prevsent>every sense of words in articles for extracting key paragraphs is automatically disambiguated in ad-vance.
</prevsent>
<prevsent>this is because to disambiguate word-senses in articles might affect the accuracy of context de-pendent (domain specific) key paragraphs retrieval, since the meaning of word characterises the do- main in which it is used.
</prevsent>
</prevsection>
<citsent citstr=" C92-2070 ">
word-sense disambigua-tion (wsd in short) is serious problem for nlp, and variety of approaches have been proposed for solving it (brown, 1991), (yarowsky, 1992).<papid> C92-2070 </papid></citsent>
<aftsection>
<nextsent>our disambignation method is based on niwa method which uses the similarity between sentence contain-ing polysemous noun and sentence of dictionary- definition (niwa, 1994).
</nextsent>
<nextsent>furthermore, we linked nouns which are disambignated with their seman-tically similar nouns mainly in order to cope with the problem of phrasal exicon.
</nextsent>
<nextsent>a phrasal exicon such as atlantic seaboard, new england gives neg-ative influence for keywords retrieval, since it can not be regarded as units, i.e. each word which is the element of phrasal exicon is assigned to each semantic ode (fukumoto, 1996).
</nextsent>
<nextsent>to the results of wsd and linking methods, we then applied term weighting method to ex-tract keywords.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A172">
<title id=" A97-1011.xml">a non projective dependency parser </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>64 the parser corresponds to over three man-years of work, which does not include the lexical analyser and the morphological disambiguator, both parts of the existing english constraint grammar parser (karls- sonet al, 1995).
</prevsent>
<prevsent>the parsers can be tested via www .
</prevsent>
</prevsection>
<citsent citstr=" C90-3030 ">
our work is partly based on the work done with the constraint grammar framework that was orig-inally proposed by fred karlsson (1990).<papid> C90-3030 </papid></citsent>
<aftsection>
<nextsent>a de- tmled description of the english constraint gram-mar (engcg) is in karlsson et al (1995).
</nextsent>
<nextsent>the basic rule types of the constraint grammar (tapanainen, 1996) 2 are remove and select for discarding and se-lecting an alternative reading of word.
</nextsent>
<nextsent>rules also have contextual tests that describe the condition ac-cording to which they may be applied.
</nextsent>
<nextsent>for example, the rule remove (v) if (-1c det); discards verb (v) reading if the preceding word (-1) is unambiguously (c) determiner (det).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A173">
<title id=" A97-1011.xml">a non projective dependency parser </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>for instance, select (imp) if (not *-1 nom-head); means that nominal head (nom-head is set that contains part-of-speech tags that may represent nominal head) may not appear anywhere to the left (not *-1).
</prevsent>
<prevsent>~at http://www.ling.helsinki.f /~tapanain/dg/ ~the cg-2 notation here (tapanainen, 1996) is dif-ferent from the former (karlsson et al, 1995).
</prevsent>
</prevsection>
<citsent citstr=" C96-1096 ">
a con-cise introduction to the formalism is also to be found in samuelsson et al (1996) and hurskainen (1996).<papid> C96-1096 </papid></citsent>
<aftsection>
<nextsent>this  anywhere  to the left or right may be re-stricted by barriers, which restrict the area of the test.
</nextsent>
<nextsent>basically, the barrier can be used to limit the test only to the current clause (by using clause boundary markers and  stop words ) or to con-stituent (by using  stop categories ) instead of the whole sentence.
</nextsent>
<nextsent>in addition, another test may be added relative to the unrestricted context position using keyword link.
</nextsent>
<nextsent>for example, the following rule discards the syntactic function 3 qi-0bj (indirect ob- ject): remove (@i-oej) if (*-1c vfin barrier svoo link not 0 svoo); the rule holds if the closest finite verb to the left is unambiguously (c) finite verb (vfin), and there is no ditransitive verb or participle (subcategorisation sv00) between the verb and the indirect object.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A174">
<title id=" A97-1011.xml">a non projective dependency parser </title>
<section> dependency  grammars  in a.  </section>
<citcontext>
<prevsection>
<prevsent>some early formalisations, c.f.
</prevsent>
<prevsent>(hays, 1964), have brought he strict projectivity (context-free) require-ment into the dependency framework.
</prevsent>
</prevsection>
<citsent citstr=" C96-1058 ">
this kind of restriction is present in many dependency-based parsing systems (mccord, 1990; sleator and tem-perley, 1991; eisner, 1996).<papid> C96-1058 </papid></citsent>
<aftsection>
<nextsent>but obviously any recognition grammar should deal with non-projective phenomena to the extent they occur in natural anguages as, for example, in the analysis hown in figure 2.
</nextsent>
<nextsent>our system has no in-built restrictions concerning projectivity, though the formalism allows us to state when crossing links are not permitted.
</nextsent>
<nextsent>we maintain that one is generally also interested in the linear order of elements, and therefore it is presented in the tree diagrams.
</nextsent>
<nextsent>but, for some pur-poses, presenting all arguments in canonical order might be more adequate.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A175">
<title id=" C00-1079.xml">representation and recognition method for multiword translation units in koreantojapanese mt system </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>as transfer problem in machine translation (mt), lexical and structural differences exist between source and target languages, which requires l-n, m-n, or n-1 mapping strategies for machine translation system.
</prevsent>
<prevsent>for such mapping strategies, we need to treat several (n, or m) words (or morphemes) as single translation unit.
</prevsent>
</prevsection>
<citsent citstr=" C90-2057 ">
although some researches (d.santos,1990; <papid> C90-2057 </papid>linden e.,1990; yoon sung hoe, 1992; ha gyu lee, 1994; d.arnold,1994) employ the term  idiom  for these units, we prefer mwtu (multi-word translation unit) because it is more general and broader term for mt environment.</citsent>
<aftsection>
<nextsent>up to now, some reseamh as focused on recognition and transfer of mwtus, although very little research has been undertaken for korean-to-japanese machine translation systems (seen-he kim,1997).
</nextsent>
<nextsent>in previous researches, some tended to simplify the problem by treating only special types of mwtus, while others had some recognition errors and took too much recognition time because they did not restrict he recognition scope (d.santos,1990; <papid> C90-2057 </papid>yoon sung hee,1992; ha gyu lee, 1994; seen-he kim, 1997).</nextsent>
<nextsent>for korean-to-english mt, lee and kim (ha gyu lee,1994) uses only weak restrictions like adjacent inforlnation for recognition scope.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A179">
<title id=" A88-1033.xml">combinatorial disambiguation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>this approach is interesting, although some details are vague 3.
</prevsent>
<prevsent>l\[owevcr, the post-parse application of a* described in this paper obtains the benefits of such within-parse approach without its deficiences in that: (a) combinatorial computations of weighcs and word- sense consistencies are avoided except when warranted by total sentence informal ion, and (b) there is no pos-sibility of early erroneous discarding of allerrmtives.
</prevsent>
</prevsection>
<citsent citstr=" P82-1015 ">
heidorn (1982) <papid> P82-1015 </papid>provides good summary of early work in weight-based analysis, as well as weight-oriented approach to attachment decisions based on syntactic onside rations only.</citsent>
<aftsection>
<nextsent>no examples are given, so it is unclear whether parse for phrase or part thereof represents only one interpretation, or all interpretations having the same structure, scored by the most likely interpretation.
</nextsent>
<nextsent>the former is obviously inadequate (c.g., for highly ambiguous ubject nps like *the stands ), while the latter seems to require either the calculation of all alternative cumulative scores, or recalculation of scores if an interpretation fails.
</nextsent>
<nextsent>246 he bucks sub~ he shoo( obj buck shoot with r f e i th i l shoot with i l buck figure 3:syntactic choice poin~ one other parser-based work should be noted, that of wittenburg (1986), as it is explicitly based on a*.
</nextsent>
<nextsent>the intent and content of the method is quite different from that described here.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A180">
<title id=" A00-1009.xml">a framework for mt and multilingual nlg systems based on uniform lexico structural processing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this paper we present linguistically motivated framework for uniform lexico- structural processing.
</prevsent>
<prevsent>it has been used for transformations of conceptual and syntactic structures during generation monolingual nd multilingual natural language generation (nlg) and for transfer in machine translation (mt).
</prevsent>
</prevsection>
<citsent citstr=" J85-1003 ">
our work extends directions taken in systems such as ariane (vauquois and boitet, 1985), <papid> J85-1003 </papid>fog (kittredge and polgu6re, 1991), joyce (rainbow and korelsky, 1992), and lfs (iordanskaja et al, 1992).<papid> C92-3158 </papid></citsent>
<aftsection>
<nextsent>although it adopts the general principles found in the above-mentioned systems, the approach presented in this paper is more practical, and we believe, would eventually integrate better with emerging statistics-based approaches tomt.
</nextsent>
<nextsent>* the work performed on the framework by this co-author was done while at cog entex, inc. the framework consists of portable java environment for building nlg or mt applications by defining modules using core tree transduction engine and single declarative ascii specification language for conceptual or syntactic dependency tree structures 1 and their transformations.
</nextsent>
<nextsent>developers can define new modules, add or remove modules, or modify their connections.
</nextsent>
<nextsent>because the processing of the transformation engine is restricted to transduction of trees, it is computationally efficient.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A181">
<title id=" A00-1009.xml">a framework for mt and multilingual nlg systems based on uniform lexico structural processing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this paper we present linguistically motivated framework for uniform lexico- structural processing.
</prevsent>
<prevsent>it has been used for transformations of conceptual and syntactic structures during generation monolingual nd multilingual natural language generation (nlg) and for transfer in machine translation (mt).
</prevsent>
</prevsection>
<citsent citstr=" C92-3158 ">
our work extends directions taken in systems such as ariane (vauquois and boitet, 1985), <papid> J85-1003 </papid>fog (kittredge and polgu6re, 1991), joyce (rainbow and korelsky, 1992), and lfs (iordanskaja et al, 1992).<papid> C92-3158 </papid></citsent>
<aftsection>
<nextsent>although it adopts the general principles found in the above-mentioned systems, the approach presented in this paper is more practical, and we believe, would eventually integrate better with emerging statistics-based approaches tomt.
</nextsent>
<nextsent>* the work performed on the framework by this co-author was done while at cog entex, inc. the framework consists of portable java environment for building nlg or mt applications by defining modules using core tree transduction engine and single declarative ascii specification language for conceptual or syntactic dependency tree structures 1 and their transformations.
</nextsent>
<nextsent>developers can define new modules, add or remove modules, or modify their connections.
</nextsent>
<nextsent>because the processing of the transformation engine is restricted to transduction of trees, it is computationally efficient.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A182">
<title id=" A00-1009.xml">a framework for mt and multilingual nlg systems based on uniform lexico structural processing </title>
<section> the framework representations.  </section>
<citcontext>
<prevsection>
<prevsent>the dsyntss and ssyntss correspond closely to the equivalent structures of the meaning-text theory (mtt; mel cuk, 1988): both structures are unordered syntactic representations, but dsynts only includes full meaning-bearing lexemes while ssynts also contains function words such as determiners, auxiliaries, and strongly governed prepositions.
</prevsent>
<prevsent>in the implemented applications, the dsyntss are the pivotal representations involved in most transformations, as this is also often the case in practice in linguistic-based mt (hutchins and somers, 1997).
</prevsent>
</prevsection>
<citsent citstr=" A97-1039 ">
figure 2 illustrates dsynts from meteorological application, meteocogent (kittredge and lavoie, 1998), represented using the standard graphical notation and also the realpro ascii notation used internally in the framework (lavoie and rambow, 1997).<papid> A97-1039 </papid></citsent>
<aftsection>
<nextsent>as figure 2 illustrates, there is straightforward mapping between the graphical notation and the ascii notation supported in the framework.
</nextsent>
<nextsent>this also applies for all the transformation rules in the framework which illustrates the declarative nature of our approach, 1 low -5 to  low ( i~r -5 attr to ( il high ( \]i~r 20 ) ) ) low -s to high 20 figure 2: dsynts (graphical nd ascii notation) the concss correspond to the standard frame- like structures used in knowledge representation, with labeled arcs corresponding to slots.
</nextsent>
<nextsent>we have used them only for very limited meteorological domain (in meteocogent), and we imagine that they will typically be defined in domain-specific manner.
</nextsent>
<nextsent>figure 3 illustrates the mapping between an interlingua defined as concs and corresponding english dsynts.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A184">
<title id=" A00-1009.xml">a framework for mt and multilingual nlg systems based on uniform lexico structural processing </title>
<section> the framework linguistic resources.  </section>
<citcontext>
<prevsection>
<prevsent>figure 8 gives an english-french transfer ule applied to weather domain for the transfer of verb modified by the adverb almost: it almost rained.
</prevsent>
<prevsent>--o ii fail i pleuvoir.
</prevsent>
</prevsection>
<citsent citstr=" J94-4004 ">
transfer-rule: sx \[ lass :verb \] ( attr almost )  - -  fa ill ir \[ lass :verb \] ( i sx \[ mood: in \] ) figure 8: english to french lexico-structural transfer rule with verb modifier almost more details on how the structural divergences described in (dorr, 1994) <papid> J94-4004 </papid>can be accounted for using our formalism can be found in (nasr et al., 1998).</citsent>
<aftsection>
<nextsent>before being processed, the rules are first compiled and indexed for optimisation.
</nextsent>
<nextsent>each module applies the following processing.
</nextsent>
<nextsent>the rules are assumed to be ordered from most specific to least specific.
</nextsent>
<nextsent>the application of the rules to the structures top-down in recursive way from the n-st rule to the last.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A185">
<title id=" A00-1009.xml">a framework for mt and multilingual nlg systems based on uniform lexico structural processing </title>
<section> using the framework to build applications.  </section>
<citcontext>
<prevsection>
<prevsent>i concs concepmd suar.tm~ ssynts suffaee:syntnetlc su uet~ os~ts t~sy~ac~ psy~s ~d:~n~c figure 9: scope of the framework transformations for example, in figure 9, starting with the  input sentence li  and passing through parsing, conversion, transfer, dsynts realization and ssynts realization to  generated sentence l2  we obtain an ll-to-l2 mt system.
</prevsent>
<prevsent>starting with  sentence planning  and passing through dsynts realization, and ssynts realization (including linearization and inflection) to  generated sentence li , we obtain monolingual nlg system for l1.
</prevsent>
</prevsection>
<citsent citstr=" A97-1037 ">
so far the framework has been used successfully for building wide variety of applications in different domains and for different languages: nlg: ? realization of english dsyntss via ssynts level for the domains of meteorology (meteocogent; kittredge and lavoie, 1998) and object modeling (modelexplainer; lavoie et al, 1997).<papid> A97-1037 </papid></citsent>
<aftsection>
<nextsent>generation of english text from conceptual interlingua for the meteorology domain (meteocogent).
</nextsent>
<nextsent>(the design of the interlingua can also support he generation of french but this functionality has not yet been implemented.)
</nextsent>
<nextsent>mt: ? transfer on the dsynts level and realization via ssynts level for english--french, english--arabic, english---korean and korean--english.
</nextsent>
<nextsent>translation in the meteorology and battlefield omains (nasr et al, 1998).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A187">
<title id=" A00-1009.xml">a framework for mt and multilingual nlg systems based on uniform lexico structural processing </title>
<section> history of the framework and comparison.  </section>
<citcontext>
<prevsection>
<prevsent>the current framework offers no support for merging handcrafted rules with new lexical rules obtained statistically while preserving the valid handcrafted changes and deleting the invalid ones.
</prevsent>
<prevsent>in general, better integration of linguistically based and statistical methods during all the development phases is greatly needed.
</prevsent>
</prevsection>
<citsent citstr=" A92-1006 ">
with other systems the framework represents generalization of several predecessor nlg systems based on meaning-text theory: fog (kittredge and polgu~re, 1991), lfs (iordanskaja et al, 1992), <papid> C92-3158 </papid>and joyce (rambow and korelsky, 1992).<papid> A92-1006 </papid></citsent>
<aftsection>
<nextsent>the framework was originally developed for the realization of deep-syntactic structures in nlg (lavoie and rambow, 1997).<papid> A97-1039 </papid></nextsent>
<nextsent>it was later extended for generation of deep-syntactic structures from conceptual interlingua (kittredge and lavoie, 1998).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A190">
<title id=" C00-1080.xml">chart parsing and constraint programming </title>
<section> i t roduct ion   </section>
<citcontext>
<prevsection>


</prevsection>
<citsent citstr=" P83-1021 ">
the parsing-as-deduction approach proposed in pereira and warren (1983) <papid> P83-1021 </papid>and exlended in shieber et al (1995) and the parsing schemala detincd in sikkel (1997) are well established parsing paradigms in computalional lin- guistics.</citsent>
<aftsection>
<nextsent>their main slrengths are iheir llexibility and lhe level of abstraction concerning control informal\]o,1 inherenl in parsing algorithms, lvurlhermore, lhcy are easily exlcnsible in more complex formalisms, e.g., at\]g- mcntcd phrase struclure rules or the id/lp formal.
</nextsent>
<nextsent>constraint programming (cp) has been used in com-putational linguislics in several areas, for example in (typed) featut e-lmsed systems (smolka, 1995), or condio tional constraints (matiasek, 1994), or adwmccd compi-lation techniques (g6tz and meurcrs, 1997) or special-ized constraint solvers (manandhar, 1994).<papid> P94-1035 </papid></nextsent>
<nextsent>but none of these approaches uses constraint programming tech-niques lo implement standard chart parsing algorithnls directly in constraint system.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A191">
<title id=" C00-1080.xml">chart parsing and constraint programming </title>
<section> i t roduct ion   </section>
<citcontext>
<prevsection>
<prevsent>the parsing-as-deduction approach proposed in pereira and warren (1983) <papid> P83-1021 </papid>and exlended in shieber et al (1995) and the parsing schemala detincd in sikkel (1997) are well established parsing paradigms in computalional lin- guistics.</prevsent>
<prevsent>their main slrengths are iheir llexibility and lhe level of abstraction concerning control informal\]o,1 inherenl in parsing algorithms, lvurlhermore, lhcy are easily exlcnsible in more complex formalisms, e.g., at\]g- mcntcd phrase struclure rules or the id/lp formal.</prevsent>
</prevsection>
<citsent citstr=" P94-1035 ">
constraint programming (cp) has been used in com-putational linguislics in several areas, for example in (typed) featut e-lmsed systems (smolka, 1995), or condio tional constraints (matiasek, 1994), or adwmccd compi-lation techniques (g6tz and meurcrs, 1997) or special-ized constraint solvers (manandhar, 1994).<papid> P94-1035 </papid></citsent>
<aftsection>
<nextsent>but none of these approaches uses constraint programming tech-niques lo implement standard chart parsing algorithnls directly in constraint system.
</nextsent>
<nextsent>in this papel; will bring these two tmmdigms to-gether by showing how to implement algorithn\]s fl om the parsing-as-deduction sctmme by viewing the parsing process as constraint propagation.
</nextsent>
<nextsent>the core idea is that the items of conventional chart parser are constraints on labeled links between the words and positions of an input string.
</nextsent>
<nextsent>then tile inference rules allow for the deduction of new constraints, again labeled and spanning parts of tim input siring, via constraint propagation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A192">
<title id=" C00-2169.xml">processing self corrections in a speech to speech system </title>
<section> i t roduct ion   </section>
<citcontext>
<prevsection>
<prevsent>special thanks to anton batliner, richard iluber and volker warnke.
</prevsent>
<prevsent>~a more detailed introduction is given by (brown el, al., 1990) 117 formul;~ted as: 5~  = argmaxti   (t ls ) this is reformulated by bayes  law for better search space reduction, but we are only inter-ested in the conditional probability p(tis ).
</prevsent>
</prevsection>
<citsent citstr=" J90-2002 ">
for further processing steps we have to introduce the concept of alignment (brown et al, 1990).<papid> J90-2002 </papid></citsent>
<aftsection>
<nextsent>let be the word sequence s1, 2 . . .
</nextsent>
<nextsent>5,l ~ si and = ~,t2.
</nextsent>
<nextsent>.tm ~ 77\] ~.
</nextsent>
<nextsent>we can link word in to word in s. this reflects the assumption that the word in is translated from the word in s. \]or example, if is  on thursday  and is  am l)onnerstag   am  can be linked to  on  but also to  thursday .
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A193">
<title id=" C00-2169.xml">processing self corrections in a speech to speech system </title>
<section> i t roduct ion   </section>
<citcontext>
<prevsection>
<prevsent>train-ing and testing were done on two separated parts of the german part of the verb mobil cor-pus (12558 turns training / 1737 turns test).
</prevsent>
<prevsent>detection correct scope recall precision recall precision test 1 49% 70% 47 % 70% test 2 71% 85% 62% 83% direct comparison to other groups is rather difficult due to very different corpora, eval-uation conditions and goals.
</prevsent>
</prevsection>
<citsent citstr=" P93-1007 ">
(nakatani and hirschberg, 1.993) <papid> P93-1007 </papid>suggest acoustic/prosodic detector to identify ips but don discuss the problem of finding the correct segmentation depth.</citsent>
<aftsection>
<nextsent>also their results are obtained on corpus where every utterance contains at least one repair.
</nextsent>
<nextsent>(shriberg, 1994) also addresses the acoustic aspects of repairs.
</nextsent>
<nextsent>parsing approaches like in (bear et al, 1992; <papid> P92-1008 </papid>itindle, 1983; core and schubert, 1999) must be proved to work with lattices rather than transliterated text.</nextsent>
<nextsent>an al-gorithm which is inherently capable of lattice processing is prot)osed by heeman (hem-nan, 1997).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A194">
<title id=" C00-2169.xml">processing self corrections in a speech to speech system </title>
<section> i t roduct ion   </section>
<citcontext>
<prevsection>
<prevsent>also their results are obtained on corpus where every utterance contains at least one repair.
</prevsent>
<prevsent>(shriberg, 1994) also addresses the acoustic aspects of repairs.
</prevsent>
</prevsection>
<citsent citstr=" P92-1008 ">
parsing approaches like in (bear et al, 1992; <papid> P92-1008 </papid>itindle, 1983; core and schubert, 1999) must be proved to work with lattices rather than transliterated text.</citsent>
<aftsection>
<nextsent>an al-gorithm which is inherently capable of lattice processing is prot)osed by heeman (hem-nan, 1997).
</nextsent>
<nextsent>he redefines the word recognition prob-lem to identify the best sequence of words, cor-responding pos tags and special rel)air tags.
</nextsent>
<nextsent>he reports recall rate of 81% and precision of 83% for detection and 78%/80% tbr correc-tion.
</nextsent>
<nextsent>the test settings are nearly the same as test 2.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A195">
<title id=" A00-1029.xml">a tool for automated revision of grammars for nlp systems </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for transaction processing systems, mis recognized words can lead to unintended transactions being processed.
</prevsent>
<prevsent>an effective constraining rammar can reduce transactional errors by limiting the number of sentence vel errors.
</prevsent>
</prevsection>
<citsent citstr=" J92-1004 ">
the problem of over-generalization speech grammars and related issues is well discussed by seneff (1992).<papid> J92-1004 </papid></citsent>
<aftsection>
<nextsent>thus, speech grammars must often balance the conflicting requirements of ? accepting wide variety of sentences to increase flexibility, and ? accepting small number of sentences to increase system accuracy and robustness.
</nextsent>
<nextsent>developing tight grammars which trade-off these conflicting constraints is tedious and accepted by grammar is the set of all terminal strings that can be generated from the start symbol by successive application of the production rules.
</nextsent>
<nextsent>the grammar may optionally have semantic interpretation rules associated with each production rule (e.g. see (allen 95)).
</nextsent>
<nextsent>210 difficult process.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A196">
<title id=" A94-1009.xml">does baum welch reestimation help taggers </title>
<section> abstract </section>
<citcontext>
<prevsection>
<prevsent>in part of speech tagging by hidden markov model, statistical model is used to assign grammatical categories to words in text.
</prevsent>
<prevsent>early work in the field re-lied on corpus which had been tagged by human annotator to train the model.
</prevsent>
</prevsection>
<citsent citstr=" A92-1018 ">
more recently, cutting et al (1992) <papid> A92-1018 </papid>sug-gest that training can be achieved with minimal lexicon and limited amount of priori information about probabilities, by using an baum-welch re-estimation to automatically refine the model.</citsent>
<aftsection>
<nextsent>in this paper, report two experiments designed to determine how much manual training information is needed.
</nextsent>
<nextsent>the first exper-iment suggests that initial biasing of ei-ther lexical or transition probabilities es-sential to achieve good accuracy.
</nextsent>
<nextsent>the second experiment reveals that there are three distinct patterns of baum-welch re-estimation.
</nextsent>
<nextsent>in two of the patterns, the re-estimation ultimately reduces the accu-racy of the tagging rather than improving it.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A197">
<title id=" A94-1009.xml">does baum welch reestimation help taggers </title>
<section> abstract </section>
<citcontext>
<prevsection>
<prevsent>the pattern which is applicable can be predicted from the quality of the ini-tial model and the similarity between the tagged training corpus (if any) and the cor-pus to be tagged.
</prevsent>
<prevsent>heuristics for decid-ing how to use re-estimation an effec-tive manner are given.
</prevsent>
</prevsection>
<citsent citstr=" J94-2001 ">
the conclusions are broadly in agreement with those of meri-aldo (1994), <papid> J94-2001 </papid>but give greater detail about the contributions of different parts of the model.</citsent>
<aftsection>
<nextsent>part-of-speech tagging is the process of assigning grammatical categories to individual words in cor-pus.
</nextsent>
<nextsent>one widely used approach makes use of statistical technique called hidden markov model (hmm).
</nextsent>
<nextsent>the model is defined by two collections of parameters: the transition probabilities, which ex-press the probability that tag follows the preceding one (or two for second order model); and the lexical probabilities, giving the probability that word has given tag without regard to words on either side of it.
</nextsent>
<nextsent>to tag text, the tags with non-zero probability are hypothesised for each word, and the most probable sequence of tags given tbe sequence of words is de-termined from the probabilities.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A199">
<title id=" A94-1009.xml">does baum welch reestimation help taggers </title>
<section> background </section>
<citcontext>
<prevsection>
<prevsent>the algorithm is again described by cutting et aland by sharman, and mathemati-cal justification for it can be tbund in huang et al (1990).
</prevsent>
<prevsent>the first major use of hmms for part of speech tagging was in claws (garside et al, 1987) in the 1970s.
</prevsent>
</prevsection>
<citsent citstr=" A88-1019 ">
with the availability of large corpora and fast computers, there has been recent resurgence of interest, and number of variations on and alter- 53 natives to the fb, viterbi and bw algorithms have been tried; see the work of, for example, church (church, 1988), <papid> A88-1019 </papid>brill (brill and marcus, 1992; brill, 1992), <papid> A92-1021 </papid>derose (derose, 1988) <papid> J88-1003 </papid>and gupiec (kupiec, 1992).</citsent>
<aftsection>
<nextsent>one of the most effective taggers based on pure hmm is that developed at xerox (cutting et al., 1992).<papid> A92-1018 </papid></nextsent>
<nextsent>an important aspect of this tagger is that it will give good accuracy with minimal amount of manually tagged training data.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A200">
<title id=" A94-1009.xml">does baum welch reestimation help taggers </title>
<section> background </section>
<citcontext>
<prevsection>
<prevsent>the algorithm is again described by cutting et aland by sharman, and mathemati-cal justification for it can be tbund in huang et al (1990).
</prevsent>
<prevsent>the first major use of hmms for part of speech tagging was in claws (garside et al, 1987) in the 1970s.
</prevsent>
</prevsection>
<citsent citstr=" A92-1021 ">
with the availability of large corpora and fast computers, there has been recent resurgence of interest, and number of variations on and alter- 53 natives to the fb, viterbi and bw algorithms have been tried; see the work of, for example, church (church, 1988), <papid> A88-1019 </papid>brill (brill and marcus, 1992; brill, 1992), <papid> A92-1021 </papid>derose (derose, 1988) <papid> J88-1003 </papid>and gupiec (kupiec, 1992).</citsent>
<aftsection>
<nextsent>one of the most effective taggers based on pure hmm is that developed at xerox (cutting et al., 1992).<papid> A92-1018 </papid></nextsent>
<nextsent>an important aspect of this tagger is that it will give good accuracy with minimal amount of manually tagged training data.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A201">
<title id=" A94-1009.xml">does baum welch reestimation help taggers </title>
<section> background </section>
<citcontext>
<prevsection>
<prevsent>the algorithm is again described by cutting et aland by sharman, and mathemati-cal justification for it can be tbund in huang et al (1990).
</prevsent>
<prevsent>the first major use of hmms for part of speech tagging was in claws (garside et al, 1987) in the 1970s.
</prevsent>
</prevsection>
<citsent citstr=" J88-1003 ">
with the availability of large corpora and fast computers, there has been recent resurgence of interest, and number of variations on and alter- 53 natives to the fb, viterbi and bw algorithms have been tried; see the work of, for example, church (church, 1988), <papid> A88-1019 </papid>brill (brill and marcus, 1992; brill, 1992), <papid> A92-1021 </papid>derose (derose, 1988) <papid> J88-1003 </papid>and gupiec (kupiec, 1992).</citsent>
<aftsection>
<nextsent>one of the most effective taggers based on pure hmm is that developed at xerox (cutting et al., 1992).<papid> A92-1018 </papid></nextsent>
<nextsent>an important aspect of this tagger is that it will give good accuracy with minimal amount of manually tagged training data.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A203">
<title id=" A94-1009.xml">does baum welch reestimation help taggers </title>
<section> background </section>
<citcontext>
<prevsection>
<prevsent>re-estimation on any of the words in class therefore counts to-wards re-estimation for all of them 1.
</prevsent>
<prevsent>the results of the xerox experiment appear very encouraging.
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
preparing tagged corpora either by hand is labour-intensive and potentially error-prone, and although semi-automatic approach can be used (marcus et al, 1993), <papid> J93-2004 </papid>it is good thing to re-duce the human involvement as much as possible.</citsent>
<aftsection>
<nextsent>however, some careful examination of the experi-ment is needed.
</nextsent>
<nextsent>in the first place, cutting et al do not compare the success rate in their work with that achieved from hand-tagged training text with no re-estimation.
</nextsent>
<nextsent>secondly, it is unclear how much the initial biasing contributes the success rate.
</nextsent>
<nextsent>if signif-icant human intervention is needed to provide the biasing, then the advantages of automatic training become rather weaker, especially if such interven-tion is needed on each new text domain.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A204">
<title id=" A94-1009.xml">does baum welch reestimation help taggers </title>
<section> background </section>
<citcontext>
<prevsection>
<prevsent>the kind of biasing cutting et al describe reflects linguistic insights combined with an understanding of the pre-dictions tagger could reasonably be expected to make and the ones it could not.
</prevsent>
<prevsent>the aim of this paper is to examine the role that training plays in the tagging process, by an experi-mental evaluation of how the accuracy of the tagger varies with the initial conditions.
</prevsent>
</prevsection>
<citsent citstr=" H89-1054 ">
the results sug-gest that completely unconstrained initial model does not produce good quality results, and that one 1the technique was originally developed by kupiec (kupiec, 1989).<papid> H89-1054 </papid></citsent>
<aftsection>
<nextsent>54 accurately trained from hand-tagged corpus will generally do better than using an approach based on re-estimation, even when the training comes from different source.
</nextsent>
<nextsent>a second experiment shows that there are different patterns of re-estimation, and that these patterns vary more or less regularly with broad characterisation the initial conditions.
</nextsent>
<nextsent>the outcome of the two experiments together points to heuristics for making effective use of training andre- estimation, together with some directions for further research.
</nextsent>
<nextsent>work similar to that described here has been car-ried out by merialdo (1994), <papid> J94-2001 </papid>with broadly similar conclusions.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A210">
<title id=" C00-1047.xml">a method of measuring term representativeness  baseline method using cooccurrence distribution </title>
<section> e,isting measures of representative~kess  </section>
<citcontext>
<prevsection>
<prevsent>if documents are categorized beforehand, we can use more sophisticated measure based on the x-  test of the hypothesis that an occurrence of  the target word is independent of categories (nagao et al 1976).
</prevsent>
<prevsent>research on automatic term extraction in nlp domains has led to several measures for weighting terms mainly by considering the unithood of word sequence.
</prevsent>
</prevsection>
<citsent citstr=" J93-1003 ">
for instance, mutual information (church ct al. 1990) and the log-likelihood (dunning 1993) <papid> J93-1003 </papid>methods for extracting word bigrams have been widely used.</citsent>
<aftsection>
<nextsent>other measures for calculating the unithood of n-grains have also been proposed (frantzi et al 1996, nakagawa et al 1998, kita et al 1994).
</nextsent>
<nextsent>2.2 problems.
</nextsent>
<nextsent>existing measures uffer from at least one of the following problems: (1) classical measures sucll as t/-idjare so sensitive to term frequencies that they fail to avoid very frequent non-informative words.
</nextsent>
<nextsent>(2) methods using cross-category word distributions (such as the z-  method) can be applied only if documents in corpus are categorized.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A211">
<title id=" C00-1047.xml">a method of measuring term representativeness  baseline method using cooccurrence distribution </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>in this case, the recall and precision of the 1,511 words against all class words were 85% and 50%, respectively.
</prevsent>
<prevsent>when using tf-idf the recall and precision of the first 1,511 words against all class words were 79% and 47%, respectively (note that tj -idfdoes not have clear threshold value, though).
</prevsent>
</prevsection>
<citsent citstr=" C94-1084 ">
although the degree of out-performance by rep(., llr) is not seemingly large, this is promising result because it has been pointed out that, in the related domains of term extraction, existing measures hardly outperform even the use of frequency (for example, daille et al 1994, <papid> C94-1084 </papid>caraballo et al 1999) when we use this type of comparison based on the accumulated numbers.</citsent>
<aftsection>
<nextsent>figure 5 compares, for all the sorting criteria, the accumulated number of words marked by (454 in total), in this case, fewer the number of words is better.
</nextsent>
<nextsent>the difference is far clearer in this case: rep(., llr) obviously outperformed the other measures.
</nextsent>
<nextsent>in contrast, tfidj and frequency barely outperformed random sorting.
</nextsent>
<nextsent>rep(., diffnum) outperformed tfand (f-idfuntil about the first 3,000 mono grams, but under-performed otherwise.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A212">
<title id=" A94-1028.xml">robust text processing in automated information retrieval </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>on the other hand, if we include even simple phrases, such as catastrophic- health program, acute care, home care, and senior citizen, we can considerably sharpen the outcome of the search as seen in the second table) query:104; no.
</prevsent>
<prevsent>relevant:21 rel document rank (no phrases) rank (phrases) wsj890918-0173 2 5 wsj891004-0119 7 1 wsj870723-(k)64 8 8 wsj870213 -(x)53 10 12 wsjgg0608-0121 14 7 wsj891005 -0005 15 4 wsj891009 -0009 35 18 wsj890920-0115 39 26 wsj890928-0184 40 61 wsj880609-0(~l 53 50 wsj891009-0188 73 46 wsj880705-oi94 97 95 wsj870601-0075 52 wsj891005-0001 72 wsj871028-0059 93 query obtained from the fields  rifle ,  desc  and  narr  will be, as may be expected, much weaker than the one using  con  field, especially without he phrasal terms, because the narrative contains far fewer specific terms while containing some that may prove distracting, e.g., firestorm.
</prevsent>
</prevsection>
<citsent citstr=" H93-1071 ">
in fact, broglio and croft (1993), <papid> H93-1071 </papid>and broglio (personal communication, 1993) showed that the exclusion of the  con  field makes the queries quite ineffective, while adding the  narr  field makes them even worse as they lose precision by as much as 30%.</citsent>
<aftsection>
<nextsent>however, adding phrasal terms can improve things considerably.
</nextsent>
<nextsent>we return to this issue later in the paper.
</nextsent>
<nextsent>an accurate syntactic analysis is an essential prerequisite for selection of phrasal terms.
</nextsent>
<nextsent>various sta-tistical methods, e.g., based on word co-occurrences prepositions) am included in the query, including those making up the phrases.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A213">
<title id=" A94-1028.xml">robust text processing in automated information retrieval </title>
<section> overall design.  </section>
<citcontext>
<prevsection>
<prevsent>this may prove particularly troublesome for systems that attempt term clustering in order to create  meta-terms  to be used in document representation.
</prevsent>
<prevsent>we have established the general architecture of nlp-ir system, depicted schematically below, in which an advanced nlp module is inserted between the textual input (new documents, user queries) and the database search engine (in our case, nist prise system).
</prevsent>
</prevsection>
<citsent citstr=" W93-0302 ">
this design has already shown some promise in producing better performance than the base statisti-cal system (strzalkowski, 1993<papid> W93-0302 </papid>b).</citsent>
<aftsection>
<nextsent>we would like to point out at the outset hat this system is completely automated, including the statistical core, and the natural language processing components, and no human inter-vention or manual encoding is required.
</nextsent>
<nextsent>nip: tagger parser terms in our system the database text is first processed with sequence of programs that include part-of- speech tagger, lexicon-based morphological stemmer and fast syntactic parser.
</nextsent>
<nextsent>subsequently certain types of phrases are extracted from the parse trees and used as compound indexing terms in addition to single-word terms.
</nextsent>
<nextsent>the extracted phrases are statistically analyzed as syntactic ontexts in order to discover variety of similarity links between smaller subphrases and words occurring in them.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A217">
<title id=" A94-1028.xml">robust text processing in automated information retrieval </title>
<section> fast parsing with ttp parser.  </section>
<citcontext>
<prevsection>
<prevsent>in runs with approximately 130 million words of trec wall street journal and san jose mercury texts, the parser speed averaged 30 minutes per megabyte or about 80 words per second, on sun sparcstationl0.
</prevsent>
<prevsent>in addition, ip has been shown to produce parse structures which are no worse 170 than those generated by full-scale linguistic parsers when compared to hand-coded parse trees.
</prevsent>
</prevsection>
<citsent citstr=" C92-1033 ">
5 full details of ttp parser have been described in the trec-1 report (strzalkowski, 1993<papid> W93-0302 </papid>a), as well as in other works (strzalkowski, 1992; <papid> C92-1033 </papid>strzalkowski &amp; scheyen, 1993).</citsent>
<aftsection>
<nextsent>as may be expected, the skip-and-fit strategy will only be effective if the input skipping can be per-formed with degree of determinism.
</nextsent>
<nextsent>this means that most of the lexical level ambiguity must be removed from the input text, prior to parsing.
</nextsent>
<nextsent>we achieve this using stochastic parts of speech tagger to pre process the text prior to parsing.
</nextsent>
<nextsent>in order to streamline the pro-cessing, we also perform morphological normalization of words on the tagged text, before parsing.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A218">
<title id=" A94-1028.xml">robust text processing in automated information retrieval </title>
<section> fast parsing with ttp parser.  </section>
<citcontext>
<prevsection>
<prevsent>one popular term weighting scheme, known as ffidf, weights terms proportionately to their inverted document frequency scores and to their in-document frequencies (to.
</prevsent>
<prevsent>the in-document frequency factor is usually normalized by the document length, that is, it is more significant for term to occur in short 100-word abstract, han in 5000-word article.
</prevsent>
</prevsection>
<citsent citstr=" H93-1070 ">
7 standard ff.idf weighting scheme (see buckley, 1993 <papid> H93-1070 </papid>for details) may be inappropriate for mixed term sets, consisting of ordinary concepts, proper names, and phrases, because: (1) it favors terms that occur fairly frequently in document, which supports only general-type queries (e.g.,  all you know about ).</citsent>
<aftsection>
<nextsent>such queries were not typical in trec.
</nextsent>
<nextsent>(2) it attaches low weights to infrequent, highly specific terms, such as names and phrases, whose only occurrences in document are often decisive for relevance.
</nextsent>
<nextsent>note that such terms cannot be reliably distinguished using their distribution in the database as the sole fac-tor, and therefore syntactic and lexical informa-tion is required.
</nextsent>
<nextsent>(3) it does not address the problem of inter-term dependencies arising when phrasal terms and their component single-word terms are all included in document representation, i.e., launch+satellite and satellite are not indepen-dent, and it is unclear whether they should be counted as two terms.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A219">
<title id=" C00-2146.xml">structural disambiguation of morphosyntactic categorial parsing for korean </title>
<section> i t roduct ion   </section>
<citcontext>
<prevsection>
<prevsent>since sequence of pre final verb-endings, auxiliary verbs and verb-endings can generate hundreds of different usages of the same verb, morpheme- based grammar modeling is considered as nat-ural consequence for korean.
</prevsent>
<prevsent>there have been various researches to dis-ambiguate the structural ambiguities in pars-ing.
</prevsent>
</prevsection>
<citsent citstr=" J93-1005 ">
lexical and contextual information has been shown to be most crucial for many pars-ing decisions, such as prepositional-phrase at-tachment (hindle and rooth, 1993).<papid> J93-1005 </papid></citsent>
<aftsection>
<nextsent>(charniak, 1995; collins, 1996) <papid> P96-1025 </papid>use the lexical intbrmation * this research was partial ly supported by kosef spe-cial basic resem ch 1)rogram (1997.9 ~ 2000.8).</nextsent>
<nextsent>and (magerman and marcus, 1991; <papid> H91-1044 </papid>magerman and weir, 1992) <papid> P92-1006 </papid>use the contextual information for struct;nral disambiguation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A220">
<title id=" C00-2146.xml">structural disambiguation of morphosyntactic categorial parsing for korean </title>
<section> i t roduct ion   </section>
<citcontext>
<prevsection>
<prevsent>there have been various researches to dis-ambiguate the structural ambiguities in pars-ing.
</prevsent>
<prevsent>lexical and contextual information has been shown to be most crucial for many pars-ing decisions, such as prepositional-phrase at-tachment (hindle and rooth, 1993).<papid> J93-1005 </papid></prevsent>
</prevsection>
<citsent citstr=" P96-1025 ">
(charniak, 1995; collins, 1996) <papid> P96-1025 </papid>use the lexical intbrmation * this research was partial ly supported by kosef spe-cial basic resem ch 1)rogram (1997.9 ~ 2000.8).</citsent>
<aftsection>
<nextsent>and (magerman and marcus, 1991; <papid> H91-1044 </papid>magerman and weir, 1992) <papid> P92-1006 </papid>use the contextual information for struct;nral disambiguation.</nextsent>
<nextsent>but, there have been few researches that used probability intbr- marion for reducing the spurious ambiguities in choosing the most plausible parse tree of ccg formalism, especially for morpho-syntactic pars-ing of agglutinative language.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A221">
<title id=" C00-2146.xml">structural disambiguation of morphosyntactic categorial parsing for korean </title>
<section> i t roduct ion   </section>
<citcontext>
<prevsection>
<prevsent>lexical and contextual information has been shown to be most crucial for many pars-ing decisions, such as prepositional-phrase at-tachment (hindle and rooth, 1993).<papid> J93-1005 </papid></prevsent>
<prevsent>(charniak, 1995; collins, 1996) <papid> P96-1025 </papid>use the lexical intbrmation * this research was partial ly supported by kosef spe-cial basic resem ch 1)rogram (1997.9 ~ 2000.8).</prevsent>
</prevsection>
<citsent citstr=" H91-1044 ">
and (magerman and marcus, 1991; <papid> H91-1044 </papid>magerman and weir, 1992) <papid> P92-1006 </papid>use the contextual information for struct;nral disambiguation.</citsent>
<aftsection>
<nextsent>but, there have been few researches that used probability intbr- marion for reducing the spurious ambiguities in choosing the most plausible parse tree of ccg formalism, especially for morpho-syntactic pars-ing of agglutinative language.
</nextsent>
<nextsent>in this paper, we describe the probabilistic nmthod (e.g., category merge probability, head- head co-occurrence, coverage heuristics) to re-duce the spurious atnbiguities and choose the most plausible parse tree for agglutinative lan-guages uch as korean.
</nextsent>
<nextsent>this section briefly reviews the basic kccg for-malism.
</nextsent>
<nextsent>following (steedman, 1985), order-preserving type-raising rules are used to convert nouns in grammar into the functors over verb.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A222">
<title id=" C00-2146.xml">structural disambiguation of morphosyntactic categorial parsing for korean </title>
<section> i t roduct ion   </section>
<citcontext>
<prevsection>
<prevsent>lexical and contextual information has been shown to be most crucial for many pars-ing decisions, such as prepositional-phrase at-tachment (hindle and rooth, 1993).<papid> J93-1005 </papid></prevsent>
<prevsent>(charniak, 1995; collins, 1996) <papid> P96-1025 </papid>use the lexical intbrmation * this research was partial ly supported by kosef spe-cial basic resem ch 1)rogram (1997.9 ~ 2000.8).</prevsent>
</prevsection>
<citsent citstr=" P92-1006 ">
and (magerman and marcus, 1991; <papid> H91-1044 </papid>magerman and weir, 1992) <papid> P92-1006 </papid>use the contextual information for struct;nral disambiguation.</citsent>
<aftsection>
<nextsent>but, there have been few researches that used probability intbr- marion for reducing the spurious ambiguities in choosing the most plausible parse tree of ccg formalism, especially for morpho-syntactic pars-ing of agglutinative language.
</nextsent>
<nextsent>in this paper, we describe the probabilistic nmthod (e.g., category merge probability, head- head co-occurrence, coverage heuristics) to re-duce the spurious atnbiguities and choose the most plausible parse tree for agglutinative lan-guages uch as korean.
</nextsent>
<nextsent>this section briefly reviews the basic kccg for-malism.
</nextsent>
<nextsent>following (steedman, 1985), order-preserving type-raising rules are used to convert nouns in grammar into the functors over verb.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A223">
<title id=" C00-2146.xml">structural disambiguation of morphosyntactic categorial parsing for korean </title>
<section> overview of  kccg.  </section>
<citcontext>
<prevsection>
<prevsent>forward composition (b ): x/(x\ar.q.sx) y/(y\arg.sy) ==~ / (x\ ( a,.:j ,: ) ), = x\arqsx ? backward comi)osition (b ): y\arg.sy x\(ar.q.sx {y}) ===  x\(a rgsx arosy) ? coordination (~): conjx ~ parsing korean chart parser has been developed based on our kccg modeling with 10(},0()0 mor-pheme dictionary.
</prevsent>
<prevsent>each morpheme entry in the dictionary has morphological category, mor- pho tactics connectivity and kccg syntax (:at- egories tbr the morpheme.
</prevsent>
</prevsection>
<citsent citstr=" W98-1110 ">
in the morphological analysis stage, un-known word treatment nmthod based on mor-pheme pattern dictionary and syllable bigrams is used after (cha et al, 1998).<papid> W98-1110 </papid></citsent>
<aftsection>
<nextsent>pos(part -of speech) tagger which is tightly coupled with the morphological analyzer removes the irrele- wmt morpheme candidates from the lnorpheme graph.
</nextsent>
<nextsent>the morpheme graph is compact representation method of korean morphologi-cal structure.
</nextsent>
<nextsent>kccg parser analyzes the mor-pheme graph at once through the morpheme graph embedding technique (lee et al, 1996).
</nextsent>
<nextsent>the kccg parser incrementally analyzes the sentence, eojeol by eojeol :1 whenever an eo- jeol is newly processed by the morphological n- alyzer, the morphenms resulted in new mor-pheme graph are embedded in chart and an-alyzed and combined with the previous parsing results.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A224">
<title id=" C00-1053.xml">deixis and conjunction in multimodal systems </title>
<section> abstract </section>
<citcontext>
<prevsection>
<prevsent>in order to realize their full potential, multimodal interfaces need to support not just input from multiple modes, but single comnmnds optinmlly distributed across the available input modes.
</prevsent>
<prevsent>a multimodal anguage processing architecture is needed to integrate semantic content from the different modes.
</prevsent>
</prevsection>
<citsent citstr=" P98-1102 ">
johnston 1998<papid> P98-1102 </papid>a proposes modular approach to multimodal language processing in which spoken language parsing is completed before lnultimodal parsing.</citsent>
<aftsection>
<nextsent>in this paper, will demonstrate the difficulties this approach faces as the spoken language parsing component is expanded to provide compositional analysis of deictic expressions.
</nextsent>
<nextsent>i propose an alternative architecture in which spoken and multimodal parsing are tightly interleaved.
</nextsent>
<nextsent>this architecture greatly simplifies the spoken language parsing grm-nmar and enables predictive information fiom spoken language parsing to drive the application of multimodal parsing and gesture combination rules.
</nextsent>
<nextsent>i also propose treatment of deictic numeral expressions that supports the broad range of pen gesture combinations that can be used to refer to collections of objects in the interface.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A266">
<title id=" C00-1053.xml">deixis and conjunction in multimodal systems </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>3 interleaviug spoken language parsing.
</prevsent>
<prevsent>and multimodal parsing there are nmnber of different ways in which spoken language parsing (slp) and multimodal parsing (mp) can be imerleaved: (1) slp populates chart with fragments, these are passed to mp which determines possible combinations with gesture, the resulting combinations are passed back to slp which continues until parse of the string is found, (2) slp parses the incoming string into series of fragments, these become edges in mp and are combined with gestures, mp is augmented with rules from slp which operate in mp in order to complete the analysis of the phrase, (3) slp and mp are merged and there is one single gralnmar covering both spoken language and multimodal parsing (cf.
</prevsent>
</prevsection>
<citsent citstr=" C00-1054 ">
johnston and bangalore 2000).<papid> C00-1054 </papid></citsent>
<aftsection>
<nextsent>1 adopt here strategy (1) represented in figure 5.
</nextsent>
<nextsent>365 commands figure 5 interleaved architecture significant advantage of (1) is that it limits the number of elements and combinations that need to considered by the nmltimodal parser.
</nextsent>
<nextsent>the complexity of the inultidilnensional parsing algorithm is exponential in the worst case (johnston 1998<papid> P98-1102 </papid>a) and so it is important to limit the number of elements that need to be considered.</nextsent>
<nextsent>another advantage of (1) over (2) and (3) is that as in the modular approach, the grammars are separated, facilitating reuse of the multimodal component for applications with different spoken colnmands.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A281">
<title id=" A00-2012.xml">arabic morphology generation using a concatenative strategy </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>z dia critic marks are used in arabic language textbooks and occasionally in regular texts to resolve ambiguous words (e.g. to mark passive verb use).
</prevsent>
<prevsent>86 (1983) two-level morphology.
</prevsent>
</prevsection>
<citsent citstr=" C96-1017 ">
in beesley (1996) <papid> C96-1017 </papid>the system is reworked into finite-state lexical transducer to perform analysis and generation.</citsent>
<aftsection>
<nextsent>in two-level systems, the lexical level includes hort vowels that are typically not realized on the the surface level.
</nextsent>
<nextsent>kiraz (1994) <papid> C94-1029 </papid>presents an analysis of arabic morphology based on the cv-, moraic-, and affixational models.</nextsent>
<nextsent>he introduces multi-tape two-level model and formalism where three tapes are used for the lexical level (root, pattern, and vocalization) and one tape for the surface level.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A282">
<title id=" A00-2012.xml">arabic morphology generation using a concatenative strategy </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in beesley (1996) <papid> C96-1017 </papid>the system is reworked into finite-state lexical transducer to perform analysis and generation.</prevsent>
<prevsent>in two-level systems, the lexical level includes hort vowels that are typically not realized on the the surface level.</prevsent>
</prevsection>
<citsent citstr=" C94-1029 ">
kiraz (1994) <papid> C94-1029 </papid>presents an analysis of arabic morphology based on the cv-, moraic-, and affixational models.</citsent>
<aftsection>
<nextsent>he introduces multi-tape two-level model and formalism where three tapes are used for the lexical level (root, pattern, and vocalization) and one tape for the surface level.
</nextsent>
<nextsent>in this paper, we propose computational approach that applies concatenative treatment to arabic morphology generation by separating the issue of in fixation from other inflectional variations.
</nextsent>
<nextsent>we are developing an arabic morphological generator using morphe (leavitt, 1994), tool for modeling morphology based on discrimination trees and regular expressions.
</nextsent>
<nextsent>morphe is part of suite of tools developed at the language technologies institute, carnegie mellon university, for knowledge-based machine translation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A283">
<title id=" A00-2012.xml">arabic morphology generation using a concatenative strategy </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>e.g., from the underlying root \[baya  \]: baa   he sold  and yabii   he sells  stem allomorphs : perfect: -bi - and -baa - imperfect: and -bi - and -bii - . verbs of the pattern cayic, where middle.
</prevsent>
<prevsent>radical is  .
</prevsent>
</prevsection>
<citsent citstr=" P98-1018 ">
e.g., from the underlying root \[hayib\]: haaba  he feared  and yahaabu  he fears  stem allomorphs : perfect: -bib- and-haab- imperfect: -hab- and-haab- in the relevant literature (e.g., beesley, 1998; <papid> P98-1018 </papid>kiraz, 1994), <papid> C94-1029 </papid>verbs belonging to the above classes are all assumed to have the pattern cvcvc.</citsent>
<aftsection>
<nextsent>the pattern does not show the verb conjugation class and makes it difficult to predict he type of stem allomorph to use.
</nextsent>
<nextsent>to avoid these problems, we keep information on the middle radical and vowel in the base form of the verb.
</nextsent>
<nextsent>in generation, classes 2 and 4 of the verb can be handled as one because they have the same perfect and imperfect stemsp 5 the only exception is the passive participle.
</nextsent>
<nextsent>verbs.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A285">
<title id=" C00-1014.xml">reusing an ontology to generate numeral classifiers </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>there has been some work on the analysis of numeral classifiers in natural language processing, particularly for japanese (asahioka et al., 1990; kamei and muraki, 1995; bond et al, * visiting csli, stanford university (1999-2000).
</prevsent>
<prevsent>i numeral-classilier combinations are shown in bold, the noun phrases they quantify are underlined.
</prevsent>
</prevsection>
<citsent citstr=" P98-1023 ">
1996; bond et al, 1998; <papid> P98-1023 </papid>yokoyama and ochiai, 1999), but very little on their generation.</citsent>
<aftsection>
<nextsent>we could only find one paper on generating classifiers in thai (sornlertlamvanich et al, 1994).<papid> C94-1091 </papid></nextsent>
<nextsent>one immediate application fox the generation of classifiers is ma-chine translation, and we shall take examples flom there, but it is in fact needed fox  the generation of any quantified noun phrase with an uncountable head noun.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A286">
<title id=" C00-1014.xml">reusing an ontology to generate numeral classifiers </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>i numeral-classilier combinations are shown in bold, the noun phrases they quantify are underlined.
</prevsent>
<prevsent>1996; bond et al, 1998; <papid> P98-1023 </papid>yokoyama and ochiai, 1999), but very little on their generation.</prevsent>
</prevsection>
<citsent citstr=" C94-1091 ">
we could only find one paper on generating classifiers in thai (sornlertlamvanich et al, 1994).<papid> C94-1091 </papid></citsent>
<aftsection>
<nextsent>one immediate application fox the generation of classifiers is ma-chine translation, and we shall take examples flom there, but it is in fact needed fox  the generation of any quantified noun phrase with an uncountable head noun.
</nextsent>
<nextsent>the second question we address is: how far can an ontology be reused for difl%rent task to the one it was originally designed fox.
</nextsent>
<nextsent>there are several large ontologies now in use (wordnet (fellb~mm, 1998); goi-taikei (lkehara et al, 1997); mikrokos- rues (nirenburg, 1989)) and it is impractical to re-build one fox  every application.
</nextsent>
<nextsent>howevel, there is no guarantee that an ontology built fox one task will be useful for another.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A289">
<title id=" A92-1011.xml">the acquisition of lexical knowledge from combined machine readable dictionary sources </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>in the general case, the integration of information from distinct mrd sources for use within lexicon develop-ment environment is probably going to remain an un-solved problem for quite some time.
</prevsent>
<prevsent>this is simply be- cause dictionaries eldom describe the same word using the same sense distinctions.
</prevsent>
</prevsection>
<citsent citstr=" C88-2166 ">
consequently, the integra-tion of information from distinct mrd sources through simple word-sense matches is likely to fail insignificant number of instances (calzolari &amp; picchi, 1986; atkins 1987; klavans 1988; <papid> C88-2166 </papid>boguraev &amp; pustejovsky 1990).<papid> C90-2007 </papid></citsent>
<aftsection>
<nextsent>in-deed, atkins &amp; levin (1990) have suggested that the task of mapping mrds onto each other is so complex that the creation of complete  ideal  database which provides reference point for the mrd sources to be in-tegrated may well be considered as an essential prereq-uisite.
</nextsent>
<nextsent>however, when dealing with mrd sources which use entry definitions which are not too dissimilar, cor-relation technique based on word sense merging can be made to yield useful results, given the appropriate tools although sense matching across dictionaries in this case too is prone to errors, there are several reasons why the effort is worthwhile.
</nextsent>
<nextsent>first, the number of correct sens~ matches across mrd sources in this case is guarantee~ to be high.
</nextsent>
<nextsent>second, there are many instances in which at incorrect sense-to-sense match does not affect the fins 80 result since the information with respect which sense correlation is being sought may generalize across closely related word senses.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A290">
<title id=" A92-1011.xml">the acquisition of lexical knowledge from combined machine readable dictionary sources </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>in the general case, the integration of information from distinct mrd sources for use within lexicon develop-ment environment is probably going to remain an un-solved problem for quite some time.
</prevsent>
<prevsent>this is simply be- cause dictionaries eldom describe the same word using the same sense distinctions.
</prevsent>
</prevsection>
<citsent citstr=" C90-2007 ">
consequently, the integra-tion of information from distinct mrd sources through simple word-sense matches is likely to fail insignificant number of instances (calzolari &amp; picchi, 1986; atkins 1987; klavans 1988; <papid> C88-2166 </papid>boguraev &amp; pustejovsky 1990).<papid> C90-2007 </papid></citsent>
<aftsection>
<nextsent>in-deed, atkins &amp; levin (1990) have suggested that the task of mapping mrds onto each other is so complex that the creation of complete  ideal  database which provides reference point for the mrd sources to be in-tegrated may well be considered as an essential prereq-uisite.
</nextsent>
<nextsent>however, when dealing with mrd sources which use entry definitions which are not too dissimilar, cor-relation technique based on word sense merging can be made to yield useful results, given the appropriate tools although sense matching across dictionaries in this case too is prone to errors, there are several reasons why the effort is worthwhile.
</nextsent>
<nextsent>first, the number of correct sens~ matches across mrd sources in this case is guarantee~ to be high.
</nextsent>
<nextsent>second, there are many instances in which at incorrect sense-to-sense match does not affect the fins 80 result since the information with respect which sense correlation is being sought may generalize across closely related word senses.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A291">
<title id=" A92-1011.xml">the acquisition of lexical knowledge from combined machine readable dictionary sources </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>we used these ldb facilities for running queries from combined mrd sources which included more than one mrd - - i.e. ldoce and the longman lexicon of con-temporary english (lloce), thesaurus closely related to ldoce.
</prevsent>
<prevsent>the lkb provides lexicon development environment which uses typed graph-based unification formalism as representation language.
</prevsent>
</prevsection>
<citsent citstr=" A92-1012 ">
a detailed description of the lkb representation language is given in papers by copestake, de paiva and sanfilippo in briscoe et al (forthcoming); various properties of the system are also discussed in briscoe (1991) and copestake (1992).<papid> A92-1012 </papid></citsent>
<aftsection>
<nextsent>the lkb allows the user to define an inheritance network of types plus restrictions associated with them, and to create lexicons where such types are assigned to lexi-cal templates extracted through ldb queries which give word-sense specific information.
</nextsent>
<nextsent>consider, for exam-ple, the lexical template relative to the first ldoce sense of the verb delight in (1) where sense specific infor-mation is integrated with reference to the lkb type strict-trans-sigii which provides general syntactic and semantic haracterization strict transitive verbs.
</nextsent>
<nextsent>1 (1) delight l_2_ strict-ti~ns-sign  cat : resu t : resu t :m-feat : ia thes s =indef-obj  cat : resu t : resu t :m-f eats : reg -morph =true  cat :ac ive :sem:arg2  - -human  sense- id :d ic ionary  -  ldoce   sense- id: idb -ent ry -no  =  9335   sense- id :sense-no   1 .
</nextsent>
<nextsent>when loaded into the lkb, the lexical template above will expand into full syntactic and semantic rep-resentation as shown in figure 2; this representa-tion arises from integrating sense-specific information with the information structure associated with the type strict-trans-s ignf xthe type specification indef-0bj in (1) corresponds to the ldb value unaccusative (see figure 1) and marks tran-sitive verbs which are amenable to the indefinite object alter-nation, e.g. book which is certain to delight them vs. book which is certain to delight.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A292">
<title id=" A92-1011.xml">the acquisition of lexical knowledge from combined machine readable dictionary sources </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>standard dictionaries are simply not equippe to offer this kind of information with consistency an exhaustiveness.
</prevsent>
<prevsent>furthermore, the technique of creatin derived dictionaries where the information contained a main source mrd is made more explicit is unhel\[ fulin this case.
</prevsent>
</prevsection>
<citsent citstr=" P81-1030 ">
for example, one approach would scare 82 to derive dictionary from ldoce where verbs are or-ganized into network defined by is-a links using the general approach to taxonomy formation described by amsler (1981).<papid> P81-1030 </papid></citsent>
<aftsection>
<nextsent>such an approach would involve the for-mation of chains through verb definitions determined by the genus term of each definition.
</nextsent>
<nextsent>unfortunately, the genus of verb definitions is often not specific enough to supply taxonomic haracterization which allows for the identification of semantic verb classes with consistency and exhaustiveness.
</nextsent>
<nextsent>in ldoce, for example, the genus of over 20% of verb senses (about 3,500) is one of 8 verbs: cause, make, be, give, put, take, move, have; many of the word senses which have the same genus belong to distinct semantic verb classes.
</nextsent>
<nextsent>this is not to say that verb taxonomies are of no value, and in the final sec-tion we will briefly discuss an important application of verb taxonomies with respect to the assignment of se-mantic classes to verb senses.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A293">
<title id=" C00-1015.xml">youll take the high road and ill take the low road using a third language to improve bilingual word alignment </title>
<section> improving word alignment by.  </section>
<citcontext>
<prevsection>
<prevsent>this paper discusses one particular such source, namely the use of third hlnguage in the aligmnent process.
</prevsent>
<prevsent>apart fronl an earlier presentation by the present author (berth, to appear b), have not seen any mention in the literature of the possibility of using third language in this way for improving word alignmorit.
</prevsent>
</prevsection>
<citsent citstr=" W99-0602 ">
simard (1999) <papid> W99-0602 </papid>describes how the use of third language can be brought to bear upon the simpler problem of senlence alignment, but he does not consider the harder problem of word alignmenl.</citsent>
<aftsection>
<nextsent>perhaps it has not being thought of for the silnplc reason that it is possible only with ###ululingual parallel corpora, and--for obvious reasons--not with b/lingual corpora, which has been the kind of parallel corpus that has received nlost attention from researchers in the field.
</nextsent>
<nextsent>since the third language acts as, as it were, pivot for the alignment of the two other languages, we refer to the method as pivot alignment, and it works as follows, with three languages, e.g. swedish (se), polish (pl) and serbian-bosnian-croatian (sbc), where the aim is to align swedish with the other two languages on the word level.
</nextsent>
<nextsent>perform the pairwise alignments se-~pl, se-- sbc, pl--- sbc, and sbc-opl; check whether there exist aligned words on the indirect  alignment path  4 se-osfjc-opl, which are not on the direct path se--- pl. if there are, add them to the se-opl alignnaents.
</nextsent>
<nextsent>do the same for the indirect path se-- pl-osbc and the direct path se-osbc in order lor this procedure to work, we must believe that 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A294">
<title id=" A92-1013.xml">computational lexicons the neat examples and the odd exemplars </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>arguments are annotated with selectional restrictions, that impose type constraints on the set of content words that may fill relation.
</prevsent>
<prevsent>selectional restrictions often do not provide all the semantic information that is necessary in nlp systems, however they are at the basis of the majority of computational pproaches to syntactic and semantic disambiguation.
</prevsent>
</prevsection>
<citsent citstr=" W91-0212 ">
it has been noticed that representing only the semantics of verbs may be inadequate (velardi et al 1988; boguraev 1991; macpherson 1991).<papid> W91-0212 </papid></citsent>
<aftsection>
<nextsent>the notion of spreading the semantic load supports the idea that every content word should be represented in the lexicon as the union of all the situations in which it could potentially participate.
</nextsent>
<nextsent>unfortunately, handwriting selectional restrictions not an easy matter, because it is time consuming and it is hard to keep consistency among the data when the lexicon has several hundred or thousand words.
</nextsent>
<nextsent>however the major difficulty is that words relate to each other in many different, often domain dependent ways.
</nextsent>
<nextsent>the nowadays vast literature on computational lexicons is i led with neat examples of the eat(animate,food) flavour, but in practice in many language domains selectional constraints between words are quite odd.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A295">
<title id=" A92-1013.xml">computational lexicons the neat examples and the odd exemplars </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this idea is a!
</prevsent>
<prevsent>the basis of many recent studies on word associations.
</prevsent>
</prevsection>
<citsent citstr=" J90-1003 ">
the results of these studies have important applications in lexicography, to detect lexico- syntactic regularities (church and hanks, 1990<papid> J90-1003 </papid>1 (calzolari and bindi,1990), <papid> C90-3010 </papid>such as, for example~ support verbs (e.g.  make-decision ) prepositional verbs (e.g.  rely-upon ) idioms, semantic relations (e.g.  part_of ) and fixed expressions (e.g.  kick the bucket ).</citsent>
<aftsection>
<nextsent>in (hindle,1990; <papid> P90-1034 </papid>zernik, 1989; webster el marcus, 1989) cooccurrence analyses augmented with syntactic parsing is used for the purpose of word classification.</nextsent>
<nextsent>all these studies are based on th~ (strong) assumption that syntactic similarity in wor(~ patterns implies semantic similarity.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A296">
<title id=" A92-1013.xml">computational lexicons the neat examples and the odd exemplars </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this idea is a!
</prevsent>
<prevsent>the basis of many recent studies on word associations.
</prevsent>
</prevsection>
<citsent citstr=" C90-3010 ">
the results of these studies have important applications in lexicography, to detect lexico- syntactic regularities (church and hanks, 1990<papid> J90-1003 </papid>1 (calzolari and bindi,1990), <papid> C90-3010 </papid>such as, for example~ support verbs (e.g.  make-decision ) prepositional verbs (e.g.  rely-upon ) idioms, semantic relations (e.g.  part_of ) and fixed expressions (e.g.  kick the bucket ).</citsent>
<aftsection>
<nextsent>in (hindle,1990; <papid> P90-1034 </papid>zernik, 1989; webster el marcus, 1989) cooccurrence analyses augmented with syntactic parsing is used for the purpose of word classification.</nextsent>
<nextsent>all these studies are based on th~ (strong) assumption that syntactic similarity in wor(~ patterns implies semantic similarity.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A297">
<title id=" A92-1013.xml">computational lexicons the neat examples and the odd exemplars </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the basis of many recent studies on word associations.
</prevsent>
<prevsent>the results of these studies have important applications in lexicography, to detect lexico- syntactic regularities (church and hanks, 1990<papid> J90-1003 </papid>1 (calzolari and bindi,1990), <papid> C90-3010 </papid>such as, for example~ support verbs (e.g.  make-decision ) prepositional verbs (e.g.  rely-upon ) idioms, semantic relations (e.g.  part_of ) and fixed expressions (e.g.  kick the bucket ).</prevsent>
</prevsection>
<citsent citstr=" P90-1034 ">
in (hindle,1990; <papid> P90-1034 </papid>zernik, 1989; webster el marcus, 1989) cooccurrence analyses augmented with syntactic parsing is used for the purpose of word classification.</citsent>
<aftsection>
<nextsent>all these studies are based on th~ (strong) assumption that syntactic similarity in wor(~ patterns implies semantic similarity.
</nextsent>
<nextsent>in (guthrie el al., 1991), sets of consistently contiguous word~, ( neighbourhood ) are extracted from machine- readable ic ionar ies , to help semantic disambiguation information retrieval.
</nextsent>
<nextsent>in (smadj~ and mckeown, 1990) statistical ly collectec associations provide pragmatic ues for lexical choic( in sentence generation.
</nextsent>
<nextsent>for example, we can learr that  make decision  is better choice than, say 96  have decision  or  take decision .
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A298">
<title id=" A92-1013.xml">computational lexicons the neat examples and the odd exemplars </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in (smadj~ and mckeown, 1990) statistical ly collectec associations provide pragmatic ues for lexical choic( in sentence generation.
</prevsent>
<prevsent>for example, we can learr that  make decision  is better choice than, say 96  have decision  or  take decision .
</prevsent>
</prevsection>
<citsent citstr=" P91-1030 ">
(hindle and rooths, 1991) <papid> P91-1030 </papid>proposes that syntactic disambiguation criterion can be gathered by comparing the probability of occurrence of noun- preposition and verb-preposition pairs in np pp structures.</citsent>
<aftsection>
<nextsent>in general word associations are collected by extracting word pairs in +-5 window.
</nextsent>
<nextsent>in (calzolari and bindi, 1990), (<papid> C90-3010 </papid>church and hanks, 1990<papid> J90-1003 </papid>) the significance of an association (x,y) is measured by the mutual information i(x,y), i.e. the probability of observing and together, compared with the probability of observing and independently.</nextsent>
<nextsent>in (smadja, 1989), (zernik and jacobs, 1990), <papid> C90-1005 </papid>the associations are filtered by selecting the word pairs (x,y) whose frequency of occurrence is above f+ks, where is the average appearance, is the standard deviation, and is an empirically determined factor.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A301">
<title id=" A92-1013.xml">computational lexicons the neat examples and the odd exemplars </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in general word associations are collected by extracting word pairs in +-5 window.
</prevsent>
<prevsent>in (calzolari and bindi, 1990), (<papid> C90-3010 </papid>church and hanks, 1990<papid> J90-1003 </papid>) the significance of an association (x,y) is measured by the mutual information i(x,y), i.e. the probability of observing and together, compared with the probability of observing and independently.</prevsent>
</prevsection>
<citsent citstr=" C90-1005 ">
in (smadja, 1989), (zernik and jacobs, 1990), <papid> C90-1005 </papid>the associations are filtered by selecting the word pairs (x,y) whose frequency of occurrence is above f+ks, where is the average appearance, is the standard deviation, and is an empirically determined factor.</citsent>
<aftsection>
<nextsent>(hindle, 1990; <papid> P90-1034 </papid>hindle and rooths,1991) <papid> P91-1030 </papid>and (smadja, 1991) <papid> P91-1036 </papid>use syntactic markers to increase the significance of the data.</nextsent>
<nextsent>(guthrie et al, 1991\] <papid> P91-1019 </papid>uses the subject classification given in machine-readable dictionaries (e.g. economics, engineering, etc.) to reinforce co occurence links.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A306">
<title id=" A92-1013.xml">computational lexicons the neat examples and the odd exemplars </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in (calzolari and bindi, 1990), (<papid> C90-3010 </papid>church and hanks, 1990<papid> J90-1003 </papid>) the significance of an association (x,y) is measured by the mutual information i(x,y), i.e. the probability of observing and together, compared with the probability of observing and independently.</prevsent>
<prevsent>in (smadja, 1989), (zernik and jacobs, 1990), <papid> C90-1005 </papid>the associations are filtered by selecting the word pairs (x,y) whose frequency of occurrence is above f+ks, where is the average appearance, is the standard deviation, and is an empirically determined factor.</prevsent>
</prevsection>
<citsent citstr=" P91-1036 ">
(hindle, 1990; <papid> P90-1034 </papid>hindle and rooths,1991) <papid> P91-1030 </papid>and (smadja, 1991) <papid> P91-1036 </papid>use syntactic markers to increase the significance of the data.</citsent>
<aftsection>
<nextsent>(guthrie et al, 1991\] <papid> P91-1019 </papid>uses the subject classification given in machine-readable dictionaries (e.g. economics, engineering, etc.) to reinforce co occurence links.</nextsent>
<nextsent>despite the use of these methods to add evidence to the data, the major problem with word-pairs collections is that reliable results are obtained only for small subset of high-frequency words on very large corpora, otherwise the association ratio becomes unstable.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A307">
<title id=" A92-1013.xml">computational lexicons the neat examples and the odd exemplars </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in (smadja, 1989), (zernik and jacobs, 1990), <papid> C90-1005 </papid>the associations are filtered by selecting the word pairs (x,y) whose frequency of occurrence is above f+ks, where is the average appearance, is the standard deviation, and is an empirically determined factor.</prevsent>
<prevsent>(hindle, 1990; <papid> P90-1034 </papid>hindle and rooths,1991) <papid> P91-1030 </papid>and (smadja, 1991) <papid> P91-1036 </papid>use syntactic markers to increase the significance of the data.</prevsent>
</prevsection>
<citsent citstr=" P91-1019 ">
(guthrie et al, 1991\] <papid> P91-1019 </papid>uses the subject classification given in machine-readable dictionaries (e.g. economics, engineering, etc.) to reinforce co occurence links.</citsent>
<aftsection>
<nextsent>despite the use of these methods to add evidence to the data, the major problem with word-pairs collections is that reliable results are obtained only for small subset of high-frequency words on very large corpora, otherwise the association ratio becomes unstable.
</nextsent>
<nextsent>for example, church run his experiment on corpus with over 20-30 millions words, and hindle reports 6 millions words as not being an adequate corpus.
</nextsent>
<nextsent>in many practical nlp/ir applications corpora are not so large, and typically span from 500,000 to few million words.
</nextsent>
<nextsent>the analysis of associations could be done on wider domains, but part for very general words, it is much more desirable to collect data from the application corpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A314">
<title id=" A92-1013.xml">computational lexicons the neat examples and the odd exemplars </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in (marziali,1991) the efficiency and precision of this grammar with respect to the full se!
</prevsent>
<prevsent>of surface syntactic links detectable by complete dcg grammar are evaluated to be 85% and 90%.
</prevsent>
</prevsection>
<citsent citstr=" J89-1002 ">
the reference output adopted to perform the evaluation is syntactic graph (seo and simmons,1989) .<papid> J89-1002 </papid></citsent>
<aftsection>
<nextsent>syntactic graphs include in unique graph the set of all possible parse trees.
</nextsent>
<nextsent>the evaluation was hand-made 98 over set of 100 sentences belonging to three domains: the economic corpus, the legal corpus, and novel.
</nextsent>
<nextsent>the performances are better for the legal corpus and the novel, due to the ungrammaticality of the economic orpus.
</nextsent>
<nextsent>the relatively high efficiency rate, as compared with the figures reported in (brent, 1991), <papid> P91-1027 </papid>are due to the fact that italian morphology is far more complex than english.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A315">
<title id=" A92-1013.xml">computational lexicons the neat examples and the odd exemplars </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the evaluation was hand-made 98 over set of 100 sentences belonging to three domains: the economic corpus, the legal corpus, and novel.
</prevsent>
<prevsent>the performances are better for the legal corpus and the novel, due to the ungrammaticality of the economic orpus.
</prevsent>
</prevsection>
<citsent citstr=" P91-1027 ">
the relatively high efficiency rate, as compared with the figures reported in (brent, 1991), <papid> P91-1027 </papid>are due to the fact that italian morphology is far more complex than english.</citsent>
<aftsection>
<nextsent>once good morphologic analyzer is available (the one used in our work is very well tested, and has first described in (russo,1987)), <papid> E87-1006 </papid>problems such as verb detection, raised in (brent, 1991), <papid> P91-1027 </papid>are negligible.</nextsent>
<nextsent>in addition, the text-cutting algorithm has positive effects on the precision.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A317">
<title id=" A92-1013.xml">computational lexicons the neat examples and the odd exemplars </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the performances are better for the legal corpus and the novel, due to the ungrammaticality of the economic orpus.
</prevsent>
<prevsent>the relatively high efficiency rate, as compared with the figures reported in (brent, 1991), <papid> P91-1027 </papid>are due to the fact that italian morphology is far more complex than english.</prevsent>
</prevsection>
<citsent citstr=" E87-1006 ">
once good morphologic analyzer is available (the one used in our work is very well tested, and has first described in (russo,1987)), <papid> E87-1006 </papid>problems such as verb detection, raised in (brent, 1991), <papid> P91-1027 </papid>are negligible.</citsent>
<aftsection>
<nextsent>in addition, the text-cutting algorithm has positive effects on the precision.
</nextsent>
<nextsent>despite this, we verified that about 35% of the syntactic associations extracted from the economic corpus are semantically unrelated, due to syntactic ambiguity.
</nextsent>
<nextsent>as shown in the following sections, semantic clustering in part solves this problem, because semantically unrelated word pairs do not accumulate statistical relevance, except for very rare and unfortunate cases.
</nextsent>
<nextsent>in any case, we need more experiments to verify the effect of more severe sentence cutting algorithm on the precision at detecting semantically related pairs.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A320">
<title id=" A92-1018.xml">a practical partofspeech tagger </title>
<section> background </section>
<citcontext>
<prevsection>
<prevsent>greene and rubin used rule-based ap-proach in the taggit program \[greene and rubin, 1971\], which was an aid in tagging the brown corpus \[francis and ku~era, 1982\].
</prevsent>
<prevsent>taggit disambiguated 77% of the cor- pus; the rest was done manually over period of several years.
</prevsent>
</prevsection>
<citsent citstr=" C90-2040 ">
more recently, koskenniemi also used rule-based approach implemented with finite-state machines \[kosken- niemi, 1990\].<papid> C90-2040 </papid></citsent>
<aftsection>
<nextsent>statistical methods have also been used (e.g., \[derose, 1988\], \[<papid> J88-1003 </papid>garside al., 1987\]).</nextsent>
<nextsent>these provide the capability of resolving ambiguity on the basis of most likely inter pre tation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A321">
<title id=" A92-1018.xml">a practical partofspeech tagger </title>
<section> background </section>
<citcontext>
<prevsection>
<prevsent>taggit disambiguated 77% of the cor- pus; the rest was done manually over period of several years.
</prevsent>
<prevsent>more recently, koskenniemi also used rule-based approach implemented with finite-state machines \[kosken- niemi, 1990\].<papid> C90-2040 </papid></prevsent>
</prevsection>
<citsent citstr=" J88-1003 ">
statistical methods have also been used (e.g., \[derose, 1988\], \[<papid> J88-1003 </papid>garside al., 1987\]).</citsent>
<aftsection>
<nextsent>these provide the capability of resolving ambiguity on the basis of most likely interpre-tation.
</nextsent>
<nextsent>a form of markov model has been widely used that assumes that word depends probabilistically on just its part-of-speech category, which in turn depends olely on the categories of the preceding two words.
</nextsent>
<nextsent>two types of training (i.e., parameter estimation) have been used with this model.
</nextsent>
<nextsent>the first makes use of tagged training corpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A322">
<title id=" A92-1018.xml">a practical partofspeech tagger </title>
<section> background </section>
<citcontext>
<prevsection>
<prevsent>at first, relatively small amount of text is manually tagged and used to train partially accurate model.
</prevsent>
<prevsent>the model is then used to tag more text, and the tags are manu-ally corrected and then used to retrain the model.
</prevsent>
</prevsection>
<citsent citstr=" A88-1019 ">
church uses the tagged brown corpus for training \[church, 1988\].<papid> A88-1019 </papid></citsent>
<aftsection>
<nextsent>these models involve probabilities for each word in the lexicon, so large tagged corpora are required for reliable estimation.
</nextsent>
<nextsent>the second method of training does not require tagged training corpus.
</nextsent>
<nextsent>in this situation the baum-welch algo-rithm (also known as the forward-backward algorithm) can be used \[baum, 1972\].
</nextsent>
<nextsent>under this regime the model is called hidden markov model (hmm), as state transitions (i.e., part-of-speech categories) are assumed to be unob- servable.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A323">
<title id=" A92-1018.xml">a practical partofspeech tagger </title>
<section> background </section>
<citcontext>
<prevsection>
<prevsent>jelinek has used this method for training text tagger \[jelinek, 1985\].
</prevsent>
<prevsent>parameter smoothing can be con-veniently achieved using the method of deleted interpola-tion in which weighted estimates are taken from second- and first-order models and uniform probability distribu-tion \[jelinek and mercer, 1980\].
</prevsent>
</prevsection>
<citsent citstr=" H89-2014 ">
kupiec used word equiv-alence classes (referred to here as ambiguity classes) based on parts of speech, to pool data from individual words \[ku- piec, 1989<papid> H89-2014 </papid>b\].</citsent>
<aftsection>
<nextsent>the most common words are still represented individually, as sufficient data exist for robust estimation.
</nextsent>
<nextsent>133 however all other words are represented according to the set of possible categories they can assume.
</nextsent>
<nextsent>in this manner, the vocabulary of 50,000 words in the brown corpus can be reduced to approximately 400 distinct ambiguity classes \[kupiec, 1992\].
</nextsent>
<nextsent>to further reduce the number of param-eters, first-order model can be employed (this assumes that word category depends only on the immediately preceding word category).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A334">
<title id=" A92-1017.xml">extended spelling correction for german </title>
<section> motivation </section>
<citcontext>
<prevsection>

<prevsent>a conscrvativc extension of standard spciling corrcction systems for german is discus scd which goes beyond normal chccking of isolated single words by taking multi-words, linguistically motivatcd non-words, as well as contexts into account.
</prevsent>
</prevsection>
<citsent citstr=" C86-1001 ">
as indicated by maurice gross in his coling 86 lccturc (gross, 1986), <papid> C86-1001 </papid>european languages contain thousands of what he calls  frozen  or  compound words .</citsent>
<aftsection>
<nextsent>in contrast to  free forms , frozen words though being separable into several words and suffixes - lack syntactic and/or semantic compositionality.
</nextsent>
<nextsent>this  lack of compositionality is apparent from lexical restrictions  (at night, but: *at day, *at evening, ctc.)
</nextsent>
<nextsent>as well as  by the impossibility of inserting material that is priori plausible  (*at {coming, present, cold, dark} night) (gross, 1986).<papid> C86-1001 </papid></nextsent>
<nextsent>since the degree of frozenness can vary, the procedure for recognizing compound words within text can be more or less complicated.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A341">
<title id=" A92-1017.xml">extended spelling correction for german </title>
<section> motivation </section>
<citcontext>
<prevsection>
<prevsent>in fact, proof-reading with lcas rather amounts to genuine grammar checking and as such belongs to different and higher level of language checking.
</prevsent>
<prevsent>context sensitive spelling checking, as proposed here, can be regarded as checking level in its own right, lying in between any checking on word level and grammar checking.
</prevsent>
</prevsection>
<citsent citstr=" A92-1015 ">
it thus could complement he two-level checker discussed in (vosse, 1992) <papid> A92-1015 </papid>by correcting especially those errors in idiomatic expressions, like  te alle tijden  -   te allen tijde , which cannot be detected on word or sentence level; compare (vosse, 1992).<papid> A92-1015 </papid></citsent>
<aftsection>
<nextsent>4 processing model good model of the system is given by deterministic multitape turing machine (hopcroft and ullman, 1979) consisting of finite control with, in effect, three tapes and tape heads.
</nextsent>
<nextsent>the following description relates to sentence level: initially, the input appears on the first tape with each of the tape cells containing either word, blank (symbolized below by single  ), or left or right sentence boundary symbol.
</nextsent>
<nextsent>thus, any input sentence can be stored by finite sequence of cells.
</nextsent>
<nextsent>the second tape holds read-only copy of the initial text.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A343">
<title id=" A92-1017.xml">extended spelling correction for german </title>
<section> motivation </section>
<citcontext>
<prevsection>
<prevsent>what is less acceptable, for each of the implcmentations mentioned above, is the lack of integration of the checking on the various levels.
</prevsent>
<prevsent>5 status of implementation.
</prevsent>
</prevsection>
<citsent citstr=" C90-2063 ">
a first prototype of the system described above has been developed in under unix within the esprit ii project 2315  translator workbench  (twb) as one of several separate modules checking basic as well as higher levels of various languages ilike grammar and style; see (thurmair, 1990) <papid> C90-2063 </papid>and (winkelmann, 1990)\].<papid> C90-1018 </papid></citsent>
<aftsection>
<nextsent>a derived and extended b-release version - covering 3.000 rewriting rules - has been integrated into both proprietary text processing software under dos and microsoft winword 1.1 under ms windows 3.0.
</nextsent>
<nextsent>in each case it runs independently from the built-in standard spelling verifier, although this is not transparent to the user who perceives just one proof reader checking each sentence of text twice, i.e., on two different levels.
</nextsent>
<nextsent>on both these implementations, some problems have received practical solutions to an acceptable degree.
</nextsent>
<nextsent>for example, the problem of mistaking an abbreviation for the end of sentence (because both end with dot), which could prevent context from being recognized, is  solved  by having the sentence segmentation routine always thus, it may happen that the checkers running one after the other over the same text - disturb each other results by proposing antagonistic orrections with respect to one and the same expression: within the correct passage  in bezug auf , for example,  bezug  will first be regarded as an error by the standard checker which then will propose to rewrite it as  bezug .
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A344">
<title id=" A92-1017.xml">extended spelling correction for german </title>
<section> motivation </section>
<citcontext>
<prevsection>
<prevsent>what is less acceptable, for each of the implcmentations mentioned above, is the lack of integration of the checking on the various levels.
</prevsent>
<prevsent>5 status of implementation.
</prevsent>
</prevsection>
<citsent citstr=" C90-1018 ">
a first prototype of the system described above has been developed in under unix within the esprit ii project 2315  translator workbench  (twb) as one of several separate modules checking basic as well as higher levels of various languages ilike grammar and style; see (thurmair, 1990) <papid> C90-2063 </papid>and (winkelmann, 1990)\].<papid> C90-1018 </papid></citsent>
<aftsection>
<nextsent>a derived and extended b-release version - covering 3.000 rewriting rules - has been integrated into both proprietary text processing software under dos and microsoft winword 1.1 under ms windows 3.0.
</nextsent>
<nextsent>in each case it runs independently from the built-in standard spelling verifier, although this is not transparent to the user who perceives just one proof reader checking each sentence of text twice, i.e., on two different levels.
</nextsent>
<nextsent>on both these implementations, some problems have received practical solutions to an acceptable degree.
</nextsent>
<nextsent>for example, the problem of mistaking an abbreviation for the end of sentence (because both end with dot), which could prevent context from being recognized, is  solved  by having the sentence segmentation routine always thus, it may happen that the checkers running one after the other over the same text - disturb each other results by proposing antagonistic orrections with respect to one and the same expression: within the correct passage  in bezug auf , for example,  bezug  will first be regarded as an error by the standard checker which then will propose to rewrite it as  bezug .
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A347">
<title id=" C00-1067.xml">on underspecified processing of dynamic semantics </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these rules operate directly on underspecified descrip-tions and fully maintain underspecifiedness.
</prevsent>
<prevsent>we also show how this behaviour call be captured by constraint propagation an existing imple-mentation of tree descriptions using finite set constraints (duchier and niehren, 2000; keller and niehren, 2000; duchier and gardent, 1999).
</prevsent>
</prevsection>
<citsent citstr=" P98-1058 ">
more specifically, we introduce dpl struc- tut cs~ extended trce structures that encode for-mulas of dynamic predicate logic (dpl) in much the same way as egg et al (1998) <papid> P98-1058 </papid>lambda structnres encode a-terms.</citsent>
<aftsection>
<nextsent>then we define constraint language tbr the description of dpl structures, called cl(dpl), in analogy to egg et al constraint langague for lambda struc-tures (clls).
</nextsent>
<nextsent>we characterize those dpl struc-tures in which all restrictions oil anaphoric ac-cessibility are obeyed by talking directly about the syntactic structure of dpl formula.
</nextsent>
<nextsent>this is ill contrast the standard procedure in dy- nanfic semantics, where the dynamic behaviour is produced by the semantics of the logic; we do not need to (and do not) talk about interpreta-tion of dpl structures and model accessibility by purely  static  means.
</nextsent>
<nextsent>the paper is structured as follows.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A348">
<title id=" C00-1067.xml">on underspecified processing of dynamic semantics </title>
<section> tree descript ions.  </section>
<citcontext>
<prevsection>
<prevsent>this implementation can be obtained by adapting an existing ilni)lelnentation a solver for dominance constraints.
</prevsent>
<prevsent>finally, we conclude and point to further work in section 6.
</prevsent>
</prevsection>
<citsent citstr=" P83-1020 ">
in this section, we define the constraint lan-guage tbr dpl structures, cl(dpl), lan-guage of tree descriptions which conserw~tively extends don finance constraints (marcus et al, 1983; <papid> P83-1020 </papid>rainbow et al, 1995; keller et al, 2000) by variable binding constraints.</citsent>
<aftsection>
<nextsent>cl(dpl) is close relative of the constraint language for 460 lamb(la structures (clls), 1)resented in (egg et al, 1998).<papid> P98-1058 </papid></nextsent>
<nextsent>it; is interl)reted over dpl struc-tures - trees extended by variable 1)inding function which can be used to encode tbrmulas of dynamic (or static) predicate logic.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A351">
<title id=" C00-1067.xml">on underspecified processing of dynamic semantics </title>
<section> tree descript ions.  </section>
<citcontext>
<prevsection>
<prevsent>it provides constraints tbr all the relations discussed above.
</prevsent>
<prevsent>there are label-ing constraints x: . (x~,.
</prevsent>
</prevsection>
<citsent citstr=" P94-1023 ">
,xr~), expressive combinations xry of dominance constraints  with set operators (dtu:hier and niehren, 200(}; cornell, 1994), <papid> P94-1023 </papid>non-intervention con-straints ~(  1*  ~* z), and binding constraints a(x)=z. cl(dpl) is interpreted over dpl structures.</citsent>
<aftsection>
<nextsent>a variable assignment into dpl structure 54 is total flmction fi om the set of variables of constraint the domain of 54.
</nextsent>
<nextsent>a pair (54, oz) of dpl structure 54 and variable assign-ment (t into 54 satisfies constraint qo ifl  it satisfies all of its atomic constraints; that is, if the relation with the same sylnbol holds of the nodes assigned to their arguments.
</nextsent>
<nextsent>we also call the pair (54, oz) solution and ad model of ~o. only some of the atonfic constraints in cl(dpl) are used in mlderspecified escrip- tions in t)articular, labeling, dominance, and binding constraints; the other constraints are helpful in processing the others.
</nextsent>
<nextsent>these three types of constraints can be transparently dis-played in constraint graphs.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A352">
<title id=" A00-2007.xml">noun phrase recognition by system combination </title>
<section> int roduct ion.  </section>
<citcontext>
<prevsection>


</prevsection>
<citsent citstr=" P98-1029 ">
(van halteren et al, 1998) and (brill and wu, 1998) <papid> P98-1029 </papid>describe series of successful experiments for im-proving the performance of part-of-speech taggers.</citsent>
<aftsection>
<nextsent>their results have been obtained by combining the output of different aggers with system combination techniques uch as majority voting.
</nextsent>
<nextsent>this approach cancels errors that are made by the minority of the taggers.
</nextsent>
<nextsent>with the best voting technique, the com-bined results decrease the lowest error rate of the component taggers by as much as 19% (van hal-teren et al, 1998).
</nextsent>
<nextsent>the fact that combination of classifiers leads to improved performance has been reported in large body of machine learning work.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A353">
<title id=" A00-2007.xml">noun phrase recognition by system combination </title>
<section> methods  and experiments.  </section>
<citcontext>
<prevsection>
<prevsent>the phrase $ 366.50 an ounce is noun phrase as well.
</prevsent>
<prevsent>however, it is not basenp since it contains two other noun phrases.
</prevsent>
</prevsection>
<citsent citstr=" W95-0107 ">
two basenp datasets have been put forward by (ramshaw and marcus, 1995).<papid> W95-0107 </papid></citsent>
<aftsection>
<nextsent>the main dataset consist of four sections (15-18) of the wall street journal (wsj) part of the penn treebank (marcus et al, 1993) <papid> J93-2004 </papid>as training material and one section (20) as test material 1.</nextsent>
<nextsent>the basenps in this data are slightly different from the ones that can be derived from the treebank, most notably in the attachment of genitive markers.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A358">
<title id=" A00-2007.xml">noun phrase recognition by system combination </title>
<section> methods  and experiments.  </section>
<citcontext>
<prevsection>
<prevsent>however, it is not basenp since it contains two other noun phrases.
</prevsent>
<prevsent>two basenp datasets have been put forward by (ramshaw and marcus, 1995).<papid> W95-0107 </papid></prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
the main dataset consist of four sections (15-18) of the wall street journal (wsj) part of the penn treebank (marcus et al, 1993) <papid> J93-2004 </papid>as training material and one section (20) as test material 1.</citsent>
<aftsection>
<nextsent>the basenps in this data are slightly different from the ones that can be derived from the treebank, most notably in the attachment of genitive markers.
</nextsent>
<nextsent>the recognition task involving arbitrary noun phrases attempts to find both basenps and noun phrases that contain other noun phrases.
</nextsent>
<nextsent>a stan-dard dataset for this task was put forward at the conll-99 workshop.
</nextsent>
<nextsent>it consist on the same parts of the penn treebank as the main basenp data set: wsj sections 15-18 as training data and section 20 as test data 2.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A417">
<title id=" A00-2007.xml">noun phrase recognition by system combination </title>
<section> re la ted  work.  </section>
<citcontext>
<prevsection>
<prevsent>they have applied their method to two segments of the penn treebank and these are still being used as benchmark datasets.
</prevsent>
<prevsent>several groups have continued working with the ramshaw and marcus datasets for base noun phrases.
</prevsent>
</prevsection>
<citsent citstr=" P98-1010 ">
(argamon et al, 1998) <papid> P98-1010 </papid>use memory-based sequence learning for recognizing both np chunks and vp chunks.</citsent>
<aftsection>
<nextsent>this method records pos tag se-quences which contain chunk boundaries and uses these sequences to classify the test data.
</nextsent>
<nextsent>its per-formance is somewhat worse than that of ramshaw and marcus (f~=1=91.6 vs. 92.0) but it is the best result obtained without using lexical information 6.
</nextsent>
<nextsent>(cardie and pierce, 1998) <papid> P98-1034 </papid>store pos tag sequences that make up complete chunks and use these se-quences as rules for classifying unseen data.</nextsent>
<nextsent>this approach performs worse than the method of arga-mon et al (f~=1=90.9).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A418">
<title id=" A00-2007.xml">noun phrase recognition by system combination </title>
<section> re la ted  work.  </section>
<citcontext>
<prevsection>
<prevsent>this method records pos tag se-quences which contain chunk boundaries and uses these sequences to classify the test data.
</prevsent>
<prevsent>its per-formance is somewhat worse than that of ramshaw and marcus (f~=1=91.6 vs. 92.0) but it is the best result obtained without using lexical information 6.
</prevsent>
</prevsection>
<citsent citstr=" P98-1034 ">
(cardie and pierce, 1998) <papid> P98-1034 </papid>store pos tag sequences that make up complete chunks and use these se-quences as rules for classifying unseen data.</citsent>
<aftsection>
<nextsent>this approach performs worse than the method of arga-mon et al (f~=1=90.9).
</nextsent>
<nextsent>three papers mention having used the memory- based learning method ibi-ig.
</nextsent>
<nextsent>(veenstra, 1998) in-troduced cascaded chunking, two-stage process in which the first stage classifications are used to im- prove the performance in second processing stage.
</nextsent>
<nextsent>this approach reaches the same performance vel as argamon et al but it requires lexical informa-tion.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A424">
<title id=" A00-2007.xml">noun phrase recognition by system combination </title>
<section> re la ted  work.  </section>
<citcontext>
<prevsection>
<prevsent>snow reaches the best performance on this task (fz=i =92.8).
</prevsent>
<prevsent>there has been less work on identifying eneral noun phrases than on recognizing basenps.
</prevsent>
</prevsection>
<citsent citstr=" W99-0708 ">
(os- borne, 1999) <papid> W99-0708 </papid>extended definite clause grammar with rules induced by learner that was based upon the maximum description length principle.</citsent>
<aftsection>
<nextsent>he pro-cessed other parts of the penn treebank than we with an f~=i rate of about 60.
</nextsent>
<nextsent>our earlier effort to process the conll dataset was performed in the same way as described in this paper but with- out using the combination method for basenps.
</nextsent>
<nextsent>we obtained an f~=i rate of 82.98 (conll-99, 1999).
</nextsent>
<nextsent>we have put forward method for recognizing noun phrases by combining the results of memory-based classifier applied to different representations of the data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A425">
<title id=" C00-1064.xml">structural feature selection for english korean statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>also, the inforum.tion can help to reduce the 1)arameter sl)ace of statisti( al alignment 1)y eliminat-ing synta(:tically uiflikely alignmenls.
</prevsent>
<prevsent>aligned texts have been used for derivation of 1)ilin- gual dictioimries and terminoh)gy databases which are useflfl for nlachine translation and cross lan-guages infornmtion retriewfl.
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
thus, lot of align-ment techniques have been suggested at; the sen-tence (gale et al, 1993), phrase (shin et al, 1996), nomt t)hrase (kupiec, 1993), word (brown et al, 1993; <papid> J93-2003 </papid>berger et al, 1996; <papid> J96-1002 </papid>melamed, 1997), collo-cation (smadja et al, 1996) and terminology level.</citsent>
<aftsection>
<nextsent>seine work has used lexical association measures for word alignments.
</nextsent>
<nextsent>however, the association mea-sures could be misled since word in source lan-guage frequently co-occurs with more titan one word in target language.
</nextsent>
<nextsent>in other work, iterative re-estimation techniques have beets emt)loyed.
</nextsent>
<nextsent>they were usually incorporated with the em algorithm mid dynmnic progranmfing.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A426">
<title id=" C00-1064.xml">structural feature selection for english korean statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>also, the inforum.tion can help to reduce the 1)arameter sl)ace of statisti( al alignment 1)y eliminat-ing synta(:tically uiflikely alignmenls.
</prevsent>
<prevsent>aligned texts have been used for derivation of 1)ilin- gual dictioimries and terminoh)gy databases which are useflfl for nlachine translation and cross lan-guages infornmtion retriewfl.
</prevsent>
</prevsection>
<citsent citstr=" J96-1002 ">
thus, lot of align-ment techniques have been suggested at; the sen-tence (gale et al, 1993), phrase (shin et al, 1996), nomt t)hrase (kupiec, 1993), word (brown et al, 1993; <papid> J93-2003 </papid>berger et al, 1996; <papid> J96-1002 </papid>melamed, 1997), collo-cation (smadja et al, 1996) and terminology level.</citsent>
<aftsection>
<nextsent>seine work has used lexical association measures for word alignments.
</nextsent>
<nextsent>however, the association mea-sures could be misled since word in source lan-guage frequently co-occurs with more titan one word in target language.
</nextsent>
<nextsent>in other work, iterative re-estimation techniques have beets emt)loyed.
</nextsent>
<nextsent>they were usually incorporated with the em algorithm mid dynmnic progranmfing.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A436">
<title id=" A00-2003.xml">a probabilistic genre independent model of pronominal ization </title>
<section> factors in pronoun generation.  </section>
<citcontext>
<prevsection>
<prevsent>unfortunately, they do not eval-uate the contribution of each of these factors, so we do not know which ones are important.
</prevsent>
<prevsent>work on corpus-based approaches to anaphora resolution is more numerous.
</prevsent>
</prevsection>
<citsent citstr=" W98-1119 ">
ge et al (1998) <papid> W98-1119 </papid>describe supervised probabilistic pronoun resolu-tion algorithm which is based on complete syntac-tic information.</citsent>
<aftsection>
<nextsent>the factors they use include dis-tance from last mention, syntactic function and con-text, agreement information, animacy of the refer-ent, simplified notion of selectional restrictions, 18 agree syn class synante form ante dist dist4 par ambig agreement in person, gender, and number syntactic function sortal class (cf.
</nextsent>
<nextsent>tab.
</nextsent>
<nextsent>2) syntactic function of antecedent.
</nextsent>
<nextsent>  for first mention,   for deadend form of antecedent (pers.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A437">
<title id=" A00-2003.xml">a probabilistic genre independent model of pronominal ization </title>
<section> factors in pronoun generation.  </section>
<citcontext>
<prevsection>
<prevsent>np, indef.
</prevsent>
<prevsent>np, proper name) distance to last mention in units dist reduced to 4 values (deadend, dist=0, dist= 1, dist =2) parallelism (syn=synante) number of competing discourse ntities table 1: overview of factors and the length of the coreference chain.
</prevsent>
</prevsection>
<citsent citstr=" W99-0611 ">
cardie &amp; wagstaff (1999) <papid> W99-0611 </papid>describe an unsupervised algorithm for noun phrase coreference resolution.</citsent>
<aftsection>
<nextsent>their fac-tors are taken from ge et al (1998), <papid> W98-1119 </papid>with two excep- tions.</nextsent>
<nextsent>first, they replace complete syntactic infor-mation with information about np bracketing.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A439">
<title id=" A00-2003.xml">a probabilistic genre independent model of pronominal ization </title>
<section> factors in pronoun generation.  </section>
<citcontext>
<prevsection>
<prevsent>first, they replace complete syntactic infor-mation with information about np bracketing.
</prevsent>
<prevsent>sec-ond, they use the sortal class of the referent which they determine on the basis of wordnet (fellbaum, 1998).
</prevsent>
</prevsection>
<citsent citstr=" J95-2003 ">
there has been no comparison between corpus- based approaches for anaphora resolution and more traditional algorithms based on focusing (sidner, 1983) or centering (grosz et al, 1995) <papid> J95-2003 </papid>except for azzam et al (1998).<papid> P98-1011 </papid></citsent>
<aftsection>
<nextsent>however, their comparison is flawed by evaluating syntax-based focus algo-rithm on the basis of insufficient syntactic informa-tion.
</nextsent>
<nextsent>for pronoun generation, the original centering model (grosz et al, 1995) <papid> J95-2003 </papid>provides rule which is supposed to decide whether referring expression has to be realized as pronoun.</nextsent>
<nextsent>however, this rule applies only to the referring expression which is the backward-looking center (cb) of the current utter- ance.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A441">
<title id=" A00-2003.xml">a probabilistic genre independent model of pronominal ization </title>
<section> factors in pronoun generation.  </section>
<citcontext>
<prevsection>
<prevsent>first, they replace complete syntactic infor-mation with information about np bracketing.
</prevsent>
<prevsent>sec-ond, they use the sortal class of the referent which they determine on the basis of wordnet (fellbaum, 1998).
</prevsent>
</prevsection>
<citsent citstr=" P98-1011 ">
there has been no comparison between corpus- based approaches for anaphora resolution and more traditional algorithms based on focusing (sidner, 1983) or centering (grosz et al, 1995) <papid> J95-2003 </papid>except for azzam et al (1998).<papid> P98-1011 </papid></citsent>
<aftsection>
<nextsent>however, their comparison is flawed by evaluating syntax-based focus algo-rithm on the basis of insufficient syntactic informa-tion.
</nextsent>
<nextsent>for pronoun generation, the original centering model (grosz et al, 1995) <papid> J95-2003 </papid>provides rule which is supposed to decide whether referring expression has to be realized as pronoun.</nextsent>
<nextsent>however, this rule applies only to the referring expression which is the backward-looking center (cb) of the current utter- ance.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A444">
<title id=" A00-2003.xml">a probabilistic genre independent model of pronominal ization </title>
<section> factors in pronoun generation.  </section>
<citcontext>
<prevsection>
<prevsent>however, this rule applies only to the referring expression which is the backward-looking center (cb) of the current utter-ance.
</prevsent>
<prevsent>with respect all other referring expression in this utterance centering is underspecified.
</prevsent>
</prevsection>
<citsent citstr=" J97-1007 ">
yeh &amp; mellish (1997) <papid> J97-1007 </papid>propose set of hand-crafted rules for the generation of anaphora (zero and personal pronouns, full nps) in chinese.</citsent>
<aftsection>
<nextsent>how- ever, the factors which appear to be important in their evaluation are similar to factors described by authors mentioned above: distance, syntactic constraints on zero pronouns, discourse structure, salience and animacy of discourse ntities.
</nextsent>
<nextsent>2.2 our factors.
</nextsent>
<nextsent>the factors we investigate in this paper only relyon annotations of nps and their co-specification rela-tions.
</nextsent>
<nextsent>we did not add any discourse structural anno-tation, because (1) the texts are extracts from larger texts which are not available to us, and (2) we have not yet found labelling scheme for discourse struc-ture that has an inter-coder reliability comparable to the muc coreference annotation scheme.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A445">
<title id=" A00-2003.xml">a probabilistic genre independent model of pronominal ization </title>
<section> data.  </section>
<citcontext>
<prevsection>
<prevsent>the mcus were labelled by the sec-ond author.
</prevsent>
<prevsent>all referring expressions were anno-tated with agreement and sortal class information.
</prevsent>
</prevsection>
<citsent citstr=" W99-0107 ">
labels were placed using the gui-based annotation tool referee (decristofaro et al, 1999).<papid> W99-0107 </papid></citsent>
<aftsection>
<nextsent>the annotators developed the sortal class anno-tation guidelines on the basis of two training texts.
</nextsent>
<nextsent>then, both labellers annotated two texts from each genre independently (eight in total).
</nextsent>
<nextsent>these eight texts were used to determine the reliability of the sortal class coding scheme.
</nextsent>
<nextsent>since sortal class an-notation is intrinsically hard, the annotators looked up the senses of the head noun of each referring np that was not pronoun or proper name in word-net.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A446">
<title id=" A00-2003.xml">a probabilistic genre independent model of pronominal ization </title>
<section> data.  </section>
<citcontext>
<prevsection>
<prevsent>of second person singular pronouns, perc.
</prevsent>
<prevsent>of third person singular masculine and feminine pronouns, reed.
</prevsent>
</prevsection>
<citsent citstr=" J96-2004 ">
len.: median length of sequences ofco-specifying referring expressions with cohen n (cohen, 1960; carletta, 1996).<papid> J96-2004 </papid></citsent>
<aftsection>
<nextsent>co-hen (1960) shows that n between 0.68 and 0.80 al-lows tentative conclusions, while e;   0.80 indicates reliable annotations.
</nextsent>
<nextsent>for genres cf (n = 0.83), ck (n = 0.84) and cl (n = 0.83), the sortal class an-notations were indeed reliable, but not for genre cg (n = 0.63).
</nextsent>
<nextsent>nevertheless, overall, the sortal class annotations were reliable (n ---- 0.8).
</nextsent>
<nextsent>problems are mainly due to the abstract classes concept, action, event, state, and property.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A447">
<title id=" A00-2003.xml">a probabilistic genre independent model of pronominal ization </title>
<section> towards probabilistic.  </section>
<citcontext>
<prevsection>
<prevsent>we use different kind of model here, logistic regression, which is especially well suited for categorical data analysis (cf.
</prevsent>
<prevsent>eg.
</prevsent>
</prevsection>
<citsent citstr=" P97-1005 ">
agresti (1990) or kessler et al (1997)).<papid> P97-1005 </papid></citsent>
<aftsection>
<nextsent>in this model, the value of the binary target variable is predicted by lin-ear combination of the predictor variables.
</nextsent>
<nextsent>vari-able weights indicate the importance of variable for classification: the higher the absolute value of the weight, the more important is. logistic regression models are not only evaluated by their performance on training and test data.
</nextsent>
<nextsent>we could easily construct perfect model of any train-ing dataset with variables, where is the size of the dataset.
</nextsent>
<nextsent>but we need models that are small, yet predict the target values well.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A448">
<title id=" A00-2003.xml">a probabilistic genre independent model of pronominal ization </title>
<section> future  work.  </section>
<citcontext>
<prevsection>
<prevsent>the variation might be just that - stylistic variation.
</prevsent>
<prevsent>it might stem from one of the traditional factors that we did not take into account here, such as thematic role.
</prevsent>
</prevsection>
<citsent citstr=" W99-0108 ">
however, we suspect that the crucial factor at play here is dis-course structure (mccoy &amp; strube, 1999).<papid> W99-0108 </papid></citsent>
<aftsection>
<nextsent>acknowledgements work on this paper was be- gun while michael strube was postdoctoral fellow at the institute for research in cognitive science, university of pennsylvania, nd maria wolters vis-ited the institute for week in summer 1999.
</nextsent>
<nextsent>we would like to thank kathleen mccoy, jonathan de-cristofaro, and the three anonymous reviewers for their comments on earlier stages of this work.
</nextsent>


</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A449">
<title id=" C00-2119.xml">using a broad coverage parser for word breaking in japanese </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the most likely analysis can be obtained by searching for the path with the minimum connective cost (hisamitsu and nitta 1990), often supplemented by additional heuristic devices such as the longest-string-match or the least-number-of-bunsetsu (phrase).
</prevsent>
<prevsent>despite its popularity, the connective cost method has major disadvantage in that hand-tuning is not only labor-intensive but also unsafe, since adjusting the cost for one string may cause another to break.
</prevsent>
</prevsection>
<citsent citstr=" P98-1068 ">
various heuristic (e.g. kurohashi and nagao 1998) and statistical (e.g. takeuchi and matsumoto 1997) augmentations of the minimum connective cost method have been proposed, bringing segmentation accuracy up to around 98-99% (e.g. kurohashi and nagao 1998, fuchi and takagi 1998).<papid> P98-1068 </papid></citsent>
<aftsection>
<nextsent>fully stochastic language models (e.g. nagata 1994), <papid> C94-1032 </papid>on the other hand, do not allow such manual cost manipulation and precisely for that reason, improvements in segmentation accuracy are harder to achieve.</nextsent>
<nextsent>attaining high accuracy using fully stochastic methods is particularly difficult for japanese due to the prevalence of orthographic variants (a word can be spelled in many different ways by combining different character sets), which exacerbates the sparse data problem.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A450">
<title id=" C00-2119.xml">using a broad coverage parser for word breaking in japanese </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>despite its popularity, the connective cost method has major disadvantage in that hand-tuning is not only labor-intensive but also unsafe, since adjusting the cost for one string may cause another to break.
</prevsent>
<prevsent>various heuristic (e.g. kurohashi and nagao 1998) and statistical (e.g. takeuchi and matsumoto 1997) augmentations of the minimum connective cost method have been proposed, bringing segmentation accuracy up to around 98-99% (e.g. kurohashi and nagao 1998, fuchi and takagi 1998).<papid> P98-1068 </papid></prevsent>
</prevsection>
<citsent citstr=" C94-1032 ">
fully stochastic language models (e.g. nagata 1994), <papid> C94-1032 </papid>on the other hand, do not allow such manual cost manipulation and precisely for that reason, improvements in segmentation accuracy are harder to achieve.</citsent>
<aftsection>
<nextsent>attaining high accuracy using fully stochastic methods is particularly difficult for japanese due to the prevalence of orthographic variants (a word can be spelled in many different ways by combining different character sets), which exacerbates the sparse data problem.
</nextsent>
<nextsent>as result, the performance of stochastic models is usually not as good as the heuristics-based language models.
</nextsent>
<nextsent>the best accuracy reported for statistical methods to date is around 95% (e.g. nagata 1994).<papid> C94-1032 </papid></nextsent>
<nextsent>our approach contrasts with the previous approaches in that the word-breaking component itself does not perform the selection of the best segmentation analysis at all.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A452">
<title id=" C00-2119.xml">using a broad coverage parser for word breaking in japanese </title>
<section> using broad-coverage parser for.  </section>
<citcontext>
<prevsection>
<prevsent>this gives the grammar the power to disambiguate multiple word analyses in the input lattice.
</prevsent>
<prevsent>13ecause we do not utilize semantic information, we perforln no selnantically motivated attachlnent of phrases during parsing.
</prevsent>
</prevsection>
<citsent citstr=" P98-2180 ">
instead, we parse them into default analysis, which can then be expanded and disambiguatcd at later stages of processing using large semantic knowledge base (richardson 1997, richardson et al 1998).<papid> P98-2180 </papid></citsent>
<aftsection>
<nextsent>one of the goals o1  ihis paper is to show that the syntactic information alone can resolve the ambiguities in the word lattice sufficiently well to select the best breaking analysis in the absence of elaborate semantic information.
</nextsent>
<nextsent>figure 1 (see appendix) shows the default attachment of the relative clause to the closest np.
</nextsent>
<nextsent>though this structure may be semantically implausible, the word-breaking analysis is correct.
</nextsent>
<nextsent>the word-breaking colnponent of out  system is described in detail in kacmarcik et al (2000).<papid> C00-1057 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A453">
<title id=" C00-2119.xml">using a broad coverage parser for word breaking in japanese </title>
<section> using broad-coverage parser for.  </section>
<citcontext>
<prevsection>
<prevsent>figure 1 (see appendix) shows the default attachment of the relative clause to the closest np.
</prevsent>
<prevsent>though this structure may be semantically implausible, the word-breaking analysis is correct.
</prevsent>
</prevsection>
<citsent citstr=" C00-1057 ">
the word-breaking colnponent of out  system is described in detail in kacmarcik et al (2000).<papid> C00-1057 </papid></citsent>
<aftsection>
<nextsent>for the lmrpose of robust parsing, the component is expected to solve the following two problems: ? lemmatization: find possible words in the input text using dictionary and its inflectional morphology, and return the dictionary entry forms (lemmas).
</nextsent>
<nextsent>note that multiple lemmas are often possible forgiven inflected form (e.g. surface form /o,~z -(- (kalte) could be an inflected form of the verbs /9~3 (kau  buy ), /0~o (katu  win )or /o,~ (karu  trim ), in which case all these forms must be returned.
</nextsent>
<nextsent>the dictionary the word-breaker uses has about 70,000 unique entries.
</nextsent>
<nextsent>orthography norlnalization: identify and norlnalize orthographic variants.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A454">
<title id=" C00-2136.xml">automatic acquisition of domain knowledge for information extraction </title>
<section> the extraction system.  </section>
<citcontext>
<prevsection>
<prevsent>it is possible to build these constituent struc-tures through flfll syntactic analysis of the text, and the discovery procedure we describe below woul(1 be applicable to such an architec-ture.
</prevsent>
<prevsent>howe, ver, for resells; of slme,(t , coverage, and system rolmstness, the more (:ommon ap- t)roa(:h at present is to peribrni t)artial syn-tactic analysis using cascade of finite-state transducers.
</prevsent>
</prevsection>
<citsent citstr=" M95-1014 ">
this is the at)t)roa(:h used by our e.xtraction system (grishman, 1995; <papid> M95-1014 </papid>yangarber and grishman, 1998).<papid> M98-1011 </papid></citsent>
<aftsection>
<nextsent>at; the heart of our syslx an is regular ex-pression pattern matcher which is cal)al)le of matching set of regular exl)ressions against partially-analyzed text and producing addi-tional annotations on the text.
</nextsent>
<nextsent>this core draws on set of knowledge bases of w~rying degrees of domain- and task-specificity.
</nextsent>
<nextsent>the lexicon in-cludes both general english dictionary and definitions of domain and scenario terms.
</nextsent>
<nextsent>the concept base arranges the domain terms into semantic hierarchy.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A455">
<title id=" C00-2136.xml">automatic acquisition of domain knowledge for information extraction </title>
<section> the extraction system.  </section>
<citcontext>
<prevsection>
<prevsent>it is possible to build these constituent struc-tures through flfll syntactic analysis of the text, and the discovery procedure we describe below woul(1 be applicable to such an architec-ture.
</prevsent>
<prevsent>howe, ver, for resells; of slme,(t , coverage, and system rolmstness, the more (:ommon ap- t)roa(:h at present is to peribrni t)artial syn-tactic analysis using cascade of finite-state transducers.
</prevsent>
</prevsection>
<citsent citstr=" M98-1011 ">
this is the at)t)roa(:h used by our e.xtraction system (grishman, 1995; <papid> M95-1014 </papid>yangarber and grishman, 1998).<papid> M98-1011 </papid></citsent>
<aftsection>
<nextsent>at; the heart of our syslx an is regular ex-pression pattern matcher which is cal)al)le of matching set of regular exl)ressions against partially-analyzed text and producing addi-tional annotations on the text.
</nextsent>
<nextsent>this core draws on set of knowledge bases of w~rying degrees of domain- and task-specificity.
</nextsent>
<nextsent>the lexicon in-cludes both general english dictionary and definitions of domain and scenario terms.
</nextsent>
<nextsent>the concept base arranges the domain terms into semantic hierarchy.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A456">
<title id=" C00-2136.xml">automatic acquisition of domain knowledge for information extraction </title>
<section> methodology.  </section>
<citcontext>
<prevsection>
<prevsent>before at)plying exdisco , we pre-proeessed the cortms using general-purpose pendency parser of english.
</prevsent>
<prevsent>the parser is based on the fdg tbrmalism (tapanainen and jgrvi- hen, 1997) and developed by the research unit for multilingual language technology at the university of helsinki, and conexor oy.
</prevsent>
</prevsection>
<citsent citstr=" A00-1039 ">
the parser is used ibr reducing each clause or noun phrase to tuple, consisting of the central ar-guments, ms described in detail in (yangarber et al, 2000).<papid> A00-1039 </papid></citsent>
<aftsection>
<nextsent>we used corlms of 9,224 articles from the wall street; journal.
</nextsent>
<nextsent>the parsed arti-cles yielded total of 440,000 clausal tuples, of which 215,000 were distinct.
</nextsent>
<nextsent>3.2 normal ization.
</nextsent>
<nextsent>we applied name recognition module prior to parsing, and replaced each name with token describing its (:lass, e.g. c-person, c-company, etc. we collapsed together all numeric expres-sions, currency wflues, dates, etc., using single token to designate ach of these classes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A457">
<title id=" C00-2136.xml">automatic acquisition of domain knowledge for information extraction </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>prcl)aring good t)atterns tbr these syste, ms re-quires (:onsiderable skill, and achieving good (:overage requires |;lie analysis of large amount of text.
</prevsent>
<prevsent>these t)rol)lems h~ve t)een impedinmnts to the -wide\].  use of extraction systenls.
</prevsent>
</prevsection>
<citsent citstr=" M92-1021 ">
these dit\[iculties have stimulate.d resear( on 1)attel .  a ( : ( lu s t ion . so lne f th s work has en - i)hasized il\]teractive tools to (:onvert examples to extractioi~ t)atterlls (yangarber and grish-man, 1997); nmch ot:  there, search has focused on methods for automatically converting cortms annotated with extraction examples into pat-terns (lehnert et al, 1992; <papid> M92-1021 </papid>fisher et al, 1995; <papid> M95-1011 </papid>miller el; al., 1998).</citsent>
<aftsection>
<nextsent>these techniques may re-duce the level of systeln expertise required to develop new extraction n)plieation, but they do not lessen the lmrden of studying large cor- lms in order to .find relevant candidates.
</nextsent>
<nextsent>the prior work most closely related to our own is that of (r.ilotf, 1996), who also seeks to lmild pattenls automatically without the need to annotate corpus with the information to be extracted.
</nextsent>
<nextsent>itowever, her work ditfers rom 01217 own in several lnportant respects.
</nextsent>
<nextsent>first, her patterns identit~y phrases that fill individual slots in the template, without specifying how these slots may be combined at later stage into complete templates.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A458">
<title id=" C00-2136.xml">automatic acquisition of domain knowledge for information extraction </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>prcl)aring good t)atterns tbr these syste, ms re-quires (:onsiderable skill, and achieving good (:overage requires |;lie analysis of large amount of text.
</prevsent>
<prevsent>these t)rol)lems h~ve t)een impedinmnts to the -wide\].  use of extraction systenls.
</prevsent>
</prevsection>
<citsent citstr=" M95-1011 ">
these dit\[iculties have stimulate.d resear( on 1)attel .  a ( : ( lu s t ion . so lne f th s work has en - i)hasized il\]teractive tools to (:onvert examples to extractioi~ t)atterlls (yangarber and grish-man, 1997); nmch ot:  there, search has focused on methods for automatically converting cortms annotated with extraction examples into pat-terns (lehnert et al, 1992; <papid> M92-1021 </papid>fisher et al, 1995; <papid> M95-1011 </papid>miller el; al., 1998).</citsent>
<aftsection>
<nextsent>these techniques may re-duce the level of systeln expertise required to develop new extraction n)plieation, but they do not lessen the lmrden of studying large cor- lms in order to .find relevant candidates.
</nextsent>
<nextsent>the prior work most closely related to our own is that of (r.ilotf, 1996), who also seeks to lmild pattenls automatically without the need to annotate corpus with the information to be extracted.
</nextsent>
<nextsent>itowever, her work ditfers rom 01217 own in several lnportant respects.
</nextsent>
<nextsent>first, her patterns identit~y phrases that fill individual slots in the template, without specifying how these slots may be combined at later stage into complete templates.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A459">
<title id=" A94-1008.xml">tagging accurately  dont guess if you know </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>whenever there is conflict between the systems, we trust the analysis proposed by the knowledge- based system.
</prevsent>
<prevsent>whenever the knowledge-based sys-tem leaves an ambiguity unresolved, we select that alternative which is closest the selection made by the statistical system.
</prevsent>
</prevsection>
<citsent citstr=" A92-1018 ">
the two systems we use are engcg (karlsson et al, 1994) and the xerox tagger (cutting et al, 1992).<papid> A92-1018 </papid></citsent>
<aftsection>
<nextsent>we discuss problems caused by the fact that these taggers use different ag sets, and present he results obtained by applying the combined taggers to previously unseen sample of text.
</nextsent>
<nextsent>2.1 english constraint grammar parser.
</nextsent>
<nextsent>the english constraint grammar parser, engcg (voutilainen et al, 1992; karlsson el al., 1994), is based on constraint grammar, aparsing framework proposed by fred karlsson (1990).<papid> C90-3030 </papid></nextsent>
<nextsent>it was developed 1989-1993 at the research unit for computational linguistics, university of helsinki, by atro voutilai-nen, juha heikkil~i and arto anttila; later on, ti- mo jrvinen has extended the syntactic description, and pasi tapanainen has made new fast imple-mentation of the cg parsing program.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A460">
<title id=" A94-1008.xml">tagging accurately  dont guess if you know </title>
<section> the taggers in outline.  </section>
<citcontext>
<prevsection>
<prevsent>we discuss problems caused by the fact that these taggers use different ag sets, and present he results obtained by applying the combined taggers to previously unseen sample of text.
</prevsent>
<prevsent>2.1 english constraint grammar parser.
</prevsent>
</prevsection>
<citsent citstr=" C90-3030 ">
the english constraint grammar parser, engcg (voutilainen et al, 1992; karlsson el al., 1994), is based on constraint grammar, aparsing framework proposed by fred karlsson (1990).<papid> C90-3030 </papid></citsent>
<aftsection>
<nextsent>it was developed 1989-1993 at the research unit for computational linguistics, university of helsinki, by atro voutilai-nen, juha heikkil~i and arto anttila; later on, ti- mo jrvinen has extended the syntactic description, and pasi tapanainen has made new fast imple-mentation of the cg parsing program.
</nextsent>
<nextsent>engcg is primarily designed for the analysis of standard writ-ten english of the british and american varieties.
</nextsent>
<nextsent>in the development and testing of the system, over 100 million words of running text have been used.
</nextsent>
<nextsent>the engtwol lexicon is based on the two-level model (koskenniemi, 1983).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A461">
<title id=" A94-1008.xml">tagging accurately  dont guess if you know </title>
<section> the taggers in outline.  </section>
<citcontext>
<prevsection>
<prevsent>usually at least 997 words out of every thousand retain the contextually appro-priate morphological reading, i.e. the recall usually is at least 99.7 %.
</prevsent>
<prevsent>if the heuristic constraints are also used, the ambiguity rate falls to 1.02-1.04 read-ings per word, with an overall recall of about 99.5 %.
</prevsent>
</prevsection>
<citsent citstr=" J93-2006 ">
this accuracy compares very favourably with results reported in (de marcken, 1990; weisehedel et al, 1993; kempe, 1994) - for instance, to reach the recall of 99.3 %, the system by (weischedel et al, 1993) <papid> J93-2006 </papid>has to leave as many as three readings per word in its output.</citsent>
<aftsection>
<nextsent>2.2 xerox tagger.
</nextsent>
<nextsent>the xerox tagger 1, xt, (cutting et al, 1992) <papid> A92-1018 </papid>is statistical tagger made by doug cutting, julian kupiec, jan pedersen and penelope sibun in xerox parc.</nextsent>
<nextsent>it was trained on the untagged brown cor-pus (francis and kubera, 1982).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A463">
<title id=" A94-1008.xml">tagging accurately  dont guess if you know </title>
<section> the taggers in outline.  </section>
<citcontext>
<prevsection>
<prevsent>for instance, for the word live there are the following alternative analyses: j (ad- jective) and vb (uninflected verb).
</prevsent>
<prevsent>unknown words not recognised by suffix tables get al tags from specific set (called open-class).
</prevsent>
</prevsection>
<citsent citstr=" H89-1054 ">
the tagger itself is based on the hidden markov model (baum, 1972) and word equivalence classes (kupiec, 1989).<papid> H89-1054 </papid></citsent>
<aftsection>
<nextsent>although the tagger is trained with the untagged brown corpus, there are several ways to  force  it to learn.
</nextsent>
<nextsent>the symbol biases represent kind of lexical probabilities forgiven word equivalence classes.
</nextsent>
<nextsent>the transition biases can be used for saying that it is likely or unlikely that tag is followed by some specific tag.
</nextsent>
<nextsent>the biases serve as default values for the hidden markov model before the training.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A464">
<title id=" A94-1047.xml">sublanguage engineering in the fog system </title>
<section> overview </section>
<citcontext>
<prevsection>
<prevsent>how- ever, some regularization and simplifica-tion of the output has been needed.
</prevsent>
<prevsent>sub-language ngineering issues include trade-offs in coverage and style, handling varia-tion and evolution of sublanguages,  leg-islating  lexical semantics and assuring language model able to accomodate new text types and support spoken output.
</prevsent>
</prevsection>
<citsent citstr=" C90-1021 ">
fog (for forecast generator) was developed ur-ing 1985-89 (kittredge al., 1986; bourbeau et al , 1990).<papid> C90-1021 </papid></citsent>
<aftsection>
<nextsent>after tests at environment canada during 1989-91, fog entered regular use during 1991-92, first for marine forecasts, and more recently for pub-lic forecasts.
</nextsent>
<nextsent>forty percent of the operational marine forecasts (roughly half of all marine forecast text) in canada is now produced using fog.
</nextsent>
<nextsent>meteorologists have been very receptive to using fog, which is now  back-end  facility of the fpa graphics workstation.
</nextsent>
<nextsent>the fpa supports the graph-ical analysis of weather while providing the rule- based concept formation eeded to drive both text generation and non-linguistic applications.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A465">
<title id=" C00-1078.xml">chart based transfer rule application in machine translation </title>
<section> introduction.  </section>
<citcontext>
<prevsection>
<prevsent>evaluation of mmmrous competing sets.
</prevsent>
<prevsent>in fact, the number of l)ossible transfer ule combinations increas-es exponentially with the length of the source, language sentence,.
</prevsent>
</prevsection>
<citsent citstr=" W98-1115 ">
this situation mirrors the t)roblem of choosing productions in non deter minis tic parser, in this pai)er, we descril)e system for choosing transfer ules, based on s- tatistical chart parsing (bol)row, 1990; chitrao and grishman, 1990; caraballo and charniak, 1997; charniak et al, 1998).<papid> W98-1115 </papid></citsent>
<aftsection>
<nextsent>in our machine % anslation system, transfer rules are generated automatically from parsed parallel text along the lines of (matsulnoto el; al,, 1993; meyers et al, 1996; <papid> C96-1078 </papid>meyers et al, 1998<papid> P98-2139 </papid>b).</nextsent>
<nextsent>our system tends to acquire large nmnber of transt~r rules, due lnainly to 3,1terna- tive ways of translating the same sequences of words, non-literal translations in parallel text and parsing e, rrors.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A466">
<title id=" C00-1078.xml">chart based transfer rule application in machine translation </title>
<section> introduction.  </section>
<citcontext>
<prevsection>
<prevsent>in fact, the number of l)ossible transfer ule combinations increas-es exponentially with the length of the source, language sentence,.
</prevsent>
<prevsent>this situation mirrors the t)roblem of choosing productions in non deter minis tic parser, in this pai)er, we descril)e system for choosing transfer ules, based on s- tatistical chart parsing (bol)row, 1990; chitrao and grishman, 1990; caraballo and charniak, 1997; charniak et al, 1998).<papid> W98-1115 </papid></prevsent>
</prevsection>
<citsent citstr=" C96-1078 ">
in our machine % anslation system, transfer rules are generated automatically from parsed parallel text along the lines of (matsulnoto el; al,, 1993; meyers et al, 1996; <papid> C96-1078 </papid>meyers et al, 1998<papid> P98-2139 </papid>b).</citsent>
<aftsection>
<nextsent>our system tends to acquire large nmnber of transt~r rules, due lnainly to 3,1terna- tive ways of translating the same sequences of words, non-literal translations in parallel text and parsing e, rrors.
</nextsent>
<nextsent>it is therefore crucial that our system choose the best set of rules efficient-ly.
</nextsent>
<nextsent>while the technique discussed he.re obviously applies to similar such systems, it could also ap-ply to hand-coded systems in which each word or group of words is related to more than one transfer ule.
</nextsent>
<nextsent>d)r example, both multra (hein, 1996) and the eurotra system described in (way el; al., 1997) require components for deciding which combination of transtbr ules to use.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A467">
<title id=" C00-1078.xml">chart based transfer rule application in machine translation </title>
<section> introduction.  </section>
<citcontext>
<prevsection>
<prevsent>in fact, the number of l)ossible transfer ule combinations increas-es exponentially with the length of the source, language sentence,.
</prevsent>
<prevsent>this situation mirrors the t)roblem of choosing productions in non deter minis tic parser, in this pai)er, we descril)e system for choosing transfer ules, based on s- tatistical chart parsing (bol)row, 1990; chitrao and grishman, 1990; caraballo and charniak, 1997; charniak et al, 1998).<papid> W98-1115 </papid></prevsent>
</prevsection>
<citsent citstr=" P98-2139 ">
in our machine % anslation system, transfer rules are generated automatically from parsed parallel text along the lines of (matsulnoto el; al,, 1993; meyers et al, 1996; <papid> C96-1078 </papid>meyers et al, 1998<papid> P98-2139 </papid>b).</citsent>
<aftsection>
<nextsent>our system tends to acquire large nmnber of transt~r rules, due lnainly to 3,1terna- tive ways of translating the same sequences of words, non-literal translations in parallel text and parsing e, rrors.
</nextsent>
<nextsent>it is therefore crucial that our system choose the best set of rules efficient-ly.
</nextsent>
<nextsent>while the technique discussed he.re obviously applies to similar such systems, it could also ap-ply to hand-coded systems in which each word or group of words is related to more than one transfer ule.
</nextsent>
<nextsent>d)r example, both multra (hein, 1996) and the eurotra system described in (way el; al., 1997) require components for deciding which combination of transtbr ules to use.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A485">
<title id=" C00-1078.xml">chart based transfer rule application in machine translation </title>
<section> parses  and  transfer  rules.  </section>
<citcontext>
<prevsection>
<prevsent>nodes (wfl- ues) are labeled with headwords and arcs (fea- tures) are labeled with gramma~;ical thnetions (subject, object), 1)repositions (in) and subor-dinate conjunctions (benre).
</prevsent>
<prevsent>a for demonstra-tion purposes, the source tree in figure 1 is the input to our translation system and the target tree is the outl)ut. the t;ransfer rules in figure 2 can be used to convert the intmt; tree into the out- 1)at tree.
</prevsent>
</prevsection>
<citsent citstr=" P93-1004 ">
these transtbr rules are pairs of corresponding rooted substructures, where substructure (matsumoto et al, 1993) <papid> P93-1004 </papid>is connected set of arcs and nodes.</citsent>
<aftsection>
<nextsent>a rule amorphologieal features and their values (gram- number: plural) are also represented as ares and nodes.
</nextsent>
<nextsent>consists of o, ither pair of  open  substructures (rule 4) or pair of  closed  substructures (rules 1, 2 and 3).
</nextsent>
<nextsent>closed substructures consist of s- ingle nodes (a,a ,b,b ,c ) or subtrees (the left hand side of rule 3).
</nextsent>
<nextsent>open substructures con-tain one or more open arcs, arcs without heads (both sul)structures in rule 4).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A486">
<title id=" A92-1020.xml">a corpus based statistical approach to automatic book indexing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>attempts have been made over the years to automate this procedure for the apparent benefits of cost saving, shorter preparation time, and possibility of producing more complete and consistent indexes.
</prevsent>
<prevsent>early work involves using occurrence characteristics of contents words \[borko, 1970\].
</prevsent>
</prevsection>
<citsent citstr=" P88-1025 ">
later people came to realize that indexes are often multi-word terms and their generation might involve more elaborated syntactic analysis on phrasal or sentential level \[salton, 1988; <papid> P88-1025 </papid>dillon and mcdonald, 1983\].</citsent>
<aftsection>
<nextsent>however, full syntactical pproach \[salton, 1988\] <papid> P88-1025 </papid>to this task has real problem with efficiency and coverage for unrestricted text.</nextsent>
<nextsent>no viable automatic solution is currently in use.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A488">
<title id=" A92-1020.xml">a corpus based statistical approach to automatic book indexing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>until recently, this problem has been considered difficult to solve without elaborated syntactical and semantic analyses \[chen, 1988\].
</prevsent>
<prevsent>recent research advances may lead to the development of viable book indexing methods for chinese books.
</prevsent>
</prevsection>
<citsent citstr=" A88-1019 ">
these include the availability of efficient and high precision word segmentation methods for chinese text \[chang et al, 1991; sproat and shih, 1990; wang et al, 1990\], the availability of statistical analysis of chinese corpus \[liu et al, 1975\] and large-scale lec tronic chinese dictionaries with part- of-speech information \[chang et al, 1988; bdc, 1992\], the corpus-based statistical part-of-speech tagger \[church, 1988; <papid> A88-1019 </papid>derose, 1988; <papid> J88-1003 </papid>beale, 1988\], <papid> P88-1026 </papid>as well as phrasal and clausal analyzers \[church 1988; <papid> A88-1019 </papid>ejerhed 1990\]</citsent>
<aftsection>
<nextsent>as being pointed out in \[salton, 1988\], <papid> P88-1025 </papid>back-of-book indexes may consist of more than one word that are derived from noun phrase.</nextsent>
<nextsent>given the text of book, an indexing system, must perform some kind of phrasal and statistical analysis in order to produce list of candidate indexes and their occurrence statistics in order to generate indexes as shown in figure 1 which is an excerpt from the reconstruction of indexes of book on transformational grammar for mandarin chinese \[tang, 1977\].</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A489">
<title id=" A92-1020.xml">a corpus based statistical approach to automatic book indexing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>until recently, this problem has been considered difficult to solve without elaborated syntactical and semantic analyses \[chen, 1988\].
</prevsent>
<prevsent>recent research advances may lead to the development of viable book indexing methods for chinese books.
</prevsent>
</prevsection>
<citsent citstr=" J88-1003 ">
these include the availability of efficient and high precision word segmentation methods for chinese text \[chang et al, 1991; sproat and shih, 1990; wang et al, 1990\], the availability of statistical analysis of chinese corpus \[liu et al, 1975\] and large-scale lec tronic chinese dictionaries with part- of-speech information \[chang et al, 1988; bdc, 1992\], the corpus-based statistical part-of-speech tagger \[church, 1988; <papid> A88-1019 </papid>derose, 1988; <papid> J88-1003 </papid>beale, 1988\], <papid> P88-1026 </papid>as well as phrasal and clausal analyzers \[church 1988; <papid> A88-1019 </papid>ejerhed 1990\]</citsent>
<aftsection>
<nextsent>as being pointed out in \[salton, 1988\], <papid> P88-1025 </papid>back-of-book indexes may consist of more than one word that are derived from noun phrase.</nextsent>
<nextsent>given the text of book, an indexing system, must perform some kind of phrasal and statistical analysis in order to produce list of candidate indexes and their occurrence statistics in order to generate indexes as shown in figure 1 which is an excerpt from the reconstruction of indexes of book on transformational grammar for mandarin chinese \[tang, 1977\].</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A490">
<title id=" A92-1020.xml">a corpus based statistical approach to automatic book indexing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>until recently, this problem has been considered difficult to solve without elaborated syntactical and semantic analyses \[chen, 1988\].
</prevsent>
<prevsent>recent research advances may lead to the development of viable book indexing methods for chinese books.
</prevsent>
</prevsection>
<citsent citstr=" P88-1026 ">
these include the availability of efficient and high precision word segmentation methods for chinese text \[chang et al, 1991; sproat and shih, 1990; wang et al, 1990\], the availability of statistical analysis of chinese corpus \[liu et al, 1975\] and large-scale lec tronic chinese dictionaries with part- of-speech information \[chang et al, 1988; bdc, 1992\], the corpus-based statistical part-of-speech tagger \[church, 1988; <papid> A88-1019 </papid>derose, 1988; <papid> J88-1003 </papid>beale, 1988\], <papid> P88-1026 </papid>as well as phrasal and clausal analyzers \[church 1988; <papid> A88-1019 </papid>ejerhed 1990\]</citsent>
<aftsection>
<nextsent>as being pointed out in \[salton, 1988\], <papid> P88-1025 </papid>back-of-book indexes may consist of more than one word that are derived from noun phrase.</nextsent>
<nextsent>given the text of book, an indexing system, must perform some kind of phrasal and statistical analysis in order to produce list of candidate indexes and their occurrence statistics in order to generate indexes as shown in figure 1 which is an excerpt from the reconstruction of indexes of book on transformational grammar for mandarin chinese \[tang, 1977\].</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A505">
<title id=" C00-2092.xml">data oriented translation </title>
<section> the data-oriented 35ranslation model.  </section>
<citcontext>
<prevsection>
<prevsent>typically, translation can be generated by large number of different deriwltions, each of which has its own probability.
</prevsent>
<prevsent>therefore, the prob-ability of translation ws ~ wt is the sum of the probabilities of its derivations: p(w~, w, ) : ~_p(d(ws,w,}) (3) 637 / \ / \ ~ ~ - ~ np vp np \ vp ) np \ pp likes plait np 71 s np vp np np np \] \] = anne np  atmeanne \] likes vp pp plaft np i anne figure 3: the composition operation the justification of this last equation is quite triv-ial.
</prevsent>
</prevsection>
<citsent citstr=" J90-2002 ">
as in any statistical mt system, we wish to choose the target sentence w~ so as to maximize p(wtlw,) (brown et al, 1990, <papid> J90-2002 </papid>p. 79).</citsent>
<aftsection>
<nextsent>if we take the sum over all possible derivations that wele formed from ws and derive wt, we can rewrite this as equa-tion 4, as seen below.
</nextsent>
<nextsent>since both ws and wt are contained in dlw,,w, ), we can remove them both and arrive at equation 5, which--as we maximize over wt--is equivalent to equation 3 above.
</nextsent>
<nextsent>maxp(wtlws) = wt = max wr d{ws,wt) 11qax wt d(ws.wt) p(w,, w,,w,)lw.d (4) p(d(w,,w,)) (5)
</nextsent>
<nextsent>when translating using the dot model, we can dis-tinguish between three computational stages: i. parsing: the formation of derivation forest, 2.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A510">
<title id=" A00-1023.xml">a question answering system supported by information extraction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>more precisely, we propose hierarchical, 3-level architecture for developing kernel ie system which is domain-independent throughout.
</prevsent>
<prevsent>the core of this system is state-of-the-art ne tagger \[srihari 1998\], named tex tract 1.0.
</prevsent>
</prevsection>
<citsent citstr=" M98-1015 ">
the tex tract ne tagger has achieved speed and accuracy comparable tothat of the few deployed ne systems, such as netowl \[krupka &amp; hausman 1998\] <papid> M98-1015 </papid>and nymble \[bikel et al 1997\].<papid> A97-1029 </papid></citsent>
<aftsection>
<nextsent>it is to be noted that in our definition of ne, we significantly expanded the type of information to be extracted.
</nextsent>
<nextsent>in addition to all the muc defined ne types (person, organization, location, time, date, money and percent), the following types/sub-types of information are also identified by the textractne module: ? duration, frequency, age ? number, fraction, decimal, ordinal, math equation ? weight, length, temperature, angle, area, capacity, speed, rate ? product, software ? address, email, phone, fax, telex, www ? name (default proper name) sub-type information like company, government agency, school (belonging to the type organization) and military person, religious person (belonging to person) are also identified.
</nextsent>
<nextsent>these new sub-types provide better foundation for defining multiple relationships between the identified entities and for supporting question answering functionality.
</nextsent>
<nextsent>for example, the key to question processor is to identify the asking point (who, what, when, where, etc.).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A511">
<title id=" A00-1023.xml">a question answering system supported by information extraction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>more precisely, we propose hierarchical, 3-level architecture for developing kernel ie system which is domain-independent throughout.
</prevsent>
<prevsent>the core of this system is state-of-the-art ne tagger \[srihari 1998\], named tex tract 1.0.
</prevsent>
</prevsection>
<citsent citstr=" A97-1029 ">
the tex tract ne tagger has achieved speed and accuracy comparable tothat of the few deployed ne systems, such as netowl \[krupka &amp; hausman 1998\] <papid> M98-1015 </papid>and nymble \[bikel et al 1997\].<papid> A97-1029 </papid></citsent>
<aftsection>
<nextsent>it is to be noted that in our definition of ne, we significantly expanded the type of information to be extracted.
</nextsent>
<nextsent>in addition to all the muc defined ne types (person, organization, location, time, date, money and percent), the following types/sub-types of information are also identified by the textractne module: ? duration, frequency, age ? number, fraction, decimal, ordinal, math equation ? weight, length, temperature, angle, area, capacity, speed, rate ? product, software ? address, email, phone, fax, telex, www ? name (default proper name) sub-type information like company, government agency, school (belonging to the type organization) and military person, religious person (belonging to person) are also identified.
</nextsent>
<nextsent>these new sub-types provide better foundation for defining multiple relationships between the identified entities and for supporting question answering functionality.
</nextsent>
<nextsent>for example, the key to question processor is to identify the asking point (who, what, when, where, etc.).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A512">
<title id=" C00-1059.xml">corpus dependent association thesauri for information retrieval </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the goal of our present research is to es-tablish method for autolnatically generating thesaurus from text corpus of domain and demonstrate its application to information re-trieval.
</prevsent>
<prevsent>thesauri are classified into taxonomy-type thesauri and association thesauri.
</prevsent>
</prevsection>
<citsent citstr=" C92-2082 ">
there has been various research on the extraction of taxonomic information | io111 corpus, including extrac-tion of hyponyms by using linguistic patterns (hearst 1992) <papid> C92-2082 </papid>and extraction of synonyms based on the similarity of sets of co-occurring words (ruge 1991; grefenstette 1992).</citsent>
<aftsection>
<nextsent>however, the perfor-mance of these methods is limited, and they should be considered as aids to augment hand- made thesauri.
</nextsent>
<nextsent>in contrast, an association the-saurus, that is collection of pairs of semanti-cally associated terms, can be possibly generated from corpus entirely automatically.
</nextsent>
<nextsent>word association orms based on co-occurrence infor-mation have been proposed by (church and hanks 1990).<papid> J90-1003 </papid></nextsent>
<nextsent>here we focus on the automatic generation of an association thesanrus.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A513">
<title id=" C00-1059.xml">corpus dependent association thesauri for information retrieval </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, the perfor-mance of these methods is limited, and they should be considered as aids to augment hand- made thesauri.
</prevsent>
<prevsent>in contrast, an association the-saurus, that is collection of pairs of semanti-cally associated terms, can be possibly generated from corpus entirely automatically.
</prevsent>
</prevsection>
<citsent citstr=" J90-1003 ">
word association orms based on co-occurrence infor-mation have been proposed by (church and hanks 1990).<papid> J90-1003 </papid></citsent>
<aftsection>
<nextsent>here we focus on the automatic generation of an association thesanrus.
</nextsent>
<nextsent>association thesauri are as useful as taxon?
</nextsent>
<nextsent>omy-type thesauri in information retrieval.
</nextsent>
<nextsent>the improvement of retrieval effectiveness by using an association thesaurus has been reported by number of papers (jing and croft 1994; schutze and pedersen 1994).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A517">
<title id=" A00-1006.xml">translation using information on dialogue participants </title>
<section> i t roduct ion   </section>
<citcontext>
<prevsection>
<prevsent>if we want to make conversation proceed smoothly using these translation sys-tems, it is important use not only linguis-tic information, which comes from the source language, but also extra-linguistic nformation, which does not come from the source language, but, is shared between the participants of the conversation.
</prevsent>
<prevsent>several dialogue translation methods that use extra-linguistic information have been pro-posed.
</prevsent>
</prevsection>
<citsent citstr=" W97-0403 ">
horiguchi outlined how  spoken lan-guage pragmatic information  can be trans-lated (horiguchi, 1997).<papid> W97-0403 </papid></citsent>
<aftsection>
<nextsent>however, she did not apply this idea to dialogue translation system.
</nextsent>
<nextsent>luperfoy et al proposed software arc hitec *current affiliation is atr spoken language trans-lation research laboratories current mail addresses are { setsuo.yarnada, eiichiro.sumita, hideki.kashioka} @slt.
</nextsent>
<nextsent>atr.
</nextsent>
<nextsent>co.jp ture that uses  % pragmatic adaptation  (lu- perfoy and others, 1998), and mima et al pro-posed method that uses  situational informa-tion  (mima and others, 1997).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A518">
<title id=" A00-1006.xml">translation using information on dialogue participants </title>
<section> i t roduct ion   </section>
<citcontext>
<prevsection>
<prevsent>the semantic dis-tance is defined by thesaurus.
</prevsent>
<prevsent>(source pattern) ==~ ((target pattern 1) ((source xample 1) (source xample 2) ?  - ) (target pattern 2) o* ) figure 1: transfer ule format transfer ule consists of source pattern, target pattern, and source example.
</prevsent>
</prevsection>
<citsent citstr=" C96-1070 ">
the source pattern consists of variables and con-stituent boundaries (furuse and iida, 1996).<papid> C96-1070 </papid></citsent>
<aftsection>
<nextsent>a constituent boundary is either functional word or the part-of-speech of left constituent last word and the part-of-speech of right con-stituent first word.
</nextsent>
<nextsent>in example (1), the con-stituent boundary iv-cn) is inserted between  accept  and  payment,  because  accept  is verb and  payment  is common noun.
</nextsent>
<nextsent>the target pattern consists of variables that cor-respond to variables in the source pattern and words of the target language.
</nextsent>
<nextsent>the source exam-ple consists of words that come from utterances referred to when person creates transfer ules (we call such utterances closed utterances).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A519">
<title id=" A00-1032.xml">language independent morphological analysis </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>emes): segments with period at the end (e.g,  calif.  and  etc. ) suffer from segmentation ambigu-ity.
</prevsent>
<prevsent>the period can denote an abbreviation, the end of sentence, or both.
</prevsent>
</prevsection>
<citsent citstr=" J97-2002 ">
the problem of sen-tence boundary ambiguity is not easy to solve (palmer and hearst, 1997).<papid> J97-2002 </papid></citsent>
<aftsection>
<nextsent>a segment with an apostrophe also has segmentation ambiguity.
</nextsent>
<nextsent>for example,  mcdonald  is ambiguous since this string can be segmented into either  mc-donald / proper noun  +    / possessive nd- ing  or  mcdonald / proper noun (company name) .
</nextsent>
<nextsent>in addition,  boys    in sentence  ... the boys  toys ...  is ambiguous.
</nextsent>
<nextsent>the string can be segmented into either  boys  / plural posses-sive  or  boys /p lura noun  ?     / punctu-ation (the end of quotation)  (manning and schiitze, 1999).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A520">
<title id=" A00-1032.xml">language independent morphological analysis </title>
<section> round-up(several segments into one lexeme):.  </section>
<citcontext>
<prevsection>
<prevsent>if lexeme consisting of sequence of segments such as proper noun (e.g.,  new york ) or phrasal verb (e.g.,  look at  and  get up ) exists in the dictionary, it should be lexeme.
</prevsent>
<prevsent>to handle such lexemes, we need to store multi- segment lexemes in the dictionary.
</prevsent>
</prevsection>
<citsent citstr=" C92-4173 ">
webster and kit handle idioms and fixed expressions in this way(webster and kit, 1992).<papid> C92-4173 </papid></citsent>
<aftsection>
<nextsent>in penn tree- bank(santorini, 1990), proper noun like  new york  is defined as two individual proper nouns  new / nnp  ?  york / nnp,  disregarding round-up of several:segments into lexeme.
</nextsent>
<nextsent>the definition of lexemes in dictionary depends on the requirement of application.
</nextsent>
<nextsent>therefore, sim-ple pattern matcher is not enough to deal with lan-guage independent tokenization.
</nextsent>
<nextsent>non-segmented languages do not have delimiter between lexemes (figure 2).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A522">
<title id=" A00-1032.xml">language independent morphological analysis </title>
<section> round-up(several segments into one lexeme):.  </section>
<citcontext>
<prevsection>
<prevsent>therefore, treatment of further segmentation and rounding up has been well considered.
</prevsent>
<prevsent>in non-segmented language, the analyzer considers all prefixes from each position in the sentence, checks whether each prefix matches the lexeme in the dictionary, stores these lexemes in graph structure, and finds the most plausible sequence of lexemes in the graph structure.
</prevsent>
</prevsection>
<citsent citstr=" C94-1032 ">
to find the sequence, nagata proposed probabilistic lan-guage model for non-segmented languages(nagata, 1994)(<papid> C94-1032 </papid>nagata, 1999).<papid> P99-1036 </papid></citsent>
<aftsection>
<nextsent>the crucial difference between segmented and 233 non-segmented languages in the process of morpho-logical analysis appears in the way of the dictionary look-up.
</nextsent>
<nextsent>the standard technique for looking up lex- emes in japanese dictionaries to use trie struc- ture(fredkin, 1960)(knuth, 1998).
</nextsent>
<nextsent>a trie structured dictionary gives all possible lexemes that start at given position in sentence ffectively(morimoto and aoe, 1993).
</nextsent>
<nextsent>we call this method of word looking-up as  common prefix search  (hereafter cps).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A523">
<title id=" A00-1032.xml">language independent morphological analysis </title>
<section> round-up(several segments into one lexeme):.  </section>
<citcontext>
<prevsection>
<prevsent>therefore, treatment of further segmentation and rounding up has been well considered.
</prevsent>
<prevsent>in non-segmented language, the analyzer considers all prefixes from each position in the sentence, checks whether each prefix matches the lexeme in the dictionary, stores these lexemes in graph structure, and finds the most plausible sequence of lexemes in the graph structure.
</prevsent>
</prevsection>
<citsent citstr=" P99-1036 ">
to find the sequence, nagata proposed probabilistic lan-guage model for non-segmented languages(nagata, 1994)(<papid> C94-1032 </papid>nagata, 1999).<papid> P99-1036 </papid></citsent>
<aftsection>
<nextsent>the crucial difference between segmented and 233 non-segmented languages in the process of morpho-logical analysis appears in the way of the dictionary look-up.
</nextsent>
<nextsent>the standard technique for looking up lex- emes in japanese dictionaries to use trie struc- ture(fredkin, 1960)(knuth, 1998).
</nextsent>
<nextsent>a trie structured dictionary gives all possible lexemes that start at given position in sentence ffectively(morimoto and aoe, 1993).
</nextsent>
<nextsent>we call this method of word looking-up as  common prefix search  (hereafter cps).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A527">
<title id=" A00-1032.xml">language independent morphological analysis </title>
<section> a naive  approach   </section>
<citcontext>
<prevsection>
<prevsent>because of no segmentation ambiguity,  lxs  performs better than  nsp  and  nor.
</prevsent>
<prevsent>the fol-lowing are typical example of segmentation errors.
</prevsent>
</prevsection>
<citsent citstr=" J97-4004 ">
the errors originate from conjunctive ambiguity and disjunctive ambiguity(guo, 1997).<papid> J97-4004 </papid></citsent>
<aftsection>
<nextsent>conjunctive ambiguity the analyzer ecognized  away, .... ahead,  anymore,  and  ~orkforce  as  way,   head,   any more,  and  ~ork force,  respectively.
</nextsent>
<nextsent>in the results of  nsp,  the number of this type of error is 11,267.
</nextsent>
<nextsent>disjunctive ambiguity the analyzer recognized  tour,   ton,  and  alaskan or  as  at our,   at on,  and  alaska nor,  respectively.
</nextsent>
<nextsent>in the results of  nsp,  the number of this type of er-ror is 233.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A530">
<title id=" C00-1060.xml">a hybrid japanese parser with handcrafted grammar and statistics </title>
<section> introduction.  </section>
<citcontext>
<prevsection>


</prevsection>
<citsent citstr=" C92-2066 ">
there have been many attempts to combine hand-crafted high-level gramnmrs, such as fb-ufag, hpsg and lfg, and statistical disambiguation techniques to ol)tain precise linguistic struc, tures (schabes, 1992; <papid> C92-2066 </papid>almey, 1996; carroll el, al., 1998).</citsent>
<aftsection>
<nextsent>one evident advantage of this apl)roaeh over lmrely statistical parsing techniques is that grammars can provide precise smnantie representations.
</nextsent>
<nextsent>however, considering that remarkable parsing accuracy in shallow level has been achieved by purely statisti-cal techniques (e.g. ratnal)arkhi (1997)), it may be thought more reasonable to use high-level gramnmrs just tin  1)osti)rocessing which nmps results of shallow syntactical analyses onto dee 1) analyses.
</nextsent>
<nextsent> l. his work was conducted while the first ~mthor was graduate student at univ. of tokyo.
</nextsent>
<nextsent>figure 1: tree with non-head aughter nh and }mad aughter h. in this work we prol)ose that hand-crafl, ed high- level grammars (:all be useful in shallow-level analy-ses and statistical models.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A531">
<title id=" C00-1060.xml">a hybrid japanese parser with handcrafted grammar and statistics </title>
<section> introduction.  </section>
<citcontext>
<prevsection>
<prevsent>applying the strategy to the equation (3), we ob-tain equations (4) and (5): p(it hi) de=f p(i (i = 1, 2) (4) hi) der p(i %, , %=, %,) (i = *, 2, t)(5) when there are only two candidates, equation (4) is used; otherwise, equation (5) is used.
</prevsent>
<prevsent>our statis-tical model is called the ~hplet/quadrut)let modal, which was named after the nmnbcr of constituents in the conditional parts of the equations.
</prevsent>
</prevsection>
<citsent citstr=" P98-2144 ">
we report that our parsing framework achieved high accuracy (88.6%) in dependency analysis of japanese with combination of an underspecified hpsg-based japanese grammar, slung (mitsu- ishi et al, 1998) <papid> P98-2144 </papid>and the maximum entropy method (berger et al, 1996).<papid> J96-1002 </papid></citsent>
<aftsection>
<nextsent>moreover, the resulting parse trees generated by our hybrid parser are legitimate trees in terms of given hand-crafted grammars, and we are expecting that we can enjoy advantages pro-vided by high-level gramnmr formalisms, such as construction of semantic structures.
</nextsent>
<nextsent>in the above explanation, we used the notion of lexical heads for the estimation of probabilities of trees for the sake of simplicity.
</nextsent>
<nextsent>but, in the present implementation, we use bunscts,ls instead of lexical heads, and relation on tree is converted to bunsetsu-dependency as shown in figure 3.
</nextsent>
<nextsent>a bun- sctsu is basic syntactic unit in japanese.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A532">
<title id=" C00-1060.xml">a hybrid japanese parser with handcrafted grammar and statistics </title>
<section> introduction.  </section>
<citcontext>
<prevsection>
<prevsent>applying the strategy to the equation (3), we ob-tain equations (4) and (5): p(it hi) de=f p(i (i = 1, 2) (4) hi) der p(i %, , %=, %,) (i = *, 2, t)(5) when there are only two candidates, equation (4) is used; otherwise, equation (5) is used.
</prevsent>
<prevsent>our statis-tical model is called the ~hplet/quadrut)let modal, which was named after the nmnbcr of constituents in the conditional parts of the equations.
</prevsent>
</prevsection>
<citsent citstr=" J96-1002 ">
we report that our parsing framework achieved high accuracy (88.6%) in dependency analysis of japanese with combination of an underspecified hpsg-based japanese grammar, slung (mitsu- ishi et al, 1998) <papid> P98-2144 </papid>and the maximum entropy method (berger et al, 1996).<papid> J96-1002 </papid></citsent>
<aftsection>
<nextsent>moreover, the resulting parse trees generated by our hybrid parser are legitimate trees in terms of given hand-crafted grammars, and we are expecting that we can enjoy advantages pro-vided by high-level gramnmr formalisms, such as construction of semantic structures.
</nextsent>
<nextsent>in the above explanation, we used the notion of lexical heads for the estimation of probabilities of trees for the sake of simplicity.
</nextsent>
<nextsent>but, in the present implementation, we use bunscts,ls instead of lexical heads, and relation on tree is converted to bunsetsu-dependency as shown in figure 3.
</nextsent>
<nextsent>a bun- sctsu is basic syntactic unit in japanese.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A533">
<title id=" C00-1060.xml">a hybrid japanese parser with handcrafted grammar and statistics </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>all models intro-duced below are based on the likelihood value of the dependency between two bunsetsus.
</prevsent>
<prevsent>but they differ from each other in the attributes or outputs which are considered when likelihood value is calculated.
</prevsent>
</prevsection>
<citsent citstr=" P98-1083 ">
there are some models which calculate the likeli-hood values of dependency between bunsetsu and as in (6), such as decision tree model (haruno et al., 1998), <papid> P98-1083 </papid>maximum entropy model (uchimoto et al., 1999), <papid> E99-1026 </papid>model based on distance and lexical in-formation (pujio and matsumoto, 1998).</citsent>
<aftsection>
<nextsent>attributes (i)i and ~i,j consist of part-of-speech (pos), lexi-cal item, presence of comma, and so on.
</nextsent>
<nextsent>and ai,j 412 is the number of intervening bnnscts us between and j. p(i -~ j) d,j ~ crl ,i,i, %, a~,j) ((0 however, these lnodels nil to reftect contextual information because attributes of the surrounding bunsets,tts are not considered.
</nextsent>
<nextsent>uchimoto et al (2000) proposed model us-ing posterior context;.
</nextsent>
<nextsent>the model utilizes not only attributes about bunscts~s i, but also attributes about all bunsets~  (including j) wlfich tbllow bun-setsu i. that is, instead of learning two output val-ues  t(true)  or  :f(false)  for the del)endency be-tween two bunsets~zs, three output values are used *br leanfing: the b~m.setsu is  bynd (dependent on bunsctsu beyond j)  ,  dpnd (del)endent on the b~tsets~t 3)  or  btwn (dependent on b unscts~t be-tween and j) .
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A534">
<title id=" C00-1060.xml">a hybrid japanese parser with handcrafted grammar and statistics </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>all models intro-duced below are based on the likelihood value of the dependency between two bunsetsus.
</prevsent>
<prevsent>but they differ from each other in the attributes or outputs which are considered when likelihood value is calculated.
</prevsent>
</prevsection>
<citsent citstr=" E99-1026 ">
there are some models which calculate the likeli-hood values of dependency between bunsetsu and as in (6), such as decision tree model (haruno et al., 1998), <papid> P98-1083 </papid>maximum entropy model (uchimoto et al., 1999), <papid> E99-1026 </papid>model based on distance and lexical in-formation (pujio and matsumoto, 1998).</citsent>
<aftsection>
<nextsent>attributes (i)i and ~i,j consist of part-of-speech (pos), lexi-cal item, presence of comma, and so on.
</nextsent>
<nextsent>and ai,j 412 is the number of intervening bnnscts us between and j. p(i -~ j) d,j ~ crl ,i,i, %, a~,j) ((0 however, these lnodels nil to reftect contextual information because attributes of the surrounding bunsets,tts are not considered.
</nextsent>
<nextsent>uchimoto et al (2000) proposed model us-ing posterior context;.
</nextsent>
<nextsent>the model utilizes not only attributes about bunscts~s i, but also attributes about all bunsets~  (including j) wlfich tbllow bun-setsu i. that is, instead of learning two output val-ues  t(true)  or  :f(false)  for the del)endency be-tween two bunsets~zs, three output values are used *br leanfing: the b~m.setsu is  bynd (dependent on bunsctsu beyond j)  ,  dpnd (del)endent on the b~tsets~t 3)  or  btwn (dependent on b unscts~t be-tween and j) .
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A535">
<title id=" C00-1060.xml">a hybrid japanese parser with handcrafted grammar and statistics </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>how- ever, the model has to assume, the in dependency of all the random variables, which may cause some er- ro rs . p(i --, j)  h ~ (by.d ? ~, %, &amp;,k) k xp(dpnd (1)i, il)5, aid) hl (btw,, \[ (i,i, qk, k)(7) j the difference between our model and these pre-vious models are discussed in section 3.
</prevsent>
<prevsent>2.2 stat is ica approaches i th grmnnmr.
</prevsent>
</prevsection>
<citsent citstr=" J93-1002 ">
there have been nlally l)rol)osals tbr statistical rameworks particularly designed tbr 1)arsers with hand-crafted grmnmars (schal)es, 1992; briscoe and carroll, 1993; <papid> J93-1002 </papid>abney, 1996; inui et al, 1!)97).</citsent>
<aftsection>
<nextsent>the main issue in tiffs type of research is how to assign likelihoods to single linguistic structure generated by gramlnar.
</nextsent>
<nextsent>some of tlmm (briscoe and carroll, 1!)93; hmi et al, 1997) treat information on contexts, but the contextual intbrmation is de.rived only fl om structure to wlfich the parser is trying to assign likelihood value.
</nextsent>
<nextsent>then, tim major difference be.- tween their method and ours is that we consider the attributes of alternative linguistic structures gener-ated by the grammar in order to deternfine the like-lihood for linguistic structures.
</nextsent>
<nextsent>2.3 slung : apanese grammar.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A537">
<title id=" C00-1060.xml">a hybrid japanese parser with handcrafted grammar and statistics </title>
<section> the  hybrid  parsing  method.  </section>
<citcontext>
<prevsection>
<prevsent>we therefore assume that the.
</prevsent>
<prevsent>number of modification candidates ix always three or less.
</prevsent>
</prevsection>
<citsent citstr=" C00-2110 ">
this idea is sinfilar to that of sekine (2000) <papid> C00-2110 </papid>study, which restricts the candidates to five, i)ut in his case, without granmmr.</citsent>
<aftsection>
<nextsent>3.2 the tr ip le /quadrup le mode l. the  diplel,/quadruplet model calculates the like-lihood of the dependency between bunsetsu and bunsctsu cn; p( --, cn) with the formulas (8) and (9), where c,~ denotes the nth candidate among b,m- sctsu s candidates; (i,i denotes some attributes of i; and ~i~?,~ denotes attributes of c,~ (including at-tributes between and cn).
</nextsent>
<nextsent>p(i -~ c,d dj p(n ?,~, %.,, %~) (.n = 1,2)(8) p(~ -~ c,~) ,,~r p(,~ ? i, %,, ,i,~, ,i%) (,n = 1, 2 , / ) (9 ) 2this heuristics is japanese version of left-association rule: see (mitsuishi et m., 1998) for detail.
</nextsent>
<nextsent>413 as (8) and (9) suggest, the model considers at-tributes of the modifier bunsetsu and attributes of all modification candidates imultaneously in the condi-tional parts of the probabilities.
</nextsent>
<nextsent>moreover, what is calculated is not tile probability of  whether the de-pendency is correct (t, see formula(6)) , but the probability of  which of tile given candidates cho-sen as tile nlodifiee (n =1, 2, or 1) .
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A541">
<title id=" A92-1034.xml">morphe a practical compiler for reversible morphology rules </title>
<section> processing.  </section>
<citcontext>
<prevsection>
<prevsent>when match is found, the inflection procedure for that clause is applied to the word root and the result is returned.
</prevsent>
<prevsent>during parsing, processing begins with an inflected form entering the tree at each leaf node where the inflection rules are applied  in reverse  and the non-passing results discarded.
</prevsent>
</prevsection>
<citsent citstr=" A88-1031 ">
applying rule  in reverse  means that the word is matched 3this class matching is equivalent alphabet subsets  inkimmo \[karttunen etal., 1983\],  restricted variables  in nabu \[slocum, 1988\], <papid> A88-1031 </papid>and  sla ing vats  in dimorph \[gibson, 1991\].</citsent>
<aftsection>
<nextsent>4these mechanisms are similar to the binding and retrieval mech-anisms used in unix utilities such as  sed .
</nextsent>
<nextsent>5string-to-string mapping is roughly equivalent to the  pairing- up \[of\] variables  innabu.
</nextsent>
<nextsent>233 ( leaf-rule v+pres -par (((:or  .... ) $) (+s  ing ) ) ((c (% gc) s) (+s %1  ing )) ( (c   s) (rs  .... ing ) ) ( (c  ie  $) (rs   le .... ing ) ) ( : ot herwi se (+s  ing ) ) ) ; verbs ike perplex &amp; carry ; verbs ike cut ; verbs lke make ; verbs lke die ; verbs lke dent figure 1: inflection rule for english present participle against he inflected forms and the operations perform de- inflection, rather than vice versa.
</nextsent>
<nextsent>after all clauses in all leaves have been tried, and presumably most results have been discarded, each remaining parse follows the network upwards, collecting the features of each node it traverses until set of full feature structures arrives at the root node.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A542">
<title id=" A94-1013.xml">adaptive sentence boundary disambiguation </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>


</prevsection>
<citsent citstr=" A88-1019 ">
labeling of sentence boundaries is necessary prerequisite for many natural language process-ing (nlp) tasks, including part-of-speech tagging (church, 1988), (<papid> A88-1019 </papid>cutting et al, 1991), and sen-tence alignment (gale and church, 1993), (<papid> J93-1004 </papid>kay and rsscheisen, 1993).<papid> J93-1006 </papid></citsent>
<aftsection>
<nextsent>end-of-sentence punctuation marks are ambiguous; for example, period can de-note an abbreviation, the end of sentence, or both, as shown in the examples below: (1) the group included dr. j.m. freeman and t. boone pickens jr.
</nextsent>
<nextsent>(2)  this issue crosses party lines and crosses philosophical lines!  said rep. john rowland (r., conn.).
</nextsent>
<nextsent>riley (1989) <papid> H89-2048 </papid>determined that in the tagged brown corpus (francis and kucera, 1982) about 90% of pe- 78 riods occur at the end of sentences, 10% at the end of abbreviations, and about 0.5% as both abbrevi-ations and sentence delimiters.</nextsent>
<nextsent>note from example (2) that exclamation points and question marks are also ambiguous, ince they too can appear at loca-tions other than sentence boundaries.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A543">
<title id=" A94-1013.xml">adaptive sentence boundary disambiguation </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>


</prevsection>
<citsent citstr=" J93-1004 ">
labeling of sentence boundaries is necessary prerequisite for many natural language process-ing (nlp) tasks, including part-of-speech tagging (church, 1988), (<papid> A88-1019 </papid>cutting et al, 1991), and sen-tence alignment (gale and church, 1993), (<papid> J93-1004 </papid>kay and rsscheisen, 1993).<papid> J93-1006 </papid></citsent>
<aftsection>
<nextsent>end-of-sentence punctuation marks are ambiguous; for example, period can de-note an abbreviation, the end of sentence, or both, as shown in the examples below: (1) the group included dr. j.m. freeman and t. boone pickens jr.
</nextsent>
<nextsent>(2)  this issue crosses party lines and crosses philosophical lines!  said rep. john rowland (r., conn.).
</nextsent>
<nextsent>riley (1989) <papid> H89-2048 </papid>determined that in the tagged brown corpus (francis and kucera, 1982) about 90% of pe- 78 riods occur at the end of sentences, 10% at the end of abbreviations, and about 0.5% as both abbrevi-ations and sentence delimiters.</nextsent>
<nextsent>note from example (2) that exclamation points and question marks are also ambiguous, ince they too can appear at loca-tions other than sentence boundaries.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A544">
<title id=" A94-1013.xml">adaptive sentence boundary disambiguation </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>


</prevsection>
<citsent citstr=" J93-1006 ">
labeling of sentence boundaries is necessary prerequisite for many natural language process-ing (nlp) tasks, including part-of-speech tagging (church, 1988), (<papid> A88-1019 </papid>cutting et al, 1991), and sen-tence alignment (gale and church, 1993), (<papid> J93-1004 </papid>kay and rsscheisen, 1993).<papid> J93-1006 </papid></citsent>
<aftsection>
<nextsent>end-of-sentence punctuation marks are ambiguous; for example, period can de-note an abbreviation, the end of sentence, or both, as shown in the examples below: (1) the group included dr. j.m. freeman and t. boone pickens jr.
</nextsent>
<nextsent>(2)  this issue crosses party lines and crosses philosophical lines!  said rep. john rowland (r., conn.).
</nextsent>
<nextsent>riley (1989) <papid> H89-2048 </papid>determined that in the tagged brown corpus (francis and kucera, 1982) about 90% of pe- 78 riods occur at the end of sentences, 10% at the end of abbreviations, and about 0.5% as both abbrevi-ations and sentence delimiters.</nextsent>
<nextsent>note from example (2) that exclamation points and question marks are also ambiguous, ince they too can appear at loca-tions other than sentence boundaries.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A545">
<title id=" A94-1013.xml">adaptive sentence boundary disambiguation </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>end-of-sentence punctuation marks are ambiguous; for example, period can de-note an abbreviation, the end of sentence, or both, as shown in the examples below: (1) the group included dr. j.m. freeman and t. boone pickens jr.
</prevsent>
<prevsent>(2)  this issue crosses party lines and crosses philosophical lines!  said rep. john rowland (r., conn.).
</prevsent>
</prevsection>
<citsent citstr=" H89-2048 ">
riley (1989) <papid> H89-2048 </papid>determined that in the tagged brown corpus (francis and kucera, 1982) about 90% of pe- 78 riods occur at the end of sentences, 10% at the end of abbreviations, and about 0.5% as both abbrevi-ations and sentence delimiters.</citsent>
<aftsection>
<nextsent>note from example (2) that exclamation points and question marks are also ambiguous, ince they too can appear at loca-tions other than sentence boundaries.
</nextsent>
<nextsent>most robust nlp systems, e.g., cutting et al (1991), find sentence delimiters by tokenizing the text stream and applying regular expression gram-mar with some amount of look-ahead, an abbrevia-tion list, and perhaps list of exception rules.
</nextsent>
<nextsent>these approaches are usually hand-tailored to the particu-lar text and relyon brittle cues such as capitalization and the number of spaces following asentence delim- iter.
</nextsent>
<nextsent>typically these approaches use only the tokens immediately preceding and following the punctua-tion mark to be disambiguated.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A556">
<title id=" A00-1038.xml">largescale controlled vocabulary indexing for named entities </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>one application de-ployed at reuters achieved 94% recall and 84% precision.
</prevsent>
<prevsent>other reported tests achieved recall and precision rates of 90% or better.
</prevsent>
</prevsection>
<citsent citstr=" M95-1018 ">
sra nametag (krupka, 1995) <papid> M95-1018 </papid>uses pattem recognition process that combines rule base with lexicons to identify and extract targeted token classes in text, such as company, people and place names.</citsent>
<aftsection>
<nextsent>it achieved 96% recall and 97% precision when tested on wall street journal news articles at muc-6.
</nextsent>
<nextsent>aone et al (1997) <papid> A97-1049 </papid>describes nametag- based application for indexing english and japa-nese language texts.</nextsent>
<nextsent>nametag addresses key weakness of approaches that use predefined topic sets.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A557">
<title id=" A00-1038.xml">largescale controlled vocabulary indexing for named entities </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>sra nametag (krupka, 1995) <papid> M95-1018 </papid>uses pattem recognition process that combines rule base with lexicons to identify and extract targeted token classes in text, such as company, people and place names.</prevsent>
<prevsent>it achieved 96% recall and 97% precision when tested on wall street journal news articles at muc-6.</prevsent>
</prevsection>
<citsent citstr=" A97-1049 ">
aone et al (1997) <papid> A97-1049 </papid>describes nametag- based application for indexing english and japa-nese language texts.</citsent>
<aftsection>
<nextsent>nametag addresses key weakness of approaches that use predefined topic sets.
</nextsent>
<nextsent>predefined topic sets are inherently limited in their coverage to those topics that have been ex-plicitly defined.
</nextsent>
<nextsent>nametag can recognize any num-ber of companies or entities of other domains whose names have structure that the rules can rec-ognize.
</nextsent>
<nextsent>(not all entity domains have structure that pattern recognition processes can exploit, e.g., product names).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A558">
<title id=" A00-1038.xml">largescale controlled vocabulary indexing for named entities </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>however, because most of the place names we targeted lacked useful 280 internal structure, manual intervention was part of creating all 800 definitions for places.
</prevsent>
<prevsent>accuracy rates for people, organization and geographic in-dexing are comparable to those for company in-dexing.
</prevsent>
</prevsection>
<citsent citstr=" P99-1009 ">
knowledge engineering can be bottleneck in building large-scale applications, which is why machine learning-based approaches are often pre-ferred, but there has been little work in quantifying the difference between the two approaches in lin-guistic tasks (brill &amp; ngai, 1999).<papid> P99-1009 </papid></citsent>
<aftsection>
<nextsent>in our case, adopting ama chine learning-based approach was problem not just because we lacked annotated training data, but for many of the topics we were required to target we had little or no available data at all.
</nextsent>
<nextsent>however, because of the regularity we ob-served in company name variants and their use across variety of news sources, we determined that the knowledge ngineering task would be quite repetitive and thus could be automated for most companies.
</nextsent>
<nextsent>our entity indexing system meets all of our busi-ness requirements for accuracy, scale and perform-ance.
</nextsent>
<nextsent>we have also substantially automated the definition building process.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A559">
<title id=" A97-1025.xml">contextual spelling correction using latent semantic analysis </title>
<section> re la ted  work.  </section>
<citcontext>
<prevsection>
<prevsent>the approach taken used letter n-grams to build the semantic space.
</prevsent>
<prevsent>in this work, we use the words di-rectly.
</prevsent>
</prevsection>
<citsent citstr=" P94-1013 ">
yarowsky (1994) <papid> P94-1013 </papid>notes that conceptual spelling correction is part of closely related class of prob-lems which include word sense disambiguation, word choice selection in machine translation, and accent and capitalization restoration.</citsent>
<aftsection>
<nextsent>this class of prob-lems has been attacked by many others.
</nextsent>
<nextsent>a number of feature-based methods have been tried, including bayesian classifiers (gale, church, and yarowsky, 1992; golding, 1995), <papid> W95-0104 </papid>decision lists (yarowsky, 1994), <papid> P94-1013 </papid>and knowledge-based approaches (mcroy, 1992).<papid> J92-1001 </papid></nextsent>
<nextsent>recently, golding and schabes (1996) <papid> P96-1010 </papid>de-scribed system, tri bayes, that combines trigram model of the words  parts of speech with bayesian classifier.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A560">
<title id=" A97-1025.xml">contextual spelling correction using latent semantic analysis </title>
<section> re la ted  work.  </section>
<citcontext>
<prevsection>
<prevsent>yarowsky (1994) <papid> P94-1013 </papid>notes that conceptual spelling correction is part of closely related class of prob-lems which include word sense disambiguation, word choice selection in machine translation, and accent and capitalization restoration.</prevsent>
<prevsent>this class of prob-lems has been attacked by many others.</prevsent>
</prevsection>
<citsent citstr=" W95-0104 ">
a number of feature-based methods have been tried, including bayesian classifiers (gale, church, and yarowsky, 1992; golding, 1995), <papid> W95-0104 </papid>decision lists (yarowsky, 1994), <papid> P94-1013 </papid>and knowledge-based approaches (mcroy, 1992).<papid> J92-1001 </papid></citsent>
<aftsection>
<nextsent>recently, golding and schabes (1996) <papid> P96-1010 </papid>de-scribed system, tri bayes, that combines trigram model of the words  parts of speech with bayesian classifier.</nextsent>
<nextsent>the trigram component of the system is used to make decisions for those confusion sets that 166 terms documents t rx d ~ rxd txd tx figure 1: singular value decomposition (svd) of matrix produces matrices t, and .</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A562">
<title id=" A97-1025.xml">contextual spelling correction using latent semantic analysis </title>
<section> re la ted  work.  </section>
<citcontext>
<prevsection>
<prevsent>yarowsky (1994) <papid> P94-1013 </papid>notes that conceptual spelling correction is part of closely related class of prob-lems which include word sense disambiguation, word choice selection in machine translation, and accent and capitalization restoration.</prevsent>
<prevsent>this class of prob-lems has been attacked by many others.</prevsent>
</prevsection>
<citsent citstr=" J92-1001 ">
a number of feature-based methods have been tried, including bayesian classifiers (gale, church, and yarowsky, 1992; golding, 1995), <papid> W95-0104 </papid>decision lists (yarowsky, 1994), <papid> P94-1013 </papid>and knowledge-based approaches (mcroy, 1992).<papid> J92-1001 </papid></citsent>
<aftsection>
<nextsent>recently, golding and schabes (1996) <papid> P96-1010 </papid>de-scribed system, tri bayes, that combines trigram model of the words  parts of speech with bayesian classifier.</nextsent>
<nextsent>the trigram component of the system is used to make decisions for those confusion sets that 166 terms documents t rx d ~ rxd txd tx figure 1: singular value decomposition (svd) of matrix produces matrices t, and .</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A563">
<title id=" A97-1025.xml">contextual spelling correction using latent semantic analysis </title>
<section> re la ted  work.  </section>
<citcontext>
<prevsection>
<prevsent>this class of prob-lems has been attacked by many others.
</prevsent>
<prevsent>a number of feature-based methods have been tried, including bayesian classifiers (gale, church, and yarowsky, 1992; golding, 1995), <papid> W95-0104 </papid>decision lists (yarowsky, 1994), <papid> P94-1013 </papid>and knowledge-based approaches (mcroy, 1992).<papid> J92-1001 </papid></prevsent>
</prevsection>
<citsent citstr=" P96-1010 ">
recently, golding and schabes (1996) <papid> P96-1010 </papid>de-scribed system, tri bayes, that combines trigram model of the words  parts of speech with bayesian classifier.</citsent>
<aftsection>
<nextsent>the trigram component of the system is used to make decisions for those confusion sets that 166 terms documents t rx d ~ rxd txd tx figure 1: singular value decomposition (svd) of matrix produces matrices t, and .
</nextsent>
<nextsent>contain words with different parts of speech.
</nextsent>
<nextsent>the bayesian component is used to predict the correct word from among same part-of-speech words.
</nextsent>
<nextsent>golding and schabes selected 18 confusion sets from list of commonly confused words plus few that represent ypographical errors.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A576">
<title id=" C00-1044.xml">effects of adjective orientation and gradability on sentence subjectivity </title>
<section> semantic orientation.  </section>
<citcontext>
<prevsection>
<prevsent>while ori-entation applies to many adjectives, there are also those that have no orientation, typically as members of groups of complementary, qualitative terms (lyons, 1977) (e.g., domestic, medical, or red).
</prevsent>
<prevsent>since orientation is inher-ently connected with cwduative judgements, it appears to be promising feature for predicting subjectivity.
</prevsent>
</prevsection>
<citsent citstr=" P97-1023 ">
hatzivassiloglou and mckeown (1997) <papid> P97-1023 </papid>presented method for autonmtically assigning + or - orientation label to adjectives known to have some semantic orien- tation.</citsent>
<aftsection>
<nextsent>their method is based on information extracted fi om conjunctions between adjectives in large corpus because orientation constrains the use of the words in specific contexts (e.g., compare corrupt and brutal with *corrupt but brutal), observed conjunctions of adjectives can be exploited to inl er whether the conjoined words are of the same or different orientation.
</nextsent>
<nextsent>using shallow parser on 21 million word corpus of wall street jour-nal articles, hatzivassiloglou and mckeown developed and trained log-linear statistical model that predicts whether any two adjectives have the same orientation with 82% accuracy.
</nextsent>
<nextsent>the predicted links o1  same or dil ferent orientation are automatically assigned strength value (essentially, confidence stimate) by tile model, and induce graph that can be partitioned with clus-tering algorithm into components so that all words in the same component belong to the same orientation class.
</nextsent>
<nextsent>once the classes have been determined, fl equency infor-mation is used to assign positive or negative labels to each class (there are slightly fewer positive terms, but with significantly higher rate of occurrence than nega-tive terms).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A578">
<title id=" C00-1044.xml">effects of adjective orientation and gradability on sentence subjectivity </title>
<section> gradability.  </section>
<citcontext>
<prevsection>
<prevsent>however, non-gradable words may also occasionally appear in such contexts or forms under exceptional circumstances.
</prevsent>
<prevsent>for example, ve o, dead can be used tk)r emphasis, and re am~ re (as in  her lhce became redder and redder ) can be used to indicate progression of coloring, qb distinguish be-tween truly gradablc adjectives and non-gradable adjec-tives in these exceptional contexts, we have developed trainable log-linear statistical model that lakes into ac-count tile number of times an ad.iective has been observed in form or context indicating gradability relative to the number of limes it has been seen in non-gradable con-texts.
</prevsent>
</prevsection>
<citsent citstr=" A88-1019 ">
we use shallow parser to retrieve from large corpus tagged for part-of-speech with church parts tagger (church, 1988) <papid> A88-1019 </papid>all adjectives and their modifiers.</citsent>
<aftsection>
<nextsent>al-though the most common use of an adverb modifying an adjective is to function as an intensilier or diminisher (quirk et al, 1985, p. 445), adverbs can also add to tile semantic ontent of the adjectival phrase instead of pro-viding grading effect (e.g., immediately available, po-litically vuhmrable), or function as cmphasizers, adding to the force o1  tile base adjective and not lo its degree (e.g., virtually impossible; compare *re o, impossible).
</nextsent>
<nextsent>therefore, we compiled by hand list of 73 adverbs and noun phrases (such as little, exceedingly, somewhat, and veo ) that are fi equently used as grading moditicrs.
</nextsent>
<nextsent>the number of times each adjective appears mod ilied by term form this list becomes first indicator of gradabil-ity.
</nextsent>
<nextsent>to detect inflected forms o1  adjectives (which, in 15  glish, always indicate gradability st, bject to the excel  tions discussed earlier), we have implemented an auto-matic lnorphology analysis component.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A579">
<title id=" C00-1044.xml">effects of adjective orientation and gradability on sentence subjectivity </title>
<section> subjectivity.  </section>
<citcontext>
<prevsection>
<prevsent>4.1 previous work on subjectivity recognition.
</prevsent>
<prevsent>in work by wiebc, bruce, and hara (wiebe ct al., 1999; bruce and wicbe, 2000), corpus of 1,001 sen-tences 3 of the wall street journal treebank corpus 3conlpoutld sentences were manually segmented into their con- juncts, and each conjtmct treated as scparale sentence.
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
302 (marcus et al, 1993) <papid> J93-2004 </papid>was nlanually annotated with sub- jeciivity chlssifications.</citsent>
<aftsection>
<nextsent>specifically, each sentence was assigned subjective or objective classitication, accord-ing to concensus lags derived by slalistical analysis of lhe chisses assigned by three human judges (see (wiebe et al, 1999) <papid> P99-1032 </papid>for further infornmtion).</nextsent>
<nextsent>the total nulnber of subjective sentences in lhe data is 486, and the total number of objeclive sentences 515.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A580">
<title id=" C00-1044.xml">effects of adjective orientation and gradability on sentence subjectivity </title>
<section> subjectivity.  </section>
<citcontext>
<prevsection>
<prevsent>in work by wiebc, bruce, and hara (wiebe ct al., 1999; bruce and wicbe, 2000), corpus of 1,001 sen-tences 3 of the wall street journal treebank corpus 3conlpoutld sentences were manually segmented into their con- juncts, and each conjtmct treated as scparale sentence.
</prevsent>
<prevsent>302 (marcus et al, 1993) <papid> J93-2004 </papid>was nlanually annotated with sub- jeciivity chlssifications.</prevsent>
</prevsection>
<citsent citstr=" P99-1032 ">
specifically, each sentence was assigned subjective or objective classitication, accord-ing to concensus lags derived by slalistical analysis of lhe chisses assigned by three human judges (see (wiebe et al, 1999) <papid> P99-1032 </papid>for further infornmtion).</citsent>
<aftsection>
<nextsent>the total nulnber of subjective sentences in lhe data is 486, and the total number of objeclive sentences 515.
</nextsent>
<nextsent>bruce and wiebe (2000) performed statistical anal-ysis of the assigned classitications, linding lhat ac(iec- tivcs are statistically signilicantly and positively corre-lated with subjective sentences in the corpus on the basis (, . the proba- of the log-likelihood ratio test statistic -,2 bility of sentence being subjective, simply given din!
</nextsent>
<nextsent>there is at least one adjective in lhe sentellee, is 56%, even though there are more objective than subjective sen- lences in the corpus.
</nextsent>
<nextsent>in addition, bruce and wicbe iden- tiffed type of adjective that is indicative of subjective sentences: those quirk et al (1985) term dynamic, which  denote qualities that a, thoughl to be subjecl to con-trol by the possessor  (p. 434).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A582">
<title id=" A94-1014.xml">acquiring knowledge from encyclopedic texts </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>but, if, in the process of learning about the dietary habits of, say beetles, the program is told that they culti-vate fungi, and the program is able to interpret hat sentence, that knowledge will also be integrated in ltm.
</prevsent>
<prevsent>consequently, our approach to understanding expository texts is bottom-up approach in which final knowledge representation structures are built from the logical form of the sentences, without inter-vening scrip tal or frame knowledge about the topic.
</prevsent>
</prevsection>
<citsent citstr=" M92-1001 ">
hence, our system does not start with frame con-taining the main slots to be filled for topic, say  diet,  as in recent muc projects (sundheim, 1992), <papid> M92-1001 </papid>but rather it will build everything relevant diet from the output of the interpretation phase.</citsent>
<aftsection>
<nextsent>then, when we are talking about the topic, it will not make any difference if the sentences refer to what the an-imals eat, or what eats them.
</nextsent>
<nextsent>every aspect dealing with the general idea of ingest can be analyzed and properly integrated into memory.
</nextsent>
<nextsent>our corpora for testing our ideas has been the world book ency-clopedia (world book, 1987), which is one or two levels less complex than the collier encyclopedia, which, in turn, is less complex than the encyclopae-dia britannica.
</nextsent>
<nextsent>in order for the integration component to integrate concept in ltm, successful parse and interpreta-tion needs to be produced for sentence or at least for one of its clauses.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A583">
<title id=" A94-1014.xml">acquiring knowledge from encyclopedic texts </title>
<section> re la ted  work.  </section>
<citcontext>
<prevsection>
<prevsent>yes, vampire bat harm human *frequency* sometimes do monkeys have enemies?
</prevsent>
<prevsent>yes, some monkey has- enemy cheetah hyena jackal leopard lion because  monkeys inhabit at-loc ground  has-enemy chee-tah hyena jackal leopard lion
</prevsent>
</prevsection>
<citsent citstr=" P84-1024 ">
in (sondheimer et al, 1984), <papid> P84-1024 </papid>frame-like structures, kl-one structures in fact, are also used to guide semantic interpretation an application domain.</citsent>
<aftsection>
<nextsent>however, the overall approach to the interpreta-tion task presented here differs from that work.
</nextsent>
<nextsent>in approaching the problem of unrestricted texts, we agree with those researchers (hobbs, 1991; <papid> M91-1030 </papid>grish-man et al, 1991) <papid> M91-1028 </papid>who think that it is possible to build correct parses and interpretations for real- world texts.</nextsent>
<nextsent>in fact, it is hard for us to see how statistical methods (de marcken, 1990; church et al., 1991) could be used for building knowledge-bases with sufficient expressive power to correctly answer questions posed by expert systems or human users.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A584">
<title id=" A94-1014.xml">acquiring knowledge from encyclopedic texts </title>
<section> re la ted  work.  </section>
<citcontext>
<prevsection>
<prevsent>in (sondheimer et al, 1984), <papid> P84-1024 </papid>frame-like structures, kl-one structures in fact, are also used to guide semantic interpretation an application domain.</prevsent>
<prevsent>however, the overall approach to the interpreta-tion task presented here differs from that work.</prevsent>
</prevsection>
<citsent citstr=" M91-1030 ">
in approaching the problem of unrestricted texts, we agree with those researchers (hobbs, 1991; <papid> M91-1030 </papid>grish-man et al, 1991) <papid> M91-1028 </papid>who think that it is possible to build correct parses and interpretations for real- world texts.</citsent>
<aftsection>
<nextsent>in fact, it is hard for us to see how statistical methods (de marcken, 1990; church et al., 1991) could be used for building knowledge-bases with sufficient expressive power to correctly answer questions posed by expert systems or human users.
</nextsent>
<nextsent>we think that the same critique applies to skim-mers (lehnert et al, 1991), <papid> M91-1033 </papid>but for very different reasons.</nextsent>
<nextsent>in order to guarantee the correctness of the knowledge-base built, every element in the sen-tence needs to be interpreted.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A585">
<title id=" A94-1014.xml">acquiring knowledge from encyclopedic texts </title>
<section> re la ted  work.  </section>
<citcontext>
<prevsection>
<prevsent>in (sondheimer et al, 1984), <papid> P84-1024 </papid>frame-like structures, kl-one structures in fact, are also used to guide semantic interpretation an application domain.</prevsent>
<prevsent>however, the overall approach to the interpreta-tion task presented here differs from that work.</prevsent>
</prevsection>
<citsent citstr=" M91-1028 ">
in approaching the problem of unrestricted texts, we agree with those researchers (hobbs, 1991; <papid> M91-1030 </papid>grish-man et al, 1991) <papid> M91-1028 </papid>who think that it is possible to build correct parses and interpretations for real- world texts.</citsent>
<aftsection>
<nextsent>in fact, it is hard for us to see how statistical methods (de marcken, 1990; church et al., 1991) could be used for building knowledge-bases with sufficient expressive power to correctly answer questions posed by expert systems or human users.
</nextsent>
<nextsent>we think that the same critique applies to skim-mers (lehnert et al, 1991), <papid> M91-1033 </papid>but for very different reasons.</nextsent>
<nextsent>in order to guarantee the correctness of the knowledge-base built, every element in the sen-tence needs to be interpreted.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A586">
<title id=" A94-1014.xml">acquiring knowledge from encyclopedic texts </title>
<section> re la ted  work.  </section>
<citcontext>
<prevsection>
<prevsent>in approaching the problem of unrestricted texts, we agree with those researchers (hobbs, 1991; <papid> M91-1030 </papid>grish-man et al, 1991) <papid> M91-1028 </papid>who think that it is possible to build correct parses and interpretations for real- world texts.</prevsent>
<prevsent>in fact, it is hard for us to see how statistical methods (de marcken, 1990; church et al., 1991) could be used for building knowledge-bases with sufficient expressive power to correctly answer questions posed by expert systems or human users.</prevsent>
</prevsection>
<citsent citstr=" M91-1033 ">
we think that the same critique applies to skim-mers (lehnert et al, 1991), <papid> M91-1033 </papid>but for very different reasons.</citsent>
<aftsection>
<nextsent>in order to guarantee the correctness of the knowledge-base built, every element in the sen-tence needs to be interpreted.
</nextsent>
<nextsent>for instance, if the adverb  mostly  is not interpreted in the sentence these owls eat mostly rodents, the integrity of the knowledge-base built is not going to suffer greatly.
</nextsent>
<nextsent>but, if we are talking about the adverb  rarely  in the sentence these owls rarely eat rodents, the situ-ation becomes much more serious, as we found out.
</nextsent>
<nextsent>this work has advanced new approach to se-mantic interpretation that occupies middle ground between those approaches that rely heavily on the parser for building structures and attaching pps, subordinate clauses, etc.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A588">
<title id=" A94-1014.xml">acquiring knowledge from encyclopedic texts </title>
<section> re la ted  work.  </section>
<citcontext>
<prevsection>
<prevsent>yet, if the parser does not build parse, albeit shallow one, the interpreter will not know what to do.
</prevsent>
<prevsent>moreover, the interpreter does not question the parser when it says this constituent is an obj, or subj, or time-np, etc. this is situation that we are not happy about, because the parser identifies some constituents incorrectly, especially the time- np.
</prevsent>
</prevsection>
<citsent citstr=" M91-1022 ">
we are studying mechanisms under which the interpreter will override the parser and will get it out of trouble in processing very complex sentences (krupka et al, 1991; <papid> M91-1022 </papid>jacobs et al, 1991).<papid> H91-1066 </papid></citsent>
<aftsection>
<nextsent>we have presented method for the acquisition of knowledge from encyclopedic texts.
</nextsent>
<nextsent>the method de-pends on understanding what is being read, which in turn depends on: (1) providing successful parse and interpretation for sentence, (2) building final knowledge representation structures from the log-ical form of the sentence, which involves creating new concepts and relations as the system is reading, and (3) integrating in ltm those concepts and rela-tions that the recognizer algorithm fails to recognize, which in many cases involves the reorganization of concepts in ltm.
</nextsent>
<nextsent>the results that are reported in this paper are very encouraging, because high percentage of the fail-ures are due to some incomplete implementations of some aspects of the system.
</nextsent>
<nextsent>for instance, in dealing with anaphora we have incorporated in our system some of the ideas reported in (grosz, 1983; hobbs, 1978), but our work is clearly insufficient in that regard.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A589">
<title id=" A94-1014.xml">acquiring knowledge from encyclopedic texts </title>
<section> re la ted  work.  </section>
<citcontext>
<prevsection>
<prevsent>yet, if the parser does not build parse, albeit shallow one, the interpreter will not know what to do.
</prevsent>
<prevsent>moreover, the interpreter does not question the parser when it says this constituent is an obj, or subj, or time-np, etc. this is situation that we are not happy about, because the parser identifies some constituents incorrectly, especially the time- np.
</prevsent>
</prevsection>
<citsent citstr=" H91-1066 ">
we are studying mechanisms under which the interpreter will override the parser and will get it out of trouble in processing very complex sentences (krupka et al, 1991; <papid> M91-1022 </papid>jacobs et al, 1991).<papid> H91-1066 </papid></citsent>
<aftsection>
<nextsent>we have presented method for the acquisition of knowledge from encyclopedic texts.
</nextsent>
<nextsent>the method de-pends on understanding what is being read, which in turn depends on: (1) providing successful parse and interpretation for sentence, (2) building final knowledge representation structures from the log-ical form of the sentence, which involves creating new concepts and relations as the system is reading, and (3) integrating in ltm those concepts and rela-tions that the recognizer algorithm fails to recognize, which in many cases involves the reorganization of concepts in ltm.
</nextsent>
<nextsent>the results that are reported in this paper are very encouraging, because high percentage of the fail-ures are due to some incomplete implementations of some aspects of the system.
</nextsent>
<nextsent>for instance, in dealing with anaphora we have incorporated in our system some of the ideas reported in (grosz, 1983; hobbs, 1978), but our work is clearly insufficient in that regard.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A590">
<title id=" A00-2005.xml">bagging and boosting a treebank parser </title>
<section> abstract </section>
<citcontext>
<prevsection>
<prevsent>the best resulting system provides roughly as large of gain in f-measure as doubling the corpus size.
</prevsent>
<prevsent>error analysis of the result of the boosting technique re-veals some inconsistent annotations in the penn treebank, suggesting semi-automatic method for finding inconsistent treebank annotations.
</prevsent>
</prevsection>
<citsent citstr=" W99-0623 ">
1 int roduct ion henderson and brill (1999) <papid> W99-0623 </papid>showed that independent human research efforts produce parsers that can be combined for an overall boost in accuracy.</citsent>
<aftsection>
<nextsent>finding an ensemble of parsers designed to complement each other is clearly desirable.
</nextsent>
<nextsent>the parsers would need to be the result of unified research effort, though, in which the errors made by one parser are targeted with priority by the developer of another parser.
</nextsent>
<nextsent>a set of five parsers which each achieve only 40% exact sentence accuracy would be extremely valu-able if they made errors in such way that at least two of the five were correct on any given sentence (and the others abstained or were wrong in different ways).
</nextsent>
<nextsent>100% sentence accuracy could be achieved by selecting the hypothesis that was proposed by the two parsers that agreed completely.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A592">
<title id=" A00-2005.xml">bagging and boosting a treebank parser </title>
<section> background </section>
<citcontext>
<prevsection>
<prevsent>given novel sentence 8test ctest , combine.
</prevsent>
<prevsent>the collection of hypotheses ti = fi(stest) us-ing the unweighted constituent voting scheme of henderson and brill (1999).<papid> W99-0623 </papid></prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
the training set for these experiments was sections 01-21 of the penn treebank (marcus et al, 1993).<papid> J93-2004 </papid></citsent>
<aftsection>
<nextsent>the test set was section 23.
</nextsent>
<nextsent>the parser induction algorithm used in all of the experiments in this pa-per was distribution of collins model 2 parser (collins, 1997).<papid> P97-1003 </papid></nextsent>
<nextsent>all comparisons made below refer to results we obtained using collins parser.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A593">
<title id=" A00-2005.xml">bagging and boosting a treebank parser </title>
<section> background </section>
<citcontext>
<prevsection>
<prevsent>the training set for these experiments was sections 01-21 of the penn treebank (marcus et al, 1993).<papid> J93-2004 </papid></prevsent>
<prevsent>the test set was section 23.</prevsent>
</prevsection>
<citsent citstr=" P97-1003 ">
the parser induction algorithm used in all of the experiments in this pa-per was distribution of collins model 2 parser (collins, 1997).<papid> P97-1003 </papid></citsent>
<aftsection>
<nextsent>all comparisons made below refer to results we obtained using collins parser.
</nextsent>
<nextsent>the results for bagging are shown in figure 2 and table 1.
</nextsent>
<nextsent>the row of figures are (from left-to-right) training set f-measure ~,test set f-measure, percent perfectly parsed sentences in training set, and per-cent perfectly parsed sentences in test set.
</nextsent>
<nextsent>an en-semble of bags was produced one bag at time.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A594">
<title id=" A00-2005.xml">bagging and boosting a treebank parser </title>
<section> classifier induction: ct ~- ~(lt).  </section>
<citcontext>
<prevsection>
<prevsent>they also give several examples for how to pick an appropriate a, and selection generally depends on the possible outputs of the underlying learner.
</prevsent>
<prevsent>boosting has been used in few nlp systems.
</prevsent>
</prevsection>
<citsent citstr=" P98-1083 ">
haruno et al (1998) <papid> P98-1083 </papid>used boosting to produce more accurate classifiers which were embedded as control 35 set instance r gain exact gain training original parser 96.25 96.31 96.28 na 64.7 na initial 93.61 93.63 93.62 0.00 55.5 0.0 bestf(15) 96.16 95.86 96.01 2.39 62.1 6.6 final(15) 96.16 95.86 96.01 2.39 62.1 6.6 test original parser 88.73 88.54 88.63 na 34.9 na initial 88.43 88.34 88.38 0.00 33.3 0.0 trainbestf(15) 89.54 88.80 89.17 0.79 34.6 1.3 testbestf(13) 89.55 88.84 89.19 0.81 34.7 1.4 final(15) 89.54 88.80 89.17 0.79 34.6 1.3 table 1: bagging the treebank mechanisms of parser for japanese.</citsent>
<aftsection>
<nextsent>the creators of ada boost used it to perform text classification (schapire and singer, 2000).
</nextsent>
<nextsent>abney et al (1999) <papid> W99-0606 </papid>performed part-of-speech tagging and prepositional phrase attachment using ada boost as core compo- nent.</nextsent>
<nextsent>they found they could achieve accuracies on both tasks that were competitive with the state of the art.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A595">
<title id=" A00-2005.xml">bagging and boosting a treebank parser </title>
<section> classifier induction: ct ~- ~(lt).  </section>
<citcontext>
<prevsection>
<prevsent>haruno et al (1998) <papid> P98-1083 </papid>used boosting to produce more accurate classifiers which were embedded as control 35 set instance r gain exact gain training original parser 96.25 96.31 96.28 na 64.7 na initial 93.61 93.63 93.62 0.00 55.5 0.0 bestf(15) 96.16 95.86 96.01 2.39 62.1 6.6 final(15) 96.16 95.86 96.01 2.39 62.1 6.6 test original parser 88.73 88.54 88.63 na 34.9 na initial 88.43 88.34 88.38 0.00 33.3 0.0 trainbestf(15) 89.54 88.80 89.17 0.79 34.6 1.3 testbestf(13) 89.55 88.84 89.19 0.81 34.7 1.4 final(15) 89.54 88.80 89.17 0.79 34.6 1.3 table 1: bagging the treebank mechanisms of parser for japanese.</prevsent>
<prevsent>the creators of ada boost used it to perform text classification (schapire and singer, 2000).</prevsent>
</prevsection>
<citsent citstr=" W99-0606 ">
abney et al (1999) <papid> W99-0606 </papid>performed part-of-speech tagging and prepositional phrase attachment using ada boost as core compo- nent.</citsent>
<aftsection>
<nextsent>they found they could achieve accuracies on both tasks that were competitive with the state of the art.
</nextsent>
<nextsent>as side effect, they found that inspecting the samples that were consistently given the most weight during boosting revealed some faulty anno-tations in the corpus.
</nextsent>
<nextsent>in all of these systems, ad- aboost has been used as traditional classification system.
</nextsent>
<nextsent>our goal is to recast boosting for parsing while con-sidering parsing system as the embedded learner.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A599">
<title id=" A97-1045.xml">construction and visualization of key term hierarchies </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>they all target frequently co-occurring words in running text.
</prevsent>
<prevsent>the earlier work of choueka (1988) proposed pure frequency approach in which only quantitative selection criteria were established and applied.
</prevsent>
</prevsection>
<citsent citstr=" J90-1003 ">
church and hanks (1990) <papid> J90-1003 </papid>introduced statistical measurement called mutual information for extracting strongly associated or collocated words.</citsent>
<aftsection>
<nextsent>tools like xtract (smadja 1993) <papid> J93-1007 </papid>were based on the work of church and others, but made step forward by incorporating various statistical measurements like z-score and variance of distribution, as well as shallow linguistic techniques like part-of-speech tagging and lemmatization of input data and partial parsing of raw output.</nextsent>
<nextsent>exemplary linguistic approaches can be found in the work by str-zalkowsky (1993) where fast and accurate syntactic parser is the prerequisite for the selection of significant phrasal terms.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A600">
<title id=" A97-1045.xml">construction and visualization of key term hierarchies </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>the earlier work of choueka (1988) proposed pure frequency approach in which only quantitative selection criteria were established and applied.
</prevsent>
<prevsent>church and hanks (1990) <papid> J90-1003 </papid>introduced statistical measurement called mutual information for extracting strongly associated or collocated words.</prevsent>
</prevsection>
<citsent citstr=" J93-1007 ">
tools like xtract (smadja 1993) <papid> J93-1007 </papid>were based on the work of church and others, but made step forward by incorporating various statistical measurements like z-score and variance of distribution, as well as shallow linguistic techniques like part-of-speech tagging and lemmatization of input data and partial parsing of raw output.</citsent>
<aftsection>
<nextsent>exemplary linguistic approaches can be found in the work by str-zalkowsky (1993) where fast and accurate syntactic parser is the prerequisite for the selection of significant phrasal terms.
</nextsent>
<nextsent>different applications aim at different ypes of key terms.
</nextsent>
<nextsent>for the purpose of generating key terms for our prototype system, we have adopted =learn data from data  approach.
</nextsent>
<nextsent>the novelty of this 307 approach lies in the automatic comparison of two sample datasets, topic focused dataset based on predefined topic and larger and more general base dataset.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A601">
<title id=" A00-1026.xml">extracting molecular binding relationships from biomedical text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>far more scientific information exists in the lit-erature than in any structured atabase.
</prevsent>
<prevsent>con-venient access to this information could signifi-cantly benefit research activities in various fields.
</prevsent>
</prevsection>
<citsent citstr=" P99-1001 ">
the emerging technology of information extraction (appelt and israel 1997, hearst 1999) <papid> P99-1001 </papid>provides means of gaining access to this in- formation.</citsent>
<aftsection>
<nextsent>in this paper we report on project extract biomolecular data from biomedical text.
</nextsent>
<nextsent>we concentrate on molecular binding affinity, which provides strong indication of macro-molecular function and is core phenomenon in molecular biology.
</nextsent>
<nextsent>our ultimate goal is to auto-matically construct database of binding rela-tionships asserted in medline citations.
</nextsent>
<nextsent>the national library of medicine medline textual database is an online reposi-tory of more than 10 million citations from the biomedical literature.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A602">
<title id=" A00-1026.xml">extracting molecular binding relationships from biomedical text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for example, our goal in this project is to extract he binding predications in (2) from the text in (1).
</prevsent>
<prevsent>(1) cc chemokine ceptor 1 (ccr1) is ex-pressed in neutrophils, monocytes, lympho-cytes, and eosinophils, and binds the leuko-cyte chemoattractant hematopoiesis regulator macro phage inflammatory protein (mip)- 1 alpha, as well as several related cc chemokines.
</prevsent>
</prevsection>
<citsent citstr=" A97-1030 ">
(2)  cc chemokine receptor 1  binds  leukocyte chemoattractant   cc chemokine ceptor 1  binds  hematopoiesis regulator macro phage inflammatory protein- 1alpha   cc chemokine ceptor 1  binds  related cc chemokine  considerable interest in information extrac-tion has concentrated on identifying named enti-ties in text pertaining to current events (for ex-ample, wacholder et al 1997, <papid> A97-1030 </papid>voorhees and harman 1998, <papid> X98-1031 </papid>and muc-7); however, several recent efforts have been directed at biomolecular data (blaschke al. 1999, craven and kumlien 1999, and rindflesch et al 2000, for example).</citsent>
<aftsection>
<nextsent>the overall goal is to transform the information 188 encoded in text into more readily accessible tbrmat, typically template with slots named for the participants in the scenario of interest.
</nextsent>
<nextsent>the template for molecular binding can be thought of as simple predication with predicate  bind  and two arguments which participate (sym- metrically) in the relationship: binds( ,  ).
</nextsent>
<nextsent>various strategies, both linguistic and statis-tical, have been used in information extraction efforts.
</nextsent>
<nextsent>we introduce prolog program called arbiter (assess and retrieve binding termi- nology) that takes advantage of an existing do- main knowledge source and relies on syntactic cues provided by partial parser in order to identify and extract binding relations from text.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A603">
<title id=" A00-1026.xml">extracting molecular binding relationships from biomedical text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for example, our goal in this project is to extract he binding predications in (2) from the text in (1).
</prevsent>
<prevsent>(1) cc chemokine ceptor 1 (ccr1) is ex-pressed in neutrophils, monocytes, lympho-cytes, and eosinophils, and binds the leuko-cyte chemoattractant hematopoiesis regulator macro phage inflammatory protein (mip)- 1 alpha, as well as several related cc chemokines.
</prevsent>
</prevsection>
<citsent citstr=" X98-1031 ">
(2)  cc chemokine receptor 1  binds  leukocyte chemoattractant   cc chemokine ceptor 1  binds  hematopoiesis regulator macro phage inflammatory protein- 1alpha   cc chemokine ceptor 1  binds  related cc chemokine  considerable interest in information extrac-tion has concentrated on identifying named enti-ties in text pertaining to current events (for ex-ample, wacholder et al 1997, <papid> A97-1030 </papid>voorhees and harman 1998, <papid> X98-1031 </papid>and muc-7); however, several recent efforts have been directed at biomolecular data (blaschke al. 1999, craven and kumlien 1999, and rindflesch et al 2000, for example).</citsent>
<aftsection>
<nextsent>the overall goal is to transform the information 188 encoded in text into more readily accessible tbrmat, typically template with slots named for the participants in the scenario of interest.
</nextsent>
<nextsent>the template for molecular binding can be thought of as simple predication with predicate  bind  and two arguments which participate (sym- metrically) in the relationship: binds( ,  ).
</nextsent>
<nextsent>various strategies, both linguistic and statis-tical, have been used in information extraction efforts.
</nextsent>
<nextsent>we introduce prolog program called arbiter (assess and retrieve binding termi- nology) that takes advantage of an existing do- main knowledge source and relies on syntactic cues provided by partial parser in order to identify and extract binding relations from text.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A604">
<title id=" A00-1026.xml">extracting molecular binding relationships from biomedical text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>examples pertinent to binding terminology include the semantic types  amino acid, peptide, or protein  and  nucleo-tide sequence .
</prevsent>
<prevsent>the specialist lexicon (with associated lexical access tools) supplies syntactic information for large compilation of biomedical and general english terms.
</prevsent>
</prevsection>
<citsent citstr=" A92-1018 ">
the specialist minimal commitment parser relies on the specialist lexicon as well as the xerox stochastic tagger (cutting et al. 1992).<papid> A92-1018 </papid></citsent>
<aftsection>
<nextsent>the output produced is in the tradition of partial parsing (hindle 1983, <papid> P83-1019 </papid>mcdonald 1992, weischedel et al 1993) <papid> J93-2006 </papid>and concentrates on the simple noun phrase, what weischedel et al.</nextsent>
<nextsent>(1993) call the  core noun phrase,  that is noun phrase with no modification to the right of the head.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A605">
<title id=" A00-1026.xml">extracting molecular binding relationships from biomedical text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the specialist lexicon (with associated lexical access tools) supplies syntactic information for large compilation of biomedical and general english terms.
</prevsent>
<prevsent>the specialist minimal commitment parser relies on the specialist lexicon as well as the xerox stochastic tagger (cutting et al. 1992).<papid> A92-1018 </papid></prevsent>
</prevsection>
<citsent citstr=" P83-1019 ">
the output produced is in the tradition of partial parsing (hindle 1983, <papid> P83-1019 </papid>mcdonald 1992, weischedel et al 1993) <papid> J93-2006 </papid>and concentrates on the simple noun phrase, what weischedel et al.</citsent>
<aftsection>
<nextsent>(1993) call the  core noun phrase,  that is noun phrase with no modification to the right of the head.
</nextsent>
<nextsent>several approaches provide similar output based on statistics (church 1988, <papid> A88-1019 </papid>zhai 1997, <papid> A97-1046 </papid>for example), finite-state machine (ait- mokhtar and chanod 1997), or hybrid ap-proach combining statistics and linguistic rules (voutilainen and padro 1997).</nextsent>
<nextsent>the specialist parser is based on the no-tion of barrier words (tersmette al. 1988), which indicate boundaries between phrases.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A606">
<title id=" A00-1026.xml">extracting molecular binding relationships from biomedical text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the specialist lexicon (with associated lexical access tools) supplies syntactic information for large compilation of biomedical and general english terms.
</prevsent>
<prevsent>the specialist minimal commitment parser relies on the specialist lexicon as well as the xerox stochastic tagger (cutting et al. 1992).<papid> A92-1018 </papid></prevsent>
</prevsection>
<citsent citstr=" J93-2006 ">
the output produced is in the tradition of partial parsing (hindle 1983, <papid> P83-1019 </papid>mcdonald 1992, weischedel et al 1993) <papid> J93-2006 </papid>and concentrates on the simple noun phrase, what weischedel et al.</citsent>
<aftsection>
<nextsent>(1993) call the  core noun phrase,  that is noun phrase with no modification to the right of the head.
</nextsent>
<nextsent>several approaches provide similar output based on statistics (church 1988, <papid> A88-1019 </papid>zhai 1997, <papid> A97-1046 </papid>for example), finite-state machine (ait- mokhtar and chanod 1997), or hybrid ap-proach combining statistics and linguistic rules (voutilainen and padro 1997).</nextsent>
<nextsent>the specialist parser is based on the no-tion of barrier words (tersmette al. 1988), which indicate boundaries between phrases.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A607">
<title id=" A00-1026.xml">extracting molecular binding relationships from biomedical text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the output produced is in the tradition of partial parsing (hindle 1983, <papid> P83-1019 </papid>mcdonald 1992, weischedel et al 1993) <papid> J93-2006 </papid>and concentrates on the simple noun phrase, what weischedel et al.</prevsent>
<prevsent>(1993) call the  core noun phrase,  that is noun phrase with no modification to the right of the head.</prevsent>
</prevsection>
<citsent citstr=" A88-1019 ">
several approaches provide similar output based on statistics (church 1988, <papid> A88-1019 </papid>zhai 1997, <papid> A97-1046 </papid>for example), finite-state machine (ait- mokhtar and chanod 1997), or hybrid ap-proach combining statistics and linguistic rules (voutilainen and padro 1997).</citsent>
<aftsection>
<nextsent>the specialist parser is based on the no-tion of barrier words (tersmette al. 1988), which indicate boundaries between phrases.
</nextsent>
<nextsent>af-ter lexical look-up and resolution of category la-bel ambiguity by the xerox tagger, comp lemen tizers, conjunctions, modals, prepositions, and verbs are marked as boundaries.
</nextsent>
<nextsent>subsequently, boundaries are considered to open new phrase (and close the preceding phrase).
</nextsent>
<nextsent>any phrase containing noun is considered to be (simple) noun phrase, and in such phrase, the right-most noun is labeled as the head, and all other items (other than determiners) are labeled as modifi-ers.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A608">
<title id=" A00-1026.xml">extracting molecular binding relationships from biomedical text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the output produced is in the tradition of partial parsing (hindle 1983, <papid> P83-1019 </papid>mcdonald 1992, weischedel et al 1993) <papid> J93-2006 </papid>and concentrates on the simple noun phrase, what weischedel et al.</prevsent>
<prevsent>(1993) call the  core noun phrase,  that is noun phrase with no modification to the right of the head.</prevsent>
</prevsection>
<citsent citstr=" A97-1046 ">
several approaches provide similar output based on statistics (church 1988, <papid> A88-1019 </papid>zhai 1997, <papid> A97-1046 </papid>for example), finite-state machine (ait- mokhtar and chanod 1997), or hybrid ap-proach combining statistics and linguistic rules (voutilainen and padro 1997).</citsent>
<aftsection>
<nextsent>the specialist parser is based on the no-tion of barrier words (tersmette al. 1988), which indicate boundaries between phrases.
</nextsent>
<nextsent>af-ter lexical look-up and resolution of category la-bel ambiguity by the xerox tagger, comp lemen tizers, conjunctions, modals, prepositions, and verbs are marked as boundaries.
</nextsent>
<nextsent>subsequently, boundaries are considered to open new phrase (and close the preceding phrase).
</nextsent>
<nextsent>any phrase containing noun is considered to be (simple) noun phrase, and in such phrase, the right-most noun is labeled as the head, and all other items (other than determiners) are labeled as modifi-ers.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A609">
<title id=" A00-2026.xml">trainable methods for surface natural language generation </title>
<section> previous approaches.  </section>
<citcontext>
<prevsection>
<prevsent>there are more sophisticated surface genera-tion packages, uch as fuf/surge (elhadad and robin, 1996), kpml (bateman, 1996), mumble (meteer et al, 1987), and realpro (lavoie and ram- bow, 1997), which produce natural anguage text from an abstract semantic representation.
</prevsent>
<prevsent>these packages require linguistic sophistication order to write the abstract semantic representation, but they are flexible because minor changes to the input can accomplish major changes to the generated text.
</prevsent>
</prevsection>
<citsent citstr=" J96-1002 ">
the only trainable approaches (known to the au- thor) to surface generation are the purely statistical machine translation (mt) systems uch as (berger et al, 1996) <papid> J96-1002 </papid>and the corpus-based generation sys-tem described in (langkilde and knight, 1998).<papid> P98-1116 </papid></citsent>
<aftsection>
<nextsent>the mt systems of (berger et al, 1996) <papid> J96-1002 </papid>learn to gen-erate text in the target language straight from the source language, without the aid of an explicit se-mantic representation.</nextsent>
<nextsent>i contrast, (langkilde and knight, 1998) <papid> P98-1116 </papid>uses corpus-derived statistical knowl-edge to rank plausible hypotheses from grammar- based surface generation component.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A611">
<title id=" A00-2026.xml">trainable methods for surface natural language generation </title>
<section> previous approaches.  </section>
<citcontext>
<prevsection>
<prevsent>there are more sophisticated surface genera-tion packages, uch as fuf/surge (elhadad and robin, 1996), kpml (bateman, 1996), mumble (meteer et al, 1987), and realpro (lavoie and ram- bow, 1997), which produce natural anguage text from an abstract semantic representation.
</prevsent>
<prevsent>these packages require linguistic sophistication order to write the abstract semantic representation, but they are flexible because minor changes to the input can accomplish major changes to the generated text.
</prevsent>
</prevsection>
<citsent citstr=" P98-1116 ">
the only trainable approaches (known to the au- thor) to surface generation are the purely statistical machine translation (mt) systems uch as (berger et al, 1996) <papid> J96-1002 </papid>and the corpus-based generation sys-tem described in (langkilde and knight, 1998).<papid> P98-1116 </papid></citsent>
<aftsection>
<nextsent>the mt systems of (berger et al, 1996) <papid> J96-1002 </papid>learn to gen-erate text in the target language straight from the source language, without the aid of an explicit se-mantic representation.</nextsent>
<nextsent>i contrast, (langkilde and knight, 1998) <papid> P98-1116 </papid>uses corpus-derived statistical knowl-edge to rank plausible hypotheses from grammar- based surface generation component.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A619">
<title id=" A00-2026.xml">trainable methods for surface natural language generation </title>
<section> d iscuss ion   </section>
<citcontext>
<prevsection>
<prevsent>nlg2 and nlg3 automatically determine attribute ordering by simultaneously searching multiple or- derings.
</prevsent>
<prevsent>in grammar-based approaches, uch pref-erences need to be manually encoded.
</prevsent>
</prevsection>
<citsent citstr=" J97-2001 ">
nlg2 and nlg3 solve the lexical choice problem by learning the words (via features in the maximum entropy probability model) that correlate with given at-tribute and local context, whereas (elhadad et al, 1997) <papid> J97-2001 </papid>uses rule-based approach to decide the word choice.</citsent>
<aftsection>
<nextsent>while trainable approaches avoid the expense of crafting grammar to determine attribute order-ing and lexicai choice, they are less accurate than grammar-based approaches.
</nextsent>
<nextsent>for short phrases, ac-curacy is typically 100% with grammar-based ap-proaches ince the grammar writer can either cor-rect or add rule to generate the phrase of interest once an error is detected.
</nextsent>
<nextsent>whereas with nlg2 and nlg3, one can tune the feature patterns, search pa-rameters, and training data itself, but there is no guarantee that the tuning will result in 100% gener-ation accuracy.
</nextsent>
<nextsent>our approach differs from the corpus-based surface generation approaches of (langkilde and knight, 1998) <papid> P98-1116 </papid>and (berger et al, 1996).<papid> J96-1002 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A627">
<title id=" A94-1001.xml">bilingual generation of job from quasi conceptual descriptions forms </title>
<section> functional ty.  </section>
<citcontext>
<prevsection>
<prevsent>3.3 linguistic realization.
</prevsent>
<prevsent>given the close correspondence tween conceptual forms and surface linguistic forms, we decided to re-ex- amine our initial assumption that the job description generator would be implemented by adapting cog entex existing text-generation shell.
</prevsent>
</prevsection>
<citsent citstr=" C92-3158 ">
versions of this generator, based on meaning-text theory (mel ~alk &amp; pertsov, 1987), have been used in other applications, including the generation of bilingual weather forecasts (goldberg et al, to appear) and statis-tical reports (iordanskaja et al, 1992).<papid> C92-3158 </papid></citsent>
<aftsection>
<nextsent>in order to pro-duce text suitable to these applications, the generator starts with deep conceptual representations, successively deriving deep-syntactic, surface-syntactic, morphologi-cal, and surface representations.
</nextsent>
<nextsent>it also incorporates so-phisticated mechanisms for text planning and para-phrase.
</nextsent>
<nextsent>for several reasons, the existing enerator was con-sidered unsuitable for this application.
</nextsent>
<nextsent>the main ratio-nale was that, since concepts already resembled pieces of surface text, those pieces hould not be reconstructed by the generator unless this was necessary to produce text of acceptable quality.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A628">
<title id=" A92-1005.xml">real time linguistic analysis for continuous speech understanding </title>
<section> recognition and understanding.  </section>
<citcontext>
<prevsection>
<prevsent>ter stage, or understanding stage, which completes the recognition activity by finding the most plausible word sequence and by understanding its meaning.
</prevsent>
<prevsent>in this way each level can focus on its own basic problems and develop specific techniques, till maintaining the advan-tage of the integration.
</prevsent>
</prevsection>
<citsent citstr=" C86-1138 ">
most of the approaches based on this idea (e.g. \[hayes et al 1986\]) <papid> C86-1138 </papid>are characterized by the use of knowledge ngineering techniques at both levels, while our recognition stage is based on probabilistic tech-nique, the hidden markov models (hmm).</citsent>
<aftsection>
<nextsent>the most recent research indicates that, as far as word recog-nition is concerned, the hmm give the best results \[lee 1990, fissore et al 1989\].
</nextsent>
<nextsent>the set of word hypotheses produced by the recogni-tion stage is called lattice (fig.
</nextsent>
<nextsent>lb).
</nextsent>
<nextsent>every word hypoth-esis is characterized by the starting and ending points of the utterance portion in which it has been spotted, and its score, expressing its acoustic likelihood, i.e. mea-sure of the probability for the word of having been ut-tered in that position.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A629">
<title id=" A94-1016.xml">three heads are better than one </title>
<section> integrating.  </section>
<citcontext>
<prevsection>
<prevsent>our hypothesis that if an mt environment can use the best results from variety of mt systems working simultaneously on the same text, the overall quality will improve.
</prevsent>
<prevsent>using this novel approach to mt in the latest version of the pan gloss mt project, we submit an input text to battery of machine translation systems (engines), collect their (possibly, incomplete) results in joint chart data structure and select the overall best translation using set of simple heuristics.
</prevsent>
</prevsection>
<citsent citstr=" H93-1038 ">
multi-engine output in our experiment we used three mt engines: * knowledge-based mt (kbmt) system, the mainline pan gloss engine (frederking et al, 1993<papid> H93-1038 </papid>b); ? an example-based mt (ebmt) system (see (nirenburg et al, 1993; nirenburg et al, 1994b); the original idea is due to nagao (na- gao, 1984)); and ? lexical transfer system, fortified with mor-phological analysis and synthesis modules and relying on number of databases - - machine-readable dictionary (the collins span- ish/english), the lexicons used by the kbmt modules, large set of user-generated bilingual glossaries as well as gazetteer and list of proper and organization ames.</citsent>
<aftsection>
<nextsent>the outputs from these engines (target language words and phrases) are recorded in chart whose positions correspond to words in the source language input.
</nextsent>
<nextsent>as result of the operation of each of the mt engines, new edges are added to the chart, each labeled with the translation of region of the input string and indexed by this region beginning and end positions.
</nextsent>
<nextsent>we will refer to all of these edges as components (as in  components of the translation ) for the remainder of this article.
</nextsent>
<nextsent>the kbmt and ebmt engines also carry quality score for each output element.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A631">
<title id=" A94-1016.xml">three heads are better than one </title>
<section> 3 su  </section>
<citcontext>
<prevsection>
<prevsent>as the databases for this are quite large (all together, over 400,000 entries), adding scores to individual entries is, in the short run, prohibitive.
</prevsent>
<prevsent>we have not as yet discovered any feasible automatic technique for gen-erating such scores.
</prevsent>
</prevsection>
<citsent citstr=" J90-2002 ">
instead, we are planning to use an english language model on the output, in man-ner similar to that done by speech and statistical translation systems (brown et al, 1990).<papid> J90-2002 </papid></citsent>
<aftsection>
<nextsent>statisti-cally generating such model is feasible, since it does not relyon knowing correspondences between source
</nextsent>



</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A632">
<title id=" C00-1025.xml">mining tables from large scale html texts </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>tables, which arc simple and easy to use, are very common presentation sclleme for writers to describe schedules, organize statistical data, summarize cxpcrilnental results, and so on, in texts ol  different domains.
</prevsent>
<prevsent>because tables provide rich inlbrmation, table acquisition is useful for many applications such as document tmderstauding, question-and-answering, text retrieval, etc. however, most of previous approaches on text data mining focus on text parts, and only few touch on tabular ones (appelt and israel, 1997; gaizauskas and wilks, 1998; hurst, 1999a).
</prevsent>
</prevsection>
<citsent citstr=" A97-1032 ">
of the papers on table extractions (douglas, hurst and quinn, 1995; douglas and hurst 1996; hurst and douglas, 1997; <papid> A97-1032 </papid>ng, lim and koo, 1999), plain texts arc their targets.</citsent>
<aftsection>
<nextsent>i11 plain text, writers often use special symbols, e.g., tabs, blanks, dashes, etc., to inake tables.
</nextsent>
<nextsent>the following shows an example.
</nextsent>
<nextsent>it depicts book titles, authors, and prices.
</nextsent>
<nextsent>title author price statistical language learning e.chamiak $30 cross-language inforlnation p.elrieval g. grefenstette $115 natural language information retrieval t.slrzalkowski $144 when detecting il  there is table in free text, we should disambiguatc tile uses of tile special symbols.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A634">
<title id=" C00-1054.xml">finite state multimodal parsing and understanding </title>
<section> abstract </section>
<citcontext>
<prevsection>

<prevsent>multimodal interfaces require effective parsing and nn(lerstanding of utterances whose content is dis-tributed across multiple input modes.
</prevsent>
</prevsection>
<citsent citstr=" P98-1102 ">
johnston 1998 <papid> P98-1102 </papid>presents an approach in which strategies lbr mul-timodal integration are stated declaratively using unification-based grammar that is used by mnlti- dilnensional chart parser to compose inputs.</citsent>
<aftsection>
<nextsent>this approach is highly expressive and supports broad class of interfaces, but offers only limited potential for lnutual compensation among the input modes, is subject signilicant concerns in terms o1  coml)uta- tional complexity, and complicates selection among alternative multimodal interpretations of the input.
</nextsent>
<nextsent>in tiffs papeh we l)resent an alternative approacla in which multimodal lmrsing and understanding are achieved using weighted finite-state device which takes speech and gesture streams as inputs and out-puts their joint interpretation.
</nextsent>
<nextsent>this approach is sig-nificantly more efficienl, enables tight-coupling of multimodal understanding with speech recognition, and provides general probabilistic fralnework for multimodal ambiguity resolution.
</nextsent>
<nextsent>multimodal interfaces are systems that allow input and/or output be conveyed over multiple different channels uch as speech, graphics, and gesture.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A635">
<title id=" C00-1054.xml">finite state multimodal parsing and understanding </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our specific concern here is with multimodal inter-faces supporting input by speech, pen, and touch, but the approach we describe has far broader applicabil-ity.
</prevsent>
<prevsent>these interfaces stand to play critical role in the ongoing migration of interaction fi oln the desktop to wireless portable computing devices (pi)as, next- generation phones) that offer limited screen real es- tale, and other keyboard-less platforms uch as pub-lic information kiosks.
</prevsent>
</prevsection>
<citsent citstr=" P97-1036 ">
to realize their full potential, multimodal inter-faces need to support not just input from multiple modes, but synergistic multimodal utterances opti-mally distributed over the available modes (john- ston et al, 1997).<papid> P97-1036 </papid></citsent>
<aftsection>
<nextsent>in order to achieve this, an f fcctive method for integration of content fi oln dill ferent modes is needed.
</nextsent>
<nextsent>johnston (1998<papid> P98-1102 </papid>b) shows how techniques from natural language processing (unification-based gramumrs and chart parsing) can be adapted to support parsing and interpretation of utterances distributed over multiple modes.</nextsent>
<nextsent>in that approach, speech and gesture recognition produce ~,- best lists of recognition results which are assigned typed feature structure representations (carpenter, 1992) and passed to luultidimensioual chart parsel ? that uses lnultimodal unification-based granunar to combine the representations assigned to the input el- ements.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A639">
<title id=" C00-1054.xml">finite state multimodal parsing and understanding </title>
<section> finite-state language processing.  </section>
<citcontext>
<prevsection>
<prevsent>the transition is tra-versed if its input symbol matches the current sym-bol in the input and generates the output symbol as-sociated with the transition.
</prevsent>
<prevsent>in other words, an fst can be regarded as 2-tape fsa with an input tape from which the input symbols are read and an output tape where the output symbols are written.
</prevsent>
</prevsection>
<citsent citstr=" J94-3001 ">
finite-state machines have been extensively ap-plied to many aspects of language processing in-cluding, speech recognition (pereira nd riley, 1997; riccardi et al, 1996), phonology (kaplan and kay, 1994), <papid> J94-3001 </papid>morphology (koskenniemi, 1984), chunk-ing (abney, 1991; joshi and hopely, 1997; ban-galore, 1997), parsing (roche, 1999), and machine translation (bangalore and riccardi, 2000).<papid> W00-0508 </papid></citsent>
<aftsection>
<nextsent>finite-state models are attractive n~echanisms for language processing since they are (a) efficiently learn able fiom data (b) generally effective for decod-ing and (c) associated with calculus for composing machines which allows for straightforward integra-tion of constraints fl om various levels of language processing.
</nextsent>
<nextsent>furdmrmore, software implementing the finite-state calculus is available for research pur-poses (mohri eta\[., 1998).
</nextsent>
<nextsent>another motivation for our choice of finite-state models is that they enable tight integration of language processing with speech and gesture recognition.
</nextsent>
<nextsent>multimodal integration involves merging semantic content fi om multiple streams to build joint inter-pretation for inultimodal utterance.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A640">
<title id=" C00-1054.xml">finite state multimodal parsing and understanding </title>
<section> finite-state language processing.  </section>
<citcontext>
<prevsection>
<prevsent>the transition is tra-versed if its input symbol matches the current sym-bol in the input and generates the output symbol as-sociated with the transition.
</prevsent>
<prevsent>in other words, an fst can be regarded as 2-tape fsa with an input tape from which the input symbols are read and an output tape where the output symbols are written.
</prevsent>
</prevsection>
<citsent citstr=" W00-0508 ">
finite-state machines have been extensively ap-plied to many aspects of language processing in-cluding, speech recognition (pereira nd riley, 1997; riccardi et al, 1996), phonology (kaplan and kay, 1994), <papid> J94-3001 </papid>morphology (koskenniemi, 1984), chunk-ing (abney, 1991; joshi and hopely, 1997; ban-galore, 1997), parsing (roche, 1999), and machine translation (bangalore and riccardi, 2000).<papid> W00-0508 </papid></citsent>
<aftsection>
<nextsent>finite-state models are attractive n~echanisms for language processing since they are (a) efficiently learn able fiom data (b) generally effective for decod-ing and (c) associated with calculus for composing machines which allows for straightforward integra-tion of constraints fl om various levels of language processing.
</nextsent>
<nextsent>furdmrmore, software implementing the finite-state calculus is available for research pur-poses (mohri eta\[., 1998).
</nextsent>
<nextsent>another motivation for our choice of finite-state models is that they enable tight integration of language processing with speech and gesture recognition.
</nextsent>
<nextsent>multimodal integration involves merging semantic content fi om multiple streams to build joint inter-pretation for inultimodal utterance.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A643">
<title id=" C00-1054.xml">finite state multimodal parsing and understanding </title>
<section> finite-state multimodal grammars.  </section>
<citcontext>
<prevsection>
<prevsent>the elements on the meaning tape are concate- nated and the buffer references are replaced to yield s: email this person and that organization g: gp cl  go e2 m: email(\[ person(ct) , org(c2) \]) figure 4: messaging domain example email(~)er.son(objid367), or.q(objids93)\]).
</prevsent>
<prevsent>as more recursive semantic phenomena such as pos- sess ives and other complex noun phrases are added to the grammar the resulting machines become larger.
</prevsent>
</prevsection>
<citsent citstr=" J97-2003 ">
however, the computational consequences of this can be lessened by lazy ewfluation tech-niques (mohri, 1997) <papid> J97-2003 </papid>and we believe that this finite- state approach to constructing semantic representa-tions is viable for broad range of sophisticated lan-guage interface tasks.</citsent>
<aftsection>
<nextsent>we have implemented size- able multimodal cfg for vpq (see section 1): 417 rules and lexicon of 2388 words.
</nextsent>
<nextsent>3.2 multimodal finite-state transducers.
</nextsent>
<nextsent>while three-tape finite-state automaton is feasi-ble in principle (rosenberg, 1964), currently avail-able tools for finite-state language processing (mohri et al, 1998) only support finite-state transducers (fsts) (two tapes).
</nextsent>
<nextsent>furthermore, speech recogniz- ers typically do not support ile use of three-tape fsa as language model.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A644">
<title id=" A92-1007.xml">automatic generation of multimodal weather reports from datasets </title>
<section> int roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>we consider the case of supplementing weather map with verbal note specifying those content portions that cannot be presented on the map or whose graphical pre-sentations distort the original information.
</prevsent>
<prevsent>the system discovers uch deficiencies of the graphical presentation and generates verbal comment on the map.
</prevsent>
</prevsection>
<citsent citstr=" C90-3059 ">
there are various projects concerning the production of weather reports, each of them setting specific goals 48 the rareas, rareas-2 and fog series of systems (bourbeau et al, 1990) <papid> C90-3059 </papid>developed by one of the most successful groups in weather report generation shares many concepts with the current project, the main differ-ences lying in the specification of the product and in the modality of the generated ocuments.</citsent>
<aftsection>
<nextsent>thus the cana-dian group deals exclusively with nl forecasts while our project considers the generation of multimodal reports and employs diverse means of specifying what one needs and in what mode he wants to receive the information.
</nextsent>
<nextsent>our previous work (kerpedjiev, 1990; kerepedjiev and noncheva, 1990) concerns the conversion of weather fore-casts from textual form to weather maps or texts in an- other language.
</nextsent>
<nextsent>this is translation problem rather than generation one.
</nextsent>
<nextsent>another feature that makes the previ-ous work different from the current one is the lack of co-ordination between the graphical and the textual parts.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A645">
<title id=" A00-1008.xml">plan based dialogue management in a physics tutor </title>
<section> integrated planning and execution for.  </section>
<citcontext>
<prevsection>
<prevsent>first, decomposition is better suited to the type of large- scale dialogue planning required in real-world tutoring system, as it is easier to establish what human speaker will say in given situation than to be able to understand why insufficient detail and generality to do means-end planning.
</prevsent>
<prevsent>second, hierarchical decomposition minimizes search time.
</prevsent>
</prevsection>
<citsent citstr=" J86-3001 ">
third, our dialogues are task-oriented and have hierarchical structure (grosz and sidner 1986).<papid> J86-3001 </papid></citsent>
<aftsection>
<nextsent>in such case, matching the structure of the domain simplifies operator development because they can often be derived from transcripts of human tutoring sessions.
</nextsent>
<nextsent>the hierarchy information is also useful in determining appropriate referring expressions.
</nextsent>
<nextsent>fourth, inter-leaved planning and execution is important for dialogue generation because we cannot predict he human user future utterances.
</nextsent>
<nextsent>in an htn-based system, it is straightforward to implement interleaved planning and execution because one only needs to expand the portion of the plan that is about to be executed.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A647">
<title id=" A00-1008.xml">plan based dialogue management in a physics tutor </title>
<section> implementation of at las-andes.  </section>
<citcontext>
<prevsection>
<prevsent>the first system we have implemented with ape is prototype atlas-andes system that replaces the hints usually given for an incorrect acceleration vector by choice of generated subdialogues.
</prevsent>
<prevsent>figure 4 shows the architecture of atlas-andes; any other system built with ape would look similar.
</prevsent>
</prevsection>
<citsent citstr=" P98-1059 ">
robust natural language understanding in atlas-andes is provided by ros6 carmel system (ros6 2000); it uses the spelling correction algorithm devised by elmi and evens (1998).<papid> P98-1059 </papid></citsent>
<aftsection>
<nextsent>5.2 structure of human tutorial dialogues.
</nextsent>
<nextsent>in an earlier analysis (kim, freedman and evens 1998) we showed that significant portion of human-human tutorial dialogues can be modeled with the hierarchical structure of task-oriented dialogues (grosz and sidner 1986).<papid> J86-3001 </papid></nextsent>
<nextsent>furthermore, main building block of the discourse hierarchy, corresponding to the transaction level in conversation analysis (sinclair and coulthard 1975), matches the tutoring episode defined by vanlehn et al (1998).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A650">
<title id=" C00-2108.xml">clustering verbs semantically according to their alternation behaviour </title>
<section> automatic acquisition of.  </section>
<citcontext>
<prevsection>
<prevsent>the basis could use for the selectional preferences was provided by the lexical heads ill the fi anm tokens.
</prevsent>
<prevsent>for example, the nouns appearing in the direct ob-ject slot of the transitive frame for the verb drink included coffee, milk, beer, indicating conceptual class like beverage tbr this argument slot.
</prevsent>
</prevsection>
<citsent citstr=" W97-0209 ">
i followed (resnik, 1993)/(resnik, 1997) <papid> W97-0209 </papid>who de-fined selectional preference as the amount of infor-mation verb provides about its semantic argument classes.</citsent>
<aftsection>
<nextsent>he utilised the wordnet taxonomy (beck- with et al, 1991) for probabilistic model captur-ing the co-occurrence behaviour of verbs and con-ceptual classes, where the conceptual classes were identified by wordnet synsets, sets of synonymous nouns within semantic hierarchy.
</nextsent>
<nextsent>referring to the above example, the three nouns coffee, milk, beer are in three different synsets -since they are not synonyms-, but are all subordinated to the synset {beverage, drink, potable}.
</nextsent>
<nextsent>the goal in this example would therefore be to determine the relevant synset as the most selectionally preferred synset for the di-rect object slot of the verb drink.
</nextsent>
<nextsent>redefined fbr iny usage, the selectional preference of verb tbr certain semantic lass within subcategorisation franm slot was deternfined by the association ass between verb and semantic lass: =des pl, c, lv~pog ~ (5) with the probabilities estimated by maxinmnl likeli- hood: f(v,, p(c*lvs) - f(vs) (6) p(cs) = f(c.,) _ f(cs) (7) f(c s) /(8) and the following interpretation: 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A651">
<title id=" C00-2108.xml">clustering verbs semantically according to their alternation behaviour </title>
<section> automatic acquisition of.  </section>
<citcontext>
<prevsection>
<prevsent>f(cs): numl)er of times semantic class ap-peared in fi ame slot of frame type disre-garding tim verb 4.
</prevsent>
<prevsent>~? c,~ **,,s f(c ~) equals f(s), the frequency of the argument slot within certain frame type, since summing over all possible classes within sub-categorisation flame slot equals the lmlnber of tinms the slot; appeared 5.
</prevsent>
</prevsection>
<citsent citstr=" C94-2123 ">
f(s): uulnber of times the franle type appeared, since the frequency of a. frame type equals the frequency of that frame with certain slot marked the fi equencies of semantic class concerning an argument slot, of frame type (dependent or inde-pendent of verb) were calculated by all approach slightly difl erent to resnik s, originally proposed by (ribas, 1994)/(<papid> C94-2123 </papid>ribas, 1995).<papid> E95-1016 </papid></citsent>
<aftsection>
<nextsent>for each noun ap-pearing in certain argument position its fi equency was divided by the nmnber of senses the noun was assigned by the wordnet hierarchy, to take account of the uncertainty about the sense of the noun.
</nextsent>
<nextsent>the fi action was allocated to each conceptual class in the hierarchy to which the noun belonged and accumu-lated upwards until top node was reached.
</nextsent>
<nextsent>tile result was numerical distribution over the word-net classes: /(noun) (8) s(c,/-- 1for example, when considering the noun coffee isolated from its context, we do not know whether we are talking about the beverage coffee, the plant coffee or coffee bean.
</nextsent>
<nextsent>thero.- fore, third of the frequency of the noun was assigned to each of the three classes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A652">
<title id=" C00-2108.xml">clustering verbs semantically according to their alternation behaviour </title>
<section> automatic acquisition of.  </section>
<citcontext>
<prevsection>
<prevsent>f(cs): numl)er of times semantic class ap-peared in fi ame slot of frame type disre-garding tim verb 4.
</prevsent>
<prevsent>~? c,~ **,,s f(c ~) equals f(s), the frequency of the argument slot within certain frame type, since summing over all possible classes within sub-categorisation flame slot equals the lmlnber of tinms the slot; appeared 5.
</prevsent>
</prevsection>
<citsent citstr=" E95-1016 ">
f(s): uulnber of times the franle type appeared, since the frequency of a. frame type equals the frequency of that frame with certain slot marked the fi equencies of semantic class concerning an argument slot, of frame type (dependent or inde-pendent of verb) were calculated by all approach slightly difl erent to resnik s, originally proposed by (ribas, 1994)/(<papid> C94-2123 </papid>ribas, 1995).<papid> E95-1016 </papid></citsent>
<aftsection>
<nextsent>for each noun ap-pearing in certain argument position its fi equency was divided by the nmnber of senses the noun was assigned by the wordnet hierarchy, to take account of the uncertainty about the sense of the noun.
</nextsent>
<nextsent>the fi action was allocated to each conceptual class in the hierarchy to which the noun belonged and accumu-lated upwards until top node was reached.
</nextsent>
<nextsent>tile result was numerical distribution over the word-net classes: /(noun) (8) s(c,/-- 1for example, when considering the noun coffee isolated from its context, we do not know whether we are talking about the beverage coffee, the plant coffee or coffee bean.
</nextsent>
<nextsent>thero.- fore, third of the frequency of the noun was assigned to each of the three classes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A653">
<title id=" A00-1034.xml">a hybrid approach for named entity and subtype tagging </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this task provides important semantic information, and is critical first step in any information extraction system.
</prevsent>
<prevsent>intense research has been focused on improving ne tagging accuracy using several different echniques.
</prevsent>
</prevsection>
<citsent citstr=" A97-1029 ">
these include rule-based systems \[krupka 1998\], hidden markov models (hmm) \[bikel et al 1997\] <papid> A97-1029 </papid>and maximum entropy models (maxent) \[borthwick 1998\].</citsent>
<aftsection>
<nextsent>a system based on manual rules may provide the best performance; however these require painstaking intense skilled labor..
</nextsent>
<nextsent>furthermore, shifting domains involves significant effort and may result in performance degradation.
</nextsent>
<nextsent>the strength of hmm models lie in their capacity for modeling local contextual information.
</nextsent>
<nextsent>hmms have been widely used in continuous peech recognition, part-of-speech tagging, ocr, etc., and are generally regarded as the most successful statistical modelling paradigm in these domains.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A655">
<title id=" A00-1034.xml">a hybrid approach for named entity and subtype tagging </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>since statistical techniques such as hmm are only as good as the data they are trained on, they are required to use back-off models to compensate for unreliable statistics.
</prevsent>
<prevsent>in contrast empirical back-off models used in hmms, maxent provides systematic method by which statistical model consistent with all obtained knowledge can be trained.
</prevsent>
</prevsection>
<citsent citstr=" M98-1018 ">
\[borthwick et al 1998\] <papid> M98-1018 </papid>discuss atechnique for combining the output of several ne taggers in black box fashion by using maxent.</citsent>
<aftsection>
<nextsent>they demonstrate the superior performance of this system; however, the system is computationally inefficient since many taggers need to be run.
</nextsent>
<nextsent>in this paper we propose ahybrid method for ne tagging which combines all the modelling techniques mentioned above.
</nextsent>
<nextsent>ne tagging is complex task and high-performance systems are required in order to be practically usable.
</nextsent>
<nextsent>furthermore, the task demonstrates characteristics that can be exploited by all three techniques.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A656">
<title id=" A00-1034.xml">a hybrid approach for named entity and subtype tagging </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the experimental results and conclusion are presented finally.
</prevsent>
<prevsent>1 fst-based pattern matching rules for.
</prevsent>
</prevsection>
<citsent citstr=" J97-2003 ">
tex tract ne the most attractive feature of the fst (finite state transducer) formalism lies in its superior time and space efficiency \[mohri 1997\] \[<papid> J97-2003 </papid>roche &amp; schabes 1997\].</citsent>
<aftsection>
<nextsent>applying deterministic fst depends linearly only on the input size of the text.
</nextsent>
<nextsent>our experiments also show that an fst rule system is extraordinarily robust.
</nextsent>
<nextsent>in addition, it has been verified by many research programs \[krupka &amp; hausman 1998\] \[<papid> M98-1015 </papid>hobbs 1993\] \[silberztein 1998\] \[srihari 1998\] \[li &amp; srihari 2000\], that fst is also convenient tool for capturing linguistic phenomena, especially for idioms and semi-productive expressions like time nes and numerical nes.</nextsent>
<nextsent>the rules which we have currently implemented include grammar for temporal expressions (time, date, duration, frequency, age, etc.), grammar for numerical expressions (money, percentage, length, weight, etc.), and grammar for other non-muc nes (e.g. contact information like address, email).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A657">
<title id=" A00-1034.xml">a hybrid approach for named entity and subtype tagging </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>applying deterministic fst depends linearly only on the input size of the text.
</prevsent>
<prevsent>our experiments also show that an fst rule system is extraordinarily robust.
</prevsent>
</prevsection>
<citsent citstr=" M98-1015 ">
in addition, it has been verified by many research programs \[krupka &amp; hausman 1998\] \[<papid> M98-1015 </papid>hobbs 1993\] \[silberztein 1998\] \[srihari 1998\] \[li &amp; srihari 2000\], that fst is also convenient tool for capturing linguistic phenomena, especially for idioms and semi-productive expressions like time nes and numerical nes.</citsent>
<aftsection>
<nextsent>the rules which we have currently implemented include grammar for temporal expressions (time, date, duration, frequency, age, etc.), grammar for numerical expressions (money, percentage, length, weight, etc.), and grammar for other non-muc nes (e.g. contact information like address, email).
</nextsent>
<nextsent>the following sample pattern rules give an idea of what our ne grammars look like.
</nextsent>
<nextsent>these rules capture typical us addresses, like: 5500 main st., williams ville, ny14221; 12345 xyz avenue, apt.
</nextsent>
<nextsent>678, los angeles, ca98765-4321.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A660">
<title id=" A00-1034.xml">a hybrid approach for named entity and subtype tagging </title>
<section> improving ne segmentation through.  </section>
<citcontext>
<prevsection>
<prevsent>the rules of preposition disambiguation are responsible for determination of boundaries involving prepositions ( of ,  and ,   , etc.).
</prevsent>
<prevsent>for example, for the sequence  of , we have the following rule: and have same tags if the lowercase of and both occur in oxfd dictionary.
</prevsent>
</prevsection>
<citsent citstr=" P99-1021 ">
a  global word sequence checking  \[mikheev, 1999\] <papid> P99-1021 </papid>is also employed.</citsent>
<aftsection>
<nextsent>for the sequence  sprint and mci , we search the document globally.
</nextsent>
<nextsent>if the word  sprint  or 251  mci  occurs individually somewhere lse, we mark  and  as common word.
</nextsent>
<nextsent>the rules of spurious capitalized word disambiguation are designed to recognize the first word in the sentence.
</nextsent>
<nextsent>if the first word is unknown in the training corpus, but occurs in oxfd as common word in lowercase, hhm unknown word model may be not accurate enough.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A661">
<title id=" C00-2125.xml">modelling speech repairs in german and mandarin chinese spoken dialogues </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>computational models in the form of finite state automata (fsa) also illustrate the describable regularity of german and mandarin chinese speech repairs informal way.
</prevsent>
<prevsent>spontaneous speech analysis has recently been playing crucial role in providing empirical evidence for applications in both theoretical nd applied fields of computational linguistics.
</prevsent>
</prevsection>
<citsent citstr=" J99-4003 ">
for the purpose of constructing more salient and robust dialogue systems, recent analyses on speech repairs, or more generally speaking, on speech disfluencies in spoken dialogues have tried to explore the distributional characteristics of irregular sequences in order to develop annotation systems to cope with speech repairs (heeman and allen 1999, <papid> J99-4003 </papid>nakatani and hirschberg 1994).</citsent>
<aftsection>
<nextsent>this new research direction, nevertheless, has until recently merely focused on the surface structure of speech repairs on the one hand.
</nextsent>
<nextsent>on the other hand, except for very few ilwestigations tarting to deal with speech repairs across several languages (eklund and shribcrg 1998), most of the studies on speech repairs have investigated only single languages.
</nextsent>
<nextsent>in addition, studies have shown that syntactic and prosodic features of spontaneous speech data provide empirical evidence with regard to reflecting the speaking habits of speakers, and also help to develop better parsing strategies and natural language processing systems (heeman and allen 1999, <papid> J99-4003 </papid>hindle 1983).<papid> P83-1019 </papid></nextsent>
<nextsent>these systems should understand and react the language use of human users (lickley and bard 1998, tseng 1998).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A665">
<title id=" C00-2125.xml">modelling speech repairs in german and mandarin chinese spoken dialogues </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this new research direction, nevertheless, has until recently merely focused on the surface structure of speech repairs on the one hand.
</prevsent>
<prevsent>on the other hand, except for very few ilwestigations tarting to deal with speech repairs across several languages (eklund and shribcrg 1998), most of the studies on speech repairs have investigated only single languages.
</prevsent>
</prevsection>
<citsent citstr=" P83-1019 ">
in addition, studies have shown that syntactic and prosodic features of spontaneous speech data provide empirical evidence with regard to reflecting the speaking habits of speakers, and also help to develop better parsing strategies and natural language processing systems (heeman and allen 1999, <papid> J99-4003 </papid>hindle 1983).<papid> P83-1019 </papid></citsent>
<aftsection>
<nextsent>these systems should understand and react the language use of human users (lickley and bard 1998, tseng 1998).
</nextsent>
<nextsent>this paper presents results of comparative stud), of speech repairs with the goal of examining and modelling repair syntax by looking into empirical cross-linguistic spccch data.
</nextsent>
<nextsent>in this paper, the phenomena of speech repairs are introduced first, followed by an empirical cross-linguistic analysis of speech repairs in german and mandarin chinese, which have different language typologies.
</nextsent>
<nextsent>speech data, therefore, were collected to look for linguistic sequences and particularities of spontaneous speech, which usually cause difficulties for language dialogue systems.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A667">
<title id=" A00-2038.xml">a new algorithm for the alignment of phonetic sequences </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>depending on the appli-cation, either of the results, or both, can be used.
</prevsent>
<prevsent>within the last few years, several different ap-proaches to phonetic alignment have been reported.
</prevsent>
</prevsection>
<citsent citstr=" J96-4002 ">
covington (1996) <papid> J96-4002 </papid>used depth-first earch and spe-cial distance function to align words for histori-cal comparison.</citsent>
<aftsection>
<nextsent>in follow-up paper (covington, 1998), <papid> P98-1043 </papid>he extended the algorithm to align words from more than two languages.</nextsent>
<nextsent>somers (1998) <papid> P98-2200 </papid>pro-posed special algorithm for aligning children ar-ticulation data with the adult model.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A670">
<title id=" A00-2038.xml">a new algorithm for the alignment of phonetic sequences </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>within the last few years, several different ap-proaches to phonetic alignment have been reported.
</prevsent>
<prevsent>covington (1996) <papid> J96-4002 </papid>used depth-first earch and spe-cial distance function to align words for histori-cal comparison.</prevsent>
</prevsection>
<citsent citstr=" P98-1043 ">
in follow-up paper (covington, 1998), <papid> P98-1043 </papid>he extended the algorithm to align words from more than two languages.</citsent>
<aftsection>
<nextsent>somers (1998) <papid> P98-2200 </papid>pro-posed special algorithm for aligning children ar-ticulation data with the adult model.</nextsent>
<nextsent>gildea and ju-rafsky (1996) applied the dp algorithm to pre-align input and output phonetic strings in order to im- prove the performance oftheir transducer induction system.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A671">
<title id=" A00-2038.xml">a new algorithm for the alignment of phonetic sequences </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>covington (1996) <papid> J96-4002 </papid>used depth-first earch and spe-cial distance function to align words for histori-cal comparison.</prevsent>
<prevsent>in follow-up paper (covington, 1998), <papid> P98-1043 </papid>he extended the algorithm to align words from more than two languages.</prevsent>
</prevsection>
<citsent citstr=" P98-2200 ">
somers (1998) <papid> P98-2200 </papid>pro-posed special algorithm for aligning children ar-ticulation data with the adult model.</citsent>
<aftsection>
<nextsent>gildea and ju-rafsky (1996) applied the dp algorithm to pre-align input and output phonetic strings in order to im- prove the performance oftheir transducer induction system.
</nextsent>
<nextsent>nerbonne and heeringa (1997) <papid> W97-1102 </papid>employed similar procedure to compute relative distance be-tween words from various dutch dialects.</nextsent>
<nextsent>some characteristics of these implementations are juxta-posed in table 1.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A673">
<title id=" A00-2038.xml">a new algorithm for the alignment of phonetic sequences </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>somers (1998) <papid> P98-2200 </papid>pro-posed special algorithm for aligning children ar-ticulation data with the adult model.</prevsent>
<prevsent>gildea and ju-rafsky (1996) applied the dp algorithm to pre-align input and output phonetic strings in order to im- prove the performance oftheir transducer induction system.</prevsent>
</prevsection>
<citsent citstr=" W97-1102 ">
nerbonne and heeringa (1997) <papid> W97-1102 </papid>employed similar procedure to compute relative distance be-tween words from various dutch dialects.</citsent>
<aftsection>
<nextsent>some characteristics of these implementations are juxta-posed in table 1.
</nextsent>
<nextsent>in this paper, present new algorithm for the alignment of cognates.
</nextsent>
<nextsent>it combines various tech-niques developed for sequence comparison with an appropriate scoring scheme for computing phonetic similarity on the basis of multivalued features.
</nextsent>
<nextsent>the new algorithm performs better, in terms of accuracy and efficiency, than comparable algorithms reported by covington (1996) <papid> J96-4002 </papid>and somers (1999).<papid> J99-2005 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A682">
<title id=" A00-2038.xml">a new algorithm for the alignment of phonetic sequences </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this paper, present new algorithm for the alignment of cognates.
</prevsent>
<prevsent>it combines various tech-niques developed for sequence comparison with an appropriate scoring scheme for computing phonetic similarity on the basis of multivalued features.
</prevsent>
</prevsection>
<citsent citstr=" J99-2005 ">
the new algorithm performs better, in terms of accuracy and efficiency, than comparable algorithms reported by covington (1996) <papid> J96-4002 </papid>and somers (1999).<papid> J99-2005 </papid></citsent>
<aftsection>
<nextsent>although the main focus of this paper is dia chronic phonol- ogy, the techniques proposed here can also be ap-plied in other contexts where it is necessary to align phonetic sequences.
</nextsent>
<nextsent>to align phonetic sequences, we first need func-tion for calculating the distance between individual phones.
</nextsent>
<nextsent>the numerical value assigned by the func-tion to pair of segments referred to as the cost, or penalty, of substitution.
</nextsent>
<nextsent>the function is often ex-tended to cover pairs consisting of segment and the null character, which correspond to the opera- 288 algorithm calculation calculation dynamic phonological of alignment of distance programming features covington (1996) <papid> J96-4002 </papid>somers (1998) <papid> P98-2200 </papid>nerbonne and heeringa (1997) <papid> W97-1102 </papid>gildea and jurafsky (1996) <papid> J96-4003 </papid>explicit explicit implicit explicit table 1: comparison of lions of insertion and deletion (also called indels).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A694">
<title id=" A00-2038.xml">a new algorithm for the alignment of phonetic sequences </title>
<section> comparing phones.  </section>
<citcontext>
<prevsection>
<prevsent>to align phonetic sequences, we first need func-tion for calculating the distance between individual phones.
</prevsent>
<prevsent>the numerical value assigned by the func-tion to pair of segments referred to as the cost, or penalty, of substitution.
</prevsent>
</prevsection>
<citsent citstr=" J96-4003 ">
the function is often ex-tended to cover pairs consisting of segment and the null character, which correspond to the opera- 288 algorithm calculation calculation dynamic phonological of alignment of distance programming features covington (1996) <papid> J96-4002 </papid>somers (1998) <papid> P98-2200 </papid>nerbonne and heeringa (1997) <papid> W97-1102 </papid>gildea and jurafsky (1996) <papid> J96-4003 </papid>explicit explicit implicit explicit table 1: comparison of lions of insertion and deletion (also called indels).</citsent>
<aftsection>
<nextsent>a distance function that satisfies the following ax-ioms is called metric: 1.
</nextsent>
<nextsent>va, : d(a, b)  _ 0 (nonnegative property).
</nextsent>
<nextsent>2.
</nextsent>
<nextsent>va, : d(a,b) = 0 ?~ = (zero property).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A739">
<title id=" A00-2039.xml">finite state reduplication in one level prosodic morphology </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>finally, the implementation a complex case from koasati presented.
</prevsent>
<prevsent>in the past two decades computational morphology has been quite successful in dealing with the chal-lenges posed by natural anguage word patterns.
</prevsent>
</prevsection>
<citsent citstr=" C96-1017 ">
using finite-state methods, it has been possible to describe both word formation and the concomi-tant phonological modifications in many languages, ranging from straightforward concatenative combi-nation (koskenniemi, 1983) over semitic-style non- concatenative intercalation (beesley (1996), <papid> C96-1017 </papid>kiraz (1994)) <papid> C94-1029 </papid>to circumfixional long-distance dependen-cies (beesley, 1998).<papid> W98-1312 </papid></citsent>
<aftsection>
<nextsent>however, sproat (1992) observes that, despite the existence of  working systems that are capa-ble of doing great deal of morphological naly- sis ,  there are still outstanding problems and ar-eas which have not received much serious attention  (ibid., 123).
</nextsent>
<nextsent>problem areas in his view include sub- tractive morphology, in fixation, the proper inclu-sion of prosodic structure and, in particular, redu- plication:  from computational point of view, one point cannot be overstressed: the copying required in reduplication places reduplication a class apart from all other morphology.
</nextsent>
<nextsent>(ibid., 60).
</nextsent>
<nextsent>productive reduplication is so troublesome for formal account based on regular languages (or regular elations) because unbounded total instances like indonesian noun plural (orang-orang  men ) are isomorphic to the copy language ww, which is context-sensitive.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A740">
<title id=" A00-2039.xml">finite state reduplication in one level prosodic morphology </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>finally, the implementation a complex case from koasati presented.
</prevsent>
<prevsent>in the past two decades computational morphology has been quite successful in dealing with the chal-lenges posed by natural anguage word patterns.
</prevsent>
</prevsection>
<citsent citstr=" C94-1029 ">
using finite-state methods, it has been possible to describe both word formation and the concomi-tant phonological modifications in many languages, ranging from straightforward concatenative combi-nation (koskenniemi, 1983) over semitic-style non- concatenative intercalation (beesley (1996), <papid> C96-1017 </papid>kiraz (1994)) <papid> C94-1029 </papid>to circumfixional long-distance dependen-cies (beesley, 1998).<papid> W98-1312 </papid></citsent>
<aftsection>
<nextsent>however, sproat (1992) observes that, despite the existence of  working systems that are capa-ble of doing great deal of morphological naly- sis ,  there are still outstanding problems and ar-eas which have not received much serious attention  (ibid., 123).
</nextsent>
<nextsent>problem areas in his view include sub- tractive morphology, in fixation, the proper inclu-sion of prosodic structure and, in particular, redu- plication:  from computational point of view, one point cannot be overstressed: the copying required in reduplication places reduplication a class apart from all other morphology.
</nextsent>
<nextsent>(ibid., 60).
</nextsent>
<nextsent>productive reduplication is so troublesome for formal account based on regular languages (or regular elations) because unbounded total instances like indonesian noun plural (orang-orang  men ) are isomorphic to the copy language ww, which is context-sensitive.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A741">
<title id=" A00-2039.xml">finite state reduplication in one level prosodic morphology </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>finally, the implementation a complex case from koasati presented.
</prevsent>
<prevsent>in the past two decades computational morphology has been quite successful in dealing with the chal-lenges posed by natural anguage word patterns.
</prevsent>
</prevsection>
<citsent citstr=" W98-1312 ">
using finite-state methods, it has been possible to describe both word formation and the concomi-tant phonological modifications in many languages, ranging from straightforward concatenative combi-nation (koskenniemi, 1983) over semitic-style non- concatenative intercalation (beesley (1996), <papid> C96-1017 </papid>kiraz (1994)) <papid> C94-1029 </papid>to circumfixional long-distance dependen-cies (beesley, 1998).<papid> W98-1312 </papid></citsent>
<aftsection>
<nextsent>however, sproat (1992) observes that, despite the existence of  working systems that are capa-ble of doing great deal of morphological naly- sis ,  there are still outstanding problems and ar-eas which have not received much serious attention  (ibid., 123).
</nextsent>
<nextsent>problem areas in his view include sub- tractive morphology, in fixation, the proper inclu-sion of prosodic structure and, in particular, redu- plication:  from computational point of view, one point cannot be overstressed: the copying required in reduplication places reduplication a class apart from all other morphology.
</nextsent>
<nextsent>(ibid., 60).
</nextsent>
<nextsent>productive reduplication is so troublesome for formal account based on regular languages (or regular elations) because unbounded total instances like indonesian noun plural (orang-orang  men ) are isomorphic to the copy language ww, which is context-sensitive.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A742">
<title id=" A00-2039.xml">finite state reduplication in one level prosodic morphology </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>productive reduplication is so troublesome for formal account based on regular languages (or regular elations) because unbounded total instances like indonesian noun plural (orang-orang  men ) are isomorphic to the copy language ww, which is context-sensitive.
</prevsent>
<prevsent>in the rest of this paper will lay out proposal for handling reduplication with finite-state methods.
</prevsent>
</prevsection>
<citsent citstr=" J94-1003 ">
as starting point, adopt bird &amp; ellison (1994) <papid> J94-1003 </papid>one-level phonology, monostratal constraint- based framework where phonological representa-tions, morphemes and generalizations are all finite- state automata (fsas) and constraint combination is accomplished via automata intersection.</citsent>
<aftsection>
<nextsent>while it is possible to transfer much of the present pro-posal to the transducer-based setting that is often preferred nowadays, the monostratal pproach still offers an attractive alternative due to its easy blend with monostratal grammars such as hpsg and the good prospects for machine learning of its surface- true constraints (ellison (1992), belz (1998)).<papid> P98-2240 </papid></nextsent>
<nextsent>after brief survey of important kinds of redupli-cation in 2, section 3 explains the necessary ex-tensions of one-level phonology to deal with the challenges presented by reduplication, within the larger domain of prosodic morphology in general.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A743">
<title id=" A00-2039.xml">finite state reduplication in one level prosodic morphology </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in the rest of this paper will lay out proposal for handling reduplication with finite-state methods.
</prevsent>
<prevsent>as starting point, adopt bird &amp; ellison (1994) <papid> J94-1003 </papid>one-level phonology, monostratal constraint- based framework where phonological representa-tions, morphemes and generalizations are all finite- state automata (fsas) and constraint combination is accomplished via automata intersection.</prevsent>
</prevsection>
<citsent citstr=" P98-2240 ">
while it is possible to transfer much of the present pro-posal to the transducer-based setting that is often preferred nowadays, the monostratal pproach still offers an attractive alternative due to its easy blend with monostratal grammars such as hpsg and the good prospects for machine learning of its surface- true constraints (ellison (1992), belz (1998)).<papid> P98-2240 </papid></citsent>
<aftsection>
<nextsent>after brief survey of important kinds of redupli-cation in 2, section 3 explains the necessary ex-tensions of one-level phonology to deal with the challenges presented by reduplication, within the larger domain of prosodic morphology in general.
</nextsent>
<nextsent>a worked-out example from koasati in 4 illus-trates the interplay of the various components in an implemented analysis, before some conclusions are drawn in section 5.
</nextsent>
<nextsent>a well-known case from the context-sensitivity debate of the eighties is the n-o-n re duplicative construction from bambara (northwestern mande, (culy, 1985)): (1) a. wulu-o-wulu  whichever dog  b. wulunyinina-o-wulunyinina  whichever dog searcher  c. wulunyininafil~la-o-wulunyininafil~la  whoever watches dog searchers  beyond total copying, (1) also illustrates the pos-sibility of so-called fixed-melody parts in redupli- 296 cation: constant/o/intervenes tween base (i.e. original) and reduplicant (i.e. copied part, in bold print), the next case from semai expressive minor redu-plication (mon-khmer, hendricks (1998)) high-lights the possibility of an interaction between redu-plication and internal truncation: (2) a. ce:t ct-ce:t  sweet  b. drph dh-drj3h  appearance of nod-ding constantly  c. cfa:l cl-cfa:l  appearance of flick-ering red object  reduplication copies the initial and final segment of the base, skipping all of its interior segments, which may be of arbitrary length.
</nextsent>
<nextsent>a final case comes from koasati punctual-aspect reduplication (muscogean, (kimball, 1988)): (3) a. ta.hfis.pin tlahas-tl 6:-pin  to be light in weight  b. la.pfit.kin llapat-ll6:-kin  to be narrow c. ak.mt.lin alk-hl o-l~itlin  to be loose  d. ok.cak.kon olk-hlo-c~kon  to be green or blue  koasati particularly interesting, because it shows that copy and original need not always be adjacent - here the reduplicant is infixed into its own base and also because it illustrates that the copy may be phonologically modified: the/h/ in the copied part of (3).c,d is best analysed as voiceless vowel, i.e. the phonetic ally closest consonantal expression of its source.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A744">
<title id=" A00-2039.xml">finite state reduplication in one level prosodic morphology </title>
<section> finite-state methods.  </section>
<citcontext>
<prevsection>
<prevsent>as consequence, skip and repeat will be visible part of the output in word form generation and must be allowed in the input for parsing as well.
</prevsent>
<prevsent>taken together, the three enrichments yield an automaton for bambara wulu, shown in figure 1.a. while skipping is not necessary for this example, 4 ~ 4 is: it will host the fixed-melody/o/.
</prevsent>
</prevsection>
<citsent citstr=" J94-3001 ">
the 2this can be seen as an application fthe ignore operator of kaplan and kay (1994), <papid> J94-3001 </papid>where e* is being ignored.</citsent>
<aftsection>
<nextsent>297 repeat arcs will of course facilitate copying, as we shall see in moment.
</nextsent>
<nextsent>a. ig i; i; i; i; : repeat seg:o figure 1: enriched automata for wulu (a.), bambara n-o-n reduplication (b.) 3.2 copying as intersection.
</nextsent>
<nextsent>bird &amp; ellison (1992) came close to discovering useful device for reduplication when they noted that automaton intersection has at least indexed- grammar power (ibid., p.48).
</nextsent>
<nextsent>they demonstrated their claim by showing that odd-length strings of indefinite length like the one described by the regular expression (a bcde g)+ can be repeated by intersecting them with an automaton accept-ing only strings of even length: the result is (abede gabede g) +.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A745">
<title id=" C00-1042.xml">statistical morphological disambiguation for agglutinative languages </title>
<section> related  work.  </section>
<citcontext>
<prevsection>
<prevsent>(1992)).
</prevsent>
<prevsent>in the rule-based approach, large mmfl e.r of hand-craft;ed linguisiic constraints are used to elinfinate impossible tags or morpho-logical t)arse.s tbr given word in given context (ka.rlsson et al, 1995).
</prevsent>
</prevsection>
<citsent citstr=" W95-0101 ">
brill (1995<papid> W95-0101 </papid>a) has presented transfl)rnmtioi based lea.rning at)l)roach, whi(:h in-duces disanlbiguation rules from tagged corpora.</citsent>
<aftsection>
<nextsent>morphologi(:al disanlbiguation inflecting or ag- glutinative languages with colnl)lex morphology in-volves more than determining the major or minor imrts-of-sl)cech of the lexiea.l items.
</nextsent>
<nextsent>typically, roof phology marks mlmber of inflectional or deriva- tioiml features and this involves ambiguity.
</nextsent>
<nextsent>for in-stance, given word nlay be chopl)ed up in difl erent ways into mort)heroes , given mort)heine may inark different features depending on the morpho tactics, or lexicalized variants of derived words may interact with productively derived versions (see ottazer and tiir (1997) for the difl erent kinds of morphological ambiguities in turkish.)
</nextsent>
<nextsent>we assume that all syn-tactically relevant fcat urcs of word forms have to be determined correctly for morphological disambigua.- tion.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A746">
<title id=" C00-1042.xml">statistical morphological disambiguation for agglutinative languages </title>
<section> related  work.  </section>
<citcontext>
<prevsection>
<prevsent>used the resulting infornlation in 285 morphological disambiguation in hebrew.
</prevsent>
<prevsent>haji~: and hla(lk~i (1998) have used ntaximunl entropy modeling approach for morphological dismnbigua- tion in czech.
</prevsent>
</prevsection>
<citsent citstr=" P98-1063 ">
ezeiza et al (1998) <papid> P98-1063 </papid>have combined stochastic and rule-based isambiguation methods for basque.</citsent>
<aftsection>
<nextsent>megyesi (19991 has adapted brill pos tagger with extended lexical templates to itungar- tan.
</nextsent>
<nextsent>previous ai)proaches to morphological dismnbi- guation of turkish text; had employed constraint- based approach (otlazer and kurusz, 1994; oflazer and tiir, 1996; oflazer and tiir, 1997).
</nextsent>
<nextsent>although results obtained earlier in these at)preaches were rea-sonable, the fact that tim constraint rules were handcrafted posed rather serious impediment the generality and improvement of these systems.
</nextsent>
<nextsent>turkish is flee constituent order language.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A748">
<title id=" C00-1042.xml">statistical morphological disambiguation for agglutinative languages </title>
<section> turk sh.  </section>
<citcontext>
<prevsection>
<prevsent>for instance in the exami)le above, we would use everything including the root; form as the morphosyntactic tag.
</prevsent>
<prevsent>in order to alleviate the data sparseness probleln we break down the flfll tags.
</prevsent>
</prevsection>
<citsent citstr=" P99-1033 ">
we represent each word as sequence of inflectional groups (igs hereafter), separated by  dbs denoting derivation boundaries, as described by oflazer (1999).<papid> P99-1033 </papid></citsent>
<aftsection>
<nextsent>thus morphologi-cal parse would be represented in the following gen-eral tbrm: tthe morphological features other than the poss are: +become: become verb, +cans: causative verb, +pos: positive polarity, +inf: marker that derives an infinitive form fl om verb, +aasg: 3sg number-person agreement, +pnon: no pos-sessive agreement, m~d +nora: nominative case.
</nextsent>
<nextsent> db mark derivational boundaries.
</nextsent>
<nextsent>286 full ~\[hgs (no roots) hfltectional groui)s possil)le oo 9,129 o1)served 10,531 2,194 %fl)le 3: numbers of q2tgs and igs root+igi ~db+ig2 ~db+- - -^db+ig.
</nextsent>
<nextsent>where igi denotes relevant inflectional fea ;urcs f the inflectional groul)s, including the 1)art-ofsl)eech for the root or any of the derived forms.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A752">
<title id=" A97-1028.xml">a statistical profile of the named entity task </title>
<section> the  named entity task.  </section>
<citcontext>
<prevsection>
<prevsent>there is currently much interest, in both research and com-mercial arenas, in natural anguage processing systems which can perform multilingual in format ion extract ion (ie), the task of automatically identifying the various aspects of text that are of interest specific users.
</prevsent>
<prevsent>an example of ie is the named ent ty (ne) task, which has become established as the important first step in many other ie tasks, provid-ing information useful for coreference and template filling.
</prevsent>
</prevsection>
<citsent citstr=" M95-1002 ">
named entity evaluation began as part of recent message understanding conferences (muc), whose objective was to standardize the evaluation of ie tasks (sundheim, 1995<papid> M95-1002 </papid>b).</citsent>
<aftsection>
<nextsent>several organized evaluations have been held to determine the state-of-the-art in ne systems, and there are commercial systems available.
</nextsent>
<nextsent>the goal of the ne task is to automatically identify the boundaries of variety of phrases in raw text, and then to categorize the phrases identified.
</nextsent>
<nextsent>there are three categories of named-entities defined by the guidelines: timex, numex, and enamex.
</nextsent>
<nextsent>timex phrases are temporal expressions, which are subdivided into date expressions (april 7) and time expressions (noon est).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A757">
<title id=" C00-1029.xml">a class based probabilistic approach to structural disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the level at which (chicken) is represented is cruciah it should be high enough for adequate counts to have accumulated, but not too high so that the hypernym is no longer representative of (chicken}.
</prevsent>
<prevsent>an exanlp le of hypernym whidl would be too high is (e t i ty ) , as not all entities are semantically similar with respect the object position ot7 cat.
</prevsent>
</prevsection>
<citsent citstr=" J98-2002 ">
the problem of choosing an appropria.te vel in the h.ierarchy at which to represent par-ticular noun sense (given predicate and argu-ment position) has been investigated by resnik (1993), li and abe (1998) <papid> J98-2002 </papid>and ll,iba,s (1995).</citsent>
<aftsection>
<nextsent>the learning mechanism presented \]lore is novel approach based on tinding semantically similar sets of concepts in hierarchy.
</nextsent>
<nextsent>we demonstrate the effectiveness of our approach using pp-attachment experiment.
</nextsent>


</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A758">
<title id=" C00-1036.xml">xml and multilingual document authoring convergent trends </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this conception, xml authoring has strong connection to the enterprise of multilingual document authoring in which the author is guided in the specilication of the document content, and where the system is responsible 4there are autlmring situations in which it may be necessary for the user to introduce new selllalllic labels eorlesl)onding lo expres-sive needs not foreseen by lhe creator of the original i)td.
</prevsent>
<prevsent>to handle such situations, it is useflfl to view the l)ti) as open-ended objecls 1o which new semantic labels and types can be added at authoring time.
</prevsent>
</prevsection>
<citsent citstr=" P98-2173 ">
for generating from this content extual output in several languages imultaneously (see (power and scott, 1998; <papid> P98-2173 </papid>hartley and paris, 1997; coch, 1996)).<papid> C96-1043 </papid></citsent>
<aftsection>
<nextsent>now there are some obvious problems with this view, due to the current limitations of xml tools.
</nextsent>
<nextsent>limitations of xml for multilingual document au-thoring.
</nextsent>
<nextsent>the first, possibly most serious, limitation originates in the fact that standard dtd is severely re-stricted in the semantic dependencies it can express be-tween two subtrces in the document structure.
</nextsent>
<nextsent>thus, if in the description of contact, city of residence is in-cluded, one may want to constrain such an information depending on the country of residence; or, in the air-craft maintenance manual example, one might want to automatically include some warning in case dangerous chemical is mentioned somewhere lse in the document.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A759">
<title id=" C00-1036.xml">xml and multilingual document authoring convergent trends </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this conception, xml authoring has strong connection to the enterprise of multilingual document authoring in which the author is guided in the specilication of the document content, and where the system is responsible 4there are autlmring situations in which it may be necessary for the user to introduce new selllalllic labels eorlesl)onding lo expres-sive needs not foreseen by lhe creator of the original i)td.
</prevsent>
<prevsent>to handle such situations, it is useflfl to view the l)ti) as open-ended objecls 1o which new semantic labels and types can be added at authoring time.
</prevsent>
</prevsection>
<citsent citstr=" C96-1043 ">
for generating from this content extual output in several languages imultaneously (see (power and scott, 1998; <papid> P98-2173 </papid>hartley and paris, 1997; coch, 1996)).<papid> C96-1043 </papid></citsent>
<aftsection>
<nextsent>now there are some obvious problems with this view, due to the current limitations of xml tools.
</nextsent>
<nextsent>limitations of xml for multilingual document au-thoring.
</nextsent>
<nextsent>the first, possibly most serious, limitation originates in the fact that standard dtd is severely re-stricted in the semantic dependencies it can express be-tween two subtrces in the document structure.
</nextsent>
<nextsent>thus, if in the description of contact, city of residence is in-cluded, one may want to constrain such an information depending on the country of residence; or, in the air-craft maintenance manual example, one might want to automatically include some warning in case dangerous chemical is mentioned somewhere lse in the document.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A760">
<title id=" C00-1036.xml">xml and multilingual document authoring convergent trends </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we have just described an approach to solving the limita-tions of usual xml tools for multilingual document au-thoring which originates in the tradition of constructive type-theory and mathematical proof editors.
</prevsent>
<prevsent>we will now sketch an approach strongly inspired by gf but which formally is more in the tradition of logic-programming based unification grammars, and which is currently un-.
</prevsent>
</prevsection>
<citsent citstr=" W00-1404 ">
der development at xerox research centre europe (see (brun et al, 2000) <papid> W00-1404 </papid>for more extended escription of this project).</citsent>
<aftsection>
<nextsent>definite clause grammars, or dcg s, (pereira and warren, 1980), are possibly the simplest unification- based extension of context-free grammars, and have good reversibility properties which make them adapted both to parsing and to generation.
</nextsent>
<nextsent>a typical view of what dcg rule looks like is the following: 5 a(a (b ,c . . .
</nextsent>
<nextsent>)) ---   text  , b(b),  text2 , e(c ) ,  text3 , {constraints (b,c,...)}.
</nextsent>
<nextsent>this rule expresses the fact that (1) some abstract structure l (b, . . .
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A761">
<title id=" A97-1004.xml">a maximum entropy approach to identifying sentence boundaries </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>the task of identifying sentence boundaries in text has not received as much attention as it deserves.
</prevsent>
<prevsent>many freely available natural language processing tools require their input to be divided into sentences, but make no mention of how to accomplish this (e.g.
</prevsent>
</prevsection>
<citsent citstr=" P96-1025 ">
(brill, 1994; collins, 1996)).<papid> P96-1025 </papid></citsent>
<aftsection>
<nextsent>others perform the division implicitly without discussing performance (e.g.
</nextsent>
<nextsent>(cutting et al, 1992)).<papid> A92-1018 </papid></nextsent>
<nextsent>on first glance, it may appear that using short list of sentence-final punctuation marks, such as.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A762">
<title id=" A97-1004.xml">a maximum entropy approach to identifying sentence boundaries </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>(brill, 1994; collins, 1996)).<papid> P96-1025 </papid></prevsent>
<prevsent>others perform the division implicitly without discussing performance (e.g.</prevsent>
</prevsection>
<citsent citstr=" A92-1018 ">
(cutting et al, 1992)).<papid> A92-1018 </papid></citsent>
<aftsection>
<nextsent>on first glance, it may appear that using short list of sentence-final punctuation marks, such as.
</nextsent>
<nextsent>, ?, and /, is sufficient.
</nextsent>
<nextsent>however, these punctua-tion marks are not used exclusively to mark sen-tence breaks.
</nextsent>
<nextsent>for example, embedded quotations may contain any of the sentence-ending punctua-tion marks and . is used as decimal point, in e- mail addresses, to indicate ellipsis and in abbrevia-tions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A763">
<title id=" A97-1004.xml">a maximum entropy approach to identifying sentence boundaries </title>
<section> p rev ious  work   </section>
<citcontext>
<prevsection>
<prevsent>all the results we will present for our a.lgorithms are on their initial, larger test.
</prevsent>
<prevsent>corpus.
</prevsent>
</prevsection>
<citsent citstr=" H89-2048 ">
in (riley, 1989), <papid> H89-2048 </papid>riley describes decision-tree based approach to the problem.</citsent>
<aftsection>
<nextsent>his performance on /he brown corpus is 99.8%, using model learned rom corpus of 25 million words.
</nextsent>
<nextsent>liberman and church suggest in (liberlnan and church, 1992) that.
</nextsent>
<nextsent>a system could be quickly built to divide newswire text into sentences with nearly negligible error rate.
</nextsent>
<nextsent>but, do not actually build such system.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A764">
<title id=" A97-1004.xml">a maximum entropy approach to identifying sentence boundaries </title>
<section> max lnum entropy.  </section>
<citcontext>
<prevsection>
<prevsent>the abbreviation list is automatically produced from the training data., and the contextual ques-tions are also automat.ically generated by scanning the training data with question templates.
</prevsent>
<prevsent>as a. re-sult, no hand-crafted rules or lists are required by the highly portable system and it can be easily re-trained for other languages or text genres.
</prevsent>
</prevsection>
<citsent citstr=" W96-0213 ">
the model used here for sentence-boundary de-tection is based on the maximum entropy model used for pos tagging in (ratnaparkhi, 1996).<papid> W96-0213 </papid></citsent>
<aftsection>
<nextsent>for each potential sentence boundary token (., ?, and !), we estimate joint probability distribution of the token and it.s surrounding context, both of which are denoted by c, occurring as an actual sentence i)oundary.
</nextsent>
<nextsent>the (list, ribul.ioll is given by: i,.
</nextsent>
<nextsent>f,(b,c) p(b, ~) = ~ 1-i j=, ,~ ) , ,,,h~,-e ~ ~ {no, y~}, where 2a token in the training data is considered an abbre-viation if it is preceded and followed by whit espace, and it contains . that is not sentence boundary.
</nextsent>
<nextsent>17 the ctj are the unknown parameters of the model, and where each u corresponds to fj, or feature.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A765">
<title id=" A94-1004.xml">modeling content identification from document images </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>instead, we have developed method that first makes generalizations about images of characters, then performs gross classification of the isolated characters and agglomerates these character shape codes into spa-tially isolated (word shape) tokens (nakayama nd spitz, 1993; sibun and spitz, this volume).
</prevsent>
<prevsent>generating word shape tokens is inexpensive, fast, and robust.
</prevsent>
</prevsection>
<citsent citstr=" C94-2108 ">
word shape tokens are potential alternative to charac-ter coded words when they are used for language deter-mination and part-of-speech tagging (nakayama nd spitz, 1993; sibun and spitz, this volume; sibun and farrar, 1994).<papid> C94-2108 </papid></citsent>
<aftsection>
<nextsent>in this paper, we describe an extension of our approach to content identification.
</nextsent>
<nextsent>in this section, we introduce word shape tokens and their generation from document images.
</nextsent>
<nextsent>first, we classify characters by determining the characteristics of the textline.
</nextsent>
<nextsent>we identify the positions of the baseline and the x-height as shown in figure 1 (spitz, 1993).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A766">
<title id=" A94-1004.xml">modeling content identification from document images </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>i 0   , , , , , , , , , , , , , , 0 2000 4000 6000 8000 10000 number of words (frequency order) figure 2: ascii and character shape code 3 content identification.
</prevsent>
<prevsent>text characterization an important domain for natural language processing.
</prevsent>
</prevsection>
<citsent citstr=" A92-1025 ">
many published techniques utilize word frequencies of text for information retrieval and text categorization (jacobs, 1992; <papid> A92-1025 </papid>cutting et al, 1993).</citsent>
<aftsection>
<nextsent>we also characterize the content of the document image by finding words that seem to specify the topic of the document.
</nextsent>
<nextsent>briefly, our strategy is to identify the fre-quently occurring word shape tokens.
</nextsent>
<nextsent>in this section, we first describe aprocess of clean-ing the input sequence which precedes the main proce-dures.
</nextsent>
<nextsent>then, we illustrate how to collect the important tokens, introducing stop list of common word shape tokens which is used to remove the tokens that are insufficiently specific to represent the content of the documents.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A767">
<title id=" A94-1041.xml">representing knowledge for planning multi sentential text </title>
<section> goals and general approach.  </section>
<citcontext>
<prevsection>
<prevsent>this implies, on the one hand, good management of the argumentative aspects, and on the other hand, the need to generate rather complex texts in perfectly consistent yle.
</prevsent>
<prevsent>the general approach of the conceptual planning is relation-based, as described in the rhetorical structure theory or rst (mann &amp; thompson, 1988).
</prevsent>
</prevsection>
<citsent citstr=" C90-3018 ">
however, as shown in (elhadad &amp; mckeown, 1990), <papid> C90-3018 </papid>previous works in text generation systems (including rst)  have generally used notion similar to rhetorical relations to describe the connection between propositions.</citsent>
<aftsection>
<nextsent>they make one-to-one mapping from these relations to connectives for generation (for example, the  opposition  relation would be realized by the connective  but ).
</nextsent>
<nextsent>in this approach it is difficult to distinguish between similar connectives (e.g. \[...\] but vs. although).
</nextsent>
<nextsent>in our work, we assume that there are two different levels of text structure.
</nextsent>
<nextsent>the deep level is represented by unordered basic relations, and the surface level is represented by (ordered) lists of atomic events and rhetorical operators.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A768">
<title id=" A97-1016.xml">automatic acquisition of two level morphological rules </title>
<section> introduction.  </section>
<citcontext>
<prevsection>
<prevsent>simons (simons, 1988) describes methods for studying morpho phonemic alternations (using anno-tated inter linear text) and grimes (grimes, 1983) presents program for discovering affix posi-tions and cooccurrence restrictions.
</prevsent>
<prevsent>koskenniemi (koskenniemi, 1990) provides sketch of discovery procedure for phonological two-level rules.
</prevsent>
</prevsection>
<citsent citstr=" C86-1069 ">
golding and thompson (golding and thompson, 1985) and wothke (wothke, 1986) <papid> C86-1069 </papid>present systems to automat- icaily calculate set of word-formation rules.</citsent>
<aftsection>
<nextsent>these rules are, however, ordered one-level rewrite rules and not unordered two-level rules, as in our system.
</nextsent>
<nextsent>kuusik (kuusik, 1996) <papid> C96-2198 </papid>also acquires ordered one- level rewrite rules, for stem sound changes in esto- nian.</nextsent>
<nextsent>daelemans et al (daelemans el al., 1996) use general symbolic machine learning program to ac-quire decision tree for matching dutch nouns to their correct diminutive suffixes.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A769">
<title id=" A97-1016.xml">automatic acquisition of two level morphological rules </title>
<section> introduction.  </section>
<citcontext>
<prevsection>
<prevsent>golding and thompson (golding and thompson, 1985) and wothke (wothke, 1986) <papid> C86-1069 </papid>present systems to automat- icaily calculate set of word-formation rules.</prevsent>
<prevsent>these rules are, however, ordered one-level rewrite rules and not unordered two-level rules, as in our system.</prevsent>
</prevsection>
<citsent citstr=" C96-2198 ">
kuusik (kuusik, 1996) <papid> C96-2198 </papid>also acquires ordered one- level rewrite rules, for stem sound changes in esto- nian.</citsent>
<aftsection>
<nextsent>daelemans et al (daelemans el al., 1996) use general symbolic machine learning program to ac-quire decision tree for matching dutch nouns to their correct diminutive suffixes.
</nextsent>
<nextsent>the input to their process is the syllable structure of the nouns and given set of five suffix allomorphs.
</nextsent>
<nextsent>they do not learn rules for possible sound changes.
</nextsent>
<nextsent>our process au-tomatically acquires the necessary two-level sound changing rules for prefix and suffix allomorphs, as well as the rules for stem sound changes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A770">
<title id=" A97-1016.xml">automatic acquisition of two level morphological rules </title>
<section> introduction.  </section>
<citcontext>
<prevsection>
<prevsent>they do not learn rules for possible sound changes.
</prevsent>
<prevsent>our process au-tomatically acquires the necessary two-level sound changing rules for prefix and suffix allomorphs, as well as the rules for stem sound changes.
</prevsent>
</prevsection>
<citsent citstr=" P94-1039 ">
connec- tionist work on the acquisition of morphology has been more concerned with implementing psycholog-ically motivated models, than with acquisition of rules for practical system ((sproat, 1992, p.216) and (gasser, 1994)).<papid> P94-1039 </papid></citsent>
<aftsection>
<nextsent>the contribution of this paper is to present com-plete method for the automatic acquisition of an op- 103 timal set of two-level rules (i.e. the second com-ponent above) for source-target word pairs.
</nextsent>
<nextsent>it is assumed that the target word is formed from the source through the addition of prefix and/or suffix 1.
</nextsent>
<nextsent>furthermore, we show how partial acqui-sition of the morpho tactic description (component one) results as by-product of the rule-acquisition process.
</nextsent>
<nextsent>for example, the morpho tactic description of the target word in the input pair \[1\] source target happy happier is computed as \[2\] happier = happy + er the right-hand side of this morpho tactic description is then mapped on the left-hand side, \[z\] happy+er happ 0 r for this example the two-level rule \[ 4\] y:i ?~ p:p - can be derived.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A771">
<title id=" A97-1016.xml">automatic acquisition of two level morphological rules </title>
<section> introduction.  </section>
<citcontext>
<prevsection>
<prevsent>the null character (0) may ap-pear as either lexical character (as in +:0) or surface character, but not as both.
</prevsent>
<prevsent>1non-linear operations (such as infixation) are not considered here, since the basic two-level model deals with it in round-about way.
</prevsent>
</prevsection>
<citsent citstr=" P96-1022 ">
we can note that exten-sions to the basic two-level model have been proposed to handle non-linear morphology (kiraz, 1996).<papid> P96-1022 </papid></citsent>
<aftsection>
<nextsent>two-level rules have the following syntax (sproat, 1992, p.145): \[ 6\] cp op lc _ rc ce (correspondence part), lc (le# contezt) and ac (right contez~) are regular expressions over the al-phabet of feasible pairs.
</nextsent>
<nextsent>in most, if not all, imple-mentations based on the two-level model, the corre-spondence part consists of single special pair.
</nextsent>
<nextsent>we also consider only single pair cps in this paper.
</nextsent>
<nextsent>the operator op is one of four types: 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A772">
<title id=" A88-1027.xml">the experience of developing a largescale natural language text processing system critique </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>today, critique is being tested in variety of appli-cations ranging from office correspondence and technical documentation to student essays.
</prevsent>
<prevsent>also, plnlp and peg have been incorporated into several other research applications, such as ma-chine translation systems.
</prevsent>
</prevsection>
<citsent citstr=" P86-1002 ">
at the 1986 a( meeting, gary hendrix de-scribed his experience in developing natural language interface for real users (hendrix, 1986).<papid> P86-1002 </papid></citsent>
<aftsection>
<nextsent>in contrast with user interface systems, we con-sider critique to be text processing system.
</nextsent>
<nextsent>the latter may be distinguished from the former by its broad coverage of texts that were prepared independently to communicate ideas and not strictly to interact with computer system.
</nextsent>
<nextsent>until now, the experience of developing large-scale natural language text processing system has not been discussed in the literature.
</nextsent>
<nextsent>this paper first describes the overall process-ing in the critique system.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A773">
<title id=" C00-2142.xml">rapid development of translation tools application to persian and turkish </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>meat is publicly available nvironmeut hat as-sists linguist in rapidly developing machine trans-lation system, in order to keep the overhead in-volved in learning and using the system as low as possible, the linguist uses use simple yet powerful ba-sic data and control structures.
</prevsent>
<prevsent>these structures are oriented towards contemporary linguistic and con  putationm linguistic theories.
</prevsent>
</prevsection>
<citsent citstr=" J92-2002 ">
in meat, linguistic knowledge is entirely repre-sented using typed feature structures (tfs) (car- penter, 1992; zajac, 1992), <papid> J92-2002 </papid>the most widely used rep-resentational formalism today.</citsent>
<aftsection>
<nextsent>we developed fast implementation of typed feature structm:es with appropriateness, based on an abstract machine view (eft carpenter and qu (1995), wintner and francez (1995).
</nextsent>
<nextsent>bilingual dictionary entries as well as all kinds of rules (morphology, syntax, transfer, gener- ation) are expressed as feature structures of specific types, so only one description language has to be mastered.
</nextsent>
<nextsent>this usually leads to rapid familiarity with the system, yielding high productivity almost from the start.
</nextsent>
<nextsent>i ht tp ://crl.nmsu.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A774">
<title id=" C00-2142.xml">rapid development of translation tools application to persian and turkish </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>$headword oj lyb hftglnh $category noun $number plural $regular false $english seven wonder nc plur ; $$ figure a persian--english dictionary entry.
</prevsent>
<prevsent>while this is already enough information to facil irate basic word-.for-word translation, in general morphological analyzer for the source language is needed to translate real-world text.
</prevsent>
</prevsection>
<citsent citstr=" W98-1310 ">
for meat, one can either import the results of an existing morpho-logical analyzer, or use the native description lan-guage, based on finite-state transducer using char-acters as left projections and typed feature struc-tures as right projections (zajac, 1998).<papid> W98-1310 </papid></citsent>
<aftsection>
<nextsent>after com-pleting the morphological nalysis of the source lan-guage and specifying the mapping of lexical features to english, glossing is available.
</nextsent>
<nextsent>the glosser is an meat application consisting of morphological nal- ysis of source language words, tbllowed by dictionary lookup for single words and compounds, and the translation into english inflected word forms.
</nextsent>
<nextsent>an example of the interface \[br the glosser is shown in.
</nextsent>
<nextsent>ligure 2.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A776">
<title id=" C00-2142.xml">rapid development of translation tools application to persian and turkish </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the main.
</prevsent>
<prevsent>component used is bidirectional island-.parser (cf.
</prevsent>
</prevsection>
<citsent citstr=" C88-2132 ">
stock et al (1988)) <papid> C88-2132 </papid>for unification-based gra.mmars.</citsent>
<aftsection>
<nextsent>the grammar ules are usually writtet~ in the style of context-free rules with \ [~ssoc ia ted unification constraints as shown i~ figure 4.
</nextsent>
<nextsent>the rules allow for the specification of the right-hand side as regular-.expression of feature structures.
</nextsent>
<nextsent>we plan to add more restricted types of grammars (e.g. based on finite-.state transducers) to give the linguist richer choice of syntactic processes to choose from.
</nextsent>
<nextsent>for the time being, the transfer capabilities of the system are restricted to lexical transdr, as we have not finished the implementation a complex trans-fer module.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A779">
<title id=" A00-2022.xml">ambiguity packing in constraint based parsing practical results </title>
<section> background </section>
<citcontext>
<prevsection>
<prevsent>162 tional subsumption test for typed feature structures, which we use in bottom-up, chart-based parsing algorithm incorporating novel, efficient accounting mechanisms to guarantee minimal chart size (sec- tion 3).
</prevsent>
<prevsent>we present full-scale evaluation of the techniques on large corpus (section 4), and com-plete the picture with an empirically-based discus-sion of grammar estrictors and parsing strategies (section 5).
</prevsent>
</prevsection>
<citsent citstr=" P91-1041 ">
equ iva lence lgor thms our feature structure subsumption algorithm 2 as-sumes totally well-typed structures (carpenter, 1992) and employs similar machinery to the quasi-destructive unification algorithm described by tomabechi (1991).<papid> P91-1041 </papid></citsent>
<aftsection>
<nextsent>in particular, it uses temporary pointers in dag nodes, each pointer tagged with generation counter, to keep track of intermediate results in processing; incrementing the generation counter invalidates all temporary pointers in sin-gle operation.
</nextsent>
<nextsent>but whereas quasi-destructive unifi-cation makes two passes (determining whether the unification will be successful and then copying out the intermediate representation) the subsumption algorithm makes only one pass, checking reentran- cies and type-supertype lationships at the same time.
</nextsent>
<nextsent>3 the algorithm, shown in figure 1, also si-multaneously tests if both feature structures ub- sume each other (i.e. they are equivalent), if either subsumes the other, or if there is no subsumption relation between them in either direction.
</nextsent>
<nextsent>the top-level entry point dag-subsumes-po and subsidiary function dag-subsumes-po 0 each return two values, held in variables \]orwardp and back- wardp, both initially true, recording whether it is possible that the first dag subsumes the second and/or vice-versa, respectively.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A780">
<title id=" A00-2022.xml">ambiguity packing in constraint based parsing practical results </title>
<section> background </section>
<citcontext>
<prevsection>
<prevsent>4 empi i ca resu t s. we have carried out an evaluation of the algo-rithms presented above using the lingo grammar (flickinger &amp; sag, 1998), publicly-available, multi-purpose, broad-coverage hpsg of english developed at csli stanford.
</prevsent>
<prevsent>with roughly 8,000 types, an av-erage feature structure size of around 300 nodes, and 64 lexical and grammar rules (fleshing out the inter-action of hpsg id schemata, wellformedness prin-ciples, and lp constraints), lingo is among the largest hpsg grammars available.
</prevsent>
</prevsection>
<citsent citstr=" A92-1012 ">
we used the lkb system (copestake, 1992, <papid> A92-1012 </papid>1999) as an experimen-tation platform since it provides parameterisable bottom-up chart parser and precise, fine-grained profiling facilities (oepen &amp; flickinger, 1998).</citsent>
<aftsection>
<nextsent>7 all of our results were obtained in this environment, running on 300 mhz ultra sparc, and using bal-anced test set of 2,100 sentences extracted from verb mobil corpora of transcribed speech: input lengths from 1 to 20 words are represented with 100 test items each; although sentences in the corpus range up to 36 words in length there are relatively few longer than 20 words.
</nextsent>
<nextsent>the category of eland its decomposition(s) in daughter edges (and corresponding subtrees) be used again, to multiply out and project local ambiguity.
</nextsent>
<nextsent>; the lingo grammar and lkb software are publicly avail-able at  t tp : / / i ngo . stanford, edu/ .
</nextsent>
<nextsent>20000 17500 15000 12500 10000 7500 5000 2500 o~ pro- and retroactive packing \] passive edges \] ,, .kit ataatmt llttvtd~l~ll , ~ w w w w v ; w ~  } 1 3 1 5 1 7 2 1 2 3 25 string length (in words) on the total chart size (truncated above 25 words).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A781">
<title id=" C00-2164.xml">perception concepts and language road and ipage </title>
<section> i t roduct ion   </section>
<citcontext>
<prevsection>


</prevsection>
<citsent citstr=" W94-0319 ">
following (reiter, 1.994), <papid> W94-0319 </papid>in the ma,jority of systems language generation starts with content deter mina tion.</citsent>
<aftsection>
<nextsent>this step is fllrther subdivided in deep con-tent dcl.cr m, inatio,n where decision takes i)lat-e what infornmtion shouhl 1)e (:ommmficated to the lmarej  mm 7q,,eto rical anning where this ilfl ormation is or- galfized in rhetorically coher(mt rammer.
</nextsent>
<nextsent>however, the definition of deet) content deternfination allows two difl ering interpretations: one perspective is that there already exists set of representations of dif-ferent contents in specific representation tbrmat.
</nextsent>
<nextsent>the task is to select one of these representations to be commmficated.
</nextsent>
<nextsent>this process is tyl)icalty imple-mented using discourse structure information and other criteria (e.g. grice maximes, (grice, 1975)).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A782">
<title id=" A97-1008.xml">an evaluation of strategies for selective utterance verification for spoken natural language dialog </title>
<section> u: it is not there.  </section>
<citcontext>
<prevsection>
<prevsent>utterance 6: to determine whether  okay  de-notes confirmation or comprehension (i.e., con-firmation that the wire has been obtained).
</prevsent>
<prevsent>effective use of expectation is necessary for con-straining the search for interpretations and achieving efficient processing of nl inputs.
</prevsent>
</prevsection>
<citsent citstr=" P89-1016 ">
this is particularly crucial in spoken nl dimog, where speakers expect fast response times (oviatt and cohen, 1989).<papid> P89-1016 </papid></citsent>
<aftsection>
<nextsent>the system model of expectations similar to that of (young et al, 1989) in that we predict the meanings of possible user responses based on the cur-rent dialog goal.
</nextsent>
<nextsent>the details of the system model can be found in (smith and hipp, 1994).
</nextsent>
<nextsent>here we re-view the key aspects that are exploited in context- dependent strategy for verification.
</nextsent>
<nextsent>we define ex-pectations based on an abstract representation of the current task goal.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A783">
<title id=" A97-1008.xml">an evaluation of strategies for selective utterance verification for spoken natural language dialog </title>
<section> parse cost/context combination.  </section>
<citcontext>
<prevsection>
<prevsent>in conclusion, while useful, there appear to be limits to the effectiveness of verification subdi- alogs.
</prevsent>
<prevsent>consequently, strategies for delayed etection and resolution of miscommunication (e.g.
</prevsent>
</prevsection>
<citsent citstr=" J95-4001 ">
(mcroy and hirst, 1995), (<papid> J95-4001 </papid>brennan and hulteen, 1995), and (lambert and carberry, 1992)) <papid> P92-1025 </papid>become nec-essary and remain an area of continued invest iga tion.</citsent>
<aftsection>
<nextsent>these include both computer-initiated as well as user-initiated strategies.
</nextsent>
<nextsent>5 acknowledgments.
</nextsent>
<nextsent>the author expresses his appreciation to d. richard hipp for his work on the error-correcting parser and for his initial work on context-independent verifica-tion.
</nextsent>
<nextsent>the author also wishes to express his thanks to steven a. gordon and robert d. hoggard for their suggestions concerning this work and an earlier draft of this paper.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A784">
<title id=" A97-1008.xml">an evaluation of strategies for selective utterance verification for spoken natural language dialog </title>
<section> parse cost/context combination.  </section>
<citcontext>
<prevsection>
<prevsent>in conclusion, while useful, there appear to be limits to the effectiveness of verification subdi- alogs.
</prevsent>
<prevsent>consequently, strategies for delayed etection and resolution of miscommunication (e.g.
</prevsent>
</prevsection>
<citsent citstr=" P92-1025 ">
(mcroy and hirst, 1995), (<papid> J95-4001 </papid>brennan and hulteen, 1995), and (lambert and carberry, 1992)) <papid> P92-1025 </papid>become nec-essary and remain an area of continued invest iga tion.</citsent>
<aftsection>
<nextsent>these include both computer-initiated as well as user-initiated strategies.
</nextsent>
<nextsent>5 acknowledgments.
</nextsent>
<nextsent>the author expresses his appreciation to d. richard hipp for his work on the error-correcting parser and for his initial work on context-independent verifica-tion.
</nextsent>
<nextsent>the author also wishes to express his thanks to steven a. gordon and robert d. hoggard for their suggestions concerning this work and an earlier draft of this paper.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A785">
<title id=" A92-1015.xml">detecting and correcting morphosyntactic errors in real texts </title>
<section> vp(num) --)verb(num trans) np( acc).  </section>
<citcontext>
<prevsection>
<prevsent>it should not fail whenever the instan- tiation of variable fails or constantin the left- hand side of the rule being reduced oes not match the corresponding value on the stack, but mark the incongruity and continue parsing instead.
</prevsent>
<prevsent>later in the process, when the parsing has finished, the syn-tactic corrector checks the marks for incongruity and coerces agreement by feature propagation.
</prevsent>
</prevsection>
<citsent citstr=" C88-2127 ">
this approach contrasts with, e.g., the approach taken by (schwind, 1988), <papid> C88-2127 </papid>who proposes to devise an error rule (cf.</citsent>
<aftsection>
<nextsent>section 4.3.3) for every unification error of interest.
</nextsent>
<nextsent>however, this makes efficient pars-ing with large grammar nearly impossible since the size of the parsing table is exponentially related to the number of rules.
</nextsent>
<nextsent>4.3.2.
</nextsent>
<nextsent>syntactic filtering consider the error in the yelow cab stops.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A786">
<title id=" C00-2113.xml">an empirical method for identifying and translating technical terminology </title>
<section> i t roduct ion   </section>
<citcontext>
<prevsection>
<prevsent>another drawback of this a.pproach is tha.t it requires substa.ntim training corpora, in many cases with pa.rt-of-speech tags.
</prevsent>
<prevsent>an.
</prevsent>
</prevsection>
<citsent citstr=" J93-1007 ">
alternative approa.ch is a. statistical one which retrieves recurrent word sequences as co\]loca.tiolls (sma.dja., 1993)(<papid> J93-1007 </papid>ha.runo et a.1., 1996)(shimolla.ta et a.1., :1997).</citsent>
<aftsection>
<nextsent>this a.pproach is robust and pra.ctical because it uses t)lain text corpora, without a.ny inibrmation depen-dent on la.ngua.ge.
</nextsent>
<nextsent>unlike the former n)- proa.ch, this a.pproach extra.cts va.rious types of local pa.tterns a.t the same time.
</nextsent>
<nextsent>therefore, post-processing, such as part of speech ta.gging and syntactic category identifica.tion, is neces-sary when we a.pply them to nlp applica.tions.
</nextsent>
<nextsent>this pa.per presents a. method for identify-ing techn icm terms froni a. corpus and a.pl)ly- ing them to a. ma.chine tra.nsla.tion system.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A787">
<title id=" C00-2113.xml">an empirical method for identifying and translating technical terminology </title>
<section> pattern -based  mt system.  </section>
<citcontext>
<prevsection>
<prevsent>the follow-ing section explains how th.e proposed method works in detail.
</prevsent>
<prevsent>we th.en present experimenta.l results a.nd conclude with discussion.
</prevsent>
</prevsection>
<citsent citstr=" C90-3001 ">
h pattern-ha.seal mt system uses set of bilin- gua.1 pa.tterns(cfg rules) (abeille et a.l., 1990) (<papid> C90-3001 </papid>ta.keda., 1.996) (<papid> P96-1020 </papid>shimohata.</citsent>
<aftsection>
<nextsent>et a.l., 1.999).
</nextsent>
<nextsent>in the pa.rsing process, the engine performs a. cfg- parsing for a.n input sentence and rewrites trees by a.pplying the source pa.tterns.
</nextsent>
<nextsent>3 erminals and non-terminals are processed under the sa.me fra.lnework but lexicalized pa.tterns ha.re priority over symbolized pa.tterns 1 plausible parse we define symbolized pattern as pattern with- out a. terminal and ~l lexicalizcd pattern as that with more than one terminal, we prepares 1000 symbolized patterns a.nd 130,000 lexicalizcd patterns as system 782 tree will be selected among possible parse trees by the number of l)atterns applied.
</nextsent>
<nextsent>then the pa.rse tree is tr~msferred into target language by using target patterns which correspond to the source patterns.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A788">
<title id=" C00-2113.xml">an empirical method for identifying and translating technical terminology </title>
<section> pattern -based  mt system.  </section>
<citcontext>
<prevsection>
<prevsent>the follow-ing section explains how th.e proposed method works in detail.
</prevsent>
<prevsent>we th.en present experimenta.l results a.nd conclude with discussion.
</prevsent>
</prevsection>
<citsent citstr=" P96-1020 ">
h pattern-ha.seal mt system uses set of bilin- gua.1 pa.tterns(cfg rules) (abeille et a.l., 1990) (<papid> C90-3001 </papid>ta.keda., 1.996) (<papid> P96-1020 </papid>shimohata.</citsent>
<aftsection>
<nextsent>et a.l., 1.999).
</nextsent>
<nextsent>in the pa.rsing process, the engine performs a. cfg- parsing for a.n input sentence and rewrites trees by a.pplying the source pa.tterns.
</nextsent>
<nextsent>3 erminals and non-terminals are processed under the sa.me fra.lnework but lexicalized pa.tterns ha.re priority over symbolized pa.tterns 1 plausible parse we define symbolized pattern as pattern with- out a. terminal and ~l lexicalizcd pattern as that with more than one terminal, we prepares 1000 symbolized patterns a.nd 130,000 lexicalizcd patterns as system 782 tree will be selected among possible parse trees by the number of l)atterns applied.
</nextsent>
<nextsent>then the pa.rse tree is tr~msferred into target language by using target patterns which correspond to the source patterns.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A789">
<title id=" C00-2113.xml">an empirical method for identifying and translating technical terminology </title>
<section> pattern -based  mt system.  </section>
<citcontext>
<prevsection>
<prevsent>in the remainder of this section, we will explain each phase in detail with english and japanese xamples.
</prevsent>
<prevsent>dictiona.ry.
</prevsent>
</prevsection>
<citsent citstr=" P97-1061 ">
we have ah eady proposed method for retriev-ing word sequences (shimohata et al, 1997).<papid> P97-1061 </papid></citsent>
<aftsection>
<nextsent>this method generates all n-character (or n- word) strings appearing in text and tilters out ffagl-nenta.1 strings with the distribution of words adjacent to the strings.
</nextsent>
<nextsent>this is based on the idea.
</nextsent>
<nextsent>that adjacent words are widely dis-tributed if the string is meaningful, m~d are lo-calized if the string is substring of meaning-ful string.
</nextsent>
<nextsent>the method introduces entropy value to mea-sure the word distribution.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A790">
<title id=" C00-2121.xml">automatic extraction of semantic relations from specialized corpora </title>
<section> motivation </section>
<citcontext>
<prevsection>
<prevsent>specifically: 1.
</prevsent>
<prevsent>state-of-the-art language modeling techniques.
</prevsent>
</prevsection>
<citsent citstr=" J96-2003 ">
(mcmahon and smith., 1996) <papid> J96-2003 </papid>require lexical intbrmation about word classes.</citsent>
<aftsection>
<nextsent>in any domain and language with minimal dependence on specialized tools and resources is very important.
</nextsent>
<nextsent>most thematic domains today in most of the languages lack semantic resources.
</nextsent>
<nextsent>adopting knowledge-poor corpus- based method not only much less labor is necessary in construction of conceptual structures but also domain-dependent semantic relations are obtained.
</nextsent>
<nextsent>new resources can be readily created in new domains or existing thesauri can be enlarged or refined by re-training on larger corpora as soon as they become available.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A791">
<title id=" C00-2121.xml">automatic extraction of semantic relations from specialized corpora </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>three main approaches have been proposed for the automatic extraction of lexical semantics knowledge: syntax-based, n-gram-based and window-based.
</prevsent>
<prevsent>syntax-based methods (referred also as knowledge-rich in contrast to the others knowledge-poor methods) (pereira and thishby, 1992; grefenstette, 1993; li and abe, 1997) represent the words under consideration as vectors containing statistic values of their syntactic properties in relation to given set of words (e.g. statistics of object syntax relations referring to set of verbs) and cluster the considered words according to similarity of the corresponding vectors.
</prevsent>
</prevsection>
<citsent citstr=" J92-4003 ">
methods that use bigrams (brown et al, 1992) <papid> J92-4003 </papid>or trigrams (martin et al, 1998) cluster words considering as word context he one or two immediately adjacent words and employ as clustering criteria the minimal loss of average 836 nmtual information and the perplexity improvement respectively.</citsent>
<aftsection>
<nextsent>such methods are oriented to language modeling and aim primarily at rough but fast clustering of large vocabularies.
</nextsent>
<nextsent>brown et al (1992) <papid> J92-4003 </papid>also proposed window method introducing the concept of  semantic stickiness  of two words as the relatively frequent close occurrence between them (less than 500 words distance).</nextsent>
<nextsent>although this is an efficient and entirely knowledge-poor method tbr extracting both semantic relations and clusters, the extracted relations are not restricted to semantic similarity but extend on thematic roles.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A794">
<title id=" C00-2121.xml">automatic extraction of semantic relations from specialized corpora </title>
<section> preprocessing.  </section>
<citcontext>
<prevsection>
<prevsent>c.frequently appearing lexical patterns which represent single semantic entities in the specific domain are treated as single (albeit composite)  semantic token .
</prevsent>
<prevsent>their detection is based on the following algorithm (cf.
</prevsent>
</prevsection>
<citsent citstr=" J93-1007 ">
smadja, 1993): <papid> J93-1007 </papid>1.</citsent>
<aftsection>
<nextsent>extract  significant bigrams  confined inside.
</nextsent>
<nextsent>noun phrases i.e. immediately adjacent words that contain relatively high amount of nmtual information: .........
</nextsent>
<nextsent>; (w. ,w2)= log 2 p(w  w2) (10) p(w )p(w 2 ) 2.
</nextsent>
<nextsent>combine significant bigrams together to.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A795">
<title id=" A00-1011.xml">rees a largescale relation and event extraction system </title>
<section> system architecture and components.  </section>
<citcontext>
<prevsection>
<prevsent>77 remplateroot / /v -   : .   . . .
</prevsent>
<prevsent>gui interaction ? figure 3: the rees system architecture the nptagger then takes the xml-tagged output of the name tagger through two phases.
</prevsent>
</prevsection>
<citsent citstr=" W95-0107 ">
first, it recognizes non-recursive base noun phrase (bnp) (our specifications for bnp resemble those in ramshaw and marcus 1995).<papid> W95-0107 </papid></citsent>
<aftsection>
<nextsent>second, it recognizes complex nps for only the four main semantic types of nps, i.e., person, organization, location, and artifact (vehicle, drug and weapon).
</nextsent>
<nextsent>it makes post- modifier attachment decisions only for those nps that are crucial to the extraction at hand.
</nextsent>
<nextsent>during this second phase, relations which can be recognized locally (e.g., age, affiliation, maker) are also recognized and stored using the xml attributes for the nps.
</nextsent>
<nextsent>for instance, the xml tag for  president of xyz corp.  below holds an affiliation attribute with the id for  xyz corp.   pnp id= 03  affiliation= o4  president of  entity id= 04  xyz corp. /entity   /pnp  building upon the xml output of the nptagger, the event tagger ecognizes events applying its lexicon-driven, syntactically-based generic patterns.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A796">
<title id=" A00-1011.xml">rees a largescale relation and event extraction system </title>
<section> system architecture and components.  </section>
<citcontext>
<prevsection>
<prevsent>no new patterns are required.
</prevsent>
<prevsent>moreover, this approach allows for easy customization capability: person with no knowledge of the pattern language would be able to configure the system to extract new events.
</prevsent>
</prevsection>
<citsent citstr=" M95-1019 ">
while the tagging component is similar to other pattern-based ie systems (e.g., appelt et al. 1995; <papid> M95-1019 </papid>aone et al 1998, <papid> M98-1012 </papid>yangarber and grishman 1998), <papid> M98-1011 </papid>our event tagger is more portable through lexicon-driven approach.</citsent>
<aftsection>
<nextsent>2.2 co-reference resolution.
</nextsent>
<nextsent>after the tagging phase, rees sends the xml output through rule-based co-reference resolution module that resolves: ? definite noun phrases of organization, person, and location types, and ? singular person pronouns: he and she.
</nextsent>
<nextsent>only  high-precision  rules are currently applied to selected types of anaphora.
</nextsent>
<nextsent>that is, we resolve only those cases of anaphora whose antecedents the module can identify with high confidence.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A797">
<title id=" A00-1011.xml">rees a largescale relation and event extraction system </title>
<section> system architecture and components.  </section>
<citcontext>
<prevsection>
<prevsent>no new patterns are required.
</prevsent>
<prevsent>moreover, this approach allows for easy customization capability: person with no knowledge of the pattern language would be able to configure the system to extract new events.
</prevsent>
</prevsection>
<citsent citstr=" M98-1012 ">
while the tagging component is similar to other pattern-based ie systems (e.g., appelt et al. 1995; <papid> M95-1019 </papid>aone et al 1998, <papid> M98-1012 </papid>yangarber and grishman 1998), <papid> M98-1011 </papid>our event tagger is more portable through lexicon-driven approach.</citsent>
<aftsection>
<nextsent>2.2 co-reference resolution.
</nextsent>
<nextsent>after the tagging phase, rees sends the xml output through rule-based co-reference resolution module that resolves: ? definite noun phrases of organization, person, and location types, and ? singular person pronouns: he and she.
</nextsent>
<nextsent>only  high-precision  rules are currently applied to selected types of anaphora.
</nextsent>
<nextsent>that is, we resolve only those cases of anaphora whose antecedents the module can identify with high confidence.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A798">
<title id=" A00-1011.xml">rees a largescale relation and event extraction system </title>
<section> system architecture and components.  </section>
<citcontext>
<prevsection>
<prevsent>no new patterns are required.
</prevsent>
<prevsent>moreover, this approach allows for easy customization capability: person with no knowledge of the pattern language would be able to configure the system to extract new events.
</prevsent>
</prevsection>
<citsent citstr=" M98-1011 ">
while the tagging component is similar to other pattern-based ie systems (e.g., appelt et al. 1995; <papid> M95-1019 </papid>aone et al 1998, <papid> M98-1012 </papid>yangarber and grishman 1998), <papid> M98-1011 </papid>our event tagger is more portable through lexicon-driven approach.</citsent>
<aftsection>
<nextsent>2.2 co-reference resolution.
</nextsent>
<nextsent>after the tagging phase, rees sends the xml output through rule-based co-reference resolution module that resolves: ? definite noun phrases of organization, person, and location types, and ? singular person pronouns: he and she.
</nextsent>
<nextsent>only  high-precision  rules are currently applied to selected types of anaphora.
</nextsent>
<nextsent>that is, we resolve only those cases of anaphora whose antecedents the module can identify with high confidence.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A799">
<title id=" C00-2118.xml">automatic lexical acquisition based on statistical distributions </title>
<section> i t roduct ion   </section>
<citcontext>
<prevsection>
<prevsent>\])eta.ileal hfforma.tion a.i)out verbs is critical to a. broad ra.nge of ni,i ) and i 1;asks, yet; ils mau us.1 (lel;( rmina.tion for la.rge numl)ers o\[  verl)s is difficult aml resource intensive.
</prevsent>
<prevsent>i{.esea.rch o,i tim a.ul,()matic, a(-quisil;ion o\[  verb-i)ased k,owl(~(ig(, has succeded in gleaning sylh.a.( l;ic l)rol)erties o\[  verl)s such as sul)ca.tegoriza.tion frames from el  line resources (i}rent, \]9!)3; lh iscoe a.nd c,a.rroll, 1997; \])err, 1997; ma.nning, \]993), ll,ecently, researchers have investigaed statistica.l corpus- ba.sed methods for lexica.l sema.ntic classitica.tion from synta.ctic prol)erties of verl) usage (aone a.nd mckee, \]996; l,a.pa.ta, and brew, \]999; schulte im wa.lde, :1998; stevenson a.nd merle.
</prevsent>
</prevsection>
<citsent citstr=" A00-2034 ">
1999; steve.n- sonet l., \] 999; mccarthy, 2000).<papid> A00-2034 </papid></citsent>
<aftsection>
<nextsent>c, orl)us-based al)pro~mhes to lexica.l sema.ntic classitic~tion in pa,rticular ha.ve dra.wn on levin hypothesis (i,evin, 1993) that verbs can be classi- lied according to the dia.thesis ajterna.tions (a.lter- nations in the syntactic expressions o\[  a.rguments) ill which they f)articil)a.te or exa.mple, whether a. * this research was partly sponsored 1)y us nsi  grants #9702331 and #9818322, swiss nsi  mlowshlp 8210- d65(;9, information sciences (.ollll(il of hurters university and illcs, u. of peimsylwmia.
</nextsent>
<nextsent> his research was con-ducted wldle the tirst author was at lutgers university.
</nextsent>
<nextsent>paola merle lat\ ] , - \])cpa.rtment of l,inguiscics univcrsil;y of ( leneva 2 rue de ~mdolle 121\] gent;re -.
</nextsent>
<nextsent>suisse merzolet res , unge, ch verb occurs in the dative/prepositiona.l phrase al- terna.tion in ;nglish.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A800">
<title id=" C00-2118.xml">automatic lexical acquisition based on statistical distributions </title>
<section> the  argument  t ruc tures.  </section>
<citcontext>
<prevsection>
<prevsent>817 passive or active use (pass), in past participle or simple past use (vbn), in causative or non- causative use (tags), and with an animate subject or not (anim), as described below.
</prevsent>
<prevsent>the first three counts (trans, ass~ vbn) were performed on the ldc 65-million word tagged acl/dci cor-pus (brown, and wall street journal 1987-1989).
</prevsent>
</prevsection>
<citsent citstr=" P97-1003 ">
the last two counts (caus and anim) were per-formed on 29-million word parsed corpus (\gall street journal 1988, provided by michael collins (collins, 1997)).<papid> P97-1003 </papid></citsent>
<aftsection>
<nextsent>the features were counted as follows: taans: the closest noun following verb was considered potential object.
</nextsent>
<nextsent>a verb immedi-ately \[bllowed by potential object was counted as transitive, otherwise as intransitive.
</nextsent>
<nextsent>pass: token tagged vbd (the tag for simple past) was counted as active.
</nextsent>
<nextsent>a token tagged vbn (the tag for past participle) was counted as active if the closest preceding auxiliary was have, and as passive if the closest preceding auxiliary was be.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A801">
<title id=" A97-2003.xml">automating nl appointment scheduling with cosma </title>
<section> the  systems.  </section>
<citcontext>
<prevsection>
<prevsent>agent systems are thus hooked up to e-mail, to calendar manager and to the dialogue server.
</prevsent>
<prevsent>the server interface is command-driven.
</prevsent>
</prevsection>
<citsent citstr=" A97-1006 ">
a client may connect to the server and open up dialogue (see figure 1 in (busemann et al , 1997)).<papid> A97-1006 </papid></citsent>
<aftsection>
<nextsent>dur-ing the dialogue, the client may request texts to be analyzed or semantic descriptions to be verbalized.
</nextsent>
<nextsent>when given text, the server returns the semantic representation, and vice versa.
</nextsent>
<nextsent>the client ensures that the server has available to it linguistically rel-evant information about the interlocutors, such as names, sexes etc. the user agents may access the dialogue server via internet.
</nextsent>
<nextsent>they use the server as their nl front end to human participants.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A802">
<title id=" A97-2003.xml">automating nl appointment scheduling with cosma </title>
<section> the  systems.  </section>
<citcontext>
<prevsection>
<prevsent>more precisely, only utterances between human and machine agent are modeled.
</prevsent>
<prevsent>the agent system used is further development of the pasha system (schmeier and schu peta, 1996).
</prevsent>
</prevsection>
<citsent citstr=" A97-1031 ">
nl analysis in the server is based on shallow parsing strategy implemented in the smes system (neumann et al , 1997).<papid> A97-1031 </papid></citsent>
<aftsection>
<nextsent>the use of smes in cosma, semantic analysis and inference, the dia-logue model mapping between human and machine dialogue structures, utterance generation, the archi-tectural framework of the server, and the pasha agent system are described in (busemann et al , 1997).<papid> A97-1006 </papid></nextsent>
<nextsent>both papers can be found in the anlp  97 conference proceedings.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A807">
<title id=" C00-1031.xml">an empirical investigation of the relation between discourse structure and coreference </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we compare the potential of two classes el  linear and hi-erarchical models of discourse to determine co-reference links and resolve anaphors.
</prevsent>
<prevsent>the comparison uses co pus of thirty texts, which were manually annotated for co-reference and discourse structure.
</prevsent>
</prevsection>
<citsent citstr=" W97-1303 ">
most current anaphora resolution systems implelnent pipeline architecture with three modules (lappin and le- ass, 1994; mitkov, 1997; <papid> W97-1303 </papid>kameyama, 1997).<papid> W97-1307 </papid></citsent>
<aftsection>
<nextsent>1.
</nextsent>
<nextsent>a collf.ct module determines list of potential antecedents (lpa) for each anaphor (l~ronourl, deli- nile noun, proper name, etc.) that have the potential to resolve it.
</nextsent>
<nextsent>with the anaphor fi om the lpa.
</nextsent>
<nextsent>antecedent on the basis of an ordering policy.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A809">
<title id=" C00-1031.xml">an empirical investigation of the relation between discourse structure and coreference </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we compare the potential of two classes el  linear and hi-erarchical models of discourse to determine co-reference links and resolve anaphors.
</prevsent>
<prevsent>the comparison uses co pus of thirty texts, which were manually annotated for co-reference and discourse structure.
</prevsent>
</prevsection>
<citsent citstr=" W97-1307 ">
most current anaphora resolution systems implelnent pipeline architecture with three modules (lappin and le- ass, 1994; mitkov, 1997; <papid> W97-1303 </papid>kameyama, 1997).<papid> W97-1307 </papid></citsent>
<aftsection>
<nextsent>1.
</nextsent>
<nextsent>a collf.ct module determines list of potential antecedents (lpa) for each anaphor (l~ronourl, deli- nile noun, proper name, etc.) that have the potential to resolve it.
</nextsent>
<nextsent>with the anaphor fi om the lpa.
</nextsent>
<nextsent>antecedent on the basis of an ordering policy.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A814">
<title id=" C00-1031.xml">an empirical investigation of the relation between discourse structure and coreference </title>
<section> a filti~,p, module eliminates referees incompatible  </section>
<citcontext>
<prevsection>
<prevsent>with the anaphor fi om the lpa.
</prevsent>
<prevsent>antecedent on the basis of an ordering policy.
</prevsent>
</prevsection>
<citsent citstr=" W98-1119 ">
in most cases, the collect module determines an lpa by enumerating all antecedents in window o1  text that precedes the anaphor under scrutiny (hobbs, 1978; lap- pin and leass, 1994; mitkov, 1997; <papid> W97-1303 </papid>kameyama, 1997; <papid> W97-1307 </papid>ge et al, 1998).<papid> W98-1119 </papid></citsent>
<aftsection>
<nextsent>this window can be as small as two or three sentences or as large as the entire preceding text.
</nextsent>
<nextsent>the filtep, module usually imposes emantic on- strain ts by requiring that the anaphor and potential an-tecedents have the same number and gendm; that selec-tional restrictions are obeyed, etc. the preference module imposes preferences on potential antecedents on the basis of their grammatical roles, parallelism, fi equency, proximity, etc. in some cases, anaphora resolution systems implement hese modules explic-itly (hobbs, 1978; lappin and leass, 1994; <papid> J94-4002 </papid>mitkov, * on leave fi om lhe faculty of computer science, university  ai.</nextsent>
<nextsent>i. cuza  of lasi.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A815">
<title id=" C00-1031.xml">an empirical investigation of the relation between discourse structure and coreference </title>
<section> a filti~,p, module eliminates referees incompatible  </section>
<citcontext>
<prevsection>
<prevsent>in most cases, the collect module determines an lpa by enumerating all antecedents in window o1  text that precedes the anaphor under scrutiny (hobbs, 1978; lap- pin and leass, 1994; mitkov, 1997; <papid> W97-1303 </papid>kameyama, 1997; <papid> W97-1307 </papid>ge et al, 1998).<papid> W98-1119 </papid></prevsent>
<prevsent>this window can be as small as two or three sentences or as large as the entire preceding text.</prevsent>
</prevsection>
<citsent citstr=" J94-4002 ">
the filtep, module usually imposes emantic on- strain ts by requiring that the anaphor and potential an-tecedents have the same number and gendm; that selec-tional restrictions are obeyed, etc. the preference module imposes preferences on potential antecedents on the basis of their grammatical roles, parallelism, fi equency, proximity, etc. in some cases, anaphora resolution systems implement hese modules explic-itly (hobbs, 1978; lappin and leass, 1994; <papid> J94-4002 </papid>mitkov, * on leave fi om lhe faculty of computer science, university  ai.</citsent>
<aftsection>
<nextsent>i. cuza  of lasi.
</nextsent>
<nextsent>1997; kameyama, 1997).<papid> W97-1307 </papid></nextsent>
<nextsent>in other cases, these modules are integrated by means of statistical (ge et al, 1998) <papid> W98-1119 </papid>or uncertainty reasoning teclmiques (mitkov, 1997).<papid> W97-1303 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A821">
<title id=" C00-1031.xml">an empirical investigation of the relation between discourse structure and coreference </title>
<section> a filti~,p, module eliminates referees incompatible  </section>
<citcontext>
<prevsection>
<prevsent>1997; kameyama, 1997).<papid> W97-1307 </papid></prevsent>
<prevsent>in other cases, these modules are integrated by means of statistical (ge et al, 1998) <papid> W98-1119 </papid>or uncertainty reasoning teclmiques (mitkov, 1997).<papid> W97-1303 </papid></prevsent>
</prevsection>
<citsent citstr=" J81-4001 ">
the fact that current anaphora resolution systems rely exclusively on the linear nature of texts in order to de-termine the lpa of an anaphor seems odd, given flint several studies have claimed that there is strong rela-tion between discourse structure and reference (sidner, 1981 ; <papid> J81-4001 </papid>grosz and sidner, 1986; <papid> J86-3001 </papid>grosz et al, 1995; <papid> J95-2003 </papid>fox, 1987; vonk ct al., 1992; azzam el al., 1998; hitzcman and pocsio, 1998).</citsent>
<aftsection>
<nextsent>these studies claim, on the one hand, that the use of referents in naturally occurring texts im-poses constraints on the interpretation discourse; and, on the other, that the structure of discourse constrains the lpas to which anaphors can be resolved.
</nextsent>
<nextsent>the oddness of the situation can be explained by lho fac!
</nextsent>
<nextsent>that both groups seem primafilcie be right.
</nextsent>
<nextsent>empirical exper-iments studies that employ linear techniques for deter-mining the lpas o1  almphol report recall and precision anaphora resolution results in the range of 80% (lappin and leass, 1994; <papid> J94-4002 </papid>ge ct al., 1998).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A822">
<title id=" C00-1031.xml">an empirical investigation of the relation between discourse structure and coreference </title>
<section> a filti~,p, module eliminates referees incompatible  </section>
<citcontext>
<prevsection>
<prevsent>1997; kameyama, 1997).<papid> W97-1307 </papid></prevsent>
<prevsent>in other cases, these modules are integrated by means of statistical (ge et al, 1998) <papid> W98-1119 </papid>or uncertainty reasoning teclmiques (mitkov, 1997).<papid> W97-1303 </papid></prevsent>
</prevsection>
<citsent citstr=" J86-3001 ">
the fact that current anaphora resolution systems rely exclusively on the linear nature of texts in order to de-termine the lpa of an anaphor seems odd, given flint several studies have claimed that there is strong rela-tion between discourse structure and reference (sidner, 1981 ; <papid> J81-4001 </papid>grosz and sidner, 1986; <papid> J86-3001 </papid>grosz et al, 1995; <papid> J95-2003 </papid>fox, 1987; vonk ct al., 1992; azzam el al., 1998; hitzcman and pocsio, 1998).</citsent>
<aftsection>
<nextsent>these studies claim, on the one hand, that the use of referents in naturally occurring texts im-poses constraints on the interpretation discourse; and, on the other, that the structure of discourse constrains the lpas to which anaphors can be resolved.
</nextsent>
<nextsent>the oddness of the situation can be explained by lho fac!
</nextsent>
<nextsent>that both groups seem primafilcie be right.
</nextsent>
<nextsent>empirical exper-iments studies that employ linear techniques for deter-mining the lpas o1  almphol report recall and precision anaphora resolution results in the range of 80% (lappin and leass, 1994; <papid> J94-4002 </papid>ge ct al., 1998).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A823">
<title id=" C00-1031.xml">an empirical investigation of the relation between discourse structure and coreference </title>
<section> a filti~,p, module eliminates referees incompatible  </section>
<citcontext>
<prevsection>
<prevsent>1997; kameyama, 1997).<papid> W97-1307 </papid></prevsent>
<prevsent>in other cases, these modules are integrated by means of statistical (ge et al, 1998) <papid> W98-1119 </papid>or uncertainty reasoning teclmiques (mitkov, 1997).<papid> W97-1303 </papid></prevsent>
</prevsection>
<citsent citstr=" J95-2003 ">
the fact that current anaphora resolution systems rely exclusively on the linear nature of texts in order to de-termine the lpa of an anaphor seems odd, given flint several studies have claimed that there is strong rela-tion between discourse structure and reference (sidner, 1981 ; <papid> J81-4001 </papid>grosz and sidner, 1986; <papid> J86-3001 </papid>grosz et al, 1995; <papid> J95-2003 </papid>fox, 1987; vonk ct al., 1992; azzam el al., 1998; hitzcman and pocsio, 1998).</citsent>
<aftsection>
<nextsent>these studies claim, on the one hand, that the use of referents in naturally occurring texts im-poses constraints on the interpretation discourse; and, on the other, that the structure of discourse constrains the lpas to which anaphors can be resolved.
</nextsent>
<nextsent>the oddness of the situation can be explained by lho fac!
</nextsent>
<nextsent>that both groups seem primafilcie be right.
</nextsent>
<nextsent>empirical exper-iments studies that employ linear techniques for deter-mining the lpas o1  almphol report recall and precision anaphora resolution results in the range of 80% (lappin and leass, 1994; <papid> J94-4002 </papid>ge ct al., 1998).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A827">
<title id=" C00-1031.xml">an empirical investigation of the relation between discourse structure and coreference </title>
<section> a filti~,p, module eliminates referees incompatible  </section>
<citcontext>
<prevsection>
<prevsent>l) iscourse-v1:k models.
</prevsent>
<prevsent>in |his class ()1 models, lpas include all lhe refcrentm expressions fotmd in the dis-course unit under scrutiny and the discourse units that hierarchically precede it.
</prevsent>
</prevsection>
<citsent citstr=" P98-1044 ">
the units that hierarchically precede given unit are determined according to veins theory (vt) (cristea et al, 1998), <papid> P98-1044 </papid>which is described brielly below.</citsent>
<aftsection>
<nextsent>2.2 veins theory.
</nextsent>
<nextsent>vt extends and formalizes the relation between dis-course t ruc ture and reference proposed by fox (1987).
</nextsent>
<nextsent>it identilies  veins , i.e., chains of elementary discourse units, over discourse structure trees that are built accord-ing to the requirements put forth in rhetorical structure theo,y (rst) (mann and thompson, 1988).
</nextsent>
<nextsent>one of the conjectures ()1  vt is that the vein expres-sion of an elementary discourse unit provides coher-ent  abstract  of the discourse fi agmcnt hat contains that unit.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A831">
<title id=" C00-1031.xml">an empirical investigation of the relation between discourse structure and coreference </title>
<section> the experiment.  </section>
<citcontext>
<prevsection>
<prevsent>tile texts were annotated manually for co-reference lations of identity (hirschman and chin- chef, 1997).
</prevsent>
<prevsent>tile co-reference relations define equiv-alence classes oil the set of all marked referents in text.
</prevsent>
</prevsection>
<citsent citstr=" W99-0307 ">
tile texts were also manually annotated by marcu et al (1999) <papid> W99-0307 </papid>with disconrse structures built in the style of mann and thompson (1988).</citsent>
<aftsection>
<nextsent>each discourse analy-sis yielded an average of 52 elementary discourse units.
</nextsent>
<nextsent>see (hirschman and chinchor, 1997) and (marcu et al, 1999) <papid> W99-0307 </papid>for details of tile annotation processes.</nextsent>
<nextsent>210 = 1 9 * v=lg* i=  ...</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A833">
<title id=" A92-1010.xml">integrating natural language components into graphical discourse </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>72
</prevsent>
<prevsent>one body of related work has primarily intended to coordinate different modes of expression within framework of natural communication (cf.
</prevsent>
</prevsection>
<citsent citstr=" E91-1003 ">
hayes, 1987; neal and shapiro, 1988; co-hen et al, 1989, feiner and mckeown, 1990; bandyopadhyay, 1990; tock, 1991; wahlster et al, 1991).<papid> E91-1003 </papid></citsent>
<aftsection>
<nextsent>the principle ffort here is to ascertain factors that can motivate the distribution of information across different modes (e.g. arens and hovy, 1990).
</nextsent>
<nextsent>a further body of related work moves towards prob-lems of interaction by exploring the potential of the combina-tion of natural language and deictic gestures (cf.
</nextsent>
<nextsent>allgayer et al., 1989; moore and swartout, 1990).
</nextsent>
<nextsent>in similar vein, ap-proaches to flexible graphical interaction based on the con- versationalmetaphor (cf.reichman, 1986, 1989; thiel, 1990) treat user inputs uch as mouse clicks, menu selections, etc. not as invocations of methods that can be executed without regard-ing the dialog context, but instead as dialog acts expressing discourse goal of the user.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A834">
<title id=" A92-1010.xml">integrating natural language components into graphical discourse </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in similar vein, ap-proaches to flexible graphical interaction based on the con- versationalmetaphor (cf.reichman, 1986, 1989; thiel, 1990) treat user inputs uch as mouse clicks, menu selections, etc. not as invocations of methods that can be executed without regard-ing the dialog context, but instead as dialog acts expressing discourse goal of the user.
</prevsent>
<prevsent>the direct manipulation fan object then becomes itself part of the dialog of the user with the sys-tem, meaning that the system can respond in more flexible way by taking into account the illocutionary and semantic as-pects of the user input.
</prevsent>
</prevsection>
<citsent citstr=" P86-1016 ">
related work from generation - cludes the correction of misconceptions the work of mccoy (1986) <papid> P86-1016 </papid>and the explicit representation information about system own knowledge and planning activities that is found in the explainable expert systems ystem of swartout and smoliar (1987).</citsent>
<aftsection>
<nextsent>none of this work, however, addresses the problems of meta-dialog concerning graphically supported in-teraction.
</nextsent>
<nextsent>in our approach we bring the kind of natural language ca-pabilities required by the first body of related work (i.e., graph-ical and natural language information functioning together) to bear on the kinds of problems that arise in the second body of related work when the direct manipulation fobjects by user creates goals that the system cannot fulfil.
</nextsent>
<nextsent>here we must not only respond to the user attempt to manipulate an object or the user deictic gesture as dialog act, but also be able to en-gage in meta-interaction debug that act if it creates prob-lematic situation.
</nextsent>
<nextsent>we show that natural language possesses properties that make it preferable over the graphical mode of expression for such meta-interaction d hence natural lan-guage generation needs to be supported even in graphics-ori- ented interfaces.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A835">
<title id=" A92-1010.xml">integrating natural language components into graphical discourse </title>
<section> komet/penman text  generation  system.  </section>
<citcontext>
<prevsection>
<prevsent>for example, concepts from the object-class are usually realized as nominal phrases, while concepts from theprocess-class (e.g., mental-process, verbal- process, action-process, relation-process) are often realized by clauses 2.the relationship between upper model and domain model is diagrammed in the context of its application for sic!
</prevsent>
<prevsent>in figure 3.
</prevsent>
</prevsection>
<citsent citstr=" H89-1022 ">
input to the komet/penman text generation system is given in terms of the sentence plan language (kasper, 1989), <papid> H89-1022 </papid>of which we will see examples below.</citsent>
<aftsection>
<nextsent>an spl expression de-fines the semantic content of sentence tobe generated; it con-sists of set of typed variables and relations defined between those variables.
</nextsent>
<nextsent>both the types and the possible relations are defined either by the upper model directly or by concepts or relations in the domain model that have been subordinated to the upper model.
</nextsent>
<nextsent>in addition to this information, spl expres-sions may also contain direct statements in terms of the gram- roar semantic interface - - in practical applications these latter are often abbreviated by use of macros (e.g. :tense pres- ent) or are defaulted.
</nextsent>
<nextsent>1 the original penman system was developed at the information.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A836">
<title id=" A92-1010.xml">integrating natural language components into graphical discourse </title>
<section> creating the natural language output.  </section>
<citcontext>
<prevsection>
<prevsent>the nucleus is that part that is most essential to the speaker purpose, while the satellite contains additional information.
</prevsent>
<prevsent>the satellite is more easily re-placed than the nucleus because of the nucleus  central role in the thematical progression ofthe discourse.
</prevsent>
</prevsection>
<citsent citstr=" J86-3001 ">
even though there are some critics questioning the use of rhetorical relations in discourse structure theory (grosz and sidner, 1986) <papid> J86-3001 </papid>we use 75 (rl /rst-nonvolitional-result :domain (el/ existence :domain (cl / concept :number mass :process (r2/show :saying cl :speechact denial :tense present))) :range (a/ascription :domain (c2 / capacity :owned-by (p/pres-form)) :range (ex / exceeded) :tense present))) figure 5 : spl-plan for  there are concepts that are not shown, because the presentation-form capacity is exceeded .</citsent>
<aftsection>
<nextsent>rst relations because they proved to be quite useful when we link portions of information.
</nextsent>
<nextsent>in komet/penman, rst-rela- tions are treated the same way as other ela tions, e.g. ascription which we used in the plan shown in figure 4.
</nextsent>
<nextsent>the spl-plan shown in figure 5 combines two relations: the ascription-relation, which we used in the spl-plan in fig-ure 4, and the existence-relation.
</nextsent>
<nextsent>existence is so called one- place-relation, because it contains only :domain-role but no :range.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A837">
<title id=" C00-1081.xml">a stochastic parser based on a structural word prediction model </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>

<prevsent>the stochastic language modeling, imported fl:om the speech recognition area, is one of the snccessflfl methodologies of natural language processing.
</prevsent>
</prevsection>
<citsent citstr=" A88-1019 ">
in fact, all language models for speech recognition are, as far  a.s we know, based on an n-gram model and most practical part-of-speech (pos) taggers are also based on word or pos n-gram model or its exten-sion (church, 1.988; <papid> A88-1019 </papid>cutting et el., 1992; merialdo, 1994; <papid> J94-2001 </papid>l)ennatas and kokkinakis, 1.995).</citsent>
<aftsection>
<nextsent>pos tag-ging is the first step of natural language process-ing, and stochastic taggers have solved this problem with satisfying accuracy for many applications.
</nextsent>
<nextsent>the next step is parsing, or that is to say discovering the structure of given sentence.
</nextsent>
<nextsent>recently, many parsers based on the stochastic approach ave been proposed.
</nextsent>
<nextsent>although their reported accuracies are high, they are not accurate nough for many appli-cations at this stage, and more attempts have to be made to improve them fm:ther.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A838">
<title id=" C00-1081.xml">a stochastic parser based on a structural word prediction model </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>

<prevsent>the stochastic language modeling, imported fl:om the speech recognition area, is one of the snccessflfl methodologies of natural language processing.
</prevsent>
</prevsection>
<citsent citstr=" J94-2001 ">
in fact, all language models for speech recognition are, as far  a.s we know, based on an n-gram model and most practical part-of-speech (pos) taggers are also based on word or pos n-gram model or its exten-sion (church, 1.988; <papid> A88-1019 </papid>cutting et el., 1992; merialdo, 1994; <papid> J94-2001 </papid>l)ennatas and kokkinakis, 1.995).</citsent>
<aftsection>
<nextsent>pos tag-ging is the first step of natural language process-ing, and stochastic taggers have solved this problem with satisfying accuracy for many applications.
</nextsent>
<nextsent>the next step is parsing, or that is to say discovering the structure of given sentence.
</nextsent>
<nextsent>recently, many parsers based on the stochastic approach ave been proposed.
</nextsent>
<nextsent>although their reported accuracies are high, they are not accurate nough for many appli-cations at this stage, and more attempts have to be made to improve them fm:ther.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A839">
<title id=" C00-1081.xml">a stochastic parser based on a structural word prediction model </title>
<section> stochast c  language mode  based.  </section>
<citcontext>
<prevsection>
<prevsent>generally speaking, word-based n-gram model is better than l os-based  n-gram model in terms of 560 predictive power; however lexica.lization of some in-frequent words may be ha.rmfu\] beta.use it may c;mse a. data-sparseness problem.
</prevsent>
<prevsent>in a. practiea.1 tagger (i(upiec, \] 989), only the nlost, frequent \] 00 words a.re lexicalized.
</prevsent>
</prevsection>
<citsent citstr=" P97-1003 ">
also, in a, sta.te-ofthe-a.rt english pa.rser (collins, 1997) <papid> P97-1003 </papid>only the words tha, occur more tha,n times in training data.</citsent>
<aftsection>
<nextsent>are lexicalized.
</nextsent>
<nextsent>for this reason, our pa.rser selectn the words to be lexicalized at the time of lea.rning.
</nextsent>
<nextsent>in the lexical-ized models described above (p/a;, i},l and f~vl), only the selected words a.re \]exica.lized.
</nextsent>
<nextsent>the selec-tion criterion is parsing a.ccuracy (see section 4) of a. hekl-out corpus, small part of the learning co pus excluded from l)a, ramcter cstima.tion.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A841">
<title id=" A94-1019.xml">recycling terms into a partial parser </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>moreover, in the compile-time approach, meta rules generate huge set of transformed rules which may make the parsing process totally inefficient.
</prevsent>
<prevsent>due to the very large size of our grammar, we have opted for the dynamic approach.
</prevsent>
</prevsection>
<citsent citstr=" P94-1027 ">
the computational performances of the application reported in (jacquemin 1994<papid> P94-1027 </papid>a) indicate that the parser only spends 10% of its time in generating meta rules and fully justify the run-time approach.</citsent>
<aftsection>
<nextsent>115 computational lexicalization the keystone of the computational tract ability is lexicalization which allows for bottom-up filtering of the rules before parsing.
</nextsent>
<nextsent>it is completed by fast mechanisms for data access uch as b-tree (for the disk resident lexicon of single words) and hash-code table (for the memory resident stop words).
</nextsent>
<nextsent>the formalism of fastr is lexicalized in the sense of schabes and joshi (1990) because it is composed of rules associated with each lexical item which is the anchor of the corresponding rules.
</nextsent>
<nextsent>the parsing algorithm for lexicalized grammars takes advantage of lexicalization through two-step strategy.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A843">
<title id=" A94-1019.xml">recycling terms into a partial parser </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>similarly, our approach to corpus linguistics makes extensive use of terminological data and investigates systematically and precisely the variations of terms in technical corpora.
</prevsent>
<prevsent>the next natural step in term and compound processing is to provide fastr with learning ability.
</prevsent>
</prevsection>
<citsent citstr=" E93-1011 ">
with this aim in mind, we are currently investigating two novel research directions : firstly, hybrid isation of fastr with connection ist model dedicated to nominal composition (jacquemin 1993) and, secondly, cooperation between fastr and lexter (bourigault 1993) <papid> E93-1011 </papid>tool for term acquisition through the filtering of part-of-speech patterns.</citsent>
<aftsection>
<nextsent>acknowledgement would like to thank jean royaut6 from inist/cnrs for his helpful and friendly collaboration on this project.
</nextsent>
<nextsent>many thanks also to benoit habert from ens fontenay for numerous constructive discussions.
</nextsent>


</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A844">
<title id=" C00-2090.xml">multilevel similar segment matching algorithm for translation memories and example based machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we consider match from to 1 as an edit distance process.
</prevsent>
<prevsent>this edition uses sequence of basic edit operations between the words of the segments, like in wagner &amp; fisher (1974) who used four basic operations: deletion, insertion, strict and equal substitution between the letters of word.
</prevsent>
</prevsection>
<citsent citstr=" J93-1004 ">
this approach as also been followed by gale &amp; church (1993) <papid> J93-1004 </papid>for their alignment algorithm, with six operations.</citsent>
<aftsection>
<nextsent>here, we only consider deletions and equal ities (i.e. equal substitutions): f+i basic operations in totap.
</nextsent>
<nextsent>one equality corresponds toeach of the layers, and deletion affects all layers at once.
</nextsent>
<nextsent>in figure 1, the items in bold match each other, and the strike through ones have to be deleted.
</nextsent>
<nextsent>the edition of into involves five deletions ( nikkei ,  journal ,  reported , that ,  really ), one equality at layer 1 ( stayed ), two at layer 2 3 lepage (1998) <papid> P98-1121 </papid>also uses deletions and one level of.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A845">
<title id=" C00-2090.xml">multilevel similar segment matching algorithm for translation memories and example based machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>one equality corresponds toeach of the layers, and deletion affects all layers at once.
</prevsent>
<prevsent>in figure 1, the items in bold match each other, and the strike through ones have to be deleted.
</prevsent>
</prevsection>
<citsent citstr=" P98-1121 ">
the edition of into involves five deletions ( nikkei ,  journal ,  reported , that ,  really ), one equality at layer 1 ( stayed ), two at layer 2 3 lepage (1998) <papid> P98-1121 </papid>also uses deletions and one level of.</citsent>
<aftsection>
<nextsent>equality lbr calculating his  pseudo-distance , for getting the similarity between two strings.
</nextsent>
<nextsent>cs c9 stm monaay strong monday adj uoull matclfing zone i3 i4 strong adj ues ay uoun ( stay ,  strong ), and four at layer 3 ( pn ,  verb ,  adj ,  noun ).
</nextsent>
<nextsent>at the word level, the similarity between the two segments is considered to be the relative number of words of the input segment hat are matched by some word of the candidate segmeut in the matching zone (from  ntt  to  monday  in our example): 1/4 in figure 1.
</nextsent>
<nextsent>the same similarity can be considered at different levels.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A846">
<title id=" A97-1038.xml">cogent help nlg meets se in a tool for authoring dynamically generated online help </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>also john-son and erdem, 1995; priestly et al , 1996; korgen, 1996) - - in tool for generating end user-level doc-umentation.
</prevsent>
<prevsent>cogent help is also unusual in that it is (to date) one of the few tools to bring nlg tech-niques to bear on the problem of authoring dy-namically generated documents (cf.
</prevsent>
</prevsection>
<citsent citstr=" W96-0416 ">
paris and van-der linden, 1996; knott et al , 1996; <papid> W96-0416 </papid>hirst and di-marco, 1995); traditionally, most applied nlg sys-tems have focused on niches where texts can be gen-erated fully automatically, such as routine reports of various types (e.g. goldberg et al , 1994; kukich et al ., 1994) or explanations of expert system reasoning (cf.</citsent>
<aftsection>
<nextsent>moore, 1995 and references therein).
</nextsent>
<nextsent>while striving to design highly sophisticated, fully automatic systems has undoubtedly ed to deeper understanding of the text generation process, it has had the unfortunate effect (to date) of limiting the use of techniques pioneered in the nlg community to just few niches where high knowledge acqui-sition costs stand chance of being balanced by substantial volume of needed texts (cf.
</nextsent>
<nextsent>reiter and mellish, 1993).
</nextsent>
<nextsent>by joining the emerging authoring support crowd and endeavoring to create new op-portunities in automated ocumentation, we hope to contribute to the broader acceptance and visi-bility of nlg technology in the overall computing community.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A847">
<title id=" A97-1038.xml">cogent help nlg meets se in a tool for authoring dynamically generated online help </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>as will be explained below, this approach has led us to (i) make use of what amounts to large-grained  phrasal  lexicon and (ii) devise and implement widget-clustering algorithm for recovering functional groupings, as part of an intermediate knowledge representation system (ikrs).
</prevsent>
<prevsent>5.1.1 phrasal lexicon as milosavljevic et al  (1996) argue, to optimize coverage and cost it makes sense to choose an un-derlying representation which ? makes precisely those distinctions that are rele-vant for the intended range of generated texts; and ? is no more abstract hanis required for the in-ference processes which need to be performed over the representation.
</prevsent>
</prevsection>
<citsent citstr=" P83-1022 ">
they go on to argue that besides eliminating great deal of unnecessary  generation from first principles,  this approach complements their use of phrasal lexicon (kukich, 1983; <papid> P83-1022 </papid>hovy, 1988) at the linguistic level.</citsent>
<aftsection>
<nextsent>applying essentially the same approach to the de-sign of cogent help, we first determined that for the intended range of generated texts it suffices to as-sociate with each widget to be documented small number of atomic propositions and properties, iden-tifiable by type.
</nextsent>
<nextsent>next we determined that since no inference is required beyond checking for equality, these propositions and properties can be conflated with their linguistic rea i za ions - i.e., the indexed, human-authored help snippets cogent help takes as input.
</nextsent>
<nextsent>while we did not originally think of cogent- help collection of input help snippets as phrasal lexicon la milosavljevic et al , in retrospect be-comes evident hat this collection can be viewed as 259 figure 1: sample application window tantamount to one; of course, since these snippets vary in size from phrases to paragraphs, the term  phrasal  is not entirely accurate.
</nextsent>
<nextsent>the types of snippets in current use include one- sentence short description of the relevant gui com- ponent; paragraph-sized lab oration on the short description; various phrase-sized messages concern-ing the conditions under which it is visible or en-abled, if appropriate; and list of references to other topics.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A848">
<title id=" A97-1038.xml">cogent help nlg meets se in a tool for authoring dynamically generated online help </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>the reason why is this: in evolving the text planning rule base, it makes sense to localize decisions as much as possible; however, to handle rule interactions in single pass, one is forced to centralize these decisions (which can become cum- bersome).
</prevsent>
<prevsent>to simplify matters, it is often appropri-ate to generate an initial version naively, then carry out revisions on it in subsequent pass (cf.
</prevsent>
</prevsection>
<citsent citstr=" W96-0401 ">
robin, 1994; wanner and hovy, 1996).<papid> W96-0401 </papid></citsent>
<aftsection>
<nextsent>in cogent help, interactions that are cumbersome to anticipate arise in dealing with the various op-tional phrase-sized messages whose inclusion condi-tions differ between the static and dynamic mode.
</nextsent>
<nextsent>to elaborate, let us consider once more the help page shown in figure 2.
</nextsent>
<nextsent>had the four buttons described in the lower right frame been enabled rather than dis-abled (at run-time), the group-level t0_.enable mes-sage would have simply been left out, in order to en- 262 hance relevance and conciseness; 4 on the other hand, had this page been generated in static mode (where such run-time conditions are not known), the text planner would have again included the description, though this time with the less specific  to enable these commands,  prep ended instead.
</nextsent>
<nextsent>now, since the various messages associated with widget have slightly different inclusion conditions, it makes sense to localize these inclusion conditions to text plan-ning rule for each message (the common parts of these conditions are shared via inheritance).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A849">
<title id=" A00-1044.xml">named entity extraction from noisy input speech and ocr </title>
<section> algorithms and data.  </section>
<citcontext>
<prevsection>
<prevsent>printing the on-line text, rather than using the original newsprint, produced the images for ocr, which were all scanned at 600 dpi.
</prevsent>
<prevsent>2.2 algorithms.
</prevsent>
</prevsection>
<citsent citstr=" A97-1029 ">
the information extraction system tested is identifinder(tm), which has previously been detailed in bikel et al (1997), <papid> A97-1029 </papid>bikel et al (1999).</citsent>
<aftsection>
<nextsent>in that system, an hmm labels each word either with one of the desired classes (e.g., person, organization, etc.) or with the label not-a- name (to represent  none of the desired classes ).
</nextsent>
<nextsent>the states of the hmm fall into regions, one region for each desired class plus one for not-a-name.
</nextsent>
<nextsent>(see figure 2-1.)
</nextsent>
<nextsent>the hmm thus has model of each desired class and of the other text.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A851">
<title id=" A94-1007.xml">symmetric pattern matching analysis for english coordinate structures </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the problem is select-ing, from all possible candidates, the correct syn-tactic structure formed by an individual coordinate conjunction, i.e., determining which constituents are conjoined by the conjunction.
</prevsent>
<prevsent>although the con-junction appears to have simple function in the english language, it has been researched as con- junct scope problem by both theoretical and com-putational linguists.
</prevsent>
</prevsection>
<citsent citstr=" P85-1023 ">
theoretically, it is possible to describe the syntactic and semantic onstraints hat govern the acceptability of structure in which two constituents are conjoined by the conjunction(lesmo and torasso, 1985; <papid> P85-1023 </papid>gazdar, 1981; schachter, 1977).</citsent>
<aftsection>
<nextsent>computationally, it is possible to describe the gram-mar and heuristic rules for these constraints by atn networks, logic grarnmars~ hpsg~ and cate-gorical grammars(kosy, 1986; <papid> P86-1013 </papid>fong and berwick, 1985; <papid> P85-1014 </papid>huang, 1984; <papid> P84-1053 </papid>boguraev et al, 1983; blackwell, 1981; niimi et al, 1986).</nextsent>
<nextsent>however, it is not easy to apply these techniques to large-scale mt systems, because there exist variety of conjoined patterns, many word ambiguities, ome unknown words and ellipses of the words simultaneously, in real environ- ments.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A852">
<title id=" A94-1007.xml">symmetric pattern matching analysis for english coordinate structures </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>although the con-junction appears to have simple function in the english language, it has been researched as con- junct scope problem by both theoretical and com-putational linguists.
</prevsent>
<prevsent>theoretically, it is possible to describe the syntactic and semantic onstraints hat govern the acceptability of structure in which two constituents are conjoined by the conjunction(lesmo and torasso, 1985; <papid> P85-1023 </papid>gazdar, 1981; schachter, 1977).</prevsent>
</prevsection>
<citsent citstr=" P86-1013 ">
computationally, it is possible to describe the gram-mar and heuristic rules for these constraints by atn networks, logic grarnmars~ hpsg~ and cate-gorical grammars(kosy, 1986; <papid> P86-1013 </papid>fong and berwick, 1985; <papid> P85-1014 </papid>huang, 1984; <papid> P84-1053 </papid>boguraev et al, 1983; blackwell, 1981; niimi et al, 1986).</citsent>
<aftsection>
<nextsent>however, it is not easy to apply these techniques to large-scale mt systems, because there exist variety of conjoined patterns, many word ambiguities, ome unknown words and ellipses of the words simultaneously, in real environ-ments.
</nextsent>
<nextsent>also, there may be several conjunctions and the equivalent words, such as commas, in single sentence.
</nextsent>
<nextsent>typically, the methods produce so many possible structures that mt systems cannot select the correct one, even if the grammars allow to write the rules in the simple notations.
</nextsent>
<nextsent>often, conjunctions might produce the reading difficulty even for the human readers.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A854">
<title id=" A94-1007.xml">symmetric pattern matching analysis for english coordinate structures </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>although the con-junction appears to have simple function in the english language, it has been researched as con- junct scope problem by both theoretical and com-putational linguists.
</prevsent>
<prevsent>theoretically, it is possible to describe the syntactic and semantic onstraints hat govern the acceptability of structure in which two constituents are conjoined by the conjunction(lesmo and torasso, 1985; <papid> P85-1023 </papid>gazdar, 1981; schachter, 1977).</prevsent>
</prevsection>
<citsent citstr=" P85-1014 ">
computationally, it is possible to describe the gram-mar and heuristic rules for these constraints by atn networks, logic grarnmars~ hpsg~ and cate-gorical grammars(kosy, 1986; <papid> P86-1013 </papid>fong and berwick, 1985; <papid> P85-1014 </papid>huang, 1984; <papid> P84-1053 </papid>boguraev et al, 1983; blackwell, 1981; niimi et al, 1986).</citsent>
<aftsection>
<nextsent>however, it is not easy to apply these techniques to large-scale mt systems, because there exist variety of conjoined patterns, many word ambiguities, ome unknown words and ellipses of the words simultaneously, in real environ-ments.
</nextsent>
<nextsent>also, there may be several conjunctions and the equivalent words, such as commas, in single sentence.
</nextsent>
<nextsent>typically, the methods produce so many possible structures that mt systems cannot select the correct one, even if the grammars allow to write the rules in the simple notations.
</nextsent>
<nextsent>often, conjunctions might produce the reading difficulty even for the human readers.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A855">
<title id=" A94-1007.xml">symmetric pattern matching analysis for english coordinate structures </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>although the con-junction appears to have simple function in the english language, it has been researched as con- junct scope problem by both theoretical and com-putational linguists.
</prevsent>
<prevsent>theoretically, it is possible to describe the syntactic and semantic onstraints hat govern the acceptability of structure in which two constituents are conjoined by the conjunction(lesmo and torasso, 1985; <papid> P85-1023 </papid>gazdar, 1981; schachter, 1977).</prevsent>
</prevsection>
<citsent citstr=" P84-1053 ">
computationally, it is possible to describe the gram-mar and heuristic rules for these constraints by atn networks, logic grarnmars~ hpsg~ and cate-gorical grammars(kosy, 1986; <papid> P86-1013 </papid>fong and berwick, 1985; <papid> P85-1014 </papid>huang, 1984; <papid> P84-1053 </papid>boguraev et al, 1983; blackwell, 1981; niimi et al, 1986).</citsent>
<aftsection>
<nextsent>however, it is not easy to apply these techniques to large-scale mt systems, because there exist variety of conjoined patterns, many word ambiguities, ome unknown words and ellipses of the words simultaneously, in real environ-ments.
</nextsent>
<nextsent>also, there may be several conjunctions and the equivalent words, such as commas, in single sentence.
</nextsent>
<nextsent>typically, the methods produce so many possible structures that mt systems cannot select the correct one, even if the grammars allow to write the rules in the simple notations.
</nextsent>
<nextsent>often, conjunctions might produce the reading difficulty even for the human readers.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A856">
<title id=" A94-1007.xml">symmetric pattern matching analysis for english coordinate structures </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, they also give the readers kind of symmetry as read-ing indication.
</prevsent>
<prevsent>they exhibit the tendency to con- join the same kind of syntactic patterns, which has been called  parallelism  (beaugrande and dressier, 1981; shinoda, 1981).
</prevsent>
</prevsection>
<citsent citstr=" C92-1029 ">
in japanese, the similar-ity is used for analyzing conjunctive structures and the method is found effective(kurohashi and nagao, 1992).<papid> C92-1029 </papid></citsent>
<aftsection>
<nextsent>while japanese language has several coordi-nate conjunctions according to the syntactic levels (a noun phrase and predicative clause), english coordinate conjunctions are used for any level of the structures.
</nextsent>
<nextsent>more robust methods are necessary for dealing with english conjunctive structures.
</nextsent>
<nextsent>we propose here an english coordinate structure analysis model, which can determine the correct syn-tactic structure in real environments by taking ad-vantage of the symmetric patterns of the parallelism.
</nextsent>
<nextsent>41 the model is based on balance matching operation for two lists of the feature sets.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A859">
<title id=" A94-1007.xml">symmetric pattern matching analysis for english coordinate structures </title>
<section> problems of the conjunctions.  </section>
<citcontext>
<prevsection>
<prevsent>this model, which was implemented in the pivot english-japanese mt system with the dictionary of 100,000 words, has been working in the analysis module.
</prevsent>
<prevsent>finally, the results in the mt system are reported together with the mt system configuration.
</prevsent>
</prevsection>
<citsent citstr=" E83-1013 ">
coordinate conjunctions for mt systems present three difficulties(kosy, 1986; <papid> P86-1013 </papid>huang, 1983; <papid> E83-1013 </papid>niimi et al., 1986; okumura et ai., 1987).</citsent>
<aftsection>
<nextsent>1.
</nextsent>
<nextsent>analysis cost: english coordinate conjunctions.
</nextsent>
<nextsent>have variety of linguistic functions.
</nextsent>
<nextsent>the con-junctions can syntactically conjoin any parts of speech; nouns, adjectives, verbs, etc., and all sorts of constituents; words, phrases and clauses.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A860">
<title id=" A00-1005.xml">partsid a dialogue based system for identifying parts for medical systems </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>duke pascal tutoring system helps students in an introductory programming class debug their programs by allowing them to analyze their syntax errors, get additional information on the error, and learn the correct syntax.
</prevsent>
<prevsent>although these systems have been quite successful, they use detailed models of the domain and therefore cannot be used for diverse applications uch as the ones required for customer service centers.
</prevsent>
</prevsection>
<citsent citstr=" J86-3001 ">
other related work on dialogue include (carberry, 1990; grosz and sidner, 1986; <papid> J86-3001 </papid>reichman, 1981).</citsent>
<aftsection>
<nextsent>of parts for medical systems initially, we were approached by the medical systems business of our company for help in reducing the number of calls handled by human operators at their call center.
</nextsent>
<nextsent>an analysis of the types of customer service provided by their call center showed that large volume of calls handled by their operators were placed by field engineers requesting identification umbers of parts for various medical systems.
</nextsent>
<nextsent>the id numbers were most often used for ordering the corresponding parts using an automated ivr system.
</nextsent>
<nextsent>therefore, the system we have built 30 figure 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A861">
<title id=" C00-2112.xml">making sense of reference to the unfamiliar </title>
<section> i t roduct ion   </section>
<citcontext>
<prevsection>
<prevsent>accord-ing to ii.ussell, definite description such as %he king of france , denotes mfique individ-ual by virtue of its meaning.
</prevsent>
<prevsent>but, according to familiarity theory (helm, 1.983), reti;rring ex-pressions need not denote mfiquely by virtue of their meaning as they refer to individuals made familiar by the discourse or other context.
</prevsent>
</prevsection>
<citsent citstr=" J86-3001 ">
this observation plays key role in centering the-ory (grosz and sidner, 1986; <papid> J86-3001 </papid>grosz et al, 1995) <papid> J95-2003 </papid>and other computational al)t)roaches in which rethrring expressions are resolved by locating their antecedents in the discourse.</citsent>
<aftsection>
<nextsent>the refer-ence of pronouns like  he , definite descriptions like  the woman , and referential tenses like  had  clearly has more to do with salience ill context thml with uniqueness of meaning.
</nextsent>
<nextsent>sim-ilarly, while names like  mary  need not denote individuals prominent in the discourse context, * \ve would like to thank the anonymous reviewers for their detailed and helpful comments.
</nextsent>
<nextsent>they must nevertheless denote individuals famil-iar to conversants if they are successflflly to re-fer.
</nextsent>
<nextsent>however, there is another (:lass of referring expressions in relation to which we believe the concept of uniqueness of meaning does have an essential role to plt~y. these include such def-inite descrit)tions as  the first man  and  the first snow drop of spring , along with such vari-ations on these as  the first three men  and  the first snowdrops of spring .
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A862">
<title id=" C00-2112.xml">making sense of reference to the unfamiliar </title>
<section> i t roduct ion   </section>
<citcontext>
<prevsection>
<prevsent>accord-ing to ii.ussell, definite description such as %he king of france , denotes mfique individ-ual by virtue of its meaning.
</prevsent>
<prevsent>but, according to familiarity theory (helm, 1.983), reti;rring ex-pressions need not denote mfiquely by virtue of their meaning as they refer to individuals made familiar by the discourse or other context.
</prevsent>
</prevsection>
<citsent citstr=" J95-2003 ">
this observation plays key role in centering the-ory (grosz and sidner, 1986; <papid> J86-3001 </papid>grosz et al, 1995) <papid> J95-2003 </papid>and other computational al)t)roaches in which rethrring expressions are resolved by locating their antecedents in the discourse.</citsent>
<aftsection>
<nextsent>the refer-ence of pronouns like  he , definite descriptions like  the woman , and referential tenses like  had  clearly has more to do with salience ill context thml with uniqueness of meaning.
</nextsent>
<nextsent>sim-ilarly, while names like  mary  need not denote individuals prominent in the discourse context, * \ve would like to thank the anonymous reviewers for their detailed and helpful comments.
</nextsent>
<nextsent>they must nevertheless denote individuals famil-iar to conversants if they are successflflly to re-fer.
</nextsent>
<nextsent>however, there is another (:lass of referring expressions in relation to which we believe the concept of uniqueness of meaning does have an essential role to plt~y. these include such def-inite descrit)tions as  the first man  and  the first snow drop of spring , along with such vari-ations on these as  the first three men  and  the first snowdrops of spring .
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A863">
<title id=" A88-1026.xml">luke an experiment in the early integration of natural language processing </title>
<section> motivation </section>
<citcontext>
<prevsection>
<prevsent>by supporting the early integration of the application program and the nl interface as this single knowledge base is being built, luke helps to ensure that it will be adequate, with respect to both its content and its structure, to support both these target tasks.
</prevsent>
<prevsent>the second way in which the two system building tasks are related is that one can support the other.
</prevsent>
</prevsection>
<citsent citstr=" P85-1008 ">
by associating natural language with concepts as they are entered into knowledge model of semantic ana lys s all of the following discussion is based on model of semantic analysis similar to that proposed in (hobbs, 1985).<papid> P85-1008 </papid></citsent>
<aftsection>
<nextsent>under this model, syntactic and semantic analysis are done as separate operations.
</nextsent>
<nextsent>the first stage of semantic analysis is conversion to initia/logical form, in which the surface content of the sentence is encoded in set of expressions that look like logical terms, but whose predicates are taken directly from the words used in the sentence.
</nextsent>
<nextsent>initial logical form captures the predicational structure of the sentence, without expressir~g it in terms of the knowledge base.
</nextsent>
<nextsent>once produced, the expressions in initial logical form are individually translated into final logical form, which is set of first-order terms whose predicates are those used in the application knowledge base.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A864">
<title id=" A00-2032.xml">mostly unsupervised statistical segmentation of japanese applications to kanji </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in contrast, we introduce novel statistical method utilizing unsegmented training data, with performance on kanji sequences comparable to and sometimes surpassing that of morphological nalyz- ers over variety of error metrics.
</prevsent>
<prevsent>because japanese is written without delimiters be-tween words) accurate word segmentation to re-cover the lexical items is key step in japanese text processing.
</prevsent>
</prevsection>
<citsent citstr=" C94-1101 ">
proposed applications of segmentation technology include extracting new technical terms, indexing documents for information retrieval, and correcting optical character recognition (ocr) er-rors (wu and tseng, 1993; nagao and mori, 1994; <papid> C94-1101 </papid>nagata, 1996<papid> W96-0205 </papid>a; nagata, 1996<papid> W96-0205 </papid>b; sproat et al , 1996; <papid> J96-3004 </papid>fung, 1998).</citsent>
<aftsection>
<nextsent>typically, japanese word segmentation is per-formed by morphological nalysis based on lexical and grammatical knowledge.
</nextsent>
<nextsent>this analysis is aided by the fact that there are three types of japanese characters, kanji, hiragana, and katakana: changes in character type often indicate word boundaries, al-though using this heuristic alone achieves less than 60% accuracy (nagata, 1997).<papid> W97-0120 </papid></nextsent>
<nextsent>character sequences consisting solely of kanji pose challenge to morphologically-based seg- reenters for several reasons.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A865">
<title id=" A00-2032.xml">mostly unsupervised statistical segmentation of japanese applications to kanji </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in contrast, we introduce novel statistical method utilizing unsegmented training data, with performance on kanji sequences comparable to and sometimes surpassing that of morphological nalyz- ers over variety of error metrics.
</prevsent>
<prevsent>because japanese is written without delimiters be-tween words) accurate word segmentation to re-cover the lexical items is key step in japanese text processing.
</prevsent>
</prevsection>
<citsent citstr=" W96-0205 ">
proposed applications of segmentation technology include extracting new technical terms, indexing documents for information retrieval, and correcting optical character recognition (ocr) er-rors (wu and tseng, 1993; nagao and mori, 1994; <papid> C94-1101 </papid>nagata, 1996<papid> W96-0205 </papid>a; nagata, 1996<papid> W96-0205 </papid>b; sproat et al , 1996; <papid> J96-3004 </papid>fung, 1998).</citsent>
<aftsection>
<nextsent>typically, japanese word segmentation is per-formed by morphological nalysis based on lexical and grammatical knowledge.
</nextsent>
<nextsent>this analysis is aided by the fact that there are three types of japanese characters, kanji, hiragana, and katakana: changes in character type often indicate word boundaries, al-though using this heuristic alone achieves less than 60% accuracy (nagata, 1997).<papid> W97-0120 </papid></nextsent>
<nextsent>character sequences consisting solely of kanji pose challenge to morphologically-based seg- reenters for several reasons.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A881">
<title id=" A00-2032.xml">mostly unsupervised statistical segmentation of japanese applications to kanji </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in contrast, we introduce novel statistical method utilizing unsegmented training data, with performance on kanji sequences comparable to and sometimes surpassing that of morphological nalyz- ers over variety of error metrics.
</prevsent>
<prevsent>because japanese is written without delimiters be-tween words) accurate word segmentation to re-cover the lexical items is key step in japanese text processing.
</prevsent>
</prevsection>
<citsent citstr=" J96-3004 ">
proposed applications of segmentation technology include extracting new technical terms, indexing documents for information retrieval, and correcting optical character recognition (ocr) er-rors (wu and tseng, 1993; nagao and mori, 1994; <papid> C94-1101 </papid>nagata, 1996<papid> W96-0205 </papid>a; nagata, 1996<papid> W96-0205 </papid>b; sproat et al , 1996; <papid> J96-3004 </papid>fung, 1998).</citsent>
<aftsection>
<nextsent>typically, japanese word segmentation is per-formed by morphological nalysis based on lexical and grammatical knowledge.
</nextsent>
<nextsent>this analysis is aided by the fact that there are three types of japanese characters, kanji, hiragana, and katakana: changes in character type often indicate word boundaries, al-though using this heuristic alone achieves less than 60% accuracy (nagata, 1997).<papid> W97-0120 </papid></nextsent>
<nextsent>character sequences consisting solely of kanji pose challenge to morphologically-based seg- reenters for several reasons.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A882">
<title id=" A00-2032.xml">mostly unsupervised statistical segmentation of japanese applications to kanji </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>proposed applications of segmentation technology include extracting new technical terms, indexing documents for information retrieval, and correcting optical character recognition (ocr) er-rors (wu and tseng, 1993; nagao and mori, 1994; <papid> C94-1101 </papid>nagata, 1996<papid> W96-0205 </papid>a; nagata, 1996<papid> W96-0205 </papid>b; sproat et al , 1996; <papid> J96-3004 </papid>fung, 1998).</prevsent>
<prevsent>typically, japanese word segmentation is per-formed by morphological nalysis based on lexical and grammatical knowledge.</prevsent>
</prevsection>
<citsent citstr=" W97-0120 ">
this analysis is aided by the fact that there are three types of japanese characters, kanji, hiragana, and katakana: changes in character type often indicate word boundaries, al-though using this heuristic alone achieves less than 60% accuracy (nagata, 1997).<papid> W97-0120 </papid></citsent>
<aftsection>
<nextsent>character sequences consisting solely of kanji pose challenge to morphologically-based seg- reenters for several reasons.
</nextsent>
<nextsent>first and most importantly, kanji sequences often contain domain terms and proper nouns: fung (1998) notes that 50-85% of the terms in various technical dictio- ~the analogous situation english would be if words were written without spaces between them.
</nextsent>
<nextsent>sequence ngth # of characters % of corpus 1 - 3 kanji 20,405,486 25.6 4 - 6 kanji 12,743,177 16.1 more than 6 kanji 3,966,408 5.1 total 37,115,071 46.8 figure 1: statistics from 1993 japanese newswire (nikkei), 79,326,406 characters total.
</nextsent>
<nextsent>naries are composed at least partly of kanji.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A884">
<title id=" A00-2032.xml">mostly unsupervised statistical segmentation of japanese applications to kanji </title>
<section> results.  </section>
<citcontext>
<prevsection>
<prevsent>rect with respect to any reasonable annotation.
</prevsent>
<prevsent>our novel metrics account for two types of er-rors.
</prevsent>
</prevsection>
<citsent citstr=" A92-1022 ">
the first, crossing bracket, is proposed bracket hat overlaps but is not contained within an annotation bracket (grishman et al , 1992).<papid> A92-1022 </papid></citsent>
<aftsection>
<nextsent>cross-ing brackets cannot coexist with annotation brack-ets, and it is unlikely that another human would create such brackets.
</nextsent>
<nextsent>the second type of er-ror, morpheme-dividing bracket, sub divides morpheme-level annotation bracket; by definition, such bracket results in loss of meaning.
</nextsent>
<nextsent>see fig-ure 6 for some examples.
</nextsent>
<nextsent>we define compatible bracket as proposed bracket that is neither crossing nor morpheme- dividing.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A885">
<title id=" A00-2032.xml">mostly unsupervised statistical segmentation of japanese applications to kanji </title>
<section> results.  </section>
<citcontext>
<prevsection>
<prevsent>we also avoid the need to segment large amount of parameter-training data because our al-gorithm draws almost all its information from an unsegmented corpus.
</prevsent>
<prevsent>indeed, the only human effort involved in our algorithm is pre-segmenting the five 50-sequence parameter training sets, which took only 42 minutes.
</prevsent>
</prevsection>
<citsent citstr=" P98-1108 ">
in contrast, previously proposed supervised approaches have used segmented train-ing sets ranging from 1000-5000 sentences (kash- ioka et al , 1998) <papid> P98-1108 </papid>to 190,000 sentences (nagata, 1996<papid> W96-0205 </papid>a).</citsent>
<aftsection>
<nextsent>to test how much annotated training data is actu-ally necessary, we experimented with using minis-cule parameter-training sets: five sets of only five strings each (from which any sequences repeated in the test data were discarded).
</nextsent>
<nextsent>it took only 4 minutes to perform the hand segmentation in this case.
</nextsent>
<nextsent>as shown in figure 8, relative word performance was not degraded and sometimes even slightly better.
</nextsent>
<nextsent>in fact, from the last column of figure 8 we see that even if our algorithm has access to only five anno-tated sequences when juman has access to ten times as many, we still achieve better precision and better measure.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A894">
<title id=" A00-2032.xml">mostly unsupervised statistical segmentation of japanese applications to kanji </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>indeed, in some cases, both are needed to achieve the best perfor- mance; also, each condition when used in isolation yields sub optimal performance with respect to some performance metrics.
</prevsent>
<prevsent>accuracy optimize optimize optimize precision recall f-measure word m &amp; m morpheme &amp; t figure 9: entries indicate whether best performance is achieved using the local maximum condition (m), the threshold condition (t), or both.
</prevsent>
</prevsection>
<citsent citstr=" H93-1037 ">
japanese many previously proposed segmenta-tion methods for japanese text make use of either pre-existing lexicon (yamron et al , 1993; <papid> H93-1037 </papid>mat-sumoto and nagao, 1994; takeuchi and matsumoto, 1995; nagata, 1997; <papid> W97-0120 </papid>fuchi and takagi, 1998) <papid> P98-1068 </papid>or pre-segmented training data (nagata, 1994; <papid> C94-1032 </papid>papa: georgiou, 1994; <papid> H94-1054 </papid>nagata, 1996<papid> W96-0205 </papid>a; kashioka et al , 1998; <papid> P98-1108 </papid>mori and nagao, 1998).</citsent>
<aftsection>
<nextsent>other approaches bootstrap from an initial segmentation provided by baseline algorithm such as juman (matsukawa et al ., 1993; <papid> H93-1045 </papid>yamamoto, 1996).<papid> W96-0113 </papid></nextsent>
<nextsent>unsupervised, non-lexicon-based methods for japanese segmentation doexist, but they often have limited applicability.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A896">
<title id=" A00-2032.xml">mostly unsupervised statistical segmentation of japanese applications to kanji </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>indeed, in some cases, both are needed to achieve the best perfor- mance; also, each condition when used in isolation yields sub optimal performance with respect to some performance metrics.
</prevsent>
<prevsent>accuracy optimize optimize optimize precision recall f-measure word m &amp; m morpheme &amp; t figure 9: entries indicate whether best performance is achieved using the local maximum condition (m), the threshold condition (t), or both.
</prevsent>
</prevsection>
<citsent citstr=" P98-1068 ">
japanese many previously proposed segmenta-tion methods for japanese text make use of either pre-existing lexicon (yamron et al , 1993; <papid> H93-1037 </papid>mat-sumoto and nagao, 1994; takeuchi and matsumoto, 1995; nagata, 1997; <papid> W97-0120 </papid>fuchi and takagi, 1998) <papid> P98-1068 </papid>or pre-segmented training data (nagata, 1994; <papid> C94-1032 </papid>papa: georgiou, 1994; <papid> H94-1054 </papid>nagata, 1996<papid> W96-0205 </papid>a; kashioka et al , 1998; <papid> P98-1108 </papid>mori and nagao, 1998).</citsent>
<aftsection>
<nextsent>other approaches bootstrap from an initial segmentation provided by baseline algorithm such as juman (matsukawa et al ., 1993; <papid> H93-1045 </papid>yamamoto, 1996).<papid> W96-0113 </papid></nextsent>
<nextsent>unsupervised, non-lexicon-based methods for japanese segmentation doexist, but they often have limited applicability.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A897">
<title id=" A00-2032.xml">mostly unsupervised statistical segmentation of japanese applications to kanji </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>indeed, in some cases, both are needed to achieve the best perfor- mance; also, each condition when used in isolation yields sub optimal performance with respect to some performance metrics.
</prevsent>
<prevsent>accuracy optimize optimize optimize precision recall f-measure word m &amp; m morpheme &amp; t figure 9: entries indicate whether best performance is achieved using the local maximum condition (m), the threshold condition (t), or both.
</prevsent>
</prevsection>
<citsent citstr=" C94-1032 ">
japanese many previously proposed segmenta-tion methods for japanese text make use of either pre-existing lexicon (yamron et al , 1993; <papid> H93-1037 </papid>mat-sumoto and nagao, 1994; takeuchi and matsumoto, 1995; nagata, 1997; <papid> W97-0120 </papid>fuchi and takagi, 1998) <papid> P98-1068 </papid>or pre-segmented training data (nagata, 1994; <papid> C94-1032 </papid>papa: georgiou, 1994; <papid> H94-1054 </papid>nagata, 1996<papid> W96-0205 </papid>a; kashioka et al , 1998; <papid> P98-1108 </papid>mori and nagao, 1998).</citsent>
<aftsection>
<nextsent>other approaches bootstrap from an initial segmentation provided by baseline algorithm such as juman (matsukawa et al ., 1993; <papid> H93-1045 </papid>yamamoto, 1996).<papid> W96-0113 </papid></nextsent>
<nextsent>unsupervised, non-lexicon-based methods for japanese segmentation doexist, but they often have limited applicability.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A898">
<title id=" A00-2032.xml">mostly unsupervised statistical segmentation of japanese applications to kanji </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>indeed, in some cases, both are needed to achieve the best perfor- mance; also, each condition when used in isolation yields sub optimal performance with respect to some performance metrics.
</prevsent>
<prevsent>accuracy optimize optimize optimize precision recall f-measure word m &amp; m morpheme &amp; t figure 9: entries indicate whether best performance is achieved using the local maximum condition (m), the threshold condition (t), or both.
</prevsent>
</prevsection>
<citsent citstr=" H94-1054 ">
japanese many previously proposed segmenta-tion methods for japanese text make use of either pre-existing lexicon (yamron et al , 1993; <papid> H93-1037 </papid>mat-sumoto and nagao, 1994; takeuchi and matsumoto, 1995; nagata, 1997; <papid> W97-0120 </papid>fuchi and takagi, 1998) <papid> P98-1068 </papid>or pre-segmented training data (nagata, 1994; <papid> C94-1032 </papid>papa: georgiou, 1994; <papid> H94-1054 </papid>nagata, 1996<papid> W96-0205 </papid>a; kashioka et al , 1998; <papid> P98-1108 </papid>mori and nagao, 1998).</citsent>
<aftsection>
<nextsent>other approaches bootstrap from an initial segmentation provided by baseline algorithm such as juman (matsukawa et al ., 1993; <papid> H93-1045 </papid>yamamoto, 1996).<papid> W96-0113 </papid></nextsent>
<nextsent>unsupervised, non-lexicon-based methods for japanese segmentation doexist, but they often have limited applicability.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A908">
<title id=" A00-2032.xml">mostly unsupervised statistical segmentation of japanese applications to kanji </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>accuracy optimize optimize optimize precision recall f-measure word m &amp; m morpheme &amp; t figure 9: entries indicate whether best performance is achieved using the local maximum condition (m), the threshold condition (t), or both.
</prevsent>
<prevsent>japanese many previously proposed segmenta-tion methods for japanese text make use of either pre-existing lexicon (yamron et al , 1993; <papid> H93-1037 </papid>mat-sumoto and nagao, 1994; takeuchi and matsumoto, 1995; nagata, 1997; <papid> W97-0120 </papid>fuchi and takagi, 1998) <papid> P98-1068 </papid>or pre-segmented training data (nagata, 1994; <papid> C94-1032 </papid>papa: georgiou, 1994; <papid> H94-1054 </papid>nagata, 1996<papid> W96-0205 </papid>a; kashioka et al , 1998; <papid> P98-1108 </papid>mori and nagao, 1998).</prevsent>
</prevsection>
<citsent citstr=" H93-1045 ">
other approaches bootstrap from an initial segmentation provided by baseline algorithm such as juman (matsukawa et al ., 1993; <papid> H93-1045 </papid>yamamoto, 1996).<papid> W96-0113 </papid></citsent>
<aftsection>
<nextsent>unsupervised, non-lexicon-based methods for japanese segmentation doexist, but they often have limited applicability.
</nextsent>
<nextsent>both tomokiyo andries (1997) and teller and batchelder (1994) explicitly avoid working with kanji charactes.
</nextsent>
<nextsent>takeda and fujisaki (1987) propose the short unit model, type of hidden markov model with linguistically- determined topology, to segment kanji compound words.
</nextsent>
<nextsent>however, their method does not handle three-character stem words or single-character stem words with affixes, both of which often occur in proper nouns.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A909">
<title id=" A00-2032.xml">mostly unsupervised statistical segmentation of japanese applications to kanji </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>accuracy optimize optimize optimize precision recall f-measure word m &amp; m morpheme &amp; t figure 9: entries indicate whether best performance is achieved using the local maximum condition (m), the threshold condition (t), or both.
</prevsent>
<prevsent>japanese many previously proposed segmenta-tion methods for japanese text make use of either pre-existing lexicon (yamron et al , 1993; <papid> H93-1037 </papid>mat-sumoto and nagao, 1994; takeuchi and matsumoto, 1995; nagata, 1997; <papid> W97-0120 </papid>fuchi and takagi, 1998) <papid> P98-1068 </papid>or pre-segmented training data (nagata, 1994; <papid> C94-1032 </papid>papa: georgiou, 1994; <papid> H94-1054 </papid>nagata, 1996<papid> W96-0205 </papid>a; kashioka et al , 1998; <papid> P98-1108 </papid>mori and nagao, 1998).</prevsent>
</prevsection>
<citsent citstr=" W96-0113 ">
other approaches bootstrap from an initial segmentation provided by baseline algorithm such as juman (matsukawa et al ., 1993; <papid> H93-1045 </papid>yamamoto, 1996).<papid> W96-0113 </papid></citsent>
<aftsection>
<nextsent>unsupervised, non-lexicon-based methods for japanese segmentation doexist, but they often have limited applicability.
</nextsent>
<nextsent>both tomokiyo andries (1997) and teller and batchelder (1994) explicitly avoid working with kanji charactes.
</nextsent>
<nextsent>takeda and fujisaki (1987) propose the short unit model, type of hidden markov model with linguistically- determined topology, to segment kanji compound words.
</nextsent>
<nextsent>however, their method does not handle three-character stem words or single-character stem words with affixes, both of which often occur in proper nouns.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A912">
<title id=" A00-2032.xml">mostly unsupervised statistical segmentation of japanese applications to kanji </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>our algorithm, on the hand, is fundamen-tally different in that it incorporates no explicit no-tion of word, but only  sees  locations between characters.
</prevsent>
<prevsent>chinese according to sproat et al  (1996), <papid> J96-3004 </papid>most prior work in chinese segmentation has exploited lexical knowledge bases; indeed, the authors assert that they were aware of only one previously pub-lished instance (the mutual-information method of sproat and shih (1990)) of purely statistical ap- proach.</prevsent>
</prevsection>
<citsent citstr=" P97-1041 ">
in later paper, palmer (1997) <papid> P97-1041 </papid>presents transformation-based algorithm, which requires pre-segmented training data.</citsent>
<aftsection>
<nextsent>to our knowledge, the chinese segmenter most similar to ours is that of sun et al  (1998).<papid> P98-2206 </papid></nextsent>
<nextsent>they also avoid using lexicon, determining whether given location constitutes word boundary in part by deciding whether the two characters on either side tend to occur together; also, they use thresholds and several types of local minima and maxima to make segmentation decisions.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A913">
<title id=" A00-2032.xml">mostly unsupervised statistical segmentation of japanese applications to kanji </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>chinese according to sproat et al  (1996), <papid> J96-3004 </papid>most prior work in chinese segmentation has exploited lexical knowledge bases; indeed, the authors assert that they were aware of only one previously pub-lished instance (the mutual-information method of sproat and shih (1990)) of purely statistical ap- proach.</prevsent>
<prevsent>in later paper, palmer (1997) <papid> P97-1041 </papid>presents transformation-based algorithm, which requires pre-segmented training data.</prevsent>
</prevsection>
<citsent citstr=" P98-2206 ">
to our knowledge, the chinese segmenter most similar to ours is that of sun et al  (1998).<papid> P98-2206 </papid></citsent>
<aftsection>
<nextsent>they also avoid using lexicon, determining whether given location constitutes word boundary in part by deciding whether the two characters on either side tend to occur together; also, they use thresholds and several types of local minima and maxima to make segmentation decisions.
</nextsent>
<nextsent>however, the statis-tics they use (mutual information and t-score) are more complex than the simple n-gram counts that we employ.
</nextsent>
<nextsent>our preliminary re implementation of their method shows that it does not perform as well as the morphological analyzers on our datasets, al-though we do not want to draw definite conclusions because some aspects of sun et al method seem incomparable to ours.
</nextsent>
<nextsent>we do note, however, that their method incorporates numerical differences between statistics, whereas we only use indicator functions; for example, once we know that one trigram is more common than another, we do not take into account he difference between the two frequencies.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A914">
<title id=" C00-2133.xml">prosody and the resolution of pronominal anaphora </title>
<section> background and related work.  </section>
<citcontext>
<prevsection>
<prevsent>many approaches arc based on notion of attentional foctls.
</prevsent>
<prevsent>entities in attentional focus are highly salient, and pronouns are assumed to refer to tile most salient entity in lhc discourse (el.
</prevsent>
</prevsection>
<citsent citstr=" P98-2204 ">
(brennan el al., 1987; az-zam et ill., 1998; strube, 1998)).<papid> P98-2204 </papid></citsent>
<aftsection>
<nextsent>centering (grosz et al., 1995) <papid> J95-2003 </papid>is i}amework for predicting local attentional focus.</nextsent>
<nextsent>it assumes that tile most salient entity from sen-tence ,3,,_\] that is realized in sentence ,5 ,, is most likely to be pronominal ized in ,3,z. that entity is termed the cb (backward-looking center) of sentence ,5 ,,.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A915">
<title id=" C00-2133.xml">prosody and the resolution of pronominal anaphora </title>
<section> background and related work.  </section>
<citcontext>
<prevsection>
<prevsent>entities in attentional focus are highly salient, and pronouns are assumed to refer to tile most salient entity in lhc discourse (el.
</prevsent>
<prevsent>(brennan el al., 1987; az-zam et ill., 1998; strube, 1998)).<papid> P98-2204 </papid></prevsent>
</prevsection>
<citsent citstr=" J95-2003 ">
centering (grosz et al., 1995) <papid> J95-2003 </papid>is i}amework for predicting local attentional focus.</citsent>
<aftsection>
<nextsent>it assumes that tile most salient entity from sen-tence ,3,,_\] that is realized in sentence ,5 ,, is most likely to be pronominal ized in ,3,z. that entity is termed the cb (backward-looking center) of sentence ,5 ,,.
</nextsent>
<nextsent>finding ille preferred ranking criteria is an active area of research.
</nextsent>
<nextsent>byron and stem (1998) <papid> P98-2241 </papid>adapted this approach, which had previously been applied to text, for spoken dialogs, but wilh linfited st,ccess.</nextsent>
<nextsent>\]n contrast to personal pronouns, demonstratives do not relyon calculalions of salience.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A916">
<title id=" C00-2133.xml">prosody and the resolution of pronominal anaphora </title>
<section> background and related work.  </section>
<citcontext>
<prevsection>
<prevsent>it assumes that tile most salient entity from sen-tence ,3,,_\] that is realized in sentence ,5 ,, is most likely to be pronominal ized in ,3,z. that entity is termed the cb (backward-looking center) of sentence ,5 ,,.
</prevsent>
<prevsent>finding ille preferred ranking criteria is an active area of research.
</prevsent>
</prevsection>
<citsent citstr=" P98-2241 ">
byron and stem (1998) <papid> P98-2241 </papid>adapted this approach, which had previously been applied to text, for spoken dialogs, but wilh linfited st,ccess.</citsent>
<aftsection>
<nextsent>\]n contrast to personal pronouns, demonstratives do not relyon calculalions of salience.
</nextsent>
<nextsent>in fact, linde (1979) found lhat while it was preferred for entities within the 919 current local ocus, that was used for items outside the current focus of attention.
</nextsent>
<nextsent>passonneau (1989) <papid> P89-1007 </papid>showed that personal and demonstrative pronouns are used in contrasting situations: personal pronouns are preferred when both the pronoun and its antecedent are in sub-ject position, while demonstrative pronouns are preferred when either the pronoun or its antecedent is not ill sub-ject position.</nextsent>
<nextsent>she also found that personal pronouns tend to co-specify with pronouns or base noun phrases; the more clause- or seutence-likc the antecedent, he more likely the speaker is to choose demonstrative pronoun.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A917">
<title id=" C00-2133.xml">prosody and the resolution of pronominal anaphora </title>
<section> background and related work.  </section>
<citcontext>
<prevsection>
<prevsent>\]n contrast to personal pronouns, demonstratives do not relyon calculalions of salience.
</prevsent>
<prevsent>in fact, linde (1979) found lhat while it was preferred for entities within the 919 current local ocus, that was used for items outside the current focus of attention.
</prevsent>
</prevsection>
<citsent citstr=" P89-1007 ">
passonneau (1989) <papid> P89-1007 </papid>showed that personal and demonstrative pronouns are used in contrasting situations: personal pronouns are preferred when both the pronoun and its antecedent are in sub-ject position, while demonstrative pronouns are preferred when either the pronoun or its antecedent is not ill sub-ject position.</citsent>
<aftsection>
<nextsent>she also found that personal pronouns tend to co-specify with pronouns or base noun phrases; the more clause- or seutence-likc the antecedent, he more likely the speaker is to choose demonstrative pronoun.
</nextsent>
<nextsent>pronoun resolution algoritlnns tend not to cover demonstratives.
</nextsent>
<nextsent>notable exceptions are webber model for discotn se deixis (webbcr, 1991) and the model de-veloped for spoken dialog by eekert and strube (1999).
</nextsent>
<nextsent>this algorithm encompasses both personal and delnon- strat ive pronouns and exploits their contrastive usage pat-terns, relying on syntactic lues and verb subcategoriza- tions as input.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A920">
<title id=" C00-2095.xml">a formalism for universal segmentation of text </title>
<section> processing  document.  </section>
<citcontext>
<prevsection>
<prevsent>ambiguity is central issue when talking about segmentation.
</prevsent>
<prevsent>tile absence or ambiguity of word separators can lead to multiple segmen- tations, and more than one of them can have meaning.
</prevsent>
</prevsection>
<citsent citstr=" J96-3004 ">
as (sproat et al, 1996) <papid> J96-3004 </papid>testify, several native chinese speakers do not always agree on one unique tokenization forgiven sentence.</citsent>
<aftsection>
<nextsent>th~nks to the use of item graphs, sumo can handle ambiguity efficiently.
</nextsent>
<nextsent>why try to fully disambiguate tokenization when there is no agreement on single best solution?
</nextsent>
<nextsent>moreover:, segmentation is usually just basic step of pro-cessing in an nlp system, and some decisions may need more information than what set- reenter is able to provide.
</nextsent>
<nextsent>an uninformed choice at this stage can affect the next stages in neg-ative way.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A921">
<title id=" C00-2095.xml">a formalism for universal segmentation of text </title>
<section> processing  document.  </section>
<citcontext>
<prevsection>
<prevsent>in the latter case, the orig-inal graph is not modified, and its transformed counterpart is only accessible through the rela-tion.
</prevsent>
<prevsent>transformation functions allow to control the flow of the process, using looping and condition- sis.
</prevsent>
</prevsection>
<citsent citstr=" C94-1071 ">
an important implication is that same resource can be applied iteratively; as shown by (roche, 1994:) <papid> C94-1071 </papid>this feature allows to implement segmentation models much more powerful than simple regular languages (see section 3.3 for an example).</citsent>
<aftsection>
<nextsent>another consequence is that sumo application consists of one big transformation function returning the completed sumo struc-ture as result.
</nextsent>
<nextsent>658
</nextsent>
<nextsent>3.1 maximum token ization.
</nextsent>
<nextsent>some cla.ssic heuristics for tokenization a.re classified 1) 3, (g i% 1997) under the collective monil er of mare\]mum tokenization.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A922">
<title id=" A94-1006.xml">ter might identifying and translating technical terminology </title>
<section> terminology: an application for.  </section>
<citcontext>
<prevsection>
<prevsent>natural language technology the statistical corpus-based renaissance in compu-tational linguistics has produced number of in-teresting technologies, including part-of-speech tag-ging and bilingual word alignment.
</prevsent>
<prevsent>unfortunately, these technologies are still not as widely deployed in practical applications as they might be.
</prevsent>
</prevsection>
<citsent citstr=" P93-1003 ">
part-of- speech taggers are used in few applications, such as speech synthesis (sproat et al, 1992) and ques-tion answering (kupiec, 1993<papid> P93-1003 </papid>b).</citsent>
<aftsection>
<nextsent>word alignment is newer, found only in few places (gale and church, 1991<papid> P91-1023 </papid>a; brown et al, 1993; <papid> J93-2003 </papid>dagan et al, 1993).<papid> W93-0301 </papid></nextsent>
<nextsent>it is used at ibm for estimating parameters of their statistical machine translation prototype (brown et *author current address: dept. of mathematics and computer science, bar ilan university, ramat gan 52900, israel.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A924">
<title id=" A94-1006.xml">ter might identifying and translating technical terminology </title>
<section> terminology: an application for.  </section>
<citcontext>
<prevsection>
<prevsent>unfortunately, these technologies are still not as widely deployed in practical applications as they might be.
</prevsent>
<prevsent>part-of- speech taggers are used in few applications, such as speech synthesis (sproat et al, 1992) and ques-tion answering (kupiec, 1993<papid> P93-1003 </papid>b).</prevsent>
</prevsection>
<citsent citstr=" P91-1023 ">
word alignment is newer, found only in few places (gale and church, 1991<papid> P91-1023 </papid>a; brown et al, 1993; <papid> J93-2003 </papid>dagan et al, 1993).<papid> W93-0301 </papid></citsent>
<aftsection>
<nextsent>it is used at ibm for estimating parameters of their statistical machine translation prototype (brown et *author current address: dept. of mathematics and computer science, bar ilan university, ramat gan 52900, israel.
</nextsent>
<nextsent>al., 1993).
</nextsent>
<nextsent>we suggest hat part of speech tagging and word alignment could have an important role in glossary construction for translation.
</nextsent>
<nextsent>glossaries are extremely important for transla-tion.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A932">
<title id=" A94-1006.xml">ter might identifying and translating technical terminology </title>
<section> terminology: an application for.  </section>
<citcontext>
<prevsection>
<prevsent>unfortunately, these technologies are still not as widely deployed in practical applications as they might be.
</prevsent>
<prevsent>part-of- speech taggers are used in few applications, such as speech synthesis (sproat et al, 1992) and ques-tion answering (kupiec, 1993<papid> P93-1003 </papid>b).</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
word alignment is newer, found only in few places (gale and church, 1991<papid> P91-1023 </papid>a; brown et al, 1993; <papid> J93-2003 </papid>dagan et al, 1993).<papid> W93-0301 </papid></citsent>
<aftsection>
<nextsent>it is used at ibm for estimating parameters of their statistical machine translation prototype (brown et *author current address: dept. of mathematics and computer science, bar ilan university, ramat gan 52900, israel.
</nextsent>
<nextsent>al., 1993).
</nextsent>
<nextsent>we suggest hat part of speech tagging and word alignment could have an important role in glossary construction for translation.
</nextsent>
<nextsent>glossaries are extremely important for transla-tion.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A935">
<title id=" A94-1006.xml">ter might identifying and translating technical terminology </title>
<section> terminology: an application for.  </section>
<citcontext>
<prevsection>
<prevsent>unfortunately, these technologies are still not as widely deployed in practical applications as they might be.
</prevsent>
<prevsent>part-of- speech taggers are used in few applications, such as speech synthesis (sproat et al, 1992) and ques-tion answering (kupiec, 1993<papid> P93-1003 </papid>b).</prevsent>
</prevsection>
<citsent citstr=" W93-0301 ">
word alignment is newer, found only in few places (gale and church, 1991<papid> P91-1023 </papid>a; brown et al, 1993; <papid> J93-2003 </papid>dagan et al, 1993).<papid> W93-0301 </papid></citsent>
<aftsection>
<nextsent>it is used at ibm for estimating parameters of their statistical machine translation prototype (brown et *author current address: dept. of mathematics and computer science, bar ilan university, ramat gan 52900, israel.
</nextsent>
<nextsent>al., 1993).
</nextsent>
<nextsent>we suggest hat part of speech tagging and word alignment could have an important role in glossary construction for translation.
</nextsent>
<nextsent>glossaries are extremely important for transla-tion.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A938">
<title id=" A94-1006.xml">ter might identifying and translating technical terminology </title>
<section> terminology: an application for.  </section>
<citcontext>
<prevsection>
<prevsent>primarily, it can sup-port customization machine translation (mt) lex-icons to new domain.
</prevsent>
<prevsent>in fact, the arguments for constructing job-specific glossary for human-based translation may hold equally well for an mt-based process, emphasizing the need for productivity tool.
</prevsent>
</prevsection>
<citsent citstr=" P88-1025 ">
the monolingual component of termigm can be used to construct erminology lists in other ap-plications, such as technical writing, book indexing, hypertext linking, natural anguage interfaces, text categorization and indexing in digital libraries and information retrieval (salton, 1988; <papid> P88-1025 </papid>cherry, 1990; harding, 1982; bourigault, 1992; <papid> C92-3150 </papid>damerau, 1993), while the bilingual component can be useful for in-formation retrieval in multilingual text collections (landauer and littman, 1990).</citsent>
<aftsection>
<nextsent>for part-of-speech tagging although part-of-speech taggers have been around for while, there are relatively few practical appli-cations of this technology.
</nextsent>
<nextsent>the monolingual task appears to be an excellent candidate.
</nextsent>
<nextsent>as has been noticed elsewhere (bourigault, 1992; <papid> C92-3150 </papid>justeson and katz, 1993), most technical terms can be found by looking for multiword noun phrases that satisfy rather estricted set of syntactic patterns.</nextsent>
<nextsent>we follow justeson and katz (1993) who emphasize the impor-tance of term frequency in selecting ood candidate terms.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A939">
<title id=" A94-1006.xml">ter might identifying and translating technical terminology </title>
<section> terminology: an application for.  </section>
<citcontext>
<prevsection>
<prevsent>primarily, it can sup-port customization machine translation (mt) lex-icons to new domain.
</prevsent>
<prevsent>in fact, the arguments for constructing job-specific glossary for human-based translation may hold equally well for an mt-based process, emphasizing the need for productivity tool.
</prevsent>
</prevsection>
<citsent citstr=" C92-3150 ">
the monolingual component of termigm can be used to construct erminology lists in other ap-plications, such as technical writing, book indexing, hypertext linking, natural anguage interfaces, text categorization and indexing in digital libraries and information retrieval (salton, 1988; <papid> P88-1025 </papid>cherry, 1990; harding, 1982; bourigault, 1992; <papid> C92-3150 </papid>damerau, 1993), while the bilingual component can be useful for in-formation retrieval in multilingual text collections (landauer and littman, 1990).</citsent>
<aftsection>
<nextsent>for part-of-speech tagging although part-of-speech taggers have been around for while, there are relatively few practical appli-cations of this technology.
</nextsent>
<nextsent>the monolingual task appears to be an excellent candidate.
</nextsent>
<nextsent>as has been noticed elsewhere (bourigault, 1992; <papid> C92-3150 </papid>justeson and katz, 1993), most technical terms can be found by looking for multiword noun phrases that satisfy rather estricted set of syntactic patterns.</nextsent>
<nextsent>we follow justeson and katz (1993) who emphasize the impor-tance of term frequency in selecting ood candidate terms.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A943">
<title id=" A94-1006.xml">ter might identifying and translating technical terminology </title>
<section> monolingual task: an application.  </section>
<citcontext>
<prevsection>
<prevsent>they can save an enormous amount of time over the current practice of reading the document to be translated, focusing on tables, figures, index, table of contents and so on, and writ-ing down terms that happen to catch the translator eye.
</prevsent>
<prevsent>this current practice is very laborious and runs the risk of missing many important terms.
</prevsent>
</prevsection>
<citsent citstr=" A88-1019 ">
ter might uses part of speech tagger (church, 1988) <papid> A88-1019 </papid>to identify list of candidate terms which is then filtered by manual pass.</citsent>
<aftsection>
<nextsent>we have found, however, that the manual pass dominates the cost of the monolingual task, and consequently, we have tried to design an interactive user interface (see fig-ure 1) that minimizes the burden on the expert er- minologist.
</nextsent>
<nextsent>the terminologist is presented with list of candidate terms, and corrects the list with minimum number of keystrokes.
</nextsent>
<nextsent>the interface is designed to make it easy for the expert to pull up evidence from relevant concordance lines to help identify incorrect candidates as well as terms that are missing from the list.
</nextsent>
<nextsent>a single key-press copies the current candidate term, or the content of any marked emacs region, into the upper-left screen.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A945">
<title id=" A94-1006.xml">ter might identifying and translating technical terminology </title>
<section> monolingual task: an application.  </section>
<citcontext>
<prevsection>
<prevsent>36 2.3 comparison with related work.
</prevsent>
<prevsent>alternative proposals are likely to miss important but infrequent terms/translations such as  format disk dialog box  and  label disk dialog box  which occur just once.
</prevsent>
</prevsection>
<citsent citstr=" J90-1003 ">
in particular, mutual information (church and hanks, 1990; <papid> J90-1003 </papid>wu and su, 1993) and other statistical methods uch as (smadja, 1993) <papid> J93-1007 </papid>and frequency-based methods uch as (justeson and katz, 1993) exclude infrequent phrases because they tend to introduce too much noise.</citsent>
<aftsection>
<nextsent>we have found that frequent headwords are likely to generate number of terms, and are therefore more important for the glossary (a  productivity  criterion).
</nextsent>
<nextsent>con-sider the frequent head word box.
</nextsent>
<nextsent>in the microsoft windows manual, for example, almost any type of box is technical term.
</nextsent>
<nextsent>by sorting on the frequency of the headword, we have been able to find many infrequent erms, and have not had too much of problem with noise (at least for common head- words).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A946">
<title id=" A94-1006.xml">ter might identifying and translating technical terminology </title>
<section> monolingual task: an application.  </section>
<citcontext>
<prevsection>
<prevsent>36 2.3 comparison with related work.
</prevsent>
<prevsent>alternative proposals are likely to miss important but infrequent terms/translations such as  format disk dialog box  and  label disk dialog box  which occur just once.
</prevsent>
</prevsection>
<citsent citstr=" J93-1007 ">
in particular, mutual information (church and hanks, 1990; <papid> J90-1003 </papid>wu and su, 1993) and other statistical methods uch as (smadja, 1993) <papid> J93-1007 </papid>and frequency-based methods uch as (justeson and katz, 1993) exclude infrequent phrases because they tend to introduce too much noise.</citsent>
<aftsection>
<nextsent>we have found that frequent headwords are likely to generate number of terms, and are therefore more important for the glossary (a  productivity  criterion).
</nextsent>
<nextsent>con-sider the frequent head word box.
</nextsent>
<nextsent>in the microsoft windows manual, for example, almost any type of box is technical term.
</nextsent>
<nextsent>by sorting on the frequency of the headword, we have been able to find many infrequent erms, and have not had too much of problem with noise (at least for common head- words).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A947">
<title id=" A94-1006.xml">ter might identifying and translating technical terminology </title>
<section> monolingual task: an application.  </section>
<citcontext>
<prevsection>
<prevsent>their method thus yields some incorrect noun phrases that will not be proposed by tagger, but on the other hand does not miss noun phrases that may be missed due to tagging errors.
</prevsent>
<prevsent>word l ignment 3.1 sentence and word alignment.
</prevsent>
</prevsection>
<citsent citstr=" P91-1034 ">
bilingual alignment methods (warwick et al, 1990; brown et al, 1991<papid> P91-1034 </papid>a; brown et al, 1993; <papid> J93-2003 </papid>gale and church, 1991<papid> P91-1023 </papid>b; gale and church, 1991<papid> P91-1023 </papid>a; kay and roscheisen, 1993; <papid> J93-1006 </papid>simard et al, 1992; church, 1993; <papid> P93-1001 </papid>kupiec, 1993<papid> P93-1003 </papid>a; matsumoto et al, 1993; <papid> P93-1004 </papid>dagan et al, 1993).<papid> W93-0301 </papid></citsent>
<aftsection>
<nextsent>have been used in statistical machine transla-tion (brown et al, 1990), <papid> J90-2002 </papid>terminology research and translation aids (isabelle, 1992; ogden and gonza-les, 1993; vander eijk, 1993), bilingual lexicography (klavans and tzoukermann, 1990; smadja, 1992), word-sense disambiguation (brown et al, 1991<papid> P91-1034 </papid>b; gale et al, 1992) and information retrieval in multilingual environment (landauer and littman, 1990).</nextsent>
<nextsent>most alignment work was concerned with align-ment at the sentence vel.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A970">
<title id=" A94-1006.xml">ter might identifying and translating technical terminology </title>
<section> monolingual task: an application.  </section>
<citcontext>
<prevsection>
<prevsent>their method thus yields some incorrect noun phrases that will not be proposed by tagger, but on the other hand does not miss noun phrases that may be missed due to tagging errors.
</prevsent>
<prevsent>word l ignment 3.1 sentence and word alignment.
</prevsent>
</prevsection>
<citsent citstr=" J93-1006 ">
bilingual alignment methods (warwick et al, 1990; brown et al, 1991<papid> P91-1034 </papid>a; brown et al, 1993; <papid> J93-2003 </papid>gale and church, 1991<papid> P91-1023 </papid>b; gale and church, 1991<papid> P91-1023 </papid>a; kay and roscheisen, 1993; <papid> J93-1006 </papid>simard et al, 1992; church, 1993; <papid> P93-1001 </papid>kupiec, 1993<papid> P93-1003 </papid>a; matsumoto et al, 1993; <papid> P93-1004 </papid>dagan et al, 1993).<papid> W93-0301 </papid></citsent>
<aftsection>
<nextsent>have been used in statistical machine transla-tion (brown et al, 1990), <papid> J90-2002 </papid>terminology research and translation aids (isabelle, 1992; ogden and gonza-les, 1993; vander eijk, 1993), bilingual lexicography (klavans and tzoukermann, 1990; smadja, 1992), word-sense disambiguation (brown et al, 1991<papid> P91-1034 </papid>b; gale et al, 1992) and information retrieval in multilingual environment (landauer and littman, 1990).</nextsent>
<nextsent>most alignment work was concerned with align-ment at the sentence vel.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A971">
<title id=" A94-1006.xml">ter might identifying and translating technical terminology </title>
<section> monolingual task: an application.  </section>
<citcontext>
<prevsection>
<prevsent>their method thus yields some incorrect noun phrases that will not be proposed by tagger, but on the other hand does not miss noun phrases that may be missed due to tagging errors.
</prevsent>
<prevsent>word l ignment 3.1 sentence and word alignment.
</prevsent>
</prevsection>
<citsent citstr=" P93-1001 ">
bilingual alignment methods (warwick et al, 1990; brown et al, 1991<papid> P91-1034 </papid>a; brown et al, 1993; <papid> J93-2003 </papid>gale and church, 1991<papid> P91-1023 </papid>b; gale and church, 1991<papid> P91-1023 </papid>a; kay and roscheisen, 1993; <papid> J93-1006 </papid>simard et al, 1992; church, 1993; <papid> P93-1001 </papid>kupiec, 1993<papid> P93-1003 </papid>a; matsumoto et al, 1993; <papid> P93-1004 </papid>dagan et al, 1993).<papid> W93-0301 </papid></citsent>
<aftsection>
<nextsent>have been used in statistical machine transla-tion (brown et al, 1990), <papid> J90-2002 </papid>terminology research and translation aids (isabelle, 1992; ogden and gonza-les, 1993; vander eijk, 1993), bilingual lexicography (klavans and tzoukermann, 1990; smadja, 1992), word-sense disambiguation (brown et al, 1991<papid> P91-1034 </papid>b; gale et al, 1992) and information retrieval in multilingual environment (landauer and littman, 1990).</nextsent>
<nextsent>most alignment work was concerned with align-ment at the sentence vel.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A974">
<title id=" A94-1006.xml">ter might identifying and translating technical terminology </title>
<section> monolingual task: an application.  </section>
<citcontext>
<prevsection>
<prevsent>their method thus yields some incorrect noun phrases that will not be proposed by tagger, but on the other hand does not miss noun phrases that may be missed due to tagging errors.
</prevsent>
<prevsent>word l ignment 3.1 sentence and word alignment.
</prevsent>
</prevsection>
<citsent citstr=" P93-1004 ">
bilingual alignment methods (warwick et al, 1990; brown et al, 1991<papid> P91-1034 </papid>a; brown et al, 1993; <papid> J93-2003 </papid>gale and church, 1991<papid> P91-1023 </papid>b; gale and church, 1991<papid> P91-1023 </papid>a; kay and roscheisen, 1993; <papid> J93-1006 </papid>simard et al, 1992; church, 1993; <papid> P93-1001 </papid>kupiec, 1993<papid> P93-1003 </papid>a; matsumoto et al, 1993; <papid> P93-1004 </papid>dagan et al, 1993).<papid> W93-0301 </papid></citsent>
<aftsection>
<nextsent>have been used in statistical machine transla-tion (brown et al, 1990), <papid> J90-2002 </papid>terminology research and translation aids (isabelle, 1992; ogden and gonza-les, 1993; vander eijk, 1993), bilingual lexicography (klavans and tzoukermann, 1990; smadja, 1992), word-sense disambiguation (brown et al, 1991<papid> P91-1034 </papid>b; gale et al, 1992) and information retrieval in multilingual environment (landauer and littman, 1990).</nextsent>
<nextsent>most alignment work was concerned with align-ment at the sentence vel.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A978">
<title id=" A94-1006.xml">ter might identifying and translating technical terminology </title>
<section> monolingual task: an application.  </section>
<citcontext>
<prevsection>
<prevsent>word l ignment 3.1 sentence and word alignment.
</prevsent>
<prevsent>bilingual alignment methods (warwick et al, 1990; brown et al, 1991<papid> P91-1034 </papid>a; brown et al, 1993; <papid> J93-2003 </papid>gale and church, 1991<papid> P91-1023 </papid>b; gale and church, 1991<papid> P91-1023 </papid>a; kay and roscheisen, 1993; <papid> J93-1006 </papid>simard et al, 1992; church, 1993; <papid> P93-1001 </papid>kupiec, 1993<papid> P93-1003 </papid>a; matsumoto et al, 1993; <papid> P93-1004 </papid>dagan et al, 1993).<papid> W93-0301 </papid></prevsent>
</prevsection>
<citsent citstr=" J90-2002 ">
have been used in statistical machine transla-tion (brown et al, 1990), <papid> J90-2002 </papid>terminology research and translation aids (isabelle, 1992; ogden and gonza-les, 1993; vander eijk, 1993), bilingual lexicography (klavans and tzoukermann, 1990; smadja, 1992), word-sense disambiguation (brown et al, 1991<papid> P91-1034 </papid>b; gale et al, 1992) and information retrieval in multilingual environment (landauer and littman, 1990).</citsent>
<aftsection>
<nextsent>most alignment work was concerned with align-ment at the sentence vel.
</nextsent>
<nextsent>algorithms for the more difficult task of word alignment were proposed in (gale and church, 1991<papid> P91-1023 </papid>a; brown et al, 1993; <papid> J93-2003 </papid>da- gan et al, 1993) <papid> W93-0301 </papid>and were applied for parameter es-timation in the ibm statistical machine translation system (brown et al, 1993).<papid> J93-2003 </papid></nextsent>
<nextsent>previously translated texts provide major source of information about technical terms.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A999">
<title id=" A97-1032.xml">layout and language preliminary experiments in assigning logical structure to table cells </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>how- ever, ignoring l-v cohesion always improves recall; these cohesion measures do not help in distinguish-ing between labels and values, or in linking labels with value-sets.
</prevsent>
<prevsent>this will be more of problem when we deal with more complex tables with complex multi-cell labels.
</prevsent>
</prevsection>
<citsent citstr=" E95-1027 ">
in future, we intend to investigate the ef-fect of more sophisticated cohesion measures, includ-ing the use of thesaural information from domain- independent sources and corpus-based knowlege ac-quisition, e.g., (mikheev and finch, 1995), <papid> E95-1027 </papid>which should form better approximations to the super- type/subtype distinction.</citsent>
<aftsection>
<nextsent>combining number of measures, in the kind of framework we have presented here, should allow graceful performance over wide range of domains using as much information as is available, from what- ever source, as well as convenient evaluation of the relative contribution of different sources.
</nextsent>
<nextsent>acknowledgements we acknowledge the support of bicc plc who sup-plied data and funded the first author during most 220 of this work, and of the engineering and phys-ical sciences research council of the uk, who funded the second author under the cisa project (ied4/1/5sls).
</nextsent>


</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A1000">
<title id=" C00-1018.xml">the use of instrumentation in grammar engineering </title>
<section> genre adaptation.  </section>
<citcontext>
<prevsection>
<prevsent>the first consists in deleting un-used disjuncts, while the second uses staged pars-ing process.
</prevsent>
<prevsent>the experiments are only sketched, to indicate the apl)licability of the instrumentation technique, and not to directly compete with other proposals on grmnnmr specialization.
</prevsent>
</prevsection>
<citsent citstr=" P94-1026 ">
for example, the work reported in (rwner and smnuelsson, 1994; samuelsson, 1994) <papid> P94-1026 </papid>diifers from the one presented be- low ill several aspects: they induce grammar from treebank, while propose to mmotate the gram-mar based on all solutions it produces.</citsent>
<aftsection>
<nextsent>no criteria for tree decomposition and category specialization are needed here, and the standard parsing algorithm can be used.
</nextsent>
<nextsent>on the other hand, the efficiency gains are not as big as those reported by (rayner and salnuelsson, 1994).
</nextsent>
<nextsent>5.1 rest i t ing the grammar.
</nextsent>
<nextsent>given large sample of genre, instrunmntation al-lows you to determine the likely constructions of that genre.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A1001">
<title id=" C00-1018.xml">the use of instrumentation in grammar engineering </title>
<section> genre adaptation.  </section>
<citcontext>
<prevsection>
<prevsent>an experiment was conducted with several corpora as detailed in table 2.
</prevsent>
<prevsent>there was some eft oft to cover the corpus hc-de, but no grammar development based on the other corpora.
</prevsent>
</prevsection>
<citsent citstr=" P99-1035 ">
the news-sc corpus is part the corl)us of verb-final sentences used by (beil et al, 1999).<papid> P99-1035 </papid></citsent>
<aftsection>
<nextsent>a training set of 1000 sentences froln each cor-pus was parsed with an instrumented base gram-mar. from the parsing results, the exercised gram-mar disjuncts were extracted and used to construct corl)us-specific reduced grammar.
</nextsent>
<nextsent>the reduced grammars were then used to parse test; set of an- other 1000 sentences dora each corpus.
</nextsent>
<nextsent>tame 3 shows the lmrt ornmnce ilnprovement on the corpora: it gives the size of the grammars in terms of the number of rules (with regular expression right-hand sides and feature annotation), the number of arcs (corresponding to unary or binary rules with dis-junctive feature annotation), and the number of dis- juncls (unary or binary rules with tmique feature annotation).
</nextsent>
<nextsent>the number of mismatches counts the sentences for which the solution(s) obtained iffered fl om those obtained with the base gramnmr, while the number of additi(ms counts the selltellces which {lid not receiw; 1)arse with the base grannnar due to resource limitations (runtinle or memory), but re-ceived one with the reduced granmmr.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A1003">
<title id=" C00-1018.xml">the use of instrumentation in grammar engineering </title>
<section> genre adaptation.  </section>
<citcontext>
<prevsection>
<prevsent>tile differences to the approach described here are clear: starting from the grammar, rather than from treebank, we annotate tile rules, rather than inducing them from scratch.
</prevsent>
<prevsent>we do not need criteria for tree decomposition and category specialization, and we can use the standard parsing algorithm.
</prevsent>
</prevsection>
<citsent citstr=" P96-1030 ">
on the other hand, the efficiency gains are not as big as those reported by (rayner and carter, 1996) (<papid> P96-1030 </papid>but note that we cannot measure ilarsing times alone, so we need to coral)are to their speed-up factor of 10).</citsent>
<aftsection>
<nextsent>and we did not (yet) start from treebank, but froln the raw set of solutions.
</nextsent>
<nextsent>i have 1)resented the adaptation of code instrmnenta- tion to grammar engineering, discussing measures and iml)lementations, and sketching several applica-tions together with preliminary results.
</nextsent>
<nextsent>the main application is to iml)rove grammar and test suite by exl)loring the relation between both of them.
</nextsent>
<nextsent>viewed this way, test suite writing can ben-efit from grammar developnlent because both de-scribe the syntactic onstructions of natural lan-guage.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A1004">
<title id=" A00-1043.xml">sentence reduction for automatic text summarization </title>
<section> sentence  reduction  based  on.  </section>
<citcontext>
<prevsection>
<prevsent>the articles in the corpus are news reports on telecommunication related issues, but they cover wide range of topics, such as law, labor, and company mergers.
</prevsent>
<prevsent>(2) the lexicon.
</prevsent>
</prevsection>
<citsent citstr=" P98-1099 ">
the system also uses large- scale, reusable lexicon we combined from multiple resources (jing and mckeown, 1998).<papid> P98-1099 </papid></citsent>
<aftsection>
<nextsent>the resources that were combined include comlex syntactic dic-tionary (macleod and grishman, 1995), english verb classes and alternations (levin, 1993), the wordnet lexical database (miller et al, 1990), the brown corpus tagged with wordnet senses (miller et al, 1993).
</nextsent>
<nextsent>the lexicon includes subcategoriza- tions for over 5,000 verbs.
</nextsent>
<nextsent>this information is used to identify the obligatory arguments of verb phrases.
</nextsent>
<nextsent>(3) the wordnet lexical database.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A1005">
<title id=" C00-2101.xml">learning semantic level information extraction rules by type oriented ilp </title>
<section> int roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>for example, umass/muc-3 needed about 1,500 person-hours of highly skilled labor to build the ie rules and represent them as dictionary (lehnert, 1992).
</prevsent>
<prevsent>all the rules must be reconstructed rom scratch when the target domain is changed.
</prevsent>
</prevsection>
<citsent citstr=" W97-1002 ">
to cope with this problem, some pioneers have studied methods for learning information extraction rules (riloff,1996; soderland ctal., 1.995; kim et el., 1995; huffman, 1996; califf and mooney, 1997).<papid> W97-1002 </papid></citsent>
<aftsection>
<nextsent>along these lines, our ap- preach is to a.pply an inductive logic program- ruing (ilp) (muggleton, 1991)system to the learning of ie rules, where information is ex-tracted from semantic representations of news articles.
</nextsent>
<nextsent>the ilp system that we employed is type-oriented ilp system i{\]\]b + (sasaki and haruno, 1997), which can efficiently and effec-tively h~mdle type (or sort) information in train-ing data.
</nextsent>
<nextsent>this section describes our approach to ie tasks.
</nextsent>
<nextsent>figure 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A1006">
<title id=" C00-2101.xml">learning semantic level information extraction rules by type oriented ilp </title>
<section> nlp  resources and tools.  </section>
<citcontext>
<prevsection>
<prevsent>extracted intbrmation is filled into the template slots.
</prevsent>
<prevsent>3.1 the semantic attribute system.
</prevsent>
</prevsection>
<citsent citstr=" P99-1062 ">
we used the semantic attribute system of  ge taikei - - japanese lexicon  (lkehara el el., 1997a; kurohashi and sakai, 1.999) <papid> P99-1062 </papid>compiled by the ntt communication science laboratories for japanese-to-english machine translation system, alt- /e (ikehm:a et al, 1994).</citsent>
<aftsection>
<nextsent>these- mantic attribute system is sort of hierarchical concept thesaurus represented as tree struc-ture in which each node is called semantic cateqory.
</nextsent>
<nextsent>an edge in the tree represents an is_a or has_a relation between two categories.
</nextsent>
<nextsent>the semantic attribute system is 11.2 levels deep and 698 semantic representation new article       \[\]\]\] s?~chy yze ~ analyze rolease(cl,pl) articles sentences announce(cl,dl) ~kackgrou d anal .... nzwledge / \[e rules ~ representation sentences semantic .ooitive ..... i re,oa x. , ii answer templates filled company: c2 ~7 a;p -  iyrule~= ~   ~ by hand draotauotd..~2 to semantic - ,opreseot t,on figure l: l/lock diagram of ie using im ) contains about 3,000 sema.ntic ategory nodes.
</nextsent>
<nextsent>more than 300,000 japanese words a.re linked to the category nodes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A1007">
<title id=" A00-3007.xml">word sense disambiguation for cross language information retrieval </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>to determine the sense of word, wsd algorithm typically uses the context of the ambiguous word, external resources such as machine-readable dictionaries, or combination of both.
</prevsent>
<prevsent>although dictionaries provide useful word sense information and thesauri provide additional information about relatiomhips between words, they lack pragmatic information as can be found in corpora.
</prevsent>
</prevsection>
<citsent citstr=" J98-1001 ">
corpora contain examples of words that enable the development of statistical models of word senses and their contexts (ide and veronis, 1998; <papid> J98-1001 </papid>leacock and chodorow, 1998).</citsent>
<aftsection>
<nextsent>there are two general problems with using corpora however; 1) corpora typically do not come pre-tagged with manually disambiguated senses, and 2) corpora are often not large nor diverse nough for all senses of word to appear often enough for reliable statistical models (data sparseness).
</nextsent>
<nextsent>although researchers have tried sense-tagging corpora automatically by using either supervised or unsupervised training methods, we have adopted wsd algorithm which avoids the necessity for sense-tagged training corpus.
</nextsent>
<nextsent>l please note that he disambiguation research described inthis paper has not yet been extended to multiple language areas.
</nextsent>
<nextsent>35 p(synsetlcontext(w)) = p(context(w) synset) p(synset) p(context(w)) (i) the problem of data sparseness usually solved by using either smoothing methods, class- based methods, or by relying on similarity-based methods between words and co-occurrence data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A1008">
<title id=" A00-3007.xml">word sense disambiguation for cross language information retrieval </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>appropriate word classes can be formed by synsets or groups of synsets.
</prevsent>
<prevsent>the evidence of certain sense (synset) is then no longer dependent on one word but on all the members of particular synset.
</prevsent>
</prevsection>
<citsent citstr=" C92-2070 ">
yarowsky (1992) <papid> C92-2070 </papid>used rogets thesaurus categories as classes for wsd.</citsent>
<aftsection>
<nextsent>his approach was based on selecting the most likely roget category for nouns given their context of 50 words on either side.
</nextsent>
<nextsent>when any of the category indicator words appeared in the context of an ambiguous word, the indicator weights for each category were summed to determine the most likely category.
</nextsent>
<nextsent>the category with the largest sum was then selected.
</nextsent>
<nextsent>a similar approach to that of yarowsky was followed by cheng and wil lensky (1997) who used training matrix of associations of words with certain category.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A1009">
<title id=" A00-3007.xml">word sense disambiguation for cross language information retrieval </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>for each document in collection read in noun stem from for each synset in which occurs get the column in the association matrix that corresponds tos if the column already exists; create anew column for otherwise for each word stem appearing in the 100-word window around get the row in that corresponds toj if the row already exists; create anew row for otherwise add distance-adjusted weight m\[a\]\[b\] figure 1: wsd algorithm: the training phase set value = 1 for each word to be disambiguated get synsets of for each synset ofw for each wi in the context ofw (within the 100-window around w) calculate pc(wilx) value *= ( 1 - pc(wilx)) p(context(w)lx) = 1 - value calculate pc(x) p(xlcontext(w)) =p~(x)* p(eontext(w)lx) display ranked list of the synsets arranged according to their p(xlcontext(w)) in decreasing order figure 2: wsd algorithm: the sense prediction phase 37
</prevsent>
<prevsent>as suggested by the wsd literature, evaluation of word sense disambiguation systems is not yet standardized (resnik and yarowsky, 1997).
</prevsent>
</prevsection>
<citsent citstr=" C96-1005 ">
some wsd evaluations have been done using the brown corpus as training and testing resources and comparing the results against semcor 3, the sense-tagged version of the brown corpus (agirre and rigau, 1996; <papid> C96-1005 </papid>gonzalo et al, 1998).<papid> W98-0705 </papid></citsent>
<aftsection>
<nextsent>others have used common test suites such as the 2094-word line data of leacock et al (1993).<papid> H93-1051 </papid></nextsent>
<nextsent>still others have tended to use their own metrics.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A1010">
<title id=" A00-3007.xml">word sense disambiguation for cross language information retrieval </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>for each document in collection read in noun stem from for each synset in which occurs get the column in the association matrix that corresponds tos if the column already exists; create anew column for otherwise for each word stem appearing in the 100-word window around get the row in that corresponds toj if the row already exists; create anew row for otherwise add distance-adjusted weight m\[a\]\[b\] figure 1: wsd algorithm: the training phase set value = 1 for each word to be disambiguated get synsets of for each synset ofw for each wi in the context ofw (within the 100-window around w) calculate pc(wilx) value *= ( 1 - pc(wilx)) p(context(w)lx) = 1 - value calculate pc(x) p(xlcontext(w)) =p~(x)* p(eontext(w)lx) display ranked list of the synsets arranged according to their p(xlcontext(w)) in decreasing order figure 2: wsd algorithm: the sense prediction phase 37
</prevsent>
<prevsent>as suggested by the wsd literature, evaluation of word sense disambiguation systems is not yet standardized (resnik and yarowsky, 1997).
</prevsent>
</prevsection>
<citsent citstr=" W98-0705 ">
some wsd evaluations have been done using the brown corpus as training and testing resources and comparing the results against semcor 3, the sense-tagged version of the brown corpus (agirre and rigau, 1996; <papid> C96-1005 </papid>gonzalo et al, 1998).<papid> W98-0705 </papid></citsent>
<aftsection>
<nextsent>others have used common test suites such as the 2094-word line data of leacock et al (1993).<papid> H93-1051 </papid></nextsent>
<nextsent>still others have tended to use their own metrics.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A1011">
<title id=" A00-3007.xml">word sense disambiguation for cross language information retrieval </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>as suggested by the wsd literature, evaluation of word sense disambiguation systems is not yet standardized (resnik and yarowsky, 1997).
</prevsent>
<prevsent>some wsd evaluations have been done using the brown corpus as training and testing resources and comparing the results against semcor 3, the sense-tagged version of the brown corpus (agirre and rigau, 1996; <papid> C96-1005 </papid>gonzalo et al, 1998).<papid> W98-0705 </papid></prevsent>
</prevsection>
<citsent citstr=" H93-1051 ">
others have used common test suites such as the 2094-word line data of leacock et al (1993).<papid> H93-1051 </papid></citsent>
<aftsection>
<nextsent>still others have tended to use their own metrics.
</nextsent>
<nextsent>we chose an evaluation with user- based component that allowed ranked list of sense selection for each target word and enabled comprehensive comparison between automatic and manual wsd results.
</nextsent>
<nextsent>in addition we wanted to base the disambiguation matrix on corpus that we use for retrieval.
</nextsent>
<nextsent>this approach allows for much richer evaluation than simple hit-or- miss test.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A1012">
<title id=" C00-2123.xml">word reordering and dpbased search in statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>pr(c~) is the language model of tim target language, whereas pr(fi le*l) is the transla-tion model.
</prevsent>
<prevsent>our approach uses word-to-word epen- dencies between source and target words.
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
the model is often further restricted so that each source word is assigned to exactly one target word (brown et al, 1993; <papid> J93-2003 </papid>ney et al, 2000).</citsent>
<aftsection>
<nextsent>these alignment models are similar to the concept of hidden markov models (hmm) in speech recognition.
</nextsent>
<nextsent>the alignment map-ping is --+ = aj from source position to target position = aj.
</nextsent>
<nextsent>the use of this alignment model raises major problems if source word has to be aligned to several target words, e.g. when translat-ing german compound nouns.
</nextsent>
<nextsent>a siulple extension will be used to handle this problem.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A1013">
<title id=" C00-1049.xml">layout and language integrating spatial and linguistic knowledge for layout understanding tasks </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>1 rus and summers ((rus and su,nmers, 1994)) state that  the non-teztual content of documents \[complement\] thetcztual content and should play art equal role .
</prevsent>
<prevsent>this is clearly desirable: textual and spatial properties, as described in tiffs paper, are inter-related and it is in fact highly beneficial to ex-ploit the relationships which exist between them.
</prevsent>
</prevsection>
<citsent citstr=" J91-1002 ">
in 1the term spatim cohesion is motivated by the work on lexical cohesion by morris and hirst ((morris and hirst, 1991)).<papid> J91-1002 </papid></citsent>
<aftsection>
<nextsent>text which is cohesive is text which has quality of unity (p. 21).
</nextsent>
<nextsent>objects which have spatial cohesion have quality of unity indicated by spatial features; in the words of morris and hirst: they  stick together .
</nextsent>
<nextsent>algorithmic terms, this implies implementing solu-tions which use both spatial and linguistic features to detect coherent extual objects its the raw text.
</nextsent>
<nextsent>apt)roaches to tile problem are limited to those ex-ploiting spatial cohesion.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A1014">
<title id=" C00-2116.xml">automatic corpus based thai word extraction with the c45 learning algorithm </title>
<section> previous works.  </section>
<citcontext>
<prevsection>
<prevsent>currently, lexicographers can make use of large corpora and show the convincing results from the experiments over corpora.
</prevsent>
<prevsent>we, therefore, introduce here new efficient method for consistently extracting and identifying list of acceptable thai words.
</prevsent>
</prevsection>
<citsent citstr=" C96-2208 ">
reviewing the previous works on thai word extraction, we found only the work of sornlertlamvanich and tanaka (1996).<papid> C96-2208 </papid></citsent>
<aftsection>
<nextsent>they employed the fiequency of the sorted character n- grams to extract thai open compounds; the strings that experienced significant change of occurrences when their lengths are extended.
</nextsent>
<nextsent>this algorithm reports about 90% accuracy of thai 802 open compound extraction.
</nextsent>
<nextsent>however, the algorithm emphasizes on open compotmd extraction and has to limit tile range of n-gram to 4-20 grams for the computational reason.
</nextsent>
<nextsent>this causes limitation in the size of corpora and efficiency in the extraction.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A1015">
<title id=" C00-2116.xml">automatic corpus based thai word extraction with the c45 learning algorithm </title>
<section> previous works.  </section>
<citcontext>
<prevsection>
<prevsent>if an erroneous tring is extracted, its errors will propagate through the rest of the input :~trings.
</prevsent>
<prevsent>:3 our approach 3.1 the c4.5 learning algorithm.
</prevsent>
</prevsection>
<citsent citstr=" P95-1037 ">
decision tree induction algorithms have been successfully applied for nlp problems such as sentence boundary dismnbiguation (pahner et al 1997), parsing (magerman 1995) <papid> P95-1037 </papid>and word segmentation (mekuavin et al 1997).</citsent>
<aftsection>
<nextsent>we employ the c4.5 (quinhln 1993) decision tree induction program as the learning algorithm for word extraction.
</nextsent>
<nextsent>the induction algorithm proceeds by evaluating content of series of attributes and iteratively building tree fiom the attribute values with the leaves of the decision tree being the value of the goal attribute.
</nextsent>
<nextsent>at each step of learning procedure, the evolving tree is branched on the attribute that pal-titions tile data items with the highest information gain.
</nextsent>
<nextsent>branches will be added until all items in the training set arc classified.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A1016">
<title id=" C00-1040.xml">an integrated model of semantic and conceptual interpretation from dependency structures </title>
<section> introduction.  </section>
<citcontext>
<prevsection>


</prevsection>
<citsent citstr=" E93-1047 ">
the syntax/semanti(:s interface has always t)een matter of com:ern for (:ollstit; lell(:y-l)ased feal; lre grammar theories (of., e.g., creary an(1 pollard ( \ ]9s5) , ~. ~ooix, ( \[989), \ ] )ah l l ) le (1.992), wedekind and kaplan (1993)).<papid> E93-1047 </papid></citsent>
<aftsection>
<nextsent>within the dependen(:y gram- lllal  collllllllllil;y: f3r loss at, te, ntion has 1)een l)ai(t t() l;his tel)it:.
</nextsent>
<nextsent>as (:onse(tuen(:e, ther(~ is no consensus how syntactic del)en(lency structures might l)e a(t- e(tuately transl ()rm(~d into semanti(: interl)rel;aiions (el., naji~:ova (:t987), milward (1992), <papid> C92-4171 </papid>lombardo et al.</nextsent>
<nextsent>(1998) for alt;ernative proposals).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A1017">
<title id=" C00-1040.xml">an integrated model of semantic and conceptual interpretation from dependency structures </title>
<section> introduction.  </section>
<citcontext>
<prevsection>
<prevsent>the syntax/semanti(:s interface has always t)een matter of com:ern for (:ollstit; lell(:y-l)ased feal; lre grammar theories (of., e.g., creary an(1 pollard ( \ ]9s5) , ~. ~ooix, ( \[989), \ ] )ah l l ) le (1.992), wedekind and kaplan (1993)).<papid> E93-1047 </papid></prevsent>
<prevsent>within the dependen(:y gram- lllal  collllllllllil;y: f3r loss at, te, ntion has 1)een l)ai(t t() l;his tel)it:.</prevsent>
</prevsection>
<citsent citstr=" C92-4171 ">
as (:onse(tuen(:e, ther(~ is no consensus how syntactic del)en(lency structures might l)e a(t- e(tuately transl ()rm(~d into semanti(: interl)rel;aiions (el., naji~:ova (:t987), milward (1992), <papid> C92-4171 </papid>lombardo et al.</citsent>
<aftsection>
<nextsent>(1998) for alt;ernative proposals).
</nextsent>
<nextsent>in this paper, we introduce, two-layered inter-pretation model.
</nextsent>
<nextsent>in first; pass, dependency graph structures which result fl om in(:remental parsing are immediately submitted to semantic intcrl)reta- tion process.
</nextsent>
<nextsent>such process is triggered by gen-eral schemata whenever semantically interl)retable subgraph of syntactic dependency gral)h t)ecomes ava.ilable (el.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A1018">
<title id=" C00-1040.xml">an integrated model of semantic and conceptual interpretation from dependency structures </title>
<section> grammar  and concept knowledge.  </section>
<citcontext>
<prevsection>
<prevsent>p ......
</prevsent>
<prevsent>-- deliver-recipient spcc\[ificr\] / l , : . . .
</prevsent>
</prevsection>
<citsent citstr=" A00-2043 ">
~ figure 3: relating grammatical ( eft box) and con-ceptual i(nowledge (right box) each lexical ambiguity is processed independent.ly within set)arate (:ontext partitions of the km)wledge base (romacker and hahn, 2000<papid> A00-2043 </papid>a).</citsent>
<aftsection>
<nextsent>ii1 the parse tree from figure 2, we can distinguish lexical nodes that have conceptual correlate (e.g.,  fcstplatte  relating to hal{d-disk,  gelicfert   re-lating to deiavh;ry) from others that do not have such correlate (e.g.,  mit  (with),  yon  (by)).
</nextsent>
<nextsent>se-mantic interpretation capitalizes oil this distinction in order to tind adequate conceptual relations be-tween the corresl)onding concept, insta.nces: i rect inkage.
</nextsent>
<nextsent>if two word no(tes with (:oncet)- tual correlates are linked by single depen(ten(:y re-lation, direct linkage is given.
</nextsent>
<nextsent>such subgraph can immediately be interpreted in ternis of (:on- ceptual relation licensed by the correspondiitg de- l)endency relation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A1021">
<title id=" C00-1040.xml">an integrated model of semantic and conceptual interpretation from dependency structures </title>
<section> related  work.  </section>
<citcontext>
<prevsection>
<prevsent>this approach as also been adopted in the few explicit attempts at incorporating seman-tic interpretation to dependency grmninar fi mne- work (milward, 1992; <papid> C92-4171 </papid>lombardo et al, 1998).</prevsent>
<prevsent>there are no constraints on how to design and orgmfize this rule set despite those that are imi)lied by the choice of the semantic theory.</prevsent>
</prevsection>
<citsent citstr=" P85-1022 ">
in particular, abstraction mechanisms (going beyond the level of sortal tax- onomies for semantic labels, cf., e.g., creary and pollard (1985)), <papid> P85-1022 </papid>such as property inheritance, de-faults, are lacking.</citsent>
<aftsection>
<nextsent>accordingly, the number of rules increases rapidly and easily reaches orders of sev-eral lmndreds in real-world setting (bean cta . , 1998).
</nextsent>
<nextsent>as an alternative, we provide small set of gencric semantic interpretation schemata (by the order of 10) and conceptual interpretation rules (by the order of 30 for 200 verb concepts) instead of assigning specific interpretation rules to each gram-mar item (in our case, single lexemes), and incor-porate inheritance-based abstraction in the use of these schemata during the intert)retation process in the knowledge base.
</nextsent>
<nextsent>we clearly want to point out that while this rule system covers wide variety of standard syntactic constructions (such as gent- tives, prepositional phrases, various tense and modal forms), it currently does not account fbr quantifica- tional issues (like scope ambiguities) for which en-tirely logic-based approach (charniak and goldman, 1988; <papid> P88-1011 </papid>moore, 1989; <papid> P89-1005 </papid>pereira and pollack, 1991) pro-vide quite sophisticated solutions.</nextsent>
<nextsent>sondheilner et al (1984) and itirst (1988) treat semantic interpretation as direct mapt)ing front syntactic to conceptual rel)resentations.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A1022">
<title id=" C00-1040.xml">an integrated model of semantic and conceptual interpretation from dependency structures </title>
<section> related  work.  </section>
<citcontext>
<prevsection>
<prevsent>accordingly, the number of rules increases rapidly and easily reaches orders of sev-eral lmndreds in real-world setting (bean cta . , 1998).
</prevsent>
<prevsent>as an alternative, we provide small set of gencric semantic interpretation schemata (by the order of 10) and conceptual interpretation rules (by the order of 30 for 200 verb concepts) instead of assigning specific interpretation rules to each gram-mar item (in our case, single lexemes), and incor-porate inheritance-based abstraction in the use of these schemata during the intert)retation process in the knowledge base.
</prevsent>
</prevsection>
<citsent citstr=" P88-1011 ">
we clearly want to point out that while this rule system covers wide variety of standard syntactic constructions (such as gent- tives, prepositional phrases, various tense and modal forms), it currently does not account fbr quantifica- tional issues (like scope ambiguities) for which en-tirely logic-based approach (charniak and goldman, 1988; <papid> P88-1011 </papid>moore, 1989; <papid> P89-1005 </papid>pereira and pollack, 1991) pro-vide quite sophisticated solutions.</citsent>
<aftsection>
<nextsent>sondheilner et al (1984) and itirst (1988) treat semantic interpretation as direct mapt)ing front syntactic to conceptual rel)resentations.
</nextsent>
<nextsent>they also shm:e with us tim representation doinain knowl-edge using kl,-one-style terminological languages, and, hence, they nmke heavy use of property inher-itance (or typing) inechanisms.
</nextsent>
<nextsent>the main diflbrence to our approach lies in the status of the semantic rules.
</nextsent>
<nextsent>sondheimer et al (1984) <papid> P84-1024 </papid>attach single in- terpretatiotl rules to each olc (filler) and, hence, have to provide utterly detailed specifications re-flecting the idiosyncrasies of each semantically rele-vant (role) attachment.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A1023">
<title id=" C00-1040.xml">an integrated model of semantic and conceptual interpretation from dependency structures </title>
<section> related  work.  </section>
<citcontext>
<prevsection>
<prevsent>accordingly, the number of rules increases rapidly and easily reaches orders of sev-eral lmndreds in real-world setting (bean cta . , 1998).
</prevsent>
<prevsent>as an alternative, we provide small set of gencric semantic interpretation schemata (by the order of 10) and conceptual interpretation rules (by the order of 30 for 200 verb concepts) instead of assigning specific interpretation rules to each gram-mar item (in our case, single lexemes), and incor-porate inheritance-based abstraction in the use of these schemata during the intert)retation process in the knowledge base.
</prevsent>
</prevsection>
<citsent citstr=" P89-1005 ">
we clearly want to point out that while this rule system covers wide variety of standard syntactic constructions (such as gent- tives, prepositional phrases, various tense and modal forms), it currently does not account fbr quantifica- tional issues (like scope ambiguities) for which en-tirely logic-based approach (charniak and goldman, 1988; <papid> P88-1011 </papid>moore, 1989; <papid> P89-1005 </papid>pereira and pollack, 1991) pro-vide quite sophisticated solutions.</citsent>
<aftsection>
<nextsent>sondheilner et al (1984) and itirst (1988) treat semantic interpretation as direct mapt)ing front syntactic to conceptual rel)resentations.
</nextsent>
<nextsent>they also shm:e with us tim representation doinain knowl-edge using kl,-one-style terminological languages, and, hence, they nmke heavy use of property inher-itance (or typing) inechanisms.
</nextsent>
<nextsent>the main diflbrence to our approach lies in the status of the semantic rules.
</nextsent>
<nextsent>sondheimer et al (1984) <papid> P84-1024 </papid>attach single in- terpretatiotl rules to each olc (filler) and, hence, have to provide utterly detailed specifications re-flecting the idiosyncrasies of each semantically rele-vant (role) attachment.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A1024">
<title id=" C00-1040.xml">an integrated model of semantic and conceptual interpretation from dependency structures </title>
<section> related  work.  </section>
<citcontext>
<prevsection>
<prevsent>they also shm:e with us tim representation doinain knowl-edge using kl,-one-style terminological languages, and, hence, they nmke heavy use of property inher-itance (or typing) inechanisms.
</prevsent>
<prevsent>the main diflbrence to our approach lies in the status of the semantic rules.
</prevsent>
</prevsection>
<citsent citstr=" P84-1024 ">
sondheimer et al (1984) <papid> P84-1024 </papid>attach single in- terpretatiotl rules to each olc (filler) and, hence, have to provide utterly detailed specifications re-flecting the idiosyncrasies of each semantically rele-vant (role) attachment.</citsent>
<aftsection>
<nextsent>property inherit mme comes only into play when the selection of alternative se-mantic rules is constrained to the one(s) inherited from the most specific case frame.
</nextsent>
<nextsent>in similar way, hirst (1988) uses strong typing at the coueeptual object level only, while we use it simultaneously at the grmnmar and the domain knowledge level for the processing of semantic schemata.
</nextsent>


</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A1025">
<title id=" C00-2140.xml">diasumm flexible summarization of spontaneous dialogues in unrestricted domains </title>
<section> introduction.  </section>
<citcontext>
<prevsection>
<prevsent>grows and demand for r~tl)id browsing, skimming, a.nd a.e- cess of speech data increases.
</prevsent>
<prevsent>another application which particulm:ly pertains to our interest in spo-- ken dialogue summarization would be the generation of meeting minutes for archival purposes a.nd/or to update l)a.rticil)a.nts .joining a.t la.ter stages on qm  progress of the conversa.tion so far.
</prevsent>
</prevsection>
<citsent citstr=" W98-1421 ">
sunmmrization of dialogues within imilcd do-mains ha.s been attempted within the context of the verbmobii, pl:ojcct ( protocol generation , (alexandersson and poller, 1998)) <papid> W98-1421 </papid>or by sri mimi summarizer (kameyama et ~d., 1996).</citsent>
<aftsection>
<nextsent>l{ecent work on spoken language summarization unrestricted domains has focused ahnost exclusively on broad-cast news, mostly due to the spoken hmguage track of recent trec evaluations (oarofolo et al, 1997; garotblo et al, 1999).
</nextsent>
<nextsent>(waibel et a.1., 1(.)98) describe meeting browser where summaries earl be gener-ated using technology established for written texts.
</nextsent>
<nextsent>(va.lenza.
</nextsent>
<nextsent>el.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A1026">
<title id=" C00-1027.xml">empirical estimates of adaptation the chance of two noriegas is closer to p2 than p2 </title>
<section> experimental results.  </section>
<citcontext>
<prevsection>
<prevsent>these garbled documents tended to cause trouble for the proposed method; in such cases, the history comes fi om one document and the test comes from another.
</prevsent>
<prevsent>in general, the proposed adaptation method per-formed well when the history is helpful for pre-dicting the test portion of the document, and it performed poorly when the history is misleading.
</prevsent>
</prevsection>
<citsent citstr=" P99-1022 ">
this suggests that we ought to measure topic shifts using methods uggested by hearst (1994) and florian &amp; yarowsky (1999).<papid> P99-1022 </papid></citsent>
<aftsection>
<nextsent>we should not use the history when we believe that there has been major topic shift.
</nextsent>
<nextsent>adaptive language models were introduced to account for repetition.
</nextsent>
<nextsent>it is well known that the second instance of word (or ngram) is nmch more likely than the first.
</nextsent>
<nextsent>but what we find surprising is just how large the effect is. the chance of two noriegas is closer to p/2 than 2.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A1027">
<title id=" C00-1076.xml">extending a formal and computational model of rhetorical structure theory with intentional structures a la grosz and sidner </title>
<section> motivation </section>
<citcontext>
<prevsection>
<prevsent>ai justtfioaiton gl ci e) the situation presented in c1 will increase the reader negative regard for the situation presented in cl ; and the situation presented in d, is restatement o1  the situation presented in &amp; . marcu (1996) has shown that on the basis of only the rhetorical judgments in (2) and without considering in-tentions, there are five valid rs-trces that one can build for text ( ) (see figure ).
</prevsent>
<prevsent>what happens though when we consider intentions as well?
</prevsent>
</prevsection>
<citsent citstr=" J92-4007 ">
moore and pollack (1992) <papid> J92-4007 </papid>have already shown that different high-level intentions yield different rs-trces.</citsent>
<aftsection>
<nextsent>but how do we formalize tile relationship between intentions and rhetorical structures?
</nextsent>
<nextsent>for example, how can we use the discourse trees in fig-ure 1 in order to determine the primary intention asso-ciated with each analysis?
</nextsent>
<nextsent>and how can we determine what would be the corresponding dominance relations in gst account of tile same text ?
</nextsent>
<nextsent>consider also slightly difl erent problem: assume that besides rhetorical judgments, such as those shown in (2), one can also make intentional judgments.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A1028">
<title id=" C00-1076.xml">extending a formal and computational model of rhetorical structure theory with intentional structures a la grosz and sidner </title>
<section> the limits of moser and moore s.  </section>
<citcontext>
<prevsection>
<prevsent>however, unified theory can.
</prevsent>
<prevsent>ill this paper, we provide such theory.
</prevsent>
</prevsection>
<citsent citstr=" J96-3006 ">
approach in recent proposal, moser and moore (1996) <papid> J96-3006 </papid>argued that the primary intentions in gst representation can be derived fi om the nuclei ot the corresponding rst rep- resentation.</citsent>
<aftsection>
<nextsent>although their proposal is consistent with the cases in which each textual span is characterized by an explicit nucleus that encodes the primary intention of that span (as in the case of text (i)), it seems that an ad-equate account of the correspondence tween gst and rst is somewhat more complicated.
</nextsent>
<nextsent>for example, in tile case of text (3) below, whose rst analysis is shown in ligure 2, we cannot apply moser and moore approach because we can associate tile primary intention of dis-course segment \[a2, b2\] neither to trait a2 nor to trait b2.
</nextsent>
<nextsent>(3) \[john wanted to play squash with janet,aq \[but he non volitional cause c2 a2 b2 figure 2: rhetorical analysis of text (3).
</nextsent>
<nextsent>also wanted to have dinner with suzanne.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A1032">
<title id=" C00-1076.xml">extending a formal and computational model of rhetorical structure theory with intentional structures a la grosz and sidner </title>
<section> melding text structures and intentions.  </section>
<citcontext>
<prevsection>
<prevsent>ill such case, the axiom ati zation provides the means for drawing intentional infer-ences on the basis of the discourse structure.
</prevsent>
<prevsent>also, al-though there are live discourse structures that are consis-tent with the rhetorical judgments in (i), they yield only three intentional interpretations, i.e., there arc only three primary intentions that one can associate to the whole text.
</prevsent>
</prevsection>
<citsent citstr=" J98-4001 ">
one intention is that discussed above, which is as-sociated with analysis i.a. another intention depends on unit bz and the justification relation that holds be-tween units a1 and lh; this intention is associated with the analyses hown in ligure 1.c and l.e. and another in-tention depends on trait bj and the justification rela-tion that holds between units l)j and bj ; this intention is associated with the analyses hown in figure 1.b and 1.d. reasoning fronl text structures to intentions can be also beneficial hi context such as that described by lochbaum (1998) <papid> J98-4001 </papid>because the rhetorical constraints can help prune the space of shared phms that woukl charac-terize an intentional interpret ati n of d iscou rse.</citsent>
<aftsection>
<nextsent>528 using intentions lbr nmnaging rhetorical aml igu t ies . assume now that besides providing.ivdgments concern-ing the rhetorical rehttions that hold between various units, an analyst (ot  progran0 provides judglnents of intentions as well.
</nextsent>
<nextsent>if, lk+t  cxaml+le, besides the relations given in (2) program determines that the dsp of span tat, 1)1\] dominates 111o dsp of unit i/i, the theory that corresponds to these judgments and 111e axioms given in section 3 yields only two wdid text structures, those presented in \[igure l.b and i.d. in this ease, the axiom- at ization provides the means of using intentional judg-ments for reducing the ambiguity that characterizes the discourse parsing process.
</nextsent>
<nextsent>hwestigating the relationship between semantic and intentional relations.
</nextsent>
<nextsent>in their seminal paper, moore and polhtck (1992) <papid> J92-4007 </papid>showed lhat text may be charac-terized by intentional and rhetorical analyses that are not isomorphic.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A1035">
<title id=" C00-2157.xml">a description language for syntactically annotated corpora </title>
<section> i t roduct ion   </section>
<citcontext>
<prevsection>


</prevsection>
<citsent citstr=" J93-2004 ">
syntactically annotated corpora like the penn treebank (marcus et al, 1993), <papid> J93-2004 </papid>the negra corpus (skut et al, 1998) or the sta-tistically dismnbiguated parses in (bell et al., 1999) provide wealth of intbrmation, which can only be exploited with an ade-quate query language.</citsent>
<aftsection>
<nextsent>for example, one might want to retrieve verbs with their sen-tential complements, or specific fronting or extra position phenomena.
</nextsent>
<nextsent>so far, queries to treebank have been formulated in script-ing languages like tgrep, perl or others.
</nextsent>
<nextsent>re-cently, some powerful query languages have been developed: an exalnple of high- level, constraint-based language is described in (duchier and niehren, 1999).
</nextsent>
<nextsent>(bird et al, 2000) propose query language for the gen-eral concept of annotation grat)hs,, graph-ical query notation tbr trees is under devel-opment in the ice project (ucl, 2000).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A1036">
<title id=" C00-2157.xml">a description language for syntactically annotated corpora </title>
<section> the query language.  </section>
<citcontext>
<prevsection>
<prevsent>semantic head.
</prevsent>
<prevsent>this can be done either by introducing empty leaf nodes plus means for node coreference (like in the penn treebank) or by admitting cross-ing edges.
</prevsent>
</prevsection>
<citsent citstr=" A97-1014 ">
in our project, the latter solution has been chosen (skut et al, 1997), <papid> A97-1014 </papid>partly tbr the reason that it is simpler to annotate (no decision on the right place of trace has to be taken).</citsent>
<aftsection>
<nextsent>we call this extension of trees with crossing edges yntaz graphs.
</nextsent>
<nextsent>an exam-ple is shown in fig.
</nextsent>
<nextsent>1.
</nextsent>
<nextsent>in order to discuss the details of the lan-guage, we will make reference to the simpler syntax graph in fig.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A1038">
<title id=" A00-1022.xml">message classification in the call center </title>
<section> integrating language technology.  </section>
<citcontext>
<prevsection>
<prevsent>the relearning step is based on data from this database.
</prevsent>
<prevsent>3.1 shallow text processing.
</prevsent>
</prevsection>
<citsent citstr=" A97-1031 ">
linguistic preprocessing of text documents is car-ried out by re-using sines, an information extrac-tion core system for real-world german text pro-cessing (neumann et al, 1997).<papid> A97-1031 </papid></citsent>
<aftsection>
<nextsent>the fundamental design criterion of sines is to provide set of basic, powerful, robust, and efficient stp components and 4almost all tools we examined build single multi- categorizer except for svm-light, which builds multiple bi-nary classifiers.
</nextsent>
<nextsent>1 ;q 159 cate gory) figure 1: architecture of the icc-mail system.
</nextsent>
<nextsent>generic linguistic knowledge sources that can eas-ily be customized to deal with different asks inflexible manner, sines includes text tokenizer, lexical processor and chunk parser.
</nextsent>
<nextsent>the chunk parser itself is subdivided into three components.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A1042">
<title id=" A00-1022.xml">message classification in the call center </title>
<section> conc lus ions  and  future  work.  </section>
<citcontext>
<prevsection>
<prevsent>further task-specific heuristics aiming at gen-eral structural inguistic properties hould be defined.
</prevsent>
<prevsent>this includes heuristics for the identi-fication of multiple requests in single e-mail that could be based on keywords and key phrases as well as on the analysis of the doc-ument structure.
</prevsent>
</prevsection>
<citsent citstr=" W97-0802 ">
our initial experiments with the integration of germanet (hamp and feldweg, 1997), <papid> W97-0802 </papid>the evolving german version of wordnet, seem to confirm the positive results described for word-net (de buenaga rodriguez et al, 1997) and will thus be extended.</citsent>
<aftsection>
<nextsent>a reorganization the existing three-level cate-gory system into semantically consistent tree structure would allow us to explore the non-terminal nodes of the tree for multi-layered sml.
</nextsent>
<nextsent>this places additional requirements on the knowledge ngineering task and thus needs to be thoroughly investigated for pay-off.
</nextsent>
<nextsent>where system-generated answers are acceptable to customers, straightforward extension of ice-mail can provide this functionality.
</nextsent>
<nextsent>for the application in hand, this was not the case.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A1043">
<title id=" C00-2117.xml">text genre detection using common word frequencies </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>biber studied the stylistic differences between written and spoken language (biber, 1988) as well as the variation of registers in cross-linguistic omparison (biber, 1995) and presented model for interpreting the functions of various linguistic features.
</prevsent>
<prevsent>unfortunately, his model can not be easily realized using existing natural anguage processing tools.
</prevsent>
</prevsection>
<citsent citstr=" C94-2174 ">
on the other hand, some computational models for detecting automatically the text genre have recently been available (karlgren and cutting, 1994; <papid> C94-2174 </papid>kessler et al, 1997).<papid> P97-1005 </papid></citsent>
<aftsection>
<nextsent>kessler gives an excellent summarization the potential applications of text genre detector.
</nextsent>
<nextsent>in particular, part-of-speech tagging, parsing accuracy and word-sense disambiguation could be considerably enhanced by taking genre into account since certain grammatical constructions or word senses are closely related to specific genres.
</nextsent>
<nextsent>moreover, in information retrieval the search results could be sorted according to the genre as well.
</nextsent>
<nextsent>towards the automatic detection of text genre, various types of style markers (i.e., countable linguistic features) have been proposed so far.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A1045">
<title id=" C00-2117.xml">text genre detection using common word frequencies </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>biber studied the stylistic differences between written and spoken language (biber, 1988) as well as the variation of registers in cross-linguistic omparison (biber, 1995) and presented model for interpreting the functions of various linguistic features.
</prevsent>
<prevsent>unfortunately, his model can not be easily realized using existing natural anguage processing tools.
</prevsent>
</prevsection>
<citsent citstr=" P97-1005 ">
on the other hand, some computational models for detecting automatically the text genre have recently been available (karlgren and cutting, 1994; <papid> C94-2174 </papid>kessler et al, 1997).<papid> P97-1005 </papid></citsent>
<aftsection>
<nextsent>kessler gives an excellent summarization the potential applications of text genre detector.
</nextsent>
<nextsent>in particular, part-of-speech tagging, parsing accuracy and word-sense disambiguation could be considerably enhanced by taking genre into account since certain grammatical constructions or word senses are closely related to specific genres.
</nextsent>
<nextsent>moreover, in information retrieval the search results could be sorted according to the genre as well.
</nextsent>
<nextsent>towards the automatic detection of text genre, various types of style markers (i.e., countable linguistic features) have been proposed so far.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A1051">
<title id=" C00-2117.xml">text genre detection using common word frequencies </title>
<section> text  genre  )etect ion.  </section>
<citcontext>
<prevsection>
<prevsent>7 . . .
</prevsent>
<prevsent>0 20 40 60 80 100 most frequent worir of bnc figure 1: comparison of the most fi equent word lists.
</prevsent>
</prevsection>
<citsent citstr=" J93-2001 ">
in order to detect automatically tile text genre we used discriminant analy.vis, well-known classification technique of lnultivariate statistics that has been used in previous work in text genre detection (biber, 1993; <papid> J93-2001 </papid>karlgrcn and cutting, 1994).</citsent>
<aftsection>
<nextsent>this methodology takes some multivariate vectors pre categorized into naturally occurring groups (i.e., training data) and extracts set of discriminant./imctions that distinguish the groups.
</nextsent>
<nextsent>the mathenlatical objective of dischminant analysis is to weight and linearly combine tile discriminating variables (i.e., style markers) in some way so that the groups are tbrced to be as statistically distinct as possible (eisenbeis &amp; avery, 1972).
</nextsent>
<nextsent>then, discrinfinant analysis can be used lbr predicting tile group membership of previously unseen cases (i.e., test data).
</nextsent>
<nextsent>in the present case, tile multivariate vectors are tile fi equencies of occurrence of tile most fi equent words of the bnc lbr each text sample and the naturally occurring groups are the four text genres.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A1052">
<title id=" C00-2152.xml">an integrated architecture for example based machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>machine translation by analogy lo pairs of corresponding expressions in the source and target languages, or  example-based transhtlion , was firs!
</prevsent>
<prevsent>proposed by (nagao 1984).
</prevsent>
</prevsection>
<citsent citstr=" C92-2115 ">
recent work in the example-based ramework inchldes memory-based translation (sate &amp; nagao 1990), similarity-driven translation (watanabe 1992), <papid> C92-2115 </papid>transl cr-driven lachine translation (furusc &amp; iida 1996), and patten based machine translation (watanabe &amp; takeda 1998).<papid> P98-2223 </papid></citsent>
<aftsection>
<nextsent>the example-based approach promises easy translation knowledge acquisition, more flexible transfer than brittle rule-based approaches, and idiomatic translations.
</nextsent>
<nextsent>at the same time, the use o1  linguistic rules offers number of important benel its.
</nextsent>
<nextsent>detailed linguistic analysis can allow an example-based machine translation system to handle wide variety of input, since rules can be used to factor out all linguistic wmations that do not influence tile exampled)ased transfer.
</nextsent>
<nextsent>rule-based language generation from detailed linguistic representations can lead to higher grammatical output quality.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A1053">
<title id=" C00-2152.xml">an integrated architecture for example based machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>machine translation by analogy lo pairs of corresponding expressions in the source and target languages, or  example-based transhtlion , was firs!
</prevsent>
<prevsent>proposed by (nagao 1984).
</prevsent>
</prevsection>
<citsent citstr=" P98-2223 ">
recent work in the example-based ramework inchldes memory-based translation (sate &amp; nagao 1990), similarity-driven translation (watanabe 1992), <papid> C92-2115 </papid>transl cr-driven lachine translation (furusc &amp; iida 1996), and patten based machine translation (watanabe &amp; takeda 1998).<papid> P98-2223 </papid></citsent>
<aftsection>
<nextsent>the example-based approach promises easy translation knowledge acquisition, more flexible transfer than brittle rule-based approaches, and idiomatic translations.
</nextsent>
<nextsent>at the same time, the use o1  linguistic rules offers number of important benel its.
</nextsent>
<nextsent>detailed linguistic analysis can allow an example-based machine translation system to handle wide variety of input, since rules can be used to factor out all linguistic wmations that do not influence tile exampled)ased transfer.
</nextsent>
<nextsent>rule-based language generation from detailed linguistic representations can lead to higher grammatical output quality.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A1054">
<title id=" A88-1032.xml">localizing expression of ambiguity </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>furthermore, as is well known, syntactic information may be insu~cient for selecting best reading.
</prevsent>
<prevsent>it may take se-mantic knowledge of arbitrary complexity to decide which alternative to choose.
</prevsent>
</prevsection>
<citsent citstr=" J86-3006 ">
in the tacitus project \[hobbs, 1986; <papid> J86-3006 </papid>hobbs and martin, 1987\] we are developing pragmatics component which, given the logical form of sentence, uses world knowledge to solve various interpretation problems, the r~=oluti,jd of syntactic ambiguity among them.</citsent>
<aftsection>
<nextsent>sentences are translated into logical form by the dia logic system for syntactic mid semantic analysis \[grosz et al, 1982\].
</nextsent>
<nextsent>in this paper we describe how information about alter-native parses is passed concisely from dia logic to the pragmatics component, and more generally, we discuss method of localizing the representation syntactic ambi-guity in the logical form of sentence.
</nextsent>
<nextsent>one possible approach to the ambiguity problem would be to produce set of logical forms for sentence, one for each parse tree, and to send them one at time to the pragrnatics component.
</nextsent>
<nextsent>this involves considerable dupli-cation of effort if the logical forms are largely the same and differ only with respect attachment.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A1055">
<title id=" A88-1032.xml">localizing expression of ambiguity </title>
<section> the  lgor thm.  </section>
<citcontext>
<prevsection>
<prevsent>a complex term is vari-able followed by  such that  symbol  \[  , followed by conjunction of one or more propositions?
</prevsent>
<prevsent>complex terms 2this notation can be translated into russel lian notation, with the consequent loss of information about grammatical subordination, by repeated application ofthe transformation p(z q) =~ p(z) q. 237 are enclosed in square brackets for readability.
</prevsent>
</prevsection>
<citsent citstr=" P85-1008 ">
events are represented by event variables, as in \[hobbs, 1985\], <papid> P85-1008 </papid>so that see (el,zl,x~) means el is seeing event by ,vl of x2.</citsent>
<aftsection>
<nextsent>one of sentence (4) lfs before attachment-finding is \[xl john(xl)l, in(x~, \[x31 pa~k(x3) ^ with(xs, the same lf after attachment-finding is pa~t(\[e, ~ee (~l, \[x~ john(x~)\], \[~ iman(~) ^ \[~ ip-~k(~) ^ with(\[y2 1 92=x3 y2=x2 ~2:el\], \[~, t~t~cop~(x,)\])\])\])\]) paraphrase of the latter lf in english would be some- thing like this: there is an event el that happened in the past; it is seeing event by xl who is john, of x2 who is the man; something yl is in the park, and that something is either the man or the seeing event; something y2 is with telescope, and that something is the park, the man, or the seeing event.
</nextsent>
<nextsent>the procedure for finding possible attachment sites in order to modify logical form is as follows.
</nextsent>
<nextsent>the program recursively descends an lf, and keeps lists of the event and entity variables that initiate complex terms.
</nextsent>
<nextsent>event variables associated with tenses are omitted.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A1057">
<title id=" A88-1032.xml">localizing expression of ambiguity </title>
<section> lost  in format ion.  </section>
<citcontext>
<prevsection>
<prevsent>in tacitus, noun phrase conjunction is encoded with the predicate andn, taking three sets as its arguments.
</prevsent>
<prevsent>the expression andn(sl,s2,sa) means that the set sl is the union of sets s~ and sa.
</prevsent>
</prevsection>
<citsent citstr=" P83-1009 ">
4 following hobbs \[1983\], <papid> P83-1009 </papid>the representation plurals involves set and typical ele-ment of the set, or reified universally quantified variable ranging over the elements of the set.</citsent>
<aftsection>
<nextsent>properties like cardi- nality are properties of the set itself, while properties that hold for each of the elements are properties of the typical element.
</nextsent>
<nextsent>an axiom schema specifies that any properties of 4if either st or s2 is not set, the singleton set consisting of just that element isused instead.
</nextsent>
<nextsent>the typical element are inherited by the individual, actual dements) thus, the phrase  british and american ships  is translated into the set sl such that andn(shs~,sa) typelt(zl ,s l) ship(x1) typelt(x2,s2) british(x2) typelt(xa, sa) american(xa) that is, the typical element xl of the set sl is ship, and sl is the union of the sets s2 and s3, where the typical element x2 of s2 is british, and the typical element xa of sa is american.
</nextsent>
<nextsent>the phrase  tall and handsome men  can be represented in the same way.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A1058">
<title id=" A00-2019.xml">an unsupervised method for detecting grammatical errors </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>toefl is taken by foreign students who are applying to us undergraduate and graduate-level programs.
</prevsent>
<prevsent>1 background.
</prevsent>
</prevsection>
<citsent citstr=" P98-2196 ">
approaches to detecting errors by non-native writers typically produce grammars that look for specific expected error types (schneider and mccoy, 1998; <papid> P98-2196 </papid>park, palmer and washburn, 1997).</citsent>
<aftsection>
<nextsent>under this approach, essays written by esl students are collected and examined for errors.
</nextsent>
<nextsent>parsers are then adapted to identify those error types that were found in the essay collection.
</nextsent>
<nextsent>we take different approach, initially viewing error detection as an extension of the word sense disambiguation (wsd) problem.
</nextsent>
<nextsent>corpus-based wsd systems identify the intended sense of polysemous word by (1) collecting set of example sentences for each of its various enses and (2) extracting salient contextual cues from these sets to (3) build statistical model for each sense.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A1059">
<title id=" A00-2019.xml">an unsupervised method for detecting grammatical errors </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we take different approach, initially viewing error detection as an extension of the word sense disambiguation (wsd) problem.
</prevsent>
<prevsent>corpus-based wsd systems identify the intended sense of polysemous word by (1) collecting set of example sentences for each of its various enses and (2) extracting salient contextual cues from these sets to (3) build statistical model for each sense.
</prevsent>
</prevsection>
<citsent citstr=" H93-1052 ">
they identify the intended sense of word in novel sentence by extracting its contextual cues and selecting the most similar word sense model (e.g., leacock, chodorow and miller (1998), yarowsky (1993)).<papid> H93-1052 </papid></citsent>
<aftsection>
<nextsent>golding (1995) <papid> W95-0104 </papid>showed how methods used for wsd (decision lists and bayesian classifiers) could be adapted to detect errors resulting from 140 common spelling confusions among sets such as there, their, and they  re.</nextsent>
<nextsent>he extracted contexts from correct usage of each conf usable word in training corpus and then identified new occurrence as an error when it matched the wrong context.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A1060">
<title id=" A00-2019.xml">an unsupervised method for detecting grammatical errors </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>corpus-based wsd systems identify the intended sense of polysemous word by (1) collecting set of example sentences for each of its various enses and (2) extracting salient contextual cues from these sets to (3) build statistical model for each sense.
</prevsent>
<prevsent>they identify the intended sense of word in novel sentence by extracting its contextual cues and selecting the most similar word sense model (e.g., leacock, chodorow and miller (1998), yarowsky (1993)).<papid> H93-1052 </papid></prevsent>
</prevsection>
<citsent citstr=" W95-0104 ">
golding (1995) <papid> W95-0104 </papid>showed how methods used for wsd (decision lists and bayesian classifiers) could be adapted to detect errors resulting from 140 common spelling confusions among sets such as there, their, and they  re.</citsent>
<aftsection>
<nextsent>he extracted contexts from correct usage of each conf usable word in training corpus and then identified new occurrence as an error when it matched the wrong context.
</nextsent>
<nextsent>however, most grammatical errors are not the result of simple word confusions.
</nextsent>
<nextsent>this complicates the task of building model of incorrect usage.
</nextsent>
<nextsent>one approach we considered was to proceed without such model: represent appropriate word usage (across enses) in single model and compare novel example to that model.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A1062">
<title id=" C00-2149.xml">context free grammar rewriting and the transfer of packed linguistic representations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>there is currently much interest in translation models that support some amount of ambiguity preservation be-tween source and target exts, so as to minimize disam-biguation decisions that the system, or an interactive user, has to make during the translation process (kay et al , 1994)..
</prevsent>
<prevsent>an important aspect ol  such models is the ability to handle, during all the stages of the translation process, packed linguistic structures, that is, structures which fac- torize in compact fashion all the different readings of sentence and obviate the need to list and treat all these readings in isolation of each other (as is standard in more traditional models for machine translation).
</prevsent>
</prevsection>
<citsent citstr=" J93-4001 ">
in the case of parsing, and more specifically, parsing with unification-based formalisms uch as lfg, tech-niques for producing packed structures have been in existence for some time (maxwell and kaplan, 1991; maxwell and kaplan, 1993; <papid> J93-4001 </papid>maxwell and kaplan, 1996; d6rre, 1997; dymetman, 1997).</citsent>
<aftsection>
<nextsent>more recently, tech-niques have been appearing for the generation from packed structures (shemtov, 1997), the transfer between packed structures (emele and dorna, 1998; <papid> P98-1060 </papid>rayner and bouillon, 1995), and the integration of such mechanisms into the whole translation process (kay, 1999; frank, 1999).</nextsent>
<nextsent>this paper focuses on the problem of transfer.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A1063">
<title id=" C00-2149.xml">context free grammar rewriting and the transfer of packed linguistic representations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>an important aspect ol  such models is the ability to handle, during all the stages of the translation process, packed linguistic structures, that is, structures which fac- torize in compact fashion all the different readings of sentence and obviate the need to list and treat all these readings in isolation of each other (as is standard in more traditional models for machine translation).
</prevsent>
<prevsent>in the case of parsing, and more specifically, parsing with unification-based formalisms uch as lfg, tech-niques for producing packed structures have been in existence for some time (maxwell and kaplan, 1991; maxwell and kaplan, 1993; <papid> J93-4001 </papid>maxwell and kaplan, 1996; d6rre, 1997; dymetman, 1997).</prevsent>
</prevsection>
<citsent citstr=" P98-1060 ">
more recently, tech-niques have been appearing for the generation from packed structures (shemtov, 1997), the transfer between packed structures (emele and dorna, 1998; <papid> P98-1060 </papid>rayner and bouillon, 1995), and the integration of such mechanisms into the whole translation process (kay, 1999; frank, 1999).</citsent>
<aftsection>
<nextsent>this paper focuses on the problem of transfer.
</nextsent>
<nextsent>the method proposed is related to those of (emele and dorna, 1998) <papid> P98-1060 </papid>and (kay, 1999).</nextsent>
<nextsent>as in these approaches, we view packed representations being descriptions of finite collection of directed labelled graphs (similar to the func-tional structures of lfg), each representing different non-ambiguous reading, which share certain subparts.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A1068">
<title id=" A00-2030.xml">a novel use of statistical parsing to extract information from text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>since 1995, few statistical parsing algorithms have demonstrated breakthrough in parsing accuracy, as measured against the upenn treebank as gold standard.
</prevsent>
<prevsent>in this paper we report adapting lexicalized, probabilistic context-free parser to information extraction and evaluate this new technique on muc-7 template elements and template relations.
</prevsent>
</prevsection>
<citsent citstr=" P96-1025 ">
since 1995, few statistical parsing algorithms (magerman, 1995; collins, 1996 <papid> P96-1025 </papid>and 1997; charniak, 1997; rathnaparki, 1997) demonstrated breakthrough in parsing accuracy, as measured against he university of pennsylvania treebank as gold standard.</citsent>
<aftsection>
<nextsent>yet, relatively few have embedded one of these algorithms in task.
</nextsent>
<nextsent>chiba, (1999) was able to use such parsing algorithm to reduce perplexity with the long term goal of improved speech recognition.
</nextsent>
<nextsent>in this paper, we report adapting lexicalized, probabilistic context-free parser with head rules (lpcfg-hr) to information extraction.
</nextsent>
<nextsent>the technique was benchmarked in the seventh message understanding conference (muc-7) in 1998.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A1069">
<title id=" A00-2030.xml">a novel use of statistical parsing to extract information from text </title>
<section> integrated sentential processing.  </section>
<citcontext>
<prevsection>
<prevsent>for this reason, we focused on designing an integrated model in which tagging, name- finding, parsing, and semantic interpretation decisions all have the opportunity to mutually influence ach other.
</prevsent>
<prevsent>a second consideration influenced our decision toward an integrated model.
</prevsent>
</prevsection>
<citsent citstr=" J93-2006 ">
we were already using generative statistical model for part-of-speech tagging (weischedel et al 1993), <papid> J93-2006 </papid>and more recently, had begun using generative statistical model for name finding (bikel et al 1997).<papid> A97-1029 </papid></citsent>
<aftsection>
<nextsent>finally, our newly constructed parser, like that of (collins 1997), <papid> P97-1003 </papid>was based on generative statistical model.</nextsent>
<nextsent>thus, each component of what would be the first three stages of our pipeline was based on 227 the same general class of statistical model.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A1070">
<title id=" A00-2030.xml">a novel use of statistical parsing to extract information from text </title>
<section> integrated sentential processing.  </section>
<citcontext>
<prevsection>
<prevsent>for this reason, we focused on designing an integrated model in which tagging, name- finding, parsing, and semantic interpretation decisions all have the opportunity to mutually influence ach other.
</prevsent>
<prevsent>a second consideration influenced our decision toward an integrated model.
</prevsent>
</prevsection>
<citsent citstr=" A97-1029 ">
we were already using generative statistical model for part-of-speech tagging (weischedel et al 1993), <papid> J93-2006 </papid>and more recently, had begun using generative statistical model for name finding (bikel et al 1997).<papid> A97-1029 </papid></citsent>
<aftsection>
<nextsent>finally, our newly constructed parser, like that of (collins 1997), <papid> P97-1003 </papid>was based on generative statistical model.</nextsent>
<nextsent>thus, each component of what would be the first three stages of our pipeline was based on 227 the same general class of statistical model.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A1071">
<title id=" A00-2030.xml">a novel use of statistical parsing to extract information from text </title>
<section> integrated sentential processing.  </section>
<citcontext>
<prevsection>
<prevsent>a second consideration influenced our decision toward an integrated model.
</prevsent>
<prevsent>we were already using generative statistical model for part-of-speech tagging (weischedel et al 1993), <papid> J93-2006 </papid>and more recently, had begun using generative statistical model for name finding (bikel et al 1997).<papid> A97-1029 </papid></prevsent>
</prevsection>
<citsent citstr=" P97-1003 ">
finally, our newly constructed parser, like that of (collins 1997), <papid> P97-1003 </papid>was based on generative statistical model.</citsent>
<aftsection>
<nextsent>thus, each component of what would be the first three stages of our pipeline was based on 227 the same general class of statistical model.
</nextsent>
<nextsent>although each model differed in its detailed probability structure, we believed that the essential elements of all three models could be generalized in single probability model.
</nextsent>
<nextsent>if the single generalized model could then be extended to semantic anal);sis, all necessary sentence level processing would be contained in that model.
</nextsent>
<nextsent>because generative statistical models had already proven successful for each of the first three stages, we were optimistic that some of their properties - especially their ability to learn from large amounts of data, and their robustness when presented with unexpected inputs - would also benefit semantic analysis.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A1074">
<title id=" A00-2030.xml">a novel use of statistical parsing to extract information from text </title>
<section> searching the model.  </section>
<citcontext>
<prevsection>
<prevsent>9.2 pruning.
</prevsent>
<prevsent>given multiple constituents that cover identical spans in the chart, only those constituents with probabilities within threshold of the highest scoring constituent are maintained; all others are pruned.
</prevsent>
</prevsection>
<citsent citstr=" W97-0302 ">
for purposes of pruning, and only for purposes of pruning, the prior probability of each constituent category is multiplied by the generative probability of that constituent (goodman, 1997).<papid> W97-0302 </papid></citsent>
<aftsection>
<nextsent>we can think of this prior probability as an estimate of the probability of generating subtree with the constituent category, starting at the topmost node.
</nextsent>
<nextsent>thus, the scores used in pruning can be considered as the product of: . the probability of generating constituent.
</nextsent>
<nextsent>of the specified category, starting at the topmost node.
</nextsent>
<nextsent>the probability of generating the structure.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A1075">
<title id=" A00-1025.xml">examining the role of statistical and linguistic knowledge sources in a general knowledge question answering system </title>
<section> query-dependent text.  </section>
<citcontext>
<prevsection>
<prevsent>summar ization fo question answering we next hypothesize that query-dependent text summarization algorithms will improve the perfor-mance of the qa system by focusing the system on the most relevant portions of the retrieved oc- uments.
</prevsent>
<prevsent>the goal for query-dependent summariza-tion algorithms is to provide short summary of document with respect to specific query.
</prevsent>
</prevsection>
<citsent citstr=" E99-1011 ">
al-though number of methods for query-dependent text summarization are beginning to be developed and evaluated in variety of realistic settings (mani et al, 1999), <papid> E99-1011 </papid>we again propose the use of vector space methods from ir, which can be easily extended to the summarization task (salton et al, 1994): 1.</citsent>
<aftsection>
<nextsent>given question and document, divide the.
</nextsent>
<nextsent>document into chunks (e.g. sentences, para-graphs, 200-word passages).
</nextsent>
<nextsent>2.
</nextsent>
<nextsent>generate the vector epresentation forthe ques-.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A1077">
<title id=" A00-1025.xml">examining the role of statistical and linguistic knowledge sources in a general knowledge question answering system </title>
<section> incorporating  the  noun phrase.  </section>
<citcontext>
<prevsection>
<prevsent>the filter operates on the ordered list of summary extracts for particular question and produces list of answer hypotheses, one for each noun phrase (np) in the extracts in the left-to-right order in which they appeared.
</prevsent>
<prevsent>the np-based qa system.
</prevsent>
</prevsection>
<citsent citstr=" P98-1034 ">
our implementa-tion of the np-based qa system uses the empire noun phrase finder, which is described in detail in cardie and pierce (1998).<papid> P98-1034 </papid></citsent>
<aftsection>
<nextsent>empire identifies base nps - - non-recursive noun phrases - - using very simple algorithm that matches part-of-speech tag se-quences based on learned noun phrase grammar.
</nextsent>
<nextsent>the approach is able to achieve 94% precision and recall for base nps derived from the penn treebank wall street journal (marcus et al, 1993).<papid> J93-2004 </papid></nextsent>
<nextsent>in the experiments below, the np filter follows the applica-tion of the document retrieval and text summariza-tion components.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A1078">
<title id=" A00-1025.xml">examining the role of statistical and linguistic knowledge sources in a general knowledge question answering system </title>
<section> incorporating  the  noun phrase.  </section>
<citcontext>
<prevsection>
<prevsent>our implementa-tion of the np-based qa system uses the empire noun phrase finder, which is described in detail in cardie and pierce (1998).<papid> P98-1034 </papid></prevsent>
<prevsent>empire identifies base nps - - non-recursive noun phrases - - using very simple algorithm that matches part-of-speech tag se-quences based on learned noun phrase grammar.</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
the approach is able to achieve 94% precision and recall for base nps derived from the penn treebank wall street journal (marcus et al, 1993).<papid> J93-2004 </papid></citsent>
<aftsection>
<nextsent>in the experiments below, the np filter follows the applica-tion of the document retrieval and text summariza-tion components.
</nextsent>
<nextsent>pronoun answer hypotheses are discarded, and the nps are assembled into 50-byte chunks.
</nextsent>
<nextsent>eva luat ion.
</nextsent>
<nextsent>results for the np-based qa sys-tem are shown in the third row of table 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A1079">
<title id=" A97-1013.xml">developing a hybrid np parser </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the language models used by natural anguage an-alyzers are traditionally based on two approaches.
</prevsent>
<prevsent>in the linguistic approach, the model is based on hand-crafted rules derived from the linguist gen-eral and/or corpus-based knowledge about the ob-ject language.
</prevsent>
</prevsection>
<citsent citstr=" P89-1015 ">
in the data-driven approach, the model is automatically generated from annotated text corpora, and the model can be represented e.g. as n-grams (garside al., 1987), local rules (hindle, 1989) <papid> P89-1015 </papid>or neural nets (schmid, 1994).<papid> C94-1027 </papid></citsent>
<aftsection>
<nextsent>most hybrid approaches combine statistical infor-mation with automatically extracted rule-based in-formation (brill, 1995; <papid> W95-0101 </papid>daelemans et al, 1996).<papid> W96-0102 </papid></nextsent>
<nextsent>rel-atively little attention has been paid to models where the statistical approach is combined with truly lin-guistic model (i.e. one generated by linguist).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A1080">
<title id=" A97-1013.xml">developing a hybrid np parser </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the language models used by natural anguage an-alyzers are traditionally based on two approaches.
</prevsent>
<prevsent>in the linguistic approach, the model is based on hand-crafted rules derived from the linguist gen-eral and/or corpus-based knowledge about the ob-ject language.
</prevsent>
</prevsection>
<citsent citstr=" C94-1027 ">
in the data-driven approach, the model is automatically generated from annotated text corpora, and the model can be represented e.g. as n-grams (garside al., 1987), local rules (hindle, 1989) <papid> P89-1015 </papid>or neural nets (schmid, 1994).<papid> C94-1027 </papid></citsent>
<aftsection>
<nextsent>most hybrid approaches combine statistical infor-mation with automatically extracted rule-based in-formation (brill, 1995; <papid> W95-0101 </papid>daelemans et al, 1996).<papid> W96-0102 </papid></nextsent>
<nextsent>rel-atively little attention has been paid to models where the statistical approach is combined with truly lin-guistic model (i.e. one generated by linguist).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A1081">
<title id=" A97-1013.xml">developing a hybrid np parser </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in the linguistic approach, the model is based on hand-crafted rules derived from the linguist gen-eral and/or corpus-based knowledge about the ob-ject language.
</prevsent>
<prevsent>in the data-driven approach, the model is automatically generated from annotated text corpora, and the model can be represented e.g. as n-grams (garside al., 1987), local rules (hindle, 1989) <papid> P89-1015 </papid>or neural nets (schmid, 1994).<papid> C94-1027 </papid></prevsent>
</prevsection>
<citsent citstr=" W95-0101 ">
most hybrid approaches combine statistical infor-mation with automatically extracted rule-based in-formation (brill, 1995; <papid> W95-0101 </papid>daelemans et al, 1996).<papid> W96-0102 </papid></citsent>
<aftsection>
<nextsent>rel-atively little attention has been paid to models where the statistical approach is combined with truly lin-guistic model (i.e. one generated by linguist).
</nextsent>
<nextsent>this paper reports one such approach: syntactic rules written by linguist are combined with statistical information using the relaxation labelling algorithm.
</nextsent>
<nextsent>80 our application is very shallow parsing: identifi-cation of verbs, premodifiers, nominal and adverbial heads, and certain kinds of postmodifiers.
</nextsent>
<nextsent>we call this parser noun phrase parser.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A1082">
<title id=" A97-1013.xml">developing a hybrid np parser </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in the linguistic approach, the model is based on hand-crafted rules derived from the linguist gen-eral and/or corpus-based knowledge about the ob-ject language.
</prevsent>
<prevsent>in the data-driven approach, the model is automatically generated from annotated text corpora, and the model can be represented e.g. as n-grams (garside al., 1987), local rules (hindle, 1989) <papid> P89-1015 </papid>or neural nets (schmid, 1994).<papid> C94-1027 </papid></prevsent>
</prevsection>
<citsent citstr=" W96-0102 ">
most hybrid approaches combine statistical infor-mation with automatically extracted rule-based in-formation (brill, 1995; <papid> W95-0101 </papid>daelemans et al, 1996).<papid> W96-0102 </papid></citsent>
<aftsection>
<nextsent>rel-atively little attention has been paid to models where the statistical approach is combined with truly lin-guistic model (i.e. one generated by linguist).
</nextsent>
<nextsent>this paper reports one such approach: syntactic rules written by linguist are combined with statistical information using the relaxation labelling algorithm.
</nextsent>
<nextsent>80 our application is very shallow parsing: identifi-cation of verbs, premodifiers, nominal and adverbial heads, and certain kinds of postmodifiers.
</nextsent>
<nextsent>we call this parser noun phrase parser.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A1083">
<title id=" A97-1013.xml">developing a hybrid np parser </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in practice, some correct readings are discarded, and some ambiguities remain unresolved (i.e. some words retain two or more alternative analyses).
</prevsent>
<prevsent>the system can use linguistic rules and corpus- based statistics.
</prevsent>
</prevsection>
<citsent citstr=" C90-3030 ">
notable about the system is that minimal human effort was needed for creating its language models (the linguistic consisting of syn-tactic disambiguation rules based on the constraint grammar framework (karlsson, 1990; <papid> C90-3030 </papid>karlsson et al., 1995); the corpus-based consisting of bigrams and trigrams): only one day was spent on writing the 107 syn-tactic disambiguation rules used by the linguis-tic parser.</citsent>
<aftsection>
<nextsent>no human annotators were needed for annotat-ing the training corpus (218,000 words of jour- nalese) used by the data-driven learning mod-ules of this system: the training corpus was an-notated by (i) tagging it with the engcg mor-phological tagger, (ii) making the tagged text syntactically ambiguous by adding the alterna-tive syntactic tags to the words, and (iii) re-solving most of these syntactic ambiguities by applying the parser with the 107 disambigua-tion rules.
</nextsent>
<nextsent>the system was tested against fresh sample of five texts (6,500 words).
</nextsent>
<nextsent>the system recall and pre-cision was measured by comparing its output to manually disambiguated version of the text.
</nextsent>
<nextsent>to in- crease the objectivity of the evaluation, system out-puts and the benchmark corpus are made publicly accessible (see section 6).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A1084">
<title id=" A97-1013.xml">developing a hybrid np parser </title>
<section> hybrid language model.  </section>
<citcontext>
<prevsection>
<prevsent>the statistical models were obtained from train-ing corpus of 218,000 words of journal ese, syntac-tically annotated using the linguistic parser (see above).
</prevsent>
<prevsent>although the linguistic cg-2 parser does not dis-ambiguate completely, it seems to have an almost perfect recall (cf.
</prevsent>
</prevsection>
<citsent citstr=" C92-2070 ">
table 1 below), and the noise in-troduced by the remaining ambiguity is assumed to be sufficiently lower than the signal, following the idea used in (yarowsky, 1992).<papid> C92-2070 </papid></citsent>
<aftsection>
<nextsent>the collected statistics were bigram and trigram occurrences . the algorithms and models were tested against hand-disambiguated benchmark corpus of over 6,500 words.
</nextsent>
<nextsent>we measure the performance of the different mod-els in terms of recall and precision.
</nextsent>
<nextsent>recall is the percentage of words that get the correct tag among the tags proposed by the system.
</nextsent>
<nextsent>precision is the percentage of tags proposed by the system that are correct.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A1085">
<title id=" A97-1013.xml">developing a hybrid np parser </title>
<section> hybrid language model.  </section>
<citcontext>
<prevsection>
<prevsent>the hand-written constraints and the statistics are combined using relaxation algorithm to analyze the test corpus, rising the precision to 96.1% and lowering the recall only to 97.2%.
</prevsent>
<prevsent>finally, reservation must be made: what we have not investigated in this paper is how much of the extra work done with the statistical module could have been done equally well or even better by spend-ing e.g. another day writing further collection of heuristic rules.
</prevsent>
</prevsection>
<citsent citstr=" A94-1008 ">
as suggested e.g. by tapanainen and voutilainen (1994) <papid> A94-1008 </papid>and chanod and tapanainen (1995), <papid> E95-1021 </papid>hand-coded heuristics may be worthwhile addition to  strictly  grammar-based rules.</citsent>
<aftsection>
<nextsent>acknowledgements we wish to thank timo j/irvinen, pasi tapanalnen and two anlp 97 referees for useful comments on earlier versions of this paper.
</nextsent>
<nextsent>the first author benefited from the collaboration of juha heikkil~ in the development of the linguistic description used by the engcg morphological tag- ger; the two-level compiler for morphological nmy- sis in engcg was written by kimmo koskenniemi; the recent version of the constraint grammar parser (cg-2) was written by pasi tapanainen.
</nextsent>
<nextsent>the con-straint grammar framework was originally proposed by fred karlsson.
</nextsent>

</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A1086">
<title id=" A97-1013.xml">developing a hybrid np parser </title>
<section> hybrid language model.  </section>
<citcontext>
<prevsection>
<prevsent>the hand-written constraints and the statistics are combined using relaxation algorithm to analyze the test corpus, rising the precision to 96.1% and lowering the recall only to 97.2%.
</prevsent>
<prevsent>finally, reservation must be made: what we have not investigated in this paper is how much of the extra work done with the statistical module could have been done equally well or even better by spend-ing e.g. another day writing further collection of heuristic rules.
</prevsent>
</prevsection>
<citsent citstr=" E95-1021 ">
as suggested e.g. by tapanainen and voutilainen (1994) <papid> A94-1008 </papid>and chanod and tapanainen (1995), <papid> E95-1021 </papid>hand-coded heuristics may be worthwhile addition to  strictly  grammar-based rules.</citsent>
<aftsection>
<nextsent>acknowledgements we wish to thank timo j/irvinen, pasi tapanalnen and two anlp 97 referees for useful comments on earlier versions of this paper.
</nextsent>
<nextsent>the first author benefited from the collaboration of juha heikkil~ in the development of the linguistic description used by the engcg morphological tag- ger; the two-level compiler for morphological nmy- sis in engcg was written by kimmo koskenniemi; the recent version of the constraint grammar parser (cg-2) was written by pasi tapanainen.
</nextsent>
<nextsent>the con-straint grammar framework was originally proposed by fred karlsson.
</nextsent>

</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A1087">
<title id=" C00-2148.xml">integrating compositional semantics into a verb lexicon </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in addition, semantic predicates are associated with each tree, which al-low for colnpositional interpretation.
</prevsent>
<prevsent>the difficulty o1  achieving adequate hand-crafted semantic representations has limited the lield of natural language t)rocessing to applications that can be contained within well-deiined sub-domains.
</prevsent>
</prevsection>
<citsent citstr=" W97-0204 ">
despite many different lexicon development ap-proaches (mel cuk, 1988; copestakc and sanfil-ippo, 1993; lowe et al, 1997), <papid> W97-0204 </papid>the field has yet to develop clear conseusus on guidelines for colnputational lexicon.</citsent>
<aftsection>
<nextsent>one of the most controver-sial areas in building such lexicon is polyselny: how senses can be computationally distinguished and characterized.
</nextsent>
<nextsent>we address this problem hy em-ploying compositional semantics and the adjunction of syntactic l)hrases to support regular verb sense extensions.
</nextsent>
<nextsent>this differs rom the lexical concep-tual structure (lcs) approach exemplified by voss (1996), which requires separate lcs representa-tion for each possible sense extension.
</nextsent>
<nextsent>in this pa-per we describe the construction of verbnet, verb lexicon with explicitly stated syntactic and seman-tic information for individual exical items, using levin verb classes (levin, 1993) to systematically construct lexical entries.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A1088">
<title id=" C00-2148.xml">integrating compositional semantics into a verb lexicon </title>
<section> levin classes.  </section>
<citcontext>
<prevsection>
<prevsent>the fundalnental ssulnption is that the syntactic frames arc direct reflection of the under- lying semantics, ltowever, levin classes exhibit in-consistencies that have hampered researchers  abil-ity to reference them directly in applications.
</prevsent>
<prevsent>many verbs ale listed in multiple classes, some of which have conflicting sets of syntactic frames.
</prevsent>
</prevsection>
<citsent citstr=" P98-1046 ">
dang et al (1998) <papid> P98-1046 </papid>showed that multiple listings could in some cases be interpreted as regular sense xtensions, and defined intersec tive levin classes, which are more line-grained, syntactically and semantically coher-ent refinement of basic levin classes.</citsent>
<aftsection>
<nextsent>we represent these verb classes and their regular sense xtensions in the ltag forlnalism.
</nextsent>
<nextsent>3.1 overview of formalism.
</nextsent>
<nextsent>lexicatized tree adjoining granunars consist of finite set of initial and auxiliary elementary hees, and two operations to combine them.
</nextsent>
<nextsent>the min-imal, non-recursive linguistic structures of lan-guage, such as verb and its complements, are cap-tured by initial trees.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A1089">
<title id=" C00-2148.xml">integrating compositional semantics into a verb lexicon </title>
<section> lexicalized riee adjoining grammars.  </section>
<citcontext>
<prevsection>
<prevsent>3.2 semantics for tags.
</prevsent>
<prevsent>there is range of previous work in incorporating semantics into tag trees.
</prevsent>
</prevsection>
<citsent citstr=" P97-1026 ">
stone and doran (1997) <papid> P97-1026 </papid>describe system used for generation that simul-taneously constructs the semantics and syntax of sentence using ltags.</citsent>
<aftsection>
<nextsent>joshi and vijay-shanker (1999), and kallmeyer and joshi (1999), describe the semantics of derivation tree as set of attach-ments of trees.
</nextsent>
<nextsent>the semantics of these attachments is given as conjunction of formulae in flat seman-tic representation.
</nextsent>
<nextsent>they provide specific method-ology for composing semantic representations much like candito and kahane (1998), where the direc-tionality of dominance in the derivation tree should be interpreted according to the operations used to build it.
</nextsent>
<nextsent>kallmeyer and joshi also use flat semantic representation to handle scope phenomena involv-ing quantifiers.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A1090">
<title id=" C00-2148.xml">integrating compositional semantics into a verb lexicon </title>
<section> lexicalized riee adjoining grammars.  </section>
<citcontext>
<prevsection>
<prevsent>the semantic predicates are primitive enough so that many may be reused in different rees.
</prevsent>
<prevsent>by using tags we get the additional benefit of an ex-isting parser that yields derivations and derived trees fiom which we can construct the compositional se-mantics of given sentence.
</prevsent>
</prevsection>
<citsent citstr=" J88-2003 ">
we decompose each event into tripar-tite structure in manner similar to moens and steedman (1988), <papid> J88-2003 </papid>introducing time function for each predicate to specify whether the predicate is true in the preparatory (d~ring(e)), cuhnination (er~d(e)), or consequent (res~ll:(e)) stage of an event.</citsent>
<aftsection>
<nextsent>hfitial trees capture tile semantics of the basic senses of verbs in each class.
</nextsent>
<nextsent>for example, many ithese restrictions are more like preferences that generate preferred reading of sentence.
</nextsent>
<nextsent>they may be relaxed epend- ing on the domain of particular pplication.
</nextsent>
<nextsent>1012 \[ cvcnt=e \] \[ event=e2 \] np.,.sh$ vp \[ cvcnt=e \] nparqo$ vp \[ event=e1 \] \[ +aninmtc \] \] \[ +animale \] v ni),~,.ql$ \] \[ +animate \] 1 1.11| ru l motion(during(e), xa,.al ) motion(during(el), xargl ) figure 1 : induced action alternation for the run verbs verbs in the run class can occur in the induced ac-tion alternation, in which the subject of the inmmsi- tive sentence has the same thematic role as the direct object in the transitive sentence.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A1091">
<title id=" A00-2027.xml">evaluating automatic dialogue strategy adaptation for a spoken dialogue system </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our results show that 1) both the mixed initiative and automatic adaptation aspects led to better system performance in terms of user satisfac-tion and dialogue fficiency, and 2) the system adap-tation behavior better matched user expectations, more efficiently resolved ialogue anomalies, and resulted in higher overall dialogue quality.
</prevsent>
<prevsent>recent advances in speech technologies have enabled spoken dialogue systems to employ mixed initiative di-alogue strategies (e.g.
</prevsent>
</prevsection>
<citsent citstr=" P96-1009 ">
(allen et al, 1996; <papid> P96-1009 </papid>sadek et al, 1996; meng et al, 1996)).</citsent>
<aftsection>
<nextsent>although these systems inter-act with users in manner more similar to human-human interactions than earlier systems employing system ini-tiative strategies, their response strategies are typically selected using only local dialogue context, disregarding dialogue history.
</nextsent>
<nextsent>therefore, their gain in naturalness and performance under optimal conditions often overshad-owed by their inability to cope with anomalies india- logues by automatically adapting dialogue strategies.
</nextsent>
<nextsent>in contrast, figure 1 shows dialogue in which the sys-tem automatically adapts dialogue strategies based on the current user utterance and dialogue history.
</nextsent>
<nextsent>1 af-ter failing to obtain valid response to an information- seeking query in utterance (4), the system adapted ia- logue strategies to provide additional information (6) that assisted the user in responding to the query.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A1092">
<title id=" A00-2027.xml">evaluating automatic dialogue strategy adaptation for a spoken dialogue system </title>
<section> experimental  design.  </section>
<citcontext>
<prevsection>
<prevsent>most of them are computer scientists, electrical engi- 205 all tasks.
</prevsent>
<prevsent>the subjects completed one task per call so that the dialogue history for one task did not affect the next task.
</prevsent>
</prevsection>
<citsent citstr=" P97-1035 ">
once they had completed all tasks in sequence using one system, they filled out questionnaire to as-sess user satisfaction by rating 8-9 statements, imilar to those in (walker et al, 1997), <papid> P97-1035 </papid>on scale of 1-5, where 5 indicated highest satisfaction.</citsent>
<aftsection>
<nextsent>approximately two days later, they attempted the same tasks using the other sys-tem.
</nextsent>
<nextsent>5 these experiments resulted in 112 dialogues with approximately 2,800 dialogue turns.
</nextsent>
<nextsent>in addition to user satisfaction ratings, we automat-ically logged, derived, and manually annotated num-ber of features (shown in boldface below).
</nextsent>
<nextsent>for each task/subject/system triplet, we computed the task suc-cess rate based on the percentage of slots correctly filled in on the task worksheet, and counted the # of calls needed to complete ach task.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="A1097">
<title id=" C00-2130.xml">corpus based development and evaluation of a system for processing definite descriptions </title>
<section> motivation </section>
<citcontext>
<prevsection>
<prevsent>the system is based on there- sults of corpus analysis previously reported, which showed how common discourse-new descriptions are in newspaper corpora, and identified several problems to be dealt with when developing compu-tational methods for interpreting bridging descrip-tions.
</prevsent>
<prevsent>the annotated corpus produced in this ear- licr work was used to extensively evaluate the pro-posed techniques for matchiug deli nite descriptions with their antecedents, discourse segmentation, rec-ognizing discourse-new descriptions, and suggest-ing anchors for bridging descriptions.
</prevsent>
</prevsection>
<citsent citstr=" J98-2001 ">
in previous work (poesio and vieira, 1998) <papid> J98-2001 </papid>we re-ported the results of corpus annotation experiments in which the subjects were asked to classify lhe uses of deli nite descriptions in wall stree journal arti-cles according to scheme derived from work by hawkins (1978) and prince (1981 ) and including three classes: i)ii{i ~ct anapltora, i)iscourse- new, and i~ridging desci{ii tion (clark, 1977).</citsent>
<aftsection>
<nextsent>this study showed that about half of the time, deli- nite descriptions are used to introduce new entity in the discourse, rather than to refer to an object al-ready mentioned.
</nextsent>
<nextsent>we also observed that our sub-jects didn always agree on the classification of given delinite; the problem was especially acute for bridging descriptions.
</nextsent>
<nextsent>in this paper, we present an implemented system for processing deli nite descriptions based on there- suits of that earlier study.
</nextsent>
<nextsent>in our system, techniques for recognising discourse-new descriptions play role as ilnportant as techniques for identifying the antecedent of anaphoric ones.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
</paper>
