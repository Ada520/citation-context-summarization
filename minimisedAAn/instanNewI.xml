<paper>
<cited id="I0">
<title id=" E12-1071.xml">unsupervised detection of downward entailing operators by maximizing classification certainty </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>reasoning about text has been long-standingchallenge in nlp, and there has been consider able debate both on what constitutes inference andwhat techniques should be used to support inference.
</prevsent>
<prevsent>one task involving inference that has recently received much attention is that of recognizing textual entailment (rte), in which the goal is to determine whether hypothesis sentence can be entailed from piece of source text (bentivogli et al 2010, for example).
</prevsent>
</prevsection>
<citsent citstr=" C08-1066 ">
an important consideration in rte is whethera sentence or context produces an entailment relation for events that are superset or subset of the original sentence (maccartney and manning, 2008).<papid> C08-1066 </papid></citsent>
<aftsection>
<nextsent>by default, contexts are upward-entailing,allowing reasoning from set of events to superset of events as seen in (1).
</nextsent>
<nextsent>in the scope of downward-entailing operator (deo), however, this entailment relation is reversed, such as in the scope of the classical deo not (2).
</nextsent>
<nextsent>there are also operators which are neither upward- nor downward entailing, such as the expression exactly three (3).
</nextsent>
<nextsent>(1) she sang in french.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1">
<title id=" E12-1071.xml">unsupervised detection of downward entailing operators by maximizing classification certainty </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in addition to this hypothesis, we further assume that there should only be one plausible deo candidate per npi context.
</prevsent>
<prevsent>while there are counter examples, this assumption is in practice very robust, and is useful constraint for our learning algorithm.
</prevsent>
</prevsection>
<citsent citstr=" H92-1045 ">
an analogy can be drawn to the one sense per discourse assumption in word sense disambiguation (gale et al 1992).<papid> H92-1045 </papid></citsent>
<aftsection>
<nextsent>the related and as we will argue, more difficult problem of detecting npis has also been studied, and in fact predates the work on deo detection.
</nextsent>
<nextsent>hoek sema (1997) performed the first corpus-based study of npis, predominantly for dutch, and there has also been work on detecting npis in german which assumes linguistic knowledge of licensing contexts for npis (lichte and soehn, 2007).
</nextsent>
<nextsent>richter et al(2010) make this assumption as well as use syntactic structure to extract npis that are multi-word expressions.parse information is an especially important consideration in freer-word-order languages like german where mwe may not appear as contiguous string.
</nextsent>
<nextsent>in this paper, we explicitly do not assume detailed linguistic knowledge about licensing contexts for npis and do not assume that aparser is available, since neither of these are guaranteed when extending this technique to resource poor languages.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2">
<title id=" E12-1071.xml">unsupervised detection of downward entailing operators by maximizing classification certainty </title>
<section> conclusion.  </section>
<citcontext>
<prevsection>
<prevsent>the performance of our algorithm suggests that it is suitable for other corpora and languages.
</prevsent>
<prevsent>interesting future research directions include detecting deos of more than one word as well as distinguishing the particular word sense and subcategorization that is downward-entailing.
</prevsent>
</prevsection>
<citsent citstr=" W10-3110 ">
an other problem that should be addressed is the scope of the downward entailment, generalizing work being done in detecting the scope of negation (councill et al 2010, <papid> W10-3110 </papid>for example).</citsent>
<aftsection>
<nextsent>acknowledgment swe would like to thank cristian danescu niculescu-mizil for his help with replicating his results on the bllip corpus.
</nextsent>
<nextsent>this project was supported by the natural sciences and engineering research council of canada.
</nextsent>


</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3">
<title id=" E09-1096.xml">feature based method for document alignment incomparable news corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>representations.
</prevsent>
<prevsent>the assignment of descriptors is trained by log-likelihood test and computed by ?????, co sine, and okapi.
</prevsent>
</prevsection>
<citsent citstr=" C04-1138 ">
similarly, pouliquen et al (2004) <papid> C04-1138 </papid>use linear combination of three types of knowledge: cognates, geographical place names reference, and map documents based on the eurovoc.</citsent>
<aftsection>
<nextsent>the major limitation of these works is the use of eurovoc, which is specific resource workable only for european languages.
</nextsent>
<nextsent>aligning documents across parallel corpora is another area of interest.
</nextsent>
<nextsent>patry and langlais (2005) use three similarity scores, cosine, normalized edit distance, and sentence alignment score, to compute the similarity between two parallel documents.
</nextsent>
<nextsent>an ada boost classifier is trained on list of scored text pairs labeled as parallel or non parallel.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4">
<title id=" E09-1096.xml">feature based method for document alignment incomparable news corpora </title>
<section> monolingual term distribution.  </section>
<citcontext>
<prevsection>
<prevsent>firstly, the smaller number of terms compared to the number of words present in any document would imply fewer possible document alignment pairs for the system.
</prevsent>
<prevsent>this increases the computation speed remarkably.
</prevsent>
</prevsection>
<citsent citstr=" I08-2084 ">
to extract automatically the list of terms in each document, we use the term extraction model from vu et al (2008).<papid> I08-2084 </papid></citsent>
<aftsection>
<nextsent>in corpora used in our experiments, the average ratios of word/term per document are 556/37, 410/28 and 384/28 for english, chinese, and malay respectively.
</nextsent>
<nextsent>the other advantage of using terms is that terms are more distinctive than words as they contain less ambiguity, thus enabling high correlation to be observed when compared with single words.
</nextsent>
<nextsent>3.3.2 bilingual dictionary incorporation in addition to using terms for the computation, we observed from equation (3) that the only mutual feature relating the two documents is the frequency distribution coefficient ???, ??
</nextsent>
<nextsent>it is likely that the alignment performance could be enhanced if more features relating the two documents are incorporated.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5">
<title id=" E09-1096.xml">feature based method for document alignment incomparable news corpora </title>
<section> monolingual term distribution.  </section>
<citcontext>
<prevsection>
<prevsent>a popular technique for time sequence matching is to use discrete fourier transform (???
</prevsent>
<prevsent>) (agrawal et al 1993).
</prevsent>
</prevsection>
<citsent citstr=" P06-1103 ">
more recently, klementiev and roth (2006) <papid> P06-1103 </papid>also use f-index (hetland, 2004), score using ???, to calculate the time distribution similarity.</citsent>
<aftsection>
<nextsent>in our model, we assume that the frequency chain of word is sequence, and calculate ???
</nextsent>
<nextsent>score for each chain by the following formula: ??
</nextsent>
<nextsent>(6) in time series research, it is proven that only the first few ? coefficients of ???
</nextsent>
<nextsent>chain are strong and important for comparison (agrawal et al, 1993).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I6">
<title id=" E06-2015.xml">semantic role labeling for coreference resolution </title>
<section> introduction </section>
<citcontext>
<prevsection>

<prevsent>extending machine learning based coreference resolution system with feature capturing automatically generated information about semantic roles improves its performance.
</prevsent>
</prevsection>
<citsent citstr=" J01-4004 ">
the last years have seen boost of work devoted to the development of machine learning based coreference resolution systems (soon et al, 2001; <papid> J01-4004 </papid>ng &amp; cardie, 2002; <papid> P02-1014 </papid>kehler et al, 2004, <papid> N04-1037 </papid>inter alia).similarly, many researchers have explored techniques for robust, broad coverage semantic parsing in terms of semantic role labeling (gildea &amp; jurafsky, 2002; <papid> J02-3001 </papid>carreras &amp; ma`rquez, 2005, srl henceforth).this paper explores whether coreference resolution can benefit from srl, more specifically,which phenomena are affected by such information.</citsent>
<aftsection>
<nextsent>the motivation comes from the fact that current coreference resolution systems are mostly relying on rather shallow features, such as the distance between the co referent expressions, string matching, and linguistic form.
</nextsent>
<nextsent>on the other hand,the literature emphasizes since the very beginning the relevance of world knowledge and inference (charniak, 1973).
</nextsent>
<nextsent>as an example, consider sentence from the automatic content extraction (ace) 2003 data.
</nextsent>
<nextsent>(1) state commission of inquiry into the sinking of the kursk will convene in moscow on wednesday, the interfax newsagency reported.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I7">
<title id=" E06-2015.xml">semantic role labeling for coreference resolution </title>
<section> introduction </section>
<citcontext>
<prevsection>

<prevsent>extending machine learning based coreference resolution system with feature capturing automatically generated information about semantic roles improves its performance.
</prevsent>
</prevsection>
<citsent citstr=" P02-1014 ">
the last years have seen boost of work devoted to the development of machine learning based coreference resolution systems (soon et al, 2001; <papid> J01-4004 </papid>ng &amp; cardie, 2002; <papid> P02-1014 </papid>kehler et al, 2004, <papid> N04-1037 </papid>inter alia).similarly, many researchers have explored techniques for robust, broad coverage semantic parsing in terms of semantic role labeling (gildea &amp; jurafsky, 2002; <papid> J02-3001 </papid>carreras &amp; ma`rquez, 2005, srl henceforth).this paper explores whether coreference resolution can benefit from srl, more specifically,which phenomena are affected by such information.</citsent>
<aftsection>
<nextsent>the motivation comes from the fact that current coreference resolution systems are mostly relying on rather shallow features, such as the distance between the co referent expressions, string matching, and linguistic form.
</nextsent>
<nextsent>on the other hand,the literature emphasizes since the very beginning the relevance of world knowledge and inference (charniak, 1973).
</nextsent>
<nextsent>as an example, consider sentence from the automatic content extraction (ace) 2003 data.
</nextsent>
<nextsent>(1) state commission of inquiry into the sinking of the kursk will convene in moscow on wednesday, the interfax newsagency reported.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I8">
<title id=" E06-2015.xml">semantic role labeling for coreference resolution </title>
<section> introduction </section>
<citcontext>
<prevsection>

<prevsent>extending machine learning based coreference resolution system with feature capturing automatically generated information about semantic roles improves its performance.
</prevsent>
</prevsection>
<citsent citstr=" N04-1037 ">
the last years have seen boost of work devoted to the development of machine learning based coreference resolution systems (soon et al, 2001; <papid> J01-4004 </papid>ng &amp; cardie, 2002; <papid> P02-1014 </papid>kehler et al, 2004, <papid> N04-1037 </papid>inter alia).similarly, many researchers have explored techniques for robust, broad coverage semantic parsing in terms of semantic role labeling (gildea &amp; jurafsky, 2002; <papid> J02-3001 </papid>carreras &amp; ma`rquez, 2005, srl henceforth).this paper explores whether coreference resolution can benefit from srl, more specifically,which phenomena are affected by such information.</citsent>
<aftsection>
<nextsent>the motivation comes from the fact that current coreference resolution systems are mostly relying on rather shallow features, such as the distance between the co referent expressions, string matching, and linguistic form.
</nextsent>
<nextsent>on the other hand,the literature emphasizes since the very beginning the relevance of world knowledge and inference (charniak, 1973).
</nextsent>
<nextsent>as an example, consider sentence from the automatic content extraction (ace) 2003 data.
</nextsent>
<nextsent>(1) state commission of inquiry into the sinking of the kursk will convene in moscow on wednesday, the interfax newsagency reported.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I9">
<title id=" E06-2015.xml">semantic role labeling for coreference resolution </title>
<section> introduction </section>
<citcontext>
<prevsection>

<prevsent>extending machine learning based coreference resolution system with feature capturing automatically generated information about semantic roles improves its performance.
</prevsent>
</prevsection>
<citsent citstr=" J02-3001 ">
the last years have seen boost of work devoted to the development of machine learning based coreference resolution systems (soon et al, 2001; <papid> J01-4004 </papid>ng &amp; cardie, 2002; <papid> P02-1014 </papid>kehler et al, 2004, <papid> N04-1037 </papid>inter alia).similarly, many researchers have explored techniques for robust, broad coverage semantic parsing in terms of semantic role labeling (gildea &amp; jurafsky, 2002; <papid> J02-3001 </papid>carreras &amp; ma`rquez, 2005, srl henceforth).this paper explores whether coreference resolution can benefit from srl, more specifically,which phenomena are affected by such information.</citsent>
<aftsection>
<nextsent>the motivation comes from the fact that current coreference resolution systems are mostly relying on rather shallow features, such as the distance between the co referent expressions, string matching, and linguistic form.
</nextsent>
<nextsent>on the other hand,the literature emphasizes since the very beginning the relevance of world knowledge and inference (charniak, 1973).
</nextsent>
<nextsent>as an example, consider sentence from the automatic content extraction (ace) 2003 data.
</nextsent>
<nextsent>(1) state commission of inquiry into the sinking of the kursk will convene in moscow on wednesday, the interfax newsagency reported.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I11">
<title id=" E06-2015.xml">semantic role labeling for coreference resolution </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>srl provides the semantic relationships that constituents have with predicates, thus allowing us to include document-level event descriptive information into the relations holding between referring expressions (res).
</prevsent>
<prevsent>this layer of semantic context abstracts from the specific lexical expressions used, and therefore represents higher level of abstraction than predicate argument statistics(kehler et al, 2004) <papid> N04-1037 </papid>and latent semantic analysis used as model of world knowledge (klebanov &amp; wiemer-hastings, 2002).</prevsent>
</prevsection>
<citsent citstr=" H05-1003 ">
in this respect, the present work is closer in spirit to ji et al (2005),<papid> H05-1003 </papid>who explore the employment of the ace 2004 relation ontology as semantic filter.</citsent>
<aftsection>
<nextsent>2.1 corpora used.
</nextsent>
<nextsent>the system was initially proto typed using themuc-6 and muc-7 datasets (chinchor &amp; sundheim, 2003; chinchor, 2001), using the standard partitioning of 30 texts for training and 20-30 texts for testing.
</nextsent>
<nextsent>then, we developed and tested the system with the ace 2003 training data corpus (mitchell et al, 2003)1.
</nextsent>
<nextsent>both the newswire (nwire) and broadcast news (bnews) sections where split into 60-20-20% document-based partitions for training, development, and testing, and later per-partition merged (merged) for system evaluation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I12">
<title id=" E06-2015.xml">semantic role labeling for coreference resolution </title>
<section> coreference resolution using srl.  </section>
<citcontext>
<prevsection>
<prevsent>the distribution of coreference chains and referring expressions is given in table 1.
</prevsent>
<prevsent>2.2 learning algorithm.
</prevsent>
</prevsection>
<citsent citstr=" J96-1002 ">
for learning coreference decisions, we used maximum entropy (berger et al, 1996) <papid> J96-1002 </papid>model.coreference resolution is viewed as binary classification task: given pair of res, the classifier has to decide whether they are co referent or not.first, set of pre-processing components includ 1we used the training data corpus only, as the availability of the test data was restricted to ace participants.</citsent>
<aftsection>
<nextsent>143 bnews nwire #coref ch.
</nextsent>
<nextsent>#pron.
</nextsent>
<nextsent>#comm.
</nextsent>
<nextsent>nouns #prop.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I17">
<title id=" E06-2015.xml">semantic role labeling for coreference resolution </title>
<section> coreference resolution using srl.  </section>
<citcontext>
<prevsection>
<prevsent>unfortunately, simple wordnet semantic class lookup exhibits problems such as coverage and sense disambiguation3, which make the wn class feature very noisy.
</prevsent>
<prevsent>as aconsequence, we propose in the following to enrich the semantic knowledge made available to the classifier by using srl information.
</prevsent>
</prevsection>
<citsent citstr=" N04-1030 ">
in our experiments we use the assert parser (pradhan et al, 2004), <papid> N04-1030 </papid>an svm based semantic role tagger which uses full syntactic analysis to automatically identify all verb predicates in sentence together with their semantic arguments, which are output as propbank arguments (palmer et al, 2005).<papid> J05-1004 </papid></citsent>
<aftsection>
<nextsent>it is often the case that the semantic arguments output by the parser do not align with any of the previously identified noun phrases.
</nextsent>
<nextsent>in this case, we pass semantic role label to re only in case the two phrases share the same head.
</nextsent>
<nextsent>labels have the form arg1 pred1 . . .
</nextsent>
<nextsent>argn predn?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I18">
<title id=" E06-2015.xml">semantic role labeling for coreference resolution </title>
<section> coreference resolution using srl.  </section>
<citcontext>
<prevsection>
<prevsent>unfortunately, simple wordnet semantic class lookup exhibits problems such as coverage and sense disambiguation3, which make the wn class feature very noisy.
</prevsent>
<prevsent>as aconsequence, we propose in the following to enrich the semantic knowledge made available to the classifier by using srl information.
</prevsent>
</prevsection>
<citsent citstr=" J05-1004 ">
in our experiments we use the assert parser (pradhan et al, 2004), <papid> N04-1030 </papid>an svm based semantic role tagger which uses full syntactic analysis to automatically identify all verb predicates in sentence together with their semantic arguments, which are output as propbank arguments (palmer et al, 2005).<papid> J05-1004 </papid></citsent>
<aftsection>
<nextsent>it is often the case that the semantic arguments output by the parser do not align with any of the previously identified noun phrases.
</nextsent>
<nextsent>in this case, we pass semantic role label to re only in case the two phrases share the same head.
</nextsent>
<nextsent>labels have the form arg1 pred1 . . .
</nextsent>
<nextsent>argn predn?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I19">
<title id=" E06-2015.xml">semantic role labeling for coreference resolution </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>144 muc-6 muc-7 original p f1 p f1 soon et al 58.6 67.3 62.3 56.1 65.5 60.4 duplicated baseline 64.9 65.6 65.3 55.1 68.5 61.1 table 2: results on mucj semrole the semantic role argument predicate pairs of rej .for the ace 2003 data, 11,406 of 32,502 automatically extracted noun phrases were tagged with 2,801 different argument-predicate pairs.
</prevsent>
<prevsent>3.1 performance metrics.
</prevsent>
</prevsection>
<citsent citstr=" M95-1005 ">
we report in the following tables the muc score (vilain et al, 1995).<papid> M95-1005 </papid></citsent>
<aftsection>
<nextsent>scores in table 2 are computed for all noun phrases appearing in either the key or the system response, whereas tables 3and 4 refer to scoring only those phrases which appear in both the key and the response.
</nextsent>
<nextsent>we discard therefore those responses not present in the key, as we are interested here in establishing the upper limit of the improvements given by srl.
</nextsent>
<nextsent>we also report the accuracy score for all three types of ace mentions, namely pronouns, common nouns and proper names.
</nextsent>
<nextsent>accuracy is the percentage of res of given mention type correctly resolved divided by the total number of res of the same type given in the key.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I21">
<title id=" E06-2023.xml">unsupervised discovery of persian morphemes </title>
<section> related works.  </section>
<citcontext>
<prevsection>
<prevsent>some of them are supervised and use some information about words such as part of speech (pos) tags, morphological rules, suffix list, lexicon, etc. other approaches are unsupervised and use only raw corpus to extract morphemes.
</prevsent>
<prevsent>in this section we concentrate on some unsupervised methods as related works.
</prevsent>
</prevsection>
<citsent citstr=" P04-2012 ">
[monson 2004] <papid> P04-2012 </papid>presents framework for unsupervised induction of natural language morphology, wherein candidate suffixes are grouped into candidate inflection classes, which are then placed in lattice structure.</citsent>
<aftsection>
<nextsent>with similar arranged inflection classes placed near one candidate in the lattice, it proposes this structure to be an ideal search space in which to isolate the true inflection classes of language.
</nextsent>
<nextsent>[schone and jurafsky 2000] <papid> W00-0712 </papid>presents an unsupervised model in which knowledge-free distributional cues are combined orthography-based with information automatically extracted from semantic word cooccurrence patterns in the input corpus.</nextsent>
<nextsent>word induction from natural language text without word boundaries is also studied in [deligne and bimtol 1997], where mdl- based model optimization measures are used.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I22">
<title id=" E06-2023.xml">unsupervised discovery of persian morphemes </title>
<section> related works.  </section>
<citcontext>
<prevsection>
<prevsent>[monson 2004] <papid> P04-2012 </papid>presents framework for unsupervised induction of natural language morphology, wherein candidate suffixes are grouped into candidate inflection classes, which are then placed in lattice structure.</prevsent>
<prevsent>with similar arranged inflection classes placed near one candidate in the lattice, it proposes this structure to be an ideal search space in which to isolate the true inflection classes of language.</prevsent>
</prevsection>
<citsent citstr=" W00-0712 ">
[schone and jurafsky 2000] <papid> W00-0712 </papid>presents an unsupervised model in which knowledge-free distributional cues are combined orthography-based with information automatically extracted from semantic word cooccurrence patterns in the input corpus.</citsent>
<aftsection>
<nextsent>word induction from natural language text without word boundaries is also studied in [deligne and bimtol 1997], where mdl- based model optimization measures are used.
</nextsent>
<nextsent>viterbi or the forward- backward algorithm (an em algo rithm) is used for improving the segmentation of the corpus.
</nextsent>
<nextsent>some of the approaches remove spaces from text and try to identify word boundaries utilizing e.g. entropy- based measures, as in [zellig and harris, 1967; redlich, 1993].
</nextsent>
<nextsent>[brent, 1999] presents general, modular probabilistic model structure for word discovery.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I23">
<title id=" E06-2023.xml">unsupervised discovery of persian morphemes </title>
<section> related works.  </section>
<citcontext>
<prevsection>
<prevsent>[brent, 1999] presents general, modular probabilistic model structure for word discovery.
</prevsent>
<prevsent>he uses minimum representation length criterion for model optimization and applies an incremental, greedy search algorithm which is suit able for on- line learning such that children might employ.
</prevsent>
</prevsection>
<citsent citstr=" W02-0606 ">
[baroni, et al 2002] <papid> W02-0606 </papid>proposes an algorithm that takes an unannotated corpus as its input, and ranked list of probable returning related pairs as its output.</citsent>
<aftsection>
<nextsent>it discovers related pairs by looking morphologically for pairs that are both ortho graphically and semantically similar.
</nextsent>
<nextsent>[goldsmith 2001] <papid> J01-2001 </papid>concentrates on stem+suffixlanguages, in particular indo-european languages, and produces output that would match as closely as possible with the analysis given by human morphologist.</nextsent>
<nextsent>he further assumes that stems form groups that he calls signatures, and each signature shares set of possible affixes.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I24">
<title id=" E06-2023.xml">unsupervised discovery of persian morphemes </title>
<section> related works.  </section>
<citcontext>
<prevsection>
<prevsent>[baroni, et al 2002] <papid> W02-0606 </papid>proposes an algorithm that takes an unannotated corpus as its input, and ranked list of probable returning related pairs as its output.</prevsent>
<prevsent>it discovers related pairs by looking morphologically for pairs that are both ortho graphically and semantically similar.</prevsent>
</prevsection>
<citsent citstr=" J01-2001 ">
[goldsmith 2001] <papid> J01-2001 </papid>concentrates on stem+suffixlanguages, in particular indo-european languages, and produces output that would match as closely as possible with the analysis given by human morphologist.</citsent>
<aftsection>
<nextsent>he further assumes that stems form groups that he calls signatures, and each signature shares set of possible affixes.
</nextsent>
<nextsent>he applies an mdl criterion for model optimization.
</nextsent>
<nextsent>our task is to find the correct segmentation of the source text into morphemes while we dont have any information about words or any structural rules to make them.
</nextsent>
<nextsent>so we use an algorithm that works based on minimization of some heuristic cost function.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I25">
<title id=" E09-1003.xml">on the use of comparable corpora to improve smt performance </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>yang and lee (2003) use an approach based on dynamic programming to identify potential parallel sentences in title pairs.
</prevsent>
<prevsent>longest common sub sequence, edit operations and match-based score functions are subsequently used to determine confidence scores.
</prevsent>
</prevsection>
<citsent citstr=" J03-3002 ">
resnik and smith (2003) <papid> J03-3002 </papid>propose their strand web-mining based system and show that their approach is able to find large numbers of similar document pairs.</citsent>
<aftsection>
<nextsent>works aimed at discovering parallel sentences 16 french: au total, 1,634 million delecteurs doivent designer les 90 deputes de la prochaine legislature parmi 1.390 candid ats presentes par 17 partis, dont huit sont representes au parlement.
</nextsent>
<nextsent>query: in total, 1,634 million voters will designate the 90 members of the next parliament among 1.390 candidates presented by 17 parties, eight of which are represented in parliament.
</nextsent>
<nextsent>result: some 1.6 million voters were registered to elect the 90 members of the legislature from 1,390 candidates from 17 parties, eight of which are represented in parliament, several civilian organisations and independent lists.
</nextsent>
<nextsent>french: notre implication en irak rend possible que dautres pays membres de lotan, comme lallemagne par exemple, envoient un plus gros contingent?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I26">
<title id=" E09-1003.xml">on the use of comparable corpora to improve smt performance </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>result: nicola duckworth, head of amnesty internationals europe and central asia department, said the non-governmental organisations (ngos) would call on putin to put an end to human rights abuses in the north caucasus, including the war-torn province of chechnya.figure 1: some examples of french source sentence, the smt translation used as query and the potential parallel sentence as determined by information retrieval.
</prevsent>
<prevsent>bold parts are the extra tails at the end of the sentences which we automatically removed.
</prevsent>
</prevsection>
<citsent citstr=" P03-1010 ">
include (utiyama and isahara, 2003), <papid> P03-1010 </papid>who use cross-language information retrieval techniques and dynamic programming to extract sentences from an english-japanese comparable corpus.they identify similar article pairs, and then, treating these pairs as parallel texts, align their sentences on sentence pair similarity score and usedp to find the least-cost alignment over the document pair.</citsent>
<aftsection>
<nextsent>fung and cheung (2004) <papid> W04-3208 </papid>approach the problem by using cosine similarity measure to match foreign and english documents.</nextsent>
<nextsent>they work on very non-parallel corpora?.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I27">
<title id=" E09-1003.xml">on the use of comparable corpora to improve smt performance </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>bold parts are the extra tails at the end of the sentences which we automatically removed.
</prevsent>
<prevsent>include (utiyama and isahara, 2003), <papid> P03-1010 </papid>who use cross-language information retrieval techniques and dynamic programming to extract sentences from an english-japanese comparable corpus.they identify similar article pairs, and then, treating these pairs as parallel texts, align their sentences on sentence pair similarity score and usedp to find the least-cost alignment over the document pair.</prevsent>
</prevsection>
<citsent citstr=" W04-3208 ">
fung and cheung (2004) <papid> W04-3208 </papid>approach the problem by using cosine similarity measure to match foreign and english documents.</citsent>
<aftsection>
<nextsent>they work on very non-parallel corpora?.
</nextsent>
<nextsent>they then generate all possible sentence pairs and select thebest ones based on threshold on cosine similarity scores.
</nextsent>
<nextsent>using the extracted sentences they learn dictionary and iterate over with more sentence pairs.
</nextsent>
<nextsent>recent work by munteanu and marcu (2005) <papid> J05-4003 </papid>uses bilingual lexicon to translate someof the words of the source sentence.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I28">
<title id=" E09-1003.xml">on the use of comparable corpora to improve smt performance </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>they then generate all possible sentence pairs and select thebest ones based on threshold on cosine similarity scores.
</prevsent>
<prevsent>using the extracted sentences they learn dictionary and iterate over with more sentence pairs.
</prevsent>
</prevsection>
<citsent citstr=" J05-4003 ">
recent work by munteanu and marcu (2005) <papid> J05-4003 </papid>uses bilingual lexicon to translate someof the words of the source sentence.</citsent>
<aftsection>
<nextsent>these translations are then used to query the database to find matching translations using information retrieval(ir) techniques.
</nextsent>
<nextsent>candidate sentences are determined based on word overlap and the decision whether sentence pair is parallel or not is performed by maximum entropy classifier trained on parallel sentences.
</nextsent>
<nextsent>bootstrapping is used andthe size of the learned bilingual dictionary is increased over iterations to get better results.
</nextsent>
<nextsent>our technique is similar to that of (munteanu and marcu, 2005) <papid> J05-4003 </papid>but we bypass the need of the bilingual dictionary by using proper smt translations and instead of maximum entropy classifier we use simple measures like the word error rate(wer) and the translation error rate (ter) to decide whether sentences are parallel or not.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I32">
<title id=" E09-1003.xml">on the use of comparable corpora to improve smt performance </title>
<section> baseline smt system.  </section>
<citcontext>
<prevsection>
<prevsent>the goal of smt is to produce target sentence from source sentence . among all possible target language sentences the one with the highest probability is chosen: e?
</prevsent>
<prevsent>= arg max pr(e|f) (1) = arg max pr(f |e) pr(e) (2) where pr(f |e) is the translation model andpr(e) is the target language model (lm).
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
this approach is usually referred to as the noisy source channel approach in smt (brown et al, 1993).<papid> J93-2003 </papid>bilingual corpora are needed to train the translation model and monolingual texts to train the target language model.</citsent>
<aftsection>
<nextsent>it is today common practice to use phrases as translation units (koehn et al, 2003; <papid> N03-1017 </papid>och andney, 2003) <papid> J03-1002 </papid>instead of the original word-based ap proach.</nextsent>
<nextsent>a phrase is defined as group of source words f?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I33">
<title id=" E09-1003.xml">on the use of comparable corpora to improve smt performance </title>
<section> baseline smt system.  </section>
<citcontext>
<prevsection>
<prevsent>= arg max pr(e|f) (1) = arg max pr(f |e) pr(e) (2) where pr(f |e) is the translation model andpr(e) is the target language model (lm).
</prevsent>
<prevsent>this approach is usually referred to as the noisy source channel approach in smt (brown et al, 1993).<papid> J93-2003 </papid>bilingual corpora are needed to train the translation model and monolingual texts to train the target language model.</prevsent>
</prevsection>
<citsent citstr=" N03-1017 ">
it is today common practice to use phrases as translation units (koehn et al, 2003; <papid> N03-1017 </papid>och andney, 2003) <papid> J03-1002 </papid>instead of the original word-based ap proach.</citsent>
<aftsection>
<nextsent>a phrase is defined as group of source words f?
</nextsent>
<nextsent>that should be translated together into group of target words e?.
</nextsent>
<nextsent>the translation model inphrase-based systems includes the phrase translation probabilities in both directions, i.e. (e?|f?)and (f? |e?).
</nextsent>
<nextsent>the use of maximum entropy approach simplifies the introduction of several additional models explaining the translation process : e?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I34">
<title id=" E09-1003.xml">on the use of comparable corpora to improve smt performance </title>
<section> baseline smt system.  </section>
<citcontext>
<prevsection>
<prevsent>= arg max pr(e|f) (1) = arg max pr(f |e) pr(e) (2) where pr(f |e) is the translation model andpr(e) is the target language model (lm).
</prevsent>
<prevsent>this approach is usually referred to as the noisy source channel approach in smt (brown et al, 1993).<papid> J93-2003 </papid>bilingual corpora are needed to train the translation model and monolingual texts to train the target language model.</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
it is today common practice to use phrases as translation units (koehn et al, 2003; <papid> N03-1017 </papid>och andney, 2003) <papid> J03-1002 </papid>instead of the original word-based ap proach.</citsent>
<aftsection>
<nextsent>a phrase is defined as group of source words f?
</nextsent>
<nextsent>that should be translated together into group of target words e?.
</nextsent>
<nextsent>the translation model inphrase-based systems includes the phrase translation probabilities in both directions, i.e. (e?|f?)and (f? |e?).
</nextsent>
<nextsent>the use of maximum entropy approach simplifies the introduction of several additional models explaining the translation process : e?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I35">
<title id=" E09-1003.xml">on the use of comparable corpora to improve smt performance </title>
<section> baseline smt system.  </section>
<citcontext>
<prevsection>
<prevsent>the use of maximum entropy approach simplifies the introduction of several additional models explaining the translation process : e?
</prevsent>
<prevsent>= arg maxpr(e|f) = arg max {exp( ? ihi(e, f))} (3)the feature functions hi are the system models and the weights are typically optimized to maximize scoring function on development smt baseline system phrase table 3.3g 4gram lm fr en automatic translations en words words 275m up to fr en human translations words 116m up to figure 2: using an smt system used to translate large amounts of monolingual data.
</prevsent>
</prevsection>
<citsent citstr=" P02-1038 ">
set (och and ney, 2002).<papid> P02-1038 </papid></citsent>
<aftsection>
<nextsent>in our system fourteen features functions were used, namely phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion model, word and phrase penalty, and target language model.
</nextsent>
<nextsent>the system is based on the moses smt toolkit (koehn et al, 2007) <papid> P07-2045 </papid>and constructed as follows.</nextsent>
<nextsent>first, giza++ is used to perform word alignments in both directions.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I36">
<title id=" E09-1003.xml">on the use of comparable corpora to improve smt performance </title>
<section> baseline smt system.  </section>
<citcontext>
<prevsection>
<prevsent>set (och and ney, 2002).<papid> P02-1038 </papid></prevsent>
<prevsent>in our system fourteen features functions were used, namely phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion model, word and phrase penalty, and target language model.</prevsent>
</prevsection>
<citsent citstr=" P07-2045 ">
the system is based on the moses smt toolkit (koehn et al, 2007) <papid> P07-2045 </papid>and constructed as follows.</citsent>
<aftsection>
<nextsent>first, giza++ is used to perform word alignments in both directions.
</nextsent>
<nextsent>second, phrases and lexical reorderings are extracted using the default settings of the moses smt toolkit.
</nextsent>
<nextsent>the 4-gram back-off target lm is trained on the english part of the bitexts and the gigaword corpus of about 3.2 billion words.
</nextsent>
<nextsent>therefore, it is likely that the target language model includes at least some of the translations of the french gigaword corpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I37">
<title id=" E09-1003.xml">on the use of comparable corpora to improve smt performance </title>
<section> system architecture.  </section>
<citcontext>
<prevsection>
<prevsent>once we have the results from information retrieval, we proceed on to decide whether sentences are parallel or not.
</prevsent>
<prevsent>at this stage we choose the best scoring sentence as determined by the toolkit 4http://ir.dcs.gla.ac.uk/resources/ linguistic utils/stop words and pass the sentence pair through further filters.
</prevsent>
</prevsection>
<citsent citstr=" J93-1004 ">
gale and church (1993) <papid> J93-1004 </papid>based their align program on the fact that longer sentences in one language tend to be translated into longer sentences in the other language, and that shorter sentences tend to be translated into shorter sentences.</citsent>
<aftsection>
<nextsent>we also usethe same logic in our initial selection of the sentence pairs.
</nextsent>
<nextsent>a sentence pair is selected for further processing if the length ratio is not more than 1.6.
</nextsent>
<nextsent>a relaxed factor of 1.6 was chosen keeping in consideration the fact that french sentences are longer than their respective english translations.
</nextsent>
<nextsent>finally, we discarded all sentences that contain alarge fraction of numbers.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I41">
<title id=" E06-2016.xml">the god model </title>
<section> the god algorithm.  </section>
<citcontext>
<prevsection>
<prevsent>domain words can be disambiguated by simply identifying the domain of the text.as consequence, concepts belonging to different domains are basically unrelated.
</prevsent>
<prevsent>this observation is crucial from methodological pointof view, allowing us to perform large scale structural analysis of the whole lexicon of language,otherwise computationally infeasible.
</prevsent>
</prevsection>
<citsent citstr=" P05-1050 ">
in fact, restricting the attention to particular domain is away to reduce the complexity of the overall relation extraction task, that is evidently quadratic in the number of terms.domain information can be expressed by exploiting domain models (dms) (gliozzo et al,2005).<papid> P05-1050 </papid></citsent>
<aftsection>
<nextsent>a dm is represented by k ? k? rectangular matrix d, containing the domain relevance for each term with respect to each domain, where is the cardinality of the vocabulary, and k?
</nextsent>
<nextsent>is the size of the domain set.
</nextsent>
<nextsent>dms can be acquired from texts in totally unsupervised way by exploiting lexical coherence assumption (gliozzo, 2005).
</nextsent>
<nextsent>to this aim, term clustering algorithms can be adopted: each cluster represents semantic domain.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I42">
<title id=" E06-2016.xml">the god model </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>once query has been formulated, and set of relations has been extracted, it is not clear how to evaluate the quality of the results.
</prevsent>
<prevsent>the first four columns of the example below show the evaluation we did for the query karl marx.
</prevsent>
</prevsection>
<citsent citstr=" W99-0707 ">
karl marx: trim economic_organisation determines superstructure trum capitalism needs capitalists frim proletariat overthrow bourgeoisie trim marx understood capitalism marx later marxists trim labour_power be production trim societies are class_societies rim private_property equals exploitation trim primitive_societies were classless trim social_relationships form economic_basis trim max_weber criticised marxist_view 1for the experiments reported in this paper we used memory-based shallow parser developed at cnts antwerp and ilk tilburg (daelemans et al, 1999) <papid> W99-0707 </papid>together with set of scripts to extract svo patterns (reinberger et al, 2004) kindly put at our disposal by the authors.</citsent>
<aftsection>
<nextsent>149 trim contradictions legitimizes class_structure re societies is political_level re class_society where false_consciousness rue social_system containing such_contradictions trim human_societies organizing production several aspects are addressed: truthfulness (i.e. true vs. false in the first column), relevance for the query (i.e. relevant vs. not-relevant inthe second column), information content (i.e. informative vs. uninformative, third column) and meaningful ness (i.e. meaningful vs. error, fourth column).
</nextsent>
<nextsent>for most of the test queries, the majority of the retrieved predicates were true, relevant, informative and meaningful, confirming the quality of the acquired dm and the validity of the relation extraction technique2.
</nextsent>
<nextsent>from the bnc, god was able to extract good quality information for many different queries in very different domains, as for example music, unix, painting and many others.
</nextsent>
<nextsent>3.3 recall.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I43">
<title id=" E09-1028.xml">effects of word confusion networks on voice search </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>most approaches for ne tasks relyon machine learning approaches using annotated data.
</prevsent>
<prevsent>these algorithms include hidden markov model, support vector machines, maximum entropy, and conditional random fields.
</prevsent>
</prevsection>
<citsent citstr=" H05-1062 ">
with the goal of improving robustness to asr errors, (favre et al,2005) <papid> H05-1062 </papid>described finite-state machine based approach to take as input asr n-best strings and extract the nes.</citsent>
<aftsection>
<nextsent>although our task of query segmentation has similarity with ne tasks, it is arguable whether the search term is well-defined entity, since user can provide varied expressions as they would for general web search.
</nextsent>
<nextsent>also, it is not clear how the current best performing ne methods based on maximum entropy or conditional random fields models can be extended to apply on weighted lattices produced by asr.
</nextsent>
<nextsent>the other related literature is natural language interface to databases (nlidbs), which had beenwell-studied during 1960s-1980s (androutsopou los, 1995).
</nextsent>
<nextsent>in this research, the aim is to map natural language query into structured query that could be used to access database.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I44">
<title id=" E89-1014.xml">a logical treatment of semi free word order and bounded discontinuous constituency </title>
<section> implementation.  </section>
<citcontext>
<prevsection>
<prevsent>first, as implied in 3, parsing of sequence as category can be reduced to testing satisfiability of the formula phon: ^ cat.
</prevsent>
<prevsent>c. this means that we should be able to use general purpose proof environment (such as edinburgh lf) to implement the logic and test various proof theories for it.
</prevsent>
</prevsection>
<citsent citstr=" P85-1021 ">
second, there is an interpretation terms of head- driven parsing (proudian and pollard 1985).<papid> P85-1021 </papid></citsent>
<aftsection>
<nextsent>third, we might try to take advantage of the simple structure of the grammars (i.e., the dependency of phon on dtrs sequences) and implement parser augmented with sequence union.
</nextsent>
<nextsent>we hope to investigate these possibilities in the future.
</nextsent>
<nextsent>there are several comments to make here.
</nextsent>
<nextsent>first, the specific logic presented here is not important in itself.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I45">
<title id=" E06-2018.xml">exploring the sense distributions of homo graphs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for example, context words such as finger and arm are typical of the hand meaning of palm, whereas coconut and oil are typical of its tree meaning.
</prevsent>
<prevsent>the essence behind many algorithms for word sense disambiguation is to implicitly or explicitly classify all possible context words into groups relating toone or another sense.
</prevsent>
</prevsection>
<citsent citstr=" P94-1013 ">
this can be done in supervised (yarowsky, 1994), <papid> P94-1013 </papid>semi-supervised (yarowsky, 1995) <papid> P95-1026 </papid>or fully unsupervised way (pantel &amp; lin, 2002).</citsent>
<aftsection>
<nextsent>however, the classification can only work if the statistical clues are clear enough and if there are not too many exceptions.
</nextsent>
<nextsent>in terms of word co-occurrence statistics, we can say that within the local contexts of an ambiguous word, context words typical of the same sense should have highco-occurrence counts, whereas context words associated with different senses should have cooccurrence counts that are considerably lower.although the relative success of previous disambiguation systems (e.g. yarowsky, 1995) <papid> P95-1026 </papid>suggests that this should be the case, the effect has usually not been quantified as the emphasis was on task-based evaluation.</nextsent>
<nextsent>also, in most cases the amount of context to be used has not been systematically examined.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I46">
<title id=" E06-2018.xml">exploring the sense distributions of homo graphs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for example, context words such as finger and arm are typical of the hand meaning of palm, whereas coconut and oil are typical of its tree meaning.
</prevsent>
<prevsent>the essence behind many algorithms for word sense disambiguation is to implicitly or explicitly classify all possible context words into groups relating toone or another sense.
</prevsent>
</prevsection>
<citsent citstr=" P95-1026 ">
this can be done in supervised (yarowsky, 1994), <papid> P94-1013 </papid>semi-supervised (yarowsky, 1995) <papid> P95-1026 </papid>or fully unsupervised way (pantel &amp; lin, 2002).</citsent>
<aftsection>
<nextsent>however, the classification can only work if the statistical clues are clear enough and if there are not too many exceptions.
</nextsent>
<nextsent>in terms of word co-occurrence statistics, we can say that within the local contexts of an ambiguous word, context words typical of the same sense should have highco-occurrence counts, whereas context words associated with different senses should have cooccurrence counts that are considerably lower.although the relative success of previous disambiguation systems (e.g. yarowsky, 1995) <papid> P95-1026 </papid>suggests that this should be the case, the effect has usually not been quantified as the emphasis was on task-based evaluation.</nextsent>
<nextsent>also, in most cases the amount of context to be used has not been systematically examined.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I50">
<title id=" E87-1037.xml">a comparison of rule invocation strategies in context free chart parsing </title>
<section> background </section>
<citcontext>
<prevsection>
<prevsent>the aim of this paper is to provide survey and practical compari-son of fundamental rule-invocation strategies within context-free chart parsing.
</prevsent>
<prevsent>and introduction an apparent tendency in computational linguistics during the last few years has been towards declara-tive grammar formalisms.
</prevsent>
</prevsection>
<citsent citstr=" C86-1016 ">
this tendency has mani-fested itself with respect to linguistic tools, perhaps seen most clearly in the evolution from atns with their strongly procedural grammars to patr-ii in its various incarnations (shieber et al 1983, kart-tunen 1986), <papid> C86-1016 </papid>and to logic-based formalisms such as dcg (pereira and warren 1980).</citsent>
<aftsection>
<nextsent>it has also man-ifested itself in linguistic theor/es, where there has been development from systems employing sequen-tial derivations in the analysis of sentence struc-tures to systems like lfg and gpsg which estab-lish relations among the elements of sentence in an order-independent and also direction-independent way.
</nextsent>
<nextsent>for example, phenomena such as rule order-ing simply do not arise in these theories.
</nextsent>
<nextsent>this research has been supported by the national swedish board for technical development.
</nextsent>
<nextsent>in addition, declarative formalisms are, in princi-ple, processor-independent.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I51">
<title id=" E87-1037.xml">a comparison of rule invocation strategies in context free chart parsing </title>
<section> background </section>
<citcontext>
<prevsection>
<prevsent>the chart-parsing framework is of interest in this connec-tion because, being c higher-order algorithm  (kay 1982:329), it lends itself easily to the processing of different grammatical formalisms.
</prevsent>
<prevsent>at the same time it is of course natural test bed for experiments with various control strategies.
</prevsent>
</prevsection>
<citsent citstr=" P81-1036 ">
previously number of comparisons of rule- invocation strategies in this or in similar settings have been reported: zthis term seems to have been coined by thompson (1981).<papid> P81-1036 </papid></citsent>
<aftsection>
<nextsent>basically, it refers to the spectrum between top-down and bottom-up rocessing of the grammar rules.
</nextsent>
<nextsent>2the other principal control-strategy dimension, the search ~g;/(depth-first vs. breadth-first), is irrelevant for the effi-ciency in chart parsing since it only affects the order in which successive (partial) analyses are developed.
</nextsent>
<nextsent>226 kay (1982) is the principal source, providing very general exposition of the control strategies and data structures involved in chart parsing.
</nextsent>
<nextsent>in con-sidering the efficiency question, kay favours ~di- rected ~ bottom-up strategy (cf.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I53">
<title id=" E87-1037.xml">a comparison of rule invocation strategies in context free chart parsing </title>
<section> background </section>
<citcontext>
<prevsection>
<prevsent>top-down filtering (of.
</prevsent>
<prevsent>section 2.2.3).
</prevsent>
</prevsection>
<citsent citstr=" P81-1001 ">
as for empirical studies, slocum (1981) <papid> P81-1001 </papid>is rich source.</citsent>
<aftsection>
<nextsent>among many other things, he provides ome performance data regarding top-down filtering.
</nextsent>
<nextsent>pratt (1975) reports on successful augmentation of bottom-up chart-like parser with top-down filter.
</nextsent>
<nextsent>tomita (1985), tomita (1986) introduces very efficient, extended lr-parsing algorithm that can deal with full context-free languages.
</nextsent>
<nextsent>based on empirical com-parisons, tomita shows his algorithm to be superior to earley algorithm and also to modified ver-sion thereof (corresponding here to %elective top- downs; cf.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I60">
<title id=" E12-1073.xml">validation of sub sentential paraphrases acquired from parallel monolingual corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for instance, the phrases six months and half year form paraphrase pair applicable inmany different contexts, as they would appropriately denote the same concept.
</prevsent>
<prevsent>although one can envisage to manually build high-coverage lists of synonyms, enumerating meaning equivalences atthe level of phrases is too daunting task for humans.
</prevsent>
</prevsection>
<citsent citstr=" J10-3003 ">
because this type of knowledge can however greatly benefit many nlp applications, automatic acquisition of such paraphrases has attracted lot of attention (androutsopoulos and malakasiotis, 2010; madnani and dorr, 2010), <papid> J10-3003 </papid>and significant research efforts have been devoted to this objective (callison-burch, 2007; bhagat, 2009; madnani, 2010).</citsent>
<aftsection>
<nextsent>central to acquiring paraphrases is the need of assessing the quality of the candidate paraphrases produced by given technique.
</nextsent>
<nextsent>most works todate have resorted to human evaluation of paraphrases on the levels of grammaticality and meaning equivalence.
</nextsent>
<nextsent>human evaluation is however often criticized as being both costly and non reproducible, and the situation is even more complicated by the inherent complexity of the task thatcan produce low inter-judge agreement.
</nextsent>
<nextsent>task based evaluation involving the use of paraphrasing into some application thus seem an acceptable solution, provided the evaluation methodologies for the given task are deemed acceptable.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I61">
<title id=" E12-1073.xml">validation of sub sentential paraphrases acquired from parallel monolingual corpora </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the distributional hypothesis, attributed tozellig harris, was for example applied to syntactic dependency paths in the work of lin and pantel (2001).
</prevsent>
<prevsent>their results take the form of equivalence patterns with two arguments such as {x asks for y, requests y, xs request for y, wants y, is requested by x, . . .}.using comparable corpora, where the same information probably exists under various linguistic forms, increases the likelihood of finding very close contexts for sub-sentential units.
</prevsent>
</prevsection>
<citsent citstr=" N03-1003 ">
barzilay and lee (2003) <papid> N03-1003 </papid>proposed multi-sequence alignment algorithm that takes structurally similar sentences and builds compact lattice representation that encodes local variations.</citsent>
<aftsection>
<nextsent>the work by bhagat and ravichandran (2008) <papid> P08-1077 </papid>describes an application of similar technique on very large scale.</nextsent>
<nextsent>the hypothesis that two words or phrases are interchangeable if they share common translation into one or more other languages hasalso been extensively studied in works on sub sentential paraphrase acquisition.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I62">
<title id=" E12-1073.xml">validation of sub sentential paraphrases acquired from parallel monolingual corpora </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>their results take the form of equivalence patterns with two arguments such as {x asks for y, requests y, xs request for y, wants y, is requested by x, . . .}.using comparable corpora, where the same information probably exists under various linguistic forms, increases the likelihood of finding very close contexts for sub-sentential units.
</prevsent>
<prevsent>barzilay and lee (2003) <papid> N03-1003 </papid>proposed multi-sequence alignment algorithm that takes structurally similar sentences and builds compact lattice representation that encodes local variations.</prevsent>
</prevsection>
<citsent citstr=" P08-1077 ">
the work by bhagat and ravichandran (2008) <papid> P08-1077 </papid>describes an application of similar technique on very large scale.</citsent>
<aftsection>
<nextsent>the hypothesis that two words or phrases are interchangeable if they share common translation into one or more other languages hasalso been extensively studied in works on sub sentential paraphrase acquisition.
</nextsent>
<nextsent>bannard andcallison-burch (2005) described pivoting approach that can exploit bilingual parallel corpora in several languages.
</nextsent>
<nextsent>the same technique hasbeen applied to the acquisition of local paraphrasing patterns in zhao et al(2008).<papid> P08-1089 </papid></nextsent>
<nextsent>the work ofcallison-burch (2008) has shown how the monolingual context of sentence to paraphrase can beused to improve the quality of the acquired para phrases.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I63">
<title id=" E12-1073.xml">validation of sub sentential paraphrases acquired from parallel monolingual corpora </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the hypothesis that two words or phrases are interchangeable if they share common translation into one or more other languages hasalso been extensively studied in works on sub sentential paraphrase acquisition.
</prevsent>
<prevsent>bannard andcallison-burch (2005) described pivoting approach that can exploit bilingual parallel corpora in several languages.
</prevsent>
</prevsection>
<citsent citstr=" P08-1089 ">
the same technique hasbeen applied to the acquisition of local paraphrasing patterns in zhao et al(2008).<papid> P08-1089 </papid></citsent>
<aftsection>
<nextsent>the work ofcallison-burch (2008) has shown how the monolingual context of sentence to paraphrase can beused to improve the quality of the acquired paraphrases.
</nextsent>
<nextsent>another approach consists in modelling local paraphrasing identification rules.
</nextsent>
<nextsent>the work of jacquemin (1999) <papid> P99-1044 </papid>on the identification of term variants, which exploits rewriting morphosyntactic rules and descriptions of morphological and semantic lexical families, can be extended to extract the various forms corresponding to input patterns from large monolingual corpora.</nextsent>
<nextsent>when parallel monolingual corpora aligned at the sentence level are available (e.g. multiple translations into the same language), the task of sub-sentential paraphrase acquisition can be cast as one of word alignment between two aligned sentences (cohn et al 2008).<papid> J08-4005 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I64">
<title id=" E12-1073.xml">validation of sub sentential paraphrases acquired from parallel monolingual corpora </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the work ofcallison-burch (2008) has shown how the monolingual context of sentence to paraphrase can beused to improve the quality of the acquired paraphrases.
</prevsent>
<prevsent>another approach consists in modelling local paraphrasing identification rules.
</prevsent>
</prevsection>
<citsent citstr=" P99-1044 ">
the work of jacquemin (1999) <papid> P99-1044 </papid>on the identification of term variants, which exploits rewriting morphosyntactic rules and descriptions of morphological and semantic lexical families, can be extended to extract the various forms corresponding to input patterns from large monolingual corpora.</citsent>
<aftsection>
<nextsent>when parallel monolingual corpora aligned at the sentence level are available (e.g. multiple translations into the same language), the task of sub-sentential paraphrase acquisition can be cast as one of word alignment between two aligned sentences (cohn et al 2008).<papid> J08-4005 </papid></nextsent>
<nextsent>barzilay and mckeown (2001) <papid> P01-1008 </papid>applied the distributionality hypothesis on such parallel sentences, and pang etal.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I65">
<title id=" E12-1073.xml">validation of sub sentential paraphrases acquired from parallel monolingual corpora </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>another approach consists in modelling local paraphrasing identification rules.
</prevsent>
<prevsent>the work of jacquemin (1999) <papid> P99-1044 </papid>on the identification of term variants, which exploits rewriting morphosyntactic rules and descriptions of morphological and semantic lexical families, can be extended to extract the various forms corresponding to input patterns from large monolingual corpora.</prevsent>
</prevsection>
<citsent citstr=" J08-4005 ">
when parallel monolingual corpora aligned at the sentence level are available (e.g. multiple translations into the same language), the task of sub-sentential paraphrase acquisition can be cast as one of word alignment between two aligned sentences (cohn et al 2008).<papid> J08-4005 </papid></citsent>
<aftsection>
<nextsent>barzilay and mckeown (2001) <papid> P01-1008 </papid>applied the distributionality hypothesis on such parallel sentences, and pang etal.</nextsent>
<nextsent>(2003) proposed an algorithm to align sentences by recursive fusion of their common syntactic constituants.finally, they has been recent interest in automatic evaluation of paraphrases (callison-burch et al 2008; liu et al 2010; <papid> D10-1090 </papid>chen and dolan, 2011; <papid> P11-1020 </papid>metzler et al 2011).<papid> P11-2096 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I66">
<title id=" E12-1073.xml">validation of sub sentential paraphrases acquired from parallel monolingual corpora </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the work of jacquemin (1999) <papid> P99-1044 </papid>on the identification of term variants, which exploits rewriting morphosyntactic rules and descriptions of morphological and semantic lexical families, can be extended to extract the various forms corresponding to input patterns from large monolingual corpora.</prevsent>
<prevsent>when parallel monolingual corpora aligned at the sentence level are available (e.g. multiple translations into the same language), the task of sub-sentential paraphrase acquisition can be cast as one of word alignment between two aligned sentences (cohn et al 2008).<papid> J08-4005 </papid></prevsent>
</prevsection>
<citsent citstr=" P01-1008 ">
barzilay and mckeown (2001) <papid> P01-1008 </papid>applied the distributionality hypothesis on such parallel sentences, and pang etal.</citsent>
<aftsection>
<nextsent>(2003) proposed an algorithm to align sentences by recursive fusion of their common syntactic constituants.finally, they has been recent interest in automatic evaluation of paraphrases (callison-burch et al 2008; liu et al 2010; <papid> D10-1090 </papid>chen and dolan, 2011; <papid> P11-1020 </papid>metzler et al 2011).<papid> P11-2096 </papid></nextsent>
<nextsent>we used the main aspects of the methodology described by cohn et al(2008) <papid> J08-4005 </papid>for constructing evaluation corpora and assessing the performance of techniques on the task of sub-sentential paraphrase acquisition.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I67">
<title id=" E12-1073.xml">validation of sub sentential paraphrases acquired from parallel monolingual corpora </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>when parallel monolingual corpora aligned at the sentence level are available (e.g. multiple translations into the same language), the task of sub-sentential paraphrase acquisition can be cast as one of word alignment between two aligned sentences (cohn et al 2008).<papid> J08-4005 </papid></prevsent>
<prevsent>barzilay and mckeown (2001) <papid> P01-1008 </papid>applied the distributionality hypothesis on such parallel sentences, and pang etal.</prevsent>
</prevsection>
<citsent citstr=" D10-1090 ">
(2003) proposed an algorithm to align sentences by recursive fusion of their common syntactic constituants.finally, they has been recent interest in automatic evaluation of paraphrases (callison-burch et al 2008; liu et al 2010; <papid> D10-1090 </papid>chen and dolan, 2011; <papid> P11-1020 </papid>metzler et al 2011).<papid> P11-2096 </papid></citsent>
<aftsection>
<nextsent>we used the main aspects of the methodology described by cohn et al(2008) <papid> J08-4005 </papid>for constructing evaluation corpora and assessing the performance of techniques on the task of sub-sentential paraphrase acquisition.</nextsent>
<nextsent>pairs of related sentences are hand-aligned to define set of reference atomic paraphrase pairs at the level of words or phrases, denoted asratom1.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I68">
<title id=" E12-1073.xml">validation of sub sentential paraphrases acquired from parallel monolingual corpora </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>when parallel monolingual corpora aligned at the sentence level are available (e.g. multiple translations into the same language), the task of sub-sentential paraphrase acquisition can be cast as one of word alignment between two aligned sentences (cohn et al 2008).<papid> J08-4005 </papid></prevsent>
<prevsent>barzilay and mckeown (2001) <papid> P01-1008 </papid>applied the distributionality hypothesis on such parallel sentences, and pang etal.</prevsent>
</prevsection>
<citsent citstr=" P11-1020 ">
(2003) proposed an algorithm to align sentences by recursive fusion of their common syntactic constituants.finally, they has been recent interest in automatic evaluation of paraphrases (callison-burch et al 2008; liu et al 2010; <papid> D10-1090 </papid>chen and dolan, 2011; <papid> P11-1020 </papid>metzler et al 2011).<papid> P11-2096 </papid></citsent>
<aftsection>
<nextsent>we used the main aspects of the methodology described by cohn et al(2008) <papid> J08-4005 </papid>for constructing evaluation corpora and assessing the performance of techniques on the task of sub-sentential paraphrase acquisition.</nextsent>
<nextsent>pairs of related sentences are hand-aligned to define set of reference atomic paraphrase pairs at the level of words or phrases, denoted asratom1.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I69">
<title id=" E12-1073.xml">validation of sub sentential paraphrases acquired from parallel monolingual corpora </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>when parallel monolingual corpora aligned at the sentence level are available (e.g. multiple translations into the same language), the task of sub-sentential paraphrase acquisition can be cast as one of word alignment between two aligned sentences (cohn et al 2008).<papid> J08-4005 </papid></prevsent>
<prevsent>barzilay and mckeown (2001) <papid> P01-1008 </papid>applied the distributionality hypothesis on such parallel sentences, and pang etal.</prevsent>
</prevsection>
<citsent citstr=" P11-2096 ">
(2003) proposed an algorithm to align sentences by recursive fusion of their common syntactic constituants.finally, they has been recent interest in automatic evaluation of paraphrases (callison-burch et al 2008; liu et al 2010; <papid> D10-1090 </papid>chen and dolan, 2011; <papid> P11-1020 </papid>metzler et al 2011).<papid> P11-2096 </papid></citsent>
<aftsection>
<nextsent>we used the main aspects of the methodology described by cohn et al(2008) <papid> J08-4005 </papid>for constructing evaluation corpora and assessing the performance of techniques on the task of sub-sentential paraphrase acquisition.</nextsent>
<nextsent>pairs of related sentences are hand-aligned to define set of reference atomic paraphrase pairs at the level of words or phrases, denoted asratom1.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I73">
<title id=" E12-1073.xml">validation of sub sentential paraphrases acquired from parallel monolingual corpora </title>
<section> experimental setting.  </section>
<citcontext>
<prevsection>
<prevsent>multiply-translated subtitles aligned multiple translations of contributed movie subtitles (tiedemann, 2007).5.
</prevsent>
<prevsent>comparable news headlines news headlines collected from google news clusters (e.g.
</prevsent>
</prevsection>
<citsent citstr=" C04-1051 ">
(dolan et al 2004)).<papid> C04-1051 </papid></citsent>
<aftsection>
<nextsent>we collected 100 sentence pairs of each typein french, for which various comparability measures are reported on table 1.
</nextsent>
<nextsent>in particular, the?% aligned tokens?
</nextsent>
<nextsent>row indicates the proportion of tokens from the sentence pairs that couldbe manually aligned by native-speaker annota tor.2 obviously, the more common tokens two sentences from pair contain, the fewer sub sentential paraphrases may be extracted from that pair.
</nextsent>
<nextsent>however, high lexical overlap increases the probability that two sentences be indeed paraphrases, and in turn the probability that some of their phrases be paraphrases.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I74">
<title id=" E12-1073.xml">validation of sub sentential paraphrases acquired from parallel monolingual corpora </title>
<section> experimental setting.  </section>
<citcontext>
<prevsection>
<prevsent>row indicates the proportion of tokens from the sentence pairs that couldbe manually aligned by native-speaker annota tor.2 obviously, the more common tokens two sentences from pair contain, the fewer sub sentential paraphrases may be extracted from that pair.
</prevsent>
<prevsent>however, high lexical overlap increases the probability that two sentences be indeed paraphrases, and in turn the probability that some of their phrases be paraphrases.
</prevsent>
</prevsection>
<citsent citstr=" P08-4006 ">
furthermore, theta ted corpora using them we considered all alignments as being correct.2the same annotator hand-aligned the 5*100=500 paraphrase pairs using the yawat (germann, 2008) <papid> P08-4006 </papid>manual alignment tool.</citsent>
<aftsection>
<nextsent>presence of common token may serve as useful clues to guide paraphrase extraction.
</nextsent>
<nextsent>for our experiments, we chose to use parallel monolingual corpora obtained by single language translation, the most direct resource type for acquiring sub-sentential paraphrase pairs.
</nextsent>
<nextsent>this allows us to define acceptable references for the task and resort to the most consensual evaluation technique for paraphrase acquisition to date.
</nextsent>
<nextsent>using such corpora, we expect to be able to extract precise paraphrases (see table 1), which will be natural candidates for further validation, which will be addressed in section 5.3.figure 1 illustrates reference alignment obtained on pair of english sentential paraphrases and the list of atomic paraphrase pairs that can be extracted from it, against which acquisition techniques will be evaluated.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I81">
<title id=" E12-1073.xml">validation of sub sentential paraphrases acquired from parallel monolingual corpora </title>
<section> individual techniques for paraphrase.  </section>
<citcontext>
<prevsection>
<prevsent>we have selected and implemented five techniques which we believe are representative of the type of knowledge that these techniques use, and have reused existing tools, initially developed for other tasks, when possible.
</prevsent>
<prevsent>4.1 statistical learning of word alignments.
</prevsent>
</prevsection>
<citsent citstr=" J04-4002 ">
(giza) the giza++ tool (och and ney, 2004) <papid> J04-4002 </papid>computes statistical word alignment models of increasing complexity from parallel corpora.</citsent>
<aftsection>
<nextsent>while originally developed in the bilingual context of statistical machine translation, nothing prevents building such models on monolingual corpora.
</nextsent>
<nextsent>however, in order to build reliable models, it is necessary to use enough training material including minimal redundancy of words.
</nextsent>
<nextsent>to this end, we provided giza++ with all possible sentence pairs from our mutiply-translated corpus to im prove the quality of its word alignments (note that 4http://www.elda.org/article125.html 719we used symmetrized alignments from the alignments in both directions).
</nextsent>
<nextsent>this constitutes significant advantage for this technique that techniques working on each sentence pair independently do not have.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I82">
<title id=" E12-1073.xml">validation of sub sentential paraphrases acquired from parallel monolingual corpora </title>
<section> individual techniques for paraphrase.  </section>
<citcontext>
<prevsection>
<prevsent>bannard and callison-burch (2005) defined paraphrasing probability between two phrases based on their translation probability through all possible pivot phrases as: ppara(p1, p2) = ? piv pt(piv|p1)pt(p2|piv) where pt denotes translation probabilies.
</prevsent>
<prevsent>we used the europarl corpus5 of parliamentary debates in english and french, consisting of approximately 1.7 million parallel sentences : this allowed us to use the same resource to build paraphrases for english, using french as the pivot language, and for french, using english as the pivot language.
</prevsent>
</prevsection>
<citsent citstr=" P07-2045 ">
the giza++ tool was used for word alignment and the moses statistical machine translation toolkit (koehn et al 2007) <papid> P07-2045 </papid>was used to compute phrase translation probabilities from these word alignments.</citsent>
<aftsection>
<nextsent>for each sentential paraphrase pair, we applied the following algorithm: for each phrase, we build the entire set of paraphrases using the previous definition.
</nextsent>
<nextsent>we then extract its best paraphrase as the one exactly appearing in the other sentence with maximum paraphrase probability, using minimal threshold value of 104.
</nextsent>
<nextsent>4.3 linguistic knowledge on term variation.
</nextsent>
<nextsent>(fastr) the fastr tool (jacquemin, 1999) <papid> P99-1044 </papid>was designed to spot term/phrase variants in large corpora.variants are described through meta rules expressing how the morphosyntactic structure of term variant can be derived from given term by means of regular expressions on word morphosyntacticcategories.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I84">
<title id=" E12-1073.xml">validation of sub sentential paraphrases acquired from parallel monolingual corpora </title>
<section> individual techniques for paraphrase.  </section>
<citcontext>
<prevsection>
<prevsent>to compute candidate paraphrase pairs using fastr, we first consider all phrases from 5http://statmt.org/europarl the first sentence and search for variants in the other sentence, then do the reverse process and finally take the intersection of the two sets.
</prevsent>
<prevsent>4.4 syntactic similarity (synt).
</prevsent>
</prevsection>
<citsent citstr=" N03-1024 ">
the algorithm introduced by pang et al(2003) <papid> N03-1024 </papid>takes two sentences as input and merges them by top-down syntactic fusion guided by compatible syntactic substructure.</citsent>
<aftsection>
<nextsent>a lexical blocking mechanism prevents constituents from fusion ning when there is evidence of the presence of word in an other constituent of one of the sentence.
</nextsent>
<nextsent>we usethe berkeley probabilistic parser (klein and manning, 2003) <papid> P03-1054 </papid>to obtain syntactic trees for english and its adapted version for french (candito et al 2010).</nextsent>
<nextsent>because this process is highly sensitive to syntactic parse errors, we use in our implementation k-best parses and retain the most compact fusion from any pair of candidate parses.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I85">
<title id=" E12-1073.xml">validation of sub sentential paraphrases acquired from parallel monolingual corpora </title>
<section> individual techniques for paraphrase.  </section>
<citcontext>
<prevsection>
<prevsent>the algorithm introduced by pang et al(2003) <papid> N03-1024 </papid>takes two sentences as input and merges them by top-down syntactic fusion guided by compatible syntactic substructure.</prevsent>
<prevsent>a lexical blocking mechanism prevents constituents from fusion ning when there is evidence of the presence of word in an other constituent of one of the sentence.</prevsent>
</prevsection>
<citsent citstr=" P03-1054 ">
we usethe berkeley probabilistic parser (klein and manning, 2003) <papid> P03-1054 </papid>to obtain syntactic trees for english and its adapted version for french (candito et al 2010).</citsent>
<aftsection>
<nextsent>because this process is highly sensitive to syntactic parse errors, we use in our implementation k-best parses and retain the most compact fusion from any pair of candidate parses.
</nextsent>
<nextsent>4.5 edit rate on word sequences (terp).
</nextsent>
<nextsent>terp (translation edit rate plus) (snover et al 2010) is score designed for the evaluation of machine translation output.
</nextsent>
<nextsent>its typical use takes system hypothesis to compute an optimal set ofword edits that can transform it into some existing reference translation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I86">
<title id=" E09-1001.xml">binvited talkb slacker semantics why superficiality dependency and avoidance of commitment can be the right way to go </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the programme of developing computational compositional semantics has large number of aspects.
</prevsent>
<prevsent>it is important that the semantics has logically-sound interpretation (e.g., koller and lascarides (2009), thater (2007)), is cross 1for instance, we cannot afford to under specify number on nouns because of examples such as the hash browns is getting angry (from pollard and sag (1994) p.85).
</prevsent>
</prevsection>
<citsent citstr=" P08-1111 ">
1 linguistically adequate (e.g., bender (2008)) <papid> P08-1111 </papid>and is compatible with generation (e.g., carroll et al (1999), carroll and oepen (2005)).</citsent>
<aftsection>
<nextsent>ideally, wewant support for shallow as well as deep syntactic analysis (which was the reason for developing rmrs), enrichment by deeper analysis (in cluding lexical semantics and anaphora resolution,both the subject of ongoing work), and (robust) inference.
</nextsent>
<nextsent>the motivation for the development ofdependency-style representations (including dependency mrs (dmrs) discussed in 4) has beento improve ease of use for consumers of the representation and human annotators, as well as use in statistical ranking of analyses/realisations (fujitaet al (2007), oepen and lnning (2006)).
</nextsent>
<nextsent>integration with distributional semantic techniques is also of interest.
</nextsent>
<nextsent>the belated introduction?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I87">
<title id=" E09-1001.xml">binvited talkb slacker semantics why superficiality dependency and avoidance of commitment can be the right way to go </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the first, argument indexing(3), is relatively clear case in which the constraints imposed by grammar engineering have asignificant effect on choice between plausible alternatives.
</prevsent>
<prevsent>i have chosen to talk about this both because of its relationship with the currently popular task of semantic role labelling and because the delph-in approach is now fairly stable after quite considerable degree of experimentation.
</prevsent>
</prevsection>
<citsent citstr=" W02-1502 ">
what am reporting is thus perspective on work done primarily by flickinger within the english resource grammar (erg: flickinger (2000)) and by bender in the context of the grammar matrix (bender et al, 2002), <papid> W02-1502 </papid>though ive been involved in many of the discussions.</citsent>
<aftsection>
<nextsent>the second main topic(4) is new work on semantic dependency representation which can be derived from mrs, extending the previous work by oepen (oepen and lnning, 2006).
</nextsent>
<nextsent>here, the motivation came from an engineering perspective, but the nature of the representation, and indeed the fact that it is possible at all, reveals some interesting aspects of semantic composition in the grammars.
</nextsent>
<nextsent>this paper concerns only representations which are output by deep grammars, which use mrs, but it will be convenient to talk in terms of rmrs and to describe the rmrss that are constructed under those assumptions.
</nextsent>
<nextsent>such rmrss are inter convert ible with mrss.2 the description is necessarily terse and contains the minimal detail necessary to follow the remainder of the paper.an rmrs is description of set of trees corresponding to scoped logical forms.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I88">
<title id=" E09-1001.xml">binvited talkb slacker semantics why superficiality dependency and avoidance of commitment can be the right way to go </title>
<section> arguments and roles.  </section>
<citcontext>
<prevsection>
<prevsent>however this does nothing for semantic generalisation, blocks the use of argument labels in syntactic generalisations and leads to an extreme proliferation of lexical types when using typed feature structure formalisms (one type would be required per lexeme).
</prevsent>
<prevsent>the labels add no additional information and could trivially beadded automatically to an rmrs if this were useful for human readers.
</prevsent>
</prevsection>
<citsent citstr=" P98-1013 ">
much more interesting is the use of richer lexical semantic generalisations, such as those employed in framenet (baker et al, 1998).<papid> P98-1013 </papid></citsent>
<aftsection>
<nextsent>in principle, at least, we could (and should) systematically link the erg to framenet, but this would be form of semantic enrichment mediated via the sem-i (cf roa et al (2008)), and not an alternative technique for argument indexation.
</nextsent>
<nextsent>the second main topic want to address is form of semantic dependency structure (dmrs: see wiki.delph-in.net for the evolving details).
</nextsent>
<nextsent>there are good engineering reasons for produc inga dependency style representation with links between predicates and no variables: ease of read ability for consumers of the representation and for human annotators, parser comparison and integration with distributional lexical semantics being the immediate goals.
</nextsent>
<nextsent>oepen has previously produced elementary dependencies from mrss but the procedure (partially sketched in oepen and lnning(2006)) was not intended to produce complete representations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I89">
<title id=" E09-1012.xml">incremental parsing models for dialog task structure </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we contrast the amount of context provided by each method and its impact on performance.
</prevsent>
<prevsent>corpora of spoken dialog are now widely available, and frequently come with annotations fortasks/games, dialog acts, named entities and elements of syntactic structure.
</prevsent>
</prevsection>
<citsent citstr=" J86-3001 ">
these types of information provide rich clues for building dialog models (grosz and sidner, 1986).<papid> J86-3001 </papid></citsent>
<aftsection>
<nextsent>dialog models canbe built ofine (for dialog mining and summa riza tion), or online (for dialog management).a dialog manager is the component of dialog system that is responsible for interpreting user actions in the dialog context, and for generating system actions.
</nextsent>
<nextsent>needless to say, dialog manager operates incrementally as the dialog progresses.
</nextsent>
<nextsent>in typical commercial dialog systems, the interpretation and generation processes operate independently of each other, with only small amount of shared context.
</nextsent>
<nextsent>by contrast, in this paper we describe dialog model that (1) tightly integrates interpretation and generation, (2) makes explicit the type and amount of shared context, (3) includes the task structure of the dialog in the context, (4)can be trained from dialog data, and (5) runs incrementally, parsing the dialog as it occurs and in ter leaving generation and interpretation.at the core of our model is parser that incrementally builds the dialog task structure as the dialog progresses.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I91">
<title id=" E09-1012.xml">incremental parsing models for dialog task structure </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>there has now been considerable work on discourse parsing using statistical bottom-up parsing (soricut and marcu, 2003), hierarchical agglomerative clustering (sporleder and lascarides, 2004), parsing from lexicalizedtree-adjoining grammars (cristea, 2000), and rule based approaches that use rhetorical relations and discourse cues (forbes et al, 2003; polanyi et al, 2004; lethanh et al, 2004).
</prevsent>
<prevsent>with the exception of cristea (2000), most of this research has been limited to non-incremental parsing of textual monologues where, in contrast to incremental dialog parsing, predicting system action is not relevant.
</prevsent>
</prevsection>
<citsent citstr=" W05-0613 ">
the work on discourse parsing that is most similar to ours is that of baldridge and lascarides (2005).<papid> W05-0613 </papid></citsent>
<aftsection>
<nextsent>they used probabilistic head driven parsing method (described in (collins, 2003)) <papid> J03-4003 </papid>to construct rhetorical structure trees for spoken dialog corpus.</nextsent>
<nextsent>however, their parser was 94 dialog task topic/subtasktopic/subtask task task clause utteranceutteranceutterance topic/subtask dialogact,predargs dialogact,predargs dialogact,predargs figure 1: schema of shared plan tree for dialog.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I92">
<title id=" E09-1012.xml">incremental parsing models for dialog task structure </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>with the exception of cristea (2000), most of this research has been limited to non-incremental parsing of textual monologues where, in contrast to incremental dialog parsing, predicting system action is not relevant.
</prevsent>
<prevsent>the work on discourse parsing that is most similar to ours is that of baldridge and lascarides (2005).<papid> W05-0613 </papid></prevsent>
</prevsection>
<citsent citstr=" J03-4003 ">
they used probabilistic head driven parsing method (described in (collins, 2003)) <papid> J03-4003 </papid>to construct rhetorical structure trees for spoken dialog corpus.</citsent>
<aftsection>
<nextsent>however, their parser was 94 dialog task topic/subtasktopic/subtask task task clause utteranceutteranceutterance topic/subtask dialogact,predargs dialogact,predargs dialogact,predargs figure 1: schema of shared plan tree for dialog.
</nextsent>
<nextsent>not incremental; it used global features such as the number of turn changes.
</nextsent>
<nextsent>also, it focused strictly in interpretation of input utterances; it could not predict actions by either dialog partner.
</nextsent>
<nextsent>in contrast to other work on discourse parsing,we wish to use the parsing process directly for dialog management (rather than for information extraction or summarization).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I93">
<title id=" E09-1012.xml">incremental parsing models for dialog task structure </title>
<section> dialog parsing.  </section>
<citcontext>
<prevsection>
<prevsent>tree structure.
</prevsent>
<prevsent>a shallow parse is one in which utterances are grouped together into subtasks, but the dominance relations among subtasks are nottracked.
</prevsent>
</prevsection>
<citsent citstr=" P06-1026 ">
we call this model chunk-based dialog model (bangalore et al, 2006).<papid> P06-1026 </papid></citsent>
<aftsection>
<nextsent>the chunk based model has limitations.
</nextsent>
<nextsent>for example, dominance relations among subtasks are important for dialog processes such as anaphora resolution (grosz and sidner, 1986).<papid> J86-3001 </papid></nextsent>
<nextsent>also, the chunk based model is representation ally inadequate for center-embedded nestings of subtasks, which do occur in our domain, although less frequently than the more prevalent tail-recursive?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I96">
<title id=" E09-1012.xml">incremental parsing models for dialog task structure </title>
<section> dialog parsing.  </section>
<citcontext>
<prevsection>
<prevsent>order item task opening shipping address request(makeorder) ack thank you for calling xyz catalog this is mary hello can have your home telephone number with area code ......
</prevsent>
<prevsent>order item task opening how may you thank yes would like help you yes one second please ack contact info to place an order figure 4: an illustration of incremental evolution of dialog structure feature vector containing contextual information for the parsing action (see section 5.1).
</prevsent>
</prevsection>
<citsent citstr=" J96-1002 ">
these feature vectors and the associated parser actions are used to train maximum entropy models (berger etal., 1996).<papid> J96-1002 </papid></citsent>
<aftsection>
<nextsent>these models are then used to incrementally incorporate the utterances for new dialog into that dialogs subtask tree as the dialog progresses, as shown in figure 3.
</nextsent>
<nextsent>4.1 shift-reduce method.
</nextsent>
<nextsent>in this method, the subtask tree is recovered through right-branching shift-reduce parsing process (hall et al, 2006; sagae and lavie, 2006).
</nextsent>
<nextsent>the parser shifts each utterance on to the stack.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I98">
<title id=" E09-1094.xml">a robust and extensible exemplar based model of thematic fit </title>
<section> exemplar-based modeling of thematic.  </section>
<citcontext>
<prevsection>
<prevsent>the plausibility of semantic role given new verb-argument pair is then determined by the support for that role among the verb-argument pairs in memory that are semantically most similar to the target pair.an immediately obvious advantage of this approach should be its potential robustness for datasparsity, since similarity-based smoothing is an intrinsic part of the model.
</prevsent>
<prevsent>even if neither the verb nor the argument of verb-argument pair occur in the exemplar memory, role plausibilities can be predicted, as long as the similarity of the target exemplars semantic representation with the semantic representations in the exemplar memory can becalculated.
</prevsent>
</prevsection>
<citsent citstr=" P97-1056 ">
an additional advantage of similarity based smoothing is that it does not involve the estimation of an exponential number of smoothing parameters, as is the case for backed-off smoothing methods (zavrel and daelemans, 1997).<papid> P97-1056 </papid></citsent>
<aftsection>
<nextsent>for this study, we will implement three different kinds of exemplar-based models.
</nextsent>
<nextsent>the first model is basic k-nearest neighbor (k-nn) model.
</nextsent>
<nextsent>in this model, the plausibility rating for semantic role given verb-argument pair is simply determined by the (relative) frequency with which that semantic role is assigned to the verb-argumentpairs that are nearest (i.e. most similar) to the target verb-argument pair (these exemplars constitute the nearest neighbor set).
</nextsent>
<nextsent>the second model adds decay function to this simple k-nn model, sothat not only the role frequency, but also the absolute semantic distance between the target itemand the neighbors in the nearest neighbor set determine the plausibility rating.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I99">
<title id=" E09-1094.xml">a robust and extensible exemplar based model of thematic fit </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>4.2 training the model.
</prevsent>
<prevsent>in exemplar-based models, training the model simply amounts to storing exemplars in memory.
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
our model uses an exemplar memory that consists of 133566 verb-role-noun triples extracted from the wall street journal and brown parts of the penn treebank (marcus et al, 1993).<papid> J93-2004 </papid></citsent>
<aftsection>
<nextsent>these were first annotated with semantic roles using state of-the-art semantic role labeling system (koomen et al, 2005).<papid> W05-0625 </papid></nextsent>
<nextsent>semantic roles are approximated by propbank argument roles (palmer et al, 2005).<papid> J05-1004 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I100">
<title id=" E09-1094.xml">a robust and extensible exemplar based model of thematic fit </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>in exemplar-based models, training the model simply amounts to storing exemplars in memory.
</prevsent>
<prevsent>our model uses an exemplar memory that consists of 133566 verb-role-noun triples extracted from the wall street journal and brown parts of the penn treebank (marcus et al, 1993).<papid> J93-2004 </papid></prevsent>
</prevsection>
<citsent citstr=" W05-0625 ">
these were first annotated with semantic roles using state of-the-art semantic role labeling system (koomen et al, 2005).<papid> W05-0625 </papid></citsent>
<aftsection>
<nextsent>semantic roles are approximated by propbank argument roles (palmer et al, 2005).<papid> J05-1004 </papid></nextsent>
<nextsent>these consist of limited set of numbered roles that are usedfor all verbs but are defined on verb-by-verb ba sis.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I101">
<title id=" E09-1094.xml">a robust and extensible exemplar based model of thematic fit </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>our model uses an exemplar memory that consists of 133566 verb-role-noun triples extracted from the wall street journal and brown parts of the penn treebank (marcus et al, 1993).<papid> J93-2004 </papid></prevsent>
<prevsent>these were first annotated with semantic roles using state of-the-art semantic role labeling system (koomen et al, 2005).<papid> W05-0625 </papid></prevsent>
</prevsection>
<citsent citstr=" J05-1004 ">
semantic roles are approximated by propbank argument roles (palmer et al, 2005).<papid> J05-1004 </papid></citsent>
<aftsection>
<nextsent>these consist of limited set of numbered roles that are usedfor all verbs but are defined on verb-by-verb basis.
</nextsent>
<nextsent>this contrasts with framenet roles, which are sense-specific.
</nextsent>
<nextsent>hence propbank roles provide shallower level of semantic role annotation.
</nextsent>
<nextsent>they also do not refer consistently to the same semantic roles over different verbs, although the a0 and a1 roles in the majority of cases do correspond to the agent and patient roles, respectively.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I102">
<title id=" E09-1094.xml">a robust and extensible exemplar based model of thematic fit </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>to create the exemplar memory, all lemmatized verb-noun role triples that contained the a0, a1, or a2 roles were extracted.
</prevsent>
<prevsent>4.3 testing the model.
</prevsent>
</prevsection>
<citsent citstr=" P98-2127 ">
to obtain the semantic distances between noun sand verbs for the calculation of the distance between exemplars (see equation 3), we make use of thesaurus compiled by lin (1998), <papid> P98-2127 </papid>which lists the 200 nearest neighbors for large number of english noun and verb lemmas, together with their similarity values.</citsent>
<aftsection>
<nextsent>this resource was created by computing the similarity between word dependency vectors that are composed of frequency counts of (head, relation, dependent) triples (dependency triples) in 64-million word parsed corpus.
</nextsent>
<nextsent>to compute these similarities, an information-theoretic similarity metric was used.
</nextsent>
<nextsent>the basic idea of this metric is that the similarity between two words is the amount of information contained in the commonality between the two words, i.e. the frequency counts of the dependency triples that occur in the descriptions of both words,divided by the amount of information in the descriptions of the words, i.e. the frequency counts of the dependency triples that occur in either of the two words.
</nextsent>
<nextsent>see lin (1998) <papid> P98-2127 </papid>for details.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I104">
<title id=" E09-2012.xml">a tool for multiword expression extraction in modern greek using syntactic parsing </title>
<section> the greek parser.  </section>
<citcontext>
<prevsection>
<prevsent>section 3 provides description of greek mwes,including syntactic classification for these.
</prevsent>
<prevsent>section 4 presents the extraction tool, and section 5 concludes the paper.
</prevsent>
</prevsection>
<citsent citstr=" W07-1216 ">
the greek parser is part of fips, multilingual symbolic parser that deals, among other languages, with english, french, spanish, italian, and german (wehrli, 2007).<papid> W07-1216 </papid></citsent>
<aftsection>
<nextsent>the greek version, fips greek (michou, 2007), has recently reached an acceptable level of lexical and grammatical coverage.
</nextsent>
<nextsent>fips relies on generative grammar concepts, and is basically made up of generic parsing module which can be refined in order to suit the specific needs of particular language.
</nextsent>
<nextsent>currently, there are approximately 60 grammar rules defined for greek, allowing for the complete parse of about 50% of the sentences in corpus like europarl (koehn, 2005), which contains proceedings ofthe european parliament.
</nextsent>
<nextsent>for the remaining sentences, partial analyses are instead proposed for the chunks identified.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I105">
<title id=" E09-2012.xml">a tool for multiword expression extraction in modern greek using syntactic parsing </title>
<section> the mwe extraction tool.  </section>
<citcontext>
<prevsection>
<prevsent>a strong association between the items ofa candidate indicates that this is likely to constitute collocation.
</prevsent>
<prevsent>the strength of association canbe measured with one of the numerous association measures implemented in our extractor.
</prevsent>
</prevsection>
<citsent citstr=" J93-1003 ">
by default, the log-likelihood ratio measure (llr) is proposed, since it was shown to be particularly suited to language data (dunning, 1993).<papid> J93-1003 </papid>in our extractor, the items of each candidate expression represent base word forms (lemmas) and they are considered in the canonical order implied by the given syntactic configuration (e.g., for verb-object candidate, the object is post verbal in svo languages like greek).</citsent>
<aftsection>
<nextsent>even if the candidate occurs in corpus in different morphosyntactic realizations, its various occurrences are successfully identified as instances of the same type thanks to the syntactic analysis performed with the parser.
</nextsent>
<nextsent>4.2 visualization.
</nextsent>
<nextsent>the extraction tool also provides visualization functions which facilitate the consultation and interpretation of results by userse.g., lexi cographers, terminologists, translators, languagelearnersby displaying them in the original context.
</nextsent>
<nextsent>the following functions are provided: filtering and sorting the results which willbe displayed can be selected according to seve 47 ral criteria: the syntactic configuration (i.e., users can select only one or several configurations theyare interested in), the llr score, the corpus frequency (users can specify the limits of the desired interval),3 the words involved (users can look up mwes containing specific words).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I106">
<title id=" E09-2012.xml">a tool for multiword expression extraction in modern greek using syntactic parsing </title>
<section> conclusion.  </section>
<citcontext>
<prevsection>
<prevsent>we presented mwe extractor with advanced concordancing functions, which can be usedto semi-automatically build greek monolin gual/bilingual mwe lexicons.
</prevsent>
<prevsent>it relies on adeep syntactic approach, whose benefits are manifold: retrieval of grammatical results, interpretation of syntactic constituents in terms of arguments, disambiguation of lexemes with multiple readings, and grouping of all morphosyntactic variants of mwes.
</prevsent>
</prevsection>
<citsent citstr=" A94-1006 ">
our system is most similar to ter might (dagan and church, 1994) <papid> A94-1006 </papid>and trans search (macklovitch et al, 2000).</citsent>
<aftsection>
<nextsent>to our knowledge, it is the first of this type for greek.
</nextsent>
<nextsent>acknowledgements this work has been supported by the swiss national science foundation (grant 100012-117944).
</nextsent>
<nextsent>the authors would like to thank eric wehrli for his support and useful comments.
</nextsent>

</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I107">
<title id=" E12-1018.xml">tree representations in probabilistic models for extended named entities detection </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>named entity recognition is traditinal task of the natural language processing domain.
</prevsent>
<prevsent>thetask aims at mapping words in text into semantic classes, such like persons, organizations or lo calizations.
</prevsent>
</prevsection>
<citsent citstr=" C96-1079 ">
while at first the ner task was quite simple, involving limited number of classes (gr ishman and sundheim, 1996), <papid> C96-1079 </papid>along the years the task complexity increased as more complex class taxonomies were defined (sekine and no bata, 2004).</citsent>
<aftsection>
<nextsent>the interest in the task is related toits use in complex frameworks for (semantic) content extraction, such like relation extraction applications (doddington et al 2004).
</nextsent>
<nextsent>this work presents research on named entity recognition task defined with new set of named entities.
</nextsent>
<nextsent>the characteristic of such set is in that named entities have tree structure.
</nextsent>
<nextsent>as conce quence the task cannot be tackled as sequence labelling approach.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I108">
<title id=" E12-1018.xml">tree representations in probabilistic models for extended named entities detection </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>as conce quence the task cannot be tackled as sequence labelling approach.
</prevsent>
<prevsent>additionally, the use of noisy data like transcriptions of french broadcast data, makes the task very challenging for traditional nlp solutions.
</prevsent>
</prevsection>
<citsent citstr=" J98-4004 ">
to deal with such problems, weadopt two-steps approach, the first being realized with conditional random fields (crf) (laf ferty et al 2001), the second with probabilistic context-free grammar (pcfg) (johnson, 1998).<papid> J98-4004 </papid></citsent>
<aftsection>
<nextsent>the motivations behind that are:?
</nextsent>
<nextsent>since the named entities have tree structure, it is reasonable to use solution coming from syntactic parsing.
</nextsent>
<nextsent>however preliminary experiments using such approaches gave poor results.
</nextsent>
<nextsent>despite the tree-structure of the entities, trees are not as complex as syntactic trees, thus, before designing an ad-hoc solution for the task, which require remarkable effort and yet it doesnt guarantee better performances, we designed solution providing good results and which required limited development effort.?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I116">
<title id=" E12-1018.xml">tree representations in probabilistic models for extended named entities detection </title>
<section> models cascade for extended named.  </section>
<citcontext>
<prevsection>
<prevsent>3.1 conditional random fields.
</prevsent>
<prevsent>crfs are particularly suitable for sequence labelling tasks (lafferty et al 2001).
</prevsent>
</prevsection>
<citsent citstr=" J96-1002 ">
beyond the possibility to include huge number of features using the same framework as maximum entropy models (berger et al 1996), <papid> J96-1002 </papid>crf models encode global conditional probabilities normalized at sentence level.</citsent>
<aftsection>
<nextsent>given sequence of words wn1 =w1, ..., wn and its corresponding components sequence en1 = e1, ..., en , crf trains the conditional probabilities (en1 |w 1 ) = 1 ny n=1 exp mx m=1 ? hm(en1, en, n+2 n2) !
</nextsent>
<nextsent>(1) where are the training parameters.
</nextsent>
<nextsent>hm(en1, en, n+2 n2) are the feature functions capturing dependencies of entities and words.
</nextsent>
<nextsent>z is the partition function: = en1 ny n=1 h(en1, en, n+2 n2) (2) which ensures that probabilities sum up to one.en1 and en are components for previous and current words, h(en1, en, n+2 n2) is an abbreviation for m=1 ? hm(en1, en, n+2 n2), i.e. the set of active feature functions at current position in the sequence.in the last few years different crf implementations have been realized.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I117">
<title id=" E12-1018.xml">tree representations in probabilistic models for extended named entities detection </title>
<section> models cascade for extended named.  </section>
<citcontext>
<prevsection>
<prevsent>hm(en1, en, n+2 n2) are the feature functions capturing dependencies of entities and words.
</prevsent>
<prevsent>z is the partition function: = en1 ny n=1 h(en1, en, n+2 n2) (2) which ensures that probabilities sum up to one.en1 and en are components for previous and current words, h(en1, en, n+2 n2) is an abbreviation for m=1 ? hm(en1, en, n+2 n2), i.e. the set of active feature functions at current position in the sequence.in the last few years different crf implementations have been realized.
</prevsent>
</prevsection>
<citsent citstr=" P10-1052 ">
the implementation we refer in this work is the one described in(lavergne et al 2010), <papid> P10-1052 </papid>which optimize the following objective function: log(p (en1 |w 1 )) + 11 + 2 2 22 (3)1 and 22 are the l1 and l2 regula riz ers (riezler and vasserman, 2004), <papid> W04-3223 </papid>and together in linear combination implement the elastic net regularizer (zou and hastie, 2005).</citsent>
<aftsection>
<nextsent>as mentioned in (lavergne et al 2010), <papid> P10-1052 </papid>this kind of regularizers are very effective for feature selection at training time, which is very good point when dealing with noisy data and big set of features.</nextsent>
<nextsent>176</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I121">
<title id=" E12-1018.xml">tree representations in probabilistic models for extended named entities detection </title>
<section> models cascade for extended named.  </section>
<citcontext>
<prevsection>
<prevsent>hm(en1, en, n+2 n2) are the feature functions capturing dependencies of entities and words.
</prevsent>
<prevsent>z is the partition function: = en1 ny n=1 h(en1, en, n+2 n2) (2) which ensures that probabilities sum up to one.en1 and en are components for previous and current words, h(en1, en, n+2 n2) is an abbreviation for m=1 ? hm(en1, en, n+2 n2), i.e. the set of active feature functions at current position in the sequence.in the last few years different crf implementations have been realized.
</prevsent>
</prevsection>
<citsent citstr=" W04-3223 ">
the implementation we refer in this work is the one described in(lavergne et al 2010), <papid> P10-1052 </papid>which optimize the following objective function: log(p (en1 |w 1 )) + 11 + 2 2 22 (3)1 and 22 are the l1 and l2 regula riz ers (riezler and vasserman, 2004), <papid> W04-3223 </papid>and together in linear combination implement the elastic net regularizer (zou and hastie, 2005).</citsent>
<aftsection>
<nextsent>as mentioned in (lavergne et al 2010), <papid> P10-1052 </papid>this kind of regularizers are very effective for feature selection at training time, which is very good point when dealing with noisy data and big set of features.</nextsent>
<nextsent>176</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I127">
<title id=" E12-1018.xml">tree representations in probabilistic models for extended named entities detection </title>
<section> models for parsing trees.  </section>
<citcontext>
<prevsection>
<prevsent>as mentioned in (lavergne et al 2010), <papid> P10-1052 </papid>this kind of regularizers are very effective for feature selection at training time, which is very good point when dealing with noisy data and big set of features.</prevsent>
<prevsent>176</prevsent>
</prevsection>
<citsent citstr=" W98-1115 ">
the models used in this work for parsing entity trees refer to the models described in (john son, 1998), <papid> J98-4004 </papid>in (charniak, 1997; caraballo and charniak, 1997) and (charniak et al 1998), <papid> W98-1115 </papid>and which constitutes the basis of the maximum entropy model for parsing described in (charniak,2000).<papid> A00-2018 </papid></citsent>
<aftsection>
<nextsent>a similar lexicalized model has been proposed also by collins (collins, 1997)<papid> P97-1003 </papid></nextsent>
<nextsent>all these models are based on pcfg trained from data and used in chart parsing algorithm to find the best parse for the given input.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I128">
<title id=" E12-1018.xml">tree representations in probabilistic models for extended named entities detection </title>
<section> models for parsing trees.  </section>
<citcontext>
<prevsection>
<prevsent>as mentioned in (lavergne et al 2010), <papid> P10-1052 </papid>this kind of regularizers are very effective for feature selection at training time, which is very good point when dealing with noisy data and big set of features.</prevsent>
<prevsent>176</prevsent>
</prevsection>
<citsent citstr=" A00-2018 ">
the models used in this work for parsing entity trees refer to the models described in (john son, 1998), <papid> J98-4004 </papid>in (charniak, 1997; caraballo and charniak, 1997) and (charniak et al 1998), <papid> W98-1115 </papid>and which constitutes the basis of the maximum entropy model for parsing described in (charniak,2000).<papid> A00-2018 </papid></citsent>
<aftsection>
<nextsent>a similar lexicalized model has been proposed also by collins (collins, 1997)<papid> P97-1003 </papid></nextsent>
<nextsent>all these models are based on pcfg trained from data and used in chart parsing algorithm to find the best parse for the given input.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I129">
<title id=" E12-1018.xml">tree representations in probabilistic models for extended named entities detection </title>
<section> models for parsing trees.  </section>
<citcontext>
<prevsection>
<prevsent>176
</prevsent>
<prevsent>the models used in this work for parsing entity trees refer to the models described in (john son, 1998), <papid> J98-4004 </papid>in (charniak, 1997; caraballo and charniak, 1997) and (charniak et al 1998), <papid> W98-1115 </papid>and which constitutes the basis of the maximum entropy model for parsing described in (charniak,2000).<papid> A00-2018 </papid></prevsent>
</prevsection>
<citsent citstr=" P97-1003 ">
a similar lexicalized model has been proposed also by collins (collins, 1997)<papid> P97-1003 </papid></citsent>
<aftsection>
<nextsent>all these models are based on pcfg trained from data and used in chart parsing algorithm to find the best parse for the given input.
</nextsent>
<nextsent>the pcfg model of (johnson, 1998) <papid> J98-4004 </papid>is made of rules of the form: ? xi ? xjxk ? xi ? where are non-terminal entities and are terminal symbols (words in our case).1 the probability associated to these rules are: pij,k = (xi ? xj , xk) (xi) (4) piw = (xi ? w) (xi) (5) the models described in (charniak, 1997;caraballo and charniak, 1997) encode probabilities involving more information, such as head words.</nextsent>
<nextsent>in order to have pcfg model made ofrules with their associated probabilities, we extract rules from the entity trees of our corpus.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I159">
<title id=" E12-1018.xml">tree representations in probabilistic models for extended named entities detection </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the models described in (johnson, 1998), <papid> J98-4004 </papid>(charniak, 1997; caraballo and charniak, 1997), (charniak et al 1998), <papid> W98-1115 </papid>(charniak, 2000), (<papid> A00-2018 </papid>collins, 1997)<papid> P97-1003 </papid>and (ratnaparkhi, 1999), constitute the main individual models proposed for constituent-based syntactic parsing.</prevsent>
<prevsent>later other approaches based on models combination have been proposed, like e.g. the reranking approach described in (collins and koo, 2005), among many, and also evolutions or improvements of these models.</prevsent>
</prevsection>
<citsent citstr=" J07-4004 ">
more recently, approaches based on log-linear models have been proposed (clark and curran, 2007; <papid> J07-4004 </papid>finkel et al 2008) <papid> P08-1109 </papid>for parsing, called also tree crf?, using also different training criteria (auli and lopez, 2011).<papid> D11-1031 </papid></citsent>
<aftsection>
<nextsent>using such models in our work has basically two problems: one related to scaling issues, since our data present large number of labels, which makes crf training problematic, even more when using tree crf?; another problem is related to the difference between syntactic parsing and named entity detection tasks, as mentioned in sub-section 4.2.
</nextsent>
<nextsent>adapting tree crf?
</nextsent>
<nextsent>to our task is thus quite complex work, it constitutes an entire work by itself, we leave it as feature work.
</nextsent>
<nextsent>concerning linear-chain crf models, the one we use is state-of-the-art implementation (lavergne et al 2010), <papid> P10-1052 </papid>as it implements the most effective optimization algorithms as well as state-of-the-art regularizers (see sub-section 3.1).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I160">
<title id=" E12-1018.xml">tree representations in probabilistic models for extended named entities detection </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the models described in (johnson, 1998), <papid> J98-4004 </papid>(charniak, 1997; caraballo and charniak, 1997), (charniak et al 1998), <papid> W98-1115 </papid>(charniak, 2000), (<papid> A00-2018 </papid>collins, 1997)<papid> P97-1003 </papid>and (ratnaparkhi, 1999), constitute the main individual models proposed for constituent-based syntactic parsing.</prevsent>
<prevsent>later other approaches based on models combination have been proposed, like e.g. the reranking approach described in (collins and koo, 2005), among many, and also evolutions or improvements of these models.</prevsent>
</prevsection>
<citsent citstr=" P08-1109 ">
more recently, approaches based on log-linear models have been proposed (clark and curran, 2007; <papid> J07-4004 </papid>finkel et al 2008) <papid> P08-1109 </papid>for parsing, called also tree crf?, using also different training criteria (auli and lopez, 2011).<papid> D11-1031 </papid></citsent>
<aftsection>
<nextsent>using such models in our work has basically two problems: one related to scaling issues, since our data present large number of labels, which makes crf training problematic, even more when using tree crf?; another problem is related to the difference between syntactic parsing and named entity detection tasks, as mentioned in sub-section 4.2.
</nextsent>
<nextsent>adapting tree crf?
</nextsent>
<nextsent>to our task is thus quite complex work, it constitutes an entire work by itself, we leave it as feature work.
</nextsent>
<nextsent>concerning linear-chain crf models, the one we use is state-of-the-art implementation (lavergne et al 2010), <papid> P10-1052 </papid>as it implements the most effective optimization algorithms as well as state-of-the-art regularizers (see sub-section 3.1).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I161">
<title id=" E12-1018.xml">tree representations in probabilistic models for extended named entities detection </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the models described in (johnson, 1998), <papid> J98-4004 </papid>(charniak, 1997; caraballo and charniak, 1997), (charniak et al 1998), <papid> W98-1115 </papid>(charniak, 2000), (<papid> A00-2018 </papid>collins, 1997)<papid> P97-1003 </papid>and (ratnaparkhi, 1999), constitute the main individual models proposed for constituent-based syntactic parsing.</prevsent>
<prevsent>later other approaches based on models combination have been proposed, like e.g. the reranking approach described in (collins and koo, 2005), among many, and also evolutions or improvements of these models.</prevsent>
</prevsection>
<citsent citstr=" D11-1031 ">
more recently, approaches based on log-linear models have been proposed (clark and curran, 2007; <papid> J07-4004 </papid>finkel et al 2008) <papid> P08-1109 </papid>for parsing, called also tree crf?, using also different training criteria (auli and lopez, 2011).<papid> D11-1031 </papid></citsent>
<aftsection>
<nextsent>using such models in our work has basically two problems: one related to scaling issues, since our data present large number of labels, which makes crf training problematic, even more when using tree crf?; another problem is related to the difference between syntactic parsing and named entity detection tasks, as mentioned in sub-section 4.2.
</nextsent>
<nextsent>adapting tree crf?
</nextsent>
<nextsent>to our task is thus quite complex work, it constitutes an entire work by itself, we leave it as feature work.
</nextsent>
<nextsent>concerning linear-chain crf models, the one we use is state-of-the-art implementation (lavergne et al 2010), <papid> P10-1052 </papid>as it implements the most effective optimization algorithms as well as state-of-the-art regularizers (see sub-section 3.1).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I173">
<title id=" E09-1032.xml">who is x201cyoux201d combining linguistic and gaze features to resolve second person references in dialogue </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in our first set of experiments, the linguistic and visual features are derived from manual transcriptions and annotations, but in the second set, they are generated through entirely automatic means.
</prevsent>
<prevsent>results show that multimodal system is often prefer able to uni modal one.
</prevsent>
</prevsection>
<citsent citstr=" P07-2027 ">
the english pronoun you is the second most frequent word in unrestricted conversation (after iand right before it).1 despite this, with the exception of gupta et al (2007<papid> P07-2027 </papid>b), gupta et al (2007<papid> P07-2027 </papid>a), its resolution has received very little attention in the lit erature.</citsent>
<aftsection>
<nextsent>this is perhaps not surprising since the vast amount of work on anaphora and reference resolution has focused on text or discourse - mediums where second-person deixis is perhaps notas prominent as it is in dialogue.
</nextsent>
<nextsent>for spoken dialogue pronoun resolution modules however, resolving you is an essential task that has an important impact on the capabilities of dialogue summarization systems.
</nextsent>
<nextsent>we thank the anonymous eacl reviewers, and surabhi gupta, john niekrasz and david demirdjian for their comments and technical assistance.
</nextsent>
<nextsent>this work was supported by the calo project (darpa grant nbch-d-03-0010).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I205">
<title id=" E09-1032.xml">who is x201cyoux201d combining linguistic and gaze features to resolve second person references in dialogue </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>(2003) and turnhout et al (2005) studied this issue in the context of mixed human-human andhuman-computer interaction using facial orientation and utterance length as clues for addressee detection, while katzenmaier et al (2004) investigated whether the degree to which user utterance fits the language model of conversational robot can be useful in detecting system-addressedutterances.
</prevsent>
<prevsent>this research exploits the fact that humans tend to speak differently to systems than to other humans.
</prevsent>
</prevsection>
<citsent citstr=" E06-1022 ">
our research is closer to that of jovanovic et al (2006<papid> E06-1022 </papid>a), jovanovic et al (2007), who studied addressing in human-human multi-party dialogue.</citsent>
<aftsection>
<nextsent>jovanovic and colleagues focus on addressee identification in face-to-face meetings with four participants.
</nextsent>
<nextsent>theyuse bayesian network classifier trained on several multimodal features (including visual features such as gaze direction, discourse features such asthe speaker and dialogue act of preceding utterances, and utterance features such as lexical clues and utterance duration).
</nextsent>
<nextsent>using combination of features from various resources was found to im prove performance (the best system achieves an accuracy of 77% on portion of the ami meeting corpus).
</nextsent>
<nextsent>although this result is very encouraging, it is achieved with the use of manually produced information - in particular, manual transcriptions,dialogue acts and annotations of visual focus of attention.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I215">
<title id=" E09-1032.xml">who is x201cyoux201d combining linguistic and gaze features to resolve second person references in dialogue </title>
<section> linguistic information.  </section>
<citcontext>
<prevsection>
<prevsent>hence we experimented with different gpt values in increments of 0.1, and compared there sulting features to the manual features using the kappa statistic.
</prevsent>
<prevsent>a threshold of 0.6 gave the best kappa scores, which ranged from 20% to 44%.6
</prevsent>
</prevsection>
<citsent citstr=" P04-1085 ">
our set of discourse features is simplified version of those employed by galley et al (2004) <papid> P04-1085 </papid>and gupta et al (2007<papid> P07-2027 </papid>a).</citsent>
<aftsection>
<nextsent>it contains three main types (summarized in table 4): ? sentential features (1 to 13) encode structural, dura tional, lexical and shallow syntactic patterns of the you-utterance.
</nextsent>
<nextsent>feature 13 is extracted using the ami named entity?
</nextsent>
<nextsent>annotations and indicates whether particular participant is mentioned in the you-utterance.
</nextsent>
<nextsent>apart from this feature, all other sentential features are automatically extracted, and besides 1, 8, 9, and 10, they are all binary.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I240">
<title id=" E12-1021.xml">incorporating lexical priors into topic models </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>importantly, we only encourage the model to follow the seed sets and do not force it.
</prevsent>
<prevsent>so if it has compelling evidence in the data to overcome the seed information then it still has the freedom to do so.
</prevsent>
</prevsection>
<citsent citstr=" P11-1026 ">
our seeding approach in combination with the interactive topic modeling (hu et al  2011) <papid> P11-1026 </papid>will allow user to both explore corpus, and also guide the exploration towards the distinctions that he/she finds more interesting.</citsent>
<aftsection>
<nextsent>our approach to allowing user to guide the topic discovery process is to let him provide seed information at the level of word type.
</nextsent>
<nextsent>namely, the user provides sets of seed words that are representative of the corpus.
</nextsent>
<nextsent>table 2 shows an example of seed sets one might use for the reuters corpus.
</nextsent>
<nextsent>this kind of supervision is similar to the seeding in bootstrapping literature (thelen and riloff, 2002) <papid> W02-1028 </papid>or prototype-based learning (haghighi and klein, 2006).<papid> N06-1041 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I241">
<title id=" E12-1021.xml">incorporating lexical priors into topic models </title>
<section> incorporating seeds.  </section>
<citcontext>
<prevsection>
<prevsent>namely, the user provides sets of seed words that are representative of the corpus.
</prevsent>
<prevsent>table 2 shows an example of seed sets one might use for the reuters corpus.
</prevsent>
</prevsection>
<citsent citstr=" W02-1028 ">
this kind of supervision is similar to the seeding in bootstrapping literature (thelen and riloff, 2002) <papid> W02-1028 </papid>or prototype-based learning (haghighi and klein, 2006).<papid> N06-1041 </papid></citsent>
<aftsection>
<nextsent>our reliance on seed sets is orthogonal to existing approaches that use external knowledge, which operate at the level of documents(blei and mcauliffe, 2008), tokens (andrzejewski and zhu, 2009)<papid> W09-2206 </papid>or pair-wise constraints (an drzejewski et al  2009).</nextsent>
<nextsent>we build model that uses the seed words in two ways: to improve both topic-word and document-topic probability distributions.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I242">
<title id=" E12-1021.xml">incorporating lexical priors into topic models </title>
<section> incorporating seeds.  </section>
<citcontext>
<prevsection>
<prevsent>namely, the user provides sets of seed words that are representative of the corpus.
</prevsent>
<prevsent>table 2 shows an example of seed sets one might use for the reuters corpus.
</prevsent>
</prevsection>
<citsent citstr=" N06-1041 ">
this kind of supervision is similar to the seeding in bootstrapping literature (thelen and riloff, 2002) <papid> W02-1028 </papid>or prototype-based learning (haghighi and klein, 2006).<papid> N06-1041 </papid></citsent>
<aftsection>
<nextsent>our reliance on seed sets is orthogonal to existing approaches that use external knowledge, which operate at the level of documents(blei and mcauliffe, 2008), tokens (andrzejewski and zhu, 2009)<papid> W09-2206 </papid>or pair-wise constraints (an drzejewski et al  2009).</nextsent>
<nextsent>we build model that uses the seed words in two ways: to improve both topic-word and document-topic probability distributions.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I244">
<title id=" E12-1021.xml">incorporating lexical priors into topic models </title>
<section> incorporating seeds.  </section>
<citcontext>
<prevsection>
<prevsent>table 2 shows an example of seed sets one might use for the reuters corpus.
</prevsent>
<prevsent>this kind of supervision is similar to the seeding in bootstrapping literature (thelen and riloff, 2002) <papid> W02-1028 </papid>or prototype-based learning (haghighi and klein, 2006).<papid> N06-1041 </papid></prevsent>
</prevsection>
<citsent citstr=" W09-2206 ">
our reliance on seed sets is orthogonal to existing approaches that use external knowledge, which operate at the level of documents(blei and mcauliffe, 2008), tokens (andrzejewski and zhu, 2009)<papid> W09-2206 </papid>or pair-wise constraints (an drzejewski et al  2009).</citsent>
<aftsection>
<nextsent>we build model that uses the seed words in two ways: to improve both topic-word and document-topic probability distributions.
</nextsent>
<nextsent>forease of exposition, we present these ideas separately and then in combination (section 2.3).
</nextsent>
<nextsent>to improve topic-word distributions, we set upa model in which each topic prefers to generate words that are related to the words in seed set (section 2.1).
</nextsent>
<nextsent>to improve document-topic distributions, we encourage the model to select document-level topics based on the existence of input seed words in that document (section 2.2).before moving on to the details of our models, we briefly recall the generative story of the lda model and the reader is encouraged to refer to (blei et al  2003) for further details.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I248">
<title id=" E12-1021.xml">incorporating lexical priors into topic models </title>
<section> for each document d,.  </section>
<citcontext>
<prevsection>
<prevsent>here, we describe the use of feature selection techniques, prevalent in the classification literature, to automatically derive the seed sets.
</prevsent>
<prevsent>if we want the topicality structure identified by the lda to align with the underlying class structure, then the seed words need to be representative of the underlying topicality structure.
</prevsent>
</prevsection>
<citsent citstr=" D09-1026 ">
to enable this, we first take class labeled data (doesnt need to be multi-class labeled data unlike (ramage et al  2009)) <papid> D09-1026 </papid>and identify the discriminating features for each class.</citsent>
<aftsection>
<nextsent>then we choose these discriminating features as the initial sets of seed words.
</nextsent>
<nextsent>in principle, this is similar to the prototype driven unsupervised learning (haghighi and klein, 2006).<papid> N06-1041 </papid></nextsent>
<nextsent>we use information gain (mitchell, 1997) to identify the required discriminating features.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I258">
<title id=" E12-1021.xml">incorporating lexical priors into topic models </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>for both the corpora we do the standard preprocessing of removing stop words and infrequent words (williamson et al  2010).
</prevsent>
<prevsent>for all the models, we use collapsed gibbs sampler (griffiths and steyvers, 2004) for the inference process.
</prevsent>
</prevsection>
<citsent citstr=" N09-1036 ">
we use the standard hyperparam eters values ? = 1.0, ? = 0.01 and ? = 1.0 and run the sampler for 1000 iterations, but one can use techniques like slice sampling to estimate the hyper parameters (johnson and goldwater, 2009).<papid> N09-1036 </papid></citsent>
<aftsection>
<nextsent>209 reuters 20 news groups f-measure vi f-measure vi lda 0.64 (?.05) 1.26 (?.16) 0.77 (?.06) 0.9 (?.13) dirichlet forest 0.67?
</nextsent>
<nextsent>(?.02) 1.17 (?.11) 0.79(?.01) 0.83?(?.03) ? over lda (+4.68%) (-7.1%) (+2.6%) (-7.8%) table 3: the effect of adding constraints by dirichlet forest encoding.
</nextsent>
<nextsent>for variational information (vi) lower score indicates better clustering.
</nextsent>
<nextsent>indicates statistical significance at = 0.01 as measured by the t-test.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I260">
<title id=" E09-1056.xml">improvements in ana logical learning application to translating multi terms of the medical domain </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>if machine translation is to meet commercial needs, it must offer sensible approach to translating terms.
</prevsent>
<prevsent>currently, mt systems offer at best database management tools which allow human (typically translator, terminologist or even the vendor of the system) to specify bilingual terminological entries.
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
more advanced tools are meant to identify inconsistencies in terminological translations and might prove useful in controlled language situations (itagaki et al, 2007).one approach to translate terms consists in using domain-specific parallel corpus with standard alignment techniques (brown et al, 1993) <papid> J93-2003 </papid>tomine new translations.</citsent>
<aftsection>
<nextsent>massive amounts of parallel data are certainly available in several pairsof languages for domains such as parliament debates or the like.
</nextsent>
<nextsent>however, having at our disposal domain-specific (e.g. computer science) bitext with an adequate coverage is another issue.
</nextsent>
<nextsent>one might argue that domain-specific comparable (or perhaps unrelated) corpora are easier to acquire, in which case context-vector techniques (rapp, 1995; <papid> P95-1050 </papid>fung and mckeown, 1997) <papid> W97-0119 </papid>can be used to identify the translation of terms.</nextsent>
<nextsent>we certainly agree with that point of view to certain extent, but as discussed by morin et al (2007), <papid> P07-1084 </papid>for many specific domains and pairs of languages, such resources simply do not exist.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I261">
<title id=" E09-1056.xml">improvements in ana logical learning application to translating multi terms of the medical domain </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>massive amounts of parallel data are certainly available in several pairsof languages for domains such as parliament debates or the like.
</prevsent>
<prevsent>however, having at our disposal domain-specific (e.g. computer science) bitext with an adequate coverage is another issue.
</prevsent>
</prevsection>
<citsent citstr=" P95-1050 ">
one might argue that domain-specific comparable (or perhaps unrelated) corpora are easier to acquire, in which case context-vector techniques (rapp, 1995; <papid> P95-1050 </papid>fung and mckeown, 1997) <papid> W97-0119 </papid>can be used to identify the translation of terms.</citsent>
<aftsection>
<nextsent>we certainly agree with that point of view to certain extent, but as discussed by morin et al (2007), <papid> P07-1084 </papid>for many specific domains and pairs of languages, such resources simply do not exist.</nextsent>
<nextsent>furthermore, the task of translation identification is more difficult and error-prone.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I262">
<title id=" E09-1056.xml">improvements in ana logical learning application to translating multi terms of the medical domain </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>massive amounts of parallel data are certainly available in several pairsof languages for domains such as parliament debates or the like.
</prevsent>
<prevsent>however, having at our disposal domain-specific (e.g. computer science) bitext with an adequate coverage is another issue.
</prevsent>
</prevsection>
<citsent citstr=" W97-0119 ">
one might argue that domain-specific comparable (or perhaps unrelated) corpora are easier to acquire, in which case context-vector techniques (rapp, 1995; <papid> P95-1050 </papid>fung and mckeown, 1997) <papid> W97-0119 </papid>can be used to identify the translation of terms.</citsent>
<aftsection>
<nextsent>we certainly agree with that point of view to certain extent, but as discussed by morin et al (2007), <papid> P07-1084 </papid>for many specific domains and pairs of languages, such resources simply do not exist.</nextsent>
<nextsent>furthermore, the task of translation identification is more difficult and error-prone.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I263">
<title id=" E09-1056.xml">improvements in ana logical learning application to translating multi terms of the medical domain </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, having at our disposal domain-specific (e.g. computer science) bitext with an adequate coverage is another issue.
</prevsent>
<prevsent>one might argue that domain-specific comparable (or perhaps unrelated) corpora are easier to acquire, in which case context-vector techniques (rapp, 1995; <papid> P95-1050 </papid>fung and mckeown, 1997) <papid> W97-0119 </papid>can be used to identify the translation of terms.</prevsent>
</prevsection>
<citsent citstr=" P07-1084 ">
we certainly agree with that point of view to certain extent, but as discussed by morin et al (2007), <papid> P07-1084 </papid>for many specific domains and pairs of languages, such resources simply do not exist.</citsent>
<aftsection>
<nextsent>furthermore, the task of translation identification is more difficult and error-prone.
</nextsent>
<nextsent>ana logical learning has recently regained some interest in the nlp community.
</nextsent>
<nextsent>lepage and denoual (2005) proposed machine translation system entirely based on the concept of formal analogy, that is, analogy on forms.
</nextsent>
<nextsent>stroppa andyvon (2005) applied ana logical learning to several morphological tasks also involving analogies on words.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I264">
<title id=" E09-1056.xml">improvements in ana logical learning application to translating multi terms of the medical domain </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>lepage and denoual (2005) proposed machine translation system entirely based on the concept of formal analogy, that is, analogy on forms.
</prevsent>
<prevsent>stroppa andyvon (2005) applied ana logical learning to several morphological tasks also involving analogies on words.
</prevsent>
</prevsection>
<citsent citstr=" D07-1092 ">
langlais and patry (2007) <papid> D07-1092 </papid>applied it to the task of translating unknown words in several european languages, an idea investigated as wellby denoual (2007) for japanese to english translation task.</citsent>
<aftsection>
<nextsent>in this study, we improve the state-of-the-art of ana logical learning by (i) proposing simple yet effective implementation of an ana logical solver;(ii) proposing an efficient solution to the search issue embedded in ana logical learning, (iii) investigating whether classifier can be trained to recognize bad candidates produced by ana logical learning.
</nextsent>
<nextsent>we evaluate our ana logical engine on the taskof translating terms of the medical domain; do main well-known for its tendency to create new words, many of which being complex lexical constructions.
</nextsent>
<nextsent>our experiments involve five language pairs, including languages with very different morphological systems.
</nextsent>
<nextsent>487 in the remainder of this paper, we first present in section 2 the principle of ana logical learning.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I268">
<title id=" E09-1056.xml">improvements in ana logical learning application to translating multi terms of the medical domain </title>
<section> practical issues.  </section>
<citcontext>
<prevsection>
<prevsent>490 reference) from spurious ones.
</prevsent>
<prevsent>we applied to this end the voted-perceptron algorithm described by freund and schapire (1999).
</prevsent>
</prevsection>
<citsent citstr=" W02-1001 ">
online voted perceptrons have been reported to work well in number of nlp tasks (collins, 2002; <papid> W02-1001 </papid>liang et al, 2006).<papid> P06-1096 </papid></citsent>
<aftsection>
<nextsent>training such classifier is mainly matter of feature engineering.
</nextsent>
<nextsent>an example is pair of source-target ana logical relations (r, r?)
</nextsent>
<nextsent>identified by the generator, and which elects t?
</nextsent>
<nextsent>as translation for the term t: ?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I269">
<title id=" E09-1056.xml">improvements in ana logical learning application to translating multi terms of the medical domain </title>
<section> practical issues.  </section>
<citcontext>
<prevsection>
<prevsent>490 reference) from spurious ones.
</prevsent>
<prevsent>we applied to this end the voted-perceptron algorithm described by freund and schapire (1999).
</prevsent>
</prevsection>
<citsent citstr=" P06-1096 ">
online voted perceptrons have been reported to work well in number of nlp tasks (collins, 2002; <papid> W02-1001 </papid>liang et al, 2006).<papid> P06-1096 </papid></citsent>
<aftsection>
<nextsent>training such classifier is mainly matter of feature engineering.
</nextsent>
<nextsent>an example is pair of source-target ana logical relations (r, r?)
</nextsent>
<nextsent>identified by the generator, and which elects t?
</nextsent>
<nextsent>as translation for the term t: ?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I270">
<title id=" E09-1056.xml">improvements in ana logical learning application to translating multi terms of the medical domain </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>we trained phrase tableon train, using the standard approach.9 how ever, because of the small training size, and the rather huge oov rate of the translation tasks we address, we did not train translation models onword-tokens, but at the character level.
</prevsent>
<prevsent>therefore phrase is indeed sequence of characters.
</prevsent>
</prevsection>
<citsent citstr=" W07-0705 ">
this idea has been successively investigated in catalan-to-spanish translation task by vilar et al (2007).<papid> W07-0705 </papid></citsent>
<aftsection>
<nextsent>we tuned the 8 coefficients of the so-called log-linear combination maximized at decoding time on the first 200 pairs of terms of the dev corpora.
</nextsent>
<nextsent>on the dev set, bleu scores10 range from 67.2 (english-to-finnish) to 77.0 (russian-to-english).
</nextsent>
<nextsent>table 7 reports the precision and recall of both translation engines.
</nextsent>
<nextsent>note that because the smt engine always propose translation, its precision equals its recall.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I272">
<title id=" E09-1056.xml">improvements in ana logical learning application to translating multi terms of the medical domain </title>
<section> discussion and future work.  </section>
<citcontext>
<prevsection>
<prevsent>we show that the ana logical translations are of better quality than those produced by phrase-based engine trained at the character level, albeit with much lower recall.
</prevsent>
<prevsent>a straightforward combination of both approaches led an improvement of 5.3 bleu points over the smt alone.
</prevsent>
</prevsection>
<citsent citstr=" P08-1059 ">
better smt performance could be obtained with system based on morphemes, see for instance (toutanova et al, 2008).<papid> P08-1059 </papid></citsent>
<aftsection>
<nextsent>however, since lists of morphemes specific to the medical domain do not exist for all the languages pairs we considered here, unsupervised methods for acquiring morphemes would be necessary, which is left as future work.
</nextsent>
<nextsent>in any case, this comparison is meaningful, since both the smt and the analogi cal device work at the character level.
</nextsent>
<nextsent>this work opens up several avenues.
</nextsent>
<nextsent>first, wewill test our approach on terminologies from different domains, varying the size of the trainingmaterial.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I273">
<title id=" E09-3003.xml">combining a statistical language model with logistic regression to predict the lexical and syntactic difficulty of texts for ffl </title>
<section> related research.  </section>
<citcontext>
<prevsection>
<prevsent>model.
</prevsent>
<prevsent>we retained from their work the use of language models instead of word lists to measure lexical complexity.
</prevsent>
</prevsection>
<citsent citstr=" P05-1065 ">
schwarm and ostendorf (2005) <papid> P05-1065 </papid>developed svm categoriser combining classifier based on trigram language models (one for each levelof difficulty), some parsing features such as average tree height, and variables traditionally used in readability.</citsent>
<aftsection>
<nextsent>heilman et al  (2007) <papid> N07-1058 </papid>extended the smoothed unigram?</nextsent>
<nextsent>model by the recognition of syntactic structures, in order to assess l2 english texts.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I274">
<title id=" E09-3003.xml">combining a statistical language model with logistic regression to predict the lexical and syntactic difficulty of texts for ffl </title>
<section> related research.  </section>
<citcontext>
<prevsection>
<prevsent>we retained from their work the use of language models instead of word lists to measure lexical complexity.
</prevsent>
<prevsent>schwarm and ostendorf (2005) <papid> P05-1065 </papid>developed svm categoriser combining classifier based on trigram language models (one for each levelof difficulty), some parsing features such as average tree height, and variables traditionally used in readability.</prevsent>
</prevsection>
<citsent citstr=" N07-1058 ">
heilman et al  (2007) <papid> N07-1058 </papid>extended the smoothed unigram?</citsent>
<aftsection>
<nextsent>model by the recognition of syntactic structures, in order to assess l2 english texts.
</nextsent>
<nextsent>later, they improved the combination oftheir various lexical and grammatical features using regression methods (heilman et al , 2008).<papid> W08-0909 </papid></nextsent>
<nextsent>wealso found regression methods to be the most efficient of the statistical models with which we ex perimented.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I275">
<title id=" E09-3003.xml">combining a statistical language model with logistic regression to predict the lexical and syntactic difficulty of texts for ffl </title>
<section> related research.  </section>
<citcontext>
<prevsection>
<prevsent>heilman et al  (2007) <papid> N07-1058 </papid>extended the smoothed unigram?</prevsent>
<prevsent>model by the recognition of syntactic structures, in order to assess l2 english texts.</prevsent>
</prevsection>
<citsent citstr=" W08-0909 ">
later, they improved the combination oftheir various lexical and grammatical features using regression methods (heilman et al , 2008).<papid> W08-0909 </papid></citsent>
<aftsection>
<nextsent>wealso found regression methods to be the most efficient of the statistical models with which we experimented.
</nextsent>
<nextsent>in this article, we consider some ways to adapt these various ideas to the specific case of ffl readability.
</nextsent>
<nextsent>in the development of new readability formula, the first step is to collect corpus labelled byreading-difficulty level, task that implies agreement on the difficulty scale.
</nextsent>
<nextsent>in the us, common choice is the 12 american grade levels corresponding to primary and secondary school.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I285">
<title id=" E09-1062.xml">performance confidence estimation for automatic summarization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in our work, we provide deeper insight into how other characteristics of the text itself and properties of document clusters can be used to identify difficult inputs.
</prevsent>
<prevsent>the task of predicting the confidence in system performance forgiven input is in fact relevant notonly for summarization, but in general for all applications aimed at facilitating information access.
</prevsent>
</prevsection>
<citsent citstr=" W02-1033 ">
in question answering for example, system may be configured not to answer questions for which the confidence of producing correct answer is low, and in this way increase the overall accuracy of the system whenever it does produce an answer (brill et al, 2002; <papid> W02-1033 </papid>dredze and czuba, 2007).similarly in machine translation, some sentences might contain difficult to translate phrases, that is, portions of the input are likely to leadto garbled output if automatic translation is at tempted.</citsent>
<aftsection>
<nextsent>automatically identifying such phrases has the potential of improving mt as shown byan oracle study (mohit and hwa, 2007).<papid> W07-0737 </papid></nextsent>
<nextsent>more recent work (birch et al, 2008) <papid> D08-1078 </papid>has shown that properties of reordering, source and target language complexity and relatedness can be used to predict translation quality.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I286">
<title id=" E09-1062.xml">performance confidence estimation for automatic summarization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the task of predicting the confidence in system performance forgiven input is in fact relevant notonly for summarization, but in general for all applications aimed at facilitating information access.
</prevsent>
<prevsent>in question answering for example, system may be configured not to answer questions for which the confidence of producing correct answer is low, and in this way increase the overall accuracy of the system whenever it does produce an answer (brill et al, 2002; <papid> W02-1033 </papid>dredze and czuba, 2007).similarly in machine translation, some sentences might contain difficult to translate phrases, that is, portions of the input are likely to leadto garbled output if automatic translation is at tempted.</prevsent>
</prevsection>
<citsent citstr=" W07-0737 ">
automatically identifying such phrases has the potential of improving mt as shown byan oracle study (mohit and hwa, 2007).<papid> W07-0737 </papid></citsent>
<aftsection>
<nextsent>more recent work (birch et al, 2008) <papid> D08-1078 </papid>has shown that properties of reordering, source and target language complexity and relatedness can be used to predict translation quality.</nextsent>
<nextsent>in information retrieval, the problem of predicting system performance has generated considerable interest and has led to notably good results (cronen-townsend et al, 2002; yom-tov et al, 2005; carmel et al, 2006).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I287">
<title id=" E09-1062.xml">performance confidence estimation for automatic summarization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in question answering for example, system may be configured not to answer questions for which the confidence of producing correct answer is low, and in this way increase the overall accuracy of the system whenever it does produce an answer (brill et al, 2002; <papid> W02-1033 </papid>dredze and czuba, 2007).similarly in machine translation, some sentences might contain difficult to translate phrases, that is, portions of the input are likely to leadto garbled output if automatic translation is at tempted.</prevsent>
<prevsent>automatically identifying such phrases has the potential of improving mt as shown byan oracle study (mohit and hwa, 2007).<papid> W07-0737 </papid></prevsent>
</prevsection>
<citsent citstr=" D08-1078 ">
more recent work (birch et al, 2008) <papid> D08-1078 </papid>has shown that properties of reordering, source and target language complexity and relatedness can be used to predict translation quality.</citsent>
<aftsection>
<nextsent>in information retrieval, the problem of predicting system performance has generated considerable interest and has led to notably good results (cronen-townsend et al, 2002; yom-tov et al, 2005; carmel et al, 2006).
</nextsent>
<nextsent>541
</nextsent>
<nextsent>in summarization, researchers have recognized that some inputs might be more successfully handled by particular subsystem (mckeown et al, 2001), but little work has been done to qualify the general characteristics of inputs that lead to subop timal performance of systems.
</nextsent>
<nextsent>only recently the issue has drawn attention: (nenkova and louis, 2008) <papid> P08-1094 </papid>present an initial analysis of the factors that influence system performance in content selection.this study was based on results from the document understanding conference (duc) evaluations (over et al, 2007) of multi-document summarization of news.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I288">
<title id=" E09-1062.xml">performance confidence estimation for automatic summarization </title>
<section> task definition.  </section>
<citcontext>
<prevsection>
<prevsent>541
</prevsent>
<prevsent>in summarization, researchers have recognized that some inputs might be more successfully handled by particular subsystem (mckeown et al, 2001), but little work has been done to qualify the general characteristics of inputs that lead to subop timal performance of systems.
</prevsent>
</prevsection>
<citsent citstr=" P08-1094 ">
only recently the issue has drawn attention: (nenkova and louis, 2008) <papid> P08-1094 </papid>present an initial analysis of the factors that influence system performance in content selection.this study was based on results from the document understanding conference (duc) evaluations (over et al, 2007) of multi-document summarization of news.</citsent>
<aftsection>
<nextsent>they showed that input, system identity and length of the target summary were all significant factors affecting summary quality.
</nextsent>
<nextsent>longer summaries were consistently better than shorter ones for the same input, so improvements can be easy in applications where varying target size is possible.
</nextsent>
<nextsent>indeed, varying summary size is desirable in many situations (kaisser et al, 2008).<papid> P08-1080 </papid></nextsent>
<nextsent>the most predictive factor of summary quality was input identity, prompting closer investigation of input properties that are indicative of deterioration in performance.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I289">
<title id=" E09-1062.xml">performance confidence estimation for automatic summarization </title>
<section> task definition.  </section>
<citcontext>
<prevsection>
<prevsent>they showed that input, system identity and length of the target summary were all significant factors affecting summary quality.
</prevsent>
<prevsent>longer summaries were consistently better than shorter ones for the same input, so improvements can be easy in applications where varying target size is possible.
</prevsent>
</prevsection>
<citsent citstr=" P08-1080 ">
indeed, varying summary size is desirable in many situations (kaisser et al, 2008).<papid> P08-1080 </papid></citsent>
<aftsection>
<nextsent>the most predictive factor of summary quality was input identity, prompting closer investigation of input properties that are indicative of deterioration in performance.
</nextsent>
<nextsent>for example, summaries of articles describing different opinions about an issue or of articles describing multiple distinct events of the same type were of overall poor quality, while summaries of more focused inputs, dealing with descriptions of single event, subject or person (biographical), were on average better.
</nextsent>
<nextsent>a number of features were defined, capturing aspects of how focused on single topic given input is. analysis of the predictive power of the features was done using only one year of duc evaluations.
</nextsent>
<nextsent>data from later evaluations was used to train and test logistic regression classifier for prediction of expected system performance.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I291">
<title id=" E09-1062.xml">performance confidence estimation for automatic summarization </title>
<section> features.  </section>
<citcontext>
<prevsection>
<prevsent>input size-related number of sentences in the input, number of tokens, vocabulary size, percentage of words used only once, type-token ratio.
</prevsent>
<prevsent>information-theoretic measures entropy ofthe input word distribution and kl divergence between the input and large document collection.1evaluations from later years did not include generic summarization, but introduced new tasks such as topic-focused and update summarization.
</prevsent>
</prevsection>
<citsent citstr=" C00-1072 ">
log-likelihood ratio for words in the input number of topic signature words (lin and hovy,2000; <papid> C00-1072 </papid>conroy et al, 2006) <papid> P06-2020 </papid>and percentage of signature words in the vocabulary.</citsent>
<aftsection>
<nextsent>document similarity in the input set these features apply to multi-document summarization only.
</nextsent>
<nextsent>pairwise similarity of documents within an input were computed using tf.idf weighted vector representations of the documents, either using all words or using only topic signature words.
</nextsent>
<nextsent>in both settings, minimum, maximum and average cosinesimilarity was computed, resulting in six similarity features.
</nextsent>
<nextsent>multi-document summaries from duc 2001 were used for feature selection.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I292">
<title id=" E09-1062.xml">performance confidence estimation for automatic summarization </title>
<section> features.  </section>
<citcontext>
<prevsection>
<prevsent>input size-related number of sentences in the input, number of tokens, vocabulary size, percentage of words used only once, type-token ratio.
</prevsent>
<prevsent>information-theoretic measures entropy ofthe input word distribution and kl divergence between the input and large document collection.1evaluations from later years did not include generic summarization, but introduced new tasks such as topic-focused and update summarization.
</prevsent>
</prevsection>
<citsent citstr=" P06-2020 ">
log-likelihood ratio for words in the input number of topic signature words (lin and hovy,2000; <papid> C00-1072 </papid>conroy et al, 2006) <papid> P06-2020 </papid>and percentage of signature words in the vocabulary.</citsent>
<aftsection>
<nextsent>document similarity in the input set these features apply to multi-document summarization only.
</nextsent>
<nextsent>pairwise similarity of documents within an input were computed using tf.idf weighted vector representations of the documents, either using all words or using only topic signature words.
</nextsent>
<nextsent>in both settings, minimum, maximum and average cosinesimilarity was computed, resulting in six similarity features.
</nextsent>
<nextsent>multi-document summaries from duc 2001 were used for feature selection.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I294">
<title id=" E09-1062.xml">performance confidence estimation for automatic summarization </title>
<section> pairwise ranking approach.  </section>
<citcontext>
<prevsection>
<prevsent>given pair of inputs, can we identify the one on which systems will perform better?
</prevsent>
<prevsent>this ranking task is easier than requiring strict decision on whether performance will be good or not.
</prevsent>
</prevsection>
<citsent citstr=" N01-1003 ">
ranking approaches are widely used in text planning and sentence ordering (walker et al, 2001; <papid> N01-1003 </papid>karamanis, 2003) to select the text with best structure among set of possible candidates.</citsent>
<aftsection>
<nextsent>under the summarization framework, (barzilay and lapata, 2008) ranked different summaries for thesame input according to their coherence.
</nextsent>
<nextsent>similarly, ranking alternative document clusters on the same topic to choose the best input will prove an added advantage to summarizer systems.
</nextsent>
<nextsent>when summarization is used as part of an information access interface, the clustering of related documents that form the input to system is doneautomatically.
</nextsent>
<nextsent>currently, the clustering of documents is completely independent of the need for subsequent summarization of the resulting clusters.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I295">
<title id=" E09-1091.xml">mint a method for effective and scalable mining of named entity transliterations from large comparable corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in cross-language information retrieval (clir) systems, they play an even more important role as the accuracy of their transliterations is shown to correlate highly with the performance of the clir systems (mandl and womser-hacker, 2005, xu and weischedel, 2005).
</prevsent>
<prevsent>traditional methods for transliterations have not proven to be very effective in clir.
</prevsent>
</prevsection>
<citsent citstr=" W03-1508 ">
machine transliteration systems (abduljaleel and larkey, 2003; al-onaizan and knight, 2002; virga and khudanpur, 2003) <papid> W03-1508 </papid>usually produce incorrect transliterations and translation lexcions such as hand-crafted or statistical dictionaries are too static to have good coverage of nes1 occur ring in the current news events.</citsent>
<aftsection>
<nextsent>hence, there is critical need for creating and continually updat * currently with university of utah.
</nextsent>
<nextsent>1 new nes are introduced to the vocabulary of lan-.
</nextsent>
<nextsent>guage every day.
</nextsent>
<nextsent>on an average, 260 and 452 new nes appeared daily in the xie and afe segments of the ldc english gigaword corpora respectively.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I296">
<title id=" E09-1091.xml">mint a method for effective and scalable mining of named entity transliterations from large comparable corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>ing multilingual named entity transliteration lexicons.
</prevsent>
<prevsent>the ubiquitous availability of comparable news corpora in multiple languages suggests promising alternative to machine transliteration, namely, the mining of named entity transliteration equivalents (netes) from such corpora.
</prevsent>
</prevsection>
<citsent citstr=" P06-1103 ">
news stories are typically rich innes and therefore, comparable news corpora can be expected to contain netes (klementiev and roth, 2006; <papid> P06-1103 </papid>tao et al, 2006).<papid> W06-1630 </papid></citsent>
<aftsection>
<nextsent>the large quantity and the perpetual availability of news corpora in many of the worlds languages, make mining of netes viable alternative to traditional approaches.
</nextsent>
<nextsent>it is this opportunity that we address in our work.
</nextsent>
<nextsent>in this paper, we detail an effective and scalable mining method, called mint (mining named-entity transliteration equivalents), for mining of netes from large comparable corpora.
</nextsent>
<nextsent>mint addresses several challenges in mining netes from large comparable corpora: exhaustive ness (in mining sparse netes), computational efficiency (in scaling on corpora size), language independence (in being applicable to many language pairs) and linguistic frugality (in requiring minimal external linguistic resources).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I297">
<title id=" E09-1091.xml">mint a method for effective and scalable mining of named entity transliterations from large comparable corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>ing multilingual named entity transliteration lexicons.
</prevsent>
<prevsent>the ubiquitous availability of comparable news corpora in multiple languages suggests promising alternative to machine transliteration, namely, the mining of named entity transliteration equivalents (netes) from such corpora.
</prevsent>
</prevsection>
<citsent citstr=" W06-1630 ">
news stories are typically rich innes and therefore, comparable news corpora can be expected to contain netes (klementiev and roth, 2006; <papid> P06-1103 </papid>tao et al, 2006).<papid> W06-1630 </papid></citsent>
<aftsection>
<nextsent>the large quantity and the perpetual availability of news corpora in many of the worlds languages, make mining of netes viable alternative to traditional approaches.
</nextsent>
<nextsent>it is this opportunity that we address in our work.
</nextsent>
<nextsent>in this paper, we detail an effective and scalable mining method, called mint (mining named-entity transliteration equivalents), for mining of netes from large comparable corpora.
</nextsent>
<nextsent>mint addresses several challenges in mining netes from large comparable corpora: exhaustive ness (in mining sparse netes), computational efficiency (in scaling on corpora size), language independence (in being applicable to many language pairs) and linguistic frugality (in requiring minimal external linguistic resources).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I300">
<title id=" E09-1091.xml">mint a method for effective and scalable mining of named entity transliterations from large comparable corpora </title>
<section> the mint mining method.  </section>
<citcontext>
<prevsection>
<prevsent>here ? 2,1?
</prevsent>
<prevsent>. ? difference in the lengths of the two strings.
</prevsent>
</prevsection>
<citsent citstr=" W07-0711 ">
generative transliteration similarity model: we also experimented with an extension of hes w-hmm model (he, 2007).<papid> W07-0711 </papid></citsent>
<aftsection>
<nextsent>the transition probability depends on both the jump width and the previous source character as in the w-hmm model.
</nextsent>
<nextsent>the emission probability depends on the current source character and the previous target character unlike the w-hmm model (udupa et al., 2009).
</nextsent>
<nextsent>instead of using any single alignment of characters in the pair (ws, wt), we marginalize over all possible alignments: ? ?
</nextsent>
<nextsent>11 1 11 ,|,|| 1 ??
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I301">
<title id=" E09-1091.xml">mint a method for effective and scalable mining of named entity transliterations from large comparable corpora </title>
<section> experimental setup.  </section>
<citcontext>
<prevsection>
<prevsent>to produce the negative samples, we paired each source language ne with random non matching target language ne.
</prevsent>
<prevsent>no language specific features were used and the same feature set was used in each of the 4 language pairs making mint language neutral.
</prevsent>
</prevsection>
<citsent citstr=" P05-1045 ">
in all the experiments, our source side language is english, and the stanford named entity recognizer (finkel et al 2005) <papid> P05-1045 </papid>was used to extract nes from the source side article.</citsent>
<aftsection>
<nextsent>it should be noted here that while the precision of the ner 803 used was consistently high, its recall was low, (~40%) especially in the new indian express corpus, perhaps due to the differences in the data used for training the ner and the data on which we used it.
</nextsent>
<nextsent>4.4 performance measures.
</nextsent>
<nextsent>our intention is to measure the effectiveness of mint by comparing its performance with the oracular (human annotator) performance.
</nextsent>
<nextsent>as transliteration equivalents must exist in the paired articles to be found by mint, we focus only on those nes that actually have at least one transliteration equivalent in the conjugate article.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I304">
<title id=" E09-1091.xml">mint a method for effective and scalable mining of named entity transliterations from large comparable corpora </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>clir systems have been studied in several works (ballesteros and croft, 1998; kraiij et al 2003).
</prevsent>
<prevsent>the limited coverage of dictionaries has been recognized as problem in clir and mt (demner-fushman &amp; oard, 2002; mandl &amp; womser-hacker, 2005; xu &weischedel;, 2005).
</prevsent>
</prevsection>
<citsent citstr=" W95-0114 ">
in order to address this problem, different kinds of approaches have been taken, from learning transformation rules from dictionaries and applying the rules to find cross-lingual spelling variants (pirkola et al, 2003), to learning translation lexicon from monolingual and/or comparable corpora (fung, 1995; <papid> W95-0114 </papid>al-onaizan and knight, 2002; koehn and knight, 2002; <papid> W02-0902 </papid>rapp, 1996).</citsent>
<aftsection>
<nextsent>while these works have focused on finding translation equivalents of all class of words, we focus specifically on transliteration equivalents of nes.
</nextsent>
<nextsent>(munteanu and marcu, 2006; <papid> P06-1011 </papid>quirk et al, 2007) addresses mining of parallel sentences and fragments from nearly parallel sentences.</nextsent>
<nextsent>in contrast, our approach mines netes from article pairs that may not even have any parallel or nearly parallel sentences.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I306">
<title id=" E09-1091.xml">mint a method for effective and scalable mining of named entity transliterations from large comparable corpora </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>clir systems have been studied in several works (ballesteros and croft, 1998; kraiij et al 2003).
</prevsent>
<prevsent>the limited coverage of dictionaries has been recognized as problem in clir and mt (demner-fushman &amp; oard, 2002; mandl &amp; womser-hacker, 2005; xu &weischedel;, 2005).
</prevsent>
</prevsection>
<citsent citstr=" W02-0902 ">
in order to address this problem, different kinds of approaches have been taken, from learning transformation rules from dictionaries and applying the rules to find cross-lingual spelling variants (pirkola et al, 2003), to learning translation lexicon from monolingual and/or comparable corpora (fung, 1995; <papid> W95-0114 </papid>al-onaizan and knight, 2002; koehn and knight, 2002; <papid> W02-0902 </papid>rapp, 1996).</citsent>
<aftsection>
<nextsent>while these works have focused on finding translation equivalents of all class of words, we focus specifically on transliteration equivalents of nes.
</nextsent>
<nextsent>(munteanu and marcu, 2006; <papid> P06-1011 </papid>quirk et al, 2007) addresses mining of parallel sentences and fragments from nearly parallel sentences.</nextsent>
<nextsent>in contrast, our approach mines netes from article pairs that may not even have any parallel or nearly parallel sentences.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I307">
<title id=" E09-1091.xml">mint a method for effective and scalable mining of named entity transliterations from large comparable corpora </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in order to address this problem, different kinds of approaches have been taken, from learning transformation rules from dictionaries and applying the rules to find cross-lingual spelling variants (pirkola et al, 2003), to learning translation lexicon from monolingual and/or comparable corpora (fung, 1995; <papid> W95-0114 </papid>al-onaizan and knight, 2002; koehn and knight, 2002; <papid> W02-0902 </papid>rapp, 1996).</prevsent>
<prevsent>while these works have focused on finding translation equivalents of all class of words, we focus specifically on transliteration equivalents of nes.</prevsent>
</prevsection>
<citsent citstr=" P06-1011 ">
(munteanu and marcu, 2006; <papid> P06-1011 </papid>quirk et al, 2007) addresses mining of parallel sentences and fragments from nearly parallel sentences.</citsent>
<aftsection>
<nextsent>in contrast, our approach mines netes from article pairs that may not even have any parallel or nearly parallel sentences.
</nextsent>
<nextsent>nete discovery from comparable corpora using time series and transliteration model was proposed in (klementiev and roth, 2006), <papid> P06-1103 </papid>and extended for nete mining for several languages in (saravanan and kumaran, 2007).</nextsent>
<nextsent>however, such methods miss vast majority of the netes due to their dependency on frequency signatures.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I311">
<title id=" E09-1013.xml">bayesian word sense induction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for instance, besides lexical information we maywant to consider parts of speech or dependencies in our sense induction problem.
</prevsent>
<prevsent>this is in marked contrast with previous lda-based models which mostly take only word-based information into account.
</prevsent>
</prevsection>
<citsent citstr=" W07-2002 ">
we evaluate our model on recently released benchmark dataset (agirre and soroa, 2007) <papid> W07-2002 </papid>and demonstrate improvements over the state-of-the-art.the remainder of this paper is structured as fol lows.</citsent>
<aftsection>
<nextsent>we first present an overview of related work (section 2) and then describe our bayesian modelin more detail (sections 3 and 4).
</nextsent>
<nextsent>section 5 describes the resources and evaluation methodology used in our experiments.
</nextsent>
<nextsent>we discuss our results in section 6, and conclude in section 7.
</nextsent>
<nextsent>sense induction is typically treated as clustering problem, where instances of target word are partitioned into classes by considering their co-occurring contexts.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I312">
<title id=" E09-1013.xml">bayesian word sense induction </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>considerable latitude is allowed in selecting and representing the cooccurring contexts.
</prevsent>
<prevsent>previous methods have used first or second order co-occurrences (purandare and pedersen, 2004; schutze, 1998), parts of speech (purandare and pedersen, 2004), and grammatical relations (pantel and lin, 2002; doro wand widdows, 2003).
</prevsent>
</prevsection>
<citsent citstr=" E06-1018 ">
the size of the context window also varies, it can be relatively small, such as two words before and after the target word (gauch and futrelle, 1993), the sentence within which the target is found (bordag, 2006), <papid> E06-1018 </papid>or even larger, such as the 20 surrounding words on either side of the target (purandare and pedersen, 2004).</citsent>
<aftsection>
<nextsent>in essence, each instance of target wordis represented as feature vector which subsequently serves as input to the chosen clustering method.
</nextsent>
<nextsent>a variety of clustering algorithms have been employed ranging from k-means (purandare and pedersen, 2004), to agglomerative clustering (schutze, 1998), and the information bottleneck (niu et al , 2007).
</nextsent>
<nextsent>graph-based methods have also been applied to the sense induction task.
</nextsent>
<nextsent>in this framework words are represented as nodes in the graph and vertices are drawn between the target and its co-occurrences.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I315">
<title id=" E09-1037.xml">cube summing approximate inference with non local features and dynamic programming without semi rings </title>
<section> abstract </section>
<citcontext>
<prevsection>
<prevsent>we introduce cube summing, technique that permits dynamic programming algorithms for summing over structures (like the forward and inside algorithms) to be extended with non-local features that violate the classical structural independenceassumptions.
</prevsent>
<prevsent>it is inspired by cube pruning (chiang, 2007; huang and chiang, 2007) in its computation of non-local features dynamically using scored k-bestlists, but also maintains additional residual quantities used in calculating approximate marginals.
</prevsent>
</prevsection>
<citsent citstr=" J99-4004 ">
when restricted to local features, cube summing reduces to anovel semi ring (k-best+residual) that generalizes many of the semi rings of good man (1999).<papid> J99-4004 </papid></citsent>
<aftsection>
<nextsent>when non-local features are included, cube summing does not reduce to any semi ring, but is compatible with generic techniques for solving dynamic programming equations.
</nextsent>
<nextsent>probabilistic nlp researchers frequently make independence assumptions to keep inference algorithms tractable.
</nextsent>
<nextsent>doing so limits the features that are available to our models, requiring features to be structurally local.
</nextsent>
<nextsent>yet many problems in nlp machine translation, parsing, named-entity recognition, and other shave benefited from the addition of non-local features that break classical independence assumptions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I316">
<title id=" E09-1037.xml">cube summing approximate inference with non local features and dynamic programming without semi rings </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>recently cube pruning (chiang, 2007; huang and chiang, 2007) was proposed as way to leverage existing dynamic programming algorithms that find optimal-scoring derivations or structures when only local features are involved.
</prevsent>
<prevsent>cube pruning permits approximate decoding with non-local features, but leaves open the question of how the feature weights or probabilities are learned.
</prevsent>
</prevsection>
<citsent citstr=" P92-1017 ">
mean while, some learning algorithms, like maximum likelihood for conditional log-linear models (laf ferty et al , 2001), unsupervised models (pereiraand schabes, 1992), <papid> P92-1017 </papid>and models with hidden variables (koo and collins, 2005; <papid> H05-1064 </papid>wang et al , 2007; <papid> D07-1003 </papid>blunsom et al , 2008), <papid> P08-1024 </papid>require summing over the scores of many structures to calculate marginals.</citsent>
<aftsection>
<nextsent>we first review the semiring-weighted logic programming view of dynamic programming algorithms (shieber et al , 1995) and identify an intuitive property of program called proof locality that follows from feature locality in the underlying probability model (2).
</nextsent>
<nextsent>we then provide an analysis of cube pruning as an approximation to the intractable problem of exact optimization over structures with non-local features and show how the use of non-local features with k-best lists breaks certain semi ring properties (3).
</nextsent>
<nextsent>the primary contribution of this paper is novel technique?
</nextsent>
<nextsent>cube summing for approximate summing over discrete structures with non-local features, whichwe relate to cube pruning (4).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I317">
<title id=" E09-1037.xml">cube summing approximate inference with non local features and dynamic programming without semi rings </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>recently cube pruning (chiang, 2007; huang and chiang, 2007) was proposed as way to leverage existing dynamic programming algorithms that find optimal-scoring derivations or structures when only local features are involved.
</prevsent>
<prevsent>cube pruning permits approximate decoding with non-local features, but leaves open the question of how the feature weights or probabilities are learned.
</prevsent>
</prevsection>
<citsent citstr=" H05-1064 ">
mean while, some learning algorithms, like maximum likelihood for conditional log-linear models (laf ferty et al , 2001), unsupervised models (pereiraand schabes, 1992), <papid> P92-1017 </papid>and models with hidden variables (koo and collins, 2005; <papid> H05-1064 </papid>wang et al , 2007; <papid> D07-1003 </papid>blunsom et al , 2008), <papid> P08-1024 </papid>require summing over the scores of many structures to calculate marginals.</citsent>
<aftsection>
<nextsent>we first review the semiring-weighted logic programming view of dynamic programming algorithms (shieber et al , 1995) and identify an intuitive property of program called proof locality that follows from feature locality in the underlying probability model (2).
</nextsent>
<nextsent>we then provide an analysis of cube pruning as an approximation to the intractable problem of exact optimization over structures with non-local features and show how the use of non-local features with k-best lists breaks certain semi ring properties (3).
</nextsent>
<nextsent>the primary contribution of this paper is novel technique?
</nextsent>
<nextsent>cube summing for approximate summing over discrete structures with non-local features, whichwe relate to cube pruning (4).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I318">
<title id=" E09-1037.xml">cube summing approximate inference with non local features and dynamic programming without semi rings </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>recently cube pruning (chiang, 2007; huang and chiang, 2007) was proposed as way to leverage existing dynamic programming algorithms that find optimal-scoring derivations or structures when only local features are involved.
</prevsent>
<prevsent>cube pruning permits approximate decoding with non-local features, but leaves open the question of how the feature weights or probabilities are learned.
</prevsent>
</prevsection>
<citsent citstr=" D07-1003 ">
mean while, some learning algorithms, like maximum likelihood for conditional log-linear models (laf ferty et al , 2001), unsupervised models (pereiraand schabes, 1992), <papid> P92-1017 </papid>and models with hidden variables (koo and collins, 2005; <papid> H05-1064 </papid>wang et al , 2007; <papid> D07-1003 </papid>blunsom et al , 2008), <papid> P08-1024 </papid>require summing over the scores of many structures to calculate marginals.</citsent>
<aftsection>
<nextsent>we first review the semiring-weighted logic programming view of dynamic programming algorithms (shieber et al , 1995) and identify an intuitive property of program called proof locality that follows from feature locality in the underlying probability model (2).
</nextsent>
<nextsent>we then provide an analysis of cube pruning as an approximation to the intractable problem of exact optimization over structures with non-local features and show how the use of non-local features with k-best lists breaks certain semi ring properties (3).
</nextsent>
<nextsent>the primary contribution of this paper is novel technique?
</nextsent>
<nextsent>cube summing for approximate summing over discrete structures with non-local features, whichwe relate to cube pruning (4).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I319">
<title id=" E09-1037.xml">cube summing approximate inference with non local features and dynamic programming without semi rings </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>recently cube pruning (chiang, 2007; huang and chiang, 2007) was proposed as way to leverage existing dynamic programming algorithms that find optimal-scoring derivations or structures when only local features are involved.
</prevsent>
<prevsent>cube pruning permits approximate decoding with non-local features, but leaves open the question of how the feature weights or probabilities are learned.
</prevsent>
</prevsection>
<citsent citstr=" P08-1024 ">
mean while, some learning algorithms, like maximum likelihood for conditional log-linear models (laf ferty et al , 2001), unsupervised models (pereiraand schabes, 1992), <papid> P92-1017 </papid>and models with hidden variables (koo and collins, 2005; <papid> H05-1064 </papid>wang et al , 2007; <papid> D07-1003 </papid>blunsom et al , 2008), <papid> P08-1024 </papid>require summing over the scores of many structures to calculate marginals.</citsent>
<aftsection>
<nextsent>we first review the semiring-weighted logic programming view of dynamic programming algorithms (shieber et al , 1995) and identify an intuitive property of program called proof locality that follows from feature locality in the underlying probability model (2).
</nextsent>
<nextsent>we then provide an analysis of cube pruning as an approximation to the intractable problem of exact optimization over structures with non-local features and show how the use of non-local features with k-best lists breaks certain semi ring properties (3).
</nextsent>
<nextsent>the primary contribution of this paper is novel technique?
</nextsent>
<nextsent>cube summing for approximate summing over discrete structures with non-local features, whichwe relate to cube pruning (4).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I322">
<title id=" E09-1037.xml">cube summing approximate inference with non local features and dynamic programming without semi rings </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>for example, in goodmans framework, the forward algorithm and the viterbi algorithm are comprised of the same logic program with different semirings.
</prevsent>
<prevsent>goodman defined other semi rings, including ones we will use here.
</prevsent>
</prevsection>
<citsent citstr=" H05-1036 ">
this formal framework was the basis for the dyna programming language, which permits declarative specification of the logic program and compiles it into an efficient, agenda based, bottom-up procedure (eisner et al , 2005).<papid> H05-1036 </papid>for our purposes, dp consists of set of recursive equations over set of indexed variables.</citsent>
<aftsection>
<nextsent>for example, the probabilistic cky algorithm (run on sentence w1w2...wn) is written as cx,i1,i = pxwi (1) cx,i,k = max y,zn;j?{i+1,...,k1} pxy ? cy,i,j ? cz,j,k goal = cs,0,n where is the nonterminal set and ? is the start symbol.
</nextsent>
<nextsent>each cx,i,j variable corresponds tothe chart value (probability of the most likely sub tree) of an x-constituent spanning the substringwi+1...wj . goal is special variable of greatest interest, though solving for goal correctly may (in general, but not in this example) require solving for all the other values.
</nextsent>
<nextsent>we will use the term in dex?
</nextsent>
<nextsent>to refer to the sub script values on variables (x, i, on cx,i,j).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I327">
<title id=" E09-1037.xml">cube summing approximate inference with non local features and dynamic programming without semi rings </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>in general, it leads to exponential-time inference in the worst case.
</prevsent>
<prevsent>there have been many algorithms proposed for approximately solving instances of these decoding and summing problems with non-local features.
</prevsent>
</prevsection>
<citsent citstr=" D08-1016 ">
some stem from work on graphical models, including loopy belief propagation (sutton and mccallum, 2004; smith and eisner, 2008), <papid> D08-1016 </papid>gibbs sampling (finkel et al , 2005), <papid> P05-1045 </papid>sequential monte carlo methods such as particle filtering (levy et al ., 2008), and variational inference (jordan et al , 1999; mackay, 1997; kurihara and sato, 2006).</citsent>
<aftsection>
<nextsent>also relevant are stacked learning (cohen and carvalho, 2005), interpret able as approximation of non-local feature values (martins et al , 2008),and m-estimation (smith et al , 2007), <papid> P07-1095 </papid>which allows training without inference.</nextsent>
<nextsent>several other approaches used frequently in nlp are approximate methods for decoding only.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I328">
<title id=" E09-1037.xml">cube summing approximate inference with non local features and dynamic programming without semi rings </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>in general, it leads to exponential-time inference in the worst case.
</prevsent>
<prevsent>there have been many algorithms proposed for approximately solving instances of these decoding and summing problems with non-local features.
</prevsent>
</prevsection>
<citsent citstr=" P05-1045 ">
some stem from work on graphical models, including loopy belief propagation (sutton and mccallum, 2004; smith and eisner, 2008), <papid> D08-1016 </papid>gibbs sampling (finkel et al , 2005), <papid> P05-1045 </papid>sequential monte carlo methods such as particle filtering (levy et al ., 2008), and variational inference (jordan et al , 1999; mackay, 1997; kurihara and sato, 2006).</citsent>
<aftsection>
<nextsent>also relevant are stacked learning (cohen and carvalho, 2005), interpret able as approximation of non-local feature values (martins et al , 2008),and m-estimation (smith et al , 2007), <papid> P07-1095 </papid>which allows training without inference.</nextsent>
<nextsent>several other approaches used frequently in nlp are approximate methods for decoding only.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I329">
<title id=" E09-1037.xml">cube summing approximate inference with non local features and dynamic programming without semi rings </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>there have been many algorithms proposed for approximately solving instances of these decoding and summing problems with non-local features.
</prevsent>
<prevsent>some stem from work on graphical models, including loopy belief propagation (sutton and mccallum, 2004; smith and eisner, 2008), <papid> D08-1016 </papid>gibbs sampling (finkel et al , 2005), <papid> P05-1045 </papid>sequential monte carlo methods such as particle filtering (levy et al ., 2008), and variational inference (jordan et al , 1999; mackay, 1997; kurihara and sato, 2006).</prevsent>
</prevsection>
<citsent citstr=" P07-1095 ">
also relevant are stacked learning (cohen and carvalho, 2005), interpret able as approximation of non-local feature values (martins et al , 2008),and m-estimation (smith et al , 2007), <papid> P07-1095 </papid>which allows training without inference.</citsent>
<aftsection>
<nextsent>several other approaches used frequently in nlp are approximate methods for decoding only.
</nextsent>
<nextsent>these include beam search (lowerre, 1976), cube pruning, which we discuss in 3, integer linear programming (roth and yih, 2004), <papid> W04-2401 </papid>in which arbitrary features can act as constraints on y, and approximate solutions like mcdonald and pereira (2006), <papid> E06-1011 </papid>in which an exact solution to related decoding problem is found and then modified to fit the problem of interest.</nextsent>
<nextsent>cube pruning (chiang, 2007; huang and chiang, 2007) is an approximate technique for decoding (eq.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I330">
<title id=" E09-1037.xml">cube summing approximate inference with non local features and dynamic programming without semi rings </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>also relevant are stacked learning (cohen and carvalho, 2005), interpret able as approximation of non-local feature values (martins et al , 2008),and m-estimation (smith et al , 2007), <papid> P07-1095 </papid>which allows training without inference.</prevsent>
<prevsent>several other approaches used frequently in nlp are approximate methods for decoding only.</prevsent>
</prevsection>
<citsent citstr=" W04-2401 ">
these include beam search (lowerre, 1976), cube pruning, which we discuss in 3, integer linear programming (roth and yih, 2004), <papid> W04-2401 </papid>in which arbitrary features can act as constraints on y, and approximate solutions like mcdonald and pereira (2006), <papid> E06-1011 </papid>in which an exact solution to related decoding problem is found and then modified to fit the problem of interest.</citsent>
<aftsection>
<nextsent>cube pruning (chiang, 2007; huang and chiang, 2007) is an approximate technique for decoding (eq.
</nextsent>
<nextsent>4); it is used widely in machine translation.
</nextsent>
<nextsent>given proof locality, it is essentially an efficient implementation of the k-best proof semiring.cube pruning goes farther in that it permits non local features to weigh in on the proof probabilities, at the expense of making the k-best operation approximate.
</nextsent>
<nextsent>we describe the two approximations cube pruning makes, then propose cube decoding, which removes the second approximation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I331">
<title id=" E09-1037.xml">cube summing approximate inference with non local features and dynamic programming without semi rings </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>also relevant are stacked learning (cohen and carvalho, 2005), interpret able as approximation of non-local feature values (martins et al , 2008),and m-estimation (smith et al , 2007), <papid> P07-1095 </papid>which allows training without inference.</prevsent>
<prevsent>several other approaches used frequently in nlp are approximate methods for decoding only.</prevsent>
</prevsection>
<citsent citstr=" E06-1011 ">
these include beam search (lowerre, 1976), cube pruning, which we discuss in 3, integer linear programming (roth and yih, 2004), <papid> W04-2401 </papid>in which arbitrary features can act as constraints on y, and approximate solutions like mcdonald and pereira (2006), <papid> E06-1011 </papid>in which an exact solution to related decoding problem is found and then modified to fit the problem of interest.</citsent>
<aftsection>
<nextsent>cube pruning (chiang, 2007; huang and chiang, 2007) is an approximate technique for decoding (eq.
</nextsent>
<nextsent>4); it is used widely in machine translation.
</nextsent>
<nextsent>given proof locality, it is essentially an efficient implementation of the k-best proof semiring.cube pruning goes farther in that it permits non local features to weigh in on the proof probabilities, at the expense of making the k-best operation approximate.
</nextsent>
<nextsent>we describe the two approximations cube pruning makes, then propose cube decoding, which removes the second approximation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I332">
<title id=" E09-1037.xml">cube summing approximate inference with non local features and dynamic programming without semi rings </title>
<section> approximate decoding.  </section>
<citcontext>
<prevsection>
<prevsent>here, max-k is simply used to re-sort the k-best proof list following function evaluation.the semi ring properties fail to hold when introducing non-local features in this way.
</prevsent>
<prevsent>in particular, cd is not associative when 1     ?.for example, consider the probabilistic cky algorithm as above, but using the cube decoding semi ring with the non-local feature functions collectively known as ngramtree?
</prevsent>
</prevsection>
<citsent citstr=" P08-1067 ">
features (huang,2008) <papid> P08-1067 </papid>that score the string of terminals and nonterminals along the path from word to word + 1when two constituents cy,i,j and cz,j,k are com bined.</citsent>
<aftsection>
<nextsent>the semi ring value associated with such feature is = ???,ngramtreepi(), 1?
</nextsent>
<nextsent>(for aspecific path pi), and we rewrite eq.
</nextsent>
<nextsent>1 as follows (where ranges for summation are omitted for space): cx,i,k = ? cd pxy cdcy,i,j cdcz,j,kcdu the combination operator is not associative since the following will give different answers:5 (pxy cd cy,i,j)cd (cz,j,k cd u) (10) ((pxy cd cy,i,j)cd cz,j,k)cd (11)in eq.
</nextsent>
<nextsent>10, the non-local feature function is executed on the k-best proof list for z, while in eq.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I333">
<title id=" E09-1037.xml">cube summing approximate inference with non local features and dynamic programming without semi rings </title>
<section> cube summing.  </section>
<citcontext>
<prevsection>
<prevsent>the aggregation operator over ope rands {ui}ni=1, all such that uis = 0, 8 is defined by: i=1 ui = (12)n i=1 ui0 + ? ?
</prevsent>
<prevsent>res (n i=1 ui )?
</prevsent>
</prevsection>
<citsent citstr=" D08-1023 ">
, max-k (n i=1 ui ) , g0, 0 6algebraic structures are typically defined with binary operators only, so we were unable to find suitable term for this structure in the literature.7blunsom and osborne (2008) <papid> D08-1023 </papid>described related approach to approximate summing using the chart computed during cube pruning, but did not keep track of the residual terms as we do here.</citsent>
<aftsection>
<nextsent>8we assume that ope rands ui to cs will never be such that uis = 1 (non-local feature functions).
</nextsent>
<nextsent>this is reasonable in the widely used log-linear model setting we have adopted, where weights are factors in proofs product score.
</nextsent>
<nextsent>where res returns the residual?
</nextsent>
<nextsent>set of scored proofs not in the k-best among its arguments, possibly the empty set.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I340">
<title id=" E09-1037.xml">cube summing approximate inference with non local features and dynamic programming without semi rings </title>
<section> implementation.  </section>
<citcontext>
<prevsection>
<prevsent>importantly, this permit sus to calculate the exact gradient of the approximate summation with respect to axiom values, following eisner et al  (2005)<papid> H05-1036 </papid></prevsent>
<prevsent>this is desirable when carrying out the optimization problems involved in parameter estimation.</prevsent>
</prevsection>
<citsent citstr=" P02-1001 ">
another differentiation technique, implemented within the semir ing, is given by eisner (2002).<papid> P02-1001 </papid></citsent>
<aftsection>
<nextsent>cube pruning is based on the k-best algorithms of huang and chiang (2005), <papid> W05-1506 </papid>which save time over generic semi ring implementations through lazy computation in both the aggregation and combination operations.</nextsent>
<nextsent>their techniques are not as clearly applicable here, because our goal is to sumover all proofs instead of only finding small subset of them.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I341">
<title id=" E09-1037.xml">cube summing approximate inference with non local features and dynamic programming without semi rings </title>
<section> implementation.  </section>
<citcontext>
<prevsection>
<prevsent>this is desirable when carrying out the optimization problems involved in parameter estimation.
</prevsent>
<prevsent>another differentiation technique, implemented within the semir ing, is given by eisner (2002).<papid> P02-1001 </papid></prevsent>
</prevsection>
<citsent citstr=" W05-1506 ">
cube pruning is based on the k-best algorithms of huang and chiang (2005), <papid> W05-1506 </papid>which save time over generic semi ring implementations through lazy computation in both the aggregation and combination operations.</citsent>
<aftsection>
<nextsent>their techniques are not as clearly applicable here, because our goal is to sumover all proofs instead of only finding small subset of them.
</nextsent>
<nextsent>if computing non-local features is computational bottleneck, they can be computed only for the o(k) proofs considered when choosing the best as in cube pruning.
</nextsent>
<nextsent>then, the computational requirements for approximate summing are nearly equivalent to cube pruning, but the approximation is less accurate.
</nextsent>
<nextsent>we now consider interesting special cases and variations of cube summing.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I351">
<title id=" E06-2007.xml">selecting the right number of senses based on clustering criterion functions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>it is based on the use of global criterion functions that assess the quality of clustering solution.
</prevsent>
<prevsent>the goal of word sense discrimination is to cluster the occurrences of word in context based on its underlying meaning.
</prevsent>
</prevsection>
<citsent citstr=" W97-0322 ">
this is often approached as problem in unsupervised learning, where the only information available is large corpus of text (e.g.,(pedersen and bruce, 1997), (<papid> W97-0322 </papid>schutze, 1998), (purandare and pedersen, 2004)).<papid> W04-2406 </papid></citsent>
<aftsection>
<nextsent>these methods usually require that the number of clusters to be discovered (k) be specified ahead of time.
</nextsent>
<nextsent>however, in most realistic settings, the value of is unknown to the user.
</nextsent>
<nextsent>word sense discrimination seeks to cluster ncontexts, each of which contain particular target word, into clusters, where we would like the value of to be automatically selected.
</nextsent>
<nextsent>each context consists of approximately paragraph of surrounding text, where the word to be discriminated (the target word) is found approximately inthe middle of the context.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I352">
<title id=" E06-2007.xml">selecting the right number of senses based on clustering criterion functions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>it is based on the use of global criterion functions that assess the quality of clustering solution.
</prevsent>
<prevsent>the goal of word sense discrimination is to cluster the occurrences of word in context based on its underlying meaning.
</prevsent>
</prevsection>
<citsent citstr=" W04-2406 ">
this is often approached as problem in unsupervised learning, where the only information available is large corpus of text (e.g.,(pedersen and bruce, 1997), (<papid> W97-0322 </papid>schutze, 1998), (purandare and pedersen, 2004)).<papid> W04-2406 </papid></citsent>
<aftsection>
<nextsent>these methods usually require that the number of clusters to be discovered (k) be specified ahead of time.
</nextsent>
<nextsent>however, in most realistic settings, the value of is unknown to the user.
</nextsent>
<nextsent>word sense discrimination seeks to cluster ncontexts, each of which contain particular target word, into clusters, where we would like the value of to be automatically selected.
</nextsent>
<nextsent>each context consists of approximately paragraph of surrounding text, where the word to be discriminated (the target word) is found approximately inthe middle of the context.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I354">
<title id=" E09-1090.xml">fast full parsing by linear chain conditional random fields </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the parsing is performed in bottom-up manner and the best derivation is efficiently obtained by using depth first search algorithm.
</prevsent>
<prevsent>experimental results demonstrate that this simple parsing framework produces fast and reasonably accurate parser.
</prevsent>
</prevsection>
<citsent citstr=" P06-1006 ">
full parsing analyzes the phrase structure of sentence and provides useful input for many kinds of high-level natural language processing such as summarization (knight and marcu, 2000), pronoun resolution (yang et al, 2006), <papid> P06-1006 </papid>and information extraction (miyao et al, 2008).<papid> P08-1006 </papid></citsent>
<aftsection>
<nextsent>one of the major obstacles that discourage the use of full parsing in large-scale natural language processing applications is its computational cost.
</nextsent>
<nextsent>for example, the medline corpus, collection of abstracts of biomedical papers, consists of 70 million sentences and would require more than two years of processing time if the parser needs one second to process sentence.
</nextsent>
<nextsent>generative models based on lexicalized pcfgs enjoyed great success as the machine learning framework for full parsing (collins, 1999; charniak, 2000), <papid> A00-2018 </papid>but recently discriminative models attract more attention due to their superior accuracy (charniak and johnson, 2005; <papid> P05-1022 </papid>huang, 2008) <papid> P08-1067 </papid>and adaptability to new grammars and languages (buchholz and marsi, 2006).<papid> W06-2920 </papid></nextsent>
<nextsent>a traditional approach to discriminative full parsing is to convert full parsing task into series of classification problems.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I355">
<title id=" E09-1090.xml">fast full parsing by linear chain conditional random fields </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the parsing is performed in bottom-up manner and the best derivation is efficiently obtained by using depth first search algorithm.
</prevsent>
<prevsent>experimental results demonstrate that this simple parsing framework produces fast and reasonably accurate parser.
</prevsent>
</prevsection>
<citsent citstr=" P08-1006 ">
full parsing analyzes the phrase structure of sentence and provides useful input for many kinds of high-level natural language processing such as summarization (knight and marcu, 2000), pronoun resolution (yang et al, 2006), <papid> P06-1006 </papid>and information extraction (miyao et al, 2008).<papid> P08-1006 </papid></citsent>
<aftsection>
<nextsent>one of the major obstacles that discourage the use of full parsing in large-scale natural language processing applications is its computational cost.
</nextsent>
<nextsent>for example, the medline corpus, collection of abstracts of biomedical papers, consists of 70 million sentences and would require more than two years of processing time if the parser needs one second to process sentence.
</nextsent>
<nextsent>generative models based on lexicalized pcfgs enjoyed great success as the machine learning framework for full parsing (collins, 1999; charniak, 2000), <papid> A00-2018 </papid>but recently discriminative models attract more attention due to their superior accuracy (charniak and johnson, 2005; <papid> P05-1022 </papid>huang, 2008) <papid> P08-1067 </papid>and adaptability to new grammars and languages (buchholz and marsi, 2006).<papid> W06-2920 </papid></nextsent>
<nextsent>a traditional approach to discriminative full parsing is to convert full parsing task into series of classification problems.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I356">
<title id=" E09-1090.xml">fast full parsing by linear chain conditional random fields </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>one of the major obstacles that discourage the use of full parsing in large-scale natural language processing applications is its computational cost.
</prevsent>
<prevsent>for example, the medline corpus, collection of abstracts of biomedical papers, consists of 70 million sentences and would require more than two years of processing time if the parser needs one second to process sentence.
</prevsent>
</prevsection>
<citsent citstr=" A00-2018 ">
generative models based on lexicalized pcfgs enjoyed great success as the machine learning framework for full parsing (collins, 1999; charniak, 2000), <papid> A00-2018 </papid>but recently discriminative models attract more attention due to their superior accuracy (charniak and johnson, 2005; <papid> P05-1022 </papid>huang, 2008) <papid> P08-1067 </papid>and adaptability to new grammars and languages (buchholz and marsi, 2006).<papid> W06-2920 </papid></citsent>
<aftsection>
<nextsent>a traditional approach to discriminative full parsing is to convert full parsing task into series of classification problems.
</nextsent>
<nextsent>ratnaparkhi (1997)<papid> W97-0301 </papid>performs full parsing in bottom-up and left-toright manner and uses maximum entropy classifier to make decisions to construct individualphrases.</nextsent>
<nextsent>sagae and lavie (2006) <papid> P06-2089 </papid>use the shift reduce parsing framework and maximum entropy model for local classification to decide parsing actions.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I357">
<title id=" E09-1090.xml">fast full parsing by linear chain conditional random fields </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>one of the major obstacles that discourage the use of full parsing in large-scale natural language processing applications is its computational cost.
</prevsent>
<prevsent>for example, the medline corpus, collection of abstracts of biomedical papers, consists of 70 million sentences and would require more than two years of processing time if the parser needs one second to process sentence.
</prevsent>
</prevsection>
<citsent citstr=" P05-1022 ">
generative models based on lexicalized pcfgs enjoyed great success as the machine learning framework for full parsing (collins, 1999; charniak, 2000), <papid> A00-2018 </papid>but recently discriminative models attract more attention due to their superior accuracy (charniak and johnson, 2005; <papid> P05-1022 </papid>huang, 2008) <papid> P08-1067 </papid>and adaptability to new grammars and languages (buchholz and marsi, 2006).<papid> W06-2920 </papid></citsent>
<aftsection>
<nextsent>a traditional approach to discriminative full parsing is to convert full parsing task into series of classification problems.
</nextsent>
<nextsent>ratnaparkhi (1997)<papid> W97-0301 </papid>performs full parsing in bottom-up and left-toright manner and uses maximum entropy classifier to make decisions to construct individualphrases.</nextsent>
<nextsent>sagae and lavie (2006) <papid> P06-2089 </papid>use the shift reduce parsing framework and maximum entropy model for local classification to decide parsing actions.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I358">
<title id=" E09-1090.xml">fast full parsing by linear chain conditional random fields </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>one of the major obstacles that discourage the use of full parsing in large-scale natural language processing applications is its computational cost.
</prevsent>
<prevsent>for example, the medline corpus, collection of abstracts of biomedical papers, consists of 70 million sentences and would require more than two years of processing time if the parser needs one second to process sentence.
</prevsent>
</prevsection>
<citsent citstr=" P08-1067 ">
generative models based on lexicalized pcfgs enjoyed great success as the machine learning framework for full parsing (collins, 1999; charniak, 2000), <papid> A00-2018 </papid>but recently discriminative models attract more attention due to their superior accuracy (charniak and johnson, 2005; <papid> P05-1022 </papid>huang, 2008) <papid> P08-1067 </papid>and adaptability to new grammars and languages (buchholz and marsi, 2006).<papid> W06-2920 </papid></citsent>
<aftsection>
<nextsent>a traditional approach to discriminative full parsing is to convert full parsing task into series of classification problems.
</nextsent>
<nextsent>ratnaparkhi (1997)<papid> W97-0301 </papid>performs full parsing in bottom-up and left-toright manner and uses maximum entropy classifier to make decisions to construct individualphrases.</nextsent>
<nextsent>sagae and lavie (2006) <papid> P06-2089 </papid>use the shift reduce parsing framework and maximum entropy model for local classification to decide parsing actions.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I359">
<title id=" E09-1090.xml">fast full parsing by linear chain conditional random fields </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>one of the major obstacles that discourage the use of full parsing in large-scale natural language processing applications is its computational cost.
</prevsent>
<prevsent>for example, the medline corpus, collection of abstracts of biomedical papers, consists of 70 million sentences and would require more than two years of processing time if the parser needs one second to process sentence.
</prevsent>
</prevsection>
<citsent citstr=" W06-2920 ">
generative models based on lexicalized pcfgs enjoyed great success as the machine learning framework for full parsing (collins, 1999; charniak, 2000), <papid> A00-2018 </papid>but recently discriminative models attract more attention due to their superior accuracy (charniak and johnson, 2005; <papid> P05-1022 </papid>huang, 2008) <papid> P08-1067 </papid>and adaptability to new grammars and languages (buchholz and marsi, 2006).<papid> W06-2920 </papid></citsent>
<aftsection>
<nextsent>a traditional approach to discriminative full parsing is to convert full parsing task into series of classification problems.
</nextsent>
<nextsent>ratnaparkhi (1997)<papid> W97-0301 </papid>performs full parsing in bottom-up and left-toright manner and uses maximum entropy classifier to make decisions to construct individualphrases.</nextsent>
<nextsent>sagae and lavie (2006) <papid> P06-2089 </papid>use the shift reduce parsing framework and maximum entropy model for local classification to decide parsing actions.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I360">
<title id=" E09-1090.xml">fast full parsing by linear chain conditional random fields </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>generative models based on lexicalized pcfgs enjoyed great success as the machine learning framework for full parsing (collins, 1999; charniak, 2000), <papid> A00-2018 </papid>but recently discriminative models attract more attention due to their superior accuracy (charniak and johnson, 2005; <papid> P05-1022 </papid>huang, 2008) <papid> P08-1067 </papid>and adaptability to new grammars and languages (buchholz and marsi, 2006).<papid> W06-2920 </papid></prevsent>
<prevsent>a traditional approach to discriminative full parsing is to convert full parsing task into series of classification problems.</prevsent>
</prevsection>
<citsent citstr=" W97-0301 ">
ratnaparkhi (1997)<papid> W97-0301 </papid>performs full parsing in bottom-up and left-toright manner and uses maximum entropy classifier to make decisions to construct individualphrases.</citsent>
<aftsection>
<nextsent>sagae and lavie (2006) <papid> P06-2089 </papid>use the shift reduce parsing framework and maximum entropy model for local classification to decide parsing actions.</nextsent>
<nextsent>these approaches are often called history-based approaches.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I361">
<title id=" E09-1090.xml">fast full parsing by linear chain conditional random fields </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a traditional approach to discriminative full parsing is to convert full parsing task into series of classification problems.
</prevsent>
<prevsent>ratnaparkhi (1997)<papid> W97-0301 </papid>performs full parsing in bottom-up and left-toright manner and uses maximum entropy classifier to make decisions to construct individualphrases.</prevsent>
</prevsection>
<citsent citstr=" P06-2089 ">
sagae and lavie (2006) <papid> P06-2089 </papid>use the shift reduce parsing framework and maximum entropy model for local classification to decide parsing actions.</citsent>
<aftsection>
<nextsent>these approaches are often called history-based approaches.
</nextsent>
<nextsent>a more recent approach to discriminative full parsing is to treat the task as single structured prediction problem.
</nextsent>
<nextsent>finkel et al (2008) <papid> P08-1109 </papid>incorporated rich local features into tree crf mod eland built competitive parser.</nextsent>
<nextsent>huang (2008) <papid> P08-1067 </papid>proposed to use parse forest to incorporate non-localfeatures.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I362">
<title id=" E09-1090.xml">fast full parsing by linear chain conditional random fields </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these approaches are often called history-based approaches.
</prevsent>
<prevsent>a more recent approach to discriminative full parsing is to treat the task as single structured prediction problem.
</prevsent>
</prevsection>
<citsent citstr=" P08-1109 ">
finkel et al (2008) <papid> P08-1109 </papid>incorporated rich local features into tree crf mod eland built competitive parser.</citsent>
<aftsection>
<nextsent>huang (2008) <papid> P08-1067 </papid>proposed to use parse forest to incorporate non-localfeatures.</nextsent>
<nextsent>they used perceptron algorithm to optimize the weights of the features and achieved state-of-the-art accuracy.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I368">
<title id=" E09-1090.xml">fast full parsing by linear chain conditional random fields </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in other words, our parser could be located somewhere between traditional history-based approaches andwhole-sentence approaches.
</prevsent>
<prevsent>one of our motivations for this work was that our parsing model may achieve better balance between accuracy and speed than existing parsers.
</prevsent>
</prevsection>
<citsent citstr=" C04-1041 ">
it is also worth mentioning that our approach is similar in spirit to super tagging for parsing with lexicalized grammar formalisms such as ccg and hpsg (clark and curran, 2004; <papid> C04-1041 </papid>ninomiya et al, 2006), <papid> W06-1619 </papid>in which significant speed-ups for parsing time are achieved.in this paper, we show that our approach is indeed appealing in that the parser runs very fast and gives competitive accuracy.</citsent>
<aftsection>
<nextsent>we evaluate our parser on the standard dataset for parsing experiments (i.e. the penn treebank) and compare it with existing approaches to full parsing.
</nextsent>
<nextsent>this paper is organized as follows.
</nextsent>
<nextsent>section 2presents the overall chunk parsing strategy.
</nextsent>
<nextsent>section 3 describes the crf model used to perform individual chunking steps.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I369">
<title id=" E09-1090.xml">fast full parsing by linear chain conditional random fields </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in other words, our parser could be located somewhere between traditional history-based approaches andwhole-sentence approaches.
</prevsent>
<prevsent>one of our motivations for this work was that our parsing model may achieve better balance between accuracy and speed than existing parsers.
</prevsent>
</prevsection>
<citsent citstr=" W06-1619 ">
it is also worth mentioning that our approach is similar in spirit to super tagging for parsing with lexicalized grammar formalisms such as ccg and hpsg (clark and curran, 2004; <papid> C04-1041 </papid>ninomiya et al, 2006), <papid> W06-1619 </papid>in which significant speed-ups for parsing time are achieved.in this paper, we show that our approach is indeed appealing in that the parser runs very fast and gives competitive accuracy.</citsent>
<aftsection>
<nextsent>we evaluate our parser on the standard dataset for parsing experiments (i.e. the penn treebank) and compare it with existing approaches to full parsing.
</nextsent>
<nextsent>this paper is organized as follows.
</nextsent>
<nextsent>section 2presents the overall chunk parsing strategy.
</nextsent>
<nextsent>section 3 describes the crf model used to perform individual chunking steps.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I370">
<title id=" E09-1090.xml">fast full parsing by linear chain conditional random fields </title>
<section> full parsing by chunking.  </section>
<citcontext>
<prevsection>
<prevsent>the full parse tree is recovered from the chunking history in straightforward way.this idea of converting full parsing into series of chunking tasks is not new by any means?
</prevsent>
<prevsent>the history of this kind of approach dates back to 1950s (joshi and hopely, 1996).
</prevsent>
</prevsection>
<citsent citstr=" E99-1016 ">
more recently, brants (1999) <papid> E99-1016 </papid>used cascaded markov model to parse german text.</citsent>
<aftsection>
<nextsent>tjong kim sang (2001) used the iob tagging method to represent chunks and memory-based learning, and achieved an f-scoreof 80.49 on the wsj corpus.
</nextsent>
<nextsent>tsuruoka and tsu jii (2005) <papid> W05-1514 </papid>improved upon their approach by using1the head word is identified by using the head percolation table (magerman, 1995).<papid> P95-1037 </papid></nextsent>
<nextsent>791 0 1000 2000 3000 4000 5000 0 5 10 15 20 25 30 # se nt en ce height figure 5: distribution of tree height in wsj sections 2-21.a maximum entropy classifier and achieved an score of 85.9.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I371">
<title id=" E09-1090.xml">fast full parsing by linear chain conditional random fields </title>
<section> full parsing by chunking.  </section>
<citcontext>
<prevsection>
<prevsent>more recently, brants (1999) <papid> E99-1016 </papid>used cascaded markov model to parse german text.</prevsent>
<prevsent>tjong kim sang (2001) used the iob tagging method to represent chunks and memory-based learning, and achieved an f-scoreof 80.49 on the wsj corpus.</prevsent>
</prevsection>
<citsent citstr=" W05-1514 ">
tsuruoka and tsu jii (2005) <papid> W05-1514 </papid>improved upon their approach by using1the head word is identified by using the head percolation table (magerman, 1995).<papid> P95-1037 </papid></citsent>
<aftsection>
<nextsent>791 0 1000 2000 3000 4000 5000 0 5 10 15 20 25 30 # se nt en ce height figure 5: distribution of tree height in wsj sections 2-21.a maximum entropy classifier and achieved an score of 85.9.
</nextsent>
<nextsent>however, there is still large gap between the accuracy of chunking-based parsers and that of widely-used practical parsers such as collins parser and charniak parser (collins, 1999; charniak, 2000).<papid> A00-2018 </papid></nextsent>
<nextsent>2.1 heights of trees.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I372">
<title id=" E09-1090.xml">fast full parsing by linear chain conditional random fields </title>
<section> full parsing by chunking.  </section>
<citcontext>
<prevsection>
<prevsent>more recently, brants (1999) <papid> E99-1016 </papid>used cascaded markov model to parse german text.</prevsent>
<prevsent>tjong kim sang (2001) used the iob tagging method to represent chunks and memory-based learning, and achieved an f-scoreof 80.49 on the wsj corpus.</prevsent>
</prevsection>
<citsent citstr=" P95-1037 ">
tsuruoka and tsu jii (2005) <papid> W05-1514 </papid>improved upon their approach by using1the head word is identified by using the head percolation table (magerman, 1995).<papid> P95-1037 </papid></citsent>
<aftsection>
<nextsent>791 0 1000 2000 3000 4000 5000 0 5 10 15 20 25 30 # se nt en ce height figure 5: distribution of tree height in wsj sections 2-21.a maximum entropy classifier and achieved an score of 85.9.
</nextsent>
<nextsent>however, there is still large gap between the accuracy of chunking-based parsers and that of widely-used practical parsers such as collins parser and charniak parser (collins, 1999; charniak, 2000).<papid> A00-2018 </papid></nextsent>
<nextsent>2.1 heights of trees.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I374">
<title id=" E09-1090.xml">fast full parsing by linear chain conditional random fields </title>
<section> chunking with crfs.  </section>
<citcontext>
<prevsection>
<prevsent>= ? i=1 log p(y(i)|x(i)) + r(?),where r(?)
</prevsent>
<prevsent>is introduced for the purpose of regularization which prevents the model from over fitting the training data.
</prevsent>
</prevsection>
<citsent citstr=" P07-1104 ">
the l1 or l2 norm is commonly used in statistical natural language processing (gao et al, 2007).<papid> P07-1104 </papid></citsent>
<aftsection>
<nextsent>we used l1-regularization, which is defined as r(?)
</nextsent>
<nextsent>= 1c ? k=1 |k|, where is the meta-parameter that controls the degree of regularization.
</nextsent>
<nextsent>we used the owl-qn algorithm (andrew and gao, 2007) to obtain the parameters that maximize the l1-regularized conditional log-likelihood.
</nextsent>
<nextsent>3.2 features.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I375">
<title id=" E09-1090.xml">fast full parsing by linear chain conditional random fields </title>
<section> chunking with crfs.  </section>
<citcontext>
<prevsection>
<prevsent>3.2 features.
</prevsent>
<prevsent>table 1 shows the features used in chunking for the base level.
</prevsent>
</prevsection>
<citsent citstr=" N03-1028 ">
since the task is basically identical to shallow parsing by crfs, we follow the feature sets used in the previous work by sha and pereira (2003).<papid> N03-1028 </papid></citsent>
<aftsection>
<nextsent>we use unigrams, bigrams, and trigrams of part-of-speech (pos) tags and words.
</nextsent>
<nextsent>the difference between our crf chunker and that in (sha and pereira, 2003) <papid> N03-1028 </papid>is that we could not use second-order crf models, hence we could not use trigram features on the bio states.</nextsent>
<nextsent>we 792 symbol unigrams s2, s1, s0, s+1, s+2 symbol bigrams s2s1, s1s0, s0s+1, s+1s+2 symbol trigrams s3s2s1, s2s1s0, s1s0s+1, s0s+1s+2, s+1s+2s+3 word unigrams h2, h1, h0, h+1, h+2 word bigrams h2h1, h1h0, h0h+1, h+1h+2 word trigrams h1h0h+1 table 1: feature templates used in the base level chunking.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I390">
<title id=" E09-1090.xml">fast full parsing by linear chain conditional random fields </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>this is themain idea of petrov and klein (2008), which significantly improved parsing accuracy.
</prevsent>
<prevsent>a totally different approach to improving the accuracy of our parser is to use the idea of self training?
</prevsent>
</prevsection>
<citsent citstr=" N06-1020 ">
described in (mcclosky et al, 2006).<papid> N06-1020 </papid></citsent>
<aftsection>
<nextsent>the basic idea is to create larger set of training data by applying an accurate parser (e.g. reranking parser) to large amount of raw text.
</nextsent>
<nextsent>we can then use the automatically created treebank as the additional training data for our parser.
</nextsent>
<nextsent>this approach suggests that accurate (but slow) parsers and fast (but not-so-accurate) parsers can actually help each other.
</nextsent>
<nextsent>also, since it is not difficult to extend our parser to produce n-best parsing hypotheses, one could build fast reranking parser by using the parser as the base (hypotheses generating) parser.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I405">
<title id=" E12-1081.xml">learning the fine grained information status of discourse entities </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>one task that could benefit from knowledge of is is identity coreference: since new entities by definition have not been previously referred to, an np marked as new does not need to be resolved, thereby improving the precision of coreferenceresolver.
</prevsent>
<prevsent>knowledge of fine-grained or subcategorized is is valuable for other nlp tasks.
</prevsent>
</prevsection>
<citsent citstr=" C08-1033 ">
for instance, an np marked as set signifies that it is in set-subset relation with its antecedent, thereby providing important clues for bridging anaphora resolution (e.g., gasperin and briscoe (2008)).<papid> C08-1033 </papid></citsent>
<aftsection>
<nextsent>despite the potential usefulness of is in nlp tasks, there has been little work on learning the is of discourse entities.
</nextsent>
<nextsent>to investigate the plausibility of learning is, nissim et al (2004) annotate set of switchboard dialogues with such information2 , and subsequently present a2these and other linguistic annotations on the switchboard dialogues were later released by the ldc as part of the nxt corpus, which is described in calhoun et al (2010).
</nextsent>
<nextsent>798rule-based approach and learning-based approach to acquiring such knowledge (nissim, 2006).<papid> W06-1612 </papid></nextsent>
<nextsent>more recently, we have improved nissimslearning-based approach by augmenting her feature set, which comprises seven string-matchingand grammatical features, with lexical and syntactic features (rahman and ng, 2011; henceforth r&n;).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I406">
<title id=" E12-1081.xml">learning the fine grained information status of discourse entities </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>despite the potential usefulness of is in nlp tasks, there has been little work on learning the is of discourse entities.
</prevsent>
<prevsent>to investigate the plausibility of learning is, nissim et al (2004) annotate set of switchboard dialogues with such information2 , and subsequently present a2these and other linguistic annotations on the switchboard dialogues were later released by the ldc as part of the nxt corpus, which is described in calhoun et al (2010).
</prevsent>
</prevsection>
<citsent citstr=" W06-1612 ">
798rule-based approach and learning-based approach to acquiring such knowledge (nissim, 2006).<papid> W06-1612 </papid></citsent>
<aftsection>
<nextsent>more recently, we have improved nissimslearning-based approach by augmenting her feature set, which comprises seven string-matchingand grammatical features, with lexical and syntactic features (rahman and ng, 2011; henceforth r&n;).
</nextsent>
<nextsent>despite the improvements, the performance on new entities remains poor: an score of 46.5% was achieved.our goal in this paper is to investigate fine grained is determination, the task of classifying discourse entity as one of the 16 is sub types defined by nissim et al (2004).3 owing in partto the increase in the number of categories, fine grained is determination is arguably more challenging task than the 3-class is determination task that nissim and r&n; investigated.
</nextsent>
<nextsent>to our knowledge, this is the first empirical investigation of automated fine-grained is determination.we propose knowledge-rich approach to fine grained is determination.
</nextsent>
<nextsent>our proposal is motivated in part by nissims and r&ns; poor performance on new entities, which we hypothesize can be attributed to their sole reliance on shallow knowledge sources.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I407">
<title id=" E12-1081.xml">learning the fine grained information status of discourse entities </title>
<section> rule-based approach.  </section>
<citcontext>
<prevsection>
<prevsent>in of course?, accident?
</prevsent>
<prevsent>in by accident?, etc. to the dialogue participants.
</prevsent>
</prevsection>
<citsent citstr=" W11-1902 ">
note that this and several other rules relyon coreference information, which we obtain from two sources: (1)chains generated automatically using the stanford deterministic coreference resolution system (lee et al  2011)<papid> W11-1902 </papid>5, and (2) manually identified coreference chains taken directly from the annotated switchboard dialogues.</citsent>
<aftsection>
<nextsent>reporting results using these two ways of obtaining chains facilitates the comparison of the is determination results that we can realistically obtain using existing coreference technologies against those thatwe could obtain if we further improved existing coreference resolvers.
</nextsent>
<nextsent>note that both sources provide identity coreference chains.
</nextsent>
<nextsent>specifically,the gold chains were annotated for nps belonging to old/identity and old/ident generic.
</nextsent>
<nextsent>hence, these chains can be used to distinguish betweenold/general nps and old/ident generic nps, be cause the former are not part of chain whereas the latter are.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I408">
<title id=" E12-1081.xml">learning the fine grained information status of discourse entities </title>
<section> rule-based approach.  </section>
<citcontext>
<prevsection>
<prevsent>the words and phrases listed in the rule, which are derived manually from the training data, provide suggestive evidence that the np under consideration is subset or specific portion of an entity or concept mentioned earlier in the dialogue.
</prevsent>
<prevsent>examples include another bedroom?, different color?, somebody else?, any place?, one of them?, and most other cities?.
</prevsent>
</prevsection>
<citsent citstr=" D11-1142 ">
condition 3 of the rule, which checks whether the head noun of the np has been mentioned previously, is good test for identity coreference, but since all the old entities have suppos 802edly been identified by the preceding rules, it becomes reasonable test for set-subset relations.for convenience, we identify part-whole relations in rule 12 based on the output produced by reverb (fader et al  2011), <papid> D11-1142 </papid>an open information extraction system.6 the output contains, among other things, relation instances, each of which is represented as triple,  a,rel,b , where rel isa relation, and and are its arguments.</citsent>
<aftsection>
<nextsent>to pre process the output, we first identify all the triples that are instances of the part-whole relation using regular expressions.
</nextsent>
<nextsent>next, we create clusters of relation arguments, such that each pair of arguments in cluster has part-whole relation.this is easy: since part-whole is transitive relation (i.e.,  a,part,b  and  b,part,c  implies  a,part,c ), we cluster the arguments by taking the transitive closure of these relation instances.
</nextsent>
<nextsent>then, given an np npi in the test set, we assign med/part to it if there is preceding np npj such that the two nps are in the same argument cluster.
</nextsent>
<nextsent>in rule 14, we use framenet (baker et al  1998) <papid> P98-1013 </papid>to determine whether med/situation should be assigned to an np, npi.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I409">
<title id=" E12-1081.xml">learning the fine grained information status of discourse entities </title>
<section> rule-based approach.  </section>
<citcontext>
<prevsection>
<prevsent>next, we create clusters of relation arguments, such that each pair of arguments in cluster has part-whole relation.this is easy: since part-whole is transitive relation (i.e.,  a,part,b  and  b,part,c  implies  a,part,c ), we cluster the arguments by taking the transitive closure of these relation instances.
</prevsent>
<prevsent>then, given an np npi in the test set, we assign med/part to it if there is preceding np npj such that the two nps are in the same argument cluster.
</prevsent>
</prevsection>
<citsent citstr=" P98-1013 ">
in rule 14, we use framenet (baker et al  1998) <papid> P98-1013 </papid>to determine whether med/situation should be assigned to an np, npi.</citsent>
<aftsection>
<nextsent>specifically, we check whether it fills an argument of frame set up by preceding np, npj , or verb.
</nextsent>
<nextsent>to exemplify, let us assume that npj is capital punishment?.
</nextsent>
<nextsent>we search for punishment?
</nextsent>
<nextsent>in framenet to access the appropriate frame, which in this case is rewards and punishments?.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I410">
<title id=" E06-3008.xml">towards robust animacy classification using morphosyntactic distributional features </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>orasan and evans (2001) make use of hyponym-relations taken from theword net resource (fellbaum, 1998) in order to classify animate referents.
</prevsent>
<prevsent>however, such method is clearly restricted to languages for which large scale lexical resources, such as the wordnet, are available.
</prevsent>
</prevsection>
<citsent citstr=" J01-3003 ">
merlo and stevenson (2001) <papid> J01-3003 </papid>present method forverb classification which relies only on distributional statistics taken from corpora in order to train decision tree classifier to distinguish between three groups of in transitive verbs.</citsent>
<aftsection>
<nextsent>1the notion of prominence has been linked to several properties such as most likely as topic, agent, most available referent, etc. 47 this paper presents experiments in automatic classification of the animacy of unseen norwegian common nouns, inspired by the method forverb classification presented in merlo and stevenson (2001).<papid> J01-3003 </papid></nextsent>
<nextsent>the learning task is, forgiven common noun, to classify it as either belonging to the class animate or inanimate.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I415">
<title id=" E06-3008.xml">towards robust animacy classification using morphosyntactic distributional features </title>
<section> conclusion.  </section>
<citcontext>
<prevsection>
<prevsent>such an approach might also include finer subdivision of the nouns.
</prevsent>
<prevsent>we have chosen to classify along binary dimension, however, it mightbe argued that this is an artificial dichotomy.
</prevsent>
</prevsection>
<citsent citstr=" W04-0216 ">
(za enen et al, 2004) <papid> W04-0216 </papid>describe an encoding scheme for the manual encoding of animacy information in part of the english switchboard corpus.they make three-way distinction between human, other animates, and in animates, where the other animates?</citsent>
<aftsection>
<nextsent>category describes rather heterogeneous group of entities: organisations, animals, intelligent machines and vehicles.
</nextsent>
<nextsent>how ever, what these seem to have in common is that they may all be construed linguistically as ani 53 freq all subj obj gen pass anaan anain refl 100 85.0 52.5 87.5 65.0 70.0 50.0 57.5 50.0 50 77.5 77.5 75.0 75.0 50.0 50.0 50.0 50.0 10 52.5 50.0 62.5 50.0 50.0 50.0 50.0 50.0 table 7: accuracy obtained when employing new classifier on new lower-frequency nouns: all and individual features freq subj&obj;&gen; subj&obj; subj&gen; obj&gen; 100 85.0 85.0 67.5 82.5 50 75.0 80.0 75.0 70.0 10 62.5 62.5 50.0 62.5table 8: accuracy obtained when employing new classifier on new lower-frequency nouns: combinations of the most frequent features mate beings, even though they, in the real world,are not.
</nextsent>
<nextsent>interestingly, the two misclassified inanimate nouns in experiment 1, were bil car?
</nextsent>
<nextsent>and fly air plane?, both vehicles.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I416">
<title id=" E06-1042.xml">a clustering approach for nearly unsupervised recognition of non literal language </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>(dolan, 1995)).
</prevsent>
<prevsent>corpus-based systems primarily extractor learn the necessarymetaphor-processing information from large corpora, thus avoiding the need for manual annotation or metaphor-map construction.
</prevsent>
</prevsection>
<citsent citstr=" P03-1008 ">
examples of such systems can be found in (murata et. al., 2000; nissim &markert;, 2003; <papid> P03-1008 </papid>mason, 2004).<papid> J04-1002 </papid></citsent>
<aftsection>
<nextsent>the work on supervised metonymy resolution by nissim &amp; markert and the work on conceptual metaphors by mason come closest to what we are trying to do with trofi.
</nextsent>
<nextsent>nissim &amp; markert (2003) <papid> P03-1008 </papid>approach metonymy resolution with machine learning methods, which[exploit] the similarity between examples of conventional metonymy?</nextsent>
<nextsent>((nissim &amp; markert, 2003),<papid> P03-1008 </papid>p. 56).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I417">
<title id=" E06-1042.xml">a clustering approach for nearly unsupervised recognition of non literal language </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>(dolan, 1995)).
</prevsent>
<prevsent>corpus-based systems primarily extractor learn the necessarymetaphor-processing information from large corpora, thus avoiding the need for manual annotation or metaphor-map construction.
</prevsent>
</prevsection>
<citsent citstr=" J04-1002 ">
examples of such systems can be found in (murata et. al., 2000; nissim &markert;, 2003; <papid> P03-1008 </papid>mason, 2004).<papid> J04-1002 </papid></citsent>
<aftsection>
<nextsent>the work on supervised metonymy resolution by nissim &amp; markert and the work on conceptual metaphors by mason come closest to what we are trying to do with trofi.
</nextsent>
<nextsent>nissim &amp; markert (2003) <papid> P03-1008 </papid>approach metonymy resolution with machine learning methods, which[exploit] the similarity between examples of conventional metonymy?</nextsent>
<nextsent>((nissim &amp; markert, 2003),<papid> P03-1008 </papid>p. 56).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I425">
<title id=" E06-1042.xml">a clustering approach for nearly unsupervised recognition of non literal language </title>
<section> trofi.  </section>
<citcontext>
<prevsection>
<prevsent>as non literal, but we make no strong claims about this.
</prevsent>
<prevsent>3.1 the data.
</prevsent>
</prevsection>
<citsent citstr=" J98-1002 ">
the trofi algorithm requires target set (called original set in (karov &amp; edelman, 1998)) ? <papid> J98-1002 </papid>theset of sentences containing the verbs to be classified into literal or non literal ? and the seed sets:the literal feedback set and the non literal feedback set.</citsent>
<aftsection>
<nextsent>these sets contain feature lists consisting of the stemmed nouns and verbs in sentence,with target or seed words and frequent words removed.
</nextsent>
<nextsent>the frequent word list (374 words) consists of the 332 most frequent words in the british national corpus plus contractions, single letters,and numbers from 0-10.
</nextsent>
<nextsent>the target set is built using the 88-89 wall street journal corpus (wsj) tagged using the (ratnaparkhi, 1996) <papid> W96-0213 </papid>tagger and the (bangalore &amp; joshi, 1999) <papid> J99-2004 </papid>supertagger; the feedback sets are built using wsj sentences con 330 algorithm 1 ke-train: (karov &amp; edelman, 1998) <papid> J98-1002 </papid>algorithm adapted to literal/nonliteral classification require: s: the set of sentences containing the target word require: l: the set of literal seed sentences require: : the set of non literal seed sentences require: w: the set of words/features, ? means is in sentence s, 3 means contains require: : threshold that determines the stopping condition 1: w-sim0(wx, wy) := 1 if wx = wy, 0 otherwise 2: s-simi0(sx, sy) := 1, for all sx, sy ? ? where sx = sy, 0 otherwise 3: := 0 4: while (true) do 5: s-simli+1(sx, sy) := ? wxsx p(wx, sx)maxwysy w-simi(wx, wy), for all sx, sy ? ? 6: s-simni+1(sx, sy) := ? wxsx p(wx, sx)maxwysy w-simi(wx, wy), for all sx, sy ? n 7: for wx, wy ? w do 8: w-simi+1(wx, wy) := { = 0 sx3wx p(wx, sx)maxsy3wy s-simii (sx, sy) else ? sx3wx p(wx, sx)maxsy3wy{s-simli (sx, sy), s-simni (sx, sy)} 9: end for 10: if wx,maxwy{w-simi+1(wx, wy) ? w-simi(wx, wy)} ?  then 11: break # algorithm converges in 1 steps.</nextsent>
<nextsent>12: end if 13: := + 1 14: end while taining seed words extracted from wordnet and the databases of known metaphors, idioms, and expressions (dokmie), namely wayne magnuson english idioms sayings &amp; slang and georgelakoffs conceptual metaphor list, as well as example sentences from these sources.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I426">
<title id=" E06-1042.xml">a clustering approach for nearly unsupervised recognition of non literal language </title>
<section> trofi.  </section>
<citcontext>
<prevsection>
<prevsent>these sets contain feature lists consisting of the stemmed nouns and verbs in sentence,with target or seed words and frequent words removed.
</prevsent>
<prevsent>the frequent word list (374 words) consists of the 332 most frequent words in the british national corpus plus contractions, single letters,and numbers from 0-10.
</prevsent>
</prevsection>
<citsent citstr=" W96-0213 ">
the target set is built using the 88-89 wall street journal corpus (wsj) tagged using the (ratnaparkhi, 1996) <papid> W96-0213 </papid>tagger and the (bangalore &amp; joshi, 1999) <papid> J99-2004 </papid>supertagger; the feedback sets are built using wsj sentences con 330 algorithm 1 ke-train: (karov &amp; edelman, 1998) <papid> J98-1002 </papid>algorithm adapted to literal/nonliteral classification require: s: the set of sentences containing the target word require: l: the set of literal seed sentences require: : the set of non literal seed sentences require: w: the set of words/features, ? means is in sentence s, 3 means contains require: : threshold that determines the stopping condition 1: w-sim0(wx, wy) := 1 if wx = wy, 0 otherwise 2: s-simi0(sx, sy) := 1, for all sx, sy ? ? where sx = sy, 0 otherwise 3: := 0 4: while (true) do 5: s-simli+1(sx, sy) := ? wxsx p(wx, sx)maxwysy w-simi(wx, wy), for all sx, sy ? ? 6: s-simni+1(sx, sy) := ? wxsx p(wx, sx)maxwysy w-simi(wx, wy), for all sx, sy ? n 7: for wx, wy ? w do 8: w-simi+1(wx, wy) := { = 0 sx3wx p(wx, sx)maxsy3wy s-simii (sx, sy) else ? sx3wx p(wx, sx)maxsy3wy{s-simli (sx, sy), s-simni (sx, sy)} 9: end for 10: if wx,maxwy{w-simi+1(wx, wy) ? w-simi(wx, wy)} ?  then 11: break # algorithm converges in 1 steps.</citsent>
<aftsection>
<nextsent>12: end if 13: := + 1 14: end while taining seed words extracted from wordnet and the databases of known metaphors, idioms, and expressions (dokmie), namely wayne magnuson english idioms sayings &amp; slang and georgelakoffs conceptual metaphor list, as well as example sentences from these sources.
</nextsent>
<nextsent>(see section 4 for the sizes of the target and feedback sets.)
</nextsent>
<nextsent>one may ask why we need trofi if we have databases like the dokmie.
</nextsent>
<nextsent>the reason is that the dokmieare unlikely to list all possible instances of non literal language and because knowing that an expression can be used non literally does not mean that you can tell when it is being used non liter ally.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I427">
<title id=" E06-1042.xml">a clustering approach for nearly unsupervised recognition of non literal language </title>
<section> trofi.  </section>
<citcontext>
<prevsection>
<prevsent>these sets contain feature lists consisting of the stemmed nouns and verbs in sentence,with target or seed words and frequent words removed.
</prevsent>
<prevsent>the frequent word list (374 words) consists of the 332 most frequent words in the british national corpus plus contractions, single letters,and numbers from 0-10.
</prevsent>
</prevsection>
<citsent citstr=" J99-2004 ">
the target set is built using the 88-89 wall street journal corpus (wsj) tagged using the (ratnaparkhi, 1996) <papid> W96-0213 </papid>tagger and the (bangalore &amp; joshi, 1999) <papid> J99-2004 </papid>supertagger; the feedback sets are built using wsj sentences con 330 algorithm 1 ke-train: (karov &amp; edelman, 1998) <papid> J98-1002 </papid>algorithm adapted to literal/nonliteral classification require: s: the set of sentences containing the target word require: l: the set of literal seed sentences require: : the set of non literal seed sentences require: w: the set of words/features, ? means is in sentence s, 3 means contains require: : threshold that determines the stopping condition 1: w-sim0(wx, wy) := 1 if wx = wy, 0 otherwise 2: s-simi0(sx, sy) := 1, for all sx, sy ? ? where sx = sy, 0 otherwise 3: := 0 4: while (true) do 5: s-simli+1(sx, sy) := ? wxsx p(wx, sx)maxwysy w-simi(wx, wy), for all sx, sy ? ? 6: s-simni+1(sx, sy) := ? wxsx p(wx, sx)maxwysy w-simi(wx, wy), for all sx, sy ? n 7: for wx, wy ? w do 8: w-simi+1(wx, wy) := { = 0 sx3wx p(wx, sx)maxsy3wy s-simii (sx, sy) else ? sx3wx p(wx, sx)maxsy3wy{s-simli (sx, sy), s-simni (sx, sy)} 9: end for 10: if wx,maxwy{w-simi+1(wx, wy) ? w-simi(wx, wy)} ?  then 11: break # algorithm converges in 1 steps.</citsent>
<aftsection>
<nextsent>12: end if 13: := + 1 14: end while taining seed words extracted from wordnet and the databases of known metaphors, idioms, and expressions (dokmie), namely wayne magnuson english idioms sayings &amp; slang and georgelakoffs conceptual metaphor list, as well as example sentences from these sources.
</nextsent>
<nextsent>(see section 4 for the sizes of the target and feedback sets.)
</nextsent>
<nextsent>one may ask why we need trofi if we have databases like the dokmie.
</nextsent>
<nextsent>the reason is that the dokmieare unlikely to list all possible instances of non literal language and because knowing that an expression can be used non literally does not mean that you can tell when it is being used non liter ally.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I433">
<title id=" E06-1042.xml">a clustering approach for nearly unsupervised recognition of non literal language </title>
<section> results.  </section>
<citcontext>
<prevsection>
<prevsent>(cohen) and ?
</prevsent>
<prevsent>(s&c;) on random sample of 200 annotated examples annotated by two different annotators was found to be 0.77.
</prevsent>
</prevsection>
<citsent citstr=" J04-1005 ">
as per ((di eugenio &amp; glass, 2004), <papid> J04-1005 </papid>cf.</citsent>
<aftsection>
<nextsent>refs therein), the standard assessment for ? values is that tentative conclusions on agreement exists when .67 ? ?
</nextsent>
<nextsent>  .8, and definite conclusion on agreement exists when ? ?
</nextsent>
<nextsent>.8.
</nextsent>
<nextsent>in the case of larger scale annotation effort, having the person leading the effort provide one or two examples of literal and non literal usages for each target verb to each annotator would almost certainly improve inter-annotator agreement.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I434">
<title id=" E06-2017.xml">computing term translation probabilities with generalized latent semantic analysis </title>
<section> term translation probabilities in.  </section>
<citcontext>
<prevsection>
<prevsent>= utdthe columns of d?
</prevsent>
<prevsent>are documents in the dimensional space.in step 2 we used point-wise mutual information (pmi) as the co-occurrence based measure of semantic associations between pairs of the vocabulary terms.
</prevsent>
</prevsection>
<citsent citstr=" N03-1032 ">
pmi has been successfully applied to semantic proximity tests for words (turney, 2001;terra and clarke, 2003) <papid> N03-1032 </papid>and was also successfully used as measure of term similarity to compute document clusters (pantel and lin, 2002).</citsent>
<aftsection>
<nextsent>in 152 our preliminary experiments, the glsa with pmi showed better performance than with other cooccurrence based measures such as the likelihood ratio, and 2 test.
</nextsent>
<nextsent>pmi between random variables representing two words, w1 and w2, is computed as pmi(w1, w2) = log (w1 = 1,w2 = 1) (w1 = 1)p (w2 = 1) .
</nextsent>
<nextsent>(4) we used the singular value decomposition (svd) in step 3 to compute glsa term vectors.
</nextsent>
<nextsent>lsa (deerwester et al, 1990) and some other related dimensionality reduction techniques, e.g. locality preserving projections (he and niyogi,2003) compute dual document-term representation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I436">
<title id=" E09-1092.xml">deriving generalized knowledge from corpora using wordnet abstraction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>as well, such sources typically do not cover the most obvious facts of the world, such as that ice cream may be delicious and may be coated with chocolate, or that children may play in parks.
</prevsent>
<prevsent>methods currently exist for extracting simple factoids?
</prevsent>
</prevsection>
<citsent citstr=" W03-0902 ">
like those about ice cream and children just mentioned (see in particular (schubert, 2002; schubert and tong, 2003)), <papid> W03-0902 </papid>but these are quite weak as general claims, and ? being unconditional ? are unsuitable for inference chaining.</citsent>
<aftsection>
<nextsent>consider however the fact that when something is said, it is generally said by person, organization or text source; this conditional statement dealing with the potential agents of saying, and could enable useful inferences.
</nextsent>
<nextsent>for example, in the sentence,the tires were worn and they said had to replace them?, they might be mistakenly identified with the tires, without the knowledge that saying is something done primarily by persons, organizations or text sources.
</nextsent>
<nextsent>similarly, looking into the future one can imagine telling household robot,the cat needs to drink something?, with the expectation that the robot will take into account that if cat drinks something, it is usually water or milk (whereas people would often have broader options).
</nextsent>
<nextsent>the work reported here is aimed at deriving generalizations of the latter sort from large sets of weaker propositions, by examining the hierarchical relations among sets of types that occur in the argument positions of verbal or other predicates.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I438">
<title id=" E09-1092.xml">deriving generalized knowledge from corpora using wordnet abstraction </title>
<section> knext.  </section>
<citcontext>
<prevsection>
<prevsent>a female-individual may have clothes.
</prevsent>
<prevsent>clothes can be washed.
</prevsent>
</prevsection>
<citsent citstr=" P97-1003 ">
((:i (:q det named-entity) enter[v] (:q the room[n])) (:i (:q det female-individual) have[v] (:q det room[n])) (:i (:q det female-individual) sleep[v]) (:i (:q det female-individual) have[v] (:q det (:f plur clothe[n]))) (:i (:q det (:f plur clothe[n])) washed[a])) here the upper-case sentences are automatically generated verbalizations of the abstracted lfs shown beneath them.1 the initial development of knext was based on the hand-constructed parse trees in the penn treebank version of the brown corpus, but subsequently schubert and collaborators refined and extended the system to work with parse trees obtained with statistical parsers (e.g., that of collins(1997) <papid> P97-1003 </papid>or charniak (2000)) <papid> A00-2018 </papid>applied to larger corpora, such as the british national corpus (bnc), 100 million-word, mixed genre collection, along with web corpora of comparable size (see work ofvan durme et al (2008) and van durme and schubert (2008) for details).</citsent>
<aftsection>
<nextsent>the bnc yielded over 21keywords like :i, :q, and :f are used to indicate infix predication, uns coped quantification, and function application, but these details need not concern us here.factoids per sentence on average, resulting in total collection of several million.
</nextsent>
<nextsent>human judging of the facto ids indicates that about 2 out of 3 facto ids are perceived as reasonable claims.the goal in this work, with respect to the example given, would be to derive with the use of alarge collection of knext outputs, general statement such as if something may sleep, it is probably either an animal or person.
</nextsent>
<nextsent>3.1 wordnet and senses.
</nextsent>
<nextsent>while the community continues to make gains in the automatic construction of reliable, general ontologies, the wordnet sense hierarchy (fell baum, 1998) continues to be the resource of choice for many computational linguists requiring an ontology-like structure.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I439">
<title id=" E09-1092.xml">deriving generalized knowledge from corpora using wordnet abstraction </title>
<section> knext.  </section>
<citcontext>
<prevsection>
<prevsent>a female-individual may have clothes.
</prevsent>
<prevsent>clothes can be washed.
</prevsent>
</prevsection>
<citsent citstr=" A00-2018 ">
((:i (:q det named-entity) enter[v] (:q the room[n])) (:i (:q det female-individual) have[v] (:q det room[n])) (:i (:q det female-individual) sleep[v]) (:i (:q det female-individual) have[v] (:q det (:f plur clothe[n]))) (:i (:q det (:f plur clothe[n])) washed[a])) here the upper-case sentences are automatically generated verbalizations of the abstracted lfs shown beneath them.1 the initial development of knext was based on the hand-constructed parse trees in the penn treebank version of the brown corpus, but subsequently schubert and collaborators refined and extended the system to work with parse trees obtained with statistical parsers (e.g., that of collins(1997) <papid> P97-1003 </papid>or charniak (2000)) <papid> A00-2018 </papid>applied to larger corpora, such as the british national corpus (bnc), 100 million-word, mixed genre collection, along with web corpora of comparable size (see work ofvan durme et al (2008) and van durme and schubert (2008) for details).</citsent>
<aftsection>
<nextsent>the bnc yielded over 21keywords like :i, :q, and :f are used to indicate infix predication, uns coped quantification, and function application, but these details need not concern us here.factoids per sentence on average, resulting in total collection of several million.
</nextsent>
<nextsent>human judging of the facto ids indicates that about 2 out of 3 facto ids are perceived as reasonable claims.the goal in this work, with respect to the example given, would be to derive with the use of alarge collection of knext outputs, general statement such as if something may sleep, it is probably either an animal or person.
</nextsent>
<nextsent>3.1 wordnet and senses.
</nextsent>
<nextsent>while the community continues to make gains in the automatic construction of reliable, general ontologies, the wordnet sense hierarchy (fell baum, 1998) continues to be the resource of choice for many computational linguists requiring an ontology-like structure.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I440">
<title id=" E09-1092.xml">deriving generalized knowledge from corpora using wordnet abstraction </title>
<section> resources.  </section>
<citcontext>
<prevsection>
<prevsent>while the community continues to make gains in the automatic construction of reliable, general ontologies, the wordnet sense hierarchy (fell baum, 1998) continues to be the resource of choice for many computational linguists requiring an ontology-like structure.
</prevsent>
<prevsent>in the work discussed here we explore the potential of wordnet as an underlying concept hierarchy on which to base generalization decisions.
</prevsent>
</prevsection>
<citsent citstr=" W97-0209 ">
the use of wordnet raises the challenge of dealing with multiple semantic concepts associated with the same word, i.e., employing wordnet requires word sense disambiguation in order to associate terms observed in text with concepts (synsets) within the hierarchy.in their work on determining selectional preferences, both resnik (1997) <papid> W97-0209 </papid>and li and abe (1998)<papid> J98-2002 </papid>relied on uniformly distributing observed frequencies forgiven word across all its senses, an approach later followed by pantel et al (2007).<papid> N07-1071 </papid>2 others within the knowledge acquisition community have favored taking the first, most dominant sense of each word (e.g., see suchanek et al (2007) and pasca (2008)).</citsent>
<aftsection>
<nextsent>as will be seen, our algorithm does not select word senses prior to generalizing them, but rath eras byproduct of the abstraction process.
</nextsent>
<nextsent>more over, it potentially selects multiple senses of aword deemed equally appropriate in given context, and in that sense provides coarse-grained disambiguation.
</nextsent>
<nextsent>this also prevents exaggeration of the contribution of term to the abstraction, as aresult of being lexicalized in particularly fine grained way.
</nextsent>
<nextsent>3.2 propositional templates.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I441">
<title id=" E09-1092.xml">deriving generalized knowledge from corpora using wordnet abstraction </title>
<section> resources.  </section>
<citcontext>
<prevsection>
<prevsent>while the community continues to make gains in the automatic construction of reliable, general ontologies, the wordnet sense hierarchy (fell baum, 1998) continues to be the resource of choice for many computational linguists requiring an ontology-like structure.
</prevsent>
<prevsent>in the work discussed here we explore the potential of wordnet as an underlying concept hierarchy on which to base generalization decisions.
</prevsent>
</prevsection>
<citsent citstr=" J98-2002 ">
the use of wordnet raises the challenge of dealing with multiple semantic concepts associated with the same word, i.e., employing wordnet requires word sense disambiguation in order to associate terms observed in text with concepts (synsets) within the hierarchy.in their work on determining selectional preferences, both resnik (1997) <papid> W97-0209 </papid>and li and abe (1998)<papid> J98-2002 </papid>relied on uniformly distributing observed frequencies forgiven word across all its senses, an approach later followed by pantel et al (2007).<papid> N07-1071 </papid>2 others within the knowledge acquisition community have favored taking the first, most dominant sense of each word (e.g., see suchanek et al (2007) and pasca (2008)).</citsent>
<aftsection>
<nextsent>as will be seen, our algorithm does not select word senses prior to generalizing them, but rath eras byproduct of the abstraction process.
</nextsent>
<nextsent>more over, it potentially selects multiple senses of aword deemed equally appropriate in given context, and in that sense provides coarse-grained disambiguation.
</nextsent>
<nextsent>this also prevents exaggeration of the contribution of term to the abstraction, as aresult of being lexicalized in particularly fine grained way.
</nextsent>
<nextsent>3.2 propositional templates.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I442">
<title id=" E09-1092.xml">deriving generalized knowledge from corpora using wordnet abstraction </title>
<section> resources.  </section>
<citcontext>
<prevsection>
<prevsent>while the community continues to make gains in the automatic construction of reliable, general ontologies, the wordnet sense hierarchy (fell baum, 1998) continues to be the resource of choice for many computational linguists requiring an ontology-like structure.
</prevsent>
<prevsent>in the work discussed here we explore the potential of wordnet as an underlying concept hierarchy on which to base generalization decisions.
</prevsent>
</prevsection>
<citsent citstr=" N07-1071 ">
the use of wordnet raises the challenge of dealing with multiple semantic concepts associated with the same word, i.e., employing wordnet requires word sense disambiguation in order to associate terms observed in text with concepts (synsets) within the hierarchy.in their work on determining selectional preferences, both resnik (1997) <papid> W97-0209 </papid>and li and abe (1998)<papid> J98-2002 </papid>relied on uniformly distributing observed frequencies forgiven word across all its senses, an approach later followed by pantel et al (2007).<papid> N07-1071 </papid>2 others within the knowledge acquisition community have favored taking the first, most dominant sense of each word (e.g., see suchanek et al (2007) and pasca (2008)).</citsent>
<aftsection>
<nextsent>as will be seen, our algorithm does not select word senses prior to generalizing them, but rath eras byproduct of the abstraction process.
</nextsent>
<nextsent>more over, it potentially selects multiple senses of aword deemed equally appropriate in given context, and in that sense provides coarse-grained disambiguation.
</nextsent>
<nextsent>this also prevents exaggeration of the contribution of term to the abstraction, as aresult of being lexicalized in particularly fine grained way.
</nextsent>
<nextsent>3.2 propositional templates.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I444">
<title id=" E09-1092.xml">deriving generalized knowledge from corpora using wordnet abstraction </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>our method as described thus far is not tied to aparticular word sense taxonomy.
</prevsent>
<prevsent>experiments reported here relied on the following model adjustments in order to make use of wordnet (version 3.0).
</prevsent>
</prevsection>
<citsent citstr=" W04-0837 ">
the function was set to return the union ofa synsets hypernym and instance hypernym rela tions.regarding the function , wordnet is constructed such that always picking the first senseof given nominal tends to be correct more of ten than not (see discussion by mccarthy et al(2004)).<papid> W04-0837 </papid></citsent>
<aftsection>
<nextsent>to exploit this structural bias, we employed modified version of that results in preference for nodes corresponding to the first sense of words to be covered, especially when the number of distinct observations were low (such as earlier, with crash helmet): l(n, n) = { 1?
</nextsent>
<nextsent>1|w | w : s(w) = (n, ...)
</nextsent>
<nextsent>1 otherwise5for the given example, this method (along with the constraints of table 1) led to the overly general type, living thing.
</nextsent>
<nextsent>811 word # gloss abstraction 6 general concept formed by extracting common features from specific examples attribute 2 an abstraction belonging to or characteristic of an entity matter 3 that which has mass and occupies space physical entity 1 an entity that has physical existence whole 2 an assemblage of parts that is regarded as single entity table 1: word, sense #?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I445">
<title id=" E09-1092.xml">deriving generalized knowledge from corpora using wordnet abstraction </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>we say senses plural, as the majority of knext propositions select senses that are more coarse-grained than wordnet synsets.
</prevsent>
<prevsent>thus, we wish to evaluate these more coarse-grained sense disambiguation results entailed by our type abstractions.7 we performed this evaluation using as comparisons the first-sense, and all-senses heuristics.
</prevsent>
</prevsection>
<citsent citstr=" P06-1014 ">
the first-sense heuristic can be thought of as striving for maximal specificity at the risk of precluding some admissible senses (reduced recall), 7allowing for multiple fine-grained senses to be judged as appropriate in given context goes back at least to sussna (1993); discussed more recently by, e.g., navigli (2006).<papid> P06-1014 </papid></citsent>
<aftsection>
<nextsent>while the all-senses heuristic insists on including all admissible senses (perfect recall) at the risk of including inadmissible ones.table 5 gives the results of two judges evaluating 314 word, sense?
</nextsent>
<nextsent>pairs across the 21 selectedtemplates.
</nextsent>
<nextsent>these sense pairs correspond to picking one word at random for each abstracted type selected for each template slot.
</nextsent>
<nextsent>judges were presented with sampled word, the originating template, and the glosses for each possible word sense (see figure 2).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I447">
<title id=" E09-1092.xml">deriving generalized knowledge from corpora using wordnet abstraction </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>li and abe (1998)<papid> J98-2002 </papid> used tree cut model over wordnet, based on the principle of minimum description length (mdl).</prevsent>
<prevsent>mccarthy has performed extensive work in the areas of selectional 814preference and wsd, e.g., (mccarthy, 1997; mccarthy, 2001).</prevsent>
</prevsection>
<citsent citstr=" J02-2003 ">
calling the generalization problem case of engineering in the face of sparse data,clark and weir (2002) <papid> J02-2003 </papid>looked at number of previous methods, one conclusion being that the approach of li and abe appears to over-generalize.</citsent>
<aftsection>
<nextsent>cao et al (2008) gave distributional method for deriving semantic restrictions for framenet frames, with the aim of building an italian framenet.
</nextsent>
<nextsent>while our goals are related, their work can be summarized as taking pre-existing gold standard, and extending it via distributional similarity measures based on shallow contexts (in this case, n-gram contexts up to length 5).
</nextsent>
<nextsent>we have presented results on strengthening type restrictions on arbitrary predicate argument structures derived directly from text.
</nextsent>
<nextsent>in describing alice, system for lifelong learning, banko and etzioni (2007) gave summary of proposition abstraction algorithm developed independently that is in some ways similar to derivetypes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I448">
<title id=" E09-1092.xml">deriving generalized knowledge from corpora using wordnet abstraction </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>we have presented results on strengthening type restrictions on arbitrary predicate argument structures derived directly from text.
</prevsent>
<prevsent>in describing alice, system for lifelong learning, banko and etzioni (2007) gave summary of proposition abstraction algorithm developed independently that is in some ways similar to derivetypes.
</prevsent>
</prevsection>
<citsent citstr=" C02-1105 ">
beyond differences in node scoring and their use of the first sense heuristic, the approach taken here differs in that it makes nouse of relative term frequency, nor contextual information outside particular propositional template.8 further, while we are concerned with general knowledge acquired over diverse texts, alice was built as an agent meant for constructing domain-specific theories, evaluated on 2.5million-page collection of web documents pertaining specifically to nutrition.minimizing word sense ambiguity by focusing on specific domain was later seen in thework of liakata and pulman (2008), who performed hierarchical clustering using output from their knext-like system first described in (li akata and pulman, 2002).<papid> C02-1105 </papid></citsent>
<aftsection>
<nextsent>terminal nodes of the resultant structure were used as the basis for inferring semantic type restrictions, reminiscent of the use of cbc clusters (pantel and lin, 2002) by pantel et al (2007), <papid> N07-1071 </papid>for typing the arguments of paraphrase rules.assigning pre-compiled instances to their first sense reading in wordnet, pasca (2008) then generalized class attributes extracted for these terms, using as resource google search engine query logs.katrenko and adriaans (2008) <papid> P08-2047 </papid>explored con8banko and etzioni abstracted over subsets of pre clustered terms, built using corpus-wide distributional frequencies strained version of the task considered here.</nextsent>
<nextsent>using manually annotated semantic relation data from semeval-2007, pre-tagged with correct argument senses, the authors chose the least common subsumer for each argument of each relation consid ered.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I450">
<title id=" E09-1092.xml">deriving generalized knowledge from corpora using wordnet abstraction </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in describing alice, system for lifelong learning, banko and etzioni (2007) gave summary of proposition abstraction algorithm developed independently that is in some ways similar to derivetypes.
</prevsent>
<prevsent>beyond differences in node scoring and their use of the first sense heuristic, the approach taken here differs in that it makes nouse of relative term frequency, nor contextual information outside particular propositional template.8 further, while we are concerned with general knowledge acquired over diverse texts, alice was built as an agent meant for constructing domain-specific theories, evaluated on 2.5million-page collection of web documents pertaining specifically to nutrition.minimizing word sense ambiguity by focusing on specific domain was later seen in thework of liakata and pulman (2008), who performed hierarchical clustering using output from their knext-like system first described in (li akata and pulman, 2002).<papid> C02-1105 </papid></prevsent>
</prevsection>
<citsent citstr=" P08-2047 ">
terminal nodes of the resultant structure were used as the basis for inferring semantic type restrictions, reminiscent of the use of cbc clusters (pantel and lin, 2002) by pantel et al (2007), <papid> N07-1071 </papid>for typing the arguments of paraphrase rules.assigning pre-compiled instances to their first sense reading in wordnet, pasca (2008) then generalized class attributes extracted for these terms, using as resource google search engine query logs.katrenko and adriaans (2008) <papid> P08-2047 </papid>explored con8banko and etzioni abstracted over subsets of pre clustered terms, built using corpus-wide distributional frequencies strained version of the task considered here.</citsent>
<aftsection>
<nextsent>using manually annotated semantic relation data from semeval-2007, pre-tagged with correct argument senses, the authors chose the least common subsumer for each argument of each relation considered.
</nextsent>
<nextsent>our approach keeps with the intuition of preferring specific over general concepts in wordnet, but allows for the handling of relations automatically discovered, whose arguments are notpre-tagged for sense and tend to be more wideranging.
</nextsent>
<nextsent>we note that the least common sub sumer for many of our predicate arguments would in most cases be far too abstract.
</nextsent>
<nextsent>as the volume of automatically acquired knowledge grows, it becomes more feasible to abstract from existential statements to stronger, more general claims on what usually obtains in the real world.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I452">
<title id=" E06-2012.xml">maytag a multi staged approach to identifying complex events in textual data </title>
<section> extraction in maytag.  </section>
<citcontext>
<prevsection>
<prevsent>3.3 statistical training.
</prevsent>
<prevsent>because we had no existing methods to address financial events or relations, we took this opportunity to develop trainable approach.
</prevsent>
</prevsection>
<citsent citstr=" P05-1053 ">
recent work has begun to address relation and event extraction through trainable means, chiefly svm classification (zelenko et al  2003, zhou et al 2005).<papid> P05-1053 </papid></citsent>
<aftsection>
<nextsent>the approach weve used here is classifier-based as well, but relies on maximum entropy modeling instead.
</nextsent>
<nextsent>most trainable approaches to event extraction are entity-anchored: given pair of relevant entities (e.g., pair of companies), the object of the endeavor is to identify the relation that holds between them (e.g., acquisition or subsidiary).
</nextsent>
<nextsent>we turn this around: starting with the head of the relation, we try to find the entities that fill its constituent roles.
</nextsent>
<nextsent>this is, unavoidably, strongly lexicalized approach.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I453">
<title id=" E09-1079.xml">tagging urdu text with parts of speech a tagger comparison </title>
<section> urdu tagset.  </section>
<citcontext>
<prevsection>
<prevsent>a tagset may consist either of general parts of speech only or it may consist of additional morpho-syntactic categories such as number, gender and case.
</prevsent>
<prevsent>in order to facilitate the tagger training and to reduce the lexical and syntactic ambiguity, we decided to concentrate on the syntactic categories of the language.
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
purely syntactic categories lead to smaller number of tags which also improves the accuracy of manual tagging2 (marcus et al, 1993).<papid> J93-2004 </papid></citsent>
<aftsection>
<nextsent>urdu is influenced from arabic, and can be considered as having three main parts of speech, namely noun, verb and particle (platts, 1909; javed, 1981; haq, 1987).
</nextsent>
<nextsent>however, some grammar ians proposed ten main parts of speech for urdu (schmidt, 1999).
</nextsent>
<nextsent>the work of urdu grammar writers provides full overview of all the features of the language.
</nextsent>
<nextsent>however, in the perspective of the tagset, their analysis is lacking the computational grounds.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I454">
<title id=" E09-1079.xml">tagging urdu text with parts of speech a tagger comparison </title>
<section> tagging methodologies.  </section>
<citcontext>
<prevsection>
<prevsent>the work on automatic part of speech tagging started in early 1960s.
</prevsent>
<prevsent>klein and simmons 693 (1963) rule based pos tagger can be considered as the first automatic tagging system.
</prevsent>
</prevsection>
<citsent citstr=" P89-1015 ">
in the rule based approach, after assigning each word its potential tags, list of hand written disambiguation rules are used to reduce the number of tags to one (klein and simmons, 1963; green and rubin, 1971; hindle, 1989; <papid> P89-1015 </papid>chanod and tapanainen 1994).</citsent>
<aftsection>
<nextsent>a rule based model has the disadvantage of requiring lots of linguistic efforts to write rules for the language.
</nextsent>
<nextsent>data-driven approaches resolve this problem by automatically extracting the information from an already tagged corpus.
</nextsent>
<nextsent>ambiguity between the tags is resolved by selecting the most likely tag for word (bahl and mercer, 1976; church, 1988; <papid> A88-1019 </papid>brill, 1992).</nextsent>
<nextsent>brills transformation based tagger uses lexical rules to assign each word the most frequent tag and then applies contextual rules over and over again to get high accuracy.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I455">
<title id=" E09-1079.xml">tagging urdu text with parts of speech a tagger comparison </title>
<section> tagging methodologies.  </section>
<citcontext>
<prevsection>
<prevsent>a rule based model has the disadvantage of requiring lots of linguistic efforts to write rules for the language.
</prevsent>
<prevsent>data-driven approaches resolve this problem by automatically extracting the information from an already tagged corpus.
</prevsent>
</prevsection>
<citsent citstr=" A88-1019 ">
ambiguity between the tags is resolved by selecting the most likely tag for word (bahl and mercer, 1976; church, 1988; <papid> A88-1019 </papid>brill, 1992).</citsent>
<aftsection>
<nextsent>brills transformation based tagger uses lexical rules to assign each word the most frequent tag and then applies contextual rules over and over again to get high accuracy.
</nextsent>
<nextsent>however, brills tagger requires training on large number of rules which reduces the efficiency of machine learning process.
</nextsent>
<nextsent>statistical approaches usually achieve an accuracy of 96%-97% (hardie, 2003: 295).
</nextsent>
<nextsent>however, statistical taggers require large training corpus to avoid data sparseness.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I456">
<title id=" E09-1079.xml">tagging urdu text with parts of speech a tagger comparison </title>
<section> tagging methodologies.  </section>
<citcontext>
<prevsection>
<prevsent>the tnt tagger is trigram hmm tagger in which the transition probability depends on two preceding tags.
</prevsent>
<prevsent>the performance of the tagger was tested on negra corpus and penn treebank corpus.
</prevsent>
</prevsection>
<citsent citstr=" A00-1031 ">
the average accuracy of the tagger is 96% to 97% (brants, 2000).<papid> A00-1031 </papid></citsent>
<aftsection>
<nextsent>the second order markov model used by the tnt tagger requires large amounts of tagged data to get reasonable frequencies of pos trigrams.
</nextsent>
<nextsent>the tnt tagger smooths the probability with linear interpolation to handle the problem of data sparseness.
</nextsent>
<nextsent>the tags of unknown words are predicted based on the word suffix.
</nextsent>
<nextsent>the longest ending string of an unknown word having one or more occurrences in the training corpus is considered as suffix.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I460">
<title id=" E09-1079.xml">tagging urdu text with parts of speech a tagger comparison </title>
<section> tagging methodologies.  </section>
<citcontext>
<prevsection>
<prevsent>the tree tagger was trained on 2 million words of the penn-treebank corpus and was evaluated on 100,000 words.
</prevsent>
<prevsent>its accuracy is compared against trigram tagger built on the same data.
</prevsent>
</prevsection>
<citsent citstr=" C94-1027 ">
the tree tagger showed an accuracy of 96.06% (schmid, 1994<papid> C94-1027 </papid>a).</citsent>
<aftsection>
<nextsent>in 2004, gimnez and mrquez proposed part of speech tagger (svm tool) based on support vector machines and reported accuracy higher than all state-of-art taggers.
</nextsent>
<nextsent>the aim of the development was to have simple, efficient, robust tagger with high accuracy.
</nextsent>
<nextsent>the support vector machine does binary classification of the data.
</nextsent>
<nextsent>it constructs an n-dimensional hyper plane that separates the data into positive and negative classes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I461">
<title id=" E09-1079.xml">tagging urdu text with parts of speech a tagger comparison </title>
<section> tagging methodologies.  </section>
<citcontext>
<prevsection>
<prevsent>the context probability of tag is the product of the probabilities of its attributes.
</prevsent>
<prevsent>the probability of an attribute given the previous tags is estimated with decision tree.
</prevsent>
</prevsection>
<citsent citstr=" C08-1098 ">
the decision tree uses different context features for the prediction of different attributes (schmid and laws, 2008).<papid> C08-1098 </papid></citsent>
<aftsection>
<nextsent>the rf tagger is well suited for languages with rich morphology and large fine grained tagset.
</nextsent>
<nextsent>the rf tagger was evaluated on the german tiger treebank and czech academic corpus which contain 700 and 1200 pos tags, respectively.
</nextsent>
<nextsent>the rf tagger achieved higher accuracy than tnt and svmtool.
</nextsent>
<nextsent>urdu is morphologically rich language.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I462">
<title id=" E09-1026.xml">semi supervised semantic role labeling </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>weseek to find role assignment in the unlabeled data such that the argument similarity between the labeled and unlabeled instances is maximized.
</prevsent>
<prevsent>experimental results on semantic role labeling show that the automatic annotations produced by our method improve performance over using hand-labeled instances alone.
</prevsent>
</prevsection>
<citsent citstr=" J02-3001 ">
recent years have seen growing interest in the task of automatically identifying and labeling the semantic roles conveyed by sentential constituents (gildea and jurafsky, 2002).<papid> J02-3001 </papid></citsent>
<aftsection>
<nextsent>this is partly due toits relevance for applications ranging from information extraction (surdeanu et al, 2003; <papid> P03-1002 </papid>moschitti et al, 2003) to question answering (shen and lapata, 2007), <papid> D07-1002 </papid>paraphrase identification (pado?</nextsent>
<nextsent>and erk, 2005), and the modeling of textual entailment relations (tatu and moldovan, 2005).<papid> H05-1047 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I463">
<title id=" E09-1026.xml">semi supervised semantic role labeling </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>experimental results on semantic role labeling show that the automatic annotations produced by our method improve performance over using hand-labeled instances alone.
</prevsent>
<prevsent>recent years have seen growing interest in the task of automatically identifying and labeling the semantic roles conveyed by sentential constituents (gildea and jurafsky, 2002).<papid> J02-3001 </papid></prevsent>
</prevsection>
<citsent citstr=" P03-1002 ">
this is partly due toits relevance for applications ranging from information extraction (surdeanu et al, 2003; <papid> P03-1002 </papid>moschitti et al, 2003) to question answering (shen and lapata, 2007), <papid> D07-1002 </papid>paraphrase identification (pado?</citsent>
<aftsection>
<nextsent>and erk, 2005), and the modeling of textual entailment relations (tatu and moldovan, 2005).<papid> H05-1047 </papid></nextsent>
<nextsent>resources like framenet (fillmore et al, 2003) and propbank (palmer et al, 2005) <papid> J05-1004 </papid>have also facilitated the development of semantic role labeling methods by providing high-quality annotations for use in training.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I464">
<title id=" E09-1026.xml">semi supervised semantic role labeling </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>experimental results on semantic role labeling show that the automatic annotations produced by our method improve performance over using hand-labeled instances alone.
</prevsent>
<prevsent>recent years have seen growing interest in the task of automatically identifying and labeling the semantic roles conveyed by sentential constituents (gildea and jurafsky, 2002).<papid> J02-3001 </papid></prevsent>
</prevsection>
<citsent citstr=" D07-1002 ">
this is partly due toits relevance for applications ranging from information extraction (surdeanu et al, 2003; <papid> P03-1002 </papid>moschitti et al, 2003) to question answering (shen and lapata, 2007), <papid> D07-1002 </papid>paraphrase identification (pado?</citsent>
<aftsection>
<nextsent>and erk, 2005), and the modeling of textual entailment relations (tatu and moldovan, 2005).<papid> H05-1047 </papid></nextsent>
<nextsent>resources like framenet (fillmore et al, 2003) and propbank (palmer et al, 2005) <papid> J05-1004 </papid>have also facilitated the development of semantic role labeling methods by providing high-quality annotations for use in training.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I465">
<title id=" E09-1026.xml">semi supervised semantic role labeling </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>recent years have seen growing interest in the task of automatically identifying and labeling the semantic roles conveyed by sentential constituents (gildea and jurafsky, 2002).<papid> J02-3001 </papid></prevsent>
<prevsent>this is partly due toits relevance for applications ranging from information extraction (surdeanu et al, 2003; <papid> P03-1002 </papid>moschitti et al, 2003) to question answering (shen and lapata, 2007), <papid> D07-1002 </papid>paraphrase identification (pado?</prevsent>
</prevsection>
<citsent citstr=" H05-1047 ">
and erk, 2005), and the modeling of textual entailment relations (tatu and moldovan, 2005).<papid> H05-1047 </papid></citsent>
<aftsection>
<nextsent>resources like framenet (fillmore et al, 2003) and propbank (palmer et al, 2005) <papid> J05-1004 </papid>have also facilitated the development of semantic role labeling methods by providing high-quality annotations for use in training.</nextsent>
<nextsent>semantic role label ers are commonly developed using supervised learning paradigm1 where classifier learns to predict role labels based on features extracted from annotated training data.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I466">
<title id=" E09-1026.xml">semi supervised semantic role labeling </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this is partly due toits relevance for applications ranging from information extraction (surdeanu et al, 2003; <papid> P03-1002 </papid>moschitti et al, 2003) to question answering (shen and lapata, 2007), <papid> D07-1002 </papid>paraphrase identification (pado?</prevsent>
<prevsent>and erk, 2005), and the modeling of textual entailment relations (tatu and moldovan, 2005).<papid> H05-1047 </papid></prevsent>
</prevsection>
<citsent citstr=" J05-1004 ">
resources like framenet (fillmore et al, 2003) and propbank (palmer et al, 2005) <papid> J05-1004 </papid>have also facilitated the development of semantic role labeling methods by providing high-quality annotations for use in training.</citsent>
<aftsection>
<nextsent>semantic role label ers are commonly developed using supervised learning paradigm1 where classifier learns to predict role labels based on features extracted from annotated training data.
</nextsent>
<nextsent>examples of the annotations provided in framenet are given in (1).
</nextsent>
<nextsent>here, the meaning of predicates (usually verbs, nouns, or adjectives) is conveyed by frames, schematic representations of situations.
</nextsent>
<nextsent>semantic roles (or frame elements) are defined for each frame and correspond to salient entities present in the situation evoked by the predicate (or frame evoking element).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I467">
<title id=" E09-1026.xml">semi supervised semantic role labeling </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the english framenet (version 1.3) contains 502 frames covering 5,866 lexical entries.
</prevsent>
<prevsent>it also comes with set of manually annotated example sentences, taken mostly from the british national corpus.
</prevsent>
</prevsection>
<citsent citstr=" W07-2018 ">
these annotations are often used 1the approaches are too numerous to list; we refer the interested reader to the proceedings of the semeval-2007shared task (baker et al, 2007) <papid> W07-2018 </papid>for an overview of the state of-the-art.</citsent>
<aftsection>
<nextsent>220as training data for semantic role labeling systems.
</nextsent>
<nextsent>however, the applicability of these systems is limited to those words for which labeled data exists, and their accuracy is strongly correlated with the amount of labeled data available.
</nextsent>
<nextsent>despite the substantial annotation effort involved in the creation of framenet (spanning approximately twelve years), the number of annotated instances varies greatly across lexical items.
</nextsent>
<nextsent>for instance, framenet contains annotations for 2,113 verbs; of these 12.3% have five or less annotated examples.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I468">
<title id=" E09-1026.xml">semi supervised semantic role labeling </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the idea here is to leverage the existing english framenet and relyon word or constituent alignments to automatically create an annotated corpus in new language.
</prevsent>
<prevsent>pado?
</prevsent>
</prevsection>
<citsent citstr=" P06-2057 ">
and lap ata (2006) transfer semantic role annotations from english onto german and johansson and nugues (2006) <papid> P06-2057 </papid>from english onto swedish.</citsent>
<aftsection>
<nextsent>a different strategy is presented in fung and chen (2004), <papid> C04-1134 </papid>where english framenet entries are mapped to concepts listed in hownet, an on-line ontology for chinese, without consulting parallel corpus.then, chinese sentences with predicates instantiating these concepts are found in monolingual corpus and their arguments are labeled with framenet roles.other work attempts to alleviate the data requirements for semantic role labeling either by relying on unsupervised learning or by extending existing resources through the use of unlabeled data.swier and stevenson (2004) <papid> W04-3213 </papid>present an unsupervised method for labeling the arguments of verbs with their semantic roles.</nextsent>
<nextsent>given verb instance, their method first selects frame from verbnet, asemantic role resource akin to framenet and propbank, and labels each argument slot with sets of possible roles.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I469">
<title id=" E09-1026.xml">semi supervised semantic role labeling </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>pado?
</prevsent>
<prevsent>and lap ata (2006) transfer semantic role annotations from english onto german and johansson and nugues (2006) <papid> P06-2057 </papid>from english onto swedish.</prevsent>
</prevsection>
<citsent citstr=" C04-1134 ">
a different strategy is presented in fung and chen (2004), <papid> C04-1134 </papid>where english framenet entries are mapped to concepts listed in hownet, an on-line ontology for chinese, without consulting parallel corpus.then, chinese sentences with predicates instantiating these concepts are found in monolingual corpus and their arguments are labeled with framenet roles.other work attempts to alleviate the data requirements for semantic role labeling either by relying on unsupervised learning or by extending existing resources through the use of unlabeled data.swier and stevenson (2004) <papid> W04-3213 </papid>present an unsupervised method for labeling the arguments of verbs with their semantic roles.</citsent>
<aftsection>
<nextsent>given verb instance, their method first selects frame from verbnet, asemantic role resource akin to framenet and propbank, and labels each argument slot with sets of possible roles.
</nextsent>
<nextsent>the algorithm proceeds iterativelyby first making initial unambiguous role assignments, and then successively updating pro babil 221 ity model on which future assignments are based.
</nextsent>
<nextsent>being unsupervised, their approach requires no manual effort other than creating the frame dictionary.
</nextsent>
<nextsent>unfortunately, existing resources do not have exhaustive coverage and large number ofverbs may be assigned no semantic role information since they are not in the dictionary in the first place.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I470">
<title id=" E09-1026.xml">semi supervised semantic role labeling </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>pado?
</prevsent>
<prevsent>and lap ata (2006) transfer semantic role annotations from english onto german and johansson and nugues (2006) <papid> P06-2057 </papid>from english onto swedish.</prevsent>
</prevsection>
<citsent citstr=" W04-3213 ">
a different strategy is presented in fung and chen (2004), <papid> C04-1134 </papid>where english framenet entries are mapped to concepts listed in hownet, an on-line ontology for chinese, without consulting parallel corpus.then, chinese sentences with predicates instantiating these concepts are found in monolingual corpus and their arguments are labeled with framenet roles.other work attempts to alleviate the data requirements for semantic role labeling either by relying on unsupervised learning or by extending existing resources through the use of unlabeled data.swier and stevenson (2004) <papid> W04-3213 </papid>present an unsupervised method for labeling the arguments of verbs with their semantic roles.</citsent>
<aftsection>
<nextsent>given verb instance, their method first selects frame from verbnet, asemantic role resource akin to framenet and propbank, and labels each argument slot with sets of possible roles.
</nextsent>
<nextsent>the algorithm proceeds iterativelyby first making initial unambiguous role assignments, and then successively updating pro babil 221 ity model on which future assignments are based.
</nextsent>
<nextsent>being unsupervised, their approach requires no manual effort other than creating the frame dictionary.
</nextsent>
<nextsent>unfortunately, existing resources do not have exhaustive coverage and large number ofverbs may be assigned no semantic role information since they are not in the dictionary in the first place.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I471">
<title id=" E09-1026.xml">semi supervised semantic role labeling </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>being unsupervised, their approach requires no manual effort other than creating the frame dictionary.
</prevsent>
<prevsent>unfortunately, existing resources do not have exhaustive coverage and large number ofverbs may be assigned no semantic role information since they are not in the dictionary in the first place.
</prevsent>
</prevsection>
<citsent citstr=" D08-1048 ">
pennacchiotti et al (2008) <papid> D08-1048 </papid>address precisely this problem by augmenting framenet with new lexical units if they are similar to an existing frame (their notion of similarity combines distributional and wordnet-based measures).</citsent>
<aftsection>
<nextsent>ina similar vein, gordon and swanson (2007) <papid> P07-1025 </papid>attempt to increase the coverage of propbank.</nextsent>
<nextsent>their approach leverages existing annotations to handle novel verbs.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I473">
<title id=" E09-1026.xml">semi supervised semantic role labeling </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>unfortunately, existing resources do not have exhaustive coverage and large number ofverbs may be assigned no semantic role information since they are not in the dictionary in the first place.
</prevsent>
<prevsent>pennacchiotti et al (2008) <papid> D08-1048 </papid>address precisely this problem by augmenting framenet with new lexical units if they are similar to an existing frame (their notion of similarity combines distributional and wordnet-based measures).</prevsent>
</prevsection>
<citsent citstr=" P07-1025 ">
ina similar vein, gordon and swanson (2007) <papid> P07-1025 </papid>attempt to increase the coverage of propbank.</citsent>
<aftsection>
<nextsent>their approach leverages existing annotations to handle novel verbs.
</nextsent>
<nextsent>rather than annotating new sentences that contain novel verbs, they find syntactically similar verbs and use their annotations as surrogate training data.
</nextsent>
<nextsent>our own work aims to reduce but not entirely eliminate the annotation effort involved in creating training data for semantic role labeling.
</nextsent>
<nextsent>we thus assume that small number of manual annotations is initially available.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I477">
<title id=" E09-1026.xml">semi supervised semantic role labeling </title>
<section> semi-supervised learning method.  </section>
<citcontext>
<prevsection>
<prevsent>all surface word forms are lemmatized.
</prevsent>
<prevsent>an example of the argument structure information we obtain forthe predicate course (see figure 1) is shown in table 1.
</prevsent>
</prevsection>
<citsent citstr=" P06-4020 ">
we obtain information about grammatical roles from the output of rasp (briscoe et al, 2006), <papid> P06-4020 </papid>broad-coverage dependency parser.</citsent>
<aftsection>
<nextsent>however,there is nothing inherent in our method that restricts us to this particular parser.
</nextsent>
<nextsent>any other parser with broadly similar dependency output could serve our purposes.
</nextsent>
<nextsent>3.2 measuring similarity.
</nextsent>
<nextsent>for each frame evoking verb in the seed corpus our method creates labeled predicate-argument representation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I479">
<title id=" E09-1026.xml">semi supervised semantic role labeling </title>
<section> experimental setup.  </section>
<citcontext>
<prevsection>
<prevsent>semantic role labeler we evaluated our method on semantic role labeling task.
</prevsent>
<prevsent>specifically, we compared the performance of generic semantic role labeler trained on the seed corpus anda larger corpus expanded with annotations produced by our method.
</prevsent>
</prevsection>
<citsent citstr=" C08-1050 ">
our semantic role labeler followed closely the implementation of johansson and nugues (2008).<papid> C08-1050 </papid></citsent>
<aftsection>
<nextsent>we extracted features from dependency parses corresponding to those routinely used in the semantic role labeling literature (see baker et al (2007) <papid> W07-2018 </papid>for an overview).svm classifiers were trained to identify the arguments and label them with appropriate roles.</nextsent>
<nextsent>for the latter we performed multi-class classification following the one-versus-one method3 (friedman, 1996).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I481">
<title id=" E89-1032.xml">an algorithm for generation in unification categorial grammar </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we indicate how the algorithm may be extended to other grammars obeying the same requirement.
</prevsent>
<prevsent>appendices contain full listing of the program and trace of execution of the algorithm.
</prevsent>
</prevsection>
<citsent citstr=" C86-1045 ">
in this paper we present an algorithm for generating sentences using unification categorial grammars (ucgs, zeevat et al 1987) but which extends to any categorial grammar with unification (e.g., categorial unification grammars, uszkoreit 1986, <papid> C86-1045 </papid>karttunen 1987).</citsent>
<aftsection>
<nextsent>we relate the algorithm to proposals by shieber (1988).<papid> C88-2128 </papid></nextsent>
<nextsent>following shieber, we address the basic generation problem; that is, given syntactic ategory and semantic representation ~, generate every possible string def med by the grammar of category with semantic representation that is logically equivalent to ~.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I482">
<title id=" E89-1032.xml">an algorithm for generation in unification categorial grammar </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>appendices contain full listing of the program and trace of execution of the algorithm.
</prevsent>
<prevsent>in this paper we present an algorithm for generating sentences using unification categorial grammars (ucgs, zeevat et al 1987) but which extends to any categorial grammar with unification (e.g., categorial unification grammars, uszkoreit 1986, <papid> C86-1045 </papid>karttunen 1987).</prevsent>
</prevsection>
<citsent citstr=" C88-2128 ">
we relate the algorithm to proposals by shieber (1988).<papid> C88-2128 </papid></citsent>
<aftsection>
<nextsent>following shieber, we address the basic generation problem; that is, given syntactic ategory and semantic representation ~, generate every possible string def med by the grammar of category with semantic representation that is logically equivalent to ~.
</nextsent>
<nextsent>in more concrete terms, this means that we dispense with any planning component and directly address the intrinsic complexity of the basic generation problem.
</nextsent>
<nextsent>the development of such algorithms as fundamentally important as the corresponding work on parsing algorithms.
</nextsent>
<nextsent>we also discuss the properties of semantic representation language (srl) and the manner of its construction which makes our algorithm effective.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I500">
<title id=" E09-1022.xml">learning to interpret utterances using dialogue history </title>
<section> learning pragmatic interpretation.  </section>
<citcontext>
<prevsection>
<prevsent>we also represent each intention it,j , observation o, and state sk in terms of features.
</prevsent>
<prevsent>we seek to learn function (f = correct | features(it,j), features(o), features(sk)) from set of training examples = {e1, ..., en} where, for = 1..n, we have: el = ( = fate(it,j), features(it,j), features(o), features(sk)).
</prevsent>
</prevsection>
<citsent citstr=" J96-1002 ">
we chose to train maximum entropy models (berger et al, 1996).<papid> J96-1002 </papid></citsent>
<aftsection>
<nextsent>our learning framework is described in section 4.1; the results in section 4.2.
</nextsent>
<nextsent>4.1 learning setup.
</nextsent>
<nextsent>we defined range of potentially useful features,which we list in figures 5, 6, and 7.
</nextsent>
<nextsent>these features formalize pragmatic distinctions that plausibly provide evidence of the correct interpretation for user utterance or action.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I501">
<title id=" E09-1022.xml">learning to interpret utterances using dialogue history </title>
<section> results in context.  </section>
<citcontext>
<prevsection>
<prevsent>our framework augments this information about utterance context with additional evidence about meaning from linguistic interaction.
</prevsent>
<prevsent>in general, dialogue coherence is an important source of evidence for all aspects of language, for both human language learning (saxtonet al, 2005) as well as machine models.
</prevsent>
</prevsection>
<citsent citstr=" W08-0103 ">
for example, bohus et al (2008) <papid> W08-0103 </papid>use users?</citsent>
<aftsection>
<nextsent>confirmations of their spoken requests in multi-modal interface to tune the systems asr rankings for recognizing subsequent utterances.
</nextsent>
<nextsent>our work to date has number of limitations.
</nextsent>
<nextsent>first, although 318 ambiguous interpretations did occur, this user study provided relatively small number of ambiguous interpretations, in machine learning terms; and most (80.2%) of those that did occur were 2-way ambiguities.
</nextsent>
<nextsent>a richer domain would require both more data and generative approach to model-building and search.second, this learning experiment has been performed after the fact, and we have not yet investigated the performance of the learned model in follow-up experiment in which coref uses the learned model in interactions with its users.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I502">
<title id=" E91-1028.xml">generating referring expressions involving relations </title>
<section> abstract </section>
<citcontext>
<prevsection>


</prevsection>
<citsent citstr=" P89-1009 ">
in this paper, we review dale \[1989\] <papid> P89-1009 </papid>algorithm for determining the content of referring expres- sion.</citsent>
<aftsection>
<nextsent>the algorithm, which only permits the use of one-place predicates, is revised and extended to deal with n-ary predicates.
</nextsent>
<nextsent>we investigate the problem of blocking  recur sion  in complex noun phrases and propose solution in the context of our algorithm.
</nextsent>
<nextsent>in very simple language generation systems, there is typically one-to-one relationship between en-tities known to the system and the linguistic forms available for describing those entities; in effect, each entity has canonical name.
</nextsent>
<nextsent>in such sys-tems, deciding upon the form of reference quired in given context requires at most choosing be- twc(,n pronoun and the canonical name.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I503">
<title id=" E09-1020.xml">an alignment algorithm using belief propagation and a structure based distortion model </title>
<section> introduction and related work.  </section>
<citcontext>
<prevsection>
<prevsent>this model(and even more so its subsequents variations), although simple, do not have computationally efficient procedure for an exact em-based training.however, we will give some theoretical and empirical evidences that loopy belief propagation can give us good approximation procedure.
</prevsent>
<prevsent>although we do not have the space to review the many alignment systems that have already been proposed, we will shortly refer to works that share some similarities with our approach.
</prevsent>
</prevsection>
<citsent citstr=" J00-2004 ">
in particular, the first alignment model we will present has already been described in (melamed, 2000).<papid> J00-2004 </papid></citsent>
<aftsection>
<nextsent>we differ however in the training and decoding procedure we propose.
</nextsent>
<nextsent>the problem of making use of syntactic trees for alignment (and translation), which is the object of our second alignment model has already received some attention, notably by (yamada and knight, 2001) <papid> P01-1067 </papid>and (gildea, 2003) <papid> P03-1011 </papid></nextsent>
<nextsent>in this paper, we will make several use of factor graphs.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I505">
<title id=" E09-1020.xml">an alignment algorithm using belief propagation and a structure based distortion model </title>
<section> introduction and related work.  </section>
<citcontext>
<prevsection>
<prevsent>in particular, the first alignment model we will present has already been described in (melamed, 2000).<papid> J00-2004 </papid></prevsent>
<prevsent>we differ however in the training and decoding procedure we propose.</prevsent>
</prevsection>
<citsent citstr=" P01-1067 ">
the problem of making use of syntactic trees for alignment (and translation), which is the object of our second alignment model has already received some attention, notably by (yamada and knight, 2001) <papid> P01-1067 </papid>and (gildea, 2003) <papid> P03-1011 </papid></citsent>
<aftsection>
<nextsent>in this paper, we will make several use of factor graphs.
</nextsent>
<nextsent>a factor graph is graphical model, much like bayesian network.
</nextsent>
<nextsent>the three most common types of graphical models (factor graphs, bayesian network and markov network) share the same purpose: intuitively, they allow to represent the dependencies among random variables; mathematically, they represent facto riza tion of the joint probability of these variables.
</nextsent>
<nextsent>formally, factor graph is bipartite graph with 2 kinds of nodes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I506">
<title id=" E09-1020.xml">an alignment algorithm using belief propagation and a structure based distortion model </title>
<section> introduction and related work.  </section>
<citcontext>
<prevsection>
<prevsent>in particular, the first alignment model we will present has already been described in (melamed, 2000).<papid> J00-2004 </papid></prevsent>
<prevsent>we differ however in the training and decoding procedure we propose.</prevsent>
</prevsection>
<citsent citstr=" P03-1011 ">
the problem of making use of syntactic trees for alignment (and translation), which is the object of our second alignment model has already received some attention, notably by (yamada and knight, 2001) <papid> P01-1067 </papid>and (gildea, 2003) <papid> P03-1011 </papid></citsent>
<aftsection>
<nextsent>in this paper, we will make several use of factor graphs.
</nextsent>
<nextsent>a factor graph is graphical model, much like bayesian network.
</nextsent>
<nextsent>the three most common types of graphical models (factor graphs, bayesian network and markov network) share the same purpose: intuitively, they allow to represent the dependencies among random variables; mathematically, they represent facto riza tion of the joint probability of these variables.
</nextsent>
<nextsent>formally, factor graph is bipartite graph with 2 kinds of nodes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I513">
<title id=" E09-1020.xml">an alignment algorithm using belief propagation and a structure based distortion model </title>
<section> the mono link model.  </section>
<citcontext>
<prevsection>
<prevsent>we evaluated the mono link algorithm with two languages pairs: french-english and japanese english.
</prevsent>
<prevsent>169 for the english-french pair, we used 200,000sentence pairs extracted from the hansard corpus (germann, 2001).
</prevsent>
</prevsection>
<citsent citstr=" W03-0301 ">
evaluation was done with the scripts and gold standard provided during the workshop hlt-naacl 20031 (mihalcea and pedersen, 2003).<papid> W03-0301 </papid></citsent>
<aftsection>
<nextsent>null links are not considered for the evaluation.
</nextsent>
<nextsent>for the english-japanese evaluation, we used 100,000 sentence pairs extracted from corpus of english/japanese news.
</nextsent>
<nextsent>we used 1000 sentence pairs extracted from pre-aligned data(utiyama and isahara, 2003) <papid> P03-1010 </papid>as gold standard.</nextsent>
<nextsent>we segmented all the japanese data with the automatic segmenter juman (kurohashi and nagao, 1994).<papid> J94-4001 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I514">
<title id=" E09-1020.xml">an alignment algorithm using belief propagation and a structure based distortion model </title>
<section> the mono link model.  </section>
<citcontext>
<prevsection>
<prevsent>null links are not considered for the evaluation.
</prevsent>
<prevsent>for the english-japanese evaluation, we used 100,000 sentence pairs extracted from corpus of english/japanese news.
</prevsent>
</prevsection>
<citsent citstr=" P03-1010 ">
we used 1000 sentence pairs extracted from pre-aligned data(utiyama and isahara, 2003) <papid> P03-1010 </papid>as gold standard.</citsent>
<aftsection>
<nextsent>we segmented all the japanese data with the automatic segmenter juman (kurohashi and nagao, 1994).<papid> J94-4001 </papid></nextsent>
<nextsent>there is caveat to this evaluation, though.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I515">
<title id=" E09-1020.xml">an alignment algorithm using belief propagation and a structure based distortion model </title>
<section> the mono link model.  </section>
<citcontext>
<prevsection>
<prevsent>for the english-japanese evaluation, we used 100,000 sentence pairs extracted from corpus of english/japanese news.
</prevsent>
<prevsent>we used 1000 sentence pairs extracted from pre-aligned data(utiyama and isahara, 2003) <papid> P03-1010 </papid>as gold standard.</prevsent>
</prevsection>
<citsent citstr=" J94-4001 ">
we segmented all the japanese data with the automatic segmenter juman (kurohashi and nagao, 1994).<papid> J94-4001 </papid></citsent>
<aftsection>
<nextsent>there is caveat to this evaluation, though.
</nextsent>
<nextsent>the reason is that the segmentation and alignment scheme used in our gold standard is not very fine-grained:mostly, big chunks of the japanese sentence covering several words are aligned to big chunks of the english sentence.
</nextsent>
<nextsent>for the evaluation, we had to consider that when two chunks are aligned, there is link between every pair of words belonging toeach chunk.
</nextsent>
<nextsent>a consequence is that our gold standard will contain lot more links than it should, some of them not relevants.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I518">
<title id=" E09-1020.xml">an alignment algorithm using belief propagation and a structure based distortion model </title>
<section> adding distortion through structure.  </section>
<citcontext>
<prevsection>
<prevsent>the evaluation setting is the same as in the previous section.
</prevsent>
<prevsent>we created syntactic trees for everysentences.
</prevsent>
</prevsection>
<citsent citstr=" J03-4003 ">
for english,we used the dan bikel implementation of the collins parser (collins, 2003).<papid> J03-4003 </papid></citsent>
<aftsection>
<nextsent>for french, the sygmart parser (chauche?,1984) <papid> P84-1004 </papid>and for japanese, the knp parser (kuro hashi and nagao, 1994).<papid> J94-4001 </papid></nextsent>
<nextsent>the line sdm:parsing (sdm standing for structure-based distortion monolink?)</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I519">
<title id=" E09-1020.xml">an alignment algorithm using belief propagation and a structure based distortion model </title>
<section> adding distortion through structure.  </section>
<citcontext>
<prevsection>
<prevsent>we created syntactic trees for everysentences.
</prevsent>
<prevsent>for english,we used the dan bikel implementation of the collins parser (collins, 2003).<papid> J03-4003 </papid></prevsent>
</prevsection>
<citsent citstr=" P84-1004 ">
for french, the sygmart parser (chauche?,1984) <papid> P84-1004 </papid>and for japanese, the knp parser (kuro hashi and nagao, 1994).<papid> J94-4001 </papid></citsent>
<aftsection>
<nextsent>the line sdm:parsing (sdm standing for structure-based distortion monolink?)
</nextsent>
<nextsent>shows the results obtained by using p-sets from the trees produced by these parsers.
</nextsent>
<nextsent>the line sdm:adjacency shows results obtained by using adjacent positionsp-sets ,as described at the end of the previous section (therefore, sdm:adjacency do not use any parser).
</nextsent>
<nextsent>several interesting observations can be made from the results.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I521">
<title id=" E09-1007.xml">clique based clustering for improving named entity recognition systems </title>
<section> description of the system.  </section>
<citcontext>
<prevsection>
<prevsent>to construct the distributional space associated to corpus, we use robust parser (in our experiments, we used xip parser (at et al, 2002)) to extract chunks (i.e. nouns, noun phrases, . . .
</prevsent>
<prevsent>) and syntactic dependencies between these chunks.given this parsers output, we identify triple instances.
</prevsent>
</prevsection>
<citsent citstr=" M98-1006 ">
each triple has the form w1.r.w2 where w1 and w2 are chunks and is syntactic relation (lin, 1998), (<papid> M98-1006 </papid>kilgarriff et al, 2004).</citsent>
<aftsection>
<nextsent>one triple gives two contexts (1.w1.r and 2.w2.r) and two chunks (w1 and w2).
</nextsent>
<nextsent>then, we only select chunks which belong to ne.
</nextsent>
<nextsent>each point in the distributional space is ne and each dimension is syntactic context.
</nextsent>
<nextsent>ct denotes the set of all syntactic contexts and |ct| represents its cardinal.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I522">
<title id=" E09-1007.xml">clique based clustering for improving named entity recognition systems </title>
<section> description of the system.  </section>
<citcontext>
<prevsection>
<prevsent>fortunately, it also leads to distribution of the clus ters?
</prevsent>
<prevsent>size (number of cliques by cluster) which is 6for data fusion tasks in information retrieval field, the scoring method in equation (7) is denoted combmnz (fox and shaw, 1994).
</prevsent>
</prevsection>
<citsent citstr=" J01-1005 ">
other scoring approaches can be used see for example (cucchiarelli and velardi, 2001).<papid> J01-1005 </papid></citsent>
<aftsection>
<nextsent>55 similar to zipf distribution.
</nextsent>
<nextsent>consequently, in our experiments, if we annotate the 100 biggest clusters, we annotate around eighty percent of the detected nes (see 3).
</nextsent>
<nextsent>automatic annotation of clusters we suppose in this context that many nes in ne are already annotated.
</nextsent>
<nextsent>thus, under this assumption, we have in each cluster provided by the cbc system, both annotated and non-annotated nes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I523">
<title id=" E09-1007.xml">clique based clustering for improving named entity recognition systems </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>if two ne occurrences are contiguous and have the same annotation, we merge the two nes in one ne occurrence.
</prevsent>
<prevsent>the system described in this paper rather targetcorpus-specific ne annotation.
</prevsent>
</prevsection>
<citsent citstr=" C04-1122 ">
therefore, our ex 56 peri ments will deal with corpus of recent news articles (see (shinyama and sekine, 2004) <papid> C04-1122 </papid>for motivations regarding our corpus choice) rather than well-known annotated corpora.</citsent>
<aftsection>
<nextsent>our corpus is constituted of news in english published on the web during two weeks in june 2008.
</nextsent>
<nextsent>this corpus is constituted of around 300,000 words(10mb) which doesnt represent very large corpus.
</nextsent>
<nextsent>these texts were taken from various press sources and they involve different themes (sports,technology, . . .
</nextsent>
<nextsent>we extracted randomly subset of articles and manually annotated 916 nes (inour experiments, we deal with three types of annotation namely  person ,  organization  and  location ).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I524">
<title id=" E09-1007.xml">clique based clustering for improving named entity recognition systems </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>stanford ner (or in short stanford) associated to the following model provided by the tool and which was trained on different news systems prec.
</prevsent>
<prevsent>rec.
</prevsent>
</prevsection>
<citsent citstr=" P05-1045 ">
f-me. 1 cbc-ner system 71.67 23.47 35.36 cbc-ner system 70.66 32.86 44.86 2 xip ner 77.77 56.55 65.48 xip + cbc 78.41 60.26 68.15 xip + cbc 76.31 60.48 67.48 3 stanford ner 67.94 68.01 67.97 stanford + cbc 69.40 71.07 70.23 stanford + cbc 70.09 72.93 71.48 4 gate ner 63.30 56.88 59.92 gate + cbc 66.43 61.79 64.03 gate + cbc 66.51 63.10 64.76 5 stanford + xip 72.85 75.87 74.33 stanford + xip + cbc 72.94 77.70 75.24 stanford + xip + cbc 73.55 78.93 76.15 6 gate + xip 69.38 66.04 67.67 gate + xip + cbc 69.62 67.79 68.69 gate + xip + cbc 69.87 69.10 69.48 7 gate + stanford 63.12 69.32 66.07 gate + stanford + cbc 65.09 72.05 68.39 gate + stanford + cbc 65.66 73.25 69.25 table 1: results given by different hybrid ner systems and coupled with the cbc-ner system corpora (conll, muc6, muc7 and ace): ner-eng-ie.crf-3-all2008-distsim.ser.gz (finkel et al, 2005) (<papid> P05-1045 </papid>line 3 in table 1), ? gate ner or in short gate (cunningham et al., 2002) (<papid> P02-1022 </papid>line 4 in table 1), ? and several hybrid systems which are given by the combination of pairs taken among the set of the three last-mentioned ner systems (lines 5 to 7 in table 1).</citsent>
<aftsection>
<nextsent>notice that these baseline hybrid systems use the annotation combination process described in 2.6.1.in table 1 we first reported in each line, there sults given by each system when they are applied alone (figures in italics).
</nextsent>
<nextsent>these performances represent our baselines.
</nextsent>
<nextsent>second, we tested for each baseline system, an extended hybrid system that integrates the cbc-ner systems (with respect to the combination process detailed in 2.6.2).
</nextsent>
<nextsent>the first two lines of table 1 show that the two cbc-ner systems alone lead to rather poor results.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I525">
<title id=" E09-1007.xml">clique based clustering for improving named entity recognition systems </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>stanford ner (or in short stanford) associated to the following model provided by the tool and which was trained on different news systems prec.
</prevsent>
<prevsent>rec.
</prevsent>
</prevsection>
<citsent citstr=" P02-1022 ">
f-me. 1 cbc-ner system 71.67 23.47 35.36 cbc-ner system 70.66 32.86 44.86 2 xip ner 77.77 56.55 65.48 xip + cbc 78.41 60.26 68.15 xip + cbc 76.31 60.48 67.48 3 stanford ner 67.94 68.01 67.97 stanford + cbc 69.40 71.07 70.23 stanford + cbc 70.09 72.93 71.48 4 gate ner 63.30 56.88 59.92 gate + cbc 66.43 61.79 64.03 gate + cbc 66.51 63.10 64.76 5 stanford + xip 72.85 75.87 74.33 stanford + xip + cbc 72.94 77.70 75.24 stanford + xip + cbc 73.55 78.93 76.15 6 gate + xip 69.38 66.04 67.67 gate + xip + cbc 69.62 67.79 68.69 gate + xip + cbc 69.87 69.10 69.48 7 gate + stanford 63.12 69.32 66.07 gate + stanford + cbc 65.09 72.05 68.39 gate + stanford + cbc 65.66 73.25 69.25 table 1: results given by different hybrid ner systems and coupled with the cbc-ner system corpora (conll, muc6, muc7 and ace): ner-eng-ie.crf-3-all2008-distsim.ser.gz (finkel et al, 2005) (<papid> P05-1045 </papid>line 3 in table 1), ? gate ner or in short gate (cunningham et al., 2002) (<papid> P02-1022 </papid>line 4 in table 1), ? and several hybrid systems which are given by the combination of pairs taken among the set of the three last-mentioned ner systems (lines 5 to 7 in table 1).</citsent>
<aftsection>
<nextsent>notice that these baseline hybrid systems use the annotation combination process described in 2.6.1.in table 1 we first reported in each line, there sults given by each system when they are applied alone (figures in italics).
</nextsent>
<nextsent>these performances represent our baselines.
</nextsent>
<nextsent>second, we tested for each baseline system, an extended hybrid system that integrates the cbc-ner systems (with respect to the combination process detailed in 2.6.2).
</nextsent>
<nextsent>the first two lines of table 1 show that the two cbc-ner systems alone lead to rather poor results.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I526">
<title id=" E09-1007.xml">clique based clustering for improving named entity recognition systems </title>
<section> related works.  </section>
<citcontext>
<prevsection>
<prevsent>whereas stanford + xip + cbc allowed to give the annotation  person .
</prevsent>
<prevsent>many previous works exist innes recognition and classification.
</prevsent>
</prevsection>
<citsent citstr=" D07-1074 ">
however, most of them do not build nes resource but exploit external gazette ers (bunescu and pasca, 2006), (cucerzan, 2007).<papid> D07-1074 </papid></citsent>
<aftsection>
<nextsent>a recent overview of the field is given in(nadeau and sekine, 2007).
</nextsent>
<nextsent>according to this paper, we can classify our method in the category of semi-supervised approaches.
</nextsent>
<nextsent>our proposal is close to (cucchiarelli and velardi, 2001) <papid> J01-1005 </papid>as it uses syntactic relations (2.2) and as it relies on existing ner systems (2.6.2).</nextsent>
<nextsent>however, the particularity of our method concerns the clustering of 9except for xip+cbc in line 2 where the precision is slightly lower than xips one.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I529">
<title id=" E09-1067.xml">discovering global patterns in linguistic networks through spectral analysis a case study of the consonant inventories </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>language and the associated socio-cognitive phenomena can be modeled as networks, where the nodes correspond to linguistic entities and the edges denote the pairwise interaction or relationship between these entities.
</prevsent>
<prevsent>the study of linguistic networks has been quite popular in the recent times and has provided us with several interesting insights into the nature of language (seechoudhury and mukherjee (to appear) for an extensive survey).
</prevsent>
</prevsection>
<citsent citstr=" C08-1076 ">
examples include study of the wordnet (sigman and cecchi, 2002), syntactic dependency network of words (ferrer-i-cancho,2005) and network of co-occurrence of consonants in sound inventories (mukherjee et al , 2008; <papid> C08-1076 </papid>mukherjee et al , 2007).this research has been conducted during the authors internship at microsoft research india.most of the existing studies on linguistic networks, however, focus only on the local structural properties such as the degree and clustering coefficient of the nodes, and shortest paths between pairs of nodes.</citsent>
<aftsection>
<nextsent>on the other hand, although it is well known fact that the spectrum of network can provide important information about its global structure, the use of this powerful mathematical machinery to infer global patterns in linguistic networks is rarely found in the literature.
</nextsent>
<nextsent>note that spectral analysis, however, has been successfully employed in the domains of biological and social networks (farkas et al , 2001; gkantsidis et al , 2003; banerjee and jost, 2007).
</nextsent>
<nextsent>in the context of linguistic networks, (belkin and goldsmith, 2002) <papid> W02-0605 </papid>is the only work we are aware of that analyzes the eigenvectors to obtain two dimensional visualize of the network.</nextsent>
<nextsent>nevertheless, the work does not study the spectrum of the graph.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I530">
<title id=" E09-1067.xml">discovering global patterns in linguistic networks through spectral analysis a case study of the consonant inventories </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>on the other hand, although it is well known fact that the spectrum of network can provide important information about its global structure, the use of this powerful mathematical machinery to infer global patterns in linguistic networks is rarely found in the literature.
</prevsent>
<prevsent>note that spectral analysis, however, has been successfully employed in the domains of biological and social networks (farkas et al , 2001; gkantsidis et al , 2003; banerjee and jost, 2007).
</prevsent>
</prevsection>
<citsent citstr=" W02-0605 ">
in the context of linguistic networks, (belkin and goldsmith, 2002) <papid> W02-0605 </papid>is the only work we are aware of that analyzes the eigenvectors to obtain two dimensional visualize of the network.</citsent>
<aftsection>
<nextsent>nevertheless, the work does not study the spectrum of the graph.
</nextsent>
<nextsent>the aim of the present work is to demonstrate the use of spectral analysis for discovering the global patterns in linguistic networks.
</nextsent>
<nextsent>these patterns, in turn, are then interpreted in the light of existing linguistic theories to gather deeper insights into the nature of the underlying linguistic phenomena.
</nextsent>
<nextsent>we apply this rather generic technique to find the principles that are responsible for shaping the consonant inventories, which is well researched problem in phonology since 1931 (tru betzkoy, 1931; lind blom and maddieson, 1988; boersma, 1998; clements, 2008).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I531">
<title id=" E09-1067.xml">discovering global patterns in linguistic networks through spectral analysis a case study of the consonant inventories </title>
<section> consonant co-occurrence network.  </section>
<citcontext>
<prevsection>
<prevsent>in contrast, these inventories show exceptionally regular patterns across the languages of the world, which is in fact, common point of consensus in phonology.
</prevsent>
<prevsent>right from the beginning of the 20th century, there have been large number of linguistically motivated attempts (tru betzkoy, 1969; lind blom and maddieson, 1988; boersma, 1998; clements, 2008) to explain the formation of these patterns across the consonantinventories.
</prevsent>
</prevsection>
<citsent citstr=" P06-2017 ">
more recently, mukherjee and his colleagues (choudhury et al , 2006; <papid> P06-2017 </papid>mukherjee et al , 2007; mukherjee et al , 2008) <papid> C08-1076 </papid>studied this problem in the framework of complex networks.</citsent>
<aftsection>
<nextsent>since here we shall conduct spectral analysis of the network defined in mukherjee et al  (2007), we briefly survey the models and the important results of their work.
</nextsent>
<nextsent>choudhury et al  (2006) <papid> P06-2017 </papid>introduced bipartitenetwork model for the consonant inventories.</nextsent>
<nextsent>formally, set of consonant inventories is represented as graph = vl, vc , elc?, where the nodes in one partition correspond to the languages (vl) andthat in the other partition correspond to the consonants (vc).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I538">
<title id=" E12-1024.xml">a probabilistic model of syntactic and semantic acquisition from child directed utterances and their meanings </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we evaluate the learnt grammar in three ways.
</prevsent>
<prevsent>first, we test the accuracy of the trained model in parsing unseen utterances onto gold standard annotations of their meaning.
</prevsent>
</prevsection>
<citsent citstr=" D10-1119 ">
we show that it outperforms state-of-the-art semantic parser (kwiatkowski et al 2010) <papid> D10-1119 </papid>when run with similar training conditions (i.e., neither system is given the corpus based initialization originally used by kwiatkowski et al. we then examine the learning curves of some individual words, showing thatthe model can learn word meanings on the basis of single exposure, similar to the fast mapping phenomenon observed in children (carey and bartlett, 1978).</citsent>
<aftsection>
<nextsent>finally, we show that our1similar to referential uncertainty but relating to propositions rather than referents.
</nextsent>
<nextsent>234 learner captures the step-like learning curves for word order regularities that thornton and tesan(2007) claim children show.
</nextsent>
<nextsent>this result counters thornton and tesans criticism of statistical grammar learners that they tend to exhibit gradual learning curves rather than the abrupt changes in linguistic competence observed in children.
</nextsent>
<nextsent>1.1 related work.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I539">
<title id=" E12-1024.xml">a probabilistic model of syntactic and semantic acquisition from child directed utterances and their meanings </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>models of child word learning have focused on semantics only, learning word meanings from utterances paired with either sets of concept symbols (yu and ballard, 2007; frank et al 2008; fazly et al 2010) or compositional meaning representation of the type used here (siskind, 1996).
</prevsent>
<prevsent>the models of alishahi and stevenson (2008)and maurits et al(2009) learn, as well as word meanings, orderings for verb-argument structures but not the full parsing model that we learn here.
</prevsent>
</prevsection>
<citsent citstr=" N06-1056 ">
semantic parser induction as addressed by zettlemoyer and collins (2005, 2007, 2009), kate and mooney (2007), wong and mooney (2006), <papid> N06-1056 </papid>wong and mooney (2007), <papid> P07-1121 </papid>lu et al(2008), <papid> D08-1082 </papid>chen et al(2010), kwiatkowski et al(2010)<papid> D10-1119 </papid>kwiatkowski et al(2011) <papid> D11-1140 </papid>and borschinger et al(2011) has the same task definition as theone addressed by this paper.</citsent>
<aftsection>
<nextsent>however, the learning approaches presented in those previous pa 2this linguistic use of the term parameter?
</nextsent>
<nextsent>is distinct from the statistical use found elsewhere in this paper.
</nextsent>
<nextsent>pers are not designed to be cognitively plausible, using batch training algorithms, multiple passes over the data, and language specific initialisations(lists of noun phrases and additional corpus statis tics), all of which we dispense with here.
</nextsent>
<nextsent>in particular, our approach is closely related that of kwiatkowski et al(2010)<papid> D10-1119 </papid>but, whereas that work required careful initial isation and multiple passes over the training data to learn discriminative parsing model, here we learn generative parsing model without either.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I540">
<title id=" E12-1024.xml">a probabilistic model of syntactic and semantic acquisition from child directed utterances and their meanings </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>models of child word learning have focused on semantics only, learning word meanings from utterances paired with either sets of concept symbols (yu and ballard, 2007; frank et al 2008; fazly et al 2010) or compositional meaning representation of the type used here (siskind, 1996).
</prevsent>
<prevsent>the models of alishahi and stevenson (2008)and maurits et al(2009) learn, as well as word meanings, orderings for verb-argument structures but not the full parsing model that we learn here.
</prevsent>
</prevsection>
<citsent citstr=" P07-1121 ">
semantic parser induction as addressed by zettlemoyer and collins (2005, 2007, 2009), kate and mooney (2007), wong and mooney (2006), <papid> N06-1056 </papid>wong and mooney (2007), <papid> P07-1121 </papid>lu et al(2008), <papid> D08-1082 </papid>chen et al(2010), kwiatkowski et al(2010)<papid> D10-1119 </papid>kwiatkowski et al(2011) <papid> D11-1140 </papid>and borschinger et al(2011) has the same task definition as theone addressed by this paper.</citsent>
<aftsection>
<nextsent>however, the learning approaches presented in those previous pa 2this linguistic use of the term parameter?
</nextsent>
<nextsent>is distinct from the statistical use found elsewhere in this paper.
</nextsent>
<nextsent>pers are not designed to be cognitively plausible, using batch training algorithms, multiple passes over the data, and language specific initialisations(lists of noun phrases and additional corpus statis tics), all of which we dispense with here.
</nextsent>
<nextsent>in particular, our approach is closely related that of kwiatkowski et al(2010)<papid> D10-1119 </papid>but, whereas that work required careful initial isation and multiple passes over the training data to learn discriminative parsing model, here we learn generative parsing model without either.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I541">
<title id=" E12-1024.xml">a probabilistic model of syntactic and semantic acquisition from child directed utterances and their meanings </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>models of child word learning have focused on semantics only, learning word meanings from utterances paired with either sets of concept symbols (yu and ballard, 2007; frank et al 2008; fazly et al 2010) or compositional meaning representation of the type used here (siskind, 1996).
</prevsent>
<prevsent>the models of alishahi and stevenson (2008)and maurits et al(2009) learn, as well as word meanings, orderings for verb-argument structures but not the full parsing model that we learn here.
</prevsent>
</prevsection>
<citsent citstr=" D08-1082 ">
semantic parser induction as addressed by zettlemoyer and collins (2005, 2007, 2009), kate and mooney (2007), wong and mooney (2006), <papid> N06-1056 </papid>wong and mooney (2007), <papid> P07-1121 </papid>lu et al(2008), <papid> D08-1082 </papid>chen et al(2010), kwiatkowski et al(2010)<papid> D10-1119 </papid>kwiatkowski et al(2011) <papid> D11-1140 </papid>and borschinger et al(2011) has the same task definition as theone addressed by this paper.</citsent>
<aftsection>
<nextsent>however, the learning approaches presented in those previous pa 2this linguistic use of the term parameter?
</nextsent>
<nextsent>is distinct from the statistical use found elsewhere in this paper.
</nextsent>
<nextsent>pers are not designed to be cognitively plausible, using batch training algorithms, multiple passes over the data, and language specific initialisations(lists of noun phrases and additional corpus statis tics), all of which we dispense with here.
</nextsent>
<nextsent>in particular, our approach is closely related that of kwiatkowski et al(2010)<papid> D10-1119 </papid>but, whereas that work required careful initial isation and multiple passes over the training data to learn discriminative parsing model, here we learn generative parsing model without either.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I544">
<title id=" E12-1024.xml">a probabilistic model of syntactic and semantic acquisition from child directed utterances and their meanings </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>models of child word learning have focused on semantics only, learning word meanings from utterances paired with either sets of concept symbols (yu and ballard, 2007; frank et al 2008; fazly et al 2010) or compositional meaning representation of the type used here (siskind, 1996).
</prevsent>
<prevsent>the models of alishahi and stevenson (2008)and maurits et al(2009) learn, as well as word meanings, orderings for verb-argument structures but not the full parsing model that we learn here.
</prevsent>
</prevsection>
<citsent citstr=" D11-1140 ">
semantic parser induction as addressed by zettlemoyer and collins (2005, 2007, 2009), kate and mooney (2007), wong and mooney (2006), <papid> N06-1056 </papid>wong and mooney (2007), <papid> P07-1121 </papid>lu et al(2008), <papid> D08-1082 </papid>chen et al(2010), kwiatkowski et al(2010)<papid> D10-1119 </papid>kwiatkowski et al(2011) <papid> D11-1140 </papid>and borschinger et al(2011) has the same task definition as theone addressed by this paper.</citsent>
<aftsection>
<nextsent>however, the learning approaches presented in those previous pa 2this linguistic use of the term parameter?
</nextsent>
<nextsent>is distinct from the statistical use found elsewhere in this paper.
</nextsent>
<nextsent>pers are not designed to be cognitively plausible, using batch training algorithms, multiple passes over the data, and language specific initialisations(lists of noun phrases and additional corpus statis tics), all of which we dispense with here.
</nextsent>
<nextsent>in particular, our approach is closely related that of kwiatkowski et al(2010)<papid> D10-1119 </papid>but, whereas that work required careful initial isation and multiple passes over the training data to learn discriminative parsing model, here we learn generative parsing model without either.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I554">
<title id=" E09-3002.xml">a memory based approach to the treatment of serial verb construction in combinatory categorial grammar </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>its combinatory operations resolve deletion under coordination, such as right node raising (sv&svo;) and gapping (svo&so;).in case of gapping, specialized rule called decomposition is used to handle with forward gapping (steedman, 1990) by extracting the filler required by gap from complete constituent.however, serial verb construction is challenging topic in ccg when we expand our attention to more analytic languages, such as chinese and thai, whose degree of pro-dropping is more free.
</prevsent>
<prevsent>in this paper, explain how we can deal with serial verb construction with ccg by incorporating memory mechanism and how we can restrict the generative power of the resulted hybrid.
</prevsent>
</prevsection>
<citsent citstr=" I08-1011 ">
the integrated memory mechanism is motivated by anaphoric resolution mechanism in categorial type logic (hendriks, 1995; moortgat, 1997), type logical grammar (morrill, 1994; jager, 1997; jager, 2001; oehrle, 2007), and ccg(jacobson, 1999), and gap resolution in memory inductive categorial grammar (boonkwan and supnithi, 2008), <papid> I08-1011 </papid>as it is designed for associating fillers and gaps found in an input sentence.</citsent>
<aftsection>
<nextsent>theoretically, discuss how this hybrid efficiently helps us deal with serial verb construction and how far the generative power grows after incorporating the memory mechanism.outline: introduce ccg in 2, and then motivate the need of memory mechanism in dealing with serial verb construction in ccg in 3.
</nextsent>
<nextsent>i describe the hybrid model of ccg and the filler-gapmemory in 4.
</nextsent>
<nextsent>i then discuss the margin of generative power introduced by the memory mechanism in 5.
</nextsent>
<nextsent>finally, conclude this paper in 6.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I557">
<title id=" E09-3002.xml">a memory based approach to the treatment of serial verb construction in combinatory categorial grammar </title>
<section> generative power.  </section>
<citcontext>
<prevsection>
<prevsent>x[?(??)]it is obvious that the rules in (35) are not lig pro duction; that is, ccg-mm cannot be generated byany ligs; or more precisely, ccg-mm is properly more powerful than ccg.
</prevsent>
<prevsent>we therefore have to find an upper bound of its generative power.
</prevsent>
</prevsection>
<citsent citstr=" E95-1011 ">
though ccg-mm is more powerful than ccgand lig, the rules in (35) reveal significant property of partially linear indexed grammar (plig) (keller and weir, 1995), <papid> E95-1011 </papid>an extension of lig whose productions are allowed to have two or more daughters sharing stack features with each other but these stacks are not shared with their mother as shown in (36).</citsent>
<aftsection>
<nextsent>(36) a[] ? a1[] . . .
</nextsent>
<nextsent>ai[??]
</nextsent>
<nextsent>aj [??]
</nextsent>
<nextsent>an[]whereby restricting the power of the gap resolution connective, the two stacks of the daughters are shared but not with their mother.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I561">
<title id=" E06-2026.xml">grammatical role labeling with integer linear programming </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>an often stressed point is that the most widely used classifiers such as naive bayes, hmm, andmemory-based learners are restricted to local decisions only.
</prevsent>
<prevsent>with grammatical role labeling, for example, there is no way to explicitly express global constraints that, say, the verb to give?
</prevsent>
</prevsection>
<citsent citstr=" C04-1197 ">
must have 3 arguments of particular grammatical role.among the approaches to overcome this restriction, i.e. that allow for global, theory based constraints, integer linear programming (ilp) has been applied to nlp (punyakanok et al, 2004) .<papid> C04-1197 </papid>we apply ilp to the problem of grammatical relation labeling, i.e. given two chunks.1 (e.g. verb and np), what is the grammatical relation between them (if there is any).</citsent>
<aftsection>
<nextsent>we have trained amaximum entropy classifier on vectors with morphological, syntactic and positional information.its output is utilized as weights to the ilp component which generates equations to solve the following problem: given subcategorization frames (expressed in functional roles, e.g. subject), and given sentence with verbs,   (auxiliary, modal, finite, non-finite, ..), and chunks,
</nextsent>
<nextsent>(  ,  ), label all pairs (
</nextsent>
<nextsent>) with grammatical role2.in this paper, we are pursuing two empirical scenarios.
</nextsent>
<nextsent>the first is to collapse all subcategoriza1currently, we use perfect chunks, that is, chunks stemming from automatically flattening treebank.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I563">
<title id=" E06-2026.xml">grammatical role labeling with integer linear programming </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>   results.
</prevsent>
<prevsent>the question then is, how close can we come to the  gjh ^`^ upper bound.
</prevsent>
</prevsection>
<citsent citstr=" P04-1051 ">
ilp has been applied to various nlp problems, including semantic role labeling (punyakanok et al., 2004), <papid> C04-1197 </papid>extraction of predicates from parse trees(klenner, 2005) and discourse ordering in generation (althaus et al, 2004).<papid> P04-1051 </papid></citsent>
<aftsection>
<nextsent>(roth and yih, 2005)discuss how to utilize ilp with conditional random fields.
</nextsent>
<nextsent>grammatical relation labeling has been coped with in couple of articles, e.g.
</nextsent>
<nextsent>(buchholz, 1999).
</nextsent>
<nextsent>there, cascaded model (of classifiers) has been proposed (using various tools around timbl).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I564">
<title id=" E12-1043.xml">combining tree structures flat features and patterns for biomedical relation extraction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>4.
</prevsent>
<prevsent>we propose hybrid kernel by combin-.
</prevsent>
</prevsection>
<citsent citstr=" E06-1051 ">
ing the proposed feature based kernel (out lined above) with the shallow linguistic (sl) kernel (giuliano et al 2006) <papid> E06-1051 </papid>and the path-enclosed tree (pet) kernel (moschitti, 2004).<papid> P04-1043 </papid></citsent>
<aftsection>
<nextsent>the aim of our work is to take advantage of different types of information (i.e., dependency patterns, regex patterns, trigger words, negative cues, syntactic dependencies among words and constituent parse trees) and their different representations (i.e. flat features, tree structures and graphs) which can complement each other to learn more accurate models.
</nextsent>
<nextsent>2the syntactic dependencies of the words of sentence create dependency graph.
</nextsent>
<nextsent>a v-walk feature consists of(wordi ? dependency typei,i+1 ? wordi+1), and an walk feature is composed of (dependency typei1,i wordi ? dependency typei,i+1).
</nextsent>
<nextsent>note that, in dependency graph, the words are nodes while the dependency types are edges.the remainder of the paper is organized as follows.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I566">
<title id=" E12-1043.xml">combining tree structures flat features and patterns for biomedical relation extraction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>4.
</prevsent>
<prevsent>we propose hybrid kernel by combin-.
</prevsent>
</prevsection>
<citsent citstr=" P04-1043 ">
ing the proposed feature based kernel (out lined above) with the shallow linguistic (sl) kernel (giuliano et al 2006) <papid> E06-1051 </papid>and the path-enclosed tree (pet) kernel (moschitti, 2004).<papid> P04-1043 </papid></citsent>
<aftsection>
<nextsent>the aim of our work is to take advantage of different types of information (i.e., dependency patterns, regex patterns, trigger words, negative cues, syntactic dependencies among words and constituent parse trees) and their different representations (i.e. flat features, tree structures and graphs) which can complement each other to learn more accurate models.
</nextsent>
<nextsent>2the syntactic dependencies of the words of sentence create dependency graph.
</nextsent>
<nextsent>a v-walk feature consists of(wordi ? dependency typei,i+1 ? wordi+1), and an walk feature is composed of (dependency typei1,i wordi ? dependency typei,i+1).
</nextsent>
<nextsent>note that, in dependency graph, the words are nodes while the dependency types are edges.the remainder of the paper is organized as follows.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I569">
<title id=" E12-1043.xml">combining tree structures flat features and patterns for biomedical relation extraction </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>several re approaches have been reported to date for the ppi task, most of which are kernel based methods.tikk et al(2010) reported benchmark evaluation of various kernels on ppi extraction.
</prevsent>
<prevsent>an interesting finding is that the shallow linguistic (sl) kernel (giuliano et al 2006) <papid> E06-1051 </papid>(to be discussed in section 4.2), despite its simplicity, is on par with the best kernels in most of the evaluation settings.kim et al(2010) proposed walk-weighted sub sequence kernel using e-walks, partial matches, non-contiguous paths, and different weights for different sub-structures (which are used to capture structural similarities during kernel computation).</prevsent>
</prevsection>
<citsent citstr=" D09-1013 ">
miwa et al(2009<papid> D09-1013 </papid>a) proposed hybrid kernel, which combines the all-paths graph (apg) kernel (airola et al 2008), <papid> W08-0601 </papid>the bag-of-words kernel, and the subset tree kernel (moschitti, 2006) (<papid> E06-1015 </papid>applied on the shortest dependency paths between target protein pairs).</citsent>
<aftsection>
<nextsent>they used multiple parser inputs.the system is regarded as the current state-of-theart ppi extraction system because of its high results on different ppi corpora (see the results in table 4).as an extension of their work, they boosted system performance by training on multiple ppi corpora instead of on single corpus and adopting corpus weighting concept with support vector machine (svm) which they call svm-cw (miwaet al 2009<papid> D09-1013 </papid>b).</nextsent>
<nextsent>since most of their results are reported by training on the combination of multiple corpora, it is not possible to compare them directly with the results published in the other related works (that usually adopt 10-fold cross validation on single ppi corpus).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I571">
<title id=" E12-1043.xml">combining tree structures flat features and patterns for biomedical relation extraction </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>several re approaches have been reported to date for the ppi task, most of which are kernel based methods.tikk et al(2010) reported benchmark evaluation of various kernels on ppi extraction.
</prevsent>
<prevsent>an interesting finding is that the shallow linguistic (sl) kernel (giuliano et al 2006) <papid> E06-1051 </papid>(to be discussed in section 4.2), despite its simplicity, is on par with the best kernels in most of the evaluation settings.kim et al(2010) proposed walk-weighted sub sequence kernel using e-walks, partial matches, non-contiguous paths, and different weights for different sub-structures (which are used to capture structural similarities during kernel computation).</prevsent>
</prevsection>
<citsent citstr=" W08-0601 ">
miwa et al(2009<papid> D09-1013 </papid>a) proposed hybrid kernel, which combines the all-paths graph (apg) kernel (airola et al 2008), <papid> W08-0601 </papid>the bag-of-words kernel, and the subset tree kernel (moschitti, 2006) (<papid> E06-1015 </papid>applied on the shortest dependency paths between target protein pairs).</citsent>
<aftsection>
<nextsent>they used multiple parser inputs.the system is regarded as the current state-of-theart ppi extraction system because of its high results on different ppi corpora (see the results in table 4).as an extension of their work, they boosted system performance by training on multiple ppi corpora instead of on single corpus and adopting corpus weighting concept with support vector machine (svm) which they call svm-cw (miwaet al 2009<papid> D09-1013 </papid>b).</nextsent>
<nextsent>since most of their results are reported by training on the combination of multiple corpora, it is not possible to compare them directly with the results published in the other related works (that usually adopt 10-fold cross validation on single ppi corpus).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I572">
<title id=" E12-1043.xml">combining tree structures flat features and patterns for biomedical relation extraction </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>several re approaches have been reported to date for the ppi task, most of which are kernel based methods.tikk et al(2010) reported benchmark evaluation of various kernels on ppi extraction.
</prevsent>
<prevsent>an interesting finding is that the shallow linguistic (sl) kernel (giuliano et al 2006) <papid> E06-1051 </papid>(to be discussed in section 4.2), despite its simplicity, is on par with the best kernels in most of the evaluation settings.kim et al(2010) proposed walk-weighted sub sequence kernel using e-walks, partial matches, non-contiguous paths, and different weights for different sub-structures (which are used to capture structural similarities during kernel computation).</prevsent>
</prevsection>
<citsent citstr=" E06-1015 ">
miwa et al(2009<papid> D09-1013 </papid>a) proposed hybrid kernel, which combines the all-paths graph (apg) kernel (airola et al 2008), <papid> W08-0601 </papid>the bag-of-words kernel, and the subset tree kernel (moschitti, 2006) (<papid> E06-1015 </papid>applied on the shortest dependency paths between target protein pairs).</citsent>
<aftsection>
<nextsent>they used multiple parser inputs.the system is regarded as the current state-of-theart ppi extraction system because of its high results on different ppi corpora (see the results in table 4).as an extension of their work, they boosted system performance by training on multiple ppi corpora instead of on single corpus and adopting corpus weighting concept with support vector machine (svm) which they call svm-cw (miwaet al 2009<papid> D09-1013 </papid>b).</nextsent>
<nextsent>since most of their results are reported by training on the combination of multiple corpora, it is not possible to compare them directly with the results published in the other related works (that usually adopt 10-fold cross validation on single ppi corpus).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I574">
<title id=" E12-1043.xml">combining tree structures flat features and patterns for biomedical relation extraction </title>
<section> proposed hybrid kernel.  </section>
<citcontext>
<prevsection>
<prevsent>a reduced graph is an extension of the smallest common subgraph of the dependency graph that aims at overcoming its limitations.
</prevsent>
<prevsent>it is known issue that the smallest common subgraph (or subtree) sometimes does not contain cue words.
</prevsent>
</prevsection>
<citsent citstr=" W11-0216 ">
previously, chowdhury et al(2011<papid> W11-0216 </papid>a) proposed linguistically motivated extension of the minimal (i.e. smallest) common subtree (which includes the candidate entity pairs), known as mildly extended dependency tree (medt).</citsent>
<aftsection>
<nextsent>however, the rules used for medt are too constrained.
</nextsent>
<nextsent>our objective in constructing the reduced graph is to include any potential modifier(s) or cue word(s) that describes the relation between the given pair of entities.
</nextsent>
<nextsent>sometimes such modifiers or cue words are not directly dependent (syntactically) on any 422 bio infer aimed iepa hprd50 lll r p f r p f r only walk features 51.8 71.2 60.0 48.7 63.2 55.0 61.0 75.2 67.4 60.2 65.0 62.5 64.6 87.8 74.4 features: dep.
</nextsent>
<nextsent>patterns, 53.8 68.8 60.4 50.6 63.9 56.5 63.9 74.6 68.9 65.0 71.8 68.2 66.5 89.6 76.4 trigger, neg.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I593">
<title id=" E12-1043.xml">combining tree structures flat features and patterns for biomedical relation extraction </title>
<section> experimental settings.  </section>
<citcontext>
<prevsection>
<prevsent>425
</prevsent>
<prevsent>we have followed the same criteria commonly used for the ppi extraction tasks, i.e. abstract wise 10-fold cross validation on individual corpus and one-answer-per-occurrence criterion.
</prevsent>
</prevsection>
<citsent citstr=" P05-1022 ">
in fact, we have used exactly the same (abstract-wise)fold splitting of the 5 benchmark (converted) corpora used by tikk et al(2010) for benchmarking various kernel methods4.the charniak-johnson reranking parser (char niak and johnson, 2005), <papid> P05-1022 </papid>along with self-trained biomedical parsing model (mcclosky, 2010), has been used for tokenization, pos-tagging and parsing of the sentences.</citsent>
<aftsection>
<nextsent>before parsing the sentences, all the entities are blinded by assigning names as entityx where is the entity index.in each example, the pos tags of the two candidate entities are changed to entityx . the parse trees produced by the charniak-johnsonreranking parser are then processed by the stanford parser5 (klein and manning, 2003) <papid> P03-1054 </papid>to obtain syntactic dependencies according to the stanford typed dependency format.</nextsent>
<nextsent>the stanford parser often skips some syntactic dependencies in output.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I594">
<title id=" E12-1043.xml">combining tree structures flat features and patterns for biomedical relation extraction </title>
<section> experimental settings.  </section>
<citcontext>
<prevsection>
<prevsent>we have followed the same criteria commonly used for the ppi extraction tasks, i.e. abstract wise 10-fold cross validation on individual corpus and one-answer-per-occurrence criterion.
</prevsent>
<prevsent>in fact, we have used exactly the same (abstract-wise)fold splitting of the 5 benchmark (converted) corpora used by tikk et al(2010) for benchmarking various kernel methods4.the charniak-johnson reranking parser (char niak and johnson, 2005), <papid> P05-1022 </papid>along with self-trained biomedical parsing model (mcclosky, 2010), has been used for tokenization, pos-tagging and parsing of the sentences.</prevsent>
</prevsection>
<citsent citstr=" P03-1054 ">
before parsing the sentences, all the entities are blinded by assigning names as entityx where is the entity index.in each example, the pos tags of the two candidate entities are changed to entityx . the parse trees produced by the charniak-johnsonreranking parser are then processed by the stanford parser5 (klein and manning, 2003) <papid> P03-1054 </papid>to obtain syntactic dependencies according to the stanford typed dependency format.</citsent>
<aftsection>
<nextsent>the stanford parser often skips some syntactic dependencies in output.
</nextsent>
<nextsent>we use the following two rules to add some of such dependencies:?
</nextsent>
<nextsent>if there is conj and?
</nextsent>
<nextsent>or conj or?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I596">
<title id=" E12-1043.xml">combining tree structures flat features and patterns for biomedical relation extraction </title>
<section> conclusion.  </section>
<citcontext>
<prevsection>
<prevsent>secondly, different techniques could be used to identify less-informative syntactic dependencies inside dependency patterns to make them more accurate and effective.
</prevsent>
<prevsent>thirdly, usage of automatically collected paraphrases of regular expression patterns instead of the patterns directly could be also helpful.
</prevsent>
</prevsection>
<citsent citstr=" E06-1052 ">
weakly supervised collection of paraphrases for re has been already investigated (e.g. romano et al(2006)) <papid> E06-1052 </papid>and, hence, can be tried for improving the tpwf kernel (which is component of the proposed hybrid kernel).</citsent>
<aftsection>
<nextsent>acknowledgments this work was carried out in the context of the project eonco - pervasive knowledge and data management in cancer care?.
</nextsent>
<nextsent>the authors are grateful to alessandro moschitti for his help in the use of svm-lighttk.
</nextsent>
<nextsent>we also thank the anonymous reviewers for helpful suggestions.
</nextsent>

</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I597">
<title id=" E12-1036.xml">user edits classification using document revision histories </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>according to wikipedia statistics, in august 2011 the english wikipedia contained 3.8 million articles with an average of 78.3 revisions per article.
</prevsent>
<prevsent>the average number of revision edits per month is about 4 million in english and almost 11 million in total for all languages.2 1http://www.wikipedia.org 2average for the 5 years period between august 2006 and august 2011.
</prevsent>
</prevsection>
<citsent citstr=" P08-2035 ">
the count includes edits by registered exploiting document revision histories has proven useful for variety of natural language processing (nlp) tasks, including sentence compression (nelken and yamangil, 2008; yamangil and nelken, 2008) <papid> P08-2035 </papid>and simplification (yatskar etal., 2010; <papid> N10-1056 </papid>woodsend and lapata, 2011), <papid> D11-1038 </papid>information retrieval (aji et al 2010; nunes et al 2011),textual entailment recognition (zanzotto and pennacchiotti, 2010), <papid> W10-3504 </papid>and paraphrase extraction (max and wisniewski, 2010; dutrey et al 2011).</citsent>
<aftsection>
<nextsent>the ability to distinguish between factual changes or edits, which alter the meaning, and fluency edits, which improve the style or readability,is crucial requirement for approaches exploiting revision histories.
</nextsent>
<nextsent>the need for an automated classification method has been identified (nelken and yamangil, 2008; max and wisniewski, 2010),but to the best of our knowledge has not been directly addressed.
</nextsent>
<nextsent>previous approaches have either applied simple heuristics (yatskar et al 2010;<papid> N10-1056 </papid>woodsend and lapata, 2011) <papid> D11-1038 </papid>or manual annotations (dutrey et al 2011) to restrict the data to the type of edits relevant to the nlp task at hand.</nextsent>
<nextsent>the work described in this paper shows that it is possible to automatically distinguish between factual and fluency edits.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I598">
<title id=" E12-1036.xml">user edits classification using document revision histories </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>according to wikipedia statistics, in august 2011 the english wikipedia contained 3.8 million articles with an average of 78.3 revisions per article.
</prevsent>
<prevsent>the average number of revision edits per month is about 4 million in english and almost 11 million in total for all languages.2 1http://www.wikipedia.org 2average for the 5 years period between august 2006 and august 2011.
</prevsent>
</prevsection>
<citsent citstr=" N10-1056 ">
the count includes edits by registered exploiting document revision histories has proven useful for variety of natural language processing (nlp) tasks, including sentence compression (nelken and yamangil, 2008; yamangil and nelken, 2008) <papid> P08-2035 </papid>and simplification (yatskar etal., 2010; <papid> N10-1056 </papid>woodsend and lapata, 2011), <papid> D11-1038 </papid>information retrieval (aji et al 2010; nunes et al 2011),textual entailment recognition (zanzotto and pennacchiotti, 2010), <papid> W10-3504 </papid>and paraphrase extraction (max and wisniewski, 2010; dutrey et al 2011).</citsent>
<aftsection>
<nextsent>the ability to distinguish between factual changes or edits, which alter the meaning, and fluency edits, which improve the style or readability,is crucial requirement for approaches exploiting revision histories.
</nextsent>
<nextsent>the need for an automated classification method has been identified (nelken and yamangil, 2008; max and wisniewski, 2010),but to the best of our knowledge has not been directly addressed.
</nextsent>
<nextsent>previous approaches have either applied simple heuristics (yatskar et al 2010;<papid> N10-1056 </papid>woodsend and lapata, 2011) <papid> D11-1038 </papid>or manual annotations (dutrey et al 2011) to restrict the data to the type of edits relevant to the nlp task at hand.</nextsent>
<nextsent>the work described in this paper shows that it is possible to automatically distinguish between factual and fluency edits.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I599">
<title id=" E12-1036.xml">user edits classification using document revision histories </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>according to wikipedia statistics, in august 2011 the english wikipedia contained 3.8 million articles with an average of 78.3 revisions per article.
</prevsent>
<prevsent>the average number of revision edits per month is about 4 million in english and almost 11 million in total for all languages.2 1http://www.wikipedia.org 2average for the 5 years period between august 2006 and august 2011.
</prevsent>
</prevsection>
<citsent citstr=" D11-1038 ">
the count includes edits by registered exploiting document revision histories has proven useful for variety of natural language processing (nlp) tasks, including sentence compression (nelken and yamangil, 2008; yamangil and nelken, 2008) <papid> P08-2035 </papid>and simplification (yatskar etal., 2010; <papid> N10-1056 </papid>woodsend and lapata, 2011), <papid> D11-1038 </papid>information retrieval (aji et al 2010; nunes et al 2011),textual entailment recognition (zanzotto and pennacchiotti, 2010), <papid> W10-3504 </papid>and paraphrase extraction (max and wisniewski, 2010; dutrey et al 2011).</citsent>
<aftsection>
<nextsent>the ability to distinguish between factual changes or edits, which alter the meaning, and fluency edits, which improve the style or readability,is crucial requirement for approaches exploiting revision histories.
</nextsent>
<nextsent>the need for an automated classification method has been identified (nelken and yamangil, 2008; max and wisniewski, 2010),but to the best of our knowledge has not been directly addressed.
</nextsent>
<nextsent>previous approaches have either applied simple heuristics (yatskar et al 2010;<papid> N10-1056 </papid>woodsend and lapata, 2011) <papid> D11-1038 </papid>or manual annotations (dutrey et al 2011) to restrict the data to the type of edits relevant to the nlp task at hand.</nextsent>
<nextsent>the work described in this paper shows that it is possible to automatically distinguish between factual and fluency edits.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I600">
<title id=" E12-1036.xml">user edits classification using document revision histories </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>according to wikipedia statistics, in august 2011 the english wikipedia contained 3.8 million articles with an average of 78.3 revisions per article.
</prevsent>
<prevsent>the average number of revision edits per month is about 4 million in english and almost 11 million in total for all languages.2 1http://www.wikipedia.org 2average for the 5 years period between august 2006 and august 2011.
</prevsent>
</prevsection>
<citsent citstr=" W10-3504 ">
the count includes edits by registered exploiting document revision histories has proven useful for variety of natural language processing (nlp) tasks, including sentence compression (nelken and yamangil, 2008; yamangil and nelken, 2008) <papid> P08-2035 </papid>and simplification (yatskar etal., 2010; <papid> N10-1056 </papid>woodsend and lapata, 2011), <papid> D11-1038 </papid>information retrieval (aji et al 2010; nunes et al 2011),textual entailment recognition (zanzotto and pennacchiotti, 2010), <papid> W10-3504 </papid>and paraphrase extraction (max and wisniewski, 2010; dutrey et al 2011).</citsent>
<aftsection>
<nextsent>the ability to distinguish between factual changes or edits, which alter the meaning, and fluency edits, which improve the style or readability,is crucial requirement for approaches exploiting revision histories.
</nextsent>
<nextsent>the need for an automated classification method has been identified (nelken and yamangil, 2008; max and wisniewski, 2010),but to the best of our knowledge has not been directly addressed.
</nextsent>
<nextsent>previous approaches have either applied simple heuristics (yatskar et al 2010;<papid> N10-1056 </papid>woodsend and lapata, 2011) <papid> D11-1038 </papid>or manual annotations (dutrey et al 2011) to restrict the data to the type of edits relevant to the nlp task at hand.</nextsent>
<nextsent>the work described in this paper shows that it is possible to automatically distinguish between factual and fluency edits.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I607">
<title id=" E12-1036.xml">user edits classification using document revision histories </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>a wide range of methods and approaches hasbeen applied to the similar tasks of textual entailment and paraphrase recognition, see androutsopoulos and malakasiotis (2010) for comprehensive review.
</prevsent>
<prevsent>these are all related because paraphrases and bidirectional ent ailments represent types of fluency edits.
</prevsent>
</prevsection>
<citsent citstr=" C08-1145 ">
a different line of research uses classifiers to predict sentence-level fluency (zwarts and dras, 2008; <papid> C08-1145 </papid>chae and nenkova, 2009).<papid> E09-1017 </papid></citsent>
<aftsection>
<nextsent>these could be useful for fluency edits detection.
</nextsent>
<nextsent>alternatively,user edits could be potential source of human produced training data for fluency models.
</nextsent>
<nextsent>within our approach we distinguish between edit segments, which represent the comparison (diff) between two document revisions, and user edits, which are the input for classification.
</nextsent>
<nextsent>an edit segment is contiguous sequence of deleted, inserted or equal words.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I608">
<title id=" E12-1036.xml">user edits classification using document revision histories </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>a wide range of methods and approaches hasbeen applied to the similar tasks of textual entailment and paraphrase recognition, see androutsopoulos and malakasiotis (2010) for comprehensive review.
</prevsent>
<prevsent>these are all related because paraphrases and bidirectional ent ailments represent types of fluency edits.
</prevsent>
</prevsection>
<citsent citstr=" E09-1017 ">
a different line of research uses classifiers to predict sentence-level fluency (zwarts and dras, 2008; <papid> C08-1145 </papid>chae and nenkova, 2009).<papid> E09-1017 </papid></citsent>
<aftsection>
<nextsent>these could be useful for fluency edits detection.
</nextsent>
<nextsent>alternatively,user edits could be potential source of human produced training data for fluency models.
</nextsent>
<nextsent>within our approach we distinguish between edit segments, which represent the comparison (diff) between two document revisions, and user edits, which are the input for classification.
</nextsent>
<nextsent>an edit segment is contiguous sequence of deleted, inserted or equal words.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I609">
<title id=" E12-1036.xml">user edits classification using document revision histories </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>we experiment with three classifiers: support vector machines (svm), random forests (rf) and logistic regression (logit).7 svms (cortes and vapnik, 1995) and logistic regression (or maximum entropy classifiers) are two widely used machine learning techniques.
</prevsent>
<prevsent>svms have been applied to many text classification problems (joachims, 1998).
</prevsent>
</prevsection>
<citsent citstr=" P09-3004 ">
maximum entropy classifier shave been applied to the similar tasks of paraphrase recognition (malakasiotis, 2009) <papid> P09-3004 </papid>and textual entailment (hickl et al 2006).</citsent>
<aftsection>
<nextsent>random forests (breiman, 2001) as well as other decision tree algorithms are successfully used for classifying wikipedia edits for the purpose of vandalism detection (potthast et al 2010; potthast and holfeld, 2011).experiments begin with the edit-distance base science.uva.nl/abronner/uec/data.
</nextsent>
<nextsent>7using weka classifiers: smo (svm), random forest &amp; logistic (hall et al 2009).
</nextsent>
<nextsent>classifiers parameters are tuned using the held-out development set.
</nextsent>
<nextsent>feature set svm rf logit flu.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I610">
<title id=" E12-1007.xml">dependency parsing of hungarian baseline results and challenges </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a large part of the methodology for syntactic parsing has been developed for english.
</prevsent>
<prevsent>however, parsing non-configurational and less config ura tional languages requires different techniques.in this study, we present results on hungarian dependency parsing and we investigate this general issue in the case of english and hungarian.
</prevsent>
</prevsection>
<citsent citstr=" W04-2407 ">
we employed three state-of-the-art data-driven parsers (nivre et al 2004; <papid> W04-2407 </papid>mcdonald et al 2005;<papid> H05-1066 </papid>bohnet, 2010), <papid> C10-1011 </papid>which achieved (un)labeled attachment scores on hungarian not so different from the corresponding english scores (and even higher on certain domains/subcorpora).</citsent>
<aftsection>
<nextsent>our investigations show that the feature representation used by the data-driven parsers is so rich that they can ? without any modification ? effectively learna reasonable model for non-configurational languages as well.we also conducted systematic and comparative error analysis of the systems outputs for hungarian and english.
</nextsent>
<nextsent>this analysis highlights the challenges of parsing hungarian and suggests that the further improvement of parsers requires special handling of language-specific phenomena.
</nextsent>
<nextsent>we believe that some of our findings can be relevant for intermediate languages on the configurational-non-configurational spectrum.
</nextsent>
<nextsent>hungarian morpho syntax hungarian is an agglutinative language, which means that word can have hundreds of word forms due to inflectional or derivational affixa tion.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I611">
<title id=" E12-1007.xml">dependency parsing of hungarian baseline results and challenges </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a large part of the methodology for syntactic parsing has been developed for english.
</prevsent>
<prevsent>however, parsing non-configurational and less config ura tional languages requires different techniques.in this study, we present results on hungarian dependency parsing and we investigate this general issue in the case of english and hungarian.
</prevsent>
</prevsection>
<citsent citstr=" H05-1066 ">
we employed three state-of-the-art data-driven parsers (nivre et al 2004; <papid> W04-2407 </papid>mcdonald et al 2005;<papid> H05-1066 </papid>bohnet, 2010), <papid> C10-1011 </papid>which achieved (un)labeled attachment scores on hungarian not so different from the corresponding english scores (and even higher on certain domains/subcorpora).</citsent>
<aftsection>
<nextsent>our investigations show that the feature representation used by the data-driven parsers is so rich that they can ? without any modification ? effectively learna reasonable model for non-configurational languages as well.we also conducted systematic and comparative error analysis of the systems outputs for hungarian and english.
</nextsent>
<nextsent>this analysis highlights the challenges of parsing hungarian and suggests that the further improvement of parsers requires special handling of language-specific phenomena.
</nextsent>
<nextsent>we believe that some of our findings can be relevant for intermediate languages on the configurational-non-configurational spectrum.
</nextsent>
<nextsent>hungarian morpho syntax hungarian is an agglutinative language, which means that word can have hundreds of word forms due to inflectional or derivational affixa tion.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I612">
<title id=" E12-1007.xml">dependency parsing of hungarian baseline results and challenges </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a large part of the methodology for syntactic parsing has been developed for english.
</prevsent>
<prevsent>however, parsing non-configurational and less config ura tional languages requires different techniques.in this study, we present results on hungarian dependency parsing and we investigate this general issue in the case of english and hungarian.
</prevsent>
</prevsection>
<citsent citstr=" C10-1011 ">
we employed three state-of-the-art data-driven parsers (nivre et al 2004; <papid> W04-2407 </papid>mcdonald et al 2005;<papid> H05-1066 </papid>bohnet, 2010), <papid> C10-1011 </papid>which achieved (un)labeled attachment scores on hungarian not so different from the corresponding english scores (and even higher on certain domains/subcorpora).</citsent>
<aftsection>
<nextsent>our investigations show that the feature representation used by the data-driven parsers is so rich that they can ? without any modification ? effectively learna reasonable model for non-configurational languages as well.we also conducted systematic and comparative error analysis of the systems outputs for hungarian and english.
</nextsent>
<nextsent>this analysis highlights the challenges of parsing hungarian and suggests that the further improvement of parsers requires special handling of language-specific phenomena.
</nextsent>
<nextsent>we believe that some of our findings can be relevant for intermediate languages on the configurational-non-configurational spectrum.
</nextsent>
<nextsent>hungarian morpho syntax hungarian is an agglutinative language, which means that word can have hundreds of word forms due to inflectional or derivational affixa tion.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I613">
<title id=" E12-1007.xml">dependency parsing of hungarian baseline results and challenges </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>this train of thought indicates that the cross-lingual comparison of final parser scores should be conducted very carefully.
</prevsent>
<prevsent>we decided to focus on dependency parsing inthis study as it is superior framework for nonconfigurational languages.
</prevsent>
</prevsection>
<citsent citstr=" J11-1007 ">
it has gained interest in natural language processing recently be cause the representation itself does not require the words inside of constituents to be consecutive and it naturally represent discontinuous constructions, which are frequent in languages where grammatical relations are often signaled by morphology instead of word order (mcdonald and nivre, 2011).<papid> J11-1007 </papid></citsent>
<aftsection>
<nextsent>the two main efficient approaches for dependency parsing are the graph-based and the transition-based parsers.
</nextsent>
<nextsent>the graph-basedmodels look for the highest scoring directed spanning tree in the complete graph whose nodes are the words of the sentence in question.
</nextsent>
<nextsent>they solve the machine learning problem of finding the optimal scoring function of subgraphs (eisner, 1996;<papid> C96-1058 </papid>mcdonald et al 2005).<papid> H05-1066 </papid></nextsent>
<nextsent>the transition-based approaches parse sentence in single left-to-right pass over the words.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I614">
<title id=" E12-1007.xml">dependency parsing of hungarian baseline results and challenges </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the two main efficient approaches for dependency parsing are the graph-based and the transition-based parsers.
</prevsent>
<prevsent>the graph-basedmodels look for the highest scoring directed spanning tree in the complete graph whose nodes are the words of the sentence in question.
</prevsent>
</prevsection>
<citsent citstr=" C96-1058 ">
they solve the machine learning problem of finding the optimal scoring function of subgraphs (eisner, 1996;<papid> C96-1058 </papid>mcdonald et al 2005).<papid> H05-1066 </papid></citsent>
<aftsection>
<nextsent>the transition-based approaches parse sentence in single left-to-right pass over the words.
</nextsent>
<nextsent>the next transition in these systems is predicted by classifier that is based on history-related features (kudo and matsumoto, 2002; <papid> W02-2016 </papid>nivre et al 2004).<papid> W04-2407 </papid>although the available treebanks for hungarian are relatively big (82k sentences) and fully manually annotated, the studies on parsing hungarian are rather limited.</nextsent>
<nextsent>the szeged (constituency) treebank (csendes et al 2005) con 56 sists of six domains ? namely, short business news, newspaper, law, literature, compositions and informatics ? and it is manually annotated for the possible alternatives of words?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I616">
<title id=" E12-1007.xml">dependency parsing of hungarian baseline results and challenges </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>they solve the machine learning problem of finding the optimal scoring function of subgraphs (eisner, 1996;<papid> C96-1058 </papid>mcdonald et al 2005).<papid> H05-1066 </papid></prevsent>
<prevsent>the transition-based approaches parse sentence in single left-to-right pass over the words.</prevsent>
</prevsection>
<citsent citstr=" W02-2016 ">
the next transition in these systems is predicted by classifier that is based on history-related features (kudo and matsumoto, 2002; <papid> W02-2016 </papid>nivre et al 2004).<papid> W04-2407 </papid>although the available treebanks for hungarian are relatively big (82k sentences) and fully manually annotated, the studies on parsing hungarian are rather limited.</citsent>
<aftsection>
<nextsent>the szeged (constituency) treebank (csendes et al 2005) con 56 sists of six domains ? namely, short business news, newspaper, law, literature, compositions and informatics ? and it is manually annotated for the possible alternatives of words?
</nextsent>
<nextsent>morphological analyses, the disambiguated analysis and constituency trees.
</nextsent>
<nextsent>we are aware of only two articles on phrase-structure parsers which were trained and evaluated on this corpus (barta et al 2005; ivan et al 2007) and there are few studies on hand-crafted parsers reporting results on small own corpora (babarczy et al 2005; proszeky et al., 2004).
</nextsent>
<nextsent>the szeged dependency treebank (vincze et al., 2010) was constructed by first automatically converting the phrase-structure trees into dependency trees, then each of them was manually investigated and corrected.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I618">
<title id=" E12-1007.xml">dependency parsing of hungarian baseline results and challenges </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>both corpora are available at www.inf.
</prevsent>
<prevsent>u-szeged.hu/rgai/szegedtreebank.
</prevsent>
</prevsection>
<citsent citstr=" D07-1096 ">
the multilingual track of the conll-2007 shared task (nivre et al 2007) <papid> D07-1096 </papid>addressed also the task of dependency parsing of hungarian.</citsent>
<aftsection>
<nextsent>the hungarian corpus used for the shared task consists of automatically converted dependency trees from the szeged constituency treebank.
</nextsent>
<nextsent>several issues of the automatic conversion tool were reconsidered before the manual annotation of the szeged dependency treebank was launched and the annotation guidelines contained instructions related to linguistic phenomena which could notbe converted from the constituency representation ? for detailed discussion, see vincze et al (2010).
</nextsent>
<nextsent>hence the annotation schemata of the conll-2007 hungarian corpus and the szeged dependency treebank are rather different and the final scores reported for the former are not directly comparable with our reported scores here (see section 5).
</nextsent>
<nextsent>we utilize the szeged dependency treebank(vincze et al 2010) as the basis of our experiments for hungarian dependency parsing.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I620">
<title id=" E12-1007.xml">dependency parsing of hungarian baseline results and challenges </title>
<section> the szeged dependency treebank.  </section>
<citcontext>
<prevsection>
<prevsent>it contains 82,000 sentences, 1.2 million words and 250,000 punctuation marks from six domains.
</prevsent>
<prevsent>the annotation employs 16 coarse grained postags, 95 morphological feature values and 29 dependency labels.
</prevsent>
</prevsection>
<citsent citstr=" W06-2920 ">
19.6% of the sentences in the corpus contain non-projective edges and 1.8% of the edges are non-projective2, which is almost 5 times more frequent than in english and is thesame as the czech non-projectivity level (buchholz and marsi, 2006).<papid> W06-2920 </papid></citsent>
<aftsection>
<nextsent>here we discuss two annotation principles along with our modification sin the dataset for this study which strongly influence the parsers?
</nextsent>
<nextsent>accuracies.named entities (nes) were treated as one token in the szeged dependency treebank.
</nextsent>
<nextsent>assuming perfect phrase recogniser on the whit espace token ised input for them is quite unrealistic.
</nextsent>
<nextsent>thus we decided to split them into tokens for this study.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I621">
<title id=" E12-1007.xml">dependency parsing of hungarian baseline results and challenges </title>
<section> the szeged dependency treebank.  </section>
<citcontext>
<prevsection>
<prevsent>we consider the last word of multiword named entities as the head because of morphological reasons (the last word of multiword units gets inflected in hungarian) and all the previous elements are attached to the succeeding word, i.e. the penultimate word is attached to thelast word, the ante penultimate word to the penultimate one etc. the reasons for these considerations are that we believe that there are no downstream applications which can exploit the information of the internal structures of named entities and we imagine pipeline where named entity recog niser precedes the parsing step.empty copula: in the ver bless clauses (predicative nouns or adjectives) the szeged dependency treebank introduces virtual nodes (16,000 items in the corpus).
</prevsent>
<prevsent>this solution means that similar tree structure is ascribed to the same sentence in the present third person singular andall the other tenses / persons.
</prevsent>
</prevsection>
<citsent citstr=" P05-1013 ">
a further argument for the use of virtual node is that the virtual node is always present at the syntactic level2using the transitive closure definition of nivre and nilsson (2005).<papid> P05-1013 </papid></citsent>
<aftsection>
<nextsent>57 corpus malt mst mate ula las ula las ula las hungarian dev 88.3 (89.9) 85.7 (87.9) 86.9 (88.5) 80.9 (82.9) 89.7 (91.1) 86.8 (89.0) test 88.7 (90.2) 86.1 (88.2) 87.5 (89.0) 81.6 (83.5) 90.1 (91.5) 87.2 (89.4) english dev 87.8 (89.1) 84.5 (86.1) 89.4 (91.2) 86.1 (87.7) 91.6 (92.7) 88.5 (90.0) test 88.8 (89.9) 86.2 (87.6) 90.7 (91.8) 87.7 (89.2) 92.6 (93.4) 90.3 (91.5) table 1: results achieved by the three parsers on the (full) hungarian (szeged dependency treebank) and english (conll-2009) datasets.
</nextsent>
<nextsent>the scores in brackets are achieved with gold-standard pos tagging.
</nextsent>
<nextsent>since it is overt in all the other forms, tenses and moods of the verb.
</nextsent>
<nextsent>still, the state-of-the-art dependency parsers cannot handle virtual nodes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I622">
<title id=" E12-1007.xml">dependency parsing of hungarian baseline results and challenges </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>we carried out experiments using three state-ofthe-art parsers on the szeged dependency tree bank (vincze et al 2010) and on the english datasets of the conll-2009 shared task (hajic?
</prevsent>
<prevsent>et al 2009).3both the training/development/test and the cross validation splits are available at www.inf.u-szeged.
</prevsent>
</prevsection>
<citsent citstr=" E03-1012 ">
hu/rgai/szegedtreebank.tools: we employed finite state automata based morphological analyser constructed from the morphdb.hu lexical resource (tron et al 2006) and we used the msd-style morphological code system of the szeged treebank (alexin etal., 2003).<papid> E03-1012 </papid></citsent>
<aftsection>
<nextsent>the output of the morphological analyser is set of possible lemmamorphologicalanalysis pairs.
</nextsent>
<nextsent>this set of possible morphological analyses for word form is then used as possible alternatives ? instead of open and closed tag sets ? in standard sequential pos tagger.
</nextsent>
<nextsent>here, we applied the conditional random fields-based stanford pos tagger (toutanova et al 2003) <papid> N03-1033 </papid>and carried out 5-fold-cross pos training/tagging in side the subcorpora.4 for the english experiments we used the predicted pos tags provided for the conll-2009 shared task (hajic?</nextsent>
<nextsent>et al 2009).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I623">
<title id=" E12-1007.xml">dependency parsing of hungarian baseline results and challenges </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>the output of the morphological analyser is set of possible lemmamorphologicalanalysis pairs.
</prevsent>
<prevsent>this set of possible morphological analyses for word form is then used as possible alternatives ? instead of open and closed tag sets ? in standard sequential pos tagger.
</prevsent>
</prevsection>
<citsent citstr=" N03-1033 ">
here, we applied the conditional random fields-based stanford pos tagger (toutanova et al 2003) <papid> N03-1033 </papid>and carried out 5-fold-cross pos training/tagging in side the subcorpora.4 for the english experiments we used the predicted pos tags provided for the conll-2009 shared task (hajic?</citsent>
<aftsection>
<nextsent>et al 2009).
</nextsent>
<nextsent>as the dependency parser we employed threestate-of-the-art data-driven parsers, transition based parser (malt) and two graph-based parsers (mst and mate parsers).
</nextsent>
<nextsent>the malt parser (nivre et al 2004) <papid> W04-2407 </papid>is transition-based system, which uses an arc-eager system along with support vector machines to learn the scoring function for transitions and which uses greedy, deterministic onebest search at parsing time.</nextsent>
<nextsent>as one of the graph based parsers, we employed the mst parser (mc donald et al 2005) <papid> H05-1066 </papid>with second-order feature decoder.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I627">
<title id=" E12-1007.xml">dependency parsing of hungarian baseline results and challenges </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>the malt parser (nivre et al 2004) <papid> W04-2407 </papid>is transition-based system, which uses an arc-eager system along with support vector machines to learn the scoring function for transitions and which uses greedy, deterministic onebest search at parsing time.</prevsent>
<prevsent>as one of the graph based parsers, we employed the mst parser (mc donald et al 2005) <papid> H05-1066 </papid>with second-order feature decoder.</prevsent>
</prevsection>
<citsent citstr=" D07-1101 ">
it uses an approximate exhaustive search for unlabeled parsing, then separate arc label classifier is applied to label each arc. the mate parser (bohnet, 2010) <papid> C10-1011 </papid>is an efficient second order dependency parser that models the interaction between siblings as well as grandchildren (car reras, 2007).<papid> D07-1101 </papid></citsent>
<aftsection>
<nextsent>its decoder works on labeled edges, i.e. it uses single-step approach for obtaining labeled dependency trees.
</nextsent>
<nextsent>mate uses rich and4the java implementation of the morphological analyser and the slightly modified pos tagger along with trained models are available at http://www.inf.u-szeged.
</nextsent>
<nextsent>hu/rgai/magyarlanc.
</nextsent>
<nextsent>58 corpus #sent.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I630">
<title id=" E12-1007.xml">dependency parsing of hungarian baseline results and challenges </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>the most frequent morphological features which cannot be disam biguated at the word level are related to suffixes with multiple functions or the word itself cannot be unambiguously segmented into morphemes.
</prevsent>
<prevsent>although the number of such ambiguous cases is low, they form important features for the parser, thus we will focus on the more accurate handling of these cases in future work.
</prevsent>
</prevsection>
<citsent citstr=" D07-1097 ">
comparison to conll-2007 results: the best performing participant of the conll-2007 shared task (nivre et al 2007) <papid> D07-1096 </papid>achieved an ula of 83.6 and las of 80.3 (hall et al 2007) <papid> D07-1097 </papid>on the hungarian corpus.</citsent>
<aftsection>
<nextsent>the difference between the top performing english and hungarian systems were 8.14 ula and 9.3 las.
</nextsent>
<nextsent>the results reported in 2007 were significantly lower and the gap between english and hungarian is higher than our current values.
</nextsent>
<nextsent>to locate the sources of difference we carried out other experiments with mate on the conll-2007 dataset using the gold-standard pos tags (the shared task used gold-standard pos tags for evaluation).
</nextsent>
<nextsent>first we trained and evaluated mate on the original conll-2007 datasets, where it achieved ula 84.3 and las 80.0.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I631">
<title id=" E12-2007.xml">just title it by an online application </title>
<section> text titling application.  </section>
<citcontext>
<prevsection>
<prevsent>for each of them, an example of the produced title is given on the following sample text: in her speech, mrs merkel has promised concrete steps towards fiscal union - in effect close integration of the tax-and-spend polices of individual eur ozone countries, with brussels imposing penalties on members that break the rules.[...]?.
</prevsent>
<prevsent>even if examples are in english, the application is actually in french (but easily reproducible in english).
</prevsent>
</prevsection>
<citsent citstr=" P84-1004 ">
the pos tagging was performed by sygfran (chauche?, 1984).<papid> P84-1004 </papid></citsent>
<aftsection>
<nextsent>2.1 postit.
</nextsent>
<nextsent>(jin, 2001) implemented set of title generation methods and evaluated them: the statistical approach based on the tf-idf obtains the best results.
</nextsent>
<nextsent>in the same way, the postit (titling using position and statistical information) method uses statistical information.
</nextsent>
<nextsent>related works have shown that verbs are not as widely spread as nouns, named entities, and adjectives (lopez, 2011a).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I632">
<title id=" E09-1053.xml">dependency trees and the strong generative capacity of ccg </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we then use these dependency trees to compare the strong generative capacities of ccg and tag and obtain surprising results: both formalisms generate the same languages of derivation trees ? but the mechanisms they use to bring the words in these trees into linear order are incomparable.
</prevsent>
<prevsent>combinatory categorial grammar (ccg; steedman (2001)) is an increasingly popular grammarformalism.
</prevsent>
</prevsection>
<citsent citstr=" J07-4004 ">
next to being theoretically well-mo tivated due to its links to combinatory logic and categorial grammar, it is distinguished by the availability of efficient open-source parsers (clark and curran, 2007), <papid> J07-4004 </papid>annotated corpora (hockenmaier and steedman, 2007; <papid> J07-3004 </papid>hockenmaier, 2006), <papid> P06-1064 </papid>and mechanisms for wide-coverage semantic construction (bos et al, 2004).<papid> C04-1180 </papid></citsent>
<aftsection>
<nextsent>however, there are limits to our understanding of the formal properties of ccg and its relation to other grammar formalisms.
</nextsent>
<nextsent>in particular, while it is well-known that ccg belongs to family of mildly context-sensitive formalisms that all generate the same string languages (vijay-shanker and weir, 1994), there are few results about the strong generative capacity of ccg.
</nextsent>
<nextsent>this makes it difficult to gauge the similarities and differences betweenccg and other formalisms in how they model linguistic phenomena such as scrambling and relative clauses (hockenmaier and young, 2008), and hampers the transfer of algorithms from one formalism to another.in this paper, we propose new method for deriving dependency tree from ccg derivation tree for pf-ccg, large fragment of ccg.
</nextsent>
<nextsent>we then explore the strong generative capacity of pf-ccg in terms of dependency trees.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I633">
<title id=" E09-1053.xml">dependency trees and the strong generative capacity of ccg </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we then use these dependency trees to compare the strong generative capacities of ccg and tag and obtain surprising results: both formalisms generate the same languages of derivation trees ? but the mechanisms they use to bring the words in these trees into linear order are incomparable.
</prevsent>
<prevsent>combinatory categorial grammar (ccg; steedman (2001)) is an increasingly popular grammarformalism.
</prevsent>
</prevsection>
<citsent citstr=" J07-3004 ">
next to being theoretically well-mo tivated due to its links to combinatory logic and categorial grammar, it is distinguished by the availability of efficient open-source parsers (clark and curran, 2007), <papid> J07-4004 </papid>annotated corpora (hockenmaier and steedman, 2007; <papid> J07-3004 </papid>hockenmaier, 2006), <papid> P06-1064 </papid>and mechanisms for wide-coverage semantic construction (bos et al, 2004).<papid> C04-1180 </papid></citsent>
<aftsection>
<nextsent>however, there are limits to our understanding of the formal properties of ccg and its relation to other grammar formalisms.
</nextsent>
<nextsent>in particular, while it is well-known that ccg belongs to family of mildly context-sensitive formalisms that all generate the same string languages (vijay-shanker and weir, 1994), there are few results about the strong generative capacity of ccg.
</nextsent>
<nextsent>this makes it difficult to gauge the similarities and differences betweenccg and other formalisms in how they model linguistic phenomena such as scrambling and relative clauses (hockenmaier and young, 2008), and hampers the transfer of algorithms from one formalism to another.in this paper, we propose new method for deriving dependency tree from ccg derivation tree for pf-ccg, large fragment of ccg.
</nextsent>
<nextsent>we then explore the strong generative capacity of pf-ccg in terms of dependency trees.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I634">
<title id=" E09-1053.xml">dependency trees and the strong generative capacity of ccg </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we then use these dependency trees to compare the strong generative capacities of ccg and tag and obtain surprising results: both formalisms generate the same languages of derivation trees ? but the mechanisms they use to bring the words in these trees into linear order are incomparable.
</prevsent>
<prevsent>combinatory categorial grammar (ccg; steedman (2001)) is an increasingly popular grammarformalism.
</prevsent>
</prevsection>
<citsent citstr=" P06-1064 ">
next to being theoretically well-mo tivated due to its links to combinatory logic and categorial grammar, it is distinguished by the availability of efficient open-source parsers (clark and curran, 2007), <papid> J07-4004 </papid>annotated corpora (hockenmaier and steedman, 2007; <papid> J07-3004 </papid>hockenmaier, 2006), <papid> P06-1064 </papid>and mechanisms for wide-coverage semantic construction (bos et al, 2004).<papid> C04-1180 </papid></citsent>
<aftsection>
<nextsent>however, there are limits to our understanding of the formal properties of ccg and its relation to other grammar formalisms.
</nextsent>
<nextsent>in particular, while it is well-known that ccg belongs to family of mildly context-sensitive formalisms that all generate the same string languages (vijay-shanker and weir, 1994), there are few results about the strong generative capacity of ccg.
</nextsent>
<nextsent>this makes it difficult to gauge the similarities and differences betweenccg and other formalisms in how they model linguistic phenomena such as scrambling and relative clauses (hockenmaier and young, 2008), and hampers the transfer of algorithms from one formalism to another.in this paper, we propose new method for deriving dependency tree from ccg derivation tree for pf-ccg, large fragment of ccg.
</nextsent>
<nextsent>we then explore the strong generative capacity of pf-ccg in terms of dependency trees.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I635">
<title id=" E09-1053.xml">dependency trees and the strong generative capacity of ccg </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we then use these dependency trees to compare the strong generative capacities of ccg and tag and obtain surprising results: both formalisms generate the same languages of derivation trees ? but the mechanisms they use to bring the words in these trees into linear order are incomparable.
</prevsent>
<prevsent>combinatory categorial grammar (ccg; steedman (2001)) is an increasingly popular grammarformalism.
</prevsent>
</prevsection>
<citsent citstr=" C04-1180 ">
next to being theoretically well-mo tivated due to its links to combinatory logic and categorial grammar, it is distinguished by the availability of efficient open-source parsers (clark and curran, 2007), <papid> J07-4004 </papid>annotated corpora (hockenmaier and steedman, 2007; <papid> J07-3004 </papid>hockenmaier, 2006), <papid> P06-1064 </papid>and mechanisms for wide-coverage semantic construction (bos et al, 2004).<papid> C04-1180 </papid></citsent>
<aftsection>
<nextsent>however, there are limits to our understanding of the formal properties of ccg and its relation to other grammar formalisms.
</nextsent>
<nextsent>in particular, while it is well-known that ccg belongs to family of mildly context-sensitive formalisms that all generate the same string languages (vijay-shanker and weir, 1994), there are few results about the strong generative capacity of ccg.
</nextsent>
<nextsent>this makes it difficult to gauge the similarities and differences betweenccg and other formalisms in how they model linguistic phenomena such as scrambling and relative clauses (hockenmaier and young, 2008), and hampers the transfer of algorithms from one formalism to another.in this paper, we propose new method for deriving dependency tree from ccg derivation tree for pf-ccg, large fragment of ccg.
</nextsent>
<nextsent>we then explore the strong generative capacity of pf-ccg in terms of dependency trees.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I636">
<title id=" E09-1053.xml">dependency trees and the strong generative capacity of ccg </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>to our knowledge, this isthe first time that the regularity of ccgs deriva tional structures has been exposed.
</prevsent>
<prevsent>however, if we take the word order into account, then the classes of pf-ccg-induced and tag-induced dependency trees are incomparable; in particular, ccg-induceddependency trees can be unboundedly non-project ive in way that tag-induced dependency trees cannot.
</prevsent>
</prevsection>
<citsent citstr=" H05-1066 ">
the fact that all our dependency structures are trees brings our approach in line with the emerging mainstream in dependency parsing (mcdonald et al., 2005; <papid> H05-1066 </papid>nivre et al, 2007) and tag derivation trees.</citsent>
<aftsection>
<nextsent>the price we pay for restricting ourselves to trees is that we derive fewer dependencies than themore powerful approach by clark et al (2002).<papid> P02-1042 </papid></nextsent>
<nextsent>indeed, we do not claim that our dependencies are linguistically meaningful beyond recording the way in which syntactic valencies are filled.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I637">
<title id=" E09-1053.xml">dependency trees and the strong generative capacity of ccg </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, if we take the word order into account, then the classes of pf-ccg-induced and tag-induced dependency trees are incomparable; in particular, ccg-induceddependency trees can be unboundedly non-project ive in way that tag-induced dependency trees cannot.
</prevsent>
<prevsent>the fact that all our dependency structures are trees brings our approach in line with the emerging mainstream in dependency parsing (mcdonald et al., 2005; <papid> H05-1066 </papid>nivre et al, 2007) and tag derivation trees.</prevsent>
</prevsection>
<citsent citstr=" P02-1042 ">
the price we pay for restricting ourselves to trees is that we derive fewer dependencies than themore powerful approach by clark et al (2002).<papid> P02-1042 </papid></citsent>
<aftsection>
<nextsent>indeed, we do not claim that our dependencies are linguistically meaningful beyond recording the way in which syntactic valencies are filled.
</nextsent>
<nextsent>however, we show that our dependency trees are still informative enough to reconstruct the semantic representations.
</nextsent>
<nextsent>the paper is structured as follows.
</nextsent>
<nextsent>in section 2, we introduce ccg and the fragment pf-ccg thatwe consider in this paper, and compare our contribution to earlier research.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I640">
<title id=" E09-1053.xml">dependency trees and the strong generative capacity of ccg </title>
<section> induction of dependency trees.  </section>
<citcontext>
<prevsection>
<prevsent>when talking about dependency tree, it is usually convenient to specify its tree structure and the linear order of its nodes separately.
</prevsent>
<prevsent>the tree structure encodes the valency structure of the sentence (im mediate dominance), whereas the linear precedence of the words is captured by the linear order.
</prevsent>
</prevsection>
<citsent citstr=" P07-1021 ">
for the purposes of this paper, we represent dependency tree as pair = (t, s), where is ground term over some suitable alphabet, and is linearization of the nodes (term addresses) of t, where by linearization of set we mean list of elements of in which each element occurs exactly once (see also kuhlmann and mhl (2007)).<papid> P07-1021 </papid></citsent>
<aftsection>
<nextsent>as examples, consider (f(a, b), [1, ?, 2]) and (f(g(a)), [1 ? 1, ?, 1]) . these expressions represent the dependency trees d1 = f and d2 = f .notice that it is because of the separate specification of the tree and the order that dependency trees can become non-projective; d2 is an example.
</nextsent>
<nextsent>a partial dependency tree is pair (t, s) where is term that may contain variables, and is linearization of those nodes of that are not labelled with variables.
</nextsent>
<nextsent>we restrict ourselves to terms in which each variable appears exactly once, and will also prefix partial dependency trees with ?-binders to order the variables.
</nextsent>
<nextsent>462 = (a,a |am ? ?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I644">
<title id=" E09-1053.xml">dependency trees and the strong generative capacity of ccg </title>
<section> conclusion.  </section>
<citcontext>
<prevsection>
<prevsent>we would then be set to compare the behavior of wide-coveragestatistical parsers for ccg with statistical dependency parsers.
</prevsent>
<prevsent>we anticipate that our results about the strong generative capacity of pf-ccg will be useful to transfer algorithms and linguistic insights between formalisms.
</prevsent>
</prevsection>
<citsent citstr=" P07-1043 ">
for instance, the crisp generation algorithm (koller and stone, 2007), <papid> P07-1043 </papid>while specified for tag, could be generalized to arbitrary grammar formalisms that use regular tree languages?</citsent>
<aftsection>
<nextsent>given our results, to ccg in particular.
</nextsent>
<nextsent>on the other hand, we find it striking that ccg and tag generate the same string languages from the same tree languages by incomparable mechanisms for ordering the words in the tree.
</nextsent>
<nextsent>indeed, the exact characterization of the class of ccg-inducable dependency languages is an open issue.
</nextsent>
<nextsent>this also has consequences for parsing complexity: we can understand why tag and lcfrs can be parsed in polynomial time from the bounded block-degree of their dependency trees (kuhlmann and mhl, 2007), but ccg can be parsed in polynomial time(vijay-shanker and weir, 1990) without being restricted in this way.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I645">
<title id=" E09-1051.xml">optimization in coreference resolution is not needed a nearly optimal algorithm with intensional constraints </title>
<section> our baseline model.  </section>
<citcontext>
<prevsection>
<prevsent>the complexity thus remains exponential in the worst case, but the initial ordering of weights is clever guide.1maximization and coping with?-constraints are also accessible via simple transformations.
</prevsent>
<prevsent>the memory-based learner timbl (daelemans et al , 2004) is used as (pairwise) classifier.timbl stores all training examples, learns feature weights and classifies test instances according to the majority class of the k-nearest (i.e. most similar) neighbors.
</prevsent>
</prevsection>
<citsent citstr=" J01-4004 ">
we have experimented with various features; table 1 lists the set we have finally used (soon et al  (2001) <papid> J01-4004 </papid>and ng and cardie (2002) <papid> P02-1014 </papid>more thoroughly discuss different features and their benefits): - distance in sentences and markables - part of speech of the head of the markables - the grammatical functions - parallelism of grammatical functions - do the heads match or not - where is the pronoun (if any): left or right - word form if pos is pronoun - salience of the non-pronominal phrases - semantic class of noun phrase heads table 1: features for pairwise classification as gold standard the tuba-d/z (telljohann et al , 2005; naumann, 2006) coreference corpus is used.</citsent>
<aftsection>
<nextsent>the tuba is treebank (1,100 german newspaper texts, 25,000 sentences) augmented with coreference annotations2.
</nextsent>
<nextsent>in total, there are 13,818 anaphoric, 1,031 cataphoric and 12,752 coreferential relations.
</nextsent>
<nextsent>there are 3,295 relative pronouns, 8,929 personal pronouns, 2,987 reflexive pronouns, and 3,921 possessive pronouns.
</nextsent>
<nextsent>there are some rather long texts in the tubacorpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I646">
<title id=" E09-1051.xml">optimization in coreference resolution is not needed a nearly optimal algorithm with intensional constraints </title>
<section> our baseline model.  </section>
<citcontext>
<prevsection>
<prevsent>the complexity thus remains exponential in the worst case, but the initial ordering of weights is clever guide.1maximization and coping with?-constraints are also accessible via simple transformations.
</prevsent>
<prevsent>the memory-based learner timbl (daelemans et al , 2004) is used as (pairwise) classifier.timbl stores all training examples, learns feature weights and classifies test instances according to the majority class of the k-nearest (i.e. most similar) neighbors.
</prevsent>
</prevsection>
<citsent citstr=" P02-1014 ">
we have experimented with various features; table 1 lists the set we have finally used (soon et al  (2001) <papid> J01-4004 </papid>and ng and cardie (2002) <papid> P02-1014 </papid>more thoroughly discuss different features and their benefits): - distance in sentences and markables - part of speech of the head of the markables - the grammatical functions - parallelism of grammatical functions - do the heads match or not - where is the pronoun (if any): left or right - word form if pos is pronoun - salience of the non-pronominal phrases - semantic class of noun phrase heads table 1: features for pairwise classification as gold standard the tuba-d/z (telljohann et al , 2005; naumann, 2006) coreference corpus is used.</citsent>
<aftsection>
<nextsent>the tuba is treebank (1,100 german newspaper texts, 25,000 sentences) augmented with coreference annotations2.
</nextsent>
<nextsent>in total, there are 13,818 anaphoric, 1,031 cataphoric and 12,752 coreferential relations.
</nextsent>
<nextsent>there are 3,295 relative pronouns, 8,929 personal pronouns, 2,987 reflexive pronouns, and 3,921 possessive pronouns.
</nextsent>
<nextsent>there are some rather long texts in the tubacorpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I647">
<title id=" E09-1051.xml">optimization in coreference resolution is not needed a nearly optimal algorithm with intensional constraints </title>
<section> our constraint-based model.  </section>
<citcontext>
<prevsection>
<prevsent>o0.5 (2) pair i, j?
</prevsent>
<prevsent>is either co referent or not.
</prevsent>
</prevsection>
<citsent citstr=" P08-2012 ">
transit ivity is captured by (see (finkel and manning, 2008) <papid> P08-2012 </papid>for an alternative but equivalent formalization): ci + jk ? cik +1, i, j,k (i     k) cik + jk ? ci +1, i, j,k (i     k) ci + cik ? jk +1, i, j,k (i     k) (3)in order to take full advantage of ilps reasoning capacities, three equations are needed given three markables.</citsent>
<aftsection>
<nextsent>the extensionalization of tran sit ivity thus produces n!3!(n3)!
</nextsent>
<nextsent>3 equations for markables.
</nextsent>
<nextsent>note that transitivityas globalconstraintought to spread over the whole candidate set, not just within in the window.transitivity without further constraints is point less.6 what we really can gain from transit ivity is consistency at the linguistic level, namely (glob ally) adhering to exclusiveness constraints (cf.
</nextsent>
<nextsent>the example in the introduction).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I649">
<title id=" E09-1051.xml">optimization in coreference resolution is not needed a nearly optimal algorithm with intensional constraints </title>
<section> representing ilp constraints.  </section>
<citcontext>
<prevsection>
<prevsent>in general, two markables that np-bind each other are exclusive: ci = 0, i, (np bound(i, j)) (5)
</prevsent>
<prevsent>intensionallyexisting ilp-based approaches to nlp (e.g.
</prevsent>
</prevsection>
<citsent citstr=" C04-1197 ">
(pun yakanok et al , 2004; <papid> C04-1197 </papid>althaus et al , 2004; <papid> P04-1051 </papid>marciniak and strube, 2005)) <papid> W05-0618 </papid>belong to the class of zero-one ilp: only binary variables are needed.</citsent>
<aftsection>
<nextsent>this has been seldom remarked (butsee (althaus et al , 2004)) <papid> P04-1051 </papid>and generic (out-ofthe-box) ilp implementations are used.</nextsent>
<nextsent>more over, these models form very restricted variant of zero-one ilp: the constraints come without anyweights.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I650">
<title id=" E09-1051.xml">optimization in coreference resolution is not needed a nearly optimal algorithm with intensional constraints </title>
<section> representing ilp constraints.  </section>
<citcontext>
<prevsection>
<prevsent>in general, two markables that np-bind each other are exclusive: ci = 0, i, (np bound(i, j)) (5)
</prevsent>
<prevsent>intensionallyexisting ilp-based approaches to nlp (e.g.
</prevsent>
</prevsection>
<citsent citstr=" P04-1051 ">
(pun yakanok et al , 2004; <papid> C04-1197 </papid>althaus et al , 2004; <papid> P04-1051 </papid>marciniak and strube, 2005)) <papid> W05-0618 </papid>belong to the class of zero-one ilp: only binary variables are needed.</citsent>
<aftsection>
<nextsent>this has been seldom remarked (butsee (althaus et al , 2004)) <papid> P04-1051 </papid>and generic (out-ofthe-box) ilp implementations are used.</nextsent>
<nextsent>more over, these models form very restricted variant of zero-one ilp: the constraints come without anyweights.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I651">
<title id=" E09-1051.xml">optimization in coreference resolution is not needed a nearly optimal algorithm with intensional constraints </title>
<section> representing ilp constraints.  </section>
<citcontext>
<prevsection>
<prevsent>in general, two markables that np-bind each other are exclusive: ci = 0, i, (np bound(i, j)) (5)
</prevsent>
<prevsent>intensionallyexisting ilp-based approaches to nlp (e.g.
</prevsent>
</prevsection>
<citsent citstr=" W05-0618 ">
(pun yakanok et al , 2004; <papid> C04-1197 </papid>althaus et al , 2004; <papid> P04-1051 </papid>marciniak and strube, 2005)) <papid> W05-0618 </papid>belong to the class of zero-one ilp: only binary variables are needed.</citsent>
<aftsection>
<nextsent>this has been seldom remarked (butsee (althaus et al , 2004)) <papid> P04-1051 </papid>and generic (out-ofthe-box) ilp implementations are used.</nextsent>
<nextsent>more over, these models form very restricted variant of zero-one ilp: the constraints come without anyweights.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I653">
<title id=" E09-1051.xml">optimization in coreference resolution is not needed a nearly optimal algorithm with intensional constraints </title>
<section> does balas-first help? empirical.  </section>
<citcontext>
<prevsection>
<prevsent>each experiment was carried out in two variants.
</prevsent>
<prevsent>one where all markables have been taken as inputan application-oriented setting, and one where only markables that represent true mentions have been taken (cf.
</prevsent>
</prevsection>
<citsent citstr=" P04-1018 ">
(luo et al ,2004; <papid> P04-1018 </papid>ponzetto and strube, 2006) <papid> N06-1025 </papid>for other approaches with an evaluation based on true mentions only).</citsent>
<aftsection>
<nextsent>the assumption is that if only true mentions are considered, the effects of model can be better measured.
</nextsent>
<nextsent>we have used the entity-constrained measure (ecm), introduced in (luo et al , 2004; <papid> P04-1018 </papid>luo, 2005).<papid> H05-1004 </papid></nextsent>
<nextsent>as argued in (klenner and ailloud, 2008), it is more appropriate to evaluate the quality of coreference sets than the muc score.9 to obtain the baseline, we merged all pairs that timbl classified as co referent into coreference sets.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I654">
<title id=" E09-1051.xml">optimization in coreference resolution is not needed a nearly optimal algorithm with intensional constraints </title>
<section> does balas-first help? empirical.  </section>
<citcontext>
<prevsection>
<prevsent>each experiment was carried out in two variants.
</prevsent>
<prevsent>one where all markables have been taken as inputan application-oriented setting, and one where only markables that represent true mentions have been taken (cf.
</prevsent>
</prevsection>
<citsent citstr=" N06-1025 ">
(luo et al ,2004; <papid> P04-1018 </papid>ponzetto and strube, 2006) <papid> N06-1025 </papid>for other approaches with an evaluation based on true mentions only).</citsent>
<aftsection>
<nextsent>the assumption is that if only true mentions are considered, the effects of model can be better measured.
</nextsent>
<nextsent>we have used the entity-constrained measure (ecm), introduced in (luo et al , 2004; <papid> P04-1018 </papid>luo, 2005).<papid> H05-1004 </papid></nextsent>
<nextsent>as argued in (klenner and ailloud, 2008), it is more appropriate to evaluate the quality of coreference sets than the muc score.9 to obtain the baseline, we merged all pairs that timbl classified as co referent into coreference sets.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I656">
<title id=" E09-1051.xml">optimization in coreference resolution is not needed a nearly optimal algorithm with intensional constraints </title>
<section> does balas-first help? empirical.  </section>
<citcontext>
<prevsection>
<prevsent>(luo et al ,2004; <papid> P04-1018 </papid>ponzetto and strube, 2006) <papid> N06-1025 </papid>for other approaches with an evaluation based on true mentions only).</prevsent>
<prevsent>the assumption is that if only true mentions are considered, the effects of model can be better measured.</prevsent>
</prevsection>
<citsent citstr=" H05-1004 ">
we have used the entity-constrained measure (ecm), introduced in (luo et al , 2004; <papid> P04-1018 </papid>luo, 2005).<papid> H05-1004 </papid></citsent>
<aftsection>
<nextsent>as argued in (klenner and ailloud, 2008), it is more appropriate to evaluate the quality of coreference sets than the muc score.9 to obtain the baseline, we merged all pairs that timbl classified as co referent into coreference sets.
</nextsent>
<nextsent>table 2 shows the results.
</nextsent>
<nextsent>all mentions true mentions timbl b-first timbl b-first 61.83 64.27 71.47 78.90 66.52 72.05 73.81 84.10 57.76 58.00 69.28 74.31 table 2: balas-first (b-first) vs. baseline in the all mentions setting?, 2.4% f-measure improvement was achieved, with true mentions?
</nextsent>
<nextsent>it is 7.43%.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I658">
<title id=" E09-1051.xml">optimization in coreference resolution is not needed a nearly optimal algorithm with intensional constraints </title>
<section> does balas-first help? empirical.  </section>
<citcontext>
<prevsection>
<prevsent>linear ordering over pairs is established by sorting according to the index of the first pair element (the from ci j).
</prevsent>
<prevsent>all mentions true mentions linear b-first linear b-first 62.83 64.27 76.08 78.90 70.39 72.05 81.40 84.10 56.73 58.00 71.41 74.31 table 3: balas order vs. linear order our experiments (cf.
</prevsent>
</prevsection>
<citsent citstr=" W06-1633 ">
table 3) indicate that the 9various authors have remarked on the shortcomings of the muc evaluation scheme (bagga and baldwin, 1998; luo, 2005; <papid> H05-1004 </papid>nicolae and nicolae, 2006).<papid> W06-1633 </papid></citsent>
<aftsection>
<nextsent>448 balas ordering does affect the empirical results.the f-measure improvement is 1.44% (all men tions?)
</nextsent>
<nextsent>and 2.82% (true mentions?).
</nextsent>
<nextsent>the search for balas-first remains, in general,np-complete.
</nextsent>
<nextsent>however, constraint models without bound ness enforcement constraints (cf.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I659">
<title id=" E09-1051.xml">optimization in coreference resolution is not needed a nearly optimal algorithm with intensional constraints </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>common to all ilp approaches (incl.
</prevsent>
<prevsent>ours) is that they apply ilp on the output of pairwise machine-learning.
</prevsent>
</prevsection>
<citsent citstr=" N07-1030 ">
denis and baldridge (2007), <papid> N07-1030 </papid>denis and baldridge (2008) <papid> D08-1069 </papid>have an ilp model to jointly determine anaphoricity and coreference, but take neither transit ivity nor exclusivity into account.</citsent>
<aftsection>
<nextsent>so no complexity problems arise in their approach.
</nextsent>
<nextsent>the model from (finkel and manning, 2008) <papid> P08-2012 </papid>utilizes transit ivity, but not exclusivity.</nextsent>
<nextsent>the benefits oftransitivity are thus restricted to an optimal balancing of the weights (e.g. given two positively classified pairs, the transit ively given third pair in some cases is negative, ilp globally resolves these cases to the optimal solution).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I660">
<title id=" E09-1051.xml">optimization in coreference resolution is not needed a nearly optimal algorithm with intensional constraints </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>common to all ilp approaches (incl.
</prevsent>
<prevsent>ours) is that they apply ilp on the output of pairwise machine-learning.
</prevsent>
</prevsection>
<citsent citstr=" D08-1069 ">
denis and baldridge (2007), <papid> N07-1030 </papid>denis and baldridge (2008) <papid> D08-1069 </papid>have an ilp model to jointly determine anaphoricity and coreference, but take neither transit ivity nor exclusivity into account.</citsent>
<aftsection>
<nextsent>so no complexity problems arise in their approach.
</nextsent>
<nextsent>the model from (finkel and manning, 2008) <papid> P08-2012 </papid>utilizes transit ivity, but not exclusivity.</nextsent>
<nextsent>the benefits oftransitivity are thus restricted to an optimal balancing of the weights (e.g. given two positively classified pairs, the transit ively given third pair in some cases is negative, ilp globally resolves these cases to the optimal solution).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I663">
<title id=" E09-1051.xml">optimization in coreference resolution is not needed a nearly optimal algorithm with intensional constraints </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>to overcome the overhead of transit ivity extensionalization, he proposes fixed transit ivity window.
</prevsent>
<prevsent>this, however,is bound to produce transit ivity gaps, so the benefits of complete transit ivity propagation are lost.
</prevsent>
</prevsection>
<citsent citstr=" W06-1616 ">
another attempt to overcome the problem of complexity with ilp models is described in (riedel and clarke, 2006) (<papid> W06-1616 </papid>dependency parsing).</citsent>
<aftsection>
<nextsent>here an incremental or better, cascaded ilp model is proposed, where at each cascade only those constraints are added that have been violated in the preceding one.
</nextsent>
<nextsent>the search stops with the first consistent solution (as we suggest in the present paper).
</nextsent>
<nextsent>however, it is difficult to quantify the number of cascades needed to come to it and moreover, the full ilp machinery is being used (so again, constraints need to be extensionalized).
</nextsent>
<nextsent>to the best of our knowledge, our work is the first that studies the proper utility of ilp optimization for nlp, while offering an intensional alternative to ilp constraints.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I664">
<title id=" E09-1009.xml">incremental parsing with parallel multiple context free grammars </title>
<section> pmcfg definition.  </section>
<citcontext>
<prevsection>
<prevsent>for the production to be well formed the conditions di(f) = d(ai) (1 ? ? a(f)) and r(f) = d(a) must hold.
</prevsent>
<prevsent>s is the start category and d(s) = 1.
</prevsent>
</prevsection>
<citsent citstr=" P93-1018 ">
we use the same definition of pmcfg as is used by seki and kato (2008) and seki et al (1993) <papid> P93-1018 </papid>with the minor difference that they use variable names like xkl while we use k; l?</citsent>
<aftsection>
<nextsent>to refer to the function arguments.
</nextsent>
<nextsent>as an example we will use the anbncn language: ? c[n ] ? s[n ] ? z[] := (1; 1?
</nextsent>
<nextsent>1; 2?
</nextsent>
<nextsent>1; 3?)
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I665">
<title id=" E12-1069.xml">identifying broken plurals irregular gender and rationality in arabic text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>furthermore, we show that for unseen words, morphological features help beyond orthographic features and that syntactic features help even more.a combination of the two techniques improves overall performance even further.
</prevsent>
<prevsent>arabic morphology is complex, partly because of its richness, and partly because of its complex morpho-syntactic agreement rules which depend on functional features not necessarily expressed in word forms.
</prevsent>
</prevsection>
<citsent citstr=" P11-2062 ">
particularly challenging are broken plurals (which resemble singular nouns), nouns with irregular gender (mascu line nouns that look feminine and feminine nouns that look masculine), and the semantic feature of rationality, which has no morphological realization (smr?, 2007b; alkuhlani and habash,2011).<papid> P11-2062 </papid></citsent>
<aftsection>
<nextsent>these features heavily participate in arabic morpho-syntactic agreement.
</nextsent>
<nextsent>alkuhlani and habash (2011) <papid> P11-2062 </papid>show that without proper modeling, arabic agreement cannot be accounted for in about third of all noun-adjective pairs and quarter of verb-subject pairs.</nextsent>
<nextsent>they also report that over half of all plurals in arabic are irregular, 8% of nominals have irregular gender and almost half of all proper nouns and 5% of all nouns are rational.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I667">
<title id=" E12-1069.xml">identifying broken plurals irregular gender and rationality in arabic text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>they also report that over half of all plurals in arabic are irregular, 8% of nominals have irregular gender and almost half of all proper nouns and 5% of all nouns are rational.
</prevsent>
<prevsent>in this paper, we present results on the task of automatic identification of functional gender,number and rationality of arabic words in context.
</prevsent>
</prevsection>
<citsent citstr=" P03-1004 ">
we consider two supervised learning tech niques: simple maximum-likelihood model withback-off (mle) and support-vector-machinebased sequence tagger, yamcha (kudo and matsumoto, 2003).<papid> P03-1004 </papid></citsent>
<aftsection>
<nextsent>we consider large number of orthographic, morphological and syntactic learning features.
</nextsent>
<nextsent>our results show that the mle technique is preferred for words seen in the training data, while the yamcha technique is optimal for unseen words, which are our real target.
</nextsent>
<nextsent>furthermore, we show that for unseen words, morphological features help beyond orthographic features and that syntactic features help even more.
</nextsent>
<nextsent>acombination of the two techniques improves over all performance even further.this paper is structured as follows: sections 2 and 3 present relevant linguistic facts and related work, respectively.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I668">
<title id=" E12-1069.xml">identifying broken plurals irregular gender and rationality in arabic text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>alrby alqdym form ms ms mp ms fs nana ms ms ms func msn mpr mpn fpi fsn nanana msi msn msn gloss be-inspired the-writers the-modern stories new from culture arab ancient english modern writers are inspired by ancient arab culture to write new stories .?
</prevsent>
<prevsent>figure 1: an example arabic sentence showing its dependency representation together with the form-based and functional gender and number features and rationality.
</prevsent>
</prevsection>
<citsent citstr=" P09-2056 ">
the dependency tree is in the catib treebank representation (habash and roth, 2009).<papid> P09-2056 </papid></citsent>
<aftsection>
<nextsent>the shown pos tags are vrb verb?, nom nominal (noun/adjective)?, and prt particle?.
</nextsent>
<nextsent>the relations are sbj subject?, obj object?
</nextsent>
<nextsent>and mod modifier?.
</nextsent>
<nextsent>the form-based features are only for gender and number.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I674">
<title id=" E12-1069.xml">identifying broken plurals irregular gender and rationality in arabic text </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>much work has been done on arabic morphological analysis, morphological disambiguation and part-of-speech (pos) tagging (al-sughaiyer and al-kharashi, 2004; soudi et al  2007; habash, 2010).
</prevsent>
<prevsent>the bulk of this work does not address form-function discrepancy or morpho-syntacticagreement issues.
</prevsent>
</prevsection>
<citsent citstr=" N04-4038 ">
this includes the most commonly used resources and tools for arabic nlp: the buckwalter arabic morphological analyzer (bama) (buckwalter, 2004) which is used in the penn arabic tree bank (patb) (maamouri et al 2004), and the various pos tagging and morphological disambiguation tools trained using them (diab et al  2004; <papid> N04-4038 </papid>habash and rambow, 2005).<papid> P05-1071 </papid></citsent>
<aftsection>
<nextsent>there are some important exceptions (goweder et al ., 2004; <papid> W04-3232 </papid>habash, 2004; smr?, 2007b; elghamry et al  2008; abbs et al  2004; attia, 2008;3we previously defined the rationality value as not applicable when we only considered nominals (alkuhlani and habash, 2011).<papid> P11-2062 </papid></nextsent>
<nextsent>in this work, we rename the rationality value as not-specified without changing its meaning.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I675">
<title id=" E12-1069.xml">identifying broken plurals irregular gender and rationality in arabic text </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>much work has been done on arabic morphological analysis, morphological disambiguation and part-of-speech (pos) tagging (al-sughaiyer and al-kharashi, 2004; soudi et al  2007; habash, 2010).
</prevsent>
<prevsent>the bulk of this work does not address form-function discrepancy or morpho-syntacticagreement issues.
</prevsent>
</prevsection>
<citsent citstr=" P05-1071 ">
this includes the most commonly used resources and tools for arabic nlp: the buckwalter arabic morphological analyzer (bama) (buckwalter, 2004) which is used in the penn arabic tree bank (patb) (maamouri et al 2004), and the various pos tagging and morphological disambiguation tools trained using them (diab et al  2004; <papid> N04-4038 </papid>habash and rambow, 2005).<papid> P05-1071 </papid></citsent>
<aftsection>
<nextsent>there are some important exceptions (goweder et al ., 2004; <papid> W04-3232 </papid>habash, 2004; smr?, 2007b; elghamry et al  2008; abbs et al  2004; attia, 2008;3we previously defined the rationality value as not applicable when we only considered nominals (alkuhlani and habash, 2011).<papid> P11-2062 </papid></nextsent>
<nextsent>in this work, we rename the rationality value as not-specified without changing its meaning.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I678">
<title id=" E12-1069.xml">identifying broken plurals irregular gender and rationality in arabic text </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the bulk of this work does not address form-function discrepancy or morpho-syntacticagreement issues.
</prevsent>
<prevsent>this includes the most commonly used resources and tools for arabic nlp: the buckwalter arabic morphological analyzer (bama) (buckwalter, 2004) which is used in the penn arabic tree bank (patb) (maamouri et al 2004), and the various pos tagging and morphological disambiguation tools trained using them (diab et al  2004; <papid> N04-4038 </papid>habash and rambow, 2005).<papid> P05-1071 </papid></prevsent>
</prevsection>
<citsent citstr=" W04-3232 ">
there are some important exceptions (goweder et al ., 2004; <papid> W04-3232 </papid>habash, 2004; smr?, 2007b; elghamry et al  2008; abbs et al  2004; attia, 2008;3we previously defined the rationality value as not applicable when we only considered nominals (alkuhlani and habash, 2011).<papid> P11-2062 </papid></citsent>
<aftsection>
<nextsent>in this work, we rename the rationality value as not-specified without changing its meaning.
</nextsent>
<nextsent>we use the value na (not-applicable) for parts-of-speech thatdo not have meaningful value for any feature, e.g., prepositions have gender, number and rationality values of na.
</nextsent>
<nextsent>altantawy et al  2010; alkuhlani and habash, 2011).<papid> P11-2062 </papid></nextsent>
<nextsent>in terms of resources, smr?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I689">
<title id=" E12-1069.xml">identifying broken plurals irregular gender and rationality in arabic text </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>their automatic technique achieves anf-score of 89.7% against gold standard set.
</prevsent>
<prevsent>unlike them, we use manually annotated corpus to train and test the prediction of gender, number and rationality features.our approach to identifying these features explores large set of orthographic, morphological and syntactic learning features.
</prevsent>
</prevsection>
<citsent citstr=" W10-1402 ">
this is very much following several previous efforts in arabic nlpin which different tagsets and morphological features have been studied for variety of purposes,e.g., base phrase chunking (diab, 2007) and dependency parsing (marton et al  2010).<papid> W10-1402 </papid></citsent>
<aftsection>
<nextsent>in this paper we use the parser of marton et al (2010)<papid> W10-1402 </papid>as our source of syntactic learning features.</nextsent>
<nextsent>we follow their splits for training, development and testing.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I716">
<title id=" E12-1069.xml">identifying broken plurals irregular gender and rationality in arabic text </title>
<section> results.  </section>
<citcontext>
<prevsection>
<prevsent>6.9 extrinsic evaluation.
</prevsent>
<prevsent>we use the predicted gender, number and rationality features that we get from training on the full train set in dependency syntactic parsing experiment.
</prevsent>
</prevsection>
<citsent citstr=" P11-1159 ">
the parsing feature set we use is the best performing feature set described in (marton et al  2011), <papid> P11-1159 </papid>which used an earlier unpublished version of our mle model.</citsent>
<aftsection>
<nextsent>the parser we use is the easy first parser (goldberg and elhadad, 2010).<papid> N10-1115 </papid></nextsent>
<nextsent>more details on this parsing experiment is in marton et al .</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I717">
<title id=" E12-1069.xml">identifying broken plurals irregular gender and rationality in arabic text </title>
<section> results.  </section>
<citcontext>
<prevsection>
<prevsent>we use the predicted gender, number and rationality features that we get from training on the full train set in dependency syntactic parsing experiment.
</prevsent>
<prevsent>the parsing feature set we use is the best performing feature set described in (marton et al  2011), <papid> P11-1159 </papid>which used an earlier unpublished version of our mle model.</prevsent>
</prevsection>
<citsent citstr=" N10-1115 ">
the parser we use is the easy first parser (goldberg and elhadad, 2010).<papid> N10-1115 </papid></citsent>
<aftsection>
<nextsent>more details on this parsing experiment is in marton et al .
</nextsent>
<nextsent>(2012).the functional gender and number features increase the labeled attachment score by 0.4% absolute over comparable model that uses the form based gender and number features.
</nextsent>
<nextsent>rationality on the other hand does not help much.
</nextsent>
<nextsent>one possible reason for this is the lower quality of the predicted rationality feature compared to the other features.another possible reason is that the rationality feature is not utilized optimally in the parser.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I718">
<title id=" E12-1074.xml">determining the placement of german verbs in englishtogerman smt </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we obtain significant improvement in translation performance.
</prevsent>
<prevsent>phrase-based smt (psmt) systems translate word sequences (phrases) from source language into target language, performing reordering of target phrases in order to generate fluent target language output.
</prevsent>
</prevsection>
<citsent citstr=" P07-2045 ">
the reordering models, such as, for example, the models implemented in moses(koehn et al 2007), <papid> P07-2045 </papid>are often limited to certain reordering range since reordering beyond this distance cannot be performed accurately.</citsent>
<aftsection>
<nextsent>this results in problems of fluency for language pairs with large differences in constituent order, such as english and german.
</nextsent>
<nextsent>when translating from english to german, verbs in the german output are often incorrectly left near their position in english, creating problems of fluency.
</nextsent>
<nextsent>verbs are also often omitted since the distortion model cannot move verbs to positions which are licensed by the german language model, making the translations difficult to understand.a common approach for handling the lon grange reordering problem within psmt is performing syntax-based or part-of-speech-based(pos-based) reordering of the input as preprocessing step before translation (e.g., collins et al (2005), <papid> P05-1066 </papid>gupta et al(2007), habash (2007), xuet al(2009), <papid> N09-1028 </papid>niehues and kolss (2009), <papid> W09-0435 </papid>katz brown et al(2011), genzel (2010)).<papid> C10-1043 </papid></nextsent>
<nextsent>we reorder english to improve the translation to german.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I719">
<title id=" E12-1074.xml">determining the placement of german verbs in englishtogerman smt </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this results in problems of fluency for language pairs with large differences in constituent order, such as english and german.
</prevsent>
<prevsent>when translating from english to german, verbs in the german output are often incorrectly left near their position in english, creating problems of fluency.
</prevsent>
</prevsection>
<citsent citstr=" P05-1066 ">
verbs are also often omitted since the distortion model cannot move verbs to positions which are licensed by the german language model, making the translations difficult to understand.a common approach for handling the lon grange reordering problem within psmt is performing syntax-based or part-of-speech-based(pos-based) reordering of the input as preprocessing step before translation (e.g., collins et al (2005), <papid> P05-1066 </papid>gupta et al(2007), habash (2007), xuet al(2009), <papid> N09-1028 </papid>niehues and kolss (2009), <papid> W09-0435 </papid>katz brown et al(2011), genzel (2010)).<papid> C10-1043 </papid></citsent>
<aftsection>
<nextsent>we reorder english to improve the translation to german.
</nextsent>
<nextsent>the verb reordering process is implemented using deterministic reordering rules on english parse trees.
</nextsent>
<nextsent>the sequence of reordering sis derived from the clause type and the composition of given verbal complex (a (possibly discontiguous) sequence of verbal elements in single clause).
</nextsent>
<nextsent>only one rule can be applied in given context and for each word to be reordered, there is unique reordered position.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I720">
<title id=" E12-1074.xml">determining the placement of german verbs in englishtogerman smt </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this results in problems of fluency for language pairs with large differences in constituent order, such as english and german.
</prevsent>
<prevsent>when translating from english to german, verbs in the german output are often incorrectly left near their position in english, creating problems of fluency.
</prevsent>
</prevsection>
<citsent citstr=" N09-1028 ">
verbs are also often omitted since the distortion model cannot move verbs to positions which are licensed by the german language model, making the translations difficult to understand.a common approach for handling the lon grange reordering problem within psmt is performing syntax-based or part-of-speech-based(pos-based) reordering of the input as preprocessing step before translation (e.g., collins et al (2005), <papid> P05-1066 </papid>gupta et al(2007), habash (2007), xuet al(2009), <papid> N09-1028 </papid>niehues and kolss (2009), <papid> W09-0435 </papid>katz brown et al(2011), genzel (2010)).<papid> C10-1043 </papid></citsent>
<aftsection>
<nextsent>we reorder english to improve the translation to german.
</nextsent>
<nextsent>the verb reordering process is implemented using deterministic reordering rules on english parse trees.
</nextsent>
<nextsent>the sequence of reordering sis derived from the clause type and the composition of given verbal complex (a (possibly discontiguous) sequence of verbal elements in single clause).
</nextsent>
<nextsent>only one rule can be applied in given context and for each word to be reordered, there is unique reordered position.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I721">
<title id=" E12-1074.xml">determining the placement of german verbs in englishtogerman smt </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this results in problems of fluency for language pairs with large differences in constituent order, such as english and german.
</prevsent>
<prevsent>when translating from english to german, verbs in the german output are often incorrectly left near their position in english, creating problems of fluency.
</prevsent>
</prevsection>
<citsent citstr=" W09-0435 ">
verbs are also often omitted since the distortion model cannot move verbs to positions which are licensed by the german language model, making the translations difficult to understand.a common approach for handling the lon grange reordering problem within psmt is performing syntax-based or part-of-speech-based(pos-based) reordering of the input as preprocessing step before translation (e.g., collins et al (2005), <papid> P05-1066 </papid>gupta et al(2007), habash (2007), xuet al(2009), <papid> N09-1028 </papid>niehues and kolss (2009), <papid> W09-0435 </papid>katz brown et al(2011), genzel (2010)).<papid> C10-1043 </papid></citsent>
<aftsection>
<nextsent>we reorder english to improve the translation to german.
</nextsent>
<nextsent>the verb reordering process is implemented using deterministic reordering rules on english parse trees.
</nextsent>
<nextsent>the sequence of reordering sis derived from the clause type and the composition of given verbal complex (a (possibly discontiguous) sequence of verbal elements in single clause).
</nextsent>
<nextsent>only one rule can be applied in given context and for each word to be reordered, there is unique reordered position.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I722">
<title id=" E12-1074.xml">determining the placement of german verbs in englishtogerman smt </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this results in problems of fluency for language pairs with large differences in constituent order, such as english and german.
</prevsent>
<prevsent>when translating from english to german, verbs in the german output are often incorrectly left near their position in english, creating problems of fluency.
</prevsent>
</prevsection>
<citsent citstr=" C10-1043 ">
verbs are also often omitted since the distortion model cannot move verbs to positions which are licensed by the german language model, making the translations difficult to understand.a common approach for handling the lon grange reordering problem within psmt is performing syntax-based or part-of-speech-based(pos-based) reordering of the input as preprocessing step before translation (e.g., collins et al (2005), <papid> P05-1066 </papid>gupta et al(2007), habash (2007), xuet al(2009), <papid> N09-1028 </papid>niehues and kolss (2009), <papid> W09-0435 </papid>katz brown et al(2011), genzel (2010)).<papid> C10-1043 </papid></citsent>
<aftsection>
<nextsent>we reorder english to improve the translation to german.
</nextsent>
<nextsent>the verb reordering process is implemented using deterministic reordering rules on english parse trees.
</nextsent>
<nextsent>the sequence of reordering sis derived from the clause type and the composition of given verbal complex (a (possibly discontiguous) sequence of verbal elements in single clause).
</nextsent>
<nextsent>only one rule can be applied in given context and for each word to be reordered, there is unique reordered position.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I725">
<title id=" E12-1074.xml">determining the placement of german verbs in englishtogerman smt </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>therefore, we needto identify exactly which parts of verbal complex must be moved and their possible positions in german sentence.reordering rules can also be extracted automatically.
</prevsent>
<prevsent>for example, niehues and kolss (2009) <papid> W09-0435 </papid>automatically extracted dis contiguous reordering rules (allowing gaps between pos tags which can include an arbitrary number of words) from word-aligned parallel corpus with pos tagged source side.</prevsent>
</prevsection>
<citsent citstr=" P08-1115 ">
since many different rules can be applied on given sentence, number of reordered sentence alternatives are created which are encoded as word lattice (dyer et al 2008).<papid> P08-1115 </papid></citsent>
<aftsection>
<nextsent>they dealt with the translation directions german-to english and english-to-german, but translation improvement was obtained only for the germanto-english direction.
</nextsent>
<nextsent>this may be due to missing information about clause boundaries since english verbs often have to be moved to the clause end.
</nextsent>
<nextsent>our reordering has access to this kind of knowledge since we are working with full syntactic parser of english.genzel (2010) <papid> C10-1043 </papid>proposed language independent method for learning reordering rules where the rules are extracted from parsed source language sentences.</nextsent>
<nextsent>for each node, all possible reorderings (permutations) of limited number of the child nodes are considered.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I729">
<title id=" E12-1074.xml">determining the placement of german verbs in englishtogerman smt </title>
<section> reordering of the english input.  </section>
<citcontext>
<prevsection>
<prevsent>4.1 labeling clauses with their type.
</prevsent>
<prevsent>as shown in section 3.1, the verb positions in german depend on the clause type.
</prevsent>
</prevsection>
<citsent citstr=" P05-1022 ">
since we use english parse trees produced by the generative parser of charniak and johnson (2005) <papid> P05-1022 </papid>which do nothave any function labels, we implemented simple rule-based clause type labeling script which 728 ..</citsent>
<aftsection>
<nextsent>whnp which np dt nn book yesterday rb advp , , sextr prp np np jj nn last week np prp yesterday rb advp , , prp np sextr np jj nn last week np prp np dt nn book np ssub vp vbd read vbd bought np whnp which ssub vp vbd bought vbd read reordering read out and translate vp vp1 1figure 1: processing steps: clause type labeling annotates the given original tree with clause type labels(in figure, s-extr and s-sub).
</nextsent>
<nextsent>subsequently, there ordering is performed (cf.
</nextsent>
<nextsent>movement of the verbs read and bought).
</nextsent>
<nextsent>the reordered sentence is finally read out and given to the decoder.enriches every clause starting node with the corresponding clause type label.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I731">
<title id=" E12-1074.xml">determining the placement of german verbs in englishtogerman smt </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>note that there is an additional category rest which indicates incorrect clause type/tensecombinations and might thus not be correctly reordered.
</prevsent>
<prevsent>these are mostly due to parsing and/or tagging errors.
</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
the performance of the systems was measured by bleu (papineni et al 2002).<papid> P02-1040 </papid></citsent>
<aftsection>
<nextsent>the evaluation results are shown in table 4.
</nextsent>
<nextsent>the contrastive system outperforms the baseline.
</nextsent>
<nextsent>its bleu score is 13.63 which is gain of 0.61 bleu points overthe baseline.
</nextsent>
<nextsent>this is statistically significant improvement at 0.05 (computed with gimpelsimplementation of the pairwise bootstrap resam pling method (koehn, 2004)).<papid> W04-3250 </papid>manual examination of the translations produced by both systems confirms the result ofthe automatic evaluation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I732">
<title id=" E12-1074.xml">determining the placement of german verbs in englishtogerman smt </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>the contrastive system outperforms the baseline.
</prevsent>
<prevsent>its bleu score is 13.63 which is gain of 0.61 bleu points overthe baseline.
</prevsent>
</prevsection>
<citsent citstr=" W04-3250 ">
this is statistically significant improvement at 0.05 (computed with gimpelsimplementation of the pairwise bootstrap resam pling method (koehn, 2004)).<papid> W04-3250 </papid>manual examination of the translations produced by both systems confirms the result ofthe automatic evaluation.</citsent>
<aftsection>
<nextsent>many translations produced by the contrastive system now have verbs in the correct positions.
</nextsent>
<nextsent>if we compare the generated translations for input sentence 1 in table 5, wesee that the contrastive system generates trans tense main extr sub int xcompsimple 675,095 170,806 449,631 8,739 composed 343,178 116,729 277,733 8,817 314,573 rest 98,464 5,158 90,139 306 146,746 table 3: counts of english clause types and usedtenses.
</nextsent>
<nextsent>bold numbers indicate clause type/tense combinations where reordering is required.
</nextsent>
<nextsent>baseline reordered bleu 13.02 13.63 table 4: scores of baseline and contrastive systems lation in which all verbs are placed correctly.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I733">
<title id=" E89-1029.xml">extended graph unification </title>
<section> introduction.  </section>
<citcontext>
<prevsection>
<prevsent>much recent work on specifying grammars for fragments of natural languages, and on producing computational systems which make use of these grammars, has used partial descriptions of com-plex feature structures {gazdar 1988}.
</prevsent>
<prevsent>gram-mars are specified in terms of partial descriptions of syntactic structures; programs that depend on these grammars perform some variant of unifica-tion in order to investigate the relationship be-tween specific strings of words and the syntac-tic structures permitted by the grammar mis some sentence grammatical, what actually is its syn-tactic structure, how can some partially specified structure be realised as string of words, and so on.
</prevsent>
</prevsection>
<citsent citstr=" P84-1075 ">
nearly all existing unification grammars of this kind use either term unification (the kind of unification used in resolution theorem provers, and hence provided as primitive in prolog) or some version of the graph unification proposed by kay {1985) and shieber (1984).<papid> P84-1075 </papid></citsent>
<aftsection>
<nextsent>we propose an ex-tension to the languages used by kay and shieber for describing raphs, and to the specification of the conditions under which graphs unify.
</nextsent>
<nextsent>this ex-tension enables us to write concise descriptions of syntactic phenomena which would be awkward to specify using the originm notations.
</nextsent>
<nextsent>we do not 2.1 gsl: syntax.
</nextsent>
<nextsent>the syntax of gsl has been kept as close as possi-ble to that of fug (kay 1985) in order to facilitate comparisons.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I735">
<title id=" E89-1029.xml">extended graph unification </title>
<section> introduction.  </section>
<citcontext>
<prevsection>
<prevsent>t ions using gsl we will illustrate the use of gsl with elements of categorial grammar for fragment of en-glish.
</prevsent>
<prevsent>gsl is not specifically designed for catego- rim grammar, but the complexity of the category structures of any non-trivial categorial grammar means that such grammars provide good testbed for notations for describing categories.
</prevsent>
</prevsection>
<citsent citstr=" P87-1012 ">
although categorial grammars have recently received con-siderable attention (pareschi &amp; steedman (1987), <papid> P87-1012 </papid>klein &amp; van benthem (1987), oehrle, bach &amp; wheeler (1987)), computational treatments have been hindered by the need to develop and ma-nipulate large category descriptions.</citsent>
<aftsection>
<nextsent>the expres-sive power of gsl is therefore well illustrated by the ease with which we can develop the category descriptions required for non-trivial categorial grammar.
</nextsent>
<nextsent>we start with the basic categorial rules: {major/x, minor/y, subcat/sub, slash/slash) (head=(major/x, minor/y, subcat/sub, slash/slash), rslash=(major/x1, minor/y1, subcat/sub1, slash/slash), slash=null!), {major/x1, minor/y1, subcat/sub1, slash/slash} (major/x, minor/y, subcat/sub, slash/slash) (major/x1, minor/y1, subcat/svb1, slash=nulli) (head=(major/x, minor/y, subcat/sub, slash/slash), lslash--(major/x1, minor/y1, subcat/sub1, slash/slash), slash/slash) the first of these is an extended version of the normal categorial rule for combining something which requires an argument to its right with an argument of the appropriate type, namely: ~ a/b we have been forced to complicate this rule, as have others trying to produce categorial gram-mars for non-trivial fragments, in order to take into account intrinsic syntactic functions uch as case and number agreement, and to deal with the fine details of sub-categorisation rules.
</nextsent>
<nextsent>in our ex-tended version of the basic rule, the of the basic version is replaced by (major/x, minor/y, sub- cat/sub, slash/slash) and the of the basic version by (major/x1, minor/y1, subcat/sub1, slash/slash).
</nextsent>
<nextsent>the major features of category are simply its main category (noun, verb, preposi-tion, conj) and its bar level (zero, one, two).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I736">
<title id=" E89-1029.xml">extended graph unification </title>
<section> introduction.  </section>
<citcontext>
<prevsection>
<prevsent>~_~ - 215 - 4 computational com-.
</prevsent>
<prevsent>plex ity we end by briefly considering the complexity of the task of seeing whether two graphs with named non-terminal nodes have common extension.
</prevsent>
</prevsection>
<citsent citstr=" P87-1033 ">
it is well-known that disjunctive unification is np- complete (kasper 1987).<papid> P87-1033 </papid></citsent>
<aftsection>
<nextsent>what is the status of unification of structures with constraints on sub- graphs?
</nextsent>
<nextsent>the definition of unification given in section 2 looks very non-deterministic--full of phrases like ~suppose is the value of initial nodes in each of g1 and g2 ~ and ~suppose appears as the value of one or more initial nodes in g1 but of none in g2 .
</nextsent>
<nextsent>we can make it much more constrained by imposing normal form on graphs.
</nextsent>
<nextsent>the first thing we need for this is an arbitrary ordering on features, which we can easily find since features are just alphanumeric strings, and these can be ordered lexicographically.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I737">
<title id=" E06-2003.xml">lingua stream an integrated environment for computational linguistics experimentation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>at the endof the stream, various tools allow analysed documents and their annotations to be conveniently visualised.
</prevsent>
<prevsent>the uses of the platform range from corpora exploration to the development of fully operational automatic analysers.
</prevsent>
</prevsection>
<citsent citstr=" P02-1022 ">
other platform or tools pursue similar goals.we share some principles with gate (cunning ham et al, 2002), <papid> P02-1022 </papid>hog (callmeier et al, 2004)and nooj1 (muller et al, 2004), but one important difference is that the lingua stream platform promotes the combination of purely declarative formalisms (when gate is mostly based on the jape language and nooj focuses on unique formalism), and allows processing streams to be designed graphically as complex graphs (when gate relies on the pipeline paradigm).</citsent>
<aftsection>
<nextsent>also, the 1formerly known as intex.
</nextsent>
<nextsent>95 figure 1: lingua stream integrated environmentlow-level architecture of lingua stream is comparable to the hog middle ware, but we are more interested in higher-level aspects such as analysis models and methodological concerns.
</nextsent>
<nextsent>finally, when other platforms usually enforce the use of dedicated document format, lingua stream is able to process any xml document.
</nextsent>
<nextsent>on the other hand, lingua stream is more targeted to experimentation tasks on low amounts of data, when tools such as gate or nooj allow to process larger ones.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I738">
<title id=" E12-1065.xml">character based kernels for novelistic plot structure </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>given corpus of 19th century novels as training data, our method can accurately distinguish held-out novel sin their original form from artificially disordered or reversed surrogates, demonstrating its ability to robustly represent important aspects of plot structure.
</prevsent>
<prevsent>every culture has stories, and storytelling is one of the key functions of human language.
</prevsent>
</prevsection>
<citsent citstr=" N09-1042 ">
yet while we have robust, flexible models for the structure of informative documents (for instance (chen et al ., 2009; <papid> N09-1042 </papid>abu jbara and radev, 2011)), current approaches have difficulty representing the narrative structure of fictional stories.</citsent>
<aftsection>
<nextsent>this causes problems for any task requiring us to model fiction, including summarization and generation of stories; kazantseva and szpakowicz (2010) <papid> J10-1003 </papid>show that state-of-the-art summarizers perform extremely poorly on short fictional texts1.</nextsent>
<nextsent>a major problem with applying models for informative1apart from kazantseva, we know of one other attempt to apply modern summarizer to fiction, by the artist jason huff, using microsoft word 2008s extractive summary feature: http://jason-huff.com/text to fiction is that the most important structure underlying the narrative its plot occurs ata high level of abstraction, while the actual narration is of series of lower-level events.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I739">
<title id=" E12-1065.xml">character based kernels for novelistic plot structure </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>every culture has stories, and storytelling is one of the key functions of human language.
</prevsent>
<prevsent>yet while we have robust, flexible models for the structure of informative documents (for instance (chen et al ., 2009; <papid> N09-1042 </papid>abu jbara and radev, 2011)), current approaches have difficulty representing the narrative structure of fictional stories.</prevsent>
</prevsection>
<citsent citstr=" J10-1003 ">
this causes problems for any task requiring us to model fiction, including summarization and generation of stories; kazantseva and szpakowicz (2010) <papid> J10-1003 </papid>show that state-of-the-art summarizers perform extremely poorly on short fictional texts1.</citsent>
<aftsection>
<nextsent>a major problem with applying models for informative1apart from kazantseva, we know of one other attempt to apply modern summarizer to fiction, by the artist jason huff, using microsoft word 2008s extractive summary feature: http://jason-huff.com/text to fiction is that the most important structure underlying the narrative its plot occurs ata high level of abstraction, while the actual narration is of series of lower-level events.
</nextsent>
<nextsent>a short synopsis of jane austens novel pride and prejudice, for example, is that elizabeth bennet first thinks mr. darcy is arrogant, but later grows to love him.
</nextsent>
<nextsent>but this is not stated straightforwardly in the text; the reader must infer it from the behavior of the characters as they participate in various everyday scenes.
</nextsent>
<nextsent>in this paper, we present the plot kernel, acoarse-grained, but robust representation of novelistic plot structure.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I741">
<title id=" E12-1065.xml">character based kernels for novelistic plot structure </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>we show detailed analysis of the character correspondences discovered by our system, and discuss their potential relevance to summarization, in section 9.
</prevsent>
<prevsent>some recent work on story understanding has focused on directly modeling the series of events that occur in the narrative.
</prevsent>
</prevsection>
<citsent citstr=" P10-1158 ">
mcintyre and lapata (2010) <papid> P10-1158 </papid>create story generation system that draws on earlier work on narrative schemas (chambers and jurafsky, 2009).<papid> P09-1068 </papid></citsent>
<aftsection>
<nextsent>their system ensures that generated stories contain plausible event-to-event transitions and are coherent.
</nextsent>
<nextsent>since it focuses onlyon events, however, it cannot enforce global notion of what the characters want or how they relate to one another.
</nextsent>
<nextsent>our own work draws on representations that explicitly model emotions rather than events.
</nextsent>
<nextsent>almand sproat (2005) were the first to describe stories in terms of an emotional trajectory.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I742">
<title id=" E12-1065.xml">character based kernels for novelistic plot structure </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>we show detailed analysis of the character correspondences discovered by our system, and discuss their potential relevance to summarization, in section 9.
</prevsent>
<prevsent>some recent work on story understanding has focused on directly modeling the series of events that occur in the narrative.
</prevsent>
</prevsection>
<citsent citstr=" P09-1068 ">
mcintyre and lapata (2010) <papid> P10-1158 </papid>create story generation system that draws on earlier work on narrative schemas (chambers and jurafsky, 2009).<papid> P09-1068 </papid></citsent>
<aftsection>
<nextsent>their system ensures that generated stories contain plausible event-to-event transitions and are coherent.
</nextsent>
<nextsent>since it focuses onlyon events, however, it cannot enforce global notion of what the characters want or how they relate to one another.
</nextsent>
<nextsent>our own work draws on representations that explicitly model emotions rather than events.
</nextsent>
<nextsent>almand sproat (2005) were the first to describe stories in terms of an emotional trajectory.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I743">
<title id=" E12-1065.xml">character based kernels for novelistic plot structure </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>they annotate emotional states in 22 grimms?
</prevsent>
<prevsent>fairy tale sand discover an increase in emotion (mostly posi tive) toward the ends of stories.
</prevsent>
</prevsection>
<citsent citstr=" H05-1073 ">
they later use this corpus to construct reasonably accurate classifier for emotional states of sentences (alm etal., 2005).<papid> H05-1073 </papid></citsent>
<aftsection>
<nextsent>volkova et al (2010) <papid> W10-0212 </papid>extend the human annotation approach using larger number of emotion categories and applying them to freely defined chunks instead of sentences.</nextsent>
<nextsent>the largest scale emotional analysis is performed by mohammad (2011), <papid> W11-1514 </papid>using crowd-sourcing to construct large emotional lexicon with which he analyzes adult texts such as plays and novels.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I744">
<title id=" E12-1065.xml">character based kernels for novelistic plot structure </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>fairy tale sand discover an increase in emotion (mostly posi tive) toward the ends of stories.
</prevsent>
<prevsent>they later use this corpus to construct reasonably accurate classifier for emotional states of sentences (alm etal., 2005).<papid> H05-1073 </papid></prevsent>
</prevsection>
<citsent citstr=" W10-0212 ">
volkova et al (2010) <papid> W10-0212 </papid>extend the human annotation approach using larger number of emotion categories and applying them to freely defined chunks instead of sentences.</citsent>
<aftsection>
<nextsent>the largest scale emotional analysis is performed by mohammad (2011), <papid> W11-1514 </papid>using crowd-sourcing to construct large emotional lexicon with which he analyzes adult texts such as plays and novels.</nextsent>
<nextsent>in this work, we adopt the concept of emotional trajectory, but apply it to particular characters rather than works as whole.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I745">
<title id=" E12-1065.xml">character based kernels for novelistic plot structure </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>they later use this corpus to construct reasonably accurate classifier for emotional states of sentences (alm etal., 2005).<papid> H05-1073 </papid></prevsent>
<prevsent>volkova et al (2010) <papid> W10-0212 </papid>extend the human annotation approach using larger number of emotion categories and applying them to freely defined chunks instead of sentences.</prevsent>
</prevsection>
<citsent citstr=" W11-1514 ">
the largest scale emotional analysis is performed by mohammad (2011), <papid> W11-1514 </papid>using crowd-sourcing to construct large emotional lexicon with which he analyzes adult texts such as plays and novels.</citsent>
<aftsection>
<nextsent>in this work, we adopt the concept of emotional trajectory, but apply it to particular characters rather than works as whole.
</nextsent>
<nextsent>in focusing on characters, we follow elson et al .
</nextsent>
<nextsent>(2010), who analyze narratives by examining their social network relationships.
</nextsent>
<nextsent>they use an automatic method based on quoted speech to find social links between characters in 19th century novels.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I746">
<title id=" E12-1065.xml">character based kernels for novelistic plot structure </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>however, they operate at very detailed level and so can be applied only to short texts.
</prevsent>
<prevsent>scheherazade (elson and mckeown, 2010) allows human annotators to mark character goals and emotional states in narrative, and indicate the causal links between them.
</prevsent>
</prevsection>
<citsent citstr=" D10-1008 ">
aesop (goyal etal., 2010) <papid> D10-1008 </papid>attempts to learn similar structure automatically.</citsent>
<aftsection>
<nextsent>aesops accuracy, however, is relatively poor even on short fables, indicating that this fine-grained approach is unlikely to be scalable to novel-length texts; our system relies on much coarser analysis.
</nextsent>
<nextsent>kazantseva and szpakowicz (2010) <papid> J10-1003 </papid>summarize short stories, although unlike the other projects we discuss here, they explicitly try to avoid giving away plot details their goal is to create spoiler free?</nextsent>
<nextsent>summaries focusing on characters, settings and themes, in order to attract potential readers.they do find it useful to detect character mentions, and also use features based on verb aspect to automatically exclude plot events while retaining descriptive passages.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I749">
<title id=" E12-1065.xml">character based kernels for novelistic plot structure </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>kazantseva and szpakowicz (2010) <papid> J10-1003 </papid>summarize short stories, although unlike the other projects we discuss here, they explicitly try to avoid giving away plot details their goal is to create spoiler free?</prevsent>
<prevsent>summaries focusing on characters, settings and themes, in order to attract potential readers.they do find it useful to detect character mentions, and also use features based on verb aspect to automatically exclude plot events while retaining descriptive passages.</prevsent>
</prevsection>
<citsent citstr=" P04-1050 ">
they compare their genre specific system with few state-of-the-art methods for summarizing news, and find it outperforms them substantially.we evaluate our system by comparing real novels to artificially produced surrogates, procedure previously used to evaluate models of discourse coherence (karamanis et al  2004; <papid> P04-1050 </papid>barzilay and lapata, 2005) <papid> P05-1018 </papid>and models of syntax (post, 2011).<papid> P11-2038 </papid>as in these settings, we anticipate that performance on this kind of task will be correlated with performance in applied settings, so we use it as an easier preliminary test of our capabilities.</citsent>
<aftsection>
<nextsent>we focus on the 19th century novel, partly following elson et al (2010) <papid> P10-1015 </papid>and partly because these texts are freely available via project guten berg.</nextsent>
<nextsent>our main dataset is composed of romances (which we loosely define as novels focusing on acourtship or love affair).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I750">
<title id=" E12-1065.xml">character based kernels for novelistic plot structure </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>kazantseva and szpakowicz (2010) <papid> J10-1003 </papid>summarize short stories, although unlike the other projects we discuss here, they explicitly try to avoid giving away plot details their goal is to create spoiler free?</prevsent>
<prevsent>summaries focusing on characters, settings and themes, in order to attract potential readers.they do find it useful to detect character mentions, and also use features based on verb aspect to automatically exclude plot events while retaining descriptive passages.</prevsent>
</prevsection>
<citsent citstr=" P05-1018 ">
they compare their genre specific system with few state-of-the-art methods for summarizing news, and find it outperforms them substantially.we evaluate our system by comparing real novels to artificially produced surrogates, procedure previously used to evaluate models of discourse coherence (karamanis et al  2004; <papid> P04-1050 </papid>barzilay and lapata, 2005) <papid> P05-1018 </papid>and models of syntax (post, 2011).<papid> P11-2038 </papid>as in these settings, we anticipate that performance on this kind of task will be correlated with performance in applied settings, so we use it as an easier preliminary test of our capabilities.</citsent>
<aftsection>
<nextsent>we focus on the 19th century novel, partly following elson et al (2010) <papid> P10-1015 </papid>and partly because these texts are freely available via project guten berg.</nextsent>
<nextsent>our main dataset is composed of romances (which we loosely define as novels focusing on acourtship or love affair).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I751">
<title id=" E12-1065.xml">character based kernels for novelistic plot structure </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>kazantseva and szpakowicz (2010) <papid> J10-1003 </papid>summarize short stories, although unlike the other projects we discuss here, they explicitly try to avoid giving away plot details their goal is to create spoiler free?</prevsent>
<prevsent>summaries focusing on characters, settings and themes, in order to attract potential readers.they do find it useful to detect character mentions, and also use features based on verb aspect to automatically exclude plot events while retaining descriptive passages.</prevsent>
</prevsection>
<citsent citstr=" P11-2038 ">
they compare their genre specific system with few state-of-the-art methods for summarizing news, and find it outperforms them substantially.we evaluate our system by comparing real novels to artificially produced surrogates, procedure previously used to evaluate models of discourse coherence (karamanis et al  2004; <papid> P04-1050 </papid>barzilay and lapata, 2005) <papid> P05-1018 </papid>and models of syntax (post, 2011).<papid> P11-2038 </papid>as in these settings, we anticipate that performance on this kind of task will be correlated with performance in applied settings, so we use it as an easier preliminary test of our capabilities.</citsent>
<aftsection>
<nextsent>we focus on the 19th century novel, partly following elson et al (2010) <papid> P10-1015 </papid>and partly because these texts are freely available via project guten berg.</nextsent>
<nextsent>our main dataset is composed of romances (which we loosely define as novels focusing on acourtship or love affair).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I752">
<title id=" E12-1065.xml">character based kernels for novelistic plot structure </title>
<section> dataset.  </section>
<citcontext>
<prevsection>
<prevsent>summaries focusing on characters, settings and themes, in order to attract potential readers.they do find it useful to detect character mentions, and also use features based on verb aspect to automatically exclude plot events while retaining descriptive passages.
</prevsent>
<prevsent>they compare their genre specific system with few state-of-the-art methods for summarizing news, and find it outperforms them substantially.we evaluate our system by comparing real novels to artificially produced surrogates, procedure previously used to evaluate models of discourse coherence (karamanis et al  2004; <papid> P04-1050 </papid>barzilay and lapata, 2005) <papid> P05-1018 </papid>and models of syntax (post, 2011).<papid> P11-2038 </papid>as in these settings, we anticipate that performance on this kind of task will be correlated with performance in applied settings, so we use it as an easier preliminary test of our capabilities.</prevsent>
</prevsection>
<citsent citstr=" P10-1015 ">
we focus on the 19th century novel, partly following elson et al (2010) <papid> P10-1015 </papid>and partly because these texts are freely available via project guten berg.</citsent>
<aftsection>
<nextsent>our main dataset is composed of romances (which we loosely define as novels focusing on acourtship or love affair).
</nextsent>
<nextsent>we select 41 texts, taking 11 as development set and the remaining30 as test set; complete list is given in appendix a. we focus on the novels used in elsonet al (2010), <papid> P10-1015 </papid>but in some cases add additional romances by an already-included author.</nextsent>
<nextsent>we also selected 10 of the least romantic works as an out of-domain set; experiments on these are in section 8.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I755">
<title id=" E12-1065.xml">character based kernels for novelistic plot structure </title>
<section> preprocessing.  </section>
<citcontext>
<prevsection>
<prevsent>in order to compare two texts, we must first extract the characters in each and some features of their relationships with one another.
</prevsent>
<prevsent>our first step is to split the text into chapters, and each chapter into paragraphs; if the text contains running dialogue where each line begins with quotation mark, we append it to the previous paragraph.
</prevsent>
</prevsection>
<citsent citstr=" A97-1004 ">
we segment each paragraph with mxterminator (reynar and ratnaparkhi, 1997) <papid> A97-1004 </papid>and parse it with the self-trained charniak parser (mcclosky et al 2006).<papid> N06-1020 </papid></citsent>
<aftsection>
<nextsent>next, we extract list of characters, compute dependency tree-based unigram features for each character, and record character frequencies and relationships over time.
</nextsent>
<nextsent>4.1 identifying characters.
</nextsent>
<nextsent>we create list of possible character references for each work by extracting all strings of proper nouns (as detected by the parser), then discarding those which occur less than 5 times.
</nextsent>
<nextsent>grouping these into useful character list is problem of cross-document coreference.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I756">
<title id=" E12-1065.xml">character based kernels for novelistic plot structure </title>
<section> preprocessing.  </section>
<citcontext>
<prevsection>
<prevsent>in order to compare two texts, we must first extract the characters in each and some features of their relationships with one another.
</prevsent>
<prevsent>our first step is to split the text into chapters, and each chapter into paragraphs; if the text contains running dialogue where each line begins with quotation mark, we append it to the previous paragraph.
</prevsent>
</prevsection>
<citsent citstr=" N06-1020 ">
we segment each paragraph with mxterminator (reynar and ratnaparkhi, 1997) <papid> A97-1004 </papid>and parse it with the self-trained charniak parser (mcclosky et al 2006).<papid> N06-1020 </papid></citsent>
<aftsection>
<nextsent>next, we extract list of characters, compute dependency tree-based unigram features for each character, and record character frequencies and relationships over time.
</nextsent>
<nextsent>4.1 identifying characters.
</nextsent>
<nextsent>we create list of possible character references for each work by extracting all strings of proper nouns (as detected by the parser), then discarding those which occur less than 5 times.
</nextsent>
<nextsent>grouping these into useful character list is problem of cross-document coreference.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I757">
<title id=" E12-1065.xml">character based kernels for novelistic plot structure </title>
<section> preprocessing.  </section>
<citcontext>
<prevsection>
<prevsent>we create list of possible character references for each work by extracting all strings of proper nouns (as detected by the parser), then discarding those which occur less than 5 times.
</prevsent>
<prevsent>grouping these into useful character list is problem of cross-document coreference.
</prevsent>
</prevsection>
<citsent citstr=" C10-1032 ">
although cross-document coreference has been extensively studied (bhattacharya and getoor, 2005) and modern systems can achieve quite high accuracy on the tac-kbp task, where the list of available entities is given in advance (dredze et al  2010), <papid> C10-1032 </papid>novelistic text poses significant challenge for the methods normally used.</citsent>
<aftsection>
<nextsent>the typical 19th-century novel contains many related characters, often named after one another.
</nextsent>
<nextsent>there are complicated social conventions determining which titles are used for whomfor instance, the eldest unmarried daughter of family can be called miss bennet?, while her younger sister must be miss elizabeth bennet?.
</nextsent>
<nextsent>and characters often use nicknames, such as lizzie?.
</nextsent>
<nextsent>our system uses the multi-stage clustering approach outlined in bhattacharya and getoor (2005), but with some features specific to 19th century european names.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I758">
<title id=" E12-1065.xml">character based kernels for novelistic plot structure </title>
<section> preprocessing.  </section>
<citcontext>
<prevsection>
<prevsent>to begin, we merge all identical mentions which contain more than two words (leaving bare first or last names unmerged).next, we heuristic ally assign each mention gender (masculine, feminine or neuter) using list of gendered titles, then list of male and female first names2.
</prevsent>
<prevsent>we then merge mentions where each is longer than one word, the genders do not clash, 2the most frequent names from the 1990 us census.
</prevsent>
</prevsection>
<citsent citstr=" N01-1007 ">
reply left-of-[name] 17 right-of-[name] feel 14 right-of-[name] look 10 right-of-[name] mind 7 right-of-[name] make 7table 1: top five stemmed unigram dependency features for miss elizabeth bennet?, protagonist of pride and prejudice, and their frequencies.and the first and last names are consistent (char niak, 2001).<papid> N01-1007 </papid></citsent>
<aftsection>
<nextsent>we then merge single-word mentions with matching multiword mentions if they appear in the same paragraph, or if not, with the multiword mention that occurs in the most paragraphs.
</nextsent>
<nextsent>when this process ends, we have resolved each mention in the novel to some specific character.
</nextsent>
<nextsent>as in previous work, we discard very infrequent characters and their mentions.for the reasons stated, this method is error prone.
</nextsent>
<nextsent>our intuition is that the simpler method described in elson et al (2010), <papid> P10-1015 </papid>which merges each mention to the most recent possible coref erent, must be even more so.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I761">
<title id=" E12-1065.xml">character based kernels for novelistic plot structure </title>
<section> preprocessing.  </section>
<citcontext>
<prevsection>
<prevsent>the first is the characters frequency as proportion of all character mentions in the chapter.
</prevsent>
<prevsent>the second is the frequency with which the character is associated with emotional language their emotional trajectory (alm et al  2005).<papid> H05-1073 </papid></prevsent>
</prevsection>
<citsent citstr=" H05-1044 ">
we use the strong subjectivity cues from the lexicon of wilson et al (2005) <papid> H05-1044 </papid>as measurement of emotion.</citsent>
<aftsection>
<nextsent>if, in particular paragraph, only one character is mentioned, we count all emotional words in that paragraph and add them to the characters total.
</nextsent>
<nextsent>to render the numbers comparable across works, each paragraph subtotal is normalized by the amount of emotional language in the novel as whole.
</nextsent>
<nextsent>then the chapter score is the average over paragraphs.
</nextsent>
<nextsent>for pairwise character relationships, we count the number of paragraphs in which only two characters are mentioned, and treat this number (as proportion of the total) as measurement of the strength of the relationship between that pair3.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I764">
<title id=" E06-3005.xml">developing an approach for why question answering </title>
<section> question analysis for why-qa.  </section>
<citcontext>
<prevsection>
<prevsent>lexical items in the question give information on the topic of the users information need.
</prevsent>
<prevsent>in keyword selection, several different approaches may be followed.
</prevsent>
</prevsection>
<citsent citstr=" P00-1071 ">
moldovan et al (2000), <papid> P00-1071 </papid>for instance, select as keywords all named entities that were recognized as proper nouns.</citsent>
<aftsection>
<nextsent>in almost all approaches to keyword extraction, syntax plays role.
</nextsent>
<nextsent>shallow parsing is used for extracting noun phrases, which are considered to be relevant key phrases in the retrieval step.
</nextsent>
<nextsent>based on the querys keywords, 41 one or more documents or paragraphs can be retrieved that may possibly contain the answer.
</nextsent>
<nextsent>a second, very important, component in question analysis is determination of the questions semantic answer type.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I766">
<title id=" E06-3005.xml">developing an approach for why question answering </title>
<section> question analysis for why-qa.  </section>
<citcontext>
<prevsection>
<prevsent>the answer type of question defines the type of answer that the system should look for.
</prevsent>
<prevsent>often-cited work on question analysis has been done by moldovan et al.
</prevsent>
</prevsection>
<citsent citstr=" H01-1069 ">
(1999), (2000), hovy et al (2001), <papid> H01-1069 </papid>and ferret et al.</citsent>
<aftsection>
<nextsent>(2002).
</nextsent>
<nextsent>they all describe question analysis methods that classify questions with respect totheir answer type.
</nextsent>
<nextsent>in their systems for factoid qa, the answer type is generally deduced directly from the question word (who, when, where, etc.): who leads to the answer type person; where leads to the answer type place, etc. this information helps the system in the search for candidate answers to the question.
</nextsent>
<nextsent>hovy et al find that, of the question analysis components used by their system, the determination of the semantic answer type makes by far the largest contribution to the performance of the entire qa system.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I769">
<title id=" E12-2006.xml">onts optima news translation system </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>1http://translate.google.com/ and http: //www.microsofttranslator.com/ for these reasons, we have developed our in-house machine translation system onts.
</prevsent>
<prevsent>its translation results will be publicly accessible aspart of the europe media monitor family of applications, (steinberger et al 2009), which gather and process about 100,000 news articles per day in about fifty languages.
</prevsent>
</prevsection>
<citsent citstr=" P07-2045 ">
onts is based on the open source phrase-based statistical machine translation toolkit moses (koehn et al 2007),<papid> P07-2045 </papid>trained mostly on freely available parallel corpora and optimised for the news domain, as statedabove.</citsent>
<aftsection>
<nextsent>the main objective of developing our in house system is thus not to improve translation quality over the existing services (this would be beyond our possibilities), but to offer our users rough translation (a gist?)
</nextsent>
<nextsent>that allows them to get an idea of the main contents of the article and to determine whether the news item at hand is relevant for their field of interest or not.
</nextsent>
<nextsent>a similar news-focused translation service is found in translation?
</nextsent>
<nextsent>(turchi et al 2009),which gathers articles in 23 languages and translates them into english.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I771">
<title id=" E12-2006.xml">onts optima news translation system </title>
<section> news translation system.  </section>
<citcontext>
<prevsection>
<prevsent>this means that translated article is evaluated positively even if it is not perfect in the target language.
</prevsent>
<prevsent>dealing with such large number of source languages and articles per day, our system should take into account the translation speed, and try to avoid using language-dependent tools such as part-of-speech taggers.
</prevsent>
</prevsection>
<citsent citstr=" N03-1017 ">
inside the moses toolkit, three different statistical approaches have been implemented:phrase based statistical machine translation (pb smt) (koehn et al 2003), <papid> N03-1017 </papid>hierarchical phrase based statistical machine translation (chiang,2007) and syntax-based statistical machine translation (marcu et al 2006).<papid> W06-1606 </papid></citsent>
<aftsection>
<nextsent>to identify the most suitable system for our requirements, werun set of experiments training the three models with europarl v4 german-english (koehn, 2005) and optimizing and testing on the news corpus (callison-burch et al 2009).
</nextsent>
<nextsent>for all of them, we use their default configurations and theyare run under the same condition on the same machine to better evaluate translation time.
</nextsent>
<nextsent>for the syntax model we use linguistic information only on the target side.
</nextsent>
<nextsent>according to our experiments, in terms of performance the hierarchical model 26 performs better than pbsmt and syntax (18.31,18.09, 17.62 bleu points), but in terms of translation speed pbsmt is better than hierarchical and syntax (1.02, 4.5, 49 second per sentence).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I772">
<title id=" E12-2006.xml">onts optima news translation system </title>
<section> news translation system.  </section>
<citcontext>
<prevsection>
<prevsent>this means that translated article is evaluated positively even if it is not perfect in the target language.
</prevsent>
<prevsent>dealing with such large number of source languages and articles per day, our system should take into account the translation speed, and try to avoid using language-dependent tools such as part-of-speech taggers.
</prevsent>
</prevsection>
<citsent citstr=" W06-1606 ">
inside the moses toolkit, three different statistical approaches have been implemented:phrase based statistical machine translation (pb smt) (koehn et al 2003), <papid> N03-1017 </papid>hierarchical phrase based statistical machine translation (chiang,2007) and syntax-based statistical machine translation (marcu et al 2006).<papid> W06-1606 </papid></citsent>
<aftsection>
<nextsent>to identify the most suitable system for our requirements, werun set of experiments training the three models with europarl v4 german-english (koehn, 2005) and optimizing and testing on the news corpus (callison-burch et al 2009).
</nextsent>
<nextsent>for all of them, we use their default configurations and theyare run under the same condition on the same machine to better evaluate translation time.
</nextsent>
<nextsent>for the syntax model we use linguistic information only on the target side.
</nextsent>
<nextsent>according to our experiments, in terms of performance the hierarchical model 26 performs better than pbsmt and syntax (18.31,18.09, 17.62 bleu points), but in terms of translation speed pbsmt is better than hierarchical and syntax (1.02, 4.5, 49 second per sentence).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I773">
<title id=" E12-2006.xml">onts optima news translation system </title>
<section> technical implementation.  </section>
<citcontext>
<prevsection>
<prevsent>the connection module is servlet implemented in java.
</prevsent>
<prevsent>it receives the rss files, isolates each single news article, identifies each source language and pre-processes it.
</prevsent>
</prevsection>
<citsent citstr=" E03-1076 ">
each news item is split into sentences, each sentence is tokenized, lower cased, passed through statistical compound word splitter, (koehn and knight, 2003), <papid> E03-1076 </papid>and the named entity annotator module.for language modelling we use the kenlm implementation, (heafield, 2011).<papid> W11-2123 </papid></citsent>
<aftsection>
<nextsent>according to the language, the correct moses servers, title and content, are fed in multi thread manner.
</nextsent>
<nextsent>we use the multi-thread version of moses (haddow, 2010).
</nextsent>
<nextsent>when all the sentences of each article are translated, the inverse proces sis run: they are detokenized, recased, and untranslated/unknown words are listed.
</nextsent>
<nextsent>the translated title and content of each article are up loaded into the rss file and it is passed to the next modules.the full system including the translation modules is running in 2xquad-core with intel hyper-threading technology processors with 48gb of memory.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I774">
<title id=" E12-2006.xml">onts optima news translation system </title>
<section> technical implementation.  </section>
<citcontext>
<prevsection>
<prevsent>the connection module is servlet implemented in java.
</prevsent>
<prevsent>it receives the rss files, isolates each single news article, identifies each source language and pre-processes it.
</prevsent>
</prevsection>
<citsent citstr=" W11-2123 ">
each news item is split into sentences, each sentence is tokenized, lower cased, passed through statistical compound word splitter, (koehn and knight, 2003), <papid> E03-1076 </papid>and the named entity annotator module.for language modelling we use the kenlm implementation, (heafield, 2011).<papid> W11-2123 </papid></citsent>
<aftsection>
<nextsent>according to the language, the correct moses servers, title and content, are fed in multi thread manner.
</nextsent>
<nextsent>we use the multi-thread version of moses (haddow, 2010).
</nextsent>
<nextsent>when all the sentences of each article are translated, the inverse proces sis run: they are detokenized, recased, and untranslated/unknown words are listed.
</nextsent>
<nextsent>the translated title and content of each article are up loaded into the rss file and it is passed to the next modules.the full system including the translation modules is running in 2xquad-core with intel hyper-threading technology processors with 48gb of memory.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I775">
<title id=" E12-1031.xml">clex a lexicon for exploring color concept and emotion associations in language </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>colors are both indicative of and have an effect on our feelings and emotions.
</prevsent>
<prevsent>some colors are associated with positive emotions, e.g., joy, trust and admiration and some with negative emotions, e.g., aggressiveness, fear, boredom and sadness (ortony et al  1988).given the importance of color and visual descriptions in conveying emotion, obtaining deeper understanding of the associations between colors, concepts and emotions may be helpful formany tasks in language understanding and generation.
</prevsent>
</prevsection>
<citsent citstr=" H05-1073 ">
a detailed set of color-concept-emotion associations (e.g., brown - darkness - boredom; red blood - anger) could be quite useful for sentiment analysis, for example, in helping to understand what emotion newspaper article, fairytale, ora tweet is trying to evoke (alm et al  2005; <papid> H05-1073 </papid>mohammad, 2011<papid> W11-1514 </papid>b; kouloumpis et al  2011).</citsent>
<aftsection>
<nextsent>color concept-emotion associations may also be useful for textual entailment, and for machine translation as source of paraphrasing.
</nextsent>
<nextsent>color-concept-emotion associations also havethe potential to enhance human-computer interactions in many real- and virtual-world domains, e.g., online shopping, and avatar construction in gaming environments.
</nextsent>
<nextsent>such knowledge may allow for clearer and hopefully more natural descriptions by users, for example searching for sky-blue shirt rather than blue or light blueshirt.
</nextsent>
<nextsent>our long term goal is to use color-emotion concept associations to enrich dialog systems with information that will help them generate more appropriate responses to users?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I776">
<title id=" E12-1031.xml">clex a lexicon for exploring color concept and emotion associations in language </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>colors are both indicative of and have an effect on our feelings and emotions.
</prevsent>
<prevsent>some colors are associated with positive emotions, e.g., joy, trust and admiration and some with negative emotions, e.g., aggressiveness, fear, boredom and sadness (ortony et al  1988).given the importance of color and visual descriptions in conveying emotion, obtaining deeper understanding of the associations between colors, concepts and emotions may be helpful formany tasks in language understanding and generation.
</prevsent>
</prevsection>
<citsent citstr=" W11-1514 ">
a detailed set of color-concept-emotion associations (e.g., brown - darkness - boredom; red blood - anger) could be quite useful for sentiment analysis, for example, in helping to understand what emotion newspaper article, fairytale, ora tweet is trying to evoke (alm et al  2005; <papid> H05-1073 </papid>mohammad, 2011<papid> W11-1514 </papid>b; kouloumpis et al  2011).</citsent>
<aftsection>
<nextsent>color concept-emotion associations may also be useful for textual entailment, and for machine translation as source of paraphrasing.
</nextsent>
<nextsent>color-concept-emotion associations also havethe potential to enhance human-computer interactions in many real- and virtual-world domains, e.g., online shopping, and avatar construction in gaming environments.
</nextsent>
<nextsent>such knowledge may allow for clearer and hopefully more natural descriptions by users, for example searching for sky-blue shirt rather than blue or light blueshirt.
</nextsent>
<nextsent>our long term goal is to use color-emotion concept associations to enrich dialog systems with information that will help them generate more appropriate responses to users?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I791">
<title id=" E12-1031.xml">clex a lexicon for exploring color concept and emotion associations in language </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>different emotional states.this work introduces new lexicon of color concept-emotion associations, created through crowdsourcing.
</prevsent>
<prevsent>we call this lexicon clex1.
</prevsent>
</prevsection>
<citsent citstr=" W10-0204 ">
itis comparable in size to only two known lexicons: wordnet-affect (strapparava and valitutti, 2004) and emolex (mohammad and tur ney, 2010).<papid> W10-0204 </papid></citsent>
<aftsection>
<nextsent>in contrast to the development of these lexicons, we do not restrict our annotators to particular set of emotions.
</nextsent>
<nextsent>this allows us to 1available for download at: http://research.microsoft.com/en-us/ downloads/ questions about the data and the access process may be sent to svitlana@jhu.edu 306collect more linguistically rich color-concept annotations associated with mood, cognitive state, behavior and attitude.
</nextsent>
<nextsent>we also do not have any restrictions on color naming, which helps us to discover rich lexicon of color terms and collocations that represent various hues, darkness, saturation and other natural language collocations.
</nextsent>
<nextsent>we also perform comprehensive analysis ofthe data by investigating several questions includ ing: what affect terms are evoked by certain color, e.g., positive vs. negative?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I807">
<title id=" E12-1031.xml">clex a lexicon for exploring color concept and emotion associations in language </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>elliotts affective reasoner is collection of programs that is able to reason about human emotions.
</prevsent>
<prevsent>the system covers set of 26 emotion categories from ortony et al 1988).
</prevsent>
</prevsection>
<citsent citstr=" W10-3405 ">
kaya (2004) and strapparava and ozbal (2010)<papid> W10-3405 </papid>both have worked on inferring emotions associated with colors using semantic similarity.</citsent>
<aftsection>
<nextsent>their 2http://www.macquarieonline.com.au 3http://www.cymbolism.com/ 4http://www.wjh.harvard.edu/inquirer/ 307research found that americans perceive red as excitement, yellow as cheer, purple as dignity and associate blue with comfort and security.
</nextsent>
<nextsent>other research includes that geared toward discovering culture-specific color-concept associations (gage,1993) and color preference, for example, in children vs. adults (ou et al  2011).
</nextsent>
<nextsent>in order to collect color-concept and color emotion associations, we use amazon mechanical turk5.
</nextsent>
<nextsent>it is fast and relatively inexpensive way to get large amount of data from many cultures all over the world.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I808">
<title id=" E12-1031.xml">clex a lexicon for exploring color concept and emotion associations in language </title>
<section> data collection.  </section>
<citcontext>
<prevsection>
<prevsent>3.1 mturk and data quality.
</prevsent>
<prevsent>amazon mechanical turk is crowdsourcing platform that has been extensively used for obtaining low-cost human annotations for various linguistic tasks over the last few years (callison burch, 2009).
</prevsent>
</prevsection>
<citsent citstr=" D08-1027 ">
the quality of the data obtained from non-expert annotators, also referred to as workers or turkers, was investigated by snow et al  (2008).<papid> D08-1027 </papid></citsent>
<aftsection>
<nextsent>their empirical results show that the quality of non-expert annotations is comparable to the quality of expert annotations on variety of natural language tasks, but the cost of the annotation is much lower.
</nextsent>
<nextsent>there are various quality control strategies thatcan be used to ensure annotation quality.
</nextsent>
<nextsent>for instance, one can restrict crowd?
</nextsent>
<nextsent>by creating pilot task that allows only workers who passed the task to proceed with annotations (chen and dolan, 2011).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I854">
<title id=" E12-3008.xml">hierarchical bayesian language modelling for the linguistically informed </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these models are not ideal for languages that have relatively free word order and/or complex morphology.
</prevsent>
<prevsent>the ability to encode additional linguistic intuitions into models that already have certain attractive properties is an important piece of the puzzle of improving machine translation quality for those languages.
</prevsent>
</prevsection>
<citsent citstr=" P06-1124 ">
but despite their widespread use, kn n-gram models are not easily extensible with additional model components that target particular linguistic phe nomena.i argue in this paper that the family of hierarchical pitman-yor language models (hpylm) (teh, 2006; <papid> P06-1124 </papid>goldwater et al, 2006) are suitable for investigations into more linguistically-informed n-gram language models.</citsent>
<aftsection>
<nextsent>firstly, the flexibility to specify arbitrary back-off distributions makes it easy to incorporate multiple models into larger n-gram model.
</nextsent>
<nextsent>secondly, the pitman-yor process prior (pitman and yor, 1997) generates distributions that are well-suited to variety of power law behaviours, as is often observed in language.
</nextsent>
<nextsent>catering for variety of those is important since the frequency distributions of, say, suffixes, couldbe quite different from that of words.
</nextsent>
<nextsent>kn smoothing is less flexibility in this regard.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I855">
<title id=" E12-3008.xml">hierarchical bayesian language modelling for the linguistically informed </title>
<section> background and related work.  </section>
<citcontext>
<prevsection>
<prevsent>part of this smoothing involves discounting the counts of n-grams in the training data;the modified version uses different levels of discounting depending on the frequency of the count.
</prevsent>
<prevsent>these methods were designed with surface word distributions, and are not necessarily suitable for smoothing distributions of other kinds of surface units.
</prevsent>
</prevsection>
<citsent citstr=" N03-2002 ">
bilmes and kirchhoff (2003) <papid> N03-2002 </papid>proposed more general framework for n-gram language mod elling.</citsent>
<aftsection>
<nextsent>their factored language model (flm) views word as vector of features, such that particular feature value is generated conditional on some history of preceding feature values.
</nextsent>
<nextsent>this allowed the inclusion of n-gram models over sequences of elements like pos tags and semanticclasses.
</nextsent>
<nextsent>in tandem, they proposed more complicated back-off paths; for example, trigrams can back-off to two underlying bigram distributions, one dropping the left-most context word and the other the right-most.
</nextsent>
<nextsent>with the right combination of features and back-off structure they got good perplexity reductions, and obtained some improvements in translation quality by applying these ideas to the smoothing of the bilingual phrase table (yang and kirchhoff, 2006).<papid> E06-1006 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I856">
<title id=" E12-3008.xml">hierarchical bayesian language modelling for the linguistically informed </title>
<section> background and related work.  </section>
<citcontext>
<prevsection>
<prevsent>this allowed the inclusion of n-gram models over sequences of elements like pos tags and semanticclasses.
</prevsent>
<prevsent>in tandem, they proposed more complicated back-off paths; for example, trigrams can back-off to two underlying bigram distributions, one dropping the left-most context word and the other the right-most.
</prevsent>
</prevsection>
<citsent citstr=" E06-1006 ">
with the right combination of features and back-off structure they got good perplexity reductions, and obtained some improvements in translation quality by applying these ideas to the smoothing of the bilingual phrase table (yang and kirchhoff, 2006).<papid> E06-1006 </papid></citsent>
<aftsection>
<nextsent>my approach has some similarity to the flm: both decompose surface word forms into elements that are generated from unrelated conditional distributions.
</nextsent>
<nextsent>they differ predominantly along twodimensions: the types of decompositions and conditioning possible, and my use of particular bayesian prior for handling smoothing.in addition to the hpylm for n-gram language modelling (teh, 2006), <papid> P06-1124 </papid>models based onthe pitman-yor process prior have also been applied to good effect in word segmentation (gold water et al, 2006; mochihashi et al, 2009) <papid> P09-1012 </papid>and speech recognition (huang and renals, 2007; neubig et al, 2010).</nextsent>
<nextsent>the graphical pitman-yor process enables branching back-off paths, which briefly revisit in 7, and have proved effective in language model domain-adaptation (wood and teh, 2009).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I858">
<title id=" E12-3008.xml">hierarchical bayesian language modelling for the linguistically informed </title>
<section> background and related work.  </section>
<citcontext>
<prevsection>
<prevsent>with the right combination of features and back-off structure they got good perplexity reductions, and obtained some improvements in translation quality by applying these ideas to the smoothing of the bilingual phrase table (yang and kirchhoff, 2006).<papid> E06-1006 </papid></prevsent>
<prevsent>my approach has some similarity to the flm: both decompose surface word forms into elements that are generated from unrelated conditional dis tributions.</prevsent>
</prevsection>
<citsent citstr=" P09-1012 ">
they differ predominantly along twodimensions: the types of decompositions and conditioning possible, and my use of particular bayesian prior for handling smoothing.in addition to the hpylm for n-gram language modelling (teh, 2006), <papid> P06-1124 </papid>models based onthe pitman-yor process prior have also been applied to good effect in word segmentation (gold water et al, 2006; mochihashi et al, 2009) <papid> P09-1012 </papid>and speech recognition (huang and renals, 2007; neubig et al, 2010).</citsent>
<aftsection>
<nextsent>the graphical pitman-yor process enables branching back-off paths, which briefly revisit in 7, and have proved effective in language model domain-adaptation (wood and teh, 2009).
</nextsent>
<nextsent>here, extend this general line of inquiry by considering how one might incorporate linguistically informed sub-models into the hpylm framework.
</nextsent>
<nextsent>i focus on compound nouns in this work for two reasons: firstly, compounding is in general very productive process, and in some languages (in cluding german, swedish and dutch) they are written as single orthographic units.
</nextsent>
<nextsent>this increases data sparsity and creates significant challenges for nlp systems that use whit espace to identify their elementary modelling units.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I859">
<title id=" E12-3008.xml">hierarchical bayesian language modelling for the linguistically informed </title>
<section> compound nouns.  </section>
<citcontext>
<prevsection>
<prevsent>that establishes this compound noun as singular neuter, which determine how it would need to agree with verbs, articles and adjectives.
</prevsent>
<prevsent>in the next section, propose model in the suggested framework that encodes this intuition.
</prevsent>
</prevsection>
<citsent citstr=" W98-0609 ">
the basic structure of german compounds comprises head component, preceded by one or more modifier components, with optional linkerelements between consecutive components (gold smith and reutter, 1998).<papid> W98-0609 </papid></citsent>
<aftsection>
<nextsent>examples ? the basic form is just the concatenation of two nouns auto + unfall = autounfall (car crash)?
</nextsent>
<nextsent>linker elements are sometimes added between components kuche + tisch = kuchentisch (kitchen table) ? components can undergo stemming during composition schule + hof = schulhof (schoolyard) ? the process is potentially recursive (geburt + tag) + kind = geburtstag + kind = geburtstagskind (birthday kid) the process is not limited to using nouns as components, for example, the numeral in zwei euro-munze (two euro coin) or the verb fahren?
</nextsent>
<nextsent>(to drive) in fahrzeug (vehicle).
</nextsent>
<nextsent>i will treat all these cases the same.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I860">
<title id=" E12-3008.xml">hierarchical bayesian language modelling for the linguistically informed </title>
<section> compound nouns.  </section>
<citcontext>
<prevsection>
<prevsent>3.2 related work on compounds.
</prevsent>
<prevsent>in machine translation and speech recognition, one approach has been to split compounds as preprocessing step and merge them back together during postprocessing, while using otherwise unmodified nlp systems.
</prevsent>
</prevsection>
<citsent citstr=" E03-1076 ">
frequency-based methods have been used for determining how aggressively to split (koehn and knight, 2003), <papid> E03-1076 </papid>since the maximal, linguistically correct segmentation is not necessarily optimal for translation.</citsent>
<aftsection>
<nextsent>this gave rise to slight improvements in machine translation evaluations (koehn et al, 2008), <papid> W08-0318 </papid>with fine tuning explored in (stymne, 2009).<papid> E09-3008 </papid></nextsent>
<nextsent>similar ideas have also been employed for speech recognition (berton et al, 1996) and predictive-text input (baroni and matiasek, 2002), where single-token compounds also pose challenges.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I861">
<title id=" E12-3008.xml">hierarchical bayesian language modelling for the linguistically informed </title>
<section> compound nouns.  </section>
<citcontext>
<prevsection>
<prevsent>in machine translation and speech recognition, one approach has been to split compounds as preprocessing step and merge them back together during postprocessing, while using otherwise unmodified nlp systems.
</prevsent>
<prevsent>frequency-based methods have been used for determining how aggressively to split (koehn and knight, 2003), <papid> E03-1076 </papid>since the maximal, linguistically correct segmentation is not necessarily optimal for translation.</prevsent>
</prevsection>
<citsent citstr=" W08-0318 ">
this gave rise to slight improvements in machine translation evaluations (koehn et al, 2008), <papid> W08-0318 </papid>with fine tuning explored in (stymne, 2009).<papid> E09-3008 </papid></citsent>
<aftsection>
<nextsent>similar ideas have also been employed for speech recognition (berton et al, 1996) and predictive-text input (baroni and matiasek, 2002), where single-token compounds also pose challenges.
</nextsent>
<nextsent>4.1 hpylm.
</nextsent>
<nextsent>formally speaking, an n-gram model is an(n? 1)-th order markov model that approximates the joint probability of sequence of words as (w) ? |w|?
</nextsent>
<nextsent>i=1 (wi|win+1, . . .
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I862">
<title id=" E12-3008.xml">hierarchical bayesian language modelling for the linguistically informed </title>
<section> compound nouns.  </section>
<citcontext>
<prevsection>
<prevsent>in machine translation and speech recognition, one approach has been to split compounds as preprocessing step and merge them back together during postprocessing, while using otherwise unmodified nlp systems.
</prevsent>
<prevsent>frequency-based methods have been used for determining how aggressively to split (koehn and knight, 2003), <papid> E03-1076 </papid>since the maximal, linguistically correct segmentation is not necessarily optimal for translation.</prevsent>
</prevsection>
<citsent citstr=" E09-3008 ">
this gave rise to slight improvements in machine translation evaluations (koehn et al, 2008), <papid> W08-0318 </papid>with fine tuning explored in (stymne, 2009).<papid> E09-3008 </papid></citsent>
<aftsection>
<nextsent>similar ideas have also been employed for speech recognition (berton et al, 1996) and predictive-text input (baroni and matiasek, 2002), where single-token compounds also pose challenges.
</nextsent>
<nextsent>4.1 hpylm.
</nextsent>
<nextsent>formally speaking, an n-gram model is an(n? 1)-th order markov model that approximates the joint probability of sequence of words as (w) ? |w|?
</nextsent>
<nextsent>i=1 (wi|win+1, . . .
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I865">
<title id=" E12-3008.xml">hierarchical bayesian language modelling for the linguistically informed </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>data and tools standard data preprocessing steps included normalising punctuation, tokenis ing and lower casing all words.
</prevsent>
<prevsent>all datasets arefrom the wmt11 shared-task.2.
</prevsent>
</prevsection>
<citsent citstr=" N06-1014 ">
the full english german bitext was filtered to exclude sentences longer than 50, resulting in 1.7 million parallel sentences; word alignments were inferred from this using the berkeley aligner (liang et al, 2006) <papid> N06-1014 </papid>and used as basis from which to extract hiero-style synchronous cfg (chiang, 2007).the weights of the log-linear translation models were tuned towards the bleu metric on development data using cdecs (dyer et al, 2010) <papid> P10-4002 </papid>implementation of mert (och, 2003).<papid> P03-1021 </papid>for this, the set news-test2008 (2051 sen tences) was used, while final case-insensitive bleu scores are measured on the official test set newstest2011 (3003 sentences).</citsent>
<aftsection>
<nextsent>all language models were trained on the target side of the preprocessed bitext containing 38 million tokens, and tested on all the german development data (i.e. news-test2008,9,10).compound segmentation to construct segmentation dictionary, used the 1-best segment ations from supervised maxent compound splitter (dyer, 2009) <papid> N09-1046 </papid>run on all token types in bitext.</nextsent>
<nextsent>in addition, word-internal hyphens were also taken 1mark johnsons implementation, http://www.cog.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I866">
<title id=" E12-3008.xml">hierarchical bayesian language modelling for the linguistically informed </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>data and tools standard data preprocessing steps included normalising punctuation, tokenis ing and lower casing all words.
</prevsent>
<prevsent>all datasets arefrom the wmt11 shared-task.2.
</prevsent>
</prevsection>
<citsent citstr=" P10-4002 ">
the full english german bitext was filtered to exclude sentences longer than 50, resulting in 1.7 million parallel sentences; word alignments were inferred from this using the berkeley aligner (liang et al, 2006) <papid> N06-1014 </papid>and used as basis from which to extract hiero-style synchronous cfg (chiang, 2007).the weights of the log-linear translation models were tuned towards the bleu metric on development data using cdecs (dyer et al, 2010) <papid> P10-4002 </papid>implementation of mert (och, 2003).<papid> P03-1021 </papid>for this, the set news-test2008 (2051 sen tences) was used, while final case-insensitive bleu scores are measured on the official test set newstest2011 (3003 sentences).</citsent>
<aftsection>
<nextsent>all language models were trained on the target side of the preprocessed bitext containing 38 million tokens, and tested on all the german development data (i.e. news-test2008,9,10).compound segmentation to construct segmentation dictionary, used the 1-best segment ations from supervised maxent compound splitter (dyer, 2009) <papid> N09-1046 </papid>run on all token types in bitext.</nextsent>
<nextsent>in addition, word-internal hyphens were also taken 1mark johnsons implementation, http://www.cog.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I867">
<title id=" E12-3008.xml">hierarchical bayesian language modelling for the linguistically informed </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>data and tools standard data preprocessing steps included normalising punctuation, tokenis ing and lower casing all words.
</prevsent>
<prevsent>all datasets arefrom the wmt11 shared-task.2.
</prevsent>
</prevsection>
<citsent citstr=" P03-1021 ">
the full english german bitext was filtered to exclude sentences longer than 50, resulting in 1.7 million parallel sentences; word alignments were inferred from this using the berkeley aligner (liang et al, 2006) <papid> N06-1014 </papid>and used as basis from which to extract hiero-style synchronous cfg (chiang, 2007).the weights of the log-linear translation models were tuned towards the bleu metric on development data using cdecs (dyer et al, 2010) <papid> P10-4002 </papid>implementation of mert (och, 2003).<papid> P03-1021 </papid>for this, the set news-test2008 (2051 sen tences) was used, while final case-insensitive bleu scores are measured on the official test set newstest2011 (3003 sentences).</citsent>
<aftsection>
<nextsent>all language models were trained on the target side of the preprocessed bitext containing 38 million tokens, and tested on all the german development data (i.e. news-test2008,9,10).compound segmentation to construct segmentation dictionary, used the 1-best segment ations from supervised maxent compound splitter (dyer, 2009) <papid> N09-1046 </papid>run on all token types in bitext.</nextsent>
<nextsent>in addition, word-internal hyphens were also taken 1mark johnsons implementation, http://www.cog.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I868">
<title id=" E12-3008.xml">hierarchical bayesian language modelling for the linguistically informed </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>all datasets arefrom the wmt11 shared-task.2.
</prevsent>
<prevsent>the full english german bitext was filtered to exclude sentences longer than 50, resulting in 1.7 million parallel sentences; word alignments were inferred from this using the berkeley aligner (liang et al, 2006) <papid> N06-1014 </papid>and used as basis from which to extract hiero-style synchronous cfg (chiang, 2007).the weights of the log-linear translation models were tuned towards the bleu metric on development data using cdecs (dyer et al, 2010) <papid> P10-4002 </papid>implementation of mert (och, 2003).<papid> P03-1021 </papid>for this, the set news-test2008 (2051 sen tences) was used, while final case-insensitive bleu scores are measured on the official test set newstest2011 (3003 sentences).</prevsent>
</prevsection>
<citsent citstr=" N09-1046 ">
all language models were trained on the target side of the preprocessed bitext containing 38 million tokens, and tested on all the german development data (i.e. news-test2008,9,10).compound segmentation to construct segmentation dictionary, used the 1-best segment ations from supervised maxent compound splitter (dyer, 2009) <papid> N09-1046 </papid>run on all token types in bitext.</citsent>
<aftsection>
<nextsent>in addition, word-internal hyphens were also taken 1mark johnsons implementation, http://www.cog.
</nextsent>
<nextsent>brown.edu/mj/software.htm 2http://www.statmt.org/wmt11/ as segmentation points.
</nextsent>
<nextsent>finally, linker elements were merged onto components as discussed in 4.2.
</nextsent>
<nextsent>any token that is split into more than one part by this procedure is regarded as compound.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I869">
<title id=" E12-1084.xml">not as awful as it seems explaining german case through computational experiments in fluid construction grammar </title>
<section> the problem of german case.  </section>
<citcontext>
<prevsection>
<prevsent>while such an approach may be desirable from mathematical?
</prevsent>
<prevsent>point of view, it puts the burden of efficient processing on the shoulders of computational linguists, who have to develop more intelligent interpreters.
</prevsent>
</prevsection>
<citsent citstr=" P84-1008 ">
one example of the gap between description and computational implementation is disjunctive feature representation, which became popular in feature-based grammar formalisms in the 1980s (karttunen, 1984).<papid> P84-1008 </papid></citsent>
<aftsection>
<nextsent>dis junctions allow an elegant notation for multiple feature values, as illustrated in example 1 for the german definite article die, which is either assigned nominative or accusativecase, and which is either feminine-singular or plural.
</nextsent>
<nextsent>the feature structure (adopted from karttunen, 1984, <papid> P84-1008 </papid>p. 30) represents dis junctions by enclosing the alternatives in curly brackets ({ }).</nextsent>
<nextsent>(1) ? ?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I871">
<title id=" E12-1084.xml">not as awful as it seems explaining german case through computational experiments in fluid construction grammar </title>
<section> the problem of german case.  </section>
<citcontext>
<prevsection>
<prevsent>most of the splits are caused by disjunctions.
</prevsent>
<prevsent>for example, when determiner-noun construction specifies that the case features of the definite article die (nominative or accusative) and the noun kinder (children?; nominative, accusative or gen itive) have to unify, the search tree splits into two hypotheses (a nominative and an accusative read ing) even though for native speakers of german, the syntactic context unambiguously points to nominative reading (because it is the only noun phrase that agrees with the main verb).
</prevsent>
</prevsection>
<citsent citstr=" C90-3013 ">
it should be no surprise, then, that lot of workhas focused on processing dis junctions more efficiently (e.g. carter, 1990; <papid> C90-3013 </papid>ramsay, 1990).<papid> J90-3004 </papid></citsent>
<aftsection>
<nextsent>as observed by flickinger (2000), however, most of these studies implicitly assume that the grammar representation has to remain unchanged.
</nextsent>
<nextsent>he then demonstrates through computational experiments how different representation can directly impact efficiency, and argues that revisions of the grammar for efficiency should be discussed more thoroughly in the literature.
</nextsent>
<nextsent>the impact of representation on processing is illustrated at the bottom of figure 1, which shows the performance of grammar that uses the same processing technique for handling the same utterance, but different representation than the disjunctive grammar.
</nextsent>
<nextsent>as can be seen, the alternative grammar (whose technical details are disclosed further below) is able to parse the german definite articles without tears, and the resulting search tree arguably better reflects the actual processing performed by native speakers of german.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I872">
<title id=" E12-1084.xml">not as awful as it seems explaining german case through computational experiments in fluid construction grammar </title>
<section> the problem of german case.  </section>
<citcontext>
<prevsection>
<prevsent>most of the splits are caused by disjunctions.
</prevsent>
<prevsent>for example, when determiner-noun construction specifies that the case features of the definite article die (nominative or accusative) and the noun kinder (children?; nominative, accusative or gen itive) have to unify, the search tree splits into two hypotheses (a nominative and an accusative read ing) even though for native speakers of german, the syntactic context unambiguously points to nominative reading (because it is the only noun phrase that agrees with the main verb).
</prevsent>
</prevsection>
<citsent citstr=" J90-3004 ">
it should be no surprise, then, that lot of workhas focused on processing dis junctions more efficiently (e.g. carter, 1990; <papid> C90-3013 </papid>ramsay, 1990).<papid> J90-3004 </papid></citsent>
<aftsection>
<nextsent>as observed by flickinger (2000), however, most of these studies implicitly assume that the grammar representation has to remain unchanged.
</nextsent>
<nextsent>he then demonstrates through computational experiments how different representation can directly impact efficiency, and argues that revisions of the grammar for efficiency should be discussed more thoroughly in the literature.
</nextsent>
<nextsent>the impact of representation on processing is illustrated at the bottom of figure 1, which shows the performance of grammar that uses the same processing technique for handling the same utterance, but different representation than the disjunctive grammar.
</nextsent>
<nextsent>as can be seen, the alternative grammar (whose technical details are disclosed further below) is able to parse the german definite articles without tears, and the resulting search tree arguably better reflects the actual processing performed by native speakers of german.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I873">
<title id=" E12-1084.xml">not as awful as it seems explaining german case through computational experiments in fluid construction grammar </title>
<section> the problem of german case.  </section>
<citcontext>
<prevsection>
<prevsent>both grammars have been implemented in fluid construction grammar (fcg; steels, 2011, 2012a) and are processed using standard depth-first search algorithm (bleys et al 2011) and general unification (without optimization for particular types or data structures; steels and de beule, 2006; de beule, 2012).
</prevsent>
<prevsent>the utterance is assumed to be segmented into words.
</prevsent>
</prevsection>
<citsent citstr=" P04-1002 ">
interested readers can explore the figure through an interactive web demonstration at http://www.fcg-net.org/demos/design-patterns/07-feature-matrices/.steels (2004), <papid> P04-1002 </papid>steels (2012b), that grammar evolves in order to optimize communicative success by dampening the search space in linguistic processing and reducing the cognitive effort needed for interpretation, while at the same time minimizing there sources required for doing so.</citsent>
<aftsection>
<nextsent>more specifically, this paper explores the following claims: 1.
</nextsent>
<nextsent>the rman definite article system can be.
</nextsent>
<nextsent>processed as efficiently as its old high german predecessor, which had less syncretism.
</nextsent>
<nextsent>2.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I874">
<title id=" E12-1084.xml">not as awful as it seems explaining german case through computational experiments in fluid construction grammar </title>
<section> ope rationalizing german case.  </section>
<citcontext>
<prevsection>
<prevsent>other feature structure platforms, such as the lkb-system (copestake, 2002), require aseparate parser and generator for formalizing bidirectional grammars, which make them less suited for substantiating the claims of this paper.
</prevsent>
<prevsent>3.1 distinctive feature matrix.
</prevsent>
</prevsection>
<citsent citstr=" P90-1025 ">
german case has become the litmus test for demonstrating how well feature-based grammar formalism copes with multi functionality, especially since ingria (1990) <papid> P90-1025 </papid>provocatively stated that unification is not the best technique for handling it.</citsent>
<aftsection>
<nextsent>people have gone to great lengths to countering rias claim, especially within the hpsg framework (e.g. mller, 1999; daniels, 2001; sag,2003), and various formalizations have been offered for german case (heinz and matiasek, 1994; mller, 2001; crysmann, 2005).
</nextsent>
<nextsent>however, these proposals either do not succeed in avoiding inefficient dis junctions or they require complex double type hierarchy (crysmann, 2005).
</nextsent>
<nextsent>the experiments in this paper use more straightforward solution, called distinctive feature matrix, which is based on an idea that was first explored by ingria (1990) <papid> P90-1025 </papid>and of which variation has recently also been proposed for lexical functional grammar (dalrymple et al 2009).</nextsent>
<nextsent>instead of treating case as single-valuedfeature, it can be represented as an array of features, as shown for the definite article die (ignor ing the genitive case for the time being): (2) die: ? ?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I884">
<title id=" E06-2002.xml">a web based demonstrator of a multilingual phrase based translation system </title>
<section> smt system description.  </section>
<citcontext>
<prevsection>
<prevsent>given string in the source language, the goal of the statistical machine translation is to select the string in the target language which maximizes the posterior distribution pr(e | f).
</prevsent>
<prevsent>by introducing the hidden word alignment variable a, the following approximate optimization criterion can be applied for that purpose: e?
</prevsent>
</prevsection>
<citsent citstr=" J96-1002 ">
= arg max pr(e | f) = arg max ? pr(e,a | f) ? arg max e,a pr(e,a | f) exploiting the maximum entropy (berger etal., 1996) <papid> J96-1002 </papid>framework, the conditional distribution pr(e,a | f) can be determined through suitable real valued functions (called features) hr(e, ,a), = 1 . . .</citsent>
<aftsection>
<nextsent>r, and takes the parametric form: p?(e,a | f) ? exp{ ? r=1 rhr(e, ,a)} the itc-irst system (chen et al, 2005) is based on log-linear model which extends the original ibm model 4 (brown et al, 1993) <papid> J93-2003 </papid>to phrases (koehn et al, 2003; <papid> N03-1017 </papid>federico and bertoldi, 2005).</nextsent>
<nextsent>in particular, target strings are built from sequences of phrases e1 . . .</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I885">
<title id=" E06-2002.xml">a web based demonstrator of a multilingual phrase based translation system </title>
<section> smt system description.  </section>
<citcontext>
<prevsection>
<prevsent>by introducing the hidden word alignment variable a, the following approximate optimization criterion can be applied for that purpose: e?
</prevsent>
<prevsent>= arg max pr(e | f) = arg max ? pr(e,a | f) ? arg max e,a pr(e,a | f) exploiting the maximum entropy (berger etal., 1996) <papid> J96-1002 </papid>framework, the conditional distribution pr(e,a | f) can be determined through suitable real valued functions (called features) hr(e, ,a), = 1 . . .</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
r, and takes the parametric form: p?(e,a | f) ? exp{ ? r=1 rhr(e, ,a)} the itc-irst system (chen et al, 2005) is based on log-linear model which extends the original ibm model 4 (brown et al, 1993) <papid> J93-2003 </papid>to phrases (koehn et al, 2003; <papid> N03-1017 </papid>federico and bertoldi, 2005).</citsent>
<aftsection>
<nextsent>in particular, target strings are built from sequences of phrases e1 . . .
</nextsent>
<nextsent>el. for each target phrase e?
</nextsent>
<nextsent>the corresponding source phrase within the source string is identified through three random quantities: the fertility ?, which establishes its length; the permutation pii, which sets its first position; the tablet f?
</nextsent>
<nextsent>, which tells its wordstring.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I886">
<title id=" E06-2002.xml">a web based demonstrator of a multilingual phrase based translation system </title>
<section> smt system description.  </section>
<citcontext>
<prevsection>
<prevsent>by introducing the hidden word alignment variable a, the following approximate optimization criterion can be applied for that purpose: e?
</prevsent>
<prevsent>= arg max pr(e | f) = arg max ? pr(e,a | f) ? arg max e,a pr(e,a | f) exploiting the maximum entropy (berger etal., 1996) <papid> J96-1002 </papid>framework, the conditional distribution pr(e,a | f) can be determined through suitable real valued functions (called features) hr(e, ,a), = 1 . . .</prevsent>
</prevsection>
<citsent citstr=" N03-1017 ">
r, and takes the parametric form: p?(e,a | f) ? exp{ ? r=1 rhr(e, ,a)} the itc-irst system (chen et al, 2005) is based on log-linear model which extends the original ibm model 4 (brown et al, 1993) <papid> J93-2003 </papid>to phrases (koehn et al, 2003; <papid> N03-1017 </papid>federico and bertoldi, 2005).</citsent>
<aftsection>
<nextsent>in particular, target strings are built from sequences of phrases e1 . . .
</nextsent>
<nextsent>el. for each target phrase e?
</nextsent>
<nextsent>the corresponding source phrase within the source string is identified through three random quantities: the fertility ?, which establishes its length; the permutation pii, which sets its first position; the tablet f?
</nextsent>
<nextsent>, which tells its wordstring.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I887">
<title id=" E06-2002.xml">a web based demonstrator of a multilingual phrase based translation system </title>
<section> smt system description.  </section>
<citcontext>
<prevsection>
<prevsent>2.3 phrase extraction and model training.
</prevsent>
<prevsent>training of the phrase-based translation model requires parallel corpus provided with word alignments in both directions, i.e. from source to target positions, and viceversa.
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
this preprocessing step can be accomplished by applying the giza++ toolkit (och and ney, 2003) <papid> J03-1002 </papid>that provides viterbi alignments based on ibm model-4.starting from the parallel training corpus, provided with direct and inverted alignments, the so called union alignment (och and ney, 2003) <papid> J03-1002 </papid>is computed.</citsent>
<aftsection>
<nextsent>phrase-pairs are extracted from each sentence pair which correspond to sub-intervals of the source and target positions, and , such that the union alignment links all positions of into and all positions of into . in general, phrases are extracted with maximum length in the source and target defined by the parameters jmax and imax.
</nextsent>
<nextsent>all such phrase-pairs are efficiently computed by an algorithm with complexity o(limaxj2max) (cet tolo et al, 2005).given all phrase-pairs extracted from the training corpus, lexicon probabilities and fertility probabilities are estimated.target language models (lms) used by the decoder and rescoring modules are, respectively, estimated from 3-gram and 4-gram statistics by applying the modified kneser-ney smoothing method (goodman and chen, 1998).
</nextsent>
<nextsent>lms are estimated with an in-house software toolkit which also provides compact binary representation of the lm which is used by the decoder.
</nextsent>
<nextsent>figure 1 shows the two-layer architecture of the demo.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I891">
<title id=" E12-1052.xml">joint satisfaction of syntactic and pragmatic constraints improves incremental spoken language understanding </title>
<section> introduction </section>
<citcontext>
<prevsection>

<prevsent>we present model of semantic processing of spoken language that (a) is robust against ill-formed input, such as can be expected from automatic speech recognisers, (b) respects both syntactic and pragmatic constraints in the computation of most likely interpretations, (c) uses principled, expressive semantic representation formalism(rmrs) with well-defined model theory, and (d) works continuously (producing meaning representations on word by-word basis, rather than only for full utterances) and incrementally (computing only the additional contribution by the new word, rather than re-computing for the whole utterance-so-far).we show that the joint satisfaction of syntactic and pragmatic constraints improves the performance of the nlu component (around 10% absolute, over syntax-only baseline).
</prevsent>
</prevsection>
<citsent citstr=" N09-1043 ">
incremental processing for spoken dialogue systems (i. e., the processing of user input even whileit still may be extended) has received renewed attention recently (aist et al 2007; baumann et al., 2009; <papid> N09-1043 </papid>bu?</citsent>
<aftsection>
<nextsent>and schlangen, 2010; skantze and hjalmarsson, 2010; <papid> W10-4301 </papid>devault et al 2011; purveret al 2011).<papid> W11-0144 </papid></nextsent>
<nextsent>most of the practical work, however, has so far focussed on realising the potential for generating more responsive system behaviour through making available processing results earlier (e. g.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I892">
<title id=" E12-1052.xml">joint satisfaction of syntactic and pragmatic constraints improves incremental spoken language understanding </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we present model of semantic processing of spoken language that (a) is robust against ill-formed input, such as can be expected from automatic speech recognisers, (b) respects both syntactic and pragmatic constraints in the computation of most likely interpretations, (c) uses principled, expressive semantic representation formalism(rmrs) with well-defined model theory, and (d) works continuously (producing meaning representations on word by-word basis, rather than only for full utterances) and incrementally (computing only the additional contribution by the new word, rather than re-computing for the whole utterance-so-far).we show that the joint satisfaction of syntactic and pragmatic constraints improves the performance of the nlu component (around 10% absolute, over syntax-only baseline).
</prevsent>
<prevsent>incremental processing for spoken dialogue systems (i. e., the processing of user input even whileit still may be extended) has received renewed attention recently (aist et al 2007; baumann et al., 2009; <papid> N09-1043 </papid>bu?</prevsent>
</prevsection>
<citsent citstr=" W10-4301 ">
and schlangen, 2010; skantze and hjalmarsson, 2010; <papid> W10-4301 </papid>devault et al 2011; purveret al 2011).<papid> W11-0144 </papid></citsent>
<aftsection>
<nextsent>most of the practical work, however, has so far focussed on realising the potential for generating more responsive system behaviour through making available processing results earlier (e. g.
</nextsent>
<nextsent>(skantze and schlangen, 2009)),<papid> E09-1085 </papid>but has otherwise followed typical pipeline architecture where processing results are passed only in one direction towards the next module.</nextsent>
<nextsent>in this paper, we investigate whether the other potential advantage of incremental processing?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I893">
<title id=" E12-1052.xml">joint satisfaction of syntactic and pragmatic constraints improves incremental spoken language understanding </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we present model of semantic processing of spoken language that (a) is robust against ill-formed input, such as can be expected from automatic speech recognisers, (b) respects both syntactic and pragmatic constraints in the computation of most likely interpretations, (c) uses principled, expressive semantic representation formalism(rmrs) with well-defined model theory, and (d) works continuously (producing meaning representations on word by-word basis, rather than only for full utterances) and incrementally (computing only the additional contribution by the new word, rather than re-computing for the whole utterance-so-far).we show that the joint satisfaction of syntactic and pragmatic constraints improves the performance of the nlu component (around 10% absolute, over syntax-only baseline).
</prevsent>
<prevsent>incremental processing for spoken dialogue systems (i. e., the processing of user input even whileit still may be extended) has received renewed attention recently (aist et al 2007; baumann et al., 2009; <papid> N09-1043 </papid>bu?</prevsent>
</prevsection>
<citsent citstr=" W11-0144 ">
and schlangen, 2010; skantze and hjalmarsson, 2010; <papid> W10-4301 </papid>devault et al 2011; purveret al 2011).<papid> W11-0144 </papid></citsent>
<aftsection>
<nextsent>most of the practical work, however, has so far focussed on realising the potential for generating more responsive system behaviour through making available processing results earlier (e. g.
</nextsent>
<nextsent>(skantze and schlangen, 2009)),<papid> E09-1085 </papid>but has otherwise followed typical pipeline architecture where processing results are passed only in one direction towards the next module.</nextsent>
<nextsent>in this paper, we investigate whether the other potential advantage of incremental processing?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I894">
<title id=" E12-1052.xml">joint satisfaction of syntactic and pragmatic constraints improves incremental spoken language understanding </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>and schlangen, 2010; skantze and hjalmarsson, 2010; <papid> W10-4301 </papid>devault et al 2011; purveret al 2011).<papid> W11-0144 </papid></prevsent>
<prevsent>most of the practical work, however, has so far focussed on realising the potential for generating more responsive system behaviour through making available processing results earlier (e. g.</prevsent>
</prevsection>
<citsent citstr=" E09-1085 ">
(skantze and schlangen, 2009)),<papid> E09-1085 </papid>but has otherwise followed typical pipeline architecture where processing results are passed only in one direction towards the next module.</citsent>
<aftsection>
<nextsent>in this paper, we investigate whether the other potential advantage of incremental processing?
</nextsent>
<nextsent>providing higher-level?-feedback to lower-levelmodules, in order to improve subsequent processing of the lower-level module can be realised as well.
</nextsent>
<nextsent>specifically, we experimented with giving syntactic parser feedback about whether semantic readings of nominal phrases it is in the process of constructing have denot ation in the given context or not.
</nextsent>
<nextsent>based on the assumption that speakers do plan their referring expressions so that they can successfully refer, we use this information to re-rank derivations; this in turn has an influence on how the derivations are expanded, given continued input.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I895">
<title id=" E12-1052.xml">joint satisfaction of syntactic and pragmatic constraints improves incremental spoken language understanding </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>we close with discussion and an outlook on future work.
</prevsent>
<prevsent>the idea of using real-world reference to inform syntactic structure building has been previously explored by number of authors.
</prevsent>
</prevsection>
<citsent citstr=" W04-0304 ">
stoness et al(2004), <papid> W04-0304 </papid>stoness et al(2005) describe proof-of-concept imple 514mentation of continuous understanding?</citsent>
<aftsection>
<nextsent>module that uses reference information in guiding bottom-up chart-parser, which is evaluated on single dialogue transcript.
</nextsent>
<nextsent>in contrast, our model uses probabilistic top-down parser with beam search (following roark (2001)) and is evaluated on large number of real-world utterances as processed by an automatic speech recogniser.
</nextsent>
<nextsent>similarly, devault and stone (2003) describe system that implements interaction between parser and higher-level modules (in this case, even more principled, trying to prove presuppositions),which however is also only tested on small, constructed data-set.
</nextsent>
<nextsent>schuler (2003) <papid> P03-1067 </papid>and schuler et al(2009) present model where information about reference is used directly within the speech recogniser, and hence informs not only syntactic processing but also word recognition.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I896">
<title id=" E12-1052.xml">joint satisfaction of syntactic and pragmatic constraints improves incremental spoken language understanding </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in contrast, our model uses probabilistic top-down parser with beam search (following roark (2001)) and is evaluated on large number of real-world utterances as processed by an automatic speech recogniser.
</prevsent>
<prevsent>similarly, devault and stone (2003) describe system that implements interaction between parser and higher-level modules (in this case, even more principled, trying to prove presuppositions),which however is also only tested on small, constructed data-set.
</prevsent>
</prevsection>
<citsent citstr=" P03-1067 ">
schuler (2003) <papid> P03-1067 </papid>and schuler et al(2009) present model where information about reference is used directly within the speech recogniser, and hence informs not only syntactic processing but also word recognition.</citsent>
<aftsection>
<nextsent>to this end, the processing is folded into the decoding step of the asr, andis realised as hierarchical hmm.
</nextsent>
<nextsent>while technically interesting, this approach is by design non modular and restricted in its syntactic expressivity.
</nextsent>
<nextsent>the work presented here also has connections to work in psycholinguistics.
</nextsent>
<nextsent>pado?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I897">
<title id=" E12-1052.xml">joint satisfaction of syntactic and pragmatic constraints improves incremental spoken language understanding </title>
<section> the model.  </section>
<citcontext>
<prevsection>
<prevsent>additionally, only single one of them is allowed to occur between the recognition of two adjacent input tokens.
</prevsent>
<prevsent>figure 1 illustrates this process for the first few words of the example sentence nimm den winkel inder dritten reihe?
</prevsent>
</prevsection>
<citsent citstr=" E09-1081 ">
(take the bracket in the third row), using the incremental unit (iu) model to represent increments and how they are linked; see (schlangen and skantze, 2009).<papid> E09-1081 </papid>2 here, syntactic 2very briefly: rounded boxes in the figures represent ius, and dashed arrows link an iu to its predecessor on the same level, where the levels correspond to processing stages.the figure shows the levels of input words, pos-tags, syntactic derivations and logical forms.</citsent>
<aftsection>
<nextsent>multiple ius sharing derivations (candidateanalysisius?)
</nextsent>
<nextsent>are represented by three features: list of the last parser actions of the derivation (ld), with rule expansion sor (robust) lexical matches; the derivation probability (p); and the remaining stack (s), where s* is the grammars start symbol and s!
</nextsent>
<nextsent>an explicit end-of-input marker.
</nextsent>
<nextsent>(to keep the figure small, we artificially reduced the beam size and cut off alternatives paths, shown in grey.)
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I898">
<title id=" E12-1052.xml">joint satisfaction of syntactic and pragmatic constraints improves incremental spoken language understanding </title>
<section> the model.  </section>
<citcontext>
<prevsection>
<prevsent>combining two rmrs structures involves at least joining their list of eps and argrels andof scope constraints.
</prevsent>
<prevsent>additionally, equations between the variables can connect two structures, which is an essential requirement for semanticconstruction.
</prevsent>
</prevsection>
<citsent citstr=" W07-1210 ">
a semantic algebra for the combination of rmrss in non-lexicalist setting is defined in (copestake, 2007)<papid> W07-1210 </papid></citsent>
<aftsection>
<nextsent>unsaturated semantic increments have open slots that need to be filled by what is called the hook of another structure.
</nextsent>
<nextsent>hook and slot are triples [`:a:x] consisting of alabel, an anchor and an index variable.
</nextsent>
<nextsent>every variable of the hook is equated with the corresponding one in the slot.
</nextsent>
<nextsent>this way the semantic representation can grow monotonic ally at each combinatory step by simply adding predicates, constraints and equations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I901">
<title id=" E12-1052.xml">joint satisfaction of syntactic and pragmatic constraints improves incremental spoken language understanding </title>
<section> experiments and results.  </section>
<citcontext>
<prevsection>
<prevsent>the degradation thus changes the derivation order in the parsing queue for the next input item and increases the chances of degraded derivations to be pruned in the following parsing step.
</prevsent>
<prevsent>4.1 data.
</prevsent>
</prevsection>
<citsent citstr=" W09-3905 ">
we use data from the pentomino puzzle piece do main (which has been used before for example by (fernandez and schlangen, 2007; schlangen et al., 2009)), <papid> W09-3905 </papid>collected in wizard-of-oz study.</citsent>
<aftsection>
<nextsent>in this specific setting, users gave instructions to the system (the wizard) in order to manipulate (select, rotate, mirror, delete) puzzle pieces on an upper board and to put them onto lower board, reaching pre-specified goal state.
</nextsent>
<nextsent>figure 2 shows an example configuration.
</nextsent>
<nextsent>each participant took partin several rounds in which the distinguishing characteristics for puzzle pieces (color, shape, proposed name, position on the board) varied widely.
</nextsent>
<nextsent>in total, 20 participants played 284 games.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I902">
<title id=" E12-1052.xml">joint satisfaction of syntactic and pragmatic constraints improves incremental spoken language understanding </title>
<section> experiments and results.  </section>
<citcontext>
<prevsection>
<prevsent>moreover, this metric can be applied at each incremental step, which is not clear how to do with more traditional metrics.
</prevsent>
<prevsent>4.4 experiments.
</prevsent>
</prevsection>
<citsent citstr=" W10-4308 ">
our parser, semantic construction and reference resolution modules are implemented within the inprotk toolkit for incremental spoken dialogue systems development (schlangen et al 2010).<papid> W10-4308 </papid></citsent>
<aftsection>
<nextsent>in this toolkit, incremental hypotheses are modified as more information becomes available over time.
</nextsent>
<nextsent>our modules support all such modifications (i. e. also allow to revert their states and output if word input is revoked).
</nextsent>
<nextsent>as explained in section 4.1, we used offline recognition results in our evaluation.
</nextsent>
<nextsent>however, the results would be identical if we were to usethe incremental speech recognition output of in protk directly.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I904">
<title id=" E12-1025.xml">active learning for interactive machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>translation needs have greatly increased during the last years due to phenomena such as globalization and technologic development.
</prevsent>
<prevsent>for example, the european parliament1 translates its proceedings to 22 languages in regular basis or project syndicate2 that translates editorials into different languages.
</prevsent>
</prevsection>
<citsent citstr=" N10-1062 ">
in these and many other examples, data can be viewed as an incoming unbounded stream since it grows continually with time (levenberg et al 2010).<papid> N10-1062 </papid></citsent>
<aftsection>
<nextsent>manual translation of such streams of data is extremely expensive given the huge volume of translation required, 1http://www.europarl.europa.eu 2http://project-syndicate.org therefore various automatic machine translation methods have been proposed.however, automatic statistical machine translation (smt) systems are far from generating error-free translations and their outputs usually require human post-editing in order to achievehigh-quality translations.
</nextsent>
<nextsent>one way of taking advantage of smt systems is to combine them with the knowledge of human translator in the interactive-predictive machine translation (imt)framework (foster et al 1998; langlais and lapalme, 2002; barrachina et al 2009), <papid> J09-1002 </papid>which isa particular case of the computer-assisted translation paradigm (isabelle and church, 1997).</nextsent>
<nextsent>in the imt framework, state-of-the-art smt mod eland human translator collaborate to obtain high quality translations while minimizing required human effort.unfortunately, the application of either post editing or imt to data streams with massive data volumes is still too expensive, simply because manual supervision of all instances requires huge amounts of manpower.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I905">
<title id=" E12-1025.xml">active learning for interactive machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in these and many other examples, data can be viewed as an incoming unbounded stream since it grows continually with time (levenberg et al 2010).<papid> N10-1062 </papid></prevsent>
<prevsent>manual translation of such streams of data is extremely expensive given the huge volume of translation required, 1http://www.europarl.europa.eu 2http://project-syndicate.org therefore various automatic machine translation methods have been proposed.however, automatic statistical machine translation (smt) systems are far from generating error-free translations and their outputs usually require human post-editing in order to achievehigh-quality translations.</prevsent>
</prevsection>
<citsent citstr=" J09-1002 ">
one way of taking advantage of smt systems is to combine them with the knowledge of human translator in the interactive-predictive machine translation (imt)framework (foster et al 1998; langlais and lapalme, 2002; barrachina et al 2009), <papid> J09-1002 </papid>which isa particular case of the computer-assisted translation paradigm (isabelle and church, 1997).</citsent>
<aftsection>
<nextsent>in the imt framework, state-of-the-art smt mod eland human translator collaborate to obtain high quality translations while minimizing required human effort.unfortunately, the application of either post editing or imt to data streams with massive data volumes is still too expensive, simply because manual supervision of all instances requires huge amounts of manpower.
</nextsent>
<nextsent>for such massive data streams the need of employing active learning(al) is compelling.
</nextsent>
<nextsent>al techniques for imt selectively ask an oracle (e.g. human transla tor) to supervise small portion of the incoming sentences.
</nextsent>
<nextsent>sentences are selected so that smt models estimated from them translate new sentences as accurately as possible.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I906">
<title id=" E12-1025.xml">active learning for interactive machine translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in short, our proposal divides the datastream into blocks where al techniques for static datasets are applied.
</prevsent>
<prevsent>additionally, we implement an incremental learning technique to efficiently train the base smt models as new data is available.
</prevsent>
</prevsection>
<citsent citstr=" N09-1047 ">
a body of work has recently been proposed to apply al techniques to smt (haffari et al 2009;<papid> N09-1047 </papid>ambati et al 2010; bloodgood and callison burch, 2010).<papid> P10-1088 </papid></citsent>
<aftsection>
<nextsent>the aim of these works is tobuild one single optimal smt model from manually translated data extracted from static datasets.
</nextsent>
<nextsent>none of them fit in the setting of data streams.
</nextsent>
<nextsent>some of the above described challenges of alfrom unbounded streams have been previously addressed in the mt literature.
</nextsent>
<nextsent>in order to deal with the evolutionary nature of the problem, nepveu et al.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I907">
<title id=" E12-1025.xml">active learning for interactive machine translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in short, our proposal divides the datastream into blocks where al techniques for static datasets are applied.
</prevsent>
<prevsent>additionally, we implement an incremental learning technique to efficiently train the base smt models as new data is available.
</prevsent>
</prevsection>
<citsent citstr=" P10-1088 ">
a body of work has recently been proposed to apply al techniques to smt (haffari et al 2009;<papid> N09-1047 </papid>ambati et al 2010; bloodgood and callison burch, 2010).<papid> P10-1088 </papid></citsent>
<aftsection>
<nextsent>the aim of these works is tobuild one single optimal smt model from manually translated data extracted from static datasets.
</nextsent>
<nextsent>none of them fit in the setting of data streams.
</nextsent>
<nextsent>some of the above described challenges of alfrom unbounded streams have been previously addressed in the mt literature.
</nextsent>
<nextsent>in order to deal with the evolutionary nature of the problem, nepveu et al.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I909">
<title id=" E12-1025.xml">active learning for interactive machine translation </title>
<section> interactive machine translation.  </section>
<citcontext>
<prevsection>
<prevsent>finally, we integrate all these elements to define an al framework for imt with an objective of obtaining an optimum balance between translation quality and human user effort.
</prevsent>
<prevsent>imt can be seen as an evolution of the smt framework.
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
given sentence from source language to be translated into sentence of target language, the fundamental equation of smt (brown et al 1993) <papid> J93-2003 </papid>is defined as follows: e?</citsent>
<aftsection>
<nextsent>= argmax pr(e | f) (1) where pr(e | f) is usually approximated by loglinear translation model (koehn et al 2003).<papid> N03-1017 </papid></nextsent>
<nextsent>inthis case, the decision rule is given by the expres sion: e?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I911">
<title id=" E12-1025.xml">active learning for interactive machine translation </title>
<section> interactive machine translation.  </section>
<citcontext>
<prevsection>
<prevsent>imt can be seen as an evolution of the smt framework.
</prevsent>
<prevsent>given sentence from source language to be translated into sentence of target language, the fundamental equation of smt (brown et al 1993) <papid> J93-2003 </papid>is defined as follows: e?</prevsent>
</prevsection>
<citsent citstr=" N03-1017 ">
= argmax pr(e | f) (1) where pr(e | f) is usually approximated by loglinear translation model (koehn et al 2003).<papid> N03-1017 </papid></citsent>
<aftsection>
<nextsent>inthis case, the decision rule is given by the expres sion: e?
</nextsent>
<nextsent>= argmax { m?
</nextsent>
<nextsent>m=1 mhm(e, f) } (2)where each hm(e, f) is feature function representing statistical model and its weight.in the imt framework, human translator is introduced in the translation process to collaborate with an smt model.
</nextsent>
<nextsent>forgiven source sentence, the smt model fully automatically generates an initial translation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I914">
<title id=" E12-1025.xml">active learning for interactive machine translation </title>
<section> sentence sampling strategies.  </section>
<citcontext>
<prevsection>
<prevsent>the intuition behind this approach is that an smt model can not generate good translations unless it has enough information to translate the sentence.
</prevsent>
<prevsent>the usual approach to compute the quality of atranslation hypothesis is to compare it to reference translation, but, in this case, it is not valid option since reference translations are not available.
</prevsent>
</prevsection>
<citsent citstr=" W03-0413 ">
hence, we use confidence estimation (gandrabur and foster, 2003; <papid> W03-0413 </papid>blatz et al 2004; <papid> C04-1046 </papid>ueffing and ney, 2007) <papid> J07-1003 </papid>to estimate the probability of correctness of the translations.</citsent>
<aftsection>
<nextsent>specifically, we estimate the quality of translation from the confidence scores of their individual words.the confidence score of word ei of the translation = e1 . . .
</nextsent>
<nextsent>ei . . .
</nextsent>
<nextsent>ei generated from the source sentence = f1 . . .
</nextsent>
<nextsent>fj . . .
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I915">
<title id=" E12-1025.xml">active learning for interactive machine translation </title>
<section> sentence sampling strategies.  </section>
<citcontext>
<prevsection>
<prevsent>the intuition behind this approach is that an smt model can not generate good translations unless it has enough information to translate the sentence.
</prevsent>
<prevsent>the usual approach to compute the quality of atranslation hypothesis is to compare it to reference translation, but, in this case, it is not valid option since reference translations are not available.
</prevsent>
</prevsection>
<citsent citstr=" C04-1046 ">
hence, we use confidence estimation (gandrabur and foster, 2003; <papid> W03-0413 </papid>blatz et al 2004; <papid> C04-1046 </papid>ueffing and ney, 2007) <papid> J07-1003 </papid>to estimate the probability of correctness of the translations.</citsent>
<aftsection>
<nextsent>specifically, we estimate the quality of translation from the confidence scores of their individual words.the confidence score of word ei of the translation = e1 . . .
</nextsent>
<nextsent>ei . . .
</nextsent>
<nextsent>ei generated from the source sentence = f1 . . .
</nextsent>
<nextsent>fj . . .
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I916">
<title id=" E12-1025.xml">active learning for interactive machine translation </title>
<section> sentence sampling strategies.  </section>
<citcontext>
<prevsection>
<prevsent>the intuition behind this approach is that an smt model can not generate good translations unless it has enough information to translate the sentence.
</prevsent>
<prevsent>the usual approach to compute the quality of atranslation hypothesis is to compare it to reference translation, but, in this case, it is not valid option since reference translations are not available.
</prevsent>
</prevsection>
<citsent citstr=" J07-1003 ">
hence, we use confidence estimation (gandrabur and foster, 2003; <papid> W03-0413 </papid>blatz et al 2004; <papid> C04-1046 </papid>ueffing and ney, 2007) <papid> J07-1003 </papid>to estimate the probability of correctness of the translations.</citsent>
<aftsection>
<nextsent>specifically, we estimate the quality of translation from the confidence scores of their individual words.the confidence score of word ei of the translation = e1 . . .
</nextsent>
<nextsent>ei . . .
</nextsent>
<nextsent>ei generated from the source sentence = f1 . . .
</nextsent>
<nextsent>fj . . .
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I919">
<title id=" E12-1025.xml">active learning for interactive machine translation </title>
<section> retraining of the smt model.  </section>
<citcontext>
<prevsection>
<prevsent>the incremental version of the em algorithm (neal and hinton, 1999) is used to incrementally train the ibm model 1.
</prevsent>
<prevsent>to retrain the smt model, we implement the online learning techniques proposed in (ortizmartnez et al 2010).
</prevsent>
</prevsection>
<citsent citstr=" P02-1038 ">
in that work, state of-the-art log-linear model (och and ney, 2002) <papid> P02-1038 </papid>and set of techniques to incrementally train this model were defined.</citsent>
<aftsection>
<nextsent>the log-linear model is composed of set of feature functions governing different aspects of the translation process, including language model, source sentence length model, inverse and direct translation models, target phrase length model, source phrase?
</nextsent>
<nextsent>length model and distortion model.
</nextsent>
<nextsent>the incremental learning algorithm allows us to process each new training sample in constant time (i.e. the computational complexity of training new sample does not depend on the number of previously seen training samples).
</nextsent>
<nextsent>to do that, set of sufficient statistics is maintained for each feature function.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I920">
<title id=" E12-1025.xml">active learning for interactive machine translation </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>since each experiment is carried out under the same conditions, if one sampling strategy outperforms its peers, then we can safely conclude that this is because the sentences selected to be translated are more informative.
</prevsent>
<prevsent>7.1 training corpus and datastream.
</prevsent>
</prevsection>
<citsent citstr=" W06-3114 ">
the training data comes from the europarl corpus as distributed for the shared task in the naacl2006 workshop on statistical machine translation (koehn and monz, 2006).<papid> W06-3114 </papid></citsent>
<aftsection>
<nextsent>we used this data to estimate the initial log-linear model used by our imt system (see section 6).
</nextsent>
<nextsent>the weights of the different feature functions were tuned by meansof minimum error rate training (och, 2003) <papid> P03-1021 </papid>executed on the europarl development corpus.</nextsent>
<nextsent>once the smt model was trained, we use the news commentary corpus (callison-burch et al 2007)to simulate the data stream.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I921">
<title id=" E12-1025.xml">active learning for interactive machine translation </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>the training data comes from the europarl corpus as distributed for the shared task in the naacl2006 workshop on statistical machine translation (koehn and monz, 2006).<papid> W06-3114 </papid></prevsent>
<prevsent>we used this data to estimate the initial log-linear model used by our imt system (see section 6).</prevsent>
</prevsection>
<citsent citstr=" P03-1021 ">
the weights of the different feature functions were tuned by meansof minimum error rate training (och, 2003) <papid> P03-1021 </papid>executed on the europarl development corpus.</citsent>
<aftsection>
<nextsent>once the smt model was trained, we use the news commentary corpus (callison-burch et al 2007)to simulate the datastream.
</nextsent>
<nextsent>the size of these corpora is shown in table 1.
</nextsent>
<nextsent>the reasons to choose the news commentary corpus to carry out our experiments are threefold: first, its size is large enough to simulate datastream and test our al techniques in the long term; second, it is out-of-domain data which allows us to simulatea real-world situation that may occur in translation company, and, finally, it consists in editorials from eclectic domain: general politics, economics and science, which effectively represents the variations in the sentence distributions of the simulated datastream.
</nextsent>
<nextsent>7.2 assessment criteria.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I922">
<title id=" E12-1025.xml">active learning for interactive machine translation </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>the reasons to choose the news commentary corpus to carry out our experiments are threefold: first, its size is large enough to simulate datastream and test our al techniques in the long term; second, it is out-of-domain data which allows us to simulatea real-world situation that may occur in translation company, and, finally, it consists in editorials from eclectic domain: general politics, economics and science, which effectively represents the variations in the sentence distributions of the simulated datastream.
</prevsent>
<prevsent>7.2 assessment criteria.
</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
we want to measure both the quality of the generated translations and the human effort required to obtain them.we measure translation quality with the well known bleu (papineni et al 2002) <papid> P02-1040 </papid>score.</citsent>
<aftsection>
<nextsent>to estimate human user effort, we simulate the actions taken by human user in its interaction with the imt system.
</nextsent>
<nextsent>the first translation hypothesis for each given source sentence is compared with single reference translation and the longest common character prefix (lcp) is obtained.
</nextsent>
<nextsent>the first non-matching character is replaced by the corresponding reference character and then new translation hypothesis is produced (see figure 1).
</nextsent>
<nextsent>this process is iterated until full match with the reference is obtained.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I926">
<title id=" E12-1048.xml">evaluating language understanding accuracy with respect to objective outcomes in a dialogue system </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, once component, such as parser, is included in larger system, it is not always clear that improvements in intrinsic evaluation scores will translate into improved over all system performance.
</prevsent>
<prevsent>therefore, extrinsic or task-based evaluation can be used to complement intrinsic evaluations.
</prevsent>
</prevsection>
<citsent citstr=" P10-1122 ">
for example, nlp components such as parsers and co-reference resolution algorithms could be compared in terms of how much they contribute to the performance of textual entailment (rte) system (sammons et al ., 2010; <papid> P10-1122 </papid>yuret et al  2010); <papid> S10-1009 </papid>parser performance could be evaluated by how well it contributes to an information retrieval task (miyao et al  2008).<papid> P08-1006 </papid></citsent>
<aftsection>
<nextsent>however, task-based evaluation can be difficult and expensive for interactive applications.
</nextsent>
<nextsent>specifically, task-based evaluation for dialogue systems typically involves collecting data from number of people interacting with the system, which is time-consuming and labor-intensive.
</nextsent>
<nextsent>thus, it is desirable to develop an off-line evaluation procedure that relates intrinsic evaluation metrics to predicted interaction outcomes, reducing the need to conduct experiments with human participants.
</nextsent>
<nextsent>this problem can be addressed via the use ofthe paradise evaluation methodology for spoken dialogue systems (walker et al  2000).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I927">
<title id=" E12-1048.xml">evaluating language understanding accuracy with respect to objective outcomes in a dialogue system </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, once component, such as parser, is included in larger system, it is not always clear that improvements in intrinsic evaluation scores will translate into improved over all system performance.
</prevsent>
<prevsent>therefore, extrinsic or task-based evaluation can be used to complement intrinsic evaluations.
</prevsent>
</prevsection>
<citsent citstr=" S10-1009 ">
for example, nlp components such as parsers and co-reference resolution algorithms could be compared in terms of how much they contribute to the performance of textual entailment (rte) system (sammons et al ., 2010; <papid> P10-1122 </papid>yuret et al  2010); <papid> S10-1009 </papid>parser performance could be evaluated by how well it contributes to an information retrieval task (miyao et al  2008).<papid> P08-1006 </papid></citsent>
<aftsection>
<nextsent>however, task-based evaluation can be difficult and expensive for interactive applications.
</nextsent>
<nextsent>specifically, task-based evaluation for dialogue systems typically involves collecting data from number of people interacting with the system, which is time-consuming and labor-intensive.
</nextsent>
<nextsent>thus, it is desirable to develop an off-line evaluation procedure that relates intrinsic evaluation metrics to predicted interaction outcomes, reducing the need to conduct experiments with human participants.
</nextsent>
<nextsent>this problem can be addressed via the use ofthe paradise evaluation methodology for spoken dialogue systems (walker et al  2000).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I928">
<title id=" E12-1048.xml">evaluating language understanding accuracy with respect to objective outcomes in a dialogue system </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, once component, such as parser, is included in larger system, it is not always clear that improvements in intrinsic evaluation scores will translate into improved over all system performance.
</prevsent>
<prevsent>therefore, extrinsic or task-based evaluation can be used to complement intrinsic evaluations.
</prevsent>
</prevsection>
<citsent citstr=" P08-1006 ">
for example, nlp components such as parsers and co-reference resolution algorithms could be compared in terms of how much they contribute to the performance of textual entailment (rte) system (sammons et al ., 2010; <papid> P10-1122 </papid>yuret et al  2010); <papid> S10-1009 </papid>parser performance could be evaluated by how well it contributes to an information retrieval task (miyao et al  2008).<papid> P08-1006 </papid></citsent>
<aftsection>
<nextsent>however, task-based evaluation can be difficult and expensive for interactive applications.
</nextsent>
<nextsent>specifically, task-based evaluation for dialogue systems typically involves collecting data from number of people interacting with the system, which is time-consuming and labor-intensive.
</nextsent>
<nextsent>thus, it is desirable to develop an off-line evaluation procedure that relates intrinsic evaluation metrics to predicted interaction outcomes, reducing the need to conduct experiments with human participants.
</nextsent>
<nextsent>this problem can be addressed via the use ofthe paradise evaluation methodology for spoken dialogue systems (walker et al  2000).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I929">
<title id=" E12-1048.xml">evaluating language understanding accuracy with respect to objective outcomes in a dialogue system </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>typically, multiple linear regression is used to fit predictive model of the desired metric based on the values of interaction parameters that can be derived from system logs with out additional user studies (e.g., dialogue length, word error rate, number of misunderstandings).
</prevsent>
<prevsent>paradise models have been used extensively in task-oriented spoken dialogue systems to establish which components of the system most need improvement, with user satisfaction as the out come metric (moller et al  2007; moller et al 2008; walker et al  2000; larsen, 2003).
</prevsent>
</prevsection>
<citsent citstr=" W06-1611 ">
in tutorial dialogue, paradise studies investigated 471which manually annotated features predict learning outcomes, to justify new features needed in the system (forbes-riley et al  2007; rotaru and litman, 2006; <papid> W06-1611 </papid>forbes-riley and litman, 2006).we adapt the paradise methodology to evaluating individual nlp components, linking commonly used intrinsic evaluation scores with extrinsic outcome metrics.</citsent>
<aftsection>
<nextsent>we describe an evaluation of an interpretation component of tutorial dialogue system, with student learning gain as the target outcome measure.
</nextsent>
<nextsent>we first describe the evaluation setup, which uses standard classification accuracy metrics for system evaluation (section 2).
</nextsent>
<nextsent>we discuss the results of the intrinsic system evaluation in section 3.
</nextsent>
<nextsent>we then show that standard evaluation metrics do not serve as good predictors of system performance for the system we evaluated.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I930">
<title id=" E12-1048.xml">evaluating language understanding accuracy with respect to objective outcomes in a dialogue system </title>
<section> evaluation procedure.  </section>
<citcontext>
<prevsection>
<prevsent>finally, we discuss some limitations and possible extensions to this approach (section 6).
</prevsent>
<prevsent>2.1 data collection.
</prevsent>
</prevsection>
<citsent citstr=" P10-4003 ">
we collected transcripts of students interacting with beetle ii (dzikovska et al  2010<papid> P10-4003 </papid>b), tutorial dialogue system for teaching conceptual knowledge in the basic electricity and electronics domain.</citsent>
<aftsection>
<nextsent>the system is learning environment with self-contained curriculum targeted at students with no knowledge of high school physics.
</nextsent>
<nextsent>when interacting with the system, students spend3-5 hours going through pre-prepared reading material, building and observing circuits in simulator, and talking with dialogue-based computer tutor via text-based chat interface.
</nextsent>
<nextsent>during the interaction, students can be asked two types of questions.
</nextsent>
<nextsent>factual questions require them to name set of objects or simple property, e.g., which components in circuit 1 are in closed path??
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I934">
<title id=" E12-1048.xml">evaluating language understanding accuracy with respect to objective outcomes in a dialogue system </title>
<section> evaluation procedure.  </section>
<citcontext>
<prevsection>
<prevsent>which appears to relate to social interaction and has no recognizable domain content.
</prevsent>
<prevsent>uninterpretable: the system could not arrive at any interpretation of the utterance.
</prevsent>
</prevsection>
<citsent citstr=" W09-3906 ">
it will respond by identifying the likely source oferror, if possible (e.g., word it does not un derstand) and asking the student to rephrase their utterance (dzikovska et al  2009).<papid> W09-3906 </papid></citsent>
<aftsection>
<nextsent>472 if the student utterance was determined to be an answer, it is further diagnosed for correctness as discussed in (dzikovska et al  2010<papid> P10-4003 </papid>b), using domain reasoner together with semantic representations of expected correct answers supplied by human tutors.</nextsent>
<nextsent>the resulting diagnosis contains the following information: ? consistency: whether the student statement correctly describes the facts mentioned in the question and the simulation environment: e.g., student saying switch is closed?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I941">
<title id=" E12-1048.xml">evaluating language understanding accuracy with respect to objective outcomes in a dialogue system </title>
<section> evaluation procedure.  </section>
<citcontext>
<prevsection>
<prevsent>in future work, performance of individual pipeline components could also be evaluated in similar fashion.
</prevsent>
<prevsent>2.3 data annotation.
</prevsent>
</prevsection>
<citsent citstr=" W06-3503 ">
the general idea of breaking down the student answer into correct, incorrect and missing parts is common in tutorial dialogue systems (nielsen et al ., 2008; dzikovska et al  2010<papid> P10-4003 </papid>b; jordan et al  2006).<papid> W06-3503 </papid></citsent>
<aftsection>
<nextsent>however, representation details are highly system specific, and difficult and time-consumingto annotate.
</nextsent>
<nextsent>therefore we implemented simplified annotation scheme which classifies whole answers as correct, partially correct but incomplete, or contradictory, without explicitly identifying the correct and incorrect parts.
</nextsent>
<nextsent>this makes it easier to create the gold standard and still retains useful information, because tutoring systems often choose the tutoring strategy based on the general answer class (correct, incomplete, or contradictory).
</nextsent>
<nextsent>in addition, this allows us to cast the problem in terms of classifier evaluation, and to use standard classifier evaluation metrics.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I942">
<title id=" E12-1048.xml">evaluating language understanding accuracy with respect to objective outcomes in a dialogue system </title>
<section> intrinsic evaluation results.  </section>
<citcontext>
<prevsection>
<prevsent>the distribution is unbalanced, and therefore in our evaluation results we use two different ways to average over per-classevaluation scores.
</prevsent>
<prevsent>macro-average combines per class scores disregarding the class sizes; micro average weighs the per-class scores by class size.
</prevsent>
</prevsection>
<citsent citstr=" H91-1061 ">
the overall classification accuracy (defined as the number of correctly classified instances out of allinstances) is mathematically equivalent to micro averaged recall; however, macro-averaging better reflects performance on small classes, and is commonly used for unbalanced classification problems (see, e.g., (lewis, 1991)).<papid> H91-1061 </papid></citsent>
<aftsection>
<nextsent>the detailed evaluation results are presented in table 2.
</nextsent>
<nextsent>we will focus on two metrics: the overall classification accuracy (listed as micro averaged recall?
</nextsent>
<nextsent>as discussed above), and the macro-averaged score.the majority class baseline is to assign cor rect?
</nextsent>
<nextsent>to every instance.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I946">
<title id=" E12-1048.xml">evaluating language understanding accuracy with respect to objective outcomes in a dialogue system </title>
<section> discussion and future work.  </section>
<citcontext>
<prevsection>
<prevsent>we plan to investigate this in the future.
</prevsent>
<prevsent>our method would apply to wide range of systems.
</prevsent>
</prevsection>
<citsent citstr=" W07-0304 ">
it can be used straightforwardly with many current spoken dialogue systems which relyon classifiers to support language understanding in domains such as call routing and technical support (gupta et al  2006; acomb et al  2007).<papid> W07-0304 </papid>we applied it to system that outputs more complex logical forms, but we showed that we could simplify its output to set of labels which still allowed us to make informed decisions.</citsent>
<aftsection>
<nextsent>similar simplifications could be derived for other systems based on domain-specific dialogue acts typically used in dialogue management.
</nextsent>
<nextsent>for slot based systems, it may be useful to consider concept accuracy for recognizing individual slot values.
</nextsent>
<nextsent>finally, for tutoring systems it is possible to annotate the answers on more fine-grainedlevel.
</nextsent>
<nextsent>nielsen et al (2008) proposed an annotation scheme based on the output of dependency parser, and trained classifier to identify individual dependencies as expressed?, contradicted or unaddressed?.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I947">
<title id=" E09-3011.xml">extraction of definitions using grammar enhanced machine learning </title>
<section> related research.  </section>
<citcontext>
<prevsection>
<prevsent>research in this area initially relied almost totally on pattern identification and extraction (cf.
</prevsent>
<prevsent>(tjong kim sang et al, 2005)) and only later, machine learning techniques have been employed (cf.
</prevsent>
</prevsection>
<citsent citstr=" W06-2609 ">
(blair-goldensohn et al, 2004;fahmi and bouma, 2006; <papid> W06-2609 </papid>miliaraki and androutsopoulos, 2004)).<papid> C04-1199 </papid></citsent>
<aftsection>
<nextsent>fahmi and bouma (2006) <papid> W06-2609 </papid>combine pattern matching and machine learning.</nextsent>
<nextsent>first, candidate definitions which consist of subject, copular verb and predicative phrase are extracted from afully parsed text using syntactic properties.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I948">
<title id=" E09-3011.xml">extraction of definitions using grammar enhanced machine learning </title>
<section> related research.  </section>
<citcontext>
<prevsection>
<prevsent>research in this area initially relied almost totally on pattern identification and extraction (cf.
</prevsent>
<prevsent>(tjong kim sang et al, 2005)) and only later, machine learning techniques have been employed (cf.
</prevsent>
</prevsection>
<citsent citstr=" C04-1199 ">
(blair-goldensohn et al, 2004;fahmi and bouma, 2006; <papid> W06-2609 </papid>miliaraki and androutsopoulos, 2004)).<papid> C04-1199 </papid></citsent>
<aftsection>
<nextsent>fahmi and bouma (2006) <papid> W06-2609 </papid>combine pattern matching and machine learning.</nextsent>
<nextsent>first, candidate definitions which consist of subject, copular verb and predicative phrase are extracted from afully parsed text using syntactic properties.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I952">
<title id=" E09-3011.xml">extraction of definitions using grammar enhanced machine learning </title>
<section> definitions.  </section>
<citcontext>
<prevsection>
<prevsent>the definiendum is the element that is defined (latin: that which is to be defined).
</prevsent>
<prevsent>the definiens provides the meaning of the definiendum(latin: that which is doing the defining).
</prevsent>
</prevsection>
<citsent citstr=" W06-0203 ">
definien dum and definiens are connected by verb or punctuation mark, the connector, which indicates the relation between definiendum and definiens (walter and pinkal, 2006).<papid> W06-0203 </papid>to be able to write grammar rules we first extracted 549 definitions manually from 45 dutch text documents.</citsent>
<aftsection>
<nextsent>those documents consisted of manuals and texts on computing (e.g. word, la tex) and descriptive documents on academic skill sand elearning.
</nextsent>
<nextsent>all of them could be relevant learn 89 type example sentence to be gnuplot is een programma om grafieken te maken gnuplot is program for drawing graphs verb e-learning omvat hulpmiddelen en toepassingen die via het internet beschikbaar zijn en creatieve mogeli jkheden bieden om de leerervaring te verbeteren . elearning comprises resources and application that are available via the internet and provide creative possibilities to improve the learning experience?
</nextsent>
<nextsent>punctuation passen: plastic kaarten voorzien van een magnetische strip, die door een gleuf gehaald worden, waardoor de gebruiker zich kan identificeren en toegang krijgt tot bepaalde faciliteiten.
</nextsent>
<nextsent>passes: plastic cards equipped with magnetic strip, that can be swiped through card reader, by means of which the identity of the user can be verified and the user gets access to certain facilities.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I960">
<title id=" E12-1062.xml">learning for microblogs with distant supervision political forecasting with twitter </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>automated algorithms can use this data to provide instant feed back on what people are saying about topic.
</prevsent>
<prevsent>two challenges in building such algorithms are(1) identifying topic-relevant posts, and (2) identifying the attitude of each post toward the topic.
</prevsent>
</prevsection>
<citsent citstr=" P09-1113 ">
this paper studies distant supervision (mintz et al ., 2009) <papid> P09-1113 </papid>as solution to both challenges.</citsent>
<aftsection>
<nextsent>we apply our approach to the problem of predicting presidential job approval polls from twitter data, and we present results that improve on previous work in this area.
</nextsent>
<nextsent>we also present novel base line that performs remarkably well without using topic identification.
</nextsent>
<nextsent>topic identification is the task of identifying text that discusses topic of interest.
</nextsent>
<nextsent>most previous work on microblogs uses simple keyword searches to find topic-relevant tweets on the assumption that short tweets do not need more sophisticated processing.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I961">
<title id=" E12-1062.xml">learning for microblogs with distant supervision political forecasting with twitter </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>sentiment analysis encompasses broad field of research, but most microblog work focuses on two moods: positive and negative sentiment.
</prevsent>
<prevsent>603 algorithms to identify these moods range from matching words in sentiment lexicon to training classifiers with hand-labeled corpus.
</prevsent>
</prevsection>
<citsent citstr=" C10-2028 ">
since labeling corpora is expensive, recent work on twitter uses emoticons (i.e., ascii smiley faces such as :-( and :-)) as noisy labels in tweets for distant supervision (pak and paroubek, 2010; davidov et al ., 2010; <papid> C10-2028 </papid>kouloumpis et al  2011).</citsent>
<aftsection>
<nextsent>this paper presents new analysis of the downstream effects of topic identification on sentiment classifiers and their application to political forecasting.
</nextsent>
<nextsent>interest in measuring the political mood of country has recently grown (oconnor et al  2010; tumasjan et al  2010; gonzalez-bailon et al ., 2010; carvalho et al  2011; <papid> P11-2099 </papid>tan et al  2011).here we compare our sentiment results to presidential job approval polls and show that the sentiment scores produced by our system are positively correlated with both the approval and disapproval job ratings.in this paper we present method for coupling two distantly supervised algorithms for topic identification and sentiment classification on twitter.</nextsent>
<nextsent>in section 4, we describe our approach to topic identification and present new annotated corpus of political tweets for future study.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I963">
<title id=" E12-1062.xml">learning for microblogs with distant supervision political forecasting with twitter </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>since labeling corpora is expensive, recent work on twitter uses emoticons (i.e., ascii smiley faces such as :-( and :-)) as noisy labels in tweets for distant supervision (pak and paroubek, 2010; davidov et al ., 2010; <papid> C10-2028 </papid>kouloumpis et al  2011).</prevsent>
<prevsent>this paper presents new analysis of the downstream effects of topic identification on sentiment classifiers and their application to political forecasting.</prevsent>
</prevsection>
<citsent citstr=" P11-2099 ">
interest in measuring the political mood of country has recently grown (oconnor et al  2010; tumasjan et al  2010; gonzalez-bailon et al ., 2010; carvalho et al  2011; <papid> P11-2099 </papid>tan et al  2011).here we compare our sentiment results to presidential job approval polls and show that the sentiment scores produced by our system are positively correlated with both the approval and disapproval job ratings.in this paper we present method for coupling two distantly supervised algorithms for topic identification and sentiment classification on twitter.</citsent>
<aftsection>
<nextsent>in section 4, we describe our approach to topic identification and present new annotated corpus of political tweets for future study.
</nextsent>
<nextsent>in section 5, we apply distant supervision to sentimentanalysis.
</nextsent>
<nextsent>finally, section 6 discusses our systems performance on modeling presidential job approval ratings from twitter data.
</nextsent>
<nextsent>the past several years have seen sentiment analysis grow into diverse research area.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I964">
<title id=" E12-1062.xml">learning for microblogs with distant supervision political forecasting with twitter </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>this approach tends to be used by applications that measure the general mood of population.
</prevsent>
<prevsent>oconnor et al (2010) use ratio of positive and negative word counts on twitter, kramer (2010) counts lexicon words on face book, and thelwall (2011) uses the publicly available sen tistrength algorithm to make weighted counts of keywords based on predefined polarity strengths.in contrast to lexicons, many approaches instead focus on ways to train supervised classifiers.
</prevsent>
</prevsection>
<citsent citstr=" P11-1016 ">
however, labeled data is expensive to create, and examples of twitter classifiers trained onhand-labeled data are few (jiang et al  2011).<papid> P11-1016 </papid></citsent>
<aftsection>
<nextsent>instead, distant supervision has grown in popularity.
</nextsent>
<nextsent>these algorithms use emoticons to serve as semantic indicators for sentiment.
</nextsent>
<nextsent>for instance, sad face (e.g., :-() serves as noisy label for anegative mood.
</nextsent>
<nextsent>read (2005) <papid> P05-2008 </papid>was the first to suggest emoticons for usenet data, followed by go et al (go et al  2009) on twitter, and many others since (bifet and frank, 2010; pak and paroubek, 2010; davidov et al  2010; <papid> C10-2028 </papid>kouloumpis et al  2011).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I965">
<title id=" E12-1062.xml">learning for microblogs with distant supervision political forecasting with twitter </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>these algorithms use emoticons to serve as semantic indicators for sentiment.
</prevsent>
<prevsent>for instance, sad face (e.g., :-() serves as noisy label for anegative mood.
</prevsent>
</prevsection>
<citsent citstr=" P05-2008 ">
read (2005) <papid> P05-2008 </papid>was the first to suggest emoticons for usenet data, followed by go et al (go et al  2009) on twitter, and many others since (bifet and frank, 2010; pak and paroubek, 2010; davidov et al  2010; <papid> C10-2028 </papid>kouloumpis et al  2011).</citsent>
<aftsection>
<nextsent>hash tags (e.g., #cool and #happy) have also been used as noisy sentiment labels (davidov et al  2010; <papid> C10-2028 </papid>kouloumpis et al  2011).</nextsent>
<nextsent>finally, multiple models can be blended into single classifier (barbosa and feng, 2010).<papid> C10-2005 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I972">
<title id=" E12-1062.xml">learning for microblogs with distant supervision political forecasting with twitter </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>read (2005) <papid> P05-2008 </papid>was the first to suggest emoticons for usenet data, followed by go et al (go et al  2009) on twitter, and many others since (bifet and frank, 2010; pak and paroubek, 2010; davidov et al  2010; <papid> C10-2028 </papid>kouloumpis et al  2011).</prevsent>
<prevsent>hash tags (e.g., #cool and #happy) have also been used as noisy sentiment labels (davidov et al  2010; <papid> C10-2028 </papid>kouloumpis et al  2011).</prevsent>
</prevsection>
<citsent citstr=" C10-2005 ">
finally, multiple models can be blended into single classifier (barbosa and feng, 2010).<papid> C10-2005 </papid></citsent>
<aftsection>
<nextsent>here, weadopt the emoticon algorithm for sentiment analysis, and evaluate it on specific domain (politics).
</nextsent>
<nextsent>topic identification in twitter has received much less attention than sentiment analysis.
</nextsent>
<nextsent>the majority of approaches simply select single keyword (e.g., obama?)
</nextsent>
<nextsent>to represent their topic (e.g., us president?)
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I979">
<title id=" E06-3002.xml">what humour tells us about discourse theories </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>computational theories such as the general theory of verbal humour (attardo and raskin, 1991) have avoided this difficult problem by adopting extra-linguistic knowledge in theform of scripts, which encode different oppositions that may arise in jokes.
</prevsent>
<prevsent>others (minsky,1986) posit general mechanism without considering specifics.
</prevsent>
</prevsection>
<citsent citstr=" H05-1067 ">
other models in computation have attempted to generate jokes using templates (attardo and raskin, 1994; binsted and ritchie, 1997) or recognize jokes using machine learning models (mihalcea and strapparava, 2005).<papid> H05-1067 </papid></citsent>
<aftsection>
<nextsent>computationally, the fact that other less likely interpretations such as p2 are not visible initially, may also result in considerably efficiency in more common situations, where ambiguities are not generated to begin with.
</nextsent>
<nextsent>for example, in joke (1) the interpretation after reading the first clause, has the word miss referring to the abstract act of missing dear person.
</nextsent>
<nextsent>after hearing the punchline, somewhere around the word aim, (the trigger point tp ), we have to revise our interpretation to one where miss is used in physical sense, as in shooting target.
</nextsent>
<nextsent>then, the forbidden idea of hurting ex-wives generates the humour.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I980">
<title id=" E06-3002.xml">what humour tells us about discourse theories </title>
<section> models of discourse.  </section>
<citcontext>
<prevsection>
<prevsent>(hobbs, 1985) introduces an early theory of discourse and the notion of coherence relations, which are applied recursively on discourse segments.
</prevsent>
<prevsent>coherence relations, such as elaboration, explanation and contrast, are relations between discourse units that bind segments of text into one global structure.
</prevsent>
</prevsection>
<citsent citstr=" J86-3001 ">
(grosz and sidner, 1986) <papid> J86-3001 </papid>incorporates two more important notions into its model - the idea of intention and fo cus.</citsent>
<aftsection>
<nextsent>the rhetorical structure theory, introduced in (mann and thompson, 1987), binds text spans with rhetorical relations, which are discourse connectives similar to coherence relations.
</nextsent>
<nextsent>the discourse representation theory (drt) (kamp, 1984) computes inter-sentential anaphora and attempts to maintain text cohesion through sets of predicates, termed discourse representation structures (drss), that represent discourse 32 no one does he can still walk by himself explanation who supports gorbachev?
</nextsent>
<nextsent>question-answer pair figure 3: rhetorical relations for joke (3) units.
</nextsent>
<nextsent>a principal drs accumulates information contained in the text, and forms the basis for resolving anaphora and discourse referents.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I981">
<title id=" E12-1027.xml">aspect ual type and temporal relation classification </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>extracting the temporal information present in atext is relevant to many natural language processing applications, including question-answering,information extraction, and even document summarization, as summaries may be more readable if they follow chronological order.
</prevsent>
<prevsent>recent evaluation campaigns have focused onthe extraction of temporal information from written text.
</prevsent>
</prevsection>
<citsent citstr=" W07-2014 ">
tempe val (verhagen et al 2007), <papid> W07-2014 </papid>in 2007, and more recently tempeval-2 (verhagen et al 2010), <papid> S10-1010 </papid>in 2010, were concerned with this problem.</citsent>
<aftsection>
<nextsent>additionally, they provided data that can be used to develop and evaluate systems that can automatically temporally tag natural language text.
</nextsent>
<nextsent>these data are annotated according to the timeml (pustejovsky et al 2003) scheme.figure 1 shows small and slightly simplified fragment of the data from tempe val, with timeml annotations.
</nextsent>
<nextsent>there, event terms, such as the term referring to the event of releasing the tapes, are annotated using event tags.
</nextsent>
<nextsent>states (such as the situations denoted by verbs like wantor love) are also considered events.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I982">
<title id=" E12-1027.xml">aspect ual type and temporal relation classification </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>extracting the temporal information present in atext is relevant to many natural language processing applications, including question-answering,information extraction, and even document summarization, as summaries may be more readable if they follow chronological order.
</prevsent>
<prevsent>recent evaluation campaigns have focused onthe extraction of temporal information from written text.
</prevsent>
</prevsection>
<citsent citstr=" S10-1010 ">
tempe val (verhagen et al 2007), <papid> W07-2014 </papid>in 2007, and more recently tempeval-2 (verhagen et al 2010), <papid> S10-1010 </papid>in 2010, were concerned with this problem.</citsent>
<aftsection>
<nextsent>additionally, they provided data that can be used to develop and evaluate systems that can automatically temporally tag natural language text.
</nextsent>
<nextsent>these data are annotated according to the timeml (pustejovsky et al 2003) scheme.figure 1 shows small and slightly simplified fragment of the data from tempe val, with timeml annotations.
</nextsent>
<nextsent>there, event terms, such as the term referring to the event of releasing the tapes, are annotated using event tags.
</nextsent>
<nextsent>states (such as the situations denoted by verbs like wantor love) are also considered events.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I986">
<title id=" E12-1027.xml">aspect ual type and temporal relation classification </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the possible values for the type of temporal relation are before, after and overlap.1 table 1 shows the results of the first tempe val evaluation.
</prevsent>
<prevsent>the results of tempeval-2 are fairly similar (verhagen et al 2010), <papid> S10-1010 </papid>but the data used are similar but not identical.</prevsent>
</prevsection>
<citsent citstr=" W07-2108 ">
the best system in tempe val for tasks and b(puscasu, 2007) <papid> W07-2108 </papid>combined statistical and knowledge based methods to propagate temporal constraints along parse trees coming from syntactic parser.</citsent>
<aftsection>
<nextsent>the best system for task (min et 1there are the additional disjunctive values before-or-overlap, overlap-or-after and vague, employed when the annotators could not make more specific decision, but these affect small number of instances.
</nextsent>
<nextsent>al., 2007) also combined rule-based and machine learning approaches.
</nextsent>
<nextsent>it employed sophisticated nlp to compute some of the features used; more specifically it used syntactic features.our goal with this work is to evaluate the impact of information about aspect ual type on thesetasks.
</nextsent>
<nextsent>the timeml annotations include an attribute class for events that encodes some aspect ual information, distinguishing between stative (annotated with the value state) and nonstative events (value occurrence).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I987">
<title id=" E12-1027.xml">aspect ual type and temporal relation classification </title>
<section> aspect ual type.  </section>
<citcontext>
<prevsection>
<prevsent>states are thus more likely to temporally overlap other temporal entities than culminations, for instance.
</prevsent>
<prevsent>second, there are grammatical consequences on how events are anchored in time.
</prevsent>
</prevsection>
<citsent citstr=" J88-2003 ">
consider the following examples, from ritchie (1979) and moens and steedman (1988): (<papid> J88-2003 </papid>1) when they built the 59th street bridge, they used the best materials.</citsent>
<aftsection>
<nextsent>(2) when they built that bridge, was still young lad.the situation of building the bridge is culminated processed, composed by the process of actively building bridge followed by the culmination of the bridge being finished.
</nextsent>
<nextsent>in sentence (1), the event described in the main clause (that ofusing the best materials) is process, but in sentence (2) it is state (the state of being younglad).
</nextsent>
<nextsent>even though the two clauses in each sentence are connected by when, the temporal relations holding between the events of each clause are different.
</nextsent>
<nextsent>on the one hand, in sentence (1) the event of using the best materials (a process) overlaps with the process of actively building the bridge and precedes the culmination of finishing the bridge.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I989">
<title id=" E12-1027.xml">aspect ual type and temporal relation classification </title>
<section> strategy.  </section>
<citcontext>
<prevsection>
<prevsent>in any case, it has the advantages of being based on very large amount of data and not requiring any manual annotation, which can introduce errors.
</prevsent>
<prevsent>3.1 the web as very large corpus.
</prevsent>
</prevsection>
<citsent citstr=" C92-2082 ">
hearst (1992) <papid> C92-2082 </papid>is one of the earliest studies where specific textual patterns are used to extract lexico semantic information from very large corpora.the authors goal was to extract hyponymy rela tions.</citsent>
<aftsection>
<nextsent>with the same goal, kozareva et al(2008) <papid> P08-1119 </papid>apply similar textual patterns to the web.</nextsent>
<nextsent>the web has been used as corpus by many other authors with the purpose of extracting syntactic or semantic properties of words or relations between them, e.g. ravichandran and hovy (2002), <papid> P02-1006 </papid>etzioni et al(2004), etc. some of this work is specially relevant to the problem of temporal information processing.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I990">
<title id=" E12-1027.xml">aspect ual type and temporal relation classification </title>
<section> strategy.  </section>
<citcontext>
<prevsection>
<prevsent>3.1 the web as very large corpus.
</prevsent>
<prevsent>hearst (1992) <papid> C92-2082 </papid>is one of the earliest studies where specific textual patterns are used to extract lexico semantic information from very large corpora.the authors goal was to extract hyponymy rela tions.</prevsent>
</prevsection>
<citsent citstr=" P08-1119 ">
with the same goal, kozareva et al(2008) <papid> P08-1119 </papid>apply similar textual patterns to the web.</citsent>
<aftsection>
<nextsent>the web has been used as corpus by many other authors with the purpose of extracting syntactic or semantic properties of words or relations between them, e.g. ravichandran and hovy (2002), <papid> P02-1006 </papid>etzioni et al(2004), etc. some of this work is specially relevant to the problem of temporal information processing.</nextsent>
<nextsent>verb ocean (chklovski and pantel, 2004) <papid> W04-3205 </papid>is database of web mined relations between verbs.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I991">
<title id=" E12-1027.xml">aspect ual type and temporal relation classification </title>
<section> strategy.  </section>
<citcontext>
<prevsection>
<prevsent>hearst (1992) <papid> C92-2082 </papid>is one of the earliest studies where specific textual patterns are used to extract lexico semantic information from very large corpora.the authors goal was to extract hyponymy rela tions.</prevsent>
<prevsent>with the same goal, kozareva et al(2008) <papid> P08-1119 </papid>apply similar textual patterns to the web.</prevsent>
</prevsection>
<citsent citstr=" P02-1006 ">
the web has been used as corpus by many other authors with the purpose of extracting syntactic or semantic properties of words or relations between them, e.g. ravichandran and hovy (2002), <papid> P02-1006 </papid>etzioni et al(2004), etc. some of this work is specially relevant to the problem of temporal information processing.</citsent>
<aftsection>
<nextsent>verb ocean (chklovski and pantel, 2004) <papid> W04-3205 </papid>is database of web mined relations between verbs.</nextsent>
<nextsent>among other kinds of relations, it includes typical precedence relations, e.g. sleeping happens before waking up.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I992">
<title id=" E12-1027.xml">aspect ual type and temporal relation classification </title>
<section> strategy.  </section>
<citcontext>
<prevsection>
<prevsent>with the same goal, kozareva et al(2008) <papid> P08-1119 </papid>apply similar textual patterns to the web.</prevsent>
<prevsent>the web has been used as corpus by many other authors with the purpose of extracting syntactic or semantic properties of words or relations between them, e.g. ravichandran and hovy (2002), <papid> P02-1006 </papid>etzioni et al(2004), etc. some of this work is specially relevant to the problem of temporal information processing.</prevsent>
</prevsection>
<citsent citstr=" W04-3205 ">
verb ocean (chklovski and pantel, 2004) <papid> W04-3205 </papid>is database of web mined relations between verbs.</citsent>
<aftsection>
<nextsent>among other kinds of relations, it includes typical precedence relations, e.g. sleeping happens before waking up.
</nextsent>
<nextsent>this type of information has in fact been used by some of the participating systems of tempeval-2 (ha et al 2010), <papid> S10-1076 </papid>with good results.</nextsent>
<nextsent>more generally, there is large body of work focusing on lexical acquisition from corpora.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I993">
<title id=" E12-1027.xml">aspect ual type and temporal relation classification </title>
<section> strategy.  </section>
<citcontext>
<prevsection>
<prevsent>verb ocean (chklovski and pantel, 2004) <papid> W04-3205 </papid>is database of web mined relations between verbs.</prevsent>
<prevsent>among other kinds of relations, it includes typical precedence relations, e.g. sleeping happens before waking up.</prevsent>
</prevsection>
<citsent citstr=" S10-1076 ">
this type of information has in fact been used by some of the participating systems of tempeval-2 (ha et al 2010), <papid> S10-1076 </papid>with good results.</citsent>
<aftsection>
<nextsent>more generally, there is large body of work focusing on lexical acquisition from corpora.
</nextsent>
<nextsent>justas an example, mayol et al(2005) learn subcategorization frames of verbs from large amounts of data.
</nextsent>
<nextsent>relevant to our work is that of siegel and mckeown (2000).<papid> J00-4004 </papid></nextsent>
<nextsent>the authors guess the aspect ual type of verbs by searching for specific patterns in one million word corpus that has been syntactically parsed.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I994">
<title id=" E12-1027.xml">aspect ual type and temporal relation classification </title>
<section> strategy.  </section>
<citcontext>
<prevsection>
<prevsent>more generally, there is large body of work focusing on lexical acquisition from corpora.
</prevsent>
<prevsent>justas an example, mayol et al(2005) learn subcategorization frames of verbs from large amounts of data.
</prevsent>
</prevsection>
<citsent citstr=" J00-4004 ">
relevant to our work is that of siegel and mckeown (2000).<papid> J00-4004 </papid></citsent>
<aftsection>
<nextsent>the authors guess the aspect ual type of verbs by searching for specific patterns in one million word corpus that has been syntactically parsed.
</nextsent>
<nextsent>they extract several linguistic indicators and combine them with machine learning algorithms.
</nextsent>
<nextsent>the indicators that they extract are naturally different from ours, since they have access to syntactic structure and we do not, but our data are based on much larger corpus.
</nextsent>
<nextsent>3.2 textual patterns as indicators of.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I995">
<title id=" E12-1027.xml">aspect ual type and temporal relation classification </title>
<section> scope and approach.  </section>
<citcontext>
<prevsection>
<prevsent>we extracted the 4,000 most common verbs froma 180 million word corpus of portuguese news paper text, cetempublico.
</prevsent>
<prevsent>because this corp usis not annotated, we used part-of-speech tagger and morphological analyzer (barreto et al 2006; silva, 2007) to detect verbs and to obtain their dictionary form.
</prevsent>
</prevsection>
<citsent citstr=" P09-4002 ">
we then used an inflection tool (branco et al 2009) <papid> P09-4002 </papid>to generate the specific verb forms that are used in the queries.</citsent>
<aftsection>
<nextsent>they are mostly third person singular forms of several different tenses.
</nextsent>
<nextsent>the indicators that we used are ratios of google hits.
</nextsent>
<nextsent>they compare two queries.several indicators were tested.
</nextsent>
<nextsent>we provide examples with the verb fazer do?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I996">
<title id=" E12-1027.xml">aspect ual type and temporal relation classification </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>5.1 experimental setup.
</prevsent>
<prevsent>in order to obtain bases for comparison, we trained machine learned classifiers on the portuguese corpus timebankpt, that is adapted from the tempe val data (see section 4.1).
</prevsent>
</prevsection>
<citsent citstr=" W07-2098 ">
we took inspiration in the work of hepple et al(2007).<papid> W07-2098 </papid></citsent>
<aftsection>
<nextsent>271 this was one of the participating systems of tempeval.
</nextsent>
<nextsent>it used machine learning algorithms implemented in weka (witten and frank, 1999).for our experiments, we used wekas implementation of the c4.5 algorithm, trees.j48 (quin lan, 1993), the ripper algorithm as implemented by wekas rules.jrip (cohen, 1995), nearest neighbors classifier, lazy.kstar (cleary and trigg, 1995), nave bayes classifier, namelywekas bayes.naivebayes (john and langley, 1995), and support vector classifier, wekas functions.smo (platt, 1998) . we chose these algorithms as they are representative of wide range of machine learning approaches.
</nextsent>
<nextsent>recall that the tasks of tempe val are to guess the type of temporal relations.
</nextsent>
<nextsent>each train or test instance thus corresponds to temporal relation,i.e. tlink element in the timeml annotations (see figures 1 and 2).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1002">
<title id=" E09-1060.xml">correcting a pos tagged corpus using three complementary methods </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>furthermore, we re-evaluate existing state-of the-art pos taggers on icelandic text using the corrected corpus.
</prevsent>
<prevsent>part-of-speech (pos) tagged corpora are valuable resources for developing pos taggers, i.e. programs which automatically tag each word in running text with morphosyntactic information.
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
corpora in various languages, such as the english penn treebank corpus (marcus et al, 1993), <papid> J93-2004 </papid>the swedish stockholm-ume?</citsent>
<aftsection>
<nextsent>corpus (ejerhed et al, 1992), and the icelandic frequency dictionary (ifd) corpus (pind et al, 1991), have been used to train (in the case of data-driven methods) and develop (in the case of linguistic rule-based methods) different taggers, and to evaluate their accuracy, e.g.
</nextsent>
<nextsent>(van halteren et al, 2001; megyesi, 2001; loftsson, 2006).
</nextsent>
<nextsent>consequently, the quality of the pos annotation in corpus (the gold standard annotation) is crucial.many corpora are annotated semi automatically.
</nextsent>
<nextsent>first, pos tagger is run on the corpus text, and, then, the text is hand-corrected by humans.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1003">
<title id=" E09-1060.xml">correcting a pos tagged corpus using three complementary methods </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>thus, it is important to apply known methods and/or develop new methods for automatically detecting tagging errors in corpora.
</prevsent>
<prevsent>once an error has been detected it can be corrected by humans or an automatic method.in this paper, we experiment with three different methods of pos error detection using the ifd corpus.
</prevsent>
</prevsection>
<citsent citstr=" E03-1068 ">
first, we use the variation n-gram method proposed by dickinson and meurers (2003).<papid> E03-1068 </papid></citsent>
<aftsection>
<nextsent>secondly, we run five different taggers on the corpus and examine those cases where all the taggers agree on tag, but, at the same time, disagree with the gold standard annotation.
</nextsent>
<nextsent>lastly, we use ice parser (loftsson and rgnvaldsson, 2007) to generate shallow parses of sentences in the corpus and then develop various patterns, based on feature agreement, for finding candidates for annotation errors.
</nextsent>
<nextsent>once error candidates have been detected byeach method, we examine the candidates manually and correct the errors.
</nextsent>
<nextsent>overall, based on these methods, we hand-correct the pos tagging of 1,334 tokens or 0.23% of the tokens in the ifdcorpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1006">
<title id=" E09-1060.xml">correcting a pos tagged corpus using three complementary methods </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>this resulted in 4417 tag corrections, i.e. about 0.34% of the tokens in the whole corpus were found to be incorrectly tagged2.
</prevsent>
<prevsent>intuitively, the variation n-gram method is most suitable for corpora containing specific genres, e.g. business news like the wsj, or very large balanced corpora, because in both types of corpora one can expect the length of the variations to be quite large.
</prevsent>
</prevsection>
<citsent citstr=" C02-1101 ">
furthermore, this method may notbe suitable for corpora tagged with large fine grained tagset, because in such cases large ratio of the variation n-grams may actually reflect true ambiguity rather than inconsistent tagging.another example of method, based on finding inconsistent tagging of word across comparable occurrences, is the one by nakagawa and matsumoto (2002).<papid> C02-1101 </papid></citsent>
<aftsection>
<nextsent>they use support vector machines (svms) to find elements in corpus that violate consistency.
</nextsent>
<nextsent>the svms assign weight to each training example in corpus ? large weight is assigned to examples that are hard for the svmsto classify.
</nextsent>
<nextsent>the hard examples are thus candidates for errors in the corpus.
</nextsent>
<nextsent>the result was remarkable 99.5% precision when examples from the wsj corpus were extracted with large weight greater than or equal to threshold value.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1007">
<title id=" E09-1060.xml">correcting a pos tagged corpus using three complementary methods </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the main problem with this approach is that is presupposes set of invalid bigrams (e.g. constructed by linguist).
</prevsent>
<prevsent>for large tagset, for example the icelandic one (see section 2.2), constructing this set is very hardtask.
</prevsent>
</prevsection>
<citsent citstr=" C08-1026 ">
moreover, this method fails to detect annotation errors where particular n-gram tag sequence 2in more recent work, dickinson (2008) <papid> C08-1026 </papid>has develop eda method for increasing the recall (the ratio of correctly detected errors to all errors in the corpus).</citsent>
<aftsection>
<nextsent>524 is valid but erroneous in the given context.pos taggers have also been used to point to possible errors in corpora.
</nextsent>
<nextsent>if the output of tagger does not agree with the gold standard then either the tagger is incorrect or the gold standard is incorrectly annotated.
</nextsent>
<nextsent>a human can then look at the disagreements and correct the gold standard where necessary.
</nextsent>
<nextsent>van halteren (2000) trained tagger on the written texts of the british national corpus sampler cd (about 1 million words).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1008">
<title id=" E09-1060.xml">correcting a pos tagged corpus using three complementary methods </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the large tagset mirrors the morphological complexity of the icelandic language.
</prevsent>
<prevsent>this, inturn, is the main reason for relatively low tagging accuracy obtained by pos taggers on icelandic text, so far.
</prevsent>
</prevsection>
<citsent citstr=" P08-2009 ">
the state-of-the art tagging accuracy, measured against the ifd corpus, is 92.06%, obtained by applying bidirectional pos tagging method (dredze and wallenberg, 2008).<papid> P08-2009 </papid></citsent>
<aftsection>
<nextsent>we have developed linguistic rule-based tagger,icetagger, achieving about 91.6% tagging accuracy (loftsson, 2008).
</nextsent>
<nextsent>evaluation has shown that the well known statistical tagger, tnt (brants,2000), <papid> A00-1031 </papid>obtains about 90.4% accuracy (helgadt tir, 2005; loftsson, 2008).</nextsent>
<nextsent>finally, an accuracy of about 93.5% has been achieved by using tagger combination method using five taggers (loftsson, 2006).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1009">
<title id=" E09-1060.xml">correcting a pos tagged corpus using three complementary methods </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the state-of-the art tagging accuracy, measured against the ifd corpus, is 92.06%, obtained by applying bidirectional pos tagging method (dredze and wallenberg, 2008).<papid> P08-2009 </papid></prevsent>
<prevsent>we have developed linguistic rule-based tagger,icetagger, achieving about 91.6% tagging accuracy (loftsson, 2008).</prevsent>
</prevsection>
<citsent citstr=" A00-1031 ">
evaluation has shown that the well known statistical tagger, tnt (brants,2000), <papid> A00-1031 </papid>obtains about 90.4% accuracy (helgadt tir, 2005; loftsson, 2008).</citsent>
<aftsection>
<nextsent>finally, an accuracy of about 93.5% has been achieved by using tagger combination method using five taggers (loftsson, 2006).
</nextsent>
<nextsent>in this section, we describe the three methods we used to detect (and correct) annotation errors in the ifd corpus.
</nextsent>
<nextsent>each method returns set of error candidates, which we then manually inspect and correct the corresponding tag if necessary.
</nextsent>
<nextsent>3.1 variation n-grams.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1010">
<title id=" E09-1060.xml">correcting a pos tagged corpus using three complementary methods </title>
<section> three methods for error detection.  </section>
<citcontext>
<prevsection>
<prevsent>indeed, we have previously shown that when five taggers all agree on tag in the ifd corpus, the corresponding accuracy is 98.9% (lofts son, 2007b).
</prevsent>
<prevsent>for the remaining 1.1% tokens, one would expect that the five taggers are actually correct in some of the cases, but the gold standard incorrectly annotated.
</prevsent>
</prevsection>
<citsent citstr=" W96-0102 ">
in general, both the precision and the recall should be higher when relying on five agreeing taggers as compared to using only single tagger.thus, we used the five taggers, mbl (daele mans et al, 1996), <papid> W96-0102 </papid>mxpost (ratnaparkhi, 1996),<papid> W96-0213 </papid>fntbl (ngai and florian, 2001), tnt, and icetagger3, in the same manner as described in (lofts son, 2006), but with the following minor changes.</citsent>
<aftsection>
<nextsent>we extended the dictionaries of the tnt tagger and ice tagger by using data from full-form morphological database of inflections (bjarnadttir, 2005).
</nextsent>
<nextsent>the accuracy of the two taggers increases substantially (because the ratio of unknown words drops dramatically) and, in turn, the corresponding accuracy when all the taggers agree increases from 98.9% to 99.1%.
</nextsent>
<nextsent>therefore, we only needed to inspect about 0.9% of the tokens in the corpus.the following example from the ifd corpus shows disagreement found between the five taggers and the gold standard: fjlskylda spkonunnar ? gamla hsinu?
</nextsent>
<nextsent>(family (the) fortune-tellers in (the) old house?).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1011">
<title id=" E09-1060.xml">correcting a pos tagged corpus using three complementary methods </title>
<section> three methods for error detection.  </section>
<citcontext>
<prevsection>
<prevsent>indeed, we have previously shown that when five taggers all agree on tag in the ifd corpus, the corresponding accuracy is 98.9% (lofts son, 2007b).
</prevsent>
<prevsent>for the remaining 1.1% tokens, one would expect that the five taggers are actually correct in some of the cases, but the gold standard incorrectly annotated.
</prevsent>
</prevsection>
<citsent citstr=" W96-0213 ">
in general, both the precision and the recall should be higher when relying on five agreeing taggers as compared to using only single tagger.thus, we used the five taggers, mbl (daele mans et al, 1996), <papid> W96-0102 </papid>mxpost (ratnaparkhi, 1996),<papid> W96-0213 </papid>fntbl (ngai and florian, 2001), tnt, and icetagger3, in the same manner as described in (lofts son, 2006), but with the following minor changes.</citsent>
<aftsection>
<nextsent>we extended the dictionaries of the tnt tagger and ice tagger by using data from full-form morphological database of inflections (bjarnadttir, 2005).
</nextsent>
<nextsent>the accuracy of the two taggers increases substantially (because the ratio of unknown words drops dramatically) and, in turn, the corresponding accuracy when all the taggers agree increases from 98.9% to 99.1%.
</nextsent>
<nextsent>therefore, we only needed to inspect about 0.9% of the tokens in the corpus.the following example from the ifd corpus shows disagreement found between the five taggers and the gold standard: fjlskylda spkonunnar ? gamla hsinu?
</nextsent>
<nextsent>(family (the) fortune-tellers in (the) old house?).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1014">
<title id=" E12-1003.xml">a bayesian approach to unsupervised semantic role induction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these distances are automatically induced within the model and shared across predicates.
</prevsent>
<prevsent>both models achieve state-of-the-art results when evaluated on propbank, with the coupled model consistently outperforming the factored counterpart in all experimental set-ups.
</prevsent>
</prevsection>
<citsent citstr=" J02-3001 ">
semantic role labeling (srl) (gildea and jurafsky, 2002), <papid> J02-3001 </papid>shallow semantic parsing task, has recently attracted lot of attention in the computational linguistic community (carreras and ma`rquez, 2005; surdeanu et al  2008; <papid> W08-2121 </papid>hajic?</citsent>
<aftsection>
<nextsent>etal., 2009).
</nextsent>
<nextsent>the task involves prediction of predicate argument structure, i.e. both identification of arguments as well as assignment of labels according to their underlying semantic role.
</nextsent>
<nextsent>for example, in the following sentences: (a) [a0 mary] opened [a1 the door].
</nextsent>
<nextsent>(b) [a0 mary] is expected to open [a1 the door].
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1015">
<title id=" E12-1003.xml">a bayesian approach to unsupervised semantic role induction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these distances are automatically induced within the model and shared across predicates.
</prevsent>
<prevsent>both models achieve state-of-the-art results when evaluated on propbank, with the coupled model consistently outperforming the factored counterpart in all experimental set-ups.
</prevsent>
</prevsection>
<citsent citstr=" W08-2121 ">
semantic role labeling (srl) (gildea and jurafsky, 2002), <papid> J02-3001 </papid>shallow semantic parsing task, has recently attracted lot of attention in the computational linguistic community (carreras and ma`rquez, 2005; surdeanu et al  2008; <papid> W08-2121 </papid>hajic?</citsent>
<aftsection>
<nextsent>etal., 2009).
</nextsent>
<nextsent>the task involves prediction of predicate argument structure, i.e. both identification of arguments as well as assignment of labels according to their underlying semantic role.
</nextsent>
<nextsent>for example, in the following sentences: (a) [a0 mary] opened [a1 the door].
</nextsent>
<nextsent>(b) [a0 mary] is expected to open [a1 the door].
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1016">
<title id=" E12-1003.xml">a bayesian approach to unsupervised semantic role induction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(b) [a0 mary] is expected to open [a1 the door].
</prevsent>
<prevsent>(c) [a1 the door] opened.
</prevsent>
</prevsection>
<citsent citstr=" D07-1002 ">
(d) [a1 the door] was opened [a0 by mary].mary always takes an agent role (a0) for the predicate open, and door is always patient (a1).srl representations have many potential applications in natural language processing and have recently been shown to be beneficial in question answering (shen and lapata, 2007; <papid> D07-1002 </papid>kaisser and webber, 2007), <papid> W07-1206 </papid>textual entailment (sammons et al ., 2009), machine translation (wu and fung, 2009; <papid> N09-2004 </papid>liu and gildea, 2010; <papid> C10-1081 </papid>wu et al  2011; gao and vogel, 2011), <papid> P11-2051 </papid>and dialogue systems (basili et al ., 2009; vander plas et al  2011), among others.though syntactic representations are often predictive of semantic roles (levin, 1993), the interface between syntactic and semantic representations isfar from trivial.</citsent>
<aftsection>
<nextsent>the lack of simple deterministic rules for mapping syntax to shallow semantics motivates the use of statistical methods.
</nextsent>
<nextsent>although current statistical approaches have been successful in predicting shallow semantic representations, they typically require large amounts of annotated data to estimate model parameters.
</nextsent>
<nextsent>these resources are scarce and expensive to create, and even the largest of them have low coverage (palmer and sporleder, 2010).<papid> C10-2107 </papid></nextsent>
<nextsent>moreover, these models are domain-specific, and their performance drops substantially when they are used in new domain (pradhan et al  2008).<papid> J08-2006 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1017">
<title id=" E12-1003.xml">a bayesian approach to unsupervised semantic role induction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(b) [a0 mary] is expected to open [a1 the door].
</prevsent>
<prevsent>(c) [a1 the door] opened.
</prevsent>
</prevsection>
<citsent citstr=" W07-1206 ">
(d) [a1 the door] was opened [a0 by mary].mary always takes an agent role (a0) for the predicate open, and door is always patient (a1).srl representations have many potential applications in natural language processing and have recently been shown to be beneficial in question answering (shen and lapata, 2007; <papid> D07-1002 </papid>kaisser and webber, 2007), <papid> W07-1206 </papid>textual entailment (sammons et al ., 2009), machine translation (wu and fung, 2009; <papid> N09-2004 </papid>liu and gildea, 2010; <papid> C10-1081 </papid>wu et al  2011; gao and vogel, 2011), <papid> P11-2051 </papid>and dialogue systems (basili et al ., 2009; vander plas et al  2011), among others.though syntactic representations are often predictive of semantic roles (levin, 1993), the interface between syntactic and semantic representations isfar from trivial.</citsent>
<aftsection>
<nextsent>the lack of simple deterministic rules for mapping syntax to shallow semantics motivates the use of statistical methods.
</nextsent>
<nextsent>although current statistical approaches have been successful in predicting shallow semantic representations, they typically require large amounts of annotated data to estimate model parameters.
</nextsent>
<nextsent>these resources are scarce and expensive to create, and even the largest of them have low coverage (palmer and sporleder, 2010).<papid> C10-2107 </papid></nextsent>
<nextsent>moreover, these models are domain-specific, and their performance drops substantially when they are used in new domain (pradhan et al  2008).<papid> J08-2006 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1018">
<title id=" E12-1003.xml">a bayesian approach to unsupervised semantic role induction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(b) [a0 mary] is expected to open [a1 the door].
</prevsent>
<prevsent>(c) [a1 the door] opened.
</prevsent>
</prevsection>
<citsent citstr=" N09-2004 ">
(d) [a1 the door] was opened [a0 by mary].mary always takes an agent role (a0) for the predicate open, and door is always patient (a1).srl representations have many potential applications in natural language processing and have recently been shown to be beneficial in question answering (shen and lapata, 2007; <papid> D07-1002 </papid>kaisser and webber, 2007), <papid> W07-1206 </papid>textual entailment (sammons et al ., 2009), machine translation (wu and fung, 2009; <papid> N09-2004 </papid>liu and gildea, 2010; <papid> C10-1081 </papid>wu et al  2011; gao and vogel, 2011), <papid> P11-2051 </papid>and dialogue systems (basili et al ., 2009; vander plas et al  2011), among others.though syntactic representations are often predictive of semantic roles (levin, 1993), the interface between syntactic and semantic representations isfar from trivial.</citsent>
<aftsection>
<nextsent>the lack of simple deterministic rules for mapping syntax to shallow semantics motivates the use of statistical methods.
</nextsent>
<nextsent>although current statistical approaches have been successful in predicting shallow semantic representations, they typically require large amounts of annotated data to estimate model parameters.
</nextsent>
<nextsent>these resources are scarce and expensive to create, and even the largest of them have low coverage (palmer and sporleder, 2010).<papid> C10-2107 </papid></nextsent>
<nextsent>moreover, these models are domain-specific, and their performance drops substantially when they are used in new domain (pradhan et al  2008).<papid> J08-2006 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1019">
<title id=" E12-1003.xml">a bayesian approach to unsupervised semantic role induction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(b) [a0 mary] is expected to open [a1 the door].
</prevsent>
<prevsent>(c) [a1 the door] opened.
</prevsent>
</prevsection>
<citsent citstr=" C10-1081 ">
(d) [a1 the door] was opened [a0 by mary].mary always takes an agent role (a0) for the predicate open, and door is always patient (a1).srl representations have many potential applications in natural language processing and have recently been shown to be beneficial in question answering (shen and lapata, 2007; <papid> D07-1002 </papid>kaisser and webber, 2007), <papid> W07-1206 </papid>textual entailment (sammons et al ., 2009), machine translation (wu and fung, 2009; <papid> N09-2004 </papid>liu and gildea, 2010; <papid> C10-1081 </papid>wu et al  2011; gao and vogel, 2011), <papid> P11-2051 </papid>and dialogue systems (basili et al ., 2009; vander plas et al  2011), among others.though syntactic representations are often predictive of semantic roles (levin, 1993), the interface between syntactic and semantic representations isfar from trivial.</citsent>
<aftsection>
<nextsent>the lack of simple deterministic rules for mapping syntax to shallow semantics motivates the use of statistical methods.
</nextsent>
<nextsent>although current statistical approaches have been successful in predicting shallow semantic representations, they typically require large amounts of annotated data to estimate model parameters.
</nextsent>
<nextsent>these resources are scarce and expensive to create, and even the largest of them have low coverage (palmer and sporleder, 2010).<papid> C10-2107 </papid></nextsent>
<nextsent>moreover, these models are domain-specific, and their performance drops substantially when they are used in new domain (pradhan et al  2008).<papid> J08-2006 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1020">
<title id=" E12-1003.xml">a bayesian approach to unsupervised semantic role induction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(b) [a0 mary] is expected to open [a1 the door].
</prevsent>
<prevsent>(c) [a1 the door] opened.
</prevsent>
</prevsection>
<citsent citstr=" P11-2051 ">
(d) [a1 the door] was opened [a0 by mary].mary always takes an agent role (a0) for the predicate open, and door is always patient (a1).srl representations have many potential applications in natural language processing and have recently been shown to be beneficial in question answering (shen and lapata, 2007; <papid> D07-1002 </papid>kaisser and webber, 2007), <papid> W07-1206 </papid>textual entailment (sammons et al ., 2009), machine translation (wu and fung, 2009; <papid> N09-2004 </papid>liu and gildea, 2010; <papid> C10-1081 </papid>wu et al  2011; gao and vogel, 2011), <papid> P11-2051 </papid>and dialogue systems (basili et al ., 2009; vander plas et al  2011), among others.though syntactic representations are often predictive of semantic roles (levin, 1993), the interface between syntactic and semantic representations isfar from trivial.</citsent>
<aftsection>
<nextsent>the lack of simple deterministic rules for mapping syntax to shallow semantics motivates the use of statistical methods.
</nextsent>
<nextsent>although current statistical approaches have been successful in predicting shallow semantic representations, they typically require large amounts of annotated data to estimate model parameters.
</nextsent>
<nextsent>these resources are scarce and expensive to create, and even the largest of them have low coverage (palmer and sporleder, 2010).<papid> C10-2107 </papid></nextsent>
<nextsent>moreover, these models are domain-specific, and their performance drops substantially when they are used in new domain (pradhan et al  2008).<papid> J08-2006 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1021">
<title id=" E12-1003.xml">a bayesian approach to unsupervised semantic role induction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the lack of simple deterministic rules for mapping syntax to shallow semantics motivates the use of statistical methods.
</prevsent>
<prevsent>although current statistical approaches have been successful in predicting shallow semantic representations, they typically require large amounts of annotated data to estimate model parameters.
</prevsent>
</prevsection>
<citsent citstr=" C10-2107 ">
these resources are scarce and expensive to create, and even the largest of them have low coverage (palmer and sporleder, 2010).<papid> C10-2107 </papid></citsent>
<aftsection>
<nextsent>moreover, these models are domain-specific, and their performance drops substantially when they are used in new domain (pradhan et al  2008).<papid> J08-2006 </papid></nextsent>
<nextsent>such domain specificity is arguably unavoidable for semantic analyzer, as even the definitions of semantic roles are typically predicate specific, and different domains can have radically different distributions of predicates (and their senses).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1022">
<title id=" E12-1003.xml">a bayesian approach to unsupervised semantic role induction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>although current statistical approaches have been successful in predicting shallow semantic representations, they typically require large amounts of annotated data to estimate model parameters.
</prevsent>
<prevsent>these resources are scarce and expensive to create, and even the largest of them have low coverage (palmer and sporleder, 2010).<papid> C10-2107 </papid></prevsent>
</prevsection>
<citsent citstr=" J08-2006 ">
moreover, these models are domain-specific, and their performance drops substantially when they are used in new domain (pradhan et al  2008).<papid> J08-2006 </papid></citsent>
<aftsection>
<nextsent>such domain specificity is arguably unavoidable for semantic analyzer, as even the definitions of semantic roles are typically predicate specific, and different domains can have radically different distributions of predicates (and their senses).
</nextsent>
<nextsent>the necessity for large amounts of human-annotated data for every language and domain is one of the major obstacles to the wide-spread adoption of semantic role representations.these challenges motivate the need for unsupervised methods which, instead of relying on labeled data, can exploit large amounts of unlabeledtexts.
</nextsent>
<nextsent>in this paper, we propose simple and effi 12 cient hierarchical bayesian models for this task.
</nextsent>
<nextsent>it is natural to split the srl task into twostages: the identification of arguments (the identification stage) and the assignment of semantic roles (the labeling stage).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1023">
<title id=" E12-1003.xml">a bayesian approach to unsupervised semantic role induction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>it is natural to split the srl task into twostages: the identification of arguments (the identification stage) and the assignment of semantic roles (the labeling stage).
</prevsent>
<prevsent>in this and in much of the previous work on unsupervised techniques, the focus is on the labeling stage.
</prevsent>
</prevsection>
<citsent citstr=" P11-1112 ">
identification, though an important problem, can be tackled with heuristics (lang and lapata, 2011<papid> P11-1112 </papid>a; grenager and manning, 2006) <papid> W06-1601 </papid>or, potentially, by using supervised classifier trained on small amount of data.</citsent>
<aftsection>
<nextsent>we follow (lang and lapata, 2011<papid> P11-1112 </papid>a), and regard the labeling stage as clustering of syntactic signatures of argument realizations for every predi cate.</nextsent>
<nextsent>in our first model, as in most of the previous work on unsupervised srl, we define an independent model for each predicate.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1045">
<title id=" E12-1003.xml">a bayesian approach to unsupervised semantic role induction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>it is natural to split the srl task into twostages: the identification of arguments (the identification stage) and the assignment of semantic roles (the labeling stage).
</prevsent>
<prevsent>in this and in much of the previous work on unsupervised techniques, the focus is on the labeling stage.
</prevsent>
</prevsection>
<citsent citstr=" W06-1601 ">
identification, though an important problem, can be tackled with heuristics (lang and lapata, 2011<papid> P11-1112 </papid>a; grenager and manning, 2006) <papid> W06-1601 </papid>or, potentially, by using supervised classifier trained on small amount of data.</citsent>
<aftsection>
<nextsent>we follow (lang and lapata, 2011<papid> P11-1112 </papid>a), and regard the labeling stage as clustering of syntactic signatures of argument realizations for every predi cate.</nextsent>
<nextsent>in our first model, as in most of the previous work on unsupervised srl, we define an independent model for each predicate.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1068">
<title id=" E12-1003.xml">a bayesian approach to unsupervised semantic role induction </title>
<section> task definition.  </section>
<citcontext>
<prevsection>
<prevsent>finally, additional related work is presented in section 8.
</prevsent>
<prevsent>in this work, instead of assuming the availability of role annotated data, we rely only on automatically generated syntactic dependency graphs.
</prevsent>
</prevsection>
<citsent citstr=" J05-1004 ">
while we cannot expect that syntactic structure can trivially map to semantic representation (palmer et al  2005)<papid> J05-1004 </papid>1, we can use syntactic cues to help us in both stages of unsupervised srl.</citsent>
<aftsection>
<nextsent>before defining our task, let us consider the two stages separately.in the argument identification stage, we implement heuristic proposed in (lang and lapata, 2011<papid> P11-1112 </papid>a) comprised of list of 8 rules, which usenonlexicalized properties of syntactic paths between predicate and candidate argument to it eratively discard non-arguments from the list of all words in sentence.</nextsent>
<nextsent>note that inducing these rules for new language would require some linguistic expertise.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1092">
<title id=" E12-1003.xml">a bayesian approach to unsupervised semantic role induction </title>
<section> task definition.  </section>
<citcontext>
<prevsection>
<prevsent>one alternative may be to annotate small number of arguments and train classifier with non lexicalized features instead.
</prevsent>
<prevsent>in the argument labeling stage, semantic roles are represented by clusters of arguments, and labeling particular argument corresponds to deciding on its role cluster.
</prevsent>
</prevsection>
<citsent citstr=" N10-1137 ">
however, instead of deal1although it provides strong baseline which is difficult to beat (grenager and manning, 2006; <papid> W06-1601 </papid>lang and lapata, 2010; <papid> N10-1137 </papid>lang and lapata, 2011<papid> P11-1112 </papid>a).</citsent>
<aftsection>
<nextsent>13ing with argument occurrences directly, we represent them as predicate specific syntactic signatures, and refer to them as argument keys.
</nextsent>
<nextsent>this representation aids our models in inducing high purity clusters (of argument keys) while reducing their granularity.
</nextsent>
<nextsent>we follow (lang and lapata, 2011<papid> P11-1112 </papid>a) and use the following syntactic features to form the argument key representation: ? active or passive verb voice (act/pass).</nextsent>
<nextsent>argument position relative to predicate (left/right).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1139">
<title id=" E12-1003.xml">a bayesian approach to unsupervised semantic role induction </title>
<section> factored model.  </section>
<citcontext>
<prevsection>
<prevsent>if 0 is drawn then the semantic role is not realized for the given occurrence, otherwise the number of additional roles is drawn from the geometric distribution geom(p,r).
</prevsent>
<prevsent>the beta priors over ? 4for prepositional phrases, we take as head the head nounof the object noun phrase as it encodes crucial lexical information.
</prevsent>
</prevsection>
<citsent citstr=" P11-1145 ">
however, the preposition is not ignored but rather encoded in the corresponding argument key, as explained in section 2.5alternatively, the clustering of arguments could be induced within the model, as done in (titov and klementiev, 2011).<papid> P11-1145 </papid></citsent>
<aftsection>
<nextsent>15 clustering of argument keys: factored model: for each predicate = 1, 2, . . .
</nextsent>
<nextsent>: bp ? crp (?)
</nextsent>
<nextsent>[partition of arg keys] coupled model: ? non inform [similarity graph] for each predicate = 1, 2, . . .
</nextsent>
<nextsent>: bp ? dd-crp (?,d) [partition of arg keys] parameters: for each predicate = 1, 2, . . .
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1186">
<title id=" E12-1003.xml">a bayesian approach to unsupervised semantic role induction </title>
<section> empirical evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>we keep the general setup of (lang and lapata, 2011<papid> P11-1112 </papid>a), to evaluate our models and compare them to the current state of the art.</prevsent>
<prevsent>we run all of our experiments on the standard conll 2008 shared task (surdeanu et al  2008) <papid> W08-2121 </papid>version of penn tree bank wsj and propbank.</prevsent>
</prevsection>
<citsent citstr=" D07-1096 ">
in addition to gold dependency analyses and gold propbank annotations, it has dependency structures generated automatically by the malt parser (nivre et al  2007).<papid> D07-1096 </papid></citsent>
<aftsection>
<nextsent>we vary our experimental setup as follows:?
</nextsent>
<nextsent>we evaluate our models on gold and automatically generated parses, and use either gold propbank annotations or the heuristic from section 2 to identify arguments, resulting in four experimental regimes.
</nextsent>
<nextsent>in order to reduce the sparsity of predicate argument fillers we consider replacing lemmas of their syntactic heads with word clusters induced by clustering algorithm as preprocessing step.
</nextsent>
<nextsent>in particular, we use brown (br) clustering (brown et al  1992) <papid> J92-4003 </papid>induced over rcv1 corpus (turian et al 2010).<papid> P10-1040 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1187">
<title id=" E12-1003.xml">a bayesian approach to unsupervised semantic role induction </title>
<section> empirical evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>we evaluate our models on gold and automatically generated parses, and use either gold propbank annotations or the heuristic from section 2 to identify arguments, resulting in four experimental regimes.
</prevsent>
<prevsent>in order to reduce the sparsity of predicate argument fillers we consider replacing lemmas of their syntactic heads with word clusters induced by clustering algorithm as preprocessing step.
</prevsent>
</prevsection>
<citsent citstr=" J92-4003 ">
in particular, we use brown (br) clustering (brown et al  1992) <papid> J92-4003 </papid>induced over rcv1 corpus (turian et al 2010).<papid> P10-1040 </papid></citsent>
<aftsection>
<nextsent>although the clustering is hierarchical, we only use cluster at the lowest level of the hierarchy for each word.we use the purity (pu) and collocation (co) metrics as well as their harmonic mean (f1) to measure the quality of the resulting clusters.
</nextsent>
<nextsent>purity measures the degree to which each cluster contains arguments sharing the same gold role: pu = 1 ? max |gj ? ci|where if ci is the set of arguments in the i-th induced cluster,gj is the set of arguments in the jth10the coupled model without discounting still outperforms the factored counterpart in our experiments.
</nextsent>
<nextsent>18gold cluster, and is the total number of arguments.
</nextsent>
<nextsent>collocation evaluates the degree to which arguments with the same gold roles are assigned to single cluster.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1188">
<title id=" E12-1003.xml">a bayesian approach to unsupervised semantic role induction </title>
<section> empirical evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>we evaluate our models on gold and automatically generated parses, and use either gold propbank annotations or the heuristic from section 2 to identify arguments, resulting in four experimental regimes.
</prevsent>
<prevsent>in order to reduce the sparsity of predicate argument fillers we consider replacing lemmas of their syntactic heads with word clusters induced by clustering algorithm as preprocessing step.
</prevsent>
</prevsection>
<citsent citstr=" P10-1040 ">
in particular, we use brown (br) clustering (brown et al  1992) <papid> J92-4003 </papid>induced over rcv1 corpus (turian et al 2010).<papid> P10-1040 </papid></citsent>
<aftsection>
<nextsent>although the clustering is hierarchical, we only use cluster at the lowest level of the hierarchy for each word.we use the purity (pu) and collocation (co) metrics as well as their harmonic mean (f1) to measure the quality of the resulting clusters.
</nextsent>
<nextsent>purity measures the degree to which each cluster contains arguments sharing the same gold role: pu = 1 ? max |gj ? ci|where if ci is the set of arguments in the i-th induced cluster,gj is the set of arguments in the jth10the coupled model without discounting still outperforms the factored counterpart in our experiments.
</nextsent>
<nextsent>18gold cluster, and is the total number of arguments.
</nextsent>
<nextsent>collocation evaluates the degree to which arguments with the same gold roles are assigned to single cluster.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1282">
<title id=" E12-1003.xml">a bayesian approach to unsupervised semantic role induction </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>due to the space constraints we are not able to present detailed analysis of the induced similarity graph d, however,argument-key pairs with the highest induced similarity encode, among other things, passivization, benefactive alternations, near-interchangeabilityof some subordinating conjunctions and prepositions (e.g., if and whether), as well as, restoring some of the unnecessary splits introduced by the argument key definition (e.g., semantic roles for adverbials do not normally depend on whether the construction is passive or active).
</prevsent>
<prevsent>most of srl research has focused on the supervised setting (carreras and ma`rquez, 2005; surdeanu et al  2008), <papid> W08-2121 </papid>however, lack of annotated resources for most languages and insufficient cover age provided by the existing resources motivates the need for using unlabeled data or other forms of weak supervision.</prevsent>
</prevsection>
<citsent citstr=" D09-1003 ">
this work includes methods based on graph alignment between labeled and unlabeled data (furstenau and lapata, 2009), using unlabeled data to improve lexical generalization (deschacht and moens, 2009), <papid> D09-1003 </papid>and projection of annotation across languages (pado and lapata, 2009; vander plas et al  2011).</citsent>
<aftsection>
<nextsent>semi-supervised and weakly-supervised techniques have also been explored for other types of semantic representations but these studies have mostly focused on restricted domains (kate and mooney, 2007; lianget al  2009; titov and kozhevnikov, 2010; <papid> P10-1098 </papid>goldwasser et al  2011; <papid> P11-1149 </papid>liang et al  2011).<papid> P11-1060 </papid>unsupervised learning has been one of the central paradigms for the closely-related area of relation extraction, where several techniques have been proposed to cluster semantically similar ver balizations of relations (lin and pantel, 2001;banko et al  2007).</nextsent>
<nextsent>early unsupervised approaches to the srl problem include the workby swier and stevenson (2004), <papid> W04-3213 </papid>where the verb net verb lexicon was used to guide unsupervised learning, and generative model of grenager and manning (2006) <papid> W06-1601 </papid>which exploits linguistic priors on syntactic-semantic interface.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1283">
<title id=" E12-1003.xml">a bayesian approach to unsupervised semantic role induction </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>most of srl research has focused on the supervised setting (carreras and ma`rquez, 2005; surdeanu et al  2008), <papid> W08-2121 </papid>however, lack of annotated resources for most languages and insufficient cover age provided by the existing resources motivates the need for using unlabeled data or other forms of weak supervision.</prevsent>
<prevsent>this work includes methods based on graph alignment between labeled and unlabeled data (furstenau and lapata, 2009), using unlabeled data to improve lexical generalization (deschacht and moens, 2009), <papid> D09-1003 </papid>and projection of annotation across languages (pado and lapata, 2009; vander plas et al  2011).</prevsent>
</prevsection>
<citsent citstr=" P10-1098 ">
semi-supervised and weakly-supervised techniques have also been explored for other types of semantic representations but these studies have mostly focused on restricted domains (kate and mooney, 2007; lianget al  2009; titov and kozhevnikov, 2010; <papid> P10-1098 </papid>goldwasser et al  2011; <papid> P11-1149 </papid>liang et al  2011).<papid> P11-1060 </papid>unsupervised learning has been one of the central paradigms for the closely-related area of relation extraction, where several techniques have been proposed to cluster semantically similar ver balizations of relations (lin and pantel, 2001;banko et al  2007).</citsent>
<aftsection>
<nextsent>early unsupervised approaches to the srl problem include the workby swier and stevenson (2004), <papid> W04-3213 </papid>where the verb net verb lexicon was used to guide unsupervised learning, and generative model of grenager and manning (2006) <papid> W06-1601 </papid>which exploits linguistic priors on syntactic-semantic interface.</nextsent>
<nextsent>more recently, the role induction problem has been studied in lang and lapata (2010) <papid> N10-1137 </papid>whereit has been reformulated as problem of detecting alterations and mapping non-standard linkings to the canonical ones.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1284">
<title id=" E12-1003.xml">a bayesian approach to unsupervised semantic role induction </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>most of srl research has focused on the supervised setting (carreras and ma`rquez, 2005; surdeanu et al  2008), <papid> W08-2121 </papid>however, lack of annotated resources for most languages and insufficient cover age provided by the existing resources motivates the need for using unlabeled data or other forms of weak supervision.</prevsent>
<prevsent>this work includes methods based on graph alignment between labeled and unlabeled data (furstenau and lapata, 2009), using unlabeled data to improve lexical generalization (deschacht and moens, 2009), <papid> D09-1003 </papid>and projection of annotation across languages (pado and lapata, 2009; vander plas et al  2011).</prevsent>
</prevsection>
<citsent citstr=" P11-1149 ">
semi-supervised and weakly-supervised techniques have also been explored for other types of semantic representations but these studies have mostly focused on restricted domains (kate and mooney, 2007; lianget al  2009; titov and kozhevnikov, 2010; <papid> P10-1098 </papid>goldwasser et al  2011; <papid> P11-1149 </papid>liang et al  2011).<papid> P11-1060 </papid>unsupervised learning has been one of the central paradigms for the closely-related area of relation extraction, where several techniques have been proposed to cluster semantically similar ver balizations of relations (lin and pantel, 2001;banko et al  2007).</citsent>
<aftsection>
<nextsent>early unsupervised approaches to the srl problem include the workby swier and stevenson (2004), <papid> W04-3213 </papid>where the verb net verb lexicon was used to guide unsupervised learning, and generative model of grenager and manning (2006) <papid> W06-1601 </papid>which exploits linguistic priors on syntactic-semantic interface.</nextsent>
<nextsent>more recently, the role induction problem has been studied in lang and lapata (2010) <papid> N10-1137 </papid>whereit has been reformulated as problem of detecting alterations and mapping non-standard linkings to the canonical ones.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1285">
<title id=" E12-1003.xml">a bayesian approach to unsupervised semantic role induction </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>most of srl research has focused on the supervised setting (carreras and ma`rquez, 2005; surdeanu et al  2008), <papid> W08-2121 </papid>however, lack of annotated resources for most languages and insufficient cover age provided by the existing resources motivates the need for using unlabeled data or other forms of weak supervision.</prevsent>
<prevsent>this work includes methods based on graph alignment between labeled and unlabeled data (furstenau and lapata, 2009), using unlabeled data to improve lexical generalization (deschacht and moens, 2009), <papid> D09-1003 </papid>and projection of annotation across languages (pado and lapata, 2009; vander plas et al  2011).</prevsent>
</prevsection>
<citsent citstr=" P11-1060 ">
semi-supervised and weakly-supervised techniques have also been explored for other types of semantic representations but these studies have mostly focused on restricted domains (kate and mooney, 2007; lianget al  2009; titov and kozhevnikov, 2010; <papid> P10-1098 </papid>goldwasser et al  2011; <papid> P11-1149 </papid>liang et al  2011).<papid> P11-1060 </papid>unsupervised learning has been one of the central paradigms for the closely-related area of relation extraction, where several techniques have been proposed to cluster semantically similar ver balizations of relations (lin and pantel, 2001;banko et al  2007).</citsent>
<aftsection>
<nextsent>early unsupervised approaches to the srl problem include the workby swier and stevenson (2004), <papid> W04-3213 </papid>where the verb net verb lexicon was used to guide unsupervised learning, and generative model of grenager and manning (2006) <papid> W06-1601 </papid>which exploits linguistic priors on syntactic-semantic interface.</nextsent>
<nextsent>more recently, the role induction problem has been studied in lang and lapata (2010) <papid> N10-1137 </papid>whereit has been reformulated as problem of detecting alterations and mapping non-standard linkings to the canonical ones.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1286">
<title id=" E12-1003.xml">a bayesian approach to unsupervised semantic role induction </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>this work includes methods based on graph alignment between labeled and unlabeled data (furstenau and lapata, 2009), using unlabeled data to improve lexical generalization (deschacht and moens, 2009), <papid> D09-1003 </papid>and projection of annotation across languages (pado and lapata, 2009; vander plas et al  2011).</prevsent>
<prevsent>semi-supervised and weakly-supervised techniques have also been explored for other types of semantic representations but these studies have mostly focused on restricted domains (kate and mooney, 2007; lianget al  2009; titov and kozhevnikov, 2010; <papid> P10-1098 </papid>goldwasser et al  2011; <papid> P11-1149 </papid>liang et al  2011).<papid> P11-1060 </papid>unsupervised learning has been one of the central paradigms for the closely-related area of relation extraction, where several techniques have been proposed to cluster semantically similar ver balizations of relations (lin and pantel, 2001;banko et al  2007).</prevsent>
</prevsection>
<citsent citstr=" W04-3213 ">
early unsupervised approaches to the srl problem include the workby swier and stevenson (2004), <papid> W04-3213 </papid>where the verb net verb lexicon was used to guide unsupervised learning, and generative model of grenager and manning (2006) <papid> W06-1601 </papid>which exploits linguistic priors on syntactic-semantic interface.</citsent>
<aftsection>
<nextsent>more recently, the role induction problem has been studied in lang and lapata (2010) <papid> N10-1137 </papid>whereit has been reformulated as problem of detecting alterations and mapping non-standard linkings to the canonical ones.</nextsent>
<nextsent>later, lang and la pata (2011<papid> P11-1112 </papid>a) proposed an algorithmic approach to clustering argument signatures which achieves higher accuracy and outperforms the syntactic baseline.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1293">
<title id=" E12-1003.xml">a bayesian approach to unsupervised semantic role induction </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>later, lang and la pata (2011<papid> P11-1112 </papid>a) proposed an algorithmic approach to clustering argument signatures which achieves higher accuracy and outperforms the syntactic baseline.</prevsent>
<prevsent>in lang and lapata (2011<papid> P11-1112 </papid>b), the role induction problem is formulated as graph partitioning problem: each vertex in the graph corresponds to predicate occurrence and edges represent lexical and syntactic similarities between theoccurrences.</prevsent>
</prevsection>
<citsent citstr=" D09-1001 ">
unsupervised induction of semantics has also been studied in poon and domingos (2009) <papid> D09-1001 </papid>and titov and klementiev (2010) butthe induced representations are not entirely compatible with the propbank-style annotations andthey have been evaluated only on question answering task for the biomedical domain.</citsent>
<aftsection>
<nextsent>also, the related task of unsupervised argument identification was considered in abend et al (2009).<papid> P09-1004 </papid></nextsent>
<nextsent>in this work we introduced two bayesian models for unsupervised role induction.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1294">
<title id=" E12-1003.xml">a bayesian approach to unsupervised semantic role induction </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in lang and lapata (2011<papid> P11-1112 </papid>b), the role induction problem is formulated as graph partitioning problem: each vertex in the graph corresponds to predicate occurrence and edges represent lexical and syntactic similarities between theoccurrences.</prevsent>
<prevsent>unsupervised induction of semantics has also been studied in poon and domingos (2009) <papid> D09-1001 </papid>and titov and klementiev (2010) butthe induced representations are not entirely compatible with the propbank-style annotations andthey have been evaluated only on question answering task for the biomedical domain.</prevsent>
</prevsection>
<citsent citstr=" P09-1004 ">
also, the related task of unsupervised argument identification was considered in abend et al (2009).<papid> P09-1004 </papid></citsent>
<aftsection>
<nextsent>in this work we introduced two bayesian models for unsupervised role induction.
</nextsent>
<nextsent>they treat the task as family of related clustering problems, one for each predicate.
</nextsent>
<nextsent>the first factored model induces each clustering independently, whereas the second model couples them by exploiting novel technique for sharing clustering preferences across family of clusterings.
</nextsent>
<nextsent>both methods achieve state-of-the-art results with the coupled model outperforming the factored counterpart in all regimes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1295">
<title id=" E12-3010.xml">improving machine translation of null subjects in italian and spanish </title>
<section> baseline machine translation of null.  </section>
<citcontext>
<prevsection>
<prevsent>percentages are calculated with respect to the total number of words.
</prevsent>
<prevsent>subjects the 1,000 sentences of our corpus were translated from both languages into french (itfr; esfr) in order to assess if personal pro-dropand impersonal pro-drop were correctly identified and translated.
</prevsent>
</prevsection>
<citsent citstr=" W09-0415 ">
we tested two systems: its-2(wehrli et al, 2009), <papid> W09-0415 </papid>rule-based mt system developed at the latl; and statistical system built using the moses toolkit out of the box (koehn etal., 2007).<papid> P07-2045 </papid></citsent>
<aftsection>
<nextsent>the latter was trained on 55,000 sentence pairs from the europarl corpus and tuned on2,000 additional sentence pairs, and includes 3 gram language model.tables 3 and 4 show percentages of correct, in correct and missing translations of personal and impersonal null subjects calculated on the basis of the number of personal and impersonal pro-drop found in the corpus.
</nextsent>
<nextsent>we considered the translation correct when the null pronoun is translated by an overt pronoun with the correct gender, person and number features in french; otherwise, we considered it in correct.
</nextsent>
<nextsent>missing translation refers to cases where the null pronoun is not generated at all in the target language.
</nextsent>
<nextsent>we chose these criteria because they allow us to evaluate the single phenomenon of null subject 82 its-2 pair pro-drop correct incorrect missing personal 66.34% 3.49% 30.15% itfr impersonal 16.78% 18.97% 64.23% average 46.78% 9.6% 43.61% personal 55.79% 3.50% 40.70% esfr impersonal 29.29% 11.40% 59.29% average 44.28% 6.93% 48.78% table 3: percentages of correct, incorrect and missing translation of zero-pronouns obtained by its-2.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1296">
<title id=" E12-3010.xml">improving machine translation of null subjects in italian and spanish </title>
<section> baseline machine translation of null.  </section>
<citcontext>
<prevsection>
<prevsent>percentages are calculated with respect to the total number of words.
</prevsent>
<prevsent>subjects the 1,000 sentences of our corpus were translated from both languages into french (itfr; esfr) in order to assess if personal pro-dropand impersonal pro-drop were correctly identified and translated.
</prevsent>
</prevsection>
<citsent citstr=" P07-2045 ">
we tested two systems: its-2(wehrli et al, 2009), <papid> W09-0415 </papid>rule-based mt system developed at the latl; and statistical system built using the moses toolkit out of the box (koehn etal., 2007).<papid> P07-2045 </papid></citsent>
<aftsection>
<nextsent>the latter was trained on 55,000 sentence pairs from the europarl corpus and tuned on2,000 additional sentence pairs, and includes 3 gram language model.tables 3 and 4 show percentages of correct, in correct and missing translations of personal and impersonal null subjects calculated on the basis of the number of personal and impersonal pro-drop found in the corpus.
</nextsent>
<nextsent>we considered the translation correct when the null pronoun is translated by an overt pronoun with the correct gender, person and number features in french; otherwise, we considered it in correct.
</nextsent>
<nextsent>missing translation refers to cases where the null pronoun is not generated at all in the target language.
</nextsent>
<nextsent>we chose these criteria because they allow us to evaluate the single phenomenon of null subject 82 its-2 pair pro-drop correct incorrect missing personal 66.34% 3.49% 30.15% itfr impersonal 16.78% 18.97% 64.23% average 46.78% 9.6% 43.61% personal 55.79% 3.50% 40.70% esfr impersonal 29.29% 11.40% 59.29% average 44.28% 6.93% 48.78% table 3: percentages of correct, incorrect and missing translation of zero-pronouns obtained by its-2.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1297">
<title id=" E12-3010.xml">improving machine translation of null subjects in italian and spanish </title>
<section> baseline machine translation of null.  </section>
<citcontext>
<prevsection>
<prevsent>average is calculated on the basis of total pro-drop in corpus.translation.
</prevsent>
<prevsent>bleu and similar mt metrics compute scores over text as whole.
</prevsent>
</prevsection>
<citsent citstr=" W06-3114 ">
for the same reason, human evaluation metrics based on adequacy and fluency were not suitable either (koehn and monz, 2006).<papid> W06-3114 </papid></citsent>
<aftsection>
<nextsent>moses generally outperforms its-2 (tables 3and 4).
</nextsent>
<nextsent>results for the two systems demonstrate that instances of personal pro-drop are better translated than impersonal pro-drop for the two languages.
</nextsent>
<nextsent>since rates of missing pronouns translations are considerable, especially for its-2,results also indicate that both systems have problems resolving non-expressed pronouns for their generation in french.
</nextsent>
<nextsent>a detailed description for each system follows.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1298">
<title id=" E12-3010.xml">improving machine translation of null subjects in italian and spanish </title>
<section> its-2 improvements.  </section>
<citcontext>
<prevsection>
<prevsent>4.1 rule-based preprocessing.
</prevsent>
<prevsent>preprocessing of input data is very common taskin natural language processing.
</prevsent>
</prevsection>
<citsent citstr=" P06-1001 ">
statistical systems often benefit from linguistic preprocessing 84to deal with rich morphology and long distance reordering issues (sadat and habash, 2006; <papid> P06-1001 </papid>habash,2007).</citsent>
<aftsection>
<nextsent>in our case, the idea behind this first component is to help the translation process of rule based system by reducing the amount of zero pronouns in the source document5, ensuring that subject pronouns get properly transferred to the target language.
</nextsent>
<nextsent>in order to assess the effect of this approach, we implemented rule-based pre processor taking asinput document in italian or spanish and returning as output the same document with dropped subject pronouns restored.
</nextsent>
<nextsent>it relies on two re sources: list of personal and impersonal verbs,and part-of-speech tagging of the source document.
</nextsent>
<nextsent>we first present these two resources before describing the approach in more detail.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1299">
<title id=" E12-3010.xml">improving machine translation of null subjects in italian and spanish </title>
<section> its-2 improvements.  </section>
<citcontext>
<prevsection>
<prevsent>this limits the coverage, but ensures domain-specific verb usage.
</prevsent>
<prevsent>part-of-speech tagging of the source document its-2, being transfer-based system, relies on parser to construct the syntactic structure of the source language and, from there, it transfers the syntactic structure onto the target language.
</prevsent>
</prevsection>
<citsent citstr=" W07-1216 ">
its 2 uses fips (wehrli, 2007), <papid> W07-1216 </papid>multilingual parser also developed at latl.</citsent>
<aftsection>
<nextsent>apart from the projection of syntactic structures, fips produces part-of speech tagging.
</nextsent>
<nextsent>outline of the approach these are the steps followed by the pre proces sor: 1.
</nextsent>
<nextsent>read part-of-speech tagged sentence..
</nextsent>
<nextsent>2.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1300">
<title id=" E12-3010.xml">improving machine translation of null subjects in italian and spanish </title>
<section> its-2 improvements.  </section>
<citcontext>
<prevsection>
<prevsent>an example of preprocessed sentences is given in figure 1.
</prevsent>
<prevsent>4.2 statistical post-editing.
</prevsent>
</prevsection>
<citsent citstr=" W07-0728 ">
since the work of simard et al (2007<papid> W07-0728 </papid>a), statistical post-editing (spe) has become very popular technique in the domain of hybrid mt. the ideais to train statistical phrase-based system in order to improve the output of rule-based system.</citsent>
<aftsection>
<nextsent>the statistical post-editing component is trained on corpus comprising machine translated sentences on the source side (translations produced by the underlying rule-based system), and their corresponding reference translations on the target side.
</nextsent>
<nextsent>in sense, spe translates?
</nextsent>
<nextsent>from imperfect target language to target language.
</nextsent>
<nextsent>both quantitative and qualitative evaluations have shown that spe can achieve significant improvements over the output of the underlying rule-based system (simard et al, 2007<papid> W07-0728 </papid>b; schwenk et al, 2009).<papid> W09-0423 </papid>we decided to incorporate post-editing component in order to assess if this approach can specifically address the issue of dropped subjectpronouns.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1304">
<title id=" E12-3010.xml">improving machine translation of null subjects in italian and spanish </title>
<section> its-2 improvements.  </section>
<citcontext>
<prevsection>
<prevsent>in sense, spe translates?
</prevsent>
<prevsent>from imperfect target language to target language.
</prevsent>
</prevsection>
<citsent citstr=" W09-0423 ">
both quantitative and qualitative evaluations have shown that spe can achieve significant improvements over the output of the underlying rule-based system (simard et al, 2007<papid> W07-0728 </papid>b; schwenk et al, 2009).<papid> W09-0423 </papid>we decided to incorporate post-editing component in order to assess if this approach can specifically address the issue of dropped subjectpronouns.</citsent>
<aftsection>
<nextsent>we first present the training corpus before describing the approach in more detail.
</nextsent>
<nextsent>training corpus to train the translation model, we translated subset of the europarl corpus using its-2.
</nextsent>
<nextsent>the translations were then aligned with corresponding reference translations, resulting in parallel corpus for each language pair, composed of 976 sentences for itfr and 1,005 sentences for esfr.
</nextsent>
<nextsent>we opted for machine translations also on the target side, rather than human reference translations, in order to ascertain if parallel corpus produced in such way, with significantly lesser cost and time requirements, couldbe an effective alternative for specific natural language processing tasks.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1305">
<title id=" E12-3010.xml">improving machine translation of null subjects in italian and spanish </title>
<section> future work.  </section>
<citcontext>
<prevsection>
<prevsent>for example, binding theory does not contain any formalization on gender, reason why specific statistical component could be more ideal option in order to tackle aspects such as mascu line/feminine pronouns.
</prevsent>
<prevsent>secondly, an overt pronoun cannot be restored from finite impersonal verb without making the sentence ungrammatical; therefore, our approach is not useful for treating impersonal sentences.
</prevsent>
</prevsection>
<citsent citstr=" D10-1062 ">
as consequence, we think that an annotation of the empty category, as done by chung and gildea (2010), <papid> D10-1062 </papid>could provide better results.</citsent>
<aftsection>
<nextsent>also, in order to correctly render the meaning of preprocessed sentence, we plan to mark restored subject pronouns in such way that the information about their absence/presence in the original text is preserved as feature in parsing and translation.
</nextsent>
<nextsent>finally, we would like to use larger corpus to train the spe component and compare the effects of utilizing machine translations on the target side versus human reference translations.
</nextsent>
<nextsent>besides, we would like to further explore variations on the plain spe technique, for example, by injecting moses translation of sentences being translated into the phrase-table of the post-editor (chen and eisele, 2010).
</nextsent>
<nextsent>in this paper we measured and compared the occurrence of one syntactic feature ? the null subject parameter ? in italian and spanish.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1306">
<title id=" E12-1017.xml">recall oriented learning of named entities in arabic wikipedia </title>
<section> arabic wikipedia ne annotation.  </section>
<citcontext>
<prevsection>
<prevsent>an empirical evaluation of this technique as applied to well-established discriminative ner model and feature set.experiments show consistent gains on the challenging problem of identifying named entities in arabic wikipedia text.
</prevsent>
<prevsent>most of the effort inner has been focused around small set of domains and general-purpose entity classes relevant to those domains especially the categories per(son), org(anization), and loc(ation) (pol),which are highly prominent in news text.
</prevsent>
</prevsection>
<citsent citstr=" D08-1030 ">
arabic is no exception: the publicly available nercorporaace (walker et al  2006), aner (be najiba et al  2008), <papid> D08-1030 </papid>and ontonotes (hovy et al  2006)<papid> N06-2015 </papid>all are in the news domain.2 however, 2ontonotes contains news-related text.</citsent>
<aftsection>
<nextsent>ace includes some text from blogs.
</nextsent>
<nextsent>in addition to the pol classes, both corpora include additional ne classes such as facility, event, product, vehicle, etc. these entities are infrequent and maynot be comprehensive enough to cover the larger set of pos 162 history science sports technology dev: damascus atom raul gonzales linux imam hussein shrine nuclear power real madrid solaris test: crusades enrico fermi 2004 summer olympics computer islamic golden age light christi ano ronaldo computer software islamic history periodic table football internet ibn tolun mosque physics portugal football team richard st allman ummaya mosque muhammad al-razi fifa world cup window system claudio filippone (per) j. ? x???; linux (software) ??
</nextsent>
<nextsent>jj ?; spanish league (championships) ? gaj.b@ ? py?@; proton (particle) ??kqk.
</nextsent>
<nextsent>; nuclear radiation (generic-misc) ? ??
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1307">
<title id=" E12-1017.xml">recall oriented learning of named entities in arabic wikipedia </title>
<section> arabic wikipedia ne annotation.  </section>
<citcontext>
<prevsection>
<prevsent>an empirical evaluation of this technique as applied to well-established discriminative ner model and feature set.experiments show consistent gains on the challenging problem of identifying named entities in arabic wikipedia text.
</prevsent>
<prevsent>most of the effort inner has been focused around small set of domains and general-purpose entity classes relevant to those domains especially the categories per(son), org(anization), and loc(ation) (pol),which are highly prominent in news text.
</prevsent>
</prevsection>
<citsent citstr=" N06-2015 ">
arabic is no exception: the publicly available nercorporaace (walker et al  2006), aner (be najiba et al  2008), <papid> D08-1030 </papid>and ontonotes (hovy et al  2006)<papid> N06-2015 </papid>all are in the news domain.2 however, 2ontonotes contains news-related text.</citsent>
<aftsection>
<nextsent>ace includes some text from blogs.
</nextsent>
<nextsent>in addition to the pol classes, both corpora include additional ne classes such as facility, event, product, vehicle, etc. these entities are infrequent and maynot be comprehensive enough to cover the larger set of pos 162 history science sports technology dev: damascus atom raul gonzales linux imam hussein shrine nuclear power real madrid solaris test: crusades enrico fermi 2004 summer olympics computer islamic golden age light christi ano ronaldo computer software islamic history periodic table football internet ibn tolun mosque physics portugal football team richard st allman ummaya mosque muhammad al-razi fifa world cup window system claudio filippone (per) j. ? x???; linux (software) ??
</nextsent>
<nextsent>jj ?; spanish league (championships) ? gaj.b@ ? py?@; proton (particle) ??kqk.
</nextsent>
<nextsent>; nuclear radiation (generic-misc) ? ??
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1308">
<title id=" E12-1017.xml">recall oriented learning of named entities in arabic wikipedia </title>
<section> arabic wikipedia ne annotation.  </section>
<citcontext>
<prevsection>
<prevsent> q???
</prevsent>
<prevsent>ak table 1: translated titles of arabic wikipedia articles in our development and test sets, and some nes with standard and article-specific classes.
</prevsent>
</prevsection>
<citsent citstr=" W09-3302 ">
additionally, prussia and amman were reserved for training annotators,and gulf war for estimating inter-annotator agreement.appropriate entity classes will vary widely by do main; occurrence rates for entity classes are quite different in news text vs. wikipedia, for instance (balasuriya et al  2009).<papid> W09-3302 </papid></citsent>
<aftsection>
<nextsent>this is abundantly clear in technical and scientific discourse, where much of the terminology is domain-specific, but it holds elsewhere.
</nextsent>
<nextsent>non-pol entities in the history domain, for instance, include important events(wars, famines) and cultural movements (roman ticism).
</nextsent>
<nextsent>ignoring such domain-critical entities likely limits the usefulness of the ne analysis.
</nextsent>
<nextsent>recognizing this limitation, some work onner has sought to codify more robust inventories of general-purpose entity types (sekine et al  2002; weischedel and brun stein, 2005; grouin et al  2011) <papid> W11-0411 </papid>or to enumerate domain-specific types (settles, 2004; <papid> W04-1221 </papid>yao et al  2003).<papid> W03-1708 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1309">
<title id=" E12-1017.xml">recall oriented learning of named entities in arabic wikipedia </title>
<section> arabic wikipedia ne annotation.  </section>
<citcontext>
<prevsection>
<prevsent>non-pol entities in the history domain, for instance, include important events(wars, famines) and cultural movements (roman ticism).
</prevsent>
<prevsent>ignoring such domain-critical entities likely limits the usefulness of the ne analysis.
</prevsent>
</prevsection>
<citsent citstr=" W11-0411 ">
recognizing this limitation, some work onner has sought to codify more robust inventories of general-purpose entity types (sekine et al  2002; weischedel and brun stein, 2005; grouin et al  2011) <papid> W11-0411 </papid>or to enumerate domain-specific types (settles, 2004; <papid> W04-1221 </papid>yao et al  2003).<papid> W03-1708 </papid></citsent>
<aftsection>
<nextsent>coarse, general-purpose categories have also been usedfor semantic tagging of nouns and verbs (cia ramita and johnson, 2003).<papid> W03-1022 </papid></nextsent>
<nextsent>yet as the number of classes or domains grows, rigorously documenting and organizing the classe seven for asingle language requires intensive effort.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1310">
<title id=" E12-1017.xml">recall oriented learning of named entities in arabic wikipedia </title>
<section> arabic wikipedia ne annotation.  </section>
<citcontext>
<prevsection>
<prevsent>non-pol entities in the history domain, for instance, include important events(wars, famines) and cultural movements (roman ticism).
</prevsent>
<prevsent>ignoring such domain-critical entities likely limits the usefulness of the ne analysis.
</prevsent>
</prevsection>
<citsent citstr=" W04-1221 ">
recognizing this limitation, some work onner has sought to codify more robust inventories of general-purpose entity types (sekine et al  2002; weischedel and brun stein, 2005; grouin et al  2011) <papid> W11-0411 </papid>or to enumerate domain-specific types (settles, 2004; <papid> W04-1221 </papid>yao et al  2003).<papid> W03-1708 </papid></citsent>
<aftsection>
<nextsent>coarse, general-purpose categories have also been usedfor semantic tagging of nouns and verbs (cia ramita and johnson, 2003).<papid> W03-1022 </papid></nextsent>
<nextsent>yet as the number of classes or domains grows, rigorously documenting and organizing the classe seven for asingle language requires intensive effort.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1311">
<title id=" E12-1017.xml">recall oriented learning of named entities in arabic wikipedia </title>
<section> arabic wikipedia ne annotation.  </section>
<citcontext>
<prevsection>
<prevsent>non-pol entities in the history domain, for instance, include important events(wars, famines) and cultural movements (roman ticism).
</prevsent>
<prevsent>ignoring such domain-critical entities likely limits the usefulness of the ne analysis.
</prevsent>
</prevsection>
<citsent citstr=" W03-1708 ">
recognizing this limitation, some work onner has sought to codify more robust inventories of general-purpose entity types (sekine et al  2002; weischedel and brun stein, 2005; grouin et al  2011) <papid> W11-0411 </papid>or to enumerate domain-specific types (settles, 2004; <papid> W04-1221 </papid>yao et al  2003).<papid> W03-1708 </papid></citsent>
<aftsection>
<nextsent>coarse, general-purpose categories have also been usedfor semantic tagging of nouns and verbs (cia ramita and johnson, 2003).<papid> W03-1022 </papid></nextsent>
<nextsent>yet as the number of classes or domains grows, rigorously documenting and organizing the classe seven for asingle language requires intensive effort.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1312">
<title id=" E12-1017.xml">recall oriented learning of named entities in arabic wikipedia </title>
<section> arabic wikipedia ne annotation.  </section>
<citcontext>
<prevsection>
<prevsent>ignoring such domain-critical entities likely limits the usefulness of the ne analysis.
</prevsent>
<prevsent>recognizing this limitation, some work onner has sought to codify more robust inventories of general-purpose entity types (sekine et al  2002; weischedel and brun stein, 2005; grouin et al  2011) <papid> W11-0411 </papid>or to enumerate domain-specific types (settles, 2004; <papid> W04-1221 </papid>yao et al  2003).<papid> W03-1708 </papid></prevsent>
</prevsection>
<citsent citstr=" W03-1022 ">
coarse, general-purpose categories have also been usedfor semantic tagging of nouns and verbs (cia ramita and johnson, 2003).<papid> W03-1022 </papid></citsent>
<aftsection>
<nextsent>yet as the number of classes or domains grows, rigorously documenting and organizing the classe seven for asingle language requires intensive effort.
</nextsent>
<nextsent>ideally, an ner system would refine the traditional classes (hovy et al  2011) <papid> P11-1147 </papid>or identify new entity classes when they arise in new domains, adapting to new data.</nextsent>
<nextsent>for this reason, we believe it is valuable to consider ner systems that identify (but do not necessarily label) entity mentions, and alsoto consider annotation schemes that allow annotators more freedom in defining entity classes.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1313">
<title id=" E12-1017.xml">recall oriented learning of named entities in arabic wikipedia </title>
<section> arabic wikipedia ne annotation.  </section>
<citcontext>
<prevsection>
<prevsent>coarse, general-purpose categories have also been usedfor semantic tagging of nouns and verbs (cia ramita and johnson, 2003).<papid> W03-1022 </papid></prevsent>
<prevsent>yet as the number of classes or domains grows, rigorously documenting and organizing the classe seven for asingle language requires intensive effort.</prevsent>
</prevsection>
<citsent citstr=" P11-1147 ">
ideally, an ner system would refine the traditional classes (hovy et al  2011) <papid> P11-1147 </papid>or identify new entity classes when they arise in new domains, adapting to new data.</citsent>
<aftsection>
<nextsent>for this reason, we believe it is valuable to consider ner systems that identify (but do not necessarily label) entity mentions, and alsoto consider annotation schemes that allow annotators more freedom in defining entity classes.
</nextsent>
<nextsent>our aim in creating an annotated dataset is to provide testbed for evaluation of new ner models.
</nextsent>
<nextsent>we will use these data as development and sible nes (sekine et al  2002).
</nextsent>
<nextsent>nezda et al (2006) annotated and evaluated an arabic ne corpus with an extended set of 18 classes (including temporal and numeric entities); this corpus has not been released publicly.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1314">
<title id=" E12-1017.xml">recall oriented learning of named entities in arabic wikipedia </title>
<section> models.  </section>
<citcontext>
<prevsection>
<prevsent>entity types in this data are pol categories (per, org, loc) and mis.
</prevsent>
<prevsent>portions of the ace corpus were held out as development and test data; the remainder is used in training.
</prevsent>
</prevsection>
<citsent citstr=" W02-1001 ">
our starting point for statistical ner is feature based linear model over sequences, trained using the structured perceptron (collins, 2002).<papid> W02-1001 </papid>8in addition to lexical and morphological9 fea 6additional details appear in the supplement.</citsent>
<aftsection>
<nextsent>7we downloaded snapshot of arabic wikipedia(http://ar.wikipedia.org) on 8/29/2009 and preprocessed the articles to extract main body text and meta data using the mwlib package for python (pediapress, 2010).8a more leisurely discussion of the structured perceptron and its connection to empirical risk minimization can be found in the supplementary document.
</nextsent>
<nextsent>9we obtain morphological analyses from the mada tool (habash and rambow, 2005; <papid> P05-1071 </papid>roth et al  2008).<papid> P08-2030 </papid></nextsent>
<nextsent>training words nes ace+aner 212,839 15,796 wikipedia (unlabeled, 397 docs) 1,110,546 ? development ace 7,776 638 wikipedia (4 domains, 8 docs) 21,203 2,073 test ace 7,789 621 wikipedia (4 domains, 20 docs) 52,650 3,781 table 4: number of words (entity mentions) in data sets.tures known to work well for arabic ner (be najiba et al  2008; <papid> D08-1030 </papid>abdul-hamid and darwish, 2010), we incorporate some additional features enabled by wikipedia.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1315">
<title id=" E12-1017.xml">recall oriented learning of named entities in arabic wikipedia </title>
<section> models.  </section>
<citcontext>
<prevsection>
<prevsent>our starting point for statistical ner is feature based linear model over sequences, trained using the structured perceptron (collins, 2002).<papid> W02-1001 </papid>8in addition to lexical and morphological9 fea 6additional details appear in the supplement.</prevsent>
<prevsent>7we downloaded snapshot of arabic wikipedia(http://ar.wikipedia.org) on 8/29/2009 and preprocessed the articles to extract main body text and meta data using the mwlib package for python (pediapress, 2010).8a more leisurely discussion of the structured perceptron and its connection to empirical risk minimization can be found in the supplementary document.</prevsent>
</prevsection>
<citsent citstr=" P05-1071 ">
9we obtain morphological analyses from the mada tool (habash and rambow, 2005; <papid> P05-1071 </papid>roth et al  2008).<papid> P08-2030 </papid></citsent>
<aftsection>
<nextsent>training words nes ace+aner 212,839 15,796 wikipedia (unlabeled, 397 docs) 1,110,546 ? development ace 7,776 638 wikipedia (4 domains, 8 docs) 21,203 2,073 test ace 7,789 621 wikipedia (4 domains, 20 docs) 52,650 3,781 table 4: number of words (entity mentions) in data sets.tures known to work well for arabic ner (be najiba et al  2008; <papid> D08-1030 </papid>abdul-hamid and darwish, 2010), we incorporate some additional features enabled by wikipedia.</nextsent>
<nextsent>we do not employ gazetteer, as the construction of broad-domaingazetteer is significant undertaking orthogonal to the challenges of new text domain like wikipedia.10 descriptive list of our features is available in the supplementary document.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1316">
<title id=" E12-1017.xml">recall oriented learning of named entities in arabic wikipedia </title>
<section> models.  </section>
<citcontext>
<prevsection>
<prevsent>our starting point for statistical ner is feature based linear model over sequences, trained using the structured perceptron (collins, 2002).<papid> W02-1001 </papid>8in addition to lexical and morphological9 fea 6additional details appear in the supplement.</prevsent>
<prevsent>7we downloaded snapshot of arabic wikipedia(http://ar.wikipedia.org) on 8/29/2009 and preprocessed the articles to extract main body text and meta data using the mwlib package for python (pediapress, 2010).8a more leisurely discussion of the structured perceptron and its connection to empirical risk minimization can be found in the supplementary document.</prevsent>
</prevsection>
<citsent citstr=" P08-2030 ">
9we obtain morphological analyses from the mada tool (habash and rambow, 2005; <papid> P05-1071 </papid>roth et al  2008).<papid> P08-2030 </papid></citsent>
<aftsection>
<nextsent>training words nes ace+aner 212,839 15,796 wikipedia (unlabeled, 397 docs) 1,110,546 ? development ace 7,776 638 wikipedia (4 domains, 8 docs) 21,203 2,073 test ace 7,789 621 wikipedia (4 domains, 20 docs) 52,650 3,781 table 4: number of words (entity mentions) in data sets.tures known to work well for arabic ner (be najiba et al  2008; <papid> D08-1030 </papid>abdul-hamid and darwish, 2010), we incorporate some additional features enabled by wikipedia.</nextsent>
<nextsent>we do not employ gazetteer, as the construction of broad-domaingazetteer is significant undertaking orthogonal to the challenges of new text domain like wikipedia.10 descriptive list of our features is available in the supplementary document.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1318">
<title id=" E12-1017.xml">recall oriented learning of named entities in arabic wikipedia </title>
<section> models.  </section>
<citcontext>
<prevsection>
<prevsent>for observed sequence and feature weights (model parameters) w, the structured hinge loss is `hinge(x,y,w) = max y?
</prevsent>
<prevsent>( g(x,y?) + c(y,y?) ) g(x,y) (1) the maximization problem inside the parentheses is known as cost-augmented decoding.
</prevsent>
</prevsection>
<citsent citstr=" W09-1119 ">
if fac 10a gazetteer ought to yield further improvements in line with previous findings inner (ratinov and roth, 2009).<papid> W09-1119 </papid></citsent>
<aftsection>
<nextsent>11though optimizing ner systems for f1 has been called into question (manning, 2006), no alternative metric has achieved widespread acceptance in the community.
</nextsent>
<nextsent>165 tors similarly to the feature function g(x,y), then we can increase penalties for that have more local mistakes.
</nextsent>
<nextsent>this raises the learners awareness about how it will be evaluated.
</nextsent>
<nextsent>incorporating cost-augmented decoding into the perceptron leads to this decoding step: y?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1319">
<title id=" E12-1017.xml">recall oriented learning of named entities in arabic wikipedia </title>
<section> models.  </section>
<citcontext>
<prevsection>
<prevsent>1 loss (ratliff et al  2006).in this framework, cost functions can be formulated to distinguish between different types of errors made during training.
</prevsent>
<prevsent>for tag sequence = y1, y2, . . .
</prevsent>
</prevsection>
<citsent citstr=" N10-1112 ">
, ym ?, gimpel and smith (2010<papid> N10-1112 </papid>b) define word-local cost functions that differently penalize precision errors (i.e., yi = ? yi 6= for the ith word), recall errors (yi 6= o?</citsent>
<aftsection>
<nextsent>yi = o), and entity class/position errors (other cases where yi 6= yi).
</nextsent>
<nextsent>as will be shown below, key problem in cross-domain ner is poor recall, so we will penalize recall errors more severely: c(y,y?) = m?
</nextsent>
<nextsent>i=1 ? ?
</nextsent>
<nextsent>0 if yi = yi ? if yi 6= ? yi = 1 otherwise (3) for penalty parameter ?   1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1320">
<title id=" E12-1017.xml">recall oriented learning of named entities in arabic wikipedia </title>
<section> models.  </section>
<citcontext>
<prevsection>
<prevsent>0 if yi = yi ? if yi 6= ? yi = 1 otherwise (3) for penalty parameter ?   1.
</prevsent>
<prevsent>we call our learner the recall-oriented?
</prevsent>
</prevsection>
<citsent citstr=" N06-2024 ">
perceptron (rop).we note that minkov et al (2006) <papid> N06-2024 </papid>similarly explored the recall vs. precision tradeoff in ner.</citsent>
<aftsection>
<nextsent>their technique was to directly tune the weight of single feature the feature marking (non entity tokens); lower weight for this feature will incur greater penalty for predicting o. below we demonstrate that our method, which is less coarse, is more successful in our setting.12 in our experiments we will show that injecting arrogance?
</nextsent>
<nextsent>into the learner via the recall-orientedloss function substantially improves recall, especially for non-pol entities (5.3).
</nextsent>
<nextsent>4.2 self-training and semisupervised.
</nextsent>
<nextsent>learning as we will show experimentally, the differences between news text and wikipedia text call for do main adaptation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1322">
<title id=" E12-1017.xml">recall oriented learning of named entities in arabic wikipedia </title>
<section> models.  </section>
<citcontext>
<prevsection>
<prevsent>(j), y?(j)jj=1) algorithm 1: self-training.
</prevsent>
<prevsent>there is no available labeled training data.
</prevsent>
</prevsection>
<citsent citstr=" W03-0407 ">
yet the available unlabeled data is vast, so we turn to semi supervised learning.here we adapt self-training, simple technique that leverages supervised learner (like the perceptron) to perform semi supervised learning (clark et al  2003; <papid> W03-0407 </papid>mihalcea, 2004; <papid> W04-2405 </papid>mcclosky et al  2006).<papid> N06-1020 </papid></citsent>
<aftsection>
<nextsent>in our version, model is trained on the labeled data, then used to label theun labeled target data.
</nextsent>
<nextsent>we iterate between training on the hypothetically-labeled target data plus the original labeled set, and relabeling the target data; see algorithm 1.
</nextsent>
<nextsent>before self-training, we remove sentences hypothesized not to contain any named entity mentions, which we found avoids further encouragement of the model toward low recall.
</nextsent>
<nextsent>we investigate two questions in the context of ner for arabic wikipedia:?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1324">
<title id=" E12-1017.xml">recall oriented learning of named entities in arabic wikipedia </title>
<section> models.  </section>
<citcontext>
<prevsection>
<prevsent>(j), y?(j)jj=1) algorithm 1: self-training.
</prevsent>
<prevsent>there is no available labeled training data.
</prevsent>
</prevsection>
<citsent citstr=" W04-2405 ">
yet the available unlabeled data is vast, so we turn to semi supervised learning.here we adapt self-training, simple technique that leverages supervised learner (like the perceptron) to perform semi supervised learning (clark et al  2003; <papid> W03-0407 </papid>mihalcea, 2004; <papid> W04-2405 </papid>mcclosky et al  2006).<papid> N06-1020 </papid></citsent>
<aftsection>
<nextsent>in our version, model is trained on the labeled data, then used to label theun labeled target data.
</nextsent>
<nextsent>we iterate between training on the hypothetically-labeled target data plus the original labeled set, and relabeling the target data; see algorithm 1.
</nextsent>
<nextsent>before self-training, we remove sentences hypothesized not to contain any named entity mentions, which we found avoids further encouragement of the model toward low recall.
</nextsent>
<nextsent>we investigate two questions in the context of ner for arabic wikipedia:?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1326">
<title id=" E12-1017.xml">recall oriented learning of named entities in arabic wikipedia </title>
<section> models.  </section>
<citcontext>
<prevsection>
<prevsent>(j), y?(j)jj=1) algorithm 1: self-training.
</prevsent>
<prevsent>there is no available labeled training data.
</prevsent>
</prevsection>
<citsent citstr=" N06-1020 ">
yet the available unlabeled data is vast, so we turn to semi supervised learning.here we adapt self-training, simple technique that leverages supervised learner (like the perceptron) to perform semi supervised learning (clark et al  2003; <papid> W03-0407 </papid>mihalcea, 2004; <papid> W04-2405 </papid>mcclosky et al  2006).<papid> N06-1020 </papid></citsent>
<aftsection>
<nextsent>in our version, model is trained on the labeled data, then used to label theun labeled target data.
</nextsent>
<nextsent>we iterate between training on the hypothetically-labeled target data plus the original labeled set, and relabeling the target data; see algorithm 1.
</nextsent>
<nextsent>before self-training, we remove sentences hypothesized not to contain any named entity mentions, which we found avoids further encouragement of the model toward low recall.
</nextsent>
<nextsent>we investigate two questions in the context of ner for arabic wikipedia:?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1328">
<title id=" E12-1017.xml">recall oriented learning of named entities in arabic wikipedia </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>note that the two rops can use different cost parameters.
</prevsent>
<prevsent>for evaluating our models we consider the named entity detection task, i.e., recognizing which spans of words constitute entities.
</prevsent>
</prevsection>
<citsent citstr=" W04-3250 ">
this is measured by per-entity precision, recall, andf1.13 to measure statistical significance of differences between models we use gimpel and smiths(2010) <papid> N10-1112 </papid>implementation of the paired bootstrap re sampler of (koehn, 2004), <papid> W04-3250 </papid>taking 10,000 samples for each comparison.</citsent>
<aftsection>
<nextsent>5.1 baseline.
</nextsent>
<nextsent>our baseline is the perceptron, trained on thepol entity boundaries in the ace+aner corpus (reg/none).14 development data was used to select the number of iterations (10).
</nextsent>
<nextsent>we performed 3-fold cross-validation on the ace data and found wide variance in the in-domain entity detection performance of this model: r f1 fold 1 70.43 63.08 66.55 fold 2 87.48 81.13 84.18 fold 3 65.09 51.13 57.27 average 74.33 65.11 69.33 (fold 1 corresponds to the ace test set described in table 4.)
</nextsent>
<nextsent>we also trained the model to perform pol detection and classification, achieving nearly identical results in the 3-way cross-validation of ace data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1333">
<title id=" E12-1017.xml">recall oriented learning of named entities in arabic wikipedia </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>our approach draws on insights from work in the areas of ner, domain adaptation, nlp with wikipedia, and semi supervised learning.
</prevsent>
<prevsent>as all are broad areas of research, we highlight only the most relevant contributions here.
</prevsent>
</prevsection>
<citsent citstr=" L08-1054 ">
research in arabic ner has been focused on compiling and optimizing the gazette ers and fea 169ture sets for standard sequential modeling algorithms (benajiba et al  2008; <papid> D08-1030 </papid>farber et al  2008;<papid> L08-1054 </papid>shaalan and raza, 2008; abdul-hamid and darwish, 2010).</citsent>
<aftsection>
<nextsent>we make use of features identified in this prior work to construct strong base line system.
</nextsent>
<nextsent>we are unaware of any arabic ner work that has addressed diverse text domains likewikipedia.
</nextsent>
<nextsent>both the english and arabic versions of wikipedia have been used, however, as resources in service of traditional ner (kazama and torisawa, 2007; <papid> D07-1073 </papid>benajiba et al  2008).<papid> D08-1030 </papid></nextsent>
<nextsent>attiaet al (2010) heuristic ally induce mapping between arabic wikipedia and arabic wordnet to construct arabic ne gazetteers.balasuriya et al (2009) <papid> W09-3302 </papid>highlight the substantial divergence between entities appearing in english wikipedia versus traditional corpora, andthe effects of this divergence on ner perfor mance.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1334">
<title id=" E12-1017.xml">recall oriented learning of named entities in arabic wikipedia </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>we make use of features identified in this prior work to construct strong base line system.
</prevsent>
<prevsent>we are unaware of any arabic ner work that has addressed diverse text domains likewikipedia.
</prevsent>
</prevsection>
<citsent citstr=" D07-1073 ">
both the english and arabic versions of wikipedia have been used, however, as resources in service of traditional ner (kazama and torisawa, 2007; <papid> D07-1073 </papid>benajiba et al  2008).<papid> D08-1030 </papid></citsent>
<aftsection>
<nextsent>attiaet al (2010) heuristic ally induce mapping between arabic wikipedia and arabic wordnet to construct arabic ne gazetteers.balasuriya et al (2009) <papid> W09-3302 </papid>highlight the substantial divergence between entities appearing in english wikipedia versus traditional corpora, andthe effects of this divergence on ner perfor mance.</nextsent>
<nextsent>there is evidence that models trained on wikipedia data generalize and perform well on corpora with narrower domains.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1338">
<title id=" E12-1017.xml">recall oriented learning of named entities in arabic wikipedia </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>attiaet al (2010) heuristic ally induce mapping between arabic wikipedia and arabic wordnet to construct arabic ne gazetteers.balasuriya et al (2009) <papid> W09-3302 </papid>highlight the substantial divergence between entities appearing in english wikipedia versus traditional corpora, andthe effects of this divergence on ner perfor mance.</prevsent>
<prevsent>there is evidence that models trained on wikipedia data generalize and perform well on corpora with narrower domains.</prevsent>
</prevsection>
<citsent citstr=" E09-1070 ">
nothman et al (2009) <papid> E09-1070 </papid>and balasuriya et al (2009) <papid> W09-3302 </papid>show that ner models trained on both automatically and manually annotated wikipedia corpora perform reasonably well on news corpora.</citsent>
<aftsection>
<nextsent>there verse scenario does not hold for models trained on news text, result we also observe in arabicner.
</nextsent>
<nextsent>other work has gone beyond the entity detection problem: florian et al (2004) <papid> N04-1001 </papid>additionally predict within-document entity coreference for arabic, chinese, and english ace text, while cucerzan (2007) <papid> D07-1074 </papid>aims to resolve every mention detected in english wikipedia pages to canonical article devoted to the entity in question.</nextsent>
<nextsent>the domain and topic diversity of nes has been studied in the framework of domain adaptationresearch.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1341">
<title id=" E12-1017.xml">recall oriented learning of named entities in arabic wikipedia </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>nothman et al (2009) <papid> E09-1070 </papid>and balasuriya et al (2009) <papid> W09-3302 </papid>show that ner models trained on both automatically and manually annotated wikipedia corpora perform reasonably well on news corpora.</prevsent>
<prevsent>there verse scenario does not hold for models trained on news text, result we also observe in arabicner.</prevsent>
</prevsection>
<citsent citstr=" N04-1001 ">
other work has gone beyond the entity detection problem: florian et al (2004) <papid> N04-1001 </papid>additionally predict within-document entity coreference for arabic, chinese, and english ace text, while cucerzan (2007) <papid> D07-1074 </papid>aims to resolve every mention detected in english wikipedia pages to canonical article devoted to the entity in question.</citsent>
<aftsection>
<nextsent>the domain and topic diversity of nes has been studied in the framework of domain adaptationresearch.
</nextsent>
<nextsent>a group of these methods use self training and select the most informative features and training instances to adapt source domain learner to the new target domain.
</nextsent>
<nextsent>wu et al (2009)<papid> D09-1158 </papid>bootstrap the ner leaner with subset of unlabeled instances that bridge the source and target domains.</nextsent>
<nextsent>jiang and zhai (2006) <papid> N06-1010 </papid>and daume?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1342">
<title id=" E12-1017.xml">recall oriented learning of named entities in arabic wikipedia </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>nothman et al (2009) <papid> E09-1070 </papid>and balasuriya et al (2009) <papid> W09-3302 </papid>show that ner models trained on both automatically and manually annotated wikipedia corpora perform reasonably well on news corpora.</prevsent>
<prevsent>there verse scenario does not hold for models trained on news text, result we also observe in arabicner.</prevsent>
</prevsection>
<citsent citstr=" D07-1074 ">
other work has gone beyond the entity detection problem: florian et al (2004) <papid> N04-1001 </papid>additionally predict within-document entity coreference for arabic, chinese, and english ace text, while cucerzan (2007) <papid> D07-1074 </papid>aims to resolve every mention detected in english wikipedia pages to canonical article devoted to the entity in question.</citsent>
<aftsection>
<nextsent>the domain and topic diversity of nes has been studied in the framework of domain adaptationresearch.
</nextsent>
<nextsent>a group of these methods use self training and select the most informative features and training instances to adapt source domain learner to the new target domain.
</nextsent>
<nextsent>wu et al (2009)<papid> D09-1158 </papid>bootstrap the ner leaner with subset of unlabeled instances that bridge the source and target domains.</nextsent>
<nextsent>jiang and zhai (2006) <papid> N06-1010 </papid>and daume?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1343">
<title id=" E12-1017.xml">recall oriented learning of named entities in arabic wikipedia </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the domain and topic diversity of nes has been studied in the framework of domain adaptationresearch.
</prevsent>
<prevsent>a group of these methods use self training and select the most informative features and training instances to adapt source domain learner to the new target domain.
</prevsent>
</prevsection>
<citsent citstr=" D09-1158 ">
wu et al (2009)<papid> D09-1158 </papid>bootstrap the ner leaner with subset of unlabeled instances that bridge the source and target domains.</citsent>
<aftsection>
<nextsent>jiang and zhai (2006) <papid> N06-1010 </papid>and daume?</nextsent>
<nextsent>iii (2007) make use of some labeled target-domain data to tune or augment the features of the source model towards the target domain.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1344">
<title id=" E12-1017.xml">recall oriented learning of named entities in arabic wikipedia </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>a group of these methods use self training and select the most informative features and training instances to adapt source domain learner to the new target domain.
</prevsent>
<prevsent>wu et al (2009)<papid> D09-1158 </papid>bootstrap the ner leaner with subset of unlabeled instances that bridge the source and target domains.</prevsent>
</prevsection>
<citsent citstr=" N06-1010 ">
jiang and zhai (2006) <papid> N06-1010 </papid>and daume?</citsent>
<aftsection>
<nextsent>iii (2007) make use of some labeled target-domain data to tune or augment the features of the source model towards the target domain.
</nextsent>
<nextsent>here, in contrast, we use labeled target-domain data only for tuning and evaluation.
</nextsent>
<nextsent>another important distinction is that domain variation in this prior work is restricted to topically-related corpora (e.g. newswire vs. broadcast news), whereas in our work, major topical differences distinguish the training and test corpora and consequently, their salient ne classes.
</nextsent>
<nextsent>in these respects our ner setting is closer to that of florian et al (2010),<papid> D10-1033 </papid>who recognize english entities in noisy text, (sur deanu et al  2011), <papid> W11-0902 </papid>which concerns information extraction in topically distinct target domain, and (dalton et al  2011), which addresses english ner in noisy and topically divergent text.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1345">
<title id=" E12-1017.xml">recall oriented learning of named entities in arabic wikipedia </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>here, in contrast, we use labeled target-domain data only for tuning and evaluation.
</prevsent>
<prevsent>another important distinction is that domain variation in this prior work is restricted to topically-related corpora (e.g. newswire vs. broadcast news), whereas in our work, major topical differences distinguish the training and test corpora and consequently, their salient ne classes.
</prevsent>
</prevsection>
<citsent citstr=" D10-1033 ">
in these respects our ner setting is closer to that of florian et al (2010),<papid> D10-1033 </papid>who recognize english entities in noisy text, (sur deanu et al  2011), <papid> W11-0902 </papid>which concerns information extraction in topically distinct target domain, and (dalton et al  2011), which addresses english ner in noisy and topically divergent text.</citsent>
<aftsection>
<nextsent>self-training (clark et al  2003; <papid> W03-0407 </papid>mihalcea, 2004; <papid> W04-2405 </papid>mcclosky et al  2006) <papid> N06-1020 </papid>is widely used in nlp and has inspired related techniques that learn from automatically labeled data (liang et al ., 2008; petrov et al  2010).<papid> D10-1069 </papid></nextsent>
<nextsent>our self-training procedure differs from some others in that we use all of the automatically labeled examples, rather than filtering them based on confidence score.cost functions have been used in non structured classification settings to penalize certain types of errors more than others (chan and stolfo, 1998; domingos, 1999; kiddon and brun, 2011).<papid> P11-2016 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1346">
<title id=" E12-1017.xml">recall oriented learning of named entities in arabic wikipedia </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>here, in contrast, we use labeled target-domain data only for tuning and evaluation.
</prevsent>
<prevsent>another important distinction is that domain variation in this prior work is restricted to topically-related corpora (e.g. newswire vs. broadcast news), whereas in our work, major topical differences distinguish the training and test corpora and consequently, their salient ne classes.
</prevsent>
</prevsection>
<citsent citstr=" W11-0902 ">
in these respects our ner setting is closer to that of florian et al (2010),<papid> D10-1033 </papid>who recognize english entities in noisy text, (sur deanu et al  2011), <papid> W11-0902 </papid>which concerns information extraction in topically distinct target domain, and (dalton et al  2011), which addresses english ner in noisy and topically divergent text.</citsent>
<aftsection>
<nextsent>self-training (clark et al  2003; <papid> W03-0407 </papid>mihalcea, 2004; <papid> W04-2405 </papid>mcclosky et al  2006) <papid> N06-1020 </papid>is widely used in nlp and has inspired related techniques that learn from automatically labeled data (liang et al ., 2008; petrov et al  2010).<papid> D10-1069 </papid></nextsent>
<nextsent>our self-training procedure differs from some others in that we use all of the automatically labeled examples, rather than filtering them based on confidence score.cost functions have been used in non structured classification settings to penalize certain types of errors more than others (chan and stolfo, 1998; domingos, 1999; kiddon and brun, 2011).<papid> P11-2016 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1352">
<title id=" E12-1017.xml">recall oriented learning of named entities in arabic wikipedia </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>another important distinction is that domain variation in this prior work is restricted to topically-related corpora (e.g. newswire vs. broadcast news), whereas in our work, major topical differences distinguish the training and test corpora and consequently, their salient ne classes.
</prevsent>
<prevsent>in these respects our ner setting is closer to that of florian et al (2010),<papid> D10-1033 </papid>who recognize english entities in noisy text, (sur deanu et al  2011), <papid> W11-0902 </papid>which concerns information extraction in topically distinct target domain, and (dalton et al  2011), which addresses english ner in noisy and topically divergent text.</prevsent>
</prevsection>
<citsent citstr=" D10-1069 ">
self-training (clark et al  2003; <papid> W03-0407 </papid>mihalcea, 2004; <papid> W04-2405 </papid>mcclosky et al  2006) <papid> N06-1020 </papid>is widely used in nlp and has inspired related techniques that learn from automatically labeled data (liang et al ., 2008; petrov et al  2010).<papid> D10-1069 </papid></citsent>
<aftsection>
<nextsent>our self-training procedure differs from some others in that we use all of the automatically labeled examples, rather than filtering them based on confidence score.cost functions have been used in non structured classification settings to penalize certain types of errors more than others (chan and stolfo, 1998; domingos, 1999; kiddon and brun, 2011).<papid> P11-2016 </papid></nextsent>
<nextsent>the goal of optimizing our structured ner model for recall is quite similar to the scenario explored by minkov et al (2006), <papid> N06-2024 </papid>as noted above.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1353">
<title id=" E12-1017.xml">recall oriented learning of named entities in arabic wikipedia </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in these respects our ner setting is closer to that of florian et al (2010),<papid> D10-1033 </papid>who recognize english entities in noisy text, (sur deanu et al  2011), <papid> W11-0902 </papid>which concerns information extraction in topically distinct target domain, and (dalton et al  2011), which addresses english ner in noisy and topically divergent text.</prevsent>
<prevsent>self-training (clark et al  2003; <papid> W03-0407 </papid>mihalcea, 2004; <papid> W04-2405 </papid>mcclosky et al  2006) <papid> N06-1020 </papid>is widely used in nlp and has inspired related techniques that learn from automatically labeled data (liang et al ., 2008; petrov et al  2010).<papid> D10-1069 </papid></prevsent>
</prevsection>
<citsent citstr=" P11-2016 ">
our self-training procedure differs from some others in that we use all of the automatically labeled examples, rather than filtering them based on confidence score.cost functions have been used in non structured classification settings to penalize certain types of errors more than others (chan and stolfo, 1998; domingos, 1999; kiddon and brun, 2011).<papid> P11-2016 </papid></citsent>
<aftsection>
<nextsent>the goal of optimizing our structured ner model for recall is quite similar to the scenario explored by minkov et al (2006), <papid> N06-2024 </papid>as noted above.</nextsent>
<nextsent>we explored the problem of learning an ner model suited to domains for which no labeled training data are available.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1355">
<title id=" E09-2006.xml">ehumanities desktop  an online system for corpus management and analysis in support of computing in the humanities </title>
<section> applications.  </section>
<citcontext>
<prevsection>
<prevsent>as further linguistic application module lexical chainer (mehler, 2005; mehler et al, 2007; waltinger et al, 2008a; waltinger et al, 2008b)has been included in the online desktop environment.
</prevsent>
<prevsent>that is, semantically related tokens of given text can be tracked and connected by meansof lexical reference system.
</prevsent>
</prevsection>
<citsent citstr=" W97-0802 ">
the system currently uses two different terminological ontologies - wordnet (fellbaum, 1998) and germanet (hamp and feldweg, 1997) - <papid> W97-0802 </papid>as chaining resources which have been mapped onto the database for mat.</citsent>
<aftsection>
<nextsent>however the list of resources for chaining can easily be extended.
</nextsent>
<nextsent>23 3.4 lexicon exploration.
</nextsent>
<nextsent>with regards to lexicon exploration, the system aggregates different lexical resources including english, german and latin.
</nextsent>
<nextsent>in this module, not onlyco-occurrence data, social and terminological ontologies but also social tagging enhanced data are available forgiven input token.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1356">
<title id=" E06-2014.xml">assist automated semantic assistance for translators </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the problem we address in this paper is that of providing contextual examples of translation equivalents for words from the general lexicon using comparable corpora and semantic annotation that is uniform for the source and target languages.
</prevsent>
<prevsent>for sentence, phrase or query expression inthe source language the tool detects these mantic type of the situation in question and gives examples of similar contexts from the target language corpus.
</prevsent>
</prevsection>
<citsent citstr=" C00-2090 ">
it is widely acknowledged that human translators can benefit from wide range of applications in computational linguistics, including machine translation (carl and way, 2003), translation memory (planas and furuse, 2000), <papid> C00-2090 </papid>etc. there have been recent research on tools detecting translation equivalents for technical vocabulary in restricted domain, e.g.</citsent>
<aftsection>
<nextsent>(dagan and church, 1997; bennison and bowker, 2000).
</nextsent>
<nextsent>the methodology in this case is based on extraction of terminology (both single and multiword units) and alignment of extracted terms using linguistic and/or statistical techniques (djean et al, 2002).
</nextsent>
<nextsent>in this project we concentrate on words from the general lexicon instead of terminology.
</nextsent>
<nextsent>the rationale for this focus is related to the fact that translation of terms is (should be) stable, while general words can vary significantly in their translation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1358">
<title id=" E09-1016.xml">lexical morphology in machine translation a feasibility study </title>
<section> issues.  </section>
<citcontext>
<prevsection>
<prevsent>this last evaluation highlights set of methodological criteria that are needed to exploit lexical morphology in machine translation.
</prevsent>
<prevsent>unknown words are problematic issue in any nlp tool.
</prevsent>
</prevsection>
<citsent citstr=" C92-1063 ">
depending on the studies (ren and perrault 1992; <papid> C92-1063 </papid>maurel 2004), it is estimated that between 5 and 10 % of the words of text written in standard?</citsent>
<aftsection>
<nextsent>language are unknown to lexical resources.
</nextsent>
<nextsent>in mt context (analysis-transfer generation), unknown words remain not only un analysed but they cannot be translated, and sometimes they also stop the translation of the whole sentence.
</nextsent>
<nextsent>usually, three main groups of unknown words are distinguished: proper names, errors, and neologisms, and the possible solution highly depends on the type of unknown word to be solved.
</nextsent>
<nextsent>in this paper, we concentrate on neologisms which are constructed following morphological process.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1359">
<title id=" E09-1078.xml">natural language generation as planning under uncertainty for spoken dialogue systems </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>using many different expressions (how to say it?).
</prevsent>
<prevsent>in spoken dialogue system (sds), an abstract communicative goal (cg) can be generated in many different ways.
</prevsent>
</prevsection>
<citsent citstr=" P08-1055 ">
for example, the cg to present database results to the user can be realized as summary (polifroni and walker, 2008; <papid> P08-1055 </papid>demberg and moore, 2006), <papid> E06-1009 </papid>or by comparing items (walkeret al, 2004), or by picking one item and recommending it to the user (young et al, 2007).</citsent>
<aftsection>
<nextsent>previous work has shown that it is useful to adapt the generated output to certain features ofthe dialogue context, for example user preferences, e.g.
</nextsent>
<nextsent>(walker et al, 2004; demberg and moore, 2006), <papid> E06-1009 </papid>user knowledge, e.g.</nextsent>
<nextsent>(janarthanam and lemon, 2008), or predicted tts quality, e.g.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1360">
<title id=" E09-1078.xml">natural language generation as planning under uncertainty for spoken dialogue systems </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>using many different expressions (how to say it?).
</prevsent>
<prevsent>in spoken dialogue system (sds), an abstract communicative goal (cg) can be generated in many different ways.
</prevsent>
</prevsection>
<citsent citstr=" E06-1009 ">
for example, the cg to present database results to the user can be realized as summary (polifroni and walker, 2008; <papid> P08-1055 </papid>demberg and moore, 2006), <papid> E06-1009 </papid>or by comparing items (walkeret al, 2004), or by picking one item and recommending it to the user (young et al, 2007).</citsent>
<aftsection>
<nextsent>previous work has shown that it is useful to adapt the generated output to certain features ofthe dialogue context, for example user preferences, e.g.
</nextsent>
<nextsent>(walker et al, 2004; demberg and moore, 2006), <papid> E06-1009 </papid>user knowledge, e.g.</nextsent>
<nextsent>(janarthanam and lemon, 2008), or predicted tts quality, e.g.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1362">
<title id=" E09-1078.xml">natural language generation as planning under uncertainty for spoken dialogue systems </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(walker et al, 2004; demberg and moore, 2006), <papid> E06-1009 </papid>user knowledge, e.g.</prevsent>
<prevsent>(janarthanam and lemon, 2008), or predicted tts quality, e.g.</prevsent>
</prevsection>
<citsent citstr=" P06-1140 ">
(nakatsu and white, 2006).<papid> P06-1140 </papid></citsent>
<aftsection>
<nextsent>in extending this previous work we treat nlgas statistical sequential planning problem, analogously to current statistical approaches to dialogue management (dm), e.g.
</nextsent>
<nextsent>(singh et al, 2002; henderson et al, 2008; <papid> J08-4002 </papid>rieser and lemon, 2008<papid> P08-1073 </papid>a) and conversation as action under uncertainty?</nextsent>
<nextsent>(paek and horvitz, 2000).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1363">
<title id=" E09-1078.xml">natural language generation as planning under uncertainty for spoken dialogue systems </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(nakatsu and white, 2006).<papid> P06-1140 </papid></prevsent>
<prevsent>in extending this previous work we treat nlgas statistical sequential planning problem, analogously to current statistical approaches to dialogue management (dm), e.g.</prevsent>
</prevsection>
<citsent citstr=" J08-4002 ">
(singh et al, 2002; henderson et al, 2008; <papid> J08-4002 </papid>rieser and lemon, 2008<papid> P08-1073 </papid>a) and conversation as action under uncertainty?</citsent>
<aftsection>
<nextsent>(paek and horvitz, 2000).
</nextsent>
<nextsent>in nlg we have similar trade-offs and unpredictability as in dm, and in some systems the content planning and dm tasks are overlapping.
</nextsent>
<nextsent>clearly, very long system utterances with many actions in them are to be avoided, because users may become confused or impatient, but each individual nlg action will convey some (potentially) useful information tothe user.
</nextsent>
<nextsent>there is therefore an optimization problem to be solved.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1364">
<title id=" E09-1078.xml">natural language generation as planning under uncertainty for spoken dialogue systems </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(nakatsu and white, 2006).<papid> P06-1140 </papid></prevsent>
<prevsent>in extending this previous work we treat nlgas statistical sequential planning problem, analogously to current statistical approaches to dialogue management (dm), e.g.</prevsent>
</prevsection>
<citsent citstr=" P08-1073 ">
(singh et al, 2002; henderson et al, 2008; <papid> J08-4002 </papid>rieser and lemon, 2008<papid> P08-1073 </papid>a) and conversation as action under uncertainty?</citsent>
<aftsection>
<nextsent>(paek and horvitz, 2000).
</nextsent>
<nextsent>in nlg we have similar trade-offs and unpredictability as in dm, and in some systems the content planning and dm tasks are overlapping.
</nextsent>
<nextsent>clearly, very long system utterances with many actions in them are to be avoided, because users may become confused or impatient, but each individual nlg action will convey some (potentially) useful information tothe user.
</nextsent>
<nextsent>there is therefore an optimization problem to be solved.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1368">
<title id=" E09-1078.xml">natural language generation as planning under uncertainty for spoken dialogue systems </title>
<section> nlg as planning under uncertainty.  </section>
<citcontext>
<prevsection>
<prevsent>we adopt the general framework of nlg as planning under uncertainty (see (lemon, 2008) for the initial version of this approach).
</prevsent>
<prevsent>some aspects of nlg have been treated as planning, e.g.
</prevsent>
</prevsection>
<citsent citstr=" P07-1043 ">
(koller and stone, 2007; <papid> P07-1043 </papid>koller and petrick, 2008), but never before as statistical planning.nlg actions take place in stochastic environment, for example consisting of user, realizer,and tts system, where the individual nlg actions have uncertain effects on the environment.for example, presenting differing numbers of attributes to the user, and making the user more or less likely to choose an item, as shown by (rieser and lemon, 2008<papid> P08-1073 </papid>b) for multimodal interaction.most sds employ fixed template-based generation.</citsent>
<aftsection>
<nextsent>our goal, however, is to employ stochastic realizer for sds, see for example (stent et al, 2004).<papid> P04-1011 </papid></nextsent>
<nextsent>this will introduce additional noise, which higher level nlg decisions will need to react to.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1373">
<title id=" E09-1078.xml">natural language generation as planning under uncertainty for spoken dialogue systems </title>
<section> nlg as planning under uncertainty.  </section>
<citcontext>
<prevsection>
<prevsent>some aspects of nlg have been treated as planning, e.g.
</prevsent>
<prevsent>(koller and stone, 2007; <papid> P07-1043 </papid>koller and petrick, 2008), but never before as statistical planning.nlg actions take place in stochastic environment, for example consisting of user, realizer,and tts system, where the individual nlg actions have uncertain effects on the environment.for example, presenting differing numbers of attributes to the user, and making the user more or less likely to choose an item, as shown by (rieser and lemon, 2008<papid> P08-1073 </papid>b) for multimodal interaction.most sds employ fixed template-based generation.</prevsent>
</prevsection>
<citsent citstr=" P04-1011 ">
our goal, however, is to employ stochastic realizer for sds, see for example (stent et al, 2004).<papid> P04-1011 </papid></citsent>
<aftsection>
<nextsent>this will introduce additional noise, which higher level nlg decisions will need to react to.
</nextsent>
<nextsent>in our framework, the nlg component must achieve high-level communicative goal from the dialogue manager (e.g. to present number of items) through planning sequence of lower level generation steps or actions, for example first to summarize all the items and then to recommend the highest ranking one.
</nextsent>
<nextsent>each such action has unpredictable effects due to the stochastic realizer.
</nextsent>
<nextsent>for example the realizer might employ 6 attributes when recommending item i4, but it might use only2 (e.g. price and cuisine for restaurants), depending on its own processing constraints (see e.g. the realizer used to collect the match project data).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1392">
<title id=" E12-2004.xml">trans ahead a writing assistant for cat and call </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>more recently, interactive mt (imt) systems have begun to shift the users role from analyses of the source text to the formation of the target translation.
</prevsent>
<prevsent>trans type project (foster et al 2002) describes such pioneering system that supports next word predictions.
</prevsent>
</prevsection>
<citsent citstr=" P09-4005 ">
koehn (2009) <papid> P09-4005 </papid>develops caitra which displays one phrase translation at time and offers alternative translation options.</citsent>
<aftsection>
<nextsent>both systems are similar in spirit to our work.
</nextsent>
<nextsent>the main difference is that we do not expect the user to be professional translator and we provide translation hints along with grammar predictions to avoid the generalization issue facing phrase-based system.
</nextsent>
<nextsent>recent work has been done on using fully fledged statistical mt systems to produce target hypotheses completing user-validated translation prefix in imt paradigm.
</nextsent>
<nextsent>barrachina et al(2008) investigate the applicability of different mt kernels within imt framework.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1393">
<title id=" E12-2004.xml">trans ahead a writing assistant for cat and call </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>recent work has been done on using fully fledged statistical mt systems to produce target hypotheses completing user-validated translation prefix in imt paradigm.
</prevsent>
<prevsent>barrachina et al(2008) investigate the applicability of different mt kernels within imt framework.
</prevsent>
</prevsection>
<citsent citstr=" W04-3225 ">
nepveu et al (2004) <papid> W04-3225 </papid>and ortiz-martinez et al(2011) further exploit user feed backs for better imt systems and user experience.</citsent>
<aftsection>
<nextsent>instead of trigged by user correction, our method is triggered by word delimiter and assists in target language learning.
</nextsent>
<nextsent>in contrast to the previous cat research, we present writing assistant that suggests subsequent grammar constructs with translations and interactively collaborates with learners, in view of reducing users?
</nextsent>
<nextsent>burden on grammar and word choice and enhancing their writing quality.
</nextsent>
<nextsent>3.1 problem statement.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1394">
<title id=" E12-2004.xml">trans ahead a writing assistant for cat and call </title>
<section> the trans ahead system.  </section>
<citcontext>
<prevsection>
<prevsent>in the fourth and final stage, we exploit cst for bilingual phrase acquisition, rather than manual dictionary, to achieve better translation coverage and variety.
</prevsent>
<prevsent>we obtain phrase pairs through leveraging ibm models to word-align the bitexts, smoothing?
</prevsent>
</prevsection>
<citsent citstr=" N03-1017 ">
the directional word alignments via grow-diagonal-final, and extracting translation equivalents using (koehn et al 2003).<papid> N03-1017 </papid></citsent>
<aftsection>
<nextsent>3.3 run-time grammar and text prediction.
</nextsent>
<nextsent>once translation equivalents and phraseological tendencies are learned, trans ahead then predicts/suggests the following grammar and text of translation prefix given the source text using the procedure in figure 3.
</nextsent>
<nextsent>we first slice the source text and its translation prefix tp into character-level and 3 inspired by (gamon and leacock, 2010).<papid> W10-1005 </papid></nextsent>
<nextsent>word-level ngrams respectively.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1395">
<title id=" E12-2004.xml">trans ahead a writing assistant for cat and call </title>
<section> the trans ahead system.  </section>
<citcontext>
<prevsection>
<prevsent>3.3 run-time grammar and text prediction.
</prevsent>
<prevsent>once translation equivalents and phraseological tendencies are learned, trans ahead then predicts/suggests the following grammar and text of translation prefix given the source text using the procedure in figure 3.
</prevsent>
</prevsection>
<citsent citstr=" W10-1005 ">
we first slice the source text and its translation prefix tp into character-level and 3 inspired by (gamon and leacock, 2010).<papid> W10-1005 </papid></citsent>
<aftsection>
<nextsent>word-level ngrams respectively.
</nextsent>
<nextsent>step (3) and (4) retrieve the translations and patterns learned from section 3.2.
</nextsent>
<nextsent>step (3) acquires the active target-language vocabulary that may be used to translate the source text.
</nextsent>
<nextsent>to alleviate the word boundary issue in mt raised by ma et al(2007), <papid> P07-1039 </papid>trans ahead non-deterministically segments the source text using character ngrams and proceeds with collaborations with the user to obtain the segmentation for mt and to complete the translation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1396">
<title id=" E12-2004.xml">trans ahead a writing assistant for cat and call </title>
<section> the trans ahead system.  </section>
<citcontext>
<prevsection>
<prevsent>step (3) and (4) retrieve the translations and patterns learned from section 3.2.
</prevsent>
<prevsent>step (3) acquires the active target-language vocabulary that may be used to translate the source text.
</prevsent>
</prevsection>
<citsent citstr=" P07-1039 ">
to alleviate the word boundary issue in mt raised by ma et al(2007), <papid> P07-1039 </papid>trans ahead non-deterministically segments the source text using character ngrams and proceeds with collaborations with the user to obtain the segmentation for mt and to complete the translation.</citsent>
<aftsection>
<nextsent>note that user vocabulary of preference (due to users?
</nextsent>
<nextsent>domain of knowledge or errors of the system) may be exploited for better system performance.
</nextsent>
<nextsent>on the other hand, step (4) extracts patterns preceding with the history ngrams of {tj}.
</nextsent>
<nextsent>figure 3.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1397">
<title id=" E12-2004.xml">trans ahead a writing assistant for cat and call </title>
<section> preliminary results.  </section>
<citcontext>
<prevsection>
<prevsent>to evaluate trans ahead in cat and call, we introduced it to class of 34 (chinese) first year college students learning english as foreign language.
</prevsent>
<prevsent>designed to be intuitive to the general public, esp. language learners, presentational tutorial lasted only for minute.
</prevsent>
</prevsection>
<citsent citstr=" W11-1412 ">
after the tutorial, the participants were asked to translate 15 procedure patternfinding(query,n,ct) (1) interinvlist=findinvertedfile(w1 of query) for each word wi in query except for w1 (2) invlist=findinvertedfile(wi) (3a) newinterinvlist= ? ; i=1; j=1 (3b) while =length(interinvlist) and =lengh(invlist) (3c) if interinvlist[i].sentno==invlist[j].sentno (3d) insert(newinterinvlist, interinvlist[i],invlist[j]) else (3e) move i,j accordingly (3f) interinvlist=newinterinvlist (4) usage= ? for each element in interinvlist (5) usage+={patterngrammargeneration(element,ct)} (6) sort patterns in usage in descending order of frequency (7) return the patterns in usage with highest frequency procedure makeprediction(s,tp) (1) assign slicengram(s) to {si} (2) assign slicengram(tp) to {tj} (3) transoptions=findtranslation({si},tp) (4) gramoptions=findpattern({tj}) (5) evaluate translation options in trans options and incorporate them into gram options (6) return gram options 18 chinese texts from (huang et al 2011<papid> W11-1412 </papid>a) one by one (half with trans ahead assistance, and the other without).</citsent>
<aftsection>
<nextsent>encouragingly, the experimental group (i.e., with the help of our system) achieved much better translation quality than the control group in bleu (papineni et al 2002) (<papid> P02-1040 </papid>i.e., 35.49 vs. 26.46) and significantly reduced the performance gap between language learners and automatic decoder of google translate (44.82).</nextsent>
<nextsent>we noticed that, for the source ?????????</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1401">
<title id=" E12-2004.xml">trans ahead a writing assistant for cat and call </title>
<section> preliminary results.  </section>
<citcontext>
<prevsection>
<prevsent>designed to be intuitive to the general public, esp. language learners, presentational tutorial lasted only for minute.
</prevsent>
<prevsent>after the tutorial, the participants were asked to translate 15 procedure patternfinding(query,n,ct) (1) interinvlist=findinvertedfile(w1 of query) for each word wi in query except for w1 (2) invlist=findinvertedfile(wi) (3a) newinterinvlist= ? ; i=1; j=1 (3b) while =length(interinvlist) and =lengh(invlist) (3c) if interinvlist[i].sentno==invlist[j].sentno (3d) insert(newinterinvlist, interinvlist[i],invlist[j]) else (3e) move i,j accordingly (3f) interinvlist=newinterinvlist (4) usage= ? for each element in interinvlist (5) usage+={patterngrammargeneration(element,ct)} (6) sort patterns in usage in descending order of frequency (7) return the patterns in usage with highest frequency procedure makeprediction(s,tp) (1) assign slicengram(s) to {si} (2) assign slicengram(tp) to {tj} (3) transoptions=findtranslation({si},tp) (4) gramoptions=findpattern({tj}) (5) evaluate translation options in trans options and incorporate them into gram options (6) return gram options 18 chinese texts from (huang et al 2011<papid> W11-1412 </papid>a) one by one (half with trans ahead assistance, and the other without).</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
encouragingly, the experimental group (i.e., with the help of our system) achieved much better translation quality than the control group in bleu (papineni et al 2002) (<papid> P02-1040 </papid>i.e., 35.49 vs. 26.46) and significantly reduced the performance gap between language learners and automatic decoder of google translate (44.82).</citsent>
<aftsection>
<nextsent>we noticed that, for the source ?????????
</nextsent>
<nextsent>?????????, 90% of the participants in the experimental group finished with more grammatical and fluent translations (see figure 4) than (less interactive) google translate (we conclude this transaction plays an important role?).
</nextsent>
<nextsent>in comparison, 50% of the translations of the source from the control group were erroneous.
</nextsent>
<nextsent>figure 4.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1407">
<title id=" E09-1047.xml">parsing coordinations </title>
<section> experimental setup and baseline.  </section>
<citcontext>
<prevsection>
<prevsent>all experiments reported here are based on data split of 90% training data and 10% test data.
</prevsent>
<prevsent>3.2 the parsers and the reranker.
</prevsent>
</prevsection>
<citsent citstr=" C04-1024 ">
two parsers were used to investigate the influence of scope information on parser performance on coordinate structures: bitpar (schmid, 2004)<papid> C04-1024 </papid>and lopar (schmid, 2000).</citsent>
<aftsection>
<nextsent>bitpar is an efficient implementation of an earley style parser that uses bit vectors.
</nextsent>
<nextsent>however, bitpar cannot handle pre-bracketed input.
</nextsent>
<nextsent>for this reason, we used lopar for the experiments where such input was required.
</nextsent>
<nextsent>lopar, as it is used here, is pure pcfg parser, which allows the input to be partially bracketed.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1408">
<title id=" E09-1047.xml">parsing coordinations </title>
<section> experimental setup and baseline.  </section>
<citcontext>
<prevsection>
<prevsent>lopar, as it is used here, is pure pcfg parser, which allows the input to be partially bracketed.
</prevsent>
<prevsent>we are aware that the results that can be obtained by pure pcfg parsers are not state of the art as reported in the shared task of the acl 2008 workshop on parsing german (kubler, 2008).
</prevsent>
</prevsection>
<citsent citstr=" W08-1005 ">
while bitpar reaches an f-score of 69.76 (see next section), the best performing parser (petrov and klein, 2008) <papid> W08-1005 </papid>reaches an score of 83.97 on tuba-d/z (but with different split of training and test data).</citsent>
<aftsection>
<nextsent>however, our experiments require certain features in the parsers, namely the capability to provide n-best analyse sand to parse pre-bracketed input.
</nextsent>
<nextsent>to our knowledge, the parsers that took part in the shared task do not provide these features.
</nextsent>
<nextsent>should they become available, the methods presented here could be applied to such parsers.
</nextsent>
<nextsent>we see no reason why our 407 figure 1: tree with coordination.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1409">
<title id=" E09-1047.xml">parsing coordinations </title>
<section> experimental setup and baseline.  </section>
<citcontext>
<prevsection>
<prevsent>we see no reason why our 407 figure 1: tree with coordination.
</prevsent>
<prevsent>methods should not be able to improve the results of these parsers further.since we are interested in parsing coordinations, all experiments are conducted with goldpos tags, so as to abstract away from pos tagging errors.
</prevsent>
</prevsection>
<citsent citstr=" J05-1003 ">
although the treebank contains morphological information, this type of information is not used in the experiments presented here.the reranking experiments were conducted using the reranker by collins and koo (2005).<papid> J05-1003 </papid></citsent>
<aftsection>
<nextsent>this reranker uses set of candidate parses for sentence and reranks them based on set of features that are extracted from the trees.
</nextsent>
<nextsent>the reranker usesa boosting method based on the approach by freund et al (1998).
</nextsent>
<nextsent>we used similar feature set to the one collins and koo used; the following types of features were included: rules, bigrams, grandparent rules, grandparent bigrams, lexicalbigrams, two-level rules, two-level bigrams, trigrams, head-modifiers, pps, and distance for head modifier relations, as well as all feature types involving rules extended by closed class lexicalization.
</nextsent>
<nextsent>for more detailed description of the rules, the interested reader is referred to collins and koo (2005).<papid> J05-1003 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1417">
<title id=" E09-1047.xml">parsing coordinations </title>
<section> experiment 3: extracting scope.  </section>
<citcontext>
<prevsection>
<prevsent>another type of boundary that should notbe crossed is clause boundary.
</prevsent>
<prevsent>since the conjunction is part of the subordinated clause in the present example, the right conjunct cannot extend beyond the end of the clause, i.e. beyond nicht.
</prevsent>
</prevsection>
<citsent citstr=" C02-1131 ">
for this reason, we used karopars (muller and ule, 2002), <papid> C02-1131 </papid>partial parser for german, to parse the sentences.</citsent>
<aftsection>
<nextsent>from the partial parses, we extracted base phrases and clauses.
</nextsent>
<nextsent>for (6), the relevant bracketing provided by karopars is the fol lowing: (   es gibt zwar { ein paar niederflurbusse } , ) aber ( das reicht ja nicht )   , sagt er . the round parentheses mark clause boundaries, the curly braces the one base phrase that is longer than one word.
</nextsent>
<nextsent>in the creation of possible con juncts, only such conjuncts are listed that do not cross base phrase or clause boundaries.
</nextsent>
<nextsent>in order to avoid unreasonably high numbers of pre-bracketed versions, we also use higher level phrases, such as coordinated noun phrases.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1418">
<title id=" E09-1047.xml">parsing coordinations </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>he manages to reduce errors for simple cases of np coordination by introducing special phrasal category of basenps.
</prevsent>
<prevsent>in the experiments presented above, no explicit distinction is made between simple and complex cases of coordination, and no transformations are performed on the treebank annotations used for training.
</prevsent>
</prevsection>
<citsent citstr=" P05-1022 ">
our experiment 1, reranking 50-best parses, is similar to the approaches of charniak and johnson (2005) <papid> P05-1022 </papid>and of hogan (2007).<papid> P07-1086 </papid></citsent>
<aftsection>
<nextsent>however, it differs from their experiments in two crucial ways: 1) compared to charniak and johnson, who use 1.1 mio.
</nextsent>
<nextsent>features, our feature set is appr.
</nextsent>
<nextsent>five times larger (more than 5 mio.
</nextsent>
<nextsent>features), with the same threshold of at least five occurrences in the training set.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1419">
<title id=" E09-1047.xml">parsing coordinations </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>he manages to reduce errors for simple cases of np coordination by introducing special phrasal category of basenps.
</prevsent>
<prevsent>in the experiments presented above, no explicit distinction is made between simple and complex cases of coordination, and no transformations are performed on the treebank annotations used for training.
</prevsent>
</prevsection>
<citsent citstr=" P07-1086 ">
our experiment 1, reranking 50-best parses, is similar to the approaches of charniak and johnson (2005) <papid> P05-1022 </papid>and of hogan (2007).<papid> P07-1086 </papid></citsent>
<aftsection>
<nextsent>however, it differs from their experiments in two crucial ways: 1) compared to charniak and johnson, who use 1.1 mio.
</nextsent>
<nextsent>features, our feature set is appr.
</nextsent>
<nextsent>five times larger (more than 5 mio.
</nextsent>
<nextsent>features), with the same threshold of at least five occurrences in the training set.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1420">
<title id=" E09-1047.xml">parsing coordinations </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>features), with the same threshold of at least five occurrences in the training set.
</prevsent>
<prevsent>2) both hogan and charniak and johnson use special features for coordinate structures, such as boolean feature for marking parallelism (charniakand johnson) or for distinguishing between coordination of base nps and coordination of complex conjuncts (hogan), while our approach refrains from such special-purpose features.
</prevsent>
</prevsection>
<citsent citstr=" J94-4001 ">
our experiments using scope information are similar to the approaches of kurohashi and na gao (1994) <papid> J94-4001 </papid>and agarwal and bogges (1992) <papid> P92-1003 </papid>in that they try to identify coordinate structure bracket ings.</citsent>
<aftsection>
<nextsent>however, the techniques used by agarwal and bogges and in the present paper are quite different.
</nextsent>
<nextsent>agarwal and bogges and kurohashi andnagao relyon shallow parsing techniques to detect parallelism of conjuncts while we use partial parser only for suggesting possible scopes of conjuncts.
</nextsent>
<nextsent>both of these approaches are limited to coordinate structures with two conjuncts only,while our approach has no such limitation.
</nextsent>
<nextsent>moreover, the goal of agarwal and bogges is quite different from ours.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1421">
<title id=" E09-1047.xml">parsing coordinations </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>features), with the same threshold of at least five occurrences in the training set.
</prevsent>
<prevsent>2) both hogan and charniak and johnson use special features for coordinate structures, such as boolean feature for marking parallelism (charniakand johnson) or for distinguishing between coordination of base nps and coordination of complex conjuncts (hogan), while our approach refrains from such special-purpose features.
</prevsent>
</prevsection>
<citsent citstr=" P92-1003 ">
our experiments using scope information are similar to the approaches of kurohashi and na gao (1994) <papid> J94-4001 </papid>and agarwal and bogges (1992) <papid> P92-1003 </papid>in that they try to identify coordinate structure bracket ings.</citsent>
<aftsection>
<nextsent>however, the techniques used by agarwal and bogges and in the present paper are quite different.
</nextsent>
<nextsent>agarwal and bogges and kurohashi andnagao relyon shallow parsing techniques to detect parallelism of conjuncts while we use partial parser only for suggesting possible scopes of conjuncts.
</nextsent>
<nextsent>both of these approaches are limited to coordinate structures with two conjuncts only,while our approach has no such limitation.
</nextsent>
<nextsent>moreover, the goal of agarwal and bogges is quite different from ours.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1426">
<title id=" E12-1056.xml">subcatlmf fleshing out a standardized format for subcategorization frame interoperability </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the subcat lmf dtd, the conversion tools and the standardized versions of verbnet and ims lex subset are publicly available.1
</prevsent>
<prevsent>computational lexicons providing accuratelexical-syntactic information, such as subcategorization frames (scfs) are vital for many nlp applications involving parsing and word sense disambiguation.
</prevsent>
</prevsection>
<citsent citstr=" P07-2051 ">
in parsing, scfs have been successfully used to improve the output of statistical parsers (klenner (2007), <papid> P07-2051 </papid>deoskar (2008), <papid> C08-1025 </papid>sigogne et al(2011)) which is particularly significant in high-precision domain-independent parsing.</citsent>
<aftsection>
<nextsent>in word sense disambiguation, scfs have been identified as important features for verb sense disambiguation (brown et al 2011), <papid> W11-0110 </papid>which is due to the correlation of verb senses and scfs (andrew et al 2004).<papid> W04-3220 </papid></nextsent>
<nextsent>scfs specify syntactic arguments of verbs and other predicate-like lexemes, e.g. the verb say 1http://www.ukp.tu-darmstadt.de/data/ubytakes two arguments that can be realized, for instance, as noun phrase and that-clause as in he says that the window is open.although number of freely available, largescale and accurate scf lexicons exist, e.g. com lex (grishman et al 1994), <papid> C94-1042 </papid>verbnet (kipperet al 2008) for english, availability and limitations in size and coverage remain an inherent is sue.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1427">
<title id=" E12-1056.xml">subcatlmf fleshing out a standardized format for subcategorization frame interoperability </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the subcat lmf dtd, the conversion tools and the standardized versions of verbnet and ims lex subset are publicly available.1
</prevsent>
<prevsent>computational lexicons providing accuratelexical-syntactic information, such as subcategorization frames (scfs) are vital for many nlp applications involving parsing and word sense disambiguation.
</prevsent>
</prevsection>
<citsent citstr=" C08-1025 ">
in parsing, scfs have been successfully used to improve the output of statistical parsers (klenner (2007), <papid> P07-2051 </papid>deoskar (2008), <papid> C08-1025 </papid>sigogne et al(2011)) which is particularly significant in high-precision domain-independent parsing.</citsent>
<aftsection>
<nextsent>in word sense disambiguation, scfs have been identified as important features for verb sense disambiguation (brown et al 2011), <papid> W11-0110 </papid>which is due to the correlation of verb senses and scfs (andrew et al 2004).<papid> W04-3220 </papid></nextsent>
<nextsent>scfs specify syntactic arguments of verbs and other predicate-like lexemes, e.g. the verb say 1http://www.ukp.tu-darmstadt.de/data/ubytakes two arguments that can be realized, for instance, as noun phrase and that-clause as in he says that the window is open.although number of freely available, largescale and accurate scf lexicons exist, e.g. com lex (grishman et al 1994), <papid> C94-1042 </papid>verbnet (kipperet al 2008) for english, availability and limitations in size and coverage remain an inherent is sue.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1428">
<title id=" E12-1056.xml">subcatlmf fleshing out a standardized format for subcategorization frame interoperability </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>computational lexicons providing accuratelexical-syntactic information, such as subcategorization frames (scfs) are vital for many nlp applications involving parsing and word sense disambiguation.
</prevsent>
<prevsent>in parsing, scfs have been successfully used to improve the output of statistical parsers (klenner (2007), <papid> P07-2051 </papid>deoskar (2008), <papid> C08-1025 </papid>sigogne et al(2011)) which is particularly significant in high-precision domain-independent parsing.</prevsent>
</prevsection>
<citsent citstr=" W11-0110 ">
in word sense disambiguation, scfs have been identified as important features for verb sense disambiguation (brown et al 2011), <papid> W11-0110 </papid>which is due to the correlation of verb senses and scfs (andrew et al 2004).<papid> W04-3220 </papid></citsent>
<aftsection>
<nextsent>scfs specify syntactic arguments of verbs and other predicate-like lexemes, e.g. the verb say 1http://www.ukp.tu-darmstadt.de/data/ubytakes two arguments that can be realized, for instance, as noun phrase and that-clause as in he says that the window is open.although number of freely available, largescale and accurate scf lexicons exist, e.g. com lex (grishman et al 1994), <papid> C94-1042 </papid>verbnet (kipperet al 2008) for english, availability and limitations in size and coverage remain an inherent is sue.</nextsent>
<nextsent>this applies even more to languages other than english.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1429">
<title id=" E12-1056.xml">subcatlmf fleshing out a standardized format for subcategorization frame interoperability </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>computational lexicons providing accuratelexical-syntactic information, such as subcategorization frames (scfs) are vital for many nlp applications involving parsing and word sense disambiguation.
</prevsent>
<prevsent>in parsing, scfs have been successfully used to improve the output of statistical parsers (klenner (2007), <papid> P07-2051 </papid>deoskar (2008), <papid> C08-1025 </papid>sigogne et al(2011)) which is particularly significant in high-precision domain-independent parsing.</prevsent>
</prevsection>
<citsent citstr=" W04-3220 ">
in word sense disambiguation, scfs have been identified as important features for verb sense disambiguation (brown et al 2011), <papid> W11-0110 </papid>which is due to the correlation of verb senses and scfs (andrew et al 2004).<papid> W04-3220 </papid></citsent>
<aftsection>
<nextsent>scfs specify syntactic arguments of verbs and other predicate-like lexemes, e.g. the verb say 1http://www.ukp.tu-darmstadt.de/data/ubytakes two arguments that can be realized, for instance, as noun phrase and that-clause as in he says that the window is open.although number of freely available, largescale and accurate scf lexicons exist, e.g. com lex (grishman et al 1994), <papid> C94-1042 </papid>verbnet (kipperet al 2008) for english, availability and limitations in size and coverage remain an inherent is sue.</nextsent>
<nextsent>this applies even more to languages other than english.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1430">
<title id=" E12-1056.xml">subcatlmf fleshing out a standardized format for subcategorization frame interoperability </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in parsing, scfs have been successfully used to improve the output of statistical parsers (klenner (2007), <papid> P07-2051 </papid>deoskar (2008), <papid> C08-1025 </papid>sigogne et al(2011)) which is particularly significant in high-precision domain-independent parsing.</prevsent>
<prevsent>in word sense disambiguation, scfs have been identified as important features for verb sense disambiguation (brown et al 2011), <papid> W11-0110 </papid>which is due to the correlation of verb senses and scfs (andrew et al 2004).<papid> W04-3220 </papid></prevsent>
</prevsection>
<citsent citstr=" C94-1042 ">
scfs specify syntactic arguments of verbs and other predicate-like lexemes, e.g. the verb say 1http://www.ukp.tu-darmstadt.de/data/ubytakes two arguments that can be realized, for instance, as noun phrase and that-clause as in he says that the window is open.although number of freely available, largescale and accurate scf lexicons exist, e.g. com lex (grishman et al 1994), <papid> C94-1042 </papid>verbnet (kipperet al 2008) for english, availability and limitations in size and coverage remain an inherent is sue.</citsent>
<aftsection>
<nextsent>this applies even more to languages other than english.
</nextsent>
<nextsent>one particular approach to address this issue isthe combination and integration of existing manually built scf lexicons.
</nextsent>
<nextsent>lexicon integration has widely been adopted for increasing the coverage of lexicons regarding lexical-semantic information types, such as semantic roles, selectional restrictions, and word senses (e.g., shi and mihalcea (2005), the semlink project2, navigli and ponzetto (2010), <papid> P10-1023 </papid>niemann and gurevych (2011), <papid> W11-0122 </papid>meyer and gurevych (2011)).currently, scfs are represented idiosyncratic ally in existing scf lexicons.</nextsent>
<nextsent>however, integration of scfs requires common, interoperable representation format.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1431">
<title id=" E12-1056.xml">subcatlmf fleshing out a standardized format for subcategorization frame interoperability </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this applies even more to languages other than english.
</prevsent>
<prevsent>one particular approach to address this issue isthe combination and integration of existing manually built scf lexicons.
</prevsent>
</prevsection>
<citsent citstr=" P10-1023 ">
lexicon integration has widely been adopted for increasing the coverage of lexicons regarding lexical-semantic information types, such as semantic roles, selectional restrictions, and word senses (e.g., shi and mihalcea (2005), the semlink project2, navigli and ponzetto (2010), <papid> P10-1023 </papid>niemann and gurevych (2011), <papid> W11-0122 </papid>meyer and gurevych (2011)).currently, scfs are represented idiosyncratic ally in existing scf lexicons.</citsent>
<aftsection>
<nextsent>however, integration of scfs requires common, interoperable representation format.
</nextsent>
<nextsent>monolingual scf integration based on common representation format has already been addressed by king and crouch (2005) and just recently by necsulescu etal.
</nextsent>
<nextsent>(2011) and padro?
</nextsent>
<nextsent>et al(2011).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1432">
<title id=" E12-1056.xml">subcatlmf fleshing out a standardized format for subcategorization frame interoperability </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this applies even more to languages other than english.
</prevsent>
<prevsent>one particular approach to address this issue isthe combination and integration of existing manually built scf lexicons.
</prevsent>
</prevsection>
<citsent citstr=" W11-0122 ">
lexicon integration has widely been adopted for increasing the coverage of lexicons regarding lexical-semantic information types, such as semantic roles, selectional restrictions, and word senses (e.g., shi and mihalcea (2005), the semlink project2, navigli and ponzetto (2010), <papid> P10-1023 </papid>niemann and gurevych (2011), <papid> W11-0122 </papid>meyer and gurevych (2011)).currently, scfs are represented idiosyncratic ally in existing scf lexicons.</citsent>
<aftsection>
<nextsent>however, integration of scfs requires common, interoperable representation format.
</nextsent>
<nextsent>monolingual scf integration based on common representation format has already been addressed by king and crouch (2005) and just recently by necsulescu etal.
</nextsent>
<nextsent>(2011) and padro?
</nextsent>
<nextsent>et al(2011).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1433">
<title id=" E12-1056.xml">subcatlmf fleshing out a standardized format for subcategorization frame interoperability </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(2011) or padro?
</prevsent>
<prevsent>et al(2011) make use of existing standards in order to create uniform scf representation for lexicon merging.
</prevsent>
</prevsection>
<citsent citstr=" L08-1234 ">
the definition of an interoperable representation format according to an existing standard, such as the iso standard lexical markup framework (lmf, iso 24613:2008, see francopoulo et al(2006)), is the 2http://verbs.colorado.edu/semlink/ 550 prerequisite for re-using this format in different contexts, thus contributing to the standardization and interoperability of language resources.while lmf models exist that cover the representation of scfs (see quochi et al(2008),<papid> L08-1234 </papid>buitelaar et al(2009)), their suitability for representing scfs at large scale remains unclear: neither of these lmf-models has been used for standardizing lexicons with large number of scfs, such as verbnet.</citsent>
<aftsection>
<nextsent>furthermore, the question of their applicability to different languages has notbeen investigated yet, situation that is complicated by the fact that scfs are highly language specific.
</nextsent>
<nextsent>the goal of this paper is to address these gapsfor the two languages english and german by presenting uniform lmf representation of scfs for english and german which is utilized for the standardization of large-scale english and german scf lexicons.
</nextsent>
<nextsent>the contributions of this paper are threefold: (1) we present the lmf model subcat-lmf, an lmf-compliant lexicon representation format featuring uniform andvery fine-grained representation of scfs for english and german.
</nextsent>
<nextsent>subcat-lmf is subset of uby-lmf (eckle-kohler et al 2012), the lmf model of the large integrated lexical resource uby (gurevych et al 2012).<papid> E12-1059 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1434">
<title id=" E12-1056.xml">subcatlmf fleshing out a standardized format for subcategorization frame interoperability </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the goal of this paper is to address these gapsfor the two languages english and german by presenting uniform lmf representation of scfs for english and german which is utilized for the standardization of large-scale english and german scf lexicons.
</prevsent>
<prevsent>the contributions of this paper are threefold: (1) we present the lmf model subcat-lmf, an lmf-compliant lexicon representation format featuring uniform andvery fine-grained representation of scfs for english and german.
</prevsent>
</prevsection>
<citsent citstr=" E12-1059 ">
subcat-lmf is subset of uby-lmf (eckle-kohler et al 2012), the lmf model of the large integrated lexical resource uby (gurevych et al 2012).<papid> E12-1059 </papid></citsent>
<aftsection>
<nextsent>(2) we convert lexicons with large-scale scf information to subcat-lmf: the english verbnet and two german lexicons, i.e., germanet (kunze and lemnitzer, 2002) and subset of imslex3 (eckle-kohler, 1999).
</nextsent>
<nextsent>(3) we perform comparison of these three lexicons regarding scf coverage and scf overlap, based on the standardized representation.the remainder of this paper is structured as fol lows: section 2 gives detailed description ofsubcat-lmf and section 3 demonstrates its usefulness for representing and cross-lingually comparing large-scale english and german lexicons.
</nextsent>
<nextsent>section 4 provides discussion including related work and section 5 concludes.
</nextsent>
<nextsent>2.1 iso-lmf: meta-model.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1435">
<title id=" E12-1056.xml">subcatlmf fleshing out a standardized format for subcategorization frame interoperability </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>the authors state that it is presently unclear, how much overlap is required to obtain sufficiently precise merging results.
</prevsent>
<prevsent>standardizing scfs: much previous work on standardizing nlp lexicons in lmf has focused on wordnet-like resources.
</prevsent>
</prevsection>
<citsent citstr=" C10-1052 ">
soria et al(2009) describe wordnet-lmf, an lmf model for representing wordnets which has been used in the kyoto project.21 later, wordnet-lmf has been adapted by henrich and hinrichs (2010) <papid> C10-1052 </papid>to germanet and by toral et al(2010) to the italian wordnet.</citsent>
<aftsection>
<nextsent>wordnet-lmf does not provide the possibility to represent subcategorization atall.
</nextsent>
<nextsent>the adaption of wordnet-lmf to gn (henrich and hinrichs, 2010) <papid> C10-1052 </papid>allows scfs to be res presented as string values.</nextsent>
<nextsent>however, this extension is not sufficient, because it provides no means to model the syntax-semantics interface,which specifies correspondences between syntactic and semantic arguments of verbs and other predicates.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1438">
<title id=" E12-1056.xml">subcatlmf fleshing out a standardized format for subcategorization frame interoperability </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>as our cross-lingual comparison of lexicons has revealed many complementary scfs in vn, gn and ils, mono- and cross-lingual alignments of these lexicons at sense level would lead to major increase in scf coverage.
</prevsent>
<prevsent>moreover, the cross-lingually uniform representation of scfs can be exploited for an additional alignment of the lexicons at the level of scf arguments.
</prevsent>
</prevsection>
<citsent citstr=" J02-3001 ">
such fine-grained alignment of scfs can be used, for instance, to project vn semantic roles to gn, thus yielding german resource for semantic role labeling (see gildea and jurafsky (2002), <papid> J02-3001 </papid>swier and stevenson (2005)).<papid> H05-1111 </papid></citsent>
<aftsection>
<nextsent>subcat-lmf could be used for standardizing further english and german lexicons.
</nextsent>
<nextsent>the automatic conversion of lexicons to subcat-lmf requires the manual definition of mapping, at least for syntactic arguments.
</nextsent>
<nextsent>furthermore, the automatic merging approach by padro?
</nextsent>
<nextsent>et al(2011)could be tested for english: given our standardized version of vn, other english scf lexicons could be merged fully automatically with the subcat-lmf version of vn.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1439">
<title id=" E12-1056.xml">subcatlmf fleshing out a standardized format for subcategorization frame interoperability </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>as our cross-lingual comparison of lexicons has revealed many complementary scfs in vn, gn and ils, mono- and cross-lingual alignments of these lexicons at sense level would lead to major increase in scf coverage.
</prevsent>
<prevsent>moreover, the cross-lingually uniform representation of scfs can be exploited for an additional alignment of the lexicons at the level of scf arguments.
</prevsent>
</prevsection>
<citsent citstr=" H05-1111 ">
such fine-grained alignment of scfs can be used, for instance, to project vn semantic roles to gn, thus yielding german resource for semantic role labeling (see gildea and jurafsky (2002), <papid> J02-3001 </papid>swier and stevenson (2005)).<papid> H05-1111 </papid></citsent>
<aftsection>
<nextsent>subcat-lmf could be used for standardizing further english and german lexicons.
</nextsent>
<nextsent>the automatic conversion of lexicons to subcat-lmf requires the manual definition of mapping, at least for syntactic arguments.
</nextsent>
<nextsent>furthermore, the automatic merging approach by padro?
</nextsent>
<nextsent>et al(2011)could be tested for english: given our standardized version of vn, other english scf lexicons could be merged fully automatically with the subcat-lmf version of vn.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1440">
<title id=" E09-1070.xml">analysing wikipedia and gold standard corpora for ner training </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>named entity recognition (ner), the task of identifying and classifying the names of people, organisations and other entities within text, is central to many nlp systems.
</prevsent>
<prevsent>ner developed from information extraction in the message understanding conferences (muc) of the 1990s.
</prevsent>
</prevsection>
<citsent citstr=" M98-1001 ">
by muc 6 and 7, ner had become distinct task: tagging proper names, and temporal and numerical expressions (chinchor, 1998).<papid> M98-1001 </papid></citsent>
<aftsection>
<nextsent>statistical machine learning systems have proven successful for ner.
</nextsent>
<nextsent>these learn patterns associated with individual entity classes, making use of many contextual, orthographic, linguistic and external knowledge features.
</nextsent>
<nextsent>however,they rely heavily on large annotated training corpora.
</nextsent>
<nextsent>this need for costly expert annotation hinders the creation of more task-adaptable, high performance named entity recognisers.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1441">
<title id=" E09-1070.xml">analysing wikipedia and gold standard corpora for ner training </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we take this experience and apply it to corpus created automatically using wikipedia.
</prevsent>
<prevsent>this corpus was created following the method of nothman et al (2008).
</prevsent>
</prevsection>
<citsent citstr=" W03-0424 ">
by training the c&c; tagger (curran and clark, 2003) <papid> W03-0424 </papid>on the gold-standard corpora andour new wikipedia-derived training data, we evaluate the usefulness of the latter and explore the nature of the training corpus as variable in ner.our wikipedia-derived corpora exceed the performance of non-corresponding training and test sets by up to 11% -score, and can be engineered to automatically produce models consistent with various ne-annotation schema.</citsent>
<aftsection>
<nextsent>we show that it is possible to automatically create large, free, named entity-annotated corpora for general or domain specific tasks.
</nextsent>
<nextsent>research into ner has rarely considered the impact of training corpora.
</nextsent>
<nextsent>the conll evaluations focused on machine learning methods (tjongkim sang, 2002; tjong kim sang and de meulder, 2003) while more recent work has often involved the use of external knowledge.
</nextsent>
<nextsent>since many tagging systems utilise gazette ers of known entities, some research has focused on their automatic extraction from the web (etzioni et al, 2005) or wikipedia (toral et al, 2008), although mikheev et al (1999) <papid> E99-1001 </papid>and others have shown that larger ne lists do not necessarily correspond to increased ner performance.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1442">
<title id=" E09-1070.xml">analysing wikipedia and gold standard corpora for ner training </title>
<section> ner and annotated corpora.  </section>
<citcontext>
<prevsection>
<prevsent>research into ner has rarely considered the impact of training corpora.
</prevsent>
<prevsent>the conll evaluations focused on machine learning methods (tjongkim sang, 2002; tjong kim sang and de meulder, 2003) while more recent work has often involved the use of external knowledge.
</prevsent>
</prevsection>
<citsent citstr=" E99-1001 ">
since many tagging systems utilise gazette ers of known entities, some research has focused on their automatic extraction from the web (etzioni et al, 2005) or wikipedia (toral et al, 2008), although mikheev et al (1999) <papid> E99-1001 </papid>and others have shown that larger ne lists do not necessarily correspond to increased ner performance.</citsent>
<aftsection>
<nextsent>nadeau et al (2006) use such lists in an unsupervised ne recogniser, outperforming some entrants of the muc named entity task.
</nextsent>
<nextsent>unlike statistical approaches which learn 612 patterns associated with particular type of entity,these unsupervised approaches are limited to identifying common entities present in lists or those caught by hand-built rules.external knowledge has also been used to augment supervised ner approaches.
</nextsent>
<nextsent>kazama and torisawa (2007) <papid> D07-1073 </papid>improve their -score by 3% by including wikipedia-based feature in their machine learner.</nextsent>
<nextsent>such approaches are limited by the gold-standard data already available.less common is the automatic creation of training data.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1443">
<title id=" E09-1070.xml">analysing wikipedia and gold standard corpora for ner training </title>
<section> ner and annotated corpora.  </section>
<citcontext>
<prevsection>
<prevsent>nadeau et al (2006) use such lists in an unsupervised ne recogniser, outperforming some entrants of the muc named entity task.
</prevsent>
<prevsent>unlike statistical approaches which learn 612 patterns associated with particular type of entity,these unsupervised approaches are limited to identifying common entities present in lists or those caught by hand-built rules.external knowledge has also been used to augment supervised ner approaches.
</prevsent>
</prevsection>
<citsent citstr=" D07-1073 ">
kazama and torisawa (2007) <papid> D07-1073 </papid>improve their -score by 3% by including wikipedia-based feature in their machine learner.</citsent>
<aftsection>
<nextsent>such approaches are limited by the gold-standard data already available.less common is the automatic creation of training data.
</nextsent>
<nextsent>an et al (2003) <papid> P03-2031 </papid>extracted sentences containing listed entities from the web, and produc eda 1.8 million word korean corpus that gave similar results to manually-annotated training data.richman and schone (2008) <papid> P08-1001 </papid>used method similar to nothman et al (2008) in order to derivene-annotated corpora in languages other than en glish.</nextsent>
<nextsent>they classify wikipedia articles in foreign languages by transferring knowledge from english wikipedia via inter-language links.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1444">
<title id=" E09-1070.xml">analysing wikipedia and gold standard corpora for ner training </title>
<section> ner and annotated corpora.  </section>
<citcontext>
<prevsection>
<prevsent>kazama and torisawa (2007) <papid> D07-1073 </papid>improve their -score by 3% by including wikipedia-based feature in their machine learner.</prevsent>
<prevsent>such approaches are limited by the gold-standard data already available.less common is the automatic creation of training data.</prevsent>
</prevsection>
<citsent citstr=" P03-2031 ">
an et al (2003) <papid> P03-2031 </papid>extracted sentences containing listed entities from the web, and produc eda 1.8 million word korean corpus that gave similar results to manually-annotated training data.richman and schone (2008) <papid> P08-1001 </papid>used method similar to nothman et al (2008) in order to derivene-annotated corpora in languages other than en glish.</citsent>
<aftsection>
<nextsent>they classify wikipedia articles in foreign languages by transferring knowledge from english wikipedia via inter-language links.
</nextsent>
<nextsent>with these classifications they automatically annotate entire articles for ner training, and suggest that their results with 340k-word spanish corpus are comparable to 20k-40k words of gold-standard training data when using muc-style evaluation metrics.
</nextsent>
<nextsent>2.1 gold-standard corpora.
</nextsent>
<nextsent>we evaluate our wikipedia-derived corpora against three sets of manually-annotated data from (a) the muc-7 named entity task (muc, 2001); (b) the english conll-03 shared task (tjong kim sang and de meulder, 2003); (c) the bbn pronoun coreference and entity type corpus (weischedel and brun stein, 2005).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1445">
<title id=" E09-1070.xml">analysing wikipedia and gold standard corpora for ner training </title>
<section> ner and annotated corpora.  </section>
<citcontext>
<prevsection>
<prevsent>kazama and torisawa (2007) <papid> D07-1073 </papid>improve their -score by 3% by including wikipedia-based feature in their machine learner.</prevsent>
<prevsent>such approaches are limited by the gold-standard data already available.less common is the automatic creation of training data.</prevsent>
</prevsection>
<citsent citstr=" P08-1001 ">
an et al (2003) <papid> P03-2031 </papid>extracted sentences containing listed entities from the web, and produc eda 1.8 million word korean corpus that gave similar results to manually-annotated training data.richman and schone (2008) <papid> P08-1001 </papid>used method similar to nothman et al (2008) in order to derivene-annotated corpora in languages other than en glish.</citsent>
<aftsection>
<nextsent>they classify wikipedia articles in foreign languages by transferring knowledge from english wikipedia via inter-language links.
</nextsent>
<nextsent>with these classifications they automatically annotate entire articles for ner training, and suggest that their results with 340k-word spanish corpus are comparable to 20k-40k words of gold-standard training data when using muc-style evaluation metrics.
</nextsent>
<nextsent>2.1 gold-standard corpora.
</nextsent>
<nextsent>we evaluate our wikipedia-derived corpora against three sets of manually-annotated data from (a) the muc-7 named entity task (muc, 2001); (b) the english conll-03 shared task (tjong kim sang and de meulder, 2003); (c) the bbn pronoun coreference and entity type corpus (weischedel and brun stein, 2005).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1447">
<title id=" E09-1070.xml">analysing wikipedia and gold standard corpora for ner training </title>
<section> corpus and error analysis approaches.  </section>
<citcontext>
<prevsection>
<prevsent>a corpus can be considered in isolation or by comparison with other corpora.
</prevsent>
<prevsent>we use three methods to explore intra- and inter-corpus consistency in muc, conll, and bbn in section 4.
</prevsent>
</prevsection>
<citsent citstr=" E03-1068 ">
3.1 n-gram tag variation dickinson and meurers (2003) <papid> E03-1068 </papid>present clever method for finding inconsistencies within pos annotated corpora, which we apply toner corpora.</citsent>
<aftsection>
<nextsent>their approach finds all n-grams in corpus which appear multiple times, albeit with variant tags for some sub-sequence, the nucleus (see e.g. table3).
</nextsent>
<nextsent>to remove valid ambiguity, they suggest using (a) minimum n-gram length; (b) minimum margin of in variant terms around the nucleus.
</nextsent>
<nextsent>for example, the bbn train corpus includes eight occurrences of the 6-gram the san francisco bay area ,.
</nextsent>
<nextsent>six instances of area are tagged as non entities, but two instances are tagged as part of the loc that precedes it.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1449">
<title id=" E09-1083.xml">frequency matters pitch accents and information status </title>
<section> exemplar theory.  </section>
<citcontext>
<prevsection>
<prevsent>these stored exemplars are then employed in the categorisation of new input percepts.
</prevsent>
<prevsent>similarly, production is facilitated by accessing these stored exemplars.
</prevsent>
</prevsection>
<citsent citstr=" P03-1062 ">
computational models ofthe exemplar memory also argue that it is in constant state of flux with new inputs updating it andold unused exemplars gradually fading away (pier re humbert, 2001).up to now, virtually no exemplar-theoretic research has examined pitch accent prosody (butsee marsi et al (2003) <papid> P03-1062 </papid>for memory-based prediction of pitch accents and prosodic boundaries, and walsh et al (2008)(discussed below)) and to the authors?</citsent>
<aftsection>
<nextsent>knowledge this paper represents the first attempt to examine the relationship between pitch accent prosody and information status from anexemplar-theoretic perspective.
</nextsent>
<nextsent>given the considerable weight of evidence for the influence of frequency of occurrence effects in variety of other linguistic domains it seems reasonable to explore such effects on pitch accent and information sta 729 tus realisations.
</nextsent>
<nextsent>for example, what effect might givenness have on frequently/infrequently occur ring pitch accent?
</nextsent>
<nextsent>does novelty produce similar result the search for possible frequency of occurrence effects takes place with respect to pitch accent shapes captured by the parametric intonation model discussed next.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1450">
<title id=" E09-1075.xml">measuring frame relatedness </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>measuring relatedness among linguistic entities is crucial topic in nlp.
</prevsent>
<prevsent>automatically assessing the degree of similarity or relatedness between two words or two expressions, is of great help in variety of tasks, such as question answering, recognizing textual entailment (rte), information extraction and discourse processing.since the very beginning of computational linguistics, many studies have been devoted to the definition and the implementation of automatic measures for word relatedness (e.g.
</prevsent>
</prevsection>
<citsent citstr=" P98-2127 ">
(ruben stein and goodenough, 1965; resnik, 1995; lin, 1998; <papid> P98-2127 </papid>budanitsky and hirst, 2006; <papid> J06-1003 </papid>mohammad and hirst, 2006)).<papid> W06-1605 </papid></citsent>
<aftsection>
<nextsent>more recently, relatedness between lexical-syntactic patterns has also been studied (lin and pantel, 2001; szpektor et al,2004), <papid> W04-3206 </papid>to support advanced tasks such as paraphrasing and rte.</nextsent>
<nextsent>unfortunately, no attention has been paid so far to the definition of relatedness atthe more abstract situational level ? i.e. relatedness between two prototypical actions, events orstate-of-affairs, taken out of context (e.g. the situations of killing and death).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1451">
<title id=" E09-1075.xml">measuring frame relatedness </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>measuring relatedness among linguistic entities is crucial topic in nlp.
</prevsent>
<prevsent>automatically assessing the degree of similarity or relatedness between two words or two expressions, is of great help in variety of tasks, such as question answering, recognizing textual entailment (rte), information extraction and discourse processing.since the very beginning of computational linguistics, many studies have been devoted to the definition and the implementation of automatic measures for word relatedness (e.g.
</prevsent>
</prevsection>
<citsent citstr=" J06-1003 ">
(ruben stein and goodenough, 1965; resnik, 1995; lin, 1998; <papid> P98-2127 </papid>budanitsky and hirst, 2006; <papid> J06-1003 </papid>mohammad and hirst, 2006)).<papid> W06-1605 </papid></citsent>
<aftsection>
<nextsent>more recently, relatedness between lexical-syntactic patterns has also been studied (lin and pantel, 2001; szpektor et al,2004), <papid> W04-3206 </papid>to support advanced tasks such as paraphrasing and rte.</nextsent>
<nextsent>unfortunately, no attention has been paid so far to the definition of relatedness atthe more abstract situational level ? i.e. relatedness between two prototypical actions, events orstate-of-affairs, taken out of context (e.g. the situations of killing and death).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1452">
<title id=" E09-1075.xml">measuring frame relatedness </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>measuring relatedness among linguistic entities is crucial topic in nlp.
</prevsent>
<prevsent>automatically assessing the degree of similarity or relatedness between two words or two expressions, is of great help in variety of tasks, such as question answering, recognizing textual entailment (rte), information extraction and discourse processing.since the very beginning of computational linguistics, many studies have been devoted to the definition and the implementation of automatic measures for word relatedness (e.g.
</prevsent>
</prevsection>
<citsent citstr=" W06-1605 ">
(ruben stein and goodenough, 1965; resnik, 1995; lin, 1998; <papid> P98-2127 </papid>budanitsky and hirst, 2006; <papid> J06-1003 </papid>mohammad and hirst, 2006)).<papid> W06-1605 </papid></citsent>
<aftsection>
<nextsent>more recently, relatedness between lexical-syntactic patterns has also been studied (lin and pantel, 2001; szpektor et al,2004), <papid> W04-3206 </papid>to support advanced tasks such as paraphrasing and rte.</nextsent>
<nextsent>unfortunately, no attention has been paid so far to the definition of relatedness atthe more abstract situational level ? i.e. relatedness between two prototypical actions, events orstate-of-affairs, taken out of context (e.g. the situations of killing and death).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1453">
<title id=" E09-1075.xml">measuring frame relatedness </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>automatically assessing the degree of similarity or relatedness between two words or two expressions, is of great help in variety of tasks, such as question answering, recognizing textual entailment (rte), information extraction and discourse processing.since the very beginning of computational linguistics, many studies have been devoted to the definition and the implementation of automatic measures for word relatedness (e.g.
</prevsent>
<prevsent>(ruben stein and goodenough, 1965; resnik, 1995; lin, 1998; <papid> P98-2127 </papid>budanitsky and hirst, 2006; <papid> J06-1003 </papid>mohammad and hirst, 2006)).<papid> W06-1605 </papid></prevsent>
</prevsection>
<citsent citstr=" W04-3206 ">
more recently, relatedness between lexical-syntactic patterns has also been studied (lin and pantel, 2001; szpektor et al,2004), <papid> W04-3206 </papid>to support advanced tasks such as paraphrasing and rte.</citsent>
<aftsection>
<nextsent>unfortunately, no attention has been paid so far to the definition of relatedness atthe more abstract situational level ? i.e. relatedness between two prototypical actions, events orstate-of-affairs, taken out of context (e.g. the situations of killing and death).
</nextsent>
<nextsent>a prominent definition of prototypical situation?
</nextsent>
<nextsent>is given in frame semantics (fillmore, 1985), where situation is modelled as conceptual structure (a frame) constituted by the predicates that can evoke the situation, and the semantic roles expressing the situations participants.as measures of word relatedness help in discovering if two word occurrences express related concepts, so measures of frame relatedness should help to discover if two large text fragments are related or talk about similar situations.
</nextsent>
<nextsent>such measures would be valuable in many tasks.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1462">
<title id=" E09-1075.xml">measuring frame relatedness </title>
<section> defining frame relatedness.  </section>
<citcontext>
<prevsection>
<prevsent>these are the participants and props in the abstract situation described.
</prevsent>
<prevsent>roles are local to individual frames, thus avoiding the commitment to small set of universal roles, whose specification has turned out to be unfeasible in the past.
</prevsent>
</prevsection>
<citsent citstr=" P98-1013 ">
the berkeley framenet project (baker et al,1998) <papid> P98-1013 </papid>has been developing frame-semantic lexicon for the core vocabulary of english since 1997.</citsent>
<aftsection>
<nextsent>the current framenet release contains about 800 frames and 10,000 lexical units.
</nextsent>
<nextsent>part of framenet 658 frame: statement this frame contains verbs and nouns that communicate the act of speaker to address message to some addressee using language.
</nextsent>
<nextsent>a number of the words can be used performatively, such as declare and insist.
</nextsent>
<nextsent>speaker evelyn said she wanted to leave.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1463">
<title id=" E09-1075.xml">measuring frame relatedness </title>
<section> defining frame relatedness.  </section>
<citcontext>
<prevsection>
<prevsent>the card contained the following information about the two frames: names;definitions; the lists of core fes; frame annotated sentence for each frame, randomly chosen from the framenet database.
</prevsent>
<prevsent>similarly to rubenstein and goodenough (1965) we gave the annotators the following instructions: (i) after looking through the whole deck, order the pairs according to amount of relatedness; (ii) you may assign thesame rank to pairs having the same degree of relatedness (i.e. ties are allowed).we checked the agreement among the 15 annotators in ranking the 5 shared pairs by using the ken dalls ? correlation coefficient (kendall,1938).
</prevsent>
</prevsection>
<citsent citstr=" J06-4002 ">
ken dalls ? can be interpreted as the difference between the probability that in the dataset two variables are in the same order versus the probability that they are in different orders (see(lapata, 2006) <papid> J06-4002 </papid>for details).</citsent>
<aftsection>
<nextsent>the average corre1a scenario frame is hub?
</nextsent>
<nextsent>frame describing general topic; specific frames modelling situations related to the topic are linked to it (e.g. commerce buy and commercial transaction are linked to commerce scenario).
</nextsent>
<nextsent>framenet contains 16 scenarios.
</nextsent>
<nextsent>659lation2 among annotators on the simple and controlled sets was ? = 0.600 and ? = 0.547.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1465">
<title id=" E09-1075.xml">measuring frame relatedness </title>
<section> measures for frame relatedness.  </section>
<citcontext>
<prevsection>
<prevsent>ideally, one could solve the problem by using sense-tagged corpus where senses?
</prevsent>
<prevsent>occurrences are mapped to frames.
</prevsent>
</prevsection>
<citsent citstr=" H93-1061 ">
while sense to-frame mappings exist (e.g. mapping between 4for sake of simplicity in the rest of the section we refer to documents, but the same holds for sentences.frames and wordnet senses in (shi and mihalcea, 2005)), sense-tagged corpora large enough for distributional studies are not yet available (e.g., the semcor wordnet-tagged corpus (miller et al, 1993) <papid> H93-1061 </papid>consists of only 700,000 words).we therefore circumvent the problem, by implementing pmi in weighted co-occurrence measure, which gives lower weights to co-occurrences of ambiguous words: cr wgt(f1, f2) = log2 ? ccf1,f2 wf1(c) ? wf2(c) ? ccf1 wf1(c) ? ?</citsent>
<aftsection>
<nextsent>ccf2 wf2(c) (6) the weighting function wf (c) estimates the probability that the document contains luof the frame in the correct sense.
</nextsent>
<nextsent>formally, given the set of senses sl of lu (e.g. charge.v#1...charge.v#24), we define slf as the set of senses mapping to the frame (e.g. charge.v#3for commerce collect).
</nextsent>
<nextsent>the weighting function is then: wf (c) = argmax lflf in p (slf |lf ) (7) where lf is the set of lus of . we estimate (slf |lf ) by counting sense occurrences of lf over the semcor corpus: (slf |lf ) = |slf | |sl| (8) in other terms, frame receives high weight in document when the document contains lu whose most frequent senses are those mapped to the frame.5 for example, in the sentence: tripp isenhour was charged with killing hawk on pur pose.?, wf (c) = 0.17, as charge.v#3 is not very frequent in semcor.
</nextsent>
<nextsent>5in eq.8 we use lid stone smoothing (lidstone, 1920) to account for unseen senses in semcor.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1467">
<title id=" E09-1075.xml">measuring frame relatedness </title>
<section> measures for frame relatedness.  </section>
<citcontext>
<prevsection>
<prevsent>similarly to measures for word relatedness, we here compute frame relatedness leveraging graph-based measures over the framenet hierarchy.
</prevsent>
<prevsent>the intuition is that the closer in the hierarchy two frames are, the more related they are6.
</prevsent>
</prevsection>
<citsent citstr=" P94-1019 ">
we here experiment with the hirst-st.onge andthe wu and palmer (wu and palmer, 1994) <papid> P94-1019 </papid>measures, as they are pure taxonomic measures, i.e. they do not require any corpus statistics.6the pathfinder through framenet tool gives practical proof of this intuition: http://fnps.coli.</citsent>
<aftsection>
<nextsent>uni-saarland.de/pathsearch.wu and palmer: this measure calculates relatedness by considering the depths of the two frames in the hierarchy, along with the depth of their least common subsumer (lcs): hr wu(f1, f2) = 2dp(lcs) ln(f1, lcs)+ln(f2, lcs)+2dp(lcs) (11) where ln is the length of the path connecting two frames, and dp is the length of the path between frame and root.
</nextsent>
<nextsent>if path does not exist, then hr wu(f1, f2) = 0.
</nextsent>
<nextsent>hirst-st.onge: two frames are semantically close if they are connected in the framenet hierarchy through not too long path which does not change direction too often?: hr hso(f1, f2) = m?
</nextsent>
<nextsent>path length d (12) where and and are constants, and is the number of changes of direction in the path.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1468">
<title id=" E09-1075.xml">measuring frame relatedness </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>finally, wordnet has in general an incomplete lexical coverage: shi and mihalcea (2005) show that 7% of framenet verbal lus do not have mapping in wordnet.
</prevsent>
<prevsent>corpus-based measures.
</prevsent>
</prevsection>
<citsent citstr=" H05-1113 ">
table 3 shows that co-occurrence measures are effective when using10the average level of correlation obtained by our measures is comparable to that obtained in other complex information-ordering tasks, e.g. measuring compositionality of verb-noun coll ations (venkatapathy and joshi, 2005) <papid> H05-1113 </papid>663 wn jcn cr wgt sent hr fe ambient temperature - temperature (4) change of phase - cause change of phase (7) shoot projectiles - use firearm (1),  use firearm (5) run risk - endangering (27) knot creation - rope manipulation (1),  rope manipulation (5) intentionally affect - rope manipulation (37),  rope manipulation (5) run risk - safe situation (51) ambient temperature - temperature (4) knot creation - rope manipulation (1),  rope manipulation (5) knot creation - rope manipulation (1),  rope manipulation (5) shoot projectiles - use firearm (1),  use firearm (5) ambient temperature - temperature (4) endangering - safe situation (62) hit target - use firearm (18) hit target - intentionally affect (91),  intentionally affect (5) shoot projectiles - use firearm (1),  use firearm (5) run risk - safe situation (51) safe situation - security (28) scouring - scrutiny (3) safe situation - security (28) suspicion - criminal investigation (40) reliance - contingency (109) cause impact - hit target (10) age - speed (113) safe situation - security (28) rape - arson (22) motion noise - motion directional (55) change of phase - cause change of phase (7) suspicion - robbery (98) body movement - motion (45) table 4: first 10 ranked frame pairs for different relatedness measure on the controlled set; in brackets, the rankin the gold standard (full list available at (suppressed)).</citsent>
<aftsection>
<nextsent>sentences as contexts, while correlation decreases by about 10 points using documents as contexts.this suggest that sentences are suitable contextual units to model situational relatedness, while documents (i.e. news) may be so large to include unrelated situations.
</nextsent>
<nextsent>it is interesting to notice that corpus-based measures promote frame pairs which are in non-hierarchical relation, more than other measures do.
</nextsent>
<nextsent>for example the pair change of phase - cause change of phase score first, and rape - arson score ninth, while the other measures tend to rank them much lower.by contrast, the two frames scouring - inspecting which are siblings in the framenet hierarchy and rank 17th in the gold standard, are ranked only 126th by cr wgt sent.
</nextsent>
<nextsent>this is due to the fact that hierarchically related frames are substitutional ? i.e. they tend not to co-occur in the same documents; while otherwise related frames are mostly in syntagmatic relation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1472">
<title id=" E09-1075.xml">measuring frame relatedness </title>
<section> conclusions.  </section>
<citcontext>
<prevsection>
<prevsent>results show that our measures have good performance, all statistically significant at the 99% level, though improvements are expected by using other evidence.
</prevsent>
<prevsent>as future work, we will build up and refine these basic measures, and investigate more complex ones.
</prevsent>
</prevsection>
<citsent citstr=" W07-2018 ">
we will also use our measures in applications, to check their effectiveness in supporting various tasks, e.g. in mapping frames across text and hypothesis in rte, in linking related frames in discourse, or in inducing frames for lu which are not in framenet (baker et al, 2007).<papid> W07-2018 </papid></citsent>
<aftsection>
<nextsent>664
</nextsent>



</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1473">
<title id=" E12-1014.xml">toward statistical machine translation without parallel corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we examine the degradation in translation performance when bilinguallyestimated translation probabilities are removed and show that 80%+ of the loss can be recovered with monolingually estimated features alone.
</prevsent>
<prevsent>we further show that our monolingual features add 1.5 bleu points when combined with standard bilingually estimated phrase table features.
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
the parameters of statistical models of translation are typically estimated from large bilingual parallel corpora (brown et al 1993).<papid> J93-2003 </papid></citsent>
<aftsection>
<nextsent>however,these resources are not available for most language pairs, and they are expensive to produce in quantities sufficient for building good translation system (germann, 2001).<papid> W01-1409 </papid></nextsent>
<nextsent>we attempt an entirely different approach; we use cheap and plentiful monolingual resources to induce an end-toend statistical machine translation system.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1475">
<title id=" E12-1014.xml">toward statistical machine translation without parallel corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we further show that our monolingual features add 1.5 bleu points when combined with standard bilingually estimated phrase table features.
</prevsent>
<prevsent>the parameters of statistical models of translation are typically estimated from large bilingual parallel corpora (brown et al 1993).<papid> J93-2003 </papid></prevsent>
</prevsection>
<citsent citstr=" W01-1409 ">
however,these resources are not available for most language pairs, and they are expensive to produce in quantities sufficient for building good translation system (germann, 2001).<papid> W01-1409 </papid></citsent>
<aftsection>
<nextsent>we attempt an entirely different approach; we use cheap and plentiful monolingual resources to induce an end-toend statistical machine translation system.
</nextsent>
<nextsent>in particular, we extend the long line of work on inducing translation lexicons (beginning with rapp (1995)) <papid> P95-1050 </papid>and propose to use multiple independent cues present in monolingual texts to estimate lexical and phrasal translation probabilities for large, mt-scale phrase-tables.</nextsent>
<nextsent>we then introduce novel algorithm to estimate reordering features from monolingual data alone, and we report the performance of phrase-based statistical model(koehn et al 2003) estimated using these monolingual features.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1476">
<title id=" E12-1014.xml">toward statistical machine translation without parallel corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however,these resources are not available for most language pairs, and they are expensive to produce in quantities sufficient for building good translation system (germann, 2001).<papid> W01-1409 </papid></prevsent>
<prevsent>we attempt an entirely different approach; we use cheap and plentiful monolingual resources to induce an end-toend statistical machine translation system.</prevsent>
</prevsection>
<citsent citstr=" P95-1050 ">
in particular, we extend the long line of work on inducing translation lexicons (beginning with rapp (1995)) <papid> P95-1050 </papid>and propose to use multiple independent cues present in monolingual texts to estimate lexical and phrasal translation probabilities for large, mt-scale phrase-tables.</citsent>
<aftsection>
<nextsent>we then introduce novel algorithm to estimate reordering features from monolingual data alone, and we report the performance of phrase-based statistical model(koehn et al 2003) estimated using these monolingual features.
</nextsent>
<nextsent>most of the prior work on lexicon induction is motivated by the idea that it could be applied to machine translation but stops short of actually doing so.
</nextsent>
<nextsent>lexicon induction holds the potential to create machine translation systems for languages which do not have extensive parallel corpora.
</nextsent>
<nextsent>training would only require two large monolingual corpora and small bilingual dictionary, if one is available.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1479">
<title id=" E12-1014.xml">toward statistical machine translation without parallel corpora </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>2.1 parameters of phrase-based smt.
</prevsent>
<prevsent>statistical machine translation (smt) was first formulated as series of probabilistic models that learn word-to-word correspondences from sentence-aligned bilingual parallel corpora(brown et al 1993).<papid> J93-2003 </papid></prevsent>
</prevsection>
<citsent citstr=" P05-1033 ">
current methods, including phrase-based (och, 2002; koehn et al 2003) and hierarchical models (chiang, 2005), <papid> P05-1033 </papid>typically start by word-aligning bilingual parallel corpus (och and ney, 2003).<papid> J03-1002 </papid></citsent>
<aftsection>
<nextsent>they extract multiword phrases that are consistent with the viterbi word alignments and use these phrases to build new translations.
</nextsent>
<nextsent>a variety of parameters are estimated using the bitexts.
</nextsent>
<nextsent>here we review the parameters of the standard phrase-based translation model (koehn et al 2007).<papid> P07-2045 </papid></nextsent>
<nextsent>later we will showhow to estimate them using monolingual texts in stead.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1480">
<title id=" E12-1014.xml">toward statistical machine translation without parallel corpora </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>2.1 parameters of phrase-based smt.
</prevsent>
<prevsent>statistical machine translation (smt) was first formulated as series of probabilistic models that learn word-to-word correspondences from sentence-aligned bilingual parallel corpora(brown et al 1993).<papid> J93-2003 </papid></prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
current methods, including phrase-based (och, 2002; koehn et al 2003) and hierarchical models (chiang, 2005), <papid> P05-1033 </papid>typically start by word-aligning bilingual parallel corpus (och and ney, 2003).<papid> J03-1002 </papid></citsent>
<aftsection>
<nextsent>they extract multiword phrases that are consistent with the viterbi word alignments and use these phrases to build new translations.
</nextsent>
<nextsent>a variety of parameters are estimated using the bitexts.
</nextsent>
<nextsent>here we review the parameters of the standard phrase-based translation model (koehn et al 2007).<papid> P07-2045 </papid></nextsent>
<nextsent>later we will showhow to estimate them using monolingual texts in stead.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1481">
<title id=" E12-1014.xml">toward statistical machine translation without parallel corpora </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>they extract multiword phrases that are consistent with the viterbi word alignments and use these phrases to build new translations.
</prevsent>
<prevsent>a variety of parameters are estimated using the bitexts.
</prevsent>
</prevsection>
<citsent citstr=" P07-2045 ">
here we review the parameters of the standard phrase-based translation model (koehn et al 2007).<papid> P07-2045 </papid></citsent>
<aftsection>
<nextsent>later we will showhow to estimate them using monolingual texts instead.
</nextsent>
<nextsent>these parameters are: ? phrase pairs.
</nextsent>
<nextsent>phrase extraction heuristics (venugopal et al 2003; <papid> P03-1041 </papid>tillmann, 2003; <papid> W03-1001 </papid>och and ney, 2004) <papid> J04-4002 </papid>produce set of phrase pairs (e, f) that are consistent with the word alignments.</nextsent>
<nextsent>in this paper we assume that the phrase pairs are given (without any scores), and we induce every other parameter of the phrase-based model from monolingual data.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1482">
<title id=" E12-1014.xml">toward statistical machine translation without parallel corpora </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>later we will showhow to estimate them using monolingual texts instead.
</prevsent>
<prevsent>these parameters are: ? phrase pairs.
</prevsent>
</prevsection>
<citsent citstr=" P03-1041 ">
phrase extraction heuristics (venugopal et al 2003; <papid> P03-1041 </papid>tillmann, 2003; <papid> W03-1001 </papid>och and ney, 2004) <papid> J04-4002 </papid>produce set of phrase pairs (e, f) that are consistent with the word alignments.</citsent>
<aftsection>
<nextsent>in this paper we assume that the phrase pairs are given (without any scores), and we induce every other parameter of the phrase-based model from monolingual data.
</nextsent>
<nextsent>phrase translation probabilities.
</nextsent>
<nextsent>each phrase pair has list of associated feature functions (ffs).
</nextsent>
<nextsent>these include phrase translation probabilities, ?(e|f) and ?(f |e), which are typically calculated via maximum likelihood estimation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1483">
<title id=" E12-1014.xml">toward statistical machine translation without parallel corpora </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>later we will showhow to estimate them using monolingual texts instead.
</prevsent>
<prevsent>these parameters are: ? phrase pairs.
</prevsent>
</prevsection>
<citsent citstr=" W03-1001 ">
phrase extraction heuristics (venugopal et al 2003; <papid> P03-1041 </papid>tillmann, 2003; <papid> W03-1001 </papid>och and ney, 2004) <papid> J04-4002 </papid>produce set of phrase pairs (e, f) that are consistent with the word alignments.</citsent>
<aftsection>
<nextsent>in this paper we assume that the phrase pairs are given (without any scores), and we induce every other parameter of the phrase-based model from monolingual data.
</nextsent>
<nextsent>phrase translation probabilities.
</nextsent>
<nextsent>each phrase pair has list of associated feature functions (ffs).
</nextsent>
<nextsent>these include phrase translation probabilities, ?(e|f) and ?(f |e), which are typically calculated via maximum likelihood estimation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1484">
<title id=" E12-1014.xml">toward statistical machine translation without parallel corpora </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>later we will showhow to estimate them using monolingual texts instead.
</prevsent>
<prevsent>these parameters are: ? phrase pairs.
</prevsent>
</prevsection>
<citsent citstr=" J04-4002 ">
phrase extraction heuristics (venugopal et al 2003; <papid> P03-1041 </papid>tillmann, 2003; <papid> W03-1001 </papid>och and ney, 2004) <papid> J04-4002 </papid>produce set of phrase pairs (e, f) that are consistent with the word alignments.</citsent>
<aftsection>
<nextsent>in this paper we assume that the phrase pairs are given (without any scores), and we induce every other parameter of the phrase-based model from monolingual data.
</nextsent>
<nextsent>phrase translation probabilities.
</nextsent>
<nextsent>each phrase pair has list of associated feature functions (ffs).
</nextsent>
<nextsent>these include phrase translation probabilities, ?(e|f) and ?(f |e), which are typically calculated via maximum likelihood estimation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1485">
<title id=" E12-1014.xml">toward statistical machine translation without parallel corpora </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>other typical features are n-gram language model scores and phrase penalty, which governs whether to use fewer longer phrases or more shorter phrases.
</prevsent>
<prevsent>these are not bilingually estimated, so wecan re-use them directly without modification.
</prevsent>
</prevsection>
<citsent citstr=" P03-1021 ">
the features are combined in loglinear model, and their weights are set through minimum error rate training (och, 2003).<papid> P03-1021 </papid></citsent>
<aftsection>
<nextsent>we use the same loglinear formulation and mert but propose alternatives derived directly from monolingual data forall parameters except for the phrase pairs themselves.
</nextsent>
<nextsent>our pipeline still requires small bitext of approximately 2,000 sentences to use as development set for mert parameter tuning.
</nextsent>
<nextsent>131 2.2 bilingual lexicon induction for smt.
</nextsent>
<nextsent>bilingual lexicon induction describes the class of algorithms that attempt to learn translations from monolingual corpora.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1487">
<title id=" E12-1014.xml">toward statistical machine translation without parallel corpora </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>bilingual lexicon induction describes the class of algorithms that attempt to learn translations from monolingual corpora.
</prevsent>
<prevsent>rapp (1995) <papid> P95-1050 </papid>was the first to propose using non-parallel texts to learn the translations of words.</prevsent>
</prevsection>
<citsent citstr=" P99-1067 ">
using large, unrelated english and german corpora (with 163m and 135mwords) and small german-english bilingual dictionary (with 22k entires), rapp (1999) <papid> P99-1067 </papid>demonstrated that reasonably accurate translations could be learned for 100 german nouns that were not contained in the seed bilingual dictionary.</citsent>
<aftsection>
<nextsent>his algorithm worked by (1) building context vector representing an unknown german word by counting its co-occurrence with all the other wordsin the german monolingual corpus, (2) projecting this german vector onto the vector space of english using the seed bilingual dictionary, (3) calculating the similarity of this sparse projected vector to vectors for english words that were constructed using the english monolingual corpus, and (4) out putting the english words with the highest similarity as the most likely translations.
</nextsent>
<nextsent>a variety of subsequent work has extended the original idea either by exploring different measures of vector similarity (fung and yee, 1998)<papid> P98-1069 </papid>or by proposing other ways of measuring similarity beyond co-occurence within context win dow.</nextsent>
<nextsent>for instance, schafer and yarowsky (2002)<papid> W02-2026 </papid>demonstrated that word translations tend to co occur in time across languages.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1489">
<title id=" E12-1014.xml">toward statistical machine translation without parallel corpora </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>using large, unrelated english and german corpora (with 163m and 135mwords) and small german-english bilingual dictionary (with 22k entires), rapp (1999) <papid> P99-1067 </papid>demonstrated that reasonably accurate translations could be learned for 100 german nouns that were not contained in the seed bilingual dictionary.</prevsent>
<prevsent>his algorithm worked by (1) building context vector representing an unknown german word by counting its co-occurrence with all the other wordsin the german monolingual corpus, (2) projecting this german vector onto the vector space of english using the seed bilingual dictionary, (3) calculating the similarity of this sparse projected vector to vectors for english words that were constructed using the english monolingual corpus, and (4) out putting the english words with the highest similarity as the most likely translations.</prevsent>
</prevsection>
<citsent citstr=" P98-1069 ">
a variety of subsequent work has extended the original idea either by exploring different measures of vector similarity (fung and yee, 1998)<papid> P98-1069 </papid>or by proposing other ways of measuring similarity beyond co-occurence within context win dow.</citsent>
<aftsection>
<nextsent>for instance, schafer and yarowsky (2002)<papid> W02-2026 </papid>demonstrated that word translations tend to co occur in time across languages.</nextsent>
<nextsent>koehn and knight (2002) <papid> W02-0902 </papid>used similarity in spelling as another kind of cue that pair of words may be translations of one another.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1490">
<title id=" E12-1014.xml">toward statistical machine translation without parallel corpora </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>his algorithm worked by (1) building context vector representing an unknown german word by counting its co-occurrence with all the other wordsin the german monolingual corpus, (2) projecting this german vector onto the vector space of english using the seed bilingual dictionary, (3) calculating the similarity of this sparse projected vector to vectors for english words that were constructed using the english monolingual corpus, and (4) out putting the english words with the highest similarity as the most likely translations.
</prevsent>
<prevsent>a variety of subsequent work has extended the original idea either by exploring different measures of vector similarity (fung and yee, 1998)<papid> P98-1069 </papid>or by proposing other ways of measuring similarity beyond co-occurence within context win dow.</prevsent>
</prevsection>
<citsent citstr=" W02-2026 ">
for instance, schafer and yarowsky (2002)<papid> W02-2026 </papid>demonstrated that word translations tend to co occur in time across languages.</citsent>
<aftsection>
<nextsent>koehn and knight (2002) <papid> W02-0902 </papid>used similarity in spelling as another kind of cue that pair of words may be translations of one another.</nextsent>
<nextsent>garera et al(2009) <papid> W09-1117 </papid>defined context vectors using dependency relations rather than adjacent words.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1491">
<title id=" E12-1014.xml">toward statistical machine translation without parallel corpora </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>a variety of subsequent work has extended the original idea either by exploring different measures of vector similarity (fung and yee, 1998)<papid> P98-1069 </papid>or by proposing other ways of measuring similarity beyond co-occurence within context win dow.</prevsent>
<prevsent>for instance, schafer and yarowsky (2002)<papid> W02-2026 </papid>demonstrated that word translations tend to co occur in time across languages.</prevsent>
</prevsection>
<citsent citstr=" W02-0902 ">
koehn and knight (2002) <papid> W02-0902 </papid>used similarity in spelling as another kind of cue that pair of words may be translations of one another.</citsent>
<aftsection>
<nextsent>garera et al(2009) <papid> W09-1117 </papid>defined context vectors using dependency relations rather than adjacent words.</nextsent>
<nextsent>bergsma and van durme (2011) used the visual similarity of labeled web images to learn translations of nouns.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1492">
<title id=" E12-1014.xml">toward statistical machine translation without parallel corpora </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>for instance, schafer and yarowsky (2002)<papid> W02-2026 </papid>demonstrated that word translations tend to co occur in time across languages.</prevsent>
<prevsent>koehn and knight (2002) <papid> W02-0902 </papid>used similarity in spelling as another kind of cue that pair of words may be translations of one another.</prevsent>
</prevsection>
<citsent citstr=" W09-1117 ">
garera et al(2009) <papid> W09-1117 </papid>defined context vectors using dependency relations rather than adjacent words.</citsent>
<aftsection>
<nextsent>bergsma and van durme (2011) used the visual similarity of labeled web images to learn translations of nouns.
</nextsent>
<nextsent>additional related work on learning translations from monolingual corpora is discussed in section 6.in this paper, we apply bilingual lexicon induction methods to statistical machine translation.
</nextsent>
<nextsent>given the obvious benefits of not having to relyon scarce bilingual parallel training data, it is surprising that bilingual lexicon induction has notbeen used for smt before now.
</nextsent>
<nextsent>there are several open questions that make its applicability to smt uncertain.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1495">
<title id=" E12-1014.xml">toward statistical machine translation without parallel corpora </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>previous research on bilingual lexicon induction learned translations only for small number of high frequency words (e.g. 100 llll lll 0 100 200 300 400 500 600 0 10 20 30 40 accuracy , % corpus frequency top 1top 10figure 2: accuracy of single-word translations induced using contextual similarity as function of the source word corpus frequency.
</prevsent>
<prevsent>accuracy is the proportion of the source words with at least one correct (bilingual dictionary) translation in the top 1 and top 10 candidate lists.
</prevsent>
</prevsection>
<citsent citstr=" P08-1088 ">
nouns in rapp (1995), <papid> P95-1050 </papid>1,000 most frequent wordsin koehn and knight (2002), <papid> W02-0902 </papid>or 2,000 most frequent nouns in haghighi et al(2008)).<papid> P08-1088 </papid></citsent>
<aftsection>
<nextsent>although previous work reported high translation accuracy, it may be misleading to extrapolate the results to smt, where it is necessary to translate much larger set of words and phrases, including many low frequency items.
</nextsent>
<nextsent>in preliminary study, we plotted the accuracy of translations against the frequency of the source words in the monolingual corpus.
</nextsent>
<nextsent>figure 2 shows the result for translations induced using contextual similarity (defined in section 3.1).
</nextsent>
<nextsent>unsurprisingly, frequent terms have substantially better chance of being paired with correct translation, with words that only occur once having low chance of being translated accurately.1 this problem is exacerbated when we move to multi-tokenphrases.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1502">
<title id=" E12-1014.xml">toward statistical machine translation without parallel corpora </title>
<section> monolingual parameter estimation.  </section>
<citcontext>
<prevsection>
<prevsent>intuitively, the more frequently sk appears with and the less common it is in the corpus in general, the higher its component value.
</prevsent>
<prevsent>similarity between two vectors is measured as the cosine of the angle between them.temporal similarity.
</prevsent>
</prevsection>
<citsent citstr=" P06-1103 ">
in addition to contextual similarity, phrases in two languages may 133 be scored in terms of their temporal similarity (schafer and yarowsky, 2002; <papid> W02-2026 </papid>klementiev androth, 2006; <papid> P06-1103 </papid>alfonseca et al 2009).<papid> D09-1109 </papid></citsent>
<aftsection>
<nextsent>the intuition is that news stories in different languages will tend to discuss the same world events on the same day.
</nextsent>
<nextsent>the frequencies of translated phrases over time give them particular signatures that will tend to spike on the same dates.
</nextsent>
<nextsent>for instance, ifthe phrase asian tsunami is used frequently during particular timespan, the spanish translation mare moto asia tico is likely to also be used frequently during that time.
</nextsent>
<nextsent>figure 4 illustrates how the temporal distribution of terrorist is more similar to spanish terrorista than to other spanish phrases.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1503">
<title id=" E12-1014.xml">toward statistical machine translation without parallel corpora </title>
<section> monolingual parameter estimation.  </section>
<citcontext>
<prevsection>
<prevsent>intuitively, the more frequently sk appears with and the less common it is in the corpus in general, the higher its component value.
</prevsent>
<prevsent>similarity between two vectors is measured as the cosine of the angle between them.temporal similarity.
</prevsent>
</prevsection>
<citsent citstr=" D09-1109 ">
in addition to contextual similarity, phrases in two languages may 133 be scored in terms of their temporal similarity (schafer and yarowsky, 2002; <papid> W02-2026 </papid>klementiev androth, 2006; <papid> P06-1103 </papid>alfonseca et al 2009).<papid> D09-1109 </papid></citsent>
<aftsection>
<nextsent>the intuition is that news stories in different languages will tend to discuss the same world events on the same day.
</nextsent>
<nextsent>the frequencies of translated phrases over time give them particular signatures that will tend to spike on the same dates.
</nextsent>
<nextsent>for instance, ifthe phrase asian tsunami is used frequently during particular timespan, the spanish translation mare moto asia tico is likely to also be used frequently during that time.
</nextsent>
<nextsent>figure 4 illustrates how the temporal distribution of terrorist is more similar to spanish terrorista than to other spanish phrases.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1505">
<title id=" E12-1014.xml">toward statistical machine translation without parallel corpora </title>
<section> monolingual parameter estimation.  </section>
<citcontext>
<prevsection>
<prevsent>we treat each linked article pair as topic and collect counts for each phrase across all articles in its corresponding language.
</prevsent>
<prevsent>thus, the size of phrase topic signature is the number of article pairs withinterlingual links in wikipedia, and each component contains the number of times the phrase appears in (the appropriate side of) the corresponding pair.
</prevsent>
</prevsection>
<citsent citstr=" D09-1092 ">
our wikipedia-based topic similarity feature, w(f, e), is similar in spirit to polylingualtopic models (mimno et al 2009), <papid> D09-1092 </papid>but it is scalable to full bilingual lexicon induction.</citsent>
<aftsection>
<nextsent>3.2 lexical similarity features.
</nextsent>
<nextsent>in addition to the three phrase similarity features used in our model ? c(f, e), t(f, e) and w(f, e) we include four additional lexical similarity features for each of phrase pair.
</nextsent>
<nextsent>the first three lexical features clex(f, e), tlex(f, e) and wlex(f, e)are the lexical equivalents of the phrase-level contextual, temporal and wikipedia topic similarity scores.
</nextsent>
<nextsent>they score the similarity of individual words within the phrases.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1506">
<title id=" E12-1014.xml">toward statistical machine translation without parallel corpora </title>
<section> monolingual parameter estimation.  </section>
<citcontext>
<prevsection>
<prevsent>berg-kirkpatrick and klein (2011) present methods for learning correspondences between the alphabets of two languages.
</prevsent>
<prevsent>we can also extend this idea to language pairs not sharing the same writing system since many cognates, borrowed words, and names remain phonetic ally similar.
</prevsent>
</prevsection>
<citsent citstr=" P97-1017 ">
transliterations can be generated for tokens in source phrase (knightand graehl, 1997), <papid> P97-1017 </papid>with o(f, e) calculating phonetic similarity rather than orthographic.</citsent>
<aftsection>
<nextsent>the three phrasal and four lexical similarity scores are incorporated into the loglinear translation model as feature functions, replacing thebilingually estimated phrase translation probabilities ? and lexical weighting probabilities w. our seven similarity scores are not the only ones that could be incorporated into the translation model.
</nextsent>
<nextsent>various other similarity scores can be computed depending on the available monolingual data and its associated meta data (see, e.g. schafer and yarowsky (2002)<papid> W02-2026 </papid>).</nextsent>
<nextsent>3.3 reordering.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1510">
<title id=" E12-1014.xml">toward statistical machine translation without parallel corpora </title>
<section> additional related work.  </section>
<citcontext>
<prevsection>
<prevsent>carbonell et al(2006) described data-driven mt system that used no parallel text.
</prevsent>
<prevsent>it produced translation lattices using bilingual dictionary and scored them using an n-gram language model.their method has no notion of translation similarity aside from bilingual dictionary.
</prevsent>
</prevsection>
<citsent citstr=" P11-1002 ">
similarly, sanchez-cartagena et al(2011) supplement an smt phrase table with translation pairs extracted from bilingual dictionary and give each frequency of one for computing translation scores.ravi and knight (2011) <papid> P11-1002 </papid>treat mt without parallel training data as decipher ment task and learn translation model from monolingual text.</citsent>
<aftsection>
<nextsent>they translate corpora of spanish time expressions and subtitles, which both have limited vocabulary, into english.
</nextsent>
<nextsent>their method has not been applied to broader domains of text.most work on learning translations from monolingual texts only examine small numbers of frequent words.
</nextsent>
<nextsent>huang et al(2005) <papid> H05-1061 </papid>and daume?</nextsent>
<nextsent>and jagarlamudi (2011) are exceptions that improve mt by mining translations for oov items.a variety of past research has focused on mining parallel or comparable corpora from the web (munteanu and marcu, 2006; <papid> P06-1011 </papid>smith et al 2010; <papid> N10-1063 </papid>uszkoreit et al 2010).<papid> C10-1124 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1511">
<title id=" E12-1014.xml">toward statistical machine translation without parallel corpora </title>
<section> additional related work.  </section>
<citcontext>
<prevsection>
<prevsent>they translate corpora of spanish time expressions and subtitles, which both have limited vocabulary, into english.
</prevsent>
<prevsent>their method has not been applied to broader domains of text.most work on learning translations from monolingual texts only examine small numbers of frequent words.
</prevsent>
</prevsection>
<citsent citstr=" H05-1061 ">
huang et al(2005) <papid> H05-1061 </papid>and daume?</citsent>
<aftsection>
<nextsent>and jagarlamudi (2011) are exceptions that improve mt by mining translations for oov items.a variety of past research has focused on mining parallel or comparable corpora from the web (munteanu and marcu, 2006; <papid> P06-1011 </papid>smith et al 2010; <papid> N10-1063 </papid>uszkoreit et al 2010).<papid> C10-1124 </papid></nextsent>
<nextsent>others use an existing smt system to discover parallel sentences within independent monolingual texts, and use them to re-train and enhance the system (schwenk, 2008; chen et al 2008; <papid> P08-2040 </papid>schwenk and senellart, 2009; rauf and schwenk, 2009; lambert et al 2011).<papid> W11-2132 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1512">
<title id=" E12-1014.xml">toward statistical machine translation without parallel corpora </title>
<section> additional related work.  </section>
<citcontext>
<prevsection>
<prevsent>their method has not been applied to broader domains of text.most work on learning translations from monolingual texts only examine small numbers of frequent words.
</prevsent>
<prevsent>huang et al(2005) <papid> H05-1061 </papid>and daume?</prevsent>
</prevsection>
<citsent citstr=" P06-1011 ">
and jagarlamudi (2011) are exceptions that improve mt by mining translations for oov items.a variety of past research has focused on mining parallel or comparable corpora from the web (munteanu and marcu, 2006; <papid> P06-1011 </papid>smith et al 2010; <papid> N10-1063 </papid>uszkoreit et al 2010).<papid> C10-1124 </papid></citsent>
<aftsection>
<nextsent>others use an existing smt system to discover parallel sentences within independent monolingual texts, and use them to re-train and enhance the system (schwenk, 2008; chen et al 2008; <papid> P08-2040 </papid>schwenk and senellart, 2009; rauf and schwenk, 2009; lambert et al 2011).<papid> W11-2132 </papid></nextsent>
<nextsent>these are complementary but orthogonal to our research goals.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1513">
<title id=" E12-1014.xml">toward statistical machine translation without parallel corpora </title>
<section> additional related work.  </section>
<citcontext>
<prevsection>
<prevsent>their method has not been applied to broader domains of text.most work on learning translations from monolingual texts only examine small numbers of frequent words.
</prevsent>
<prevsent>huang et al(2005) <papid> H05-1061 </papid>and daume?</prevsent>
</prevsection>
<citsent citstr=" N10-1063 ">
and jagarlamudi (2011) are exceptions that improve mt by mining translations for oov items.a variety of past research has focused on mining parallel or comparable corpora from the web (munteanu and marcu, 2006; <papid> P06-1011 </papid>smith et al 2010; <papid> N10-1063 </papid>uszkoreit et al 2010).<papid> C10-1124 </papid></citsent>
<aftsection>
<nextsent>others use an existing smt system to discover parallel sentences within independent monolingual texts, and use them to re-train and enhance the system (schwenk, 2008; chen et al 2008; <papid> P08-2040 </papid>schwenk and senellart, 2009; rauf and schwenk, 2009; lambert et al 2011).<papid> W11-2132 </papid></nextsent>
<nextsent>these are complementary but orthogonal to our research goals.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1514">
<title id=" E12-1014.xml">toward statistical machine translation without parallel corpora </title>
<section> additional related work.  </section>
<citcontext>
<prevsection>
<prevsent>their method has not been applied to broader domains of text.most work on learning translations from monolingual texts only examine small numbers of frequent words.
</prevsent>
<prevsent>huang et al(2005) <papid> H05-1061 </papid>and daume?</prevsent>
</prevsection>
<citsent citstr=" C10-1124 ">
and jagarlamudi (2011) are exceptions that improve mt by mining translations for oov items.a variety of past research has focused on mining parallel or comparable corpora from the web (munteanu and marcu, 2006; <papid> P06-1011 </papid>smith et al 2010; <papid> N10-1063 </papid>uszkoreit et al 2010).<papid> C10-1124 </papid></citsent>
<aftsection>
<nextsent>others use an existing smt system to discover parallel sentences within independent monolingual texts, and use them to re-train and enhance the system (schwenk, 2008; chen et al 2008; <papid> P08-2040 </papid>schwenk and senellart, 2009; rauf and schwenk, 2009; lambert et al 2011).<papid> W11-2132 </papid></nextsent>
<nextsent>these are complementary but orthogonal to our research goals.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1515">
<title id=" E12-1014.xml">toward statistical machine translation without parallel corpora </title>
<section> additional related work.  </section>
<citcontext>
<prevsection>
<prevsent>huang et al(2005) <papid> H05-1061 </papid>and daume?</prevsent>
<prevsent>and jagarlamudi (2011) are exceptions that improve mt by mining translations for oov items.a variety of past research has focused on mining parallel or comparable corpora from the web (munteanu and marcu, 2006; <papid> P06-1011 </papid>smith et al 2010; <papid> N10-1063 </papid>uszkoreit et al 2010).<papid> C10-1124 </papid></prevsent>
</prevsection>
<citsent citstr=" P08-2040 ">
others use an existing smt system to discover parallel sentences within independent monolingual texts, and use them to re-train and enhance the system (schwenk, 2008; chen et al 2008; <papid> P08-2040 </papid>schwenk and senellart, 2009; rauf and schwenk, 2009; lambert et al 2011).<papid> W11-2132 </papid></citsent>
<aftsection>
<nextsent>these are complementary but orthogonal to our research goals.
</nextsent>
<nextsent>this paper has demonstrated novel set of techniques for successfully estimating phrase-basedsmt parameters from monolingual corpora, potentially circumventing the need for large bitexts, which are expensive to obtain for new languages and domains.
</nextsent>
<nextsent>we evaluated the performance ofour algorithms in full end-to-end translation system.
</nextsent>
<nextsent>assuming that bilingual-corpus-derived phrase table is available, we were able utilize our monolingually-estimated features to recover over 82% of bleu loss that resulted from removing the bilingual-corpus-derived phrase-table probabilities.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1516">
<title id=" E12-1014.xml">toward statistical machine translation without parallel corpora </title>
<section> additional related work.  </section>
<citcontext>
<prevsection>
<prevsent>huang et al(2005) <papid> H05-1061 </papid>and daume?</prevsent>
<prevsent>and jagarlamudi (2011) are exceptions that improve mt by mining translations for oov items.a variety of past research has focused on mining parallel or comparable corpora from the web (munteanu and marcu, 2006; <papid> P06-1011 </papid>smith et al 2010; <papid> N10-1063 </papid>uszkoreit et al 2010).<papid> C10-1124 </papid></prevsent>
</prevsection>
<citsent citstr=" W11-2132 ">
others use an existing smt system to discover parallel sentences within independent monolingual texts, and use them to re-train and enhance the system (schwenk, 2008; chen et al 2008; <papid> P08-2040 </papid>schwenk and senellart, 2009; rauf and schwenk, 2009; lambert et al 2011).<papid> W11-2132 </papid></citsent>
<aftsection>
<nextsent>these are complementary but orthogonal to our research goals.
</nextsent>
<nextsent>this paper has demonstrated novel set of techniques for successfully estimating phrase-basedsmt parameters from monolingual corpora, potentially circumventing the need for large bitexts, which are expensive to obtain for new languages and domains.
</nextsent>
<nextsent>we evaluated the performance ofour algorithms in full end-to-end translation system.
</nextsent>
<nextsent>assuming that bilingual-corpus-derived phrase table is available, we were able utilize our monolingually-estimated features to recover over 82% of bleu loss that resulted from removing the bilingual-corpus-derived phrase-table probabilities.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1517">
<title id=" E09-1033.xml">rich bitext projection features for parse reranking </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>improved parses of bi text should result in improved machine translation.
</prevsent>
<prevsent>second, as more and more texts are available in several languages, it will be increasingly the case that text to be parsed is itself part of bitext.
</prevsent>
</prevsection>
<citsent citstr=" N06-1020 ">
third, we hope that the improved parses of bitextwill serve as higher quality training data for improving monolingual parsing using process similar to self-training (mcclosky et al, 2006).<papid> N06-1020 </papid></citsent>
<aftsection>
<nextsent>it is well known that different languages encode different types of grammatical information (agree ment, case, tense etc.) and that what can be left unspecified in one language must be made explicit np np np dt nn baby cc and np dt nn woman sbar who had gray hair figure 1: english parse with high attachment in another.
</nextsent>
<nextsent>this information can be used for syntactic disambiguation.
</nextsent>
<nextsent>however, it is surprisingly hard to do this well.
</nextsent>
<nextsent>we use parses and alignments that are automatically generated and hence imperfect.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1518">
<title id=" E09-1033.xml">rich bitext projection features for parse reranking </title>
<section> approach.  </section>
<citcontext>
<prevsection>
<prevsent>(frau?)
</prevsent>
<prevsent>as shown in figure 3.we follow collins?
</prevsent>
</prevsection>
<citsent citstr=" P02-1035 ">
(2000) approach to discriminative reranking (see also (riezler et al, 2002)).<papid> P02-1035 </papid></citsent>
<aftsection>
<nextsent>given new sentence to parse, we first select the best parse trees according to generative model.
</nextsent>
<nextsent>then we use new features to learn discriminatively how to rerank the parses in this n-best list.
</nextsent>
<nextsent>we use features derived using projections of the 1-best german parse onto the hypothesized english parse under consideration.
</nextsent>
<nextsent>in more detail, we take the 100 best english parses from the bitpar parser (schmid, 2004) <papid> C04-1024 </papid>and rerank them.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1519">
<title id=" E09-1033.xml">rich bitext projection features for parse reranking </title>
<section> approach.  </section>
<citcontext>
<prevsection>
<prevsent>then we use new features to learn discriminatively how to rerank the parses in this n-best list.
</prevsent>
<prevsent>we use features derived using projections of the 1-best german parse onto the hypothesized english parse under consideration.
</prevsent>
</prevsection>
<citsent citstr=" C04-1024 ">
in more detail, we take the 100 best english parses from the bitpar parser (schmid, 2004) <papid> C04-1024 </papid>and rerank them.</citsent>
<aftsection>
<nextsent>we have good chance of finding the optimal parse among the 100-best1.
</nextsent>
<nextsent>an automatically generated word alignment determines translational correspondence between german and english.
</nextsent>
<nextsent>we use features which measure syntactic di 1using an oracle to select the best parse results in an f1 of 95.90, an improvement of 8.01 absolute over the baseline.
</nextsent>
<nextsent>verge nce between the german and english trees totry to rank the english trees which have less divergence higher.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1520">
<title id=" E09-1033.xml">rich bitext projection features for parse reranking </title>
<section> approach.  </section>
<citcontext>
<prevsection>
<prevsent>we use features which measure syntactic di 1using an oracle to select the best parse results in an f1 of 95.90, an improvement of 8.01 absolute over the baseline.
</prevsent>
<prevsent>verge nce between the german and english trees totry to rank the english trees which have less divergence higher.
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
our test set is 3718 sentences from the english penn treebank (marcus et al, 1993) <papid> J93-2004 </papid>which were translated into german.</citsent>
<aftsection>
<nextsent>we hold out these sentences, and train bitpar on the remaining penn treebank training sentences.
</nextsent>
<nextsent>the average f1 parsing accuracy of bitpar on this test set is 87.89%, which is our baseline2.
</nextsent>
<nextsent>we implement features based on projecting the german parse to each of the english 100-best parses in turn via the word alignment.
</nextsent>
<nextsent>by performing cross-validation and measuring test performance within each fold, we compare our new system with the baseline on the 3718 sentence set.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1521">
<title id=" E09-1033.xml">rich bitext projection features for parse reranking </title>
<section> training.  </section>
<citcontext>
<prevsection>
<prevsent>log-linear models are often trained using the maximum entropy criterion, but we train our model directly to maximize f1.
</prevsent>
<prevsent>we score f1 by comparing hypothesized parses for the discriminative training set with the gold standard.
</prevsent>
</prevsection>
<citsent citstr=" P03-1021 ">
to tryto find the optimal ? vector, we perform direct accuracy maximization, meaning that we search for the ? vector which directly optimizes f1 on the training set.och (2003) <papid> P03-1021 </papid>has described an efficient exact one dimensional accuracy maximization technique for similar search problem in machine translation.</citsent>
<aftsection>
<nextsent>the technique involves calculating an explicit representation of the piece wise constant functiongm(x) which evaluates the accuracy of the hypotheses which would be picked by eq.
</nextsent>
<nextsent>2 from set of hypotheses if we hold all weights constant, except for the weight m, which is set to x. this is calculated in one pass over the data.
</nextsent>
<nextsent>the algorithm for training is initial ized with achoice for ? and is described in figure 4.
</nextsent>
<nextsent>the function f1(?)
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1524">
<title id=" E09-1033.xml">rich bitext projection features for parse reranking </title>
<section> data and experiments.  </section>
<citcontext>
<prevsection>
<prevsent>it computes compact parse forest for all possible analyses.
</prevsent>
<prevsent>as all possible analyses are computed, any number of best parses can be extracted.
</prevsent>
</prevsection>
<citsent citstr=" W98-1115 ">
in contrast, other treebank parsers use sophisticated search strategies to find the most probable analysis without examining the set of all possible analyses (charniak et al, 1998; <papid> W98-1115 </papid>klein and manning, 2003).<papid> N03-1016 </papid></citsent>
<aftsection>
<nextsent>bitpar is particularly useful for n-best parsing as the n-best parses can be computed efficiently.
</nextsent>
<nextsent>for the 3718 sentences in the translated set, we created 100-best english parses and 1-best german parses.
</nextsent>
<nextsent>the german parser was trained on the tiger treebank.
</nextsent>
<nextsent>for the europarl corpus, we created 1-best parses for both languages.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1525">
<title id=" E09-1033.xml">rich bitext projection features for parse reranking </title>
<section> data and experiments.  </section>
<citcontext>
<prevsection>
<prevsent>it computes compact parse forest for all possible analyses.
</prevsent>
<prevsent>as all possible analyses are computed, any number of best parses can be extracted.
</prevsent>
</prevsection>
<citsent citstr=" N03-1016 ">
in contrast, other treebank parsers use sophisticated search strategies to find the most probable analysis without examining the set of all possible analyses (charniak et al, 1998; <papid> W98-1115 </papid>klein and manning, 2003).<papid> N03-1016 </papid></citsent>
<aftsection>
<nextsent>bitpar is particularly useful for n-best parsing as the n-best parses can be computed efficiently.
</nextsent>
<nextsent>for the 3718 sentences in the translated set, we created 100-best english parses and 1-best german parses.
</nextsent>
<nextsent>the german parser was trained on the tiger treebank.
</nextsent>
<nextsent>for the europarl corpus, we created 1-best parses for both languages.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1526">
<title id=" E09-1033.xml">rich bitext projection features for parse reranking </title>
<section> data and experiments.  </section>
<citcontext>
<prevsection>
<prevsent>word alignment.
</prevsent>
<prevsent>we use word alignment of the translated sentences from the penn tree bank, as well as word alignment of the europarl corpus.
</prevsent>
</prevsection>
<citsent citstr=" J07-3002 ">
we align these two datasets together with data from the jrc acquis (steinberger et al, 2006) to try to obtain better quality alignments (it is well known that alignment quality improves as the amount of data increases (fraser and marcu, 2007)).<papid> J07-3002 </papid></citsent>
<aftsection>
<nextsent>we aligned approximately 3.08 million sentence pairs.
</nextsent>
<nextsent>we tried to obtain better alignment quality as alignment quality is problem in many cases where syntactic projection would otherwise work well (fossum and knight, 2008).
</nextsent>
<nextsent>287 system train +base test +base 1 baseline 87.89 87.89.
</nextsent>
<nextsent>2 contrastive 88.70 0.82 88.45 0.56.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1527">
<title id=" E09-1033.xml">rich bitext projection features for parse reranking </title>
<section> data and experiments.  </section>
<citcontext>
<prevsection>
<prevsent>2 contrastive 88.70 0.82 88.45 0.56.
</prevsent>
<prevsent>(5 trials/fold) 3 contrastive 88.82 0.93 88.55 0.66.
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
(greedy selection) table 1: average f1 of 7-way cross-validation to generate the alignments, we used model 4 (brown et al, 1993), <papid> J93-2003 </papid>as implemented in giza++ (och and ney, 2003).<papid> J03-1002 </papid></citsent>
<aftsection>
<nextsent>as is standard practice, we trained model 4 with english as the source language, and then trained model 4 with german asthe source language, resulting in two viterbi alignments.
</nextsent>
<nextsent>these were combined using the grow diag final and symmetrization heuristic (koehn et al, 2003).<papid> N03-1017 </papid>experiments.</nextsent>
<nextsent>we perform 7-way cross validation on 3718 sentences.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1528">
<title id=" E09-1033.xml">rich bitext projection features for parse reranking </title>
<section> data and experiments.  </section>
<citcontext>
<prevsection>
<prevsent>2 contrastive 88.70 0.82 88.45 0.56.
</prevsent>
<prevsent>(5 trials/fold) 3 contrastive 88.82 0.93 88.55 0.66.
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
(greedy selection) table 1: average f1 of 7-way cross-validation to generate the alignments, we used model 4 (brown et al, 1993), <papid> J93-2003 </papid>as implemented in giza++ (och and ney, 2003).<papid> J03-1002 </papid></citsent>
<aftsection>
<nextsent>as is standard practice, we trained model 4 with english as the source language, and then trained model 4 with german asthe source language, resulting in two viterbi alignments.
</nextsent>
<nextsent>these were combined using the grow diag final and symmetrization heuristic (koehn et al, 2003).<papid> N03-1017 </papid>experiments.</nextsent>
<nextsent>we perform 7-way cross validation on 3718 sentences.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1529">
<title id=" E09-1033.xml">rich bitext projection features for parse reranking </title>
<section> data and experiments.  </section>
<citcontext>
<prevsection>
<prevsent>(greedy selection) table 1: average f1 of 7-way cross-validation to generate the alignments, we used model 4 (brown et al, 1993), <papid> J93-2003 </papid>as implemented in giza++ (och and ney, 2003).<papid> J03-1002 </papid></prevsent>
<prevsent>as is standard practice, we trained model 4 with english as the source language, and then trained model 4 with german asthe source language, resulting in two viterbi align ments.</prevsent>
</prevsection>
<citsent citstr=" N03-1017 ">
these were combined using the grow diag final and symmetrization heuristic (koehn et al, 2003).<papid> N03-1017 </papid>experiments.</citsent>
<aftsection>
<nextsent>we perform 7-way cross validation on 3718 sentences.
</nextsent>
<nextsent>in each fold of the cross-validation, the training set is 3186 sentences, while the test set is 532 sentences.
</nextsent>
<nextsent>our results are shown in table 1.
</nextsent>
<nextsent>in row 1, we take the hypothesis ranked best by bitpar.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1530">
<title id=" E09-1033.xml">rich bitext projection features for parse reranking </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>we believe that using the full set of 34 features (many of which are very similar to oneanother) made the training problem harder with out improving the fit to the training data, and that greedy feature selection helps with this (see also section 7).
</prevsent>
<prevsent>as we mentioned in section 2, work on parse reranking is relevant, but vital difference is that we use features based only on syntactic projection of the two languages in bitext.
</prevsent>
</prevsection>
<citsent citstr=" P05-1022 ">
for an overview of different types of features that have been used in parse reranking see charniak and johnson (2005).<papid> P05-1022 </papid></citsent>
<aftsection>
<nextsent>like collins (2000) we use cross-validation to train our model, but we have access to much less data (3718 sentences total, which is less than 1/10of the data collins used).
</nextsent>
<nextsent>we use rich feature functions which were designed by hand to specifically address problems in english parses which can be disambiguated using the german translation.
</nextsent>
<nextsent>syntactic projection has been used to bootstrap treebanks in resource poor languages.
</nextsent>
<nextsent>some examples of projection of syntactic parses from english to resource poor language for which no parser is available are the works of yarowsky and ngai (2001), <papid> N01-1026 </papid>hwa et al (2005) and goyal and chatterjee (2006).<papid> P06-2039 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1531">
<title id=" E09-1033.xml">rich bitext projection features for parse reranking </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>we use rich feature functions which were designed by hand to specifically address problems in english parses which can be disambiguated using the german translation.
</prevsent>
<prevsent>syntactic projection has been used to bootstrap treebanks in resource poor languages.
</prevsent>
</prevsection>
<citsent citstr=" N01-1026 ">
some examples of projection of syntactic parses from english to resource poor language for which no parser is available are the works of yarowsky and ngai (2001), <papid> N01-1026 </papid>hwa et al (2005) and goyal and chatterjee (2006).<papid> P06-2039 </papid></citsent>
<aftsection>
<nextsent>our work differs from theirs in that we are performing parse reranking task in english using knowledge gained from german parses, and parsing accuracy is generally thought to be worse in german than in english.
</nextsent>
<nextsent>hopkins and kuhn (2006) <papid> W06-2002 </papid>conducted research with goals similar to ours.</nextsent>
<nextsent>they showed how to build powerful generative model which flexibly incorporates features from parallel text in four languages, but were not able to show an improvement in parsing performance.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1532">
<title id=" E09-1033.xml">rich bitext projection features for parse reranking </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>we use rich feature functions which were designed by hand to specifically address problems in english parses which can be disambiguated using the german translation.
</prevsent>
<prevsent>syntactic projection has been used to bootstrap treebanks in resource poor languages.
</prevsent>
</prevsection>
<citsent citstr=" P06-2039 ">
some examples of projection of syntactic parses from english to resource poor language for which no parser is available are the works of yarowsky and ngai (2001), <papid> N01-1026 </papid>hwa et al (2005) and goyal and chatterjee (2006).<papid> P06-2039 </papid></citsent>
<aftsection>
<nextsent>our work differs from theirs in that we are performing parse reranking task in english using knowledge gained from german parses, and parsing accuracy is generally thought to be worse in german than in english.
</nextsent>
<nextsent>hopkins and kuhn (2006) <papid> W06-2002 </papid>conducted research with goals similar to ours.</nextsent>
<nextsent>they showed how to build powerful generative model which flexibly incorporates features from parallel text in four languages, but were not able to show an improvement in parsing performance.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1533">
<title id=" E09-1033.xml">rich bitext projection features for parse reranking </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>some examples of projection of syntactic parses from english to resource poor language for which no parser is available are the works of yarowsky and ngai (2001), <papid> N01-1026 </papid>hwa et al (2005) and goyal and chatterjee (2006).<papid> P06-2039 </papid></prevsent>
<prevsent>our work differs from theirs in that we are performing parse reranking task in english using knowledge gained from german parses, and parsing accuracy is generally thought to be worse in german than in english.</prevsent>
</prevsection>
<citsent citstr=" W06-2002 ">
hopkins and kuhn (2006) <papid> W06-2002 </papid>conducted research with goals similar to ours.</citsent>
<aftsection>
<nextsent>they showed how to build powerful generative model which flexibly incorporates features from parallel text in four languages, but were not able to show an improvement in parsing performance.
</nextsent>
<nextsent>after the submission of our paper for review, two papers outlining relevant work were published.
</nextsent>
<nextsent>burkett and klein (2008) <papid> D08-1092 </papid>describe system for simultaneously improving chinese and english parses of chinese/english bitext.</nextsent>
<nextsent>this work is complementary to ours.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1534">
<title id=" E09-1033.xml">rich bitext projection features for parse reranking </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>they showed how to build powerful generative model which flexibly incorporates features from parallel text in four languages, but were not able to show an improvement in parsing performance.
</prevsent>
<prevsent>after the submission of our paper for review, two papers outlining relevant work were published.
</prevsent>
</prevsection>
<citsent citstr=" D08-1092 ">
burkett and klein (2008) <papid> D08-1092 </papid>describe system for simultaneously improving chinese and english parses of chinese/english bitext.</citsent>
<aftsection>
<nextsent>this work is complementary to ours.
</nextsent>
<nextsent>the system is trained using gold standard trees in both chinese and english, in contrast with our system which only has access to gold standard trees in english.
</nextsent>
<nextsent>their system uses tree alignment which varies within training, but this does not appear to make large difference in performance.
</nextsent>
<nextsent>they use coarsely defined features which are language independent.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1535">
<title id=" E09-2016.xml">three bionlp tools powered by a biological lexicon </title>
<section> summary of the bio lexicon.  </section>
<citcontext>
<prevsection>
<prevsent>in this section, we provide summary of the bio lexicon (sasaki et al, 2008).
</prevsent>
<prevsent>it contains words belonging to four part-of-speech categories: verb, noun, adjective, and adverb.
</prevsent>
</prevsection>
<citsent citstr=" L08-1234 ">
quochi et al(2008) <papid> L08-1234 </papid>designed the database model of the bio lexicon which follows the lexical markup framework (francopoulo et al, 2008).</citsent>
<aftsection>
<nextsent>2.1 entries in the biology lexicon.
</nextsent>
<nextsent>the bio lexicon accommodates both general english words and terminologies.
</nextsent>
<nextsent>biomedical terms were gathered from existing biomedical databases.
</nextsent>
<nextsent>detailed information regarding the sources of biomedical terms can be found in (rebholz-schuhmann et al, 2008).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1536">
<title id=" E09-2016.xml">three bionlp tools powered by a biological lexicon </title>
<section> application 1: bltagger.  </section>
<citcontext>
<prevsection>
<prevsent>the selection criteria for the best path are determined by the crf tagging model trained on the genia corpus (kim et al, 2003).
</prevsent>
<prevsent>in this example, il-2/nn-biomed -/- mediated/vvn activation/nn of/in is selected as the best path.
</prevsent>
</prevsection>
<citsent citstr=" W04-3230 ">
following kudo et al (2004), <papid> W04-3230 </papid>we adapted the core engine of the crf-based morphological analyzer, mecab2, to our pos tagging task.</citsent>
<aftsection>
<nextsent>the features used were: ? pos ? biomed ? pos-biomed ? bigram of adjacent pos ? bigram of adjacent biomed ? bigram of adjacent pos-biomed during the construction of the trellis, white space is considered as the delimiter unless otherwise stated within dictionary entries.
</nextsent>
<nextsent>this means that unknown tokens are character sequences without spaces.
</nextsent>
<nextsent>as the bio lexicon associates biomedical semantic ids with terms, the bltagger attaches semantic ids to the tokenizing/tagging results.
</nextsent>
<nextsent>bio lexicon enju (miyao, et al, 2003) is an hpsg parser, which is tuned to the biomedical domain.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1537">
<title id=" E91-1031.xml">prediction in chart parsing algorithms for categorial unification grammar </title>
<section> preliminaries.  </section>
<citcontext>
<prevsection>

<prevsent>categorial unification grammar..
</prevsent>
</prevsection>
<citsent citstr=" C86-1045 ">
unification- based versions of categorial grammar, known as cug or ucg, have attracted considerable attention recently (see, for instance, uszkoreit, 1986, <papid> C86-1045 </papid>karttunen, 1986, bouma, 1988, bouma et al, 1988, and calder et al, 1988).<papid> C88-1018 </papid></citsent>
<aftsection>
<nextsent>the categories of categorial grammar (cg) can be encoded easily as feature-structures, in which the attribute   cat   dominates either anatomic value (in case of anatomic category) or structure with at-tributes   val  ,   dir   and   arg   (in case of complex category).
</nextsent>
<nextsent>morphosyntactic information can be added by introducing additional labels.
</nextsent>
<nextsent>an example of such category represented as attribute-value matrix is presented below.
</nextsent>
<nextsent>n \ [+nom\] /n \ [+nom, +sg\] = val : case : nora i : ight arg : case : nom hum : sg the combinatory rules of classical cg, ~ /b (rightward application) and ---, b \a (leftward ap- plication), can be encoded as highly schematic rewrite rules associated with an attribute-value graph: rightward appl icat ion rule : xo ~ xi x2 xo:  1  \[- xl : \] cat : 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1538">
<title id=" E91-1031.xml">prediction in chart parsing algorithms for categorial unification grammar </title>
<section> preliminaries.  </section>
<citcontext>
<prevsection>

<prevsent>categorial unification grammar..
</prevsent>
</prevsection>
<citsent citstr=" C88-1018 ">
unification- based versions of categorial grammar, known as cug or ucg, have attracted considerable attention recently (see, for instance, uszkoreit, 1986, <papid> C86-1045 </papid>karttunen, 1986, bouma, 1988, bouma et al, 1988, and calder et al, 1988).<papid> C88-1018 </papid></citsent>
<aftsection>
<nextsent>the categories of categorial grammar (cg) can be encoded easily as feature-structures, in which the attribute   cat   dominates either anatomic value (in case of anatomic category) or structure with at-tributes   val  ,   dir   and   arg   (in case of complex category).
</nextsent>
<nextsent>morphosyntactic information can be added by introducing additional labels.
</nextsent>
<nextsent>an example of such category represented as attribute-value matrix is presented below.
</nextsent>
<nextsent>n \ [+nom\] /n \ [+nom, +sg\] = val : case : nora i : ight arg : case : nom hum : sg the combinatory rules of classical cg, ~ /b (rightward application) and ---, b \a (leftward ap- plication), can be encoded as highly schematic rewrite rules associated with an attribute-value graph: rightward appl icat ion rule : xo ~ xi x2 xo:  1  \[- xl : \] cat : 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1539">
<title id=" E91-1031.xml">prediction in chart parsing algorithms for categorial unification grammar </title>
<section> preliminaries.  </section>
<citcontext>
<prevsection>
<prevsent>con-sequently, cug not only shows resemblances with tra-ditional categorial grammar, but also with head-driven phrase structure grammar (pollard &amp;: sag, 1987), an- other lexical ist and unification-based framework.
</prevsent>
<prevsent>chart parsing of unification grammar (ug).
</prevsent>
</prevsection>
<citsent citstr=" P85-1018 ">
parsing methods for context-free grammar can be extended to unification-based grammar formalisms (see shieber, 1985 <papid> P85-1018 </papid>or haas, 1989), <papid> J89-4001 </papid>and therefore they can in principle be used to parse cug.</citsent>
<aftsection>
<nextsent>a chart-parser scans sentence from left to right, while entering items, representing (partial) derivations, in chart.
</nextsent>
<nextsent>assume that items are represented as prolog terms of the form item(begin, end, lh s, parsed, toparse), where lhs is feature-structure and parsed and toparse contain lists of feature-structures.
</nextsent>
<nextsent>an item(o, 1, \[s\],\[np\], \[v np\] ) represents partial derivation ranging from position 0 to 1 of constituent with feature-structure s, of which daughter np has been found and of which daughters and np are still to be parsed.
</nextsent>
<nextsent>a word with lexical entry word : cat at position begin, leads to addition of an item item(begin, begin + 1, cat, \[word\], \[ \]).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1540">
<title id=" E91-1031.xml">prediction in chart parsing algorithms for categorial unification grammar </title>
<section> preliminaries.  </section>
<citcontext>
<prevsection>
<prevsent>con-sequently, cug not only shows resemblances with tra-ditional categorial grammar, but also with head-driven phrase structure grammar (pollard &amp;: sag, 1987), an- other lexical ist and unification-based framework.
</prevsent>
<prevsent>chart parsing of unification grammar (ug).
</prevsent>
</prevsection>
<citsent citstr=" J89-4001 ">
parsing methods for context-free grammar can be extended to unification-based grammar formalisms (see shieber, 1985 <papid> P85-1018 </papid>or haas, 1989), <papid> J89-4001 </papid>and therefore they can in principle be used to parse cug.</citsent>
<aftsection>
<nextsent>a chart-parser scans sentence from left to right, while entering items, representing (partial) derivations, in chart.
</nextsent>
<nextsent>assume that items are represented as prolog terms of the form item(begin, end, lh s, parsed, toparse), where lhs is feature-structure and parsed and toparse contain lists of feature-structures.
</nextsent>
<nextsent>an item(o, 1, \[s\],\[np\], \[v np\] ) represents partial derivation ranging from position 0 to 1 of constituent with feature-structure s, of which daughter np has been found and of which daughters and np are still to be parsed.
</nextsent>
<nextsent>a word with lexical entry word : cat at position begin, leads to addition of an item item(begin, begin + 1, cat, \[word\], \[ \]).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1548">
<title id=" E09-1024.xml">reranking models for spoken language understanding </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these exploit all possible word/concept sub sequences (with gaps) of the spoken sentence as features (i.e.all possible n-grams).
</prevsent>
<prevsent>gaps allow for the encod 202 ing of long distance dependencies between words in relatively small n-grams.
</prevsent>
</prevsection>
<citsent citstr=" W04-2403 ">
given the huge size of this feature space, we adopted kernel methods and in particular sequence kernels (shawe-taylor and cristianini, 2004) and tree kernels (raymond and riccardi, 2007; moschitti and bejan, 2004; <papid> W04-2403 </papid>moschitti, 2006) to implicitly encode n-grams and other structural information in svms.</citsent>
<aftsection>
<nextsent>we experimented with different approaches for training the discriminative models and two different corpora: the well-known media corpus (bonneau-maynard et al, 2005) and new corpus acquired in the european project luna1 (ray mond et al, 2007).
</nextsent>
<nextsent>the results show great improvement with respect to both the fst-based model and the svm model alone, which are the current state-of-the-art for concept classification on such corpora.
</nextsent>
<nextsent>the rest of the paper is organized as follows: sections 2 and 3 show the generative and discriminative models, respectively.
</nextsent>
<nextsent>the experiments and results are reported in section 4 whereas the conclusions are drawn in section 5.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1549">
<title id=" E09-1024.xml">reranking models for spoken language understanding </title>
<section> discriminative re-ranking.  </section>
<citcontext>
<prevsection>
<prevsent>+ ?(cjn1 , j n2)) (1)where ? ?
</prevsent>
<prevsent>{0, 1}, nc(n1) is the number of children of n1 and cjn is the j-th child of the node n. note that, since the productions are the same,nc(n1) = nc(n2).
</prevsent>
</prevsection>
<citsent citstr=" P02-1034 ">
?(n1, n2) evaluates the number of stfs common to n1 and n2 as proved in (collins and duffy, 2002).<papid> P02-1034 </papid></citsent>
<aftsection>
<nextsent>moreover, decay factor ? can be added by modifying steps (2) and (3) as follows2: 2.
</nextsent>
<nextsent>?(n1, n2) = ?, 3.
</nextsent>
<nextsent>?(n1, n2) = ? nc(n1) j=1 (?
</nextsent>
<nextsent>+ ?(c n1 , j n2)).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1551">
<title id=" E09-1024.xml">reranking models for spoken language understanding </title>
<section> discriminative re-ranking.  </section>
<citcontext>
<prevsection>
<prevsent>given the above data, the sequence kern elis used to evaluate the number of common ngrams between si and sj . since the string kernel skips some elements of the target sequences, the counted n-grams include: concept sequences, word sequences and any sub sequence of words and concepts at any distance in the sentence.
</prevsent>
<prevsent>such counts are used in our re-ranking function as follows: let ei be the pair ? s1i , s2i ? we evaluate the kernel: kr(e1, e2) = sk(s11, s12) + sk(s21, s22) (3) ? sk(s11, s22)?
</prevsent>
</prevsection>
<citsent citstr=" W06-2909 ">
sk(s21, s12)this schema, consisting in summing four different kernels, has been already applied in (collins and duffy, 2002) <papid> P02-1034 </papid>for syntactic parsing re-ranking, where the basic kernel was tree kernel instead ofsk and in (moschitti et al, 2006), <papid> W06-2909 </papid>where, to re rank semantic role labeling annotations, tree kernel was used on semantic tree similar to the one introduced in the next section.</citsent>
<aftsection>
<nextsent>3.8 re-ranking models using trees.
</nextsent>
<nextsent>since the aim in concept annotation re-ranking isto exploit innovative and effective source of information, we can use the power of tree kernels to generate correlation between concepts and word structures.fig.
</nextsent>
<nextsent>2 describes the structural association between the concept and the word level.
</nextsent>
<nextsent>this kind oftrees allows us to engineer new kernels and consequently new features (moschitti et al, 2008), 206 figure 2: an example of the semantic tree used for stk or ptk corpus train set test set luna words concepts words concepts dialogs woz 183 67dialogs hh 180 turns woz 1.019 373turns hh 6.999 tokens woz 8.512 2.887 2.888 984tokens woz 62.639 17.423 - vocab.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1552">
<title id=" E09-1024.xml">reranking models for spoken language understanding </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>we trained all the sclms used in our experiments with the srilm toolkit (stolcke, 2002) and weused an interpolated model for probability estimation with the kneser-ney discount (chen and goodman, 1998).
</prevsent>
<prevsent>we then converted the model in an fst as described in section 2.1.
</prevsent>
</prevsection>
<citsent citstr=" N01-1025 ">
the model used to obtain the svm baseline for concept classification was trained using yamcha (kudo and matsumoto, 2001).<papid> N01-1025 </papid></citsent>
<aftsection>
<nextsent>for there ranking models based on structure kernels, svms or perceptron, we used the svm-light-tk toolkit(available at dit.unitn.it/moschitti).
</nextsent>
<nextsent>for ?
</nextsent>
<nextsent>(see section 3.2), cost-factor and trade-off parameters, we used, 0.4, 1 and 1, respectively.
</nextsent>
<nextsent>4.3 training approaches.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1553">
<title id=" E12-3009.xml">mining cooccurrence matrices for sopmi paradigm word candidates </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>it relies on two sets of paradigm words,positive and negative, which determine the polarity of unseen words.
</prevsent>
<prevsent>the method is resource lean and therefore often used in languages other than english.
</prevsent>
</prevsection>
<citsent citstr=" N07-2048 ">
recent examples include japanese (wang and araki, 2007) <papid> N07-2048 </papid>and german (remus et al , 2006).</citsent>
<aftsection>
<nextsent>unfortunately, the selection of paradigm words rarely receives sufficient attention and is typically done in an ad hoc manner.
</nextsent>
<nextsent>one notable example of manual paradigm word selection method was presented in (read and carroll, 2009).
</nextsent>
<nextsent>in this context, an interesting variation of the semantic orientation pointwise mutual information (so-pmi) algorithm for japanese was suggested by (wang and araki, 2007).<papid> N07-2048 </papid></nextsent>
<nextsent>authors, motivated by excessive leaning toward positive opinions, proposed to modify the algorithm by introducing balancing factor and detecting neutral ex pressions.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1555">
<title id=" E12-3009.xml">mining cooccurrence matrices for sopmi paradigm word candidates </title>
<section> svd.  </section>
<citcontext>
<prevsection>
<prevsent>the word co-occurrence matrix (1325x1325) was the subject of singular value decomposition(svd), well-known matrix factor ization technique which decomposes matrix into three matrices: = uv where ? is matrix whose diagonals are the singular values of a, and are left and right eigenvectors matrices.
</prevsent>
<prevsent>the usage of svd decompositions has longand successful history of applications in extracting meaning from word frequencies in word document matrices, as for example the welles tablished algorithm of latent semantic indexing (lsi).
</prevsent>
</prevsection>
<citsent citstr=" E09-1067 ">
more recently, the usability of analyzing the structure of language via spectral analysis of co-occurrence matrices was demonstrated by studies such as (mukherjee et al , 2009).<papid> E09-1067 </papid></citsent>
<aftsection>
<nextsent>the focus was on phonology with the intention to discover principles governing consonant inventories and quantify their importance.
</nextsent>
<nextsent>our work, as we believe, is the first to apply svd in the context of co-occurrence matrices and so-pmi.we suspect that the svd technique can be helpful by selecting lexemes that represent certain amounts of latent co-occurrence structure.
</nextsent>
<nextsent>furthermore, the fact that 20 eigenvalues constitutes approximately half of the norm of the spectrum (horn and johnson, 1990), as on table 1, suggests that there may exist small number of organizing principles which could be potentially helpful to improve the selection of lexemes into paradigm sets.
</nextsent>
<nextsent>c 10 0.728 0.410 20 0.797 0.498 100 0.924 0.720 table 1: frobenius norm of the spectrum for 10, 20 and 100 first eigenvalues.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1556">
<title id=" E06-3003.xml">an approach to summarizing short stories </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a more thorough evaluation involving human judges is under way.
</prevsent>
<prevsent>in the course of recent years the scientific community working on the problem of automatic text summarization has been experiencing an upsurge.
</prevsent>
</prevsection>
<citsent citstr=" J02-4002 ">
a multitude of different techniques has been applied to this end, some of the more remarkable of them being (marcu, 1997; mani et al. 1998; teufel and moens, 2002; <papid> J02-4002 </papid>elhadad et al, 2005), to name just few.</citsent>
<aftsection>
<nextsent>these researchers worked on various text genres: scientific and popular scientific articles (marcu, 1997; mani et al., 1998), texts in computational linguistics (teufel and moens, 2002), <papid> J02-4002 </papid>and medical texts(elhadad et al, 2002).</nextsent>
<nextsent>all these genres are examples of texts characterized by rigid structure, relative abundance of surface markers and straightforwardness.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1558">
<title id=" E06-3003.xml">an approach to summarizing short stories </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these researchers worked on various text genres: scientific and popular scientific articles (marcu, 1997; mani et al., 1998), texts in computational linguistics (teufel and moens, 2002), <papid> J02-4002 </papid>and medical texts(elhadad et al, 2002).</prevsent>
<prevsent>all these genres are examples of texts characterized by rigid structure, relative abundance of surface markers and straightforwardness.</prevsent>
</prevsection>
<citsent citstr=" J02-4003 ">
relatively few attempts have been made at summarizing less structured genres, some of them being dialogue and speech summarization (zechner, 2002; <papid> J02-4003 </papid>koumpis et al 2001).</citsent>
<aftsection>
<nextsent>the issue of summarizing fiction remains largely untouched, since few very thorough earlier works (charniak, 1972; lehnert, 1982).
</nextsent>
<nextsent>the work presented here seeks to fill in this gap.
</nextsent>
<nextsent>the ultimate objective of the project is stated as follows: to produce indicative summaries of short works of fiction such that they be helpful to potential reader in deciding whether she would be interested in reading particular story or not.to this end, revealing the plot was deemed unnecessary and even undesirable.
</nextsent>
<nextsent>instead, the current approach relies on the following assumption: when reader is presented with an extracted summary outlining the general settings of story (such as time, place and who it is about), she will have enough information to decide how interested she would be in reading story.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1559">
<title id=" E06-3003.xml">an approach to summarizing short stories </title>
<section> data pre-processing.  </section>
<citcontext>
<prevsection>
<prevsent>section 5 draws some conclusions and outlines possible directions in which this work may evolve.
</prevsent>
<prevsent>before working on selecting salient descriptive sentences, the stories of the training set were analyzed for presence of surface markers denoting characters, locations and temporal anchors.
</prevsent>
</prevsection>
<citsent citstr=" P02-1022 ">
to this end, the gate gazetteer (cunningham et al., 2002) <papid> P02-1022 </papid>was used, and only entities recognized by it automatically were considered.the findings were as follows.</citsent>
<aftsection>
<nextsent>each story contained multiple mentions of characters (an average of 64 mentions per story).
</nextsent>
<nextsent>yet only 22 location markers were found, most of these being street names.
</nextsent>
<nextsent>the 22 markers were found in 10 out of 14 stories, leaving 4 stories without any identifiable location markers.
</nextsent>
<nextsent>only 4 temporal anchors were identified in all 14 stories: 2 absolute (such as years) and 2 relative (names of holidays).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1560">
<title id=" E06-3003.xml">an approach to summarizing short stories </title>
<section> data pre-processing.  </section>
<citcontext>
<prevsection>
<prevsent>that could make me like my girl any less.
</prevsent>
<prevsent>the author created system that resolved 1st and 3rd person singular pronouns (i, me, my, he,his etc.) and singular nominal anaphoric expressions (e.g. the man, but not men).
</prevsent>
</prevsection>
<citsent citstr=" A97-1011 ">
the system was implemented in java, within the gate framework, using conn exor machin ese syntax parser (tapanainen and jrvinen, 1997).<papid> A97-1011 </papid>a generalized overview of the system is provided below.</citsent>
<aftsection>
<nextsent>during the first step, the documents were parsed using conn exor machinesesyntax parser.
</nextsent>
<nextsent>the parsed data was then forwarded to the gazetteer in gate, which recognized nouns denoting persons.
</nextsent>
<nextsent>the original version of the gazetteer recognized only named entities and professions, but the gazetteer was extended to include common animate nouns such asman, woman, etc. as the next step, an implementation based on classical pronoun resolution algorithm (lappin and leass, 1994) <papid> J94-4002 </papid>was applied to the texts.</nextsent>
<nextsent>subsequently, anaphoric noun phrases were identified using the rules outlined figure 1.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1561">
<title id=" E06-3003.xml">an approach to summarizing short stories </title>
<section> data pre-processing.  </section>
<citcontext>
<prevsection>
<prevsent>during the first step, the documents were parsed using conn exor machinesesyntax parser.
</prevsent>
<prevsent>the parsed data was then forwarded to the gazetteer in gate, which recognized nouns denoting persons.
</prevsent>
</prevsection>
<citsent citstr=" J94-4002 ">
the original version of the gazetteer recognized only named entities and professions, but the gazetteer was extended to include common animate nouns such asman, woman, etc. as the next step, an implementation based on classical pronoun resolution algorithm (lappin and leass, 1994) <papid> J94-4002 </papid>was applied to the texts.</citsent>
<aftsection>
<nextsent>subsequently, anaphoric noun phrases were identified using the rules outlined figure 1.
</nextsent>
<nextsent>a fragment of desired summary for the cost of kindness by jerome k. jerome.
</nextsent>
<nextsent>the cost of kindness jerome k. jerome (1859-1927) augustus crackle thorpe would be quitting wychwood-on-the-heath the following monday, never to setfoot--so the rev. augustus crackle thorpe himself and every single member of his congregation hoped sin cerely--in the neighbourhood again.
</nextsent>
<nextsent>the rev. augustus crackle thorpe, m.a., might possibly have been of service to his church in, say, some east-end parish of unsavoury reputation, some mission station far advanced amid the hordes of heathendom.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1563">
<title id=" E06-3003.xml">an approach to summarizing short stories </title>
<section> selecting descriptive sentences using.  </section>
<citcontext>
<prevsection>
<prevsent>the characteristics used were default aspect of the main verb of clause, tense, temporal expressions, semantic category of verb, voice and some properties of the direct object.
</prevsent>
<prevsent>each of these characteristics is listed below, along with motivation for it, and information about how it was calculated.
</prevsent>
</prevsection>
<citsent citstr=" P02-1027 ">
it must be mentioned that several researchers looked into determining automatically various semantic properties of verbs, such as (siegel, 1998; merlo et al, 2002).<papid> P02-1027 </papid></citsent>
<aftsection>
<nextsent>yet these approaches dealt with properties of verbs in general and not with particular usages in the context of concrete sentences.
</nextsent>
<nextsent>default verbal aspect.
</nextsent>
<nextsent>a set of verbs, referred to as stative verbs, tends to produce mostly sta tive clauses.
</nextsent>
<nextsent>examples of such verbs include be, like, feel, love, hate and many others.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1564">
<title id=" E09-2001.xml">frolog an accommodating text adventure game </title>
<section> the text-adventure game.  </section>
<citcontext>
<prevsection>
<prevsent>the parsing and the realization modules use the same linguistic resources, namely reversible grammar, lemma lexicon and morphological lexicon represented in the xmg grammatical formalism (crabbe?
</prevsent>
<prevsent>and duchier, 2004).
</prevsent>
</prevsection>
<citsent citstr=" C08-1032 ">
the xmggrammar used specifies tree adjoining grammar (tag) of around 500 trees and integrates asemantic dimension a` la (gardent, 2008).<papid> C08-1032 </papid></citsent>
<aftsection>
<nextsent>an example of the semantics associated with the player input open the chest?
</nextsent>
<nextsent>is depicted in figure 2.
</nextsent>
<nextsent>np ? = you np?
</nextsent>
<nextsent>vp np?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1565">
<title id=" E09-2001.xml">frolog an accommodating text adventure game </title>
<section> the text-adventure game.  </section>
<citcontext>
<prevsection>
<prevsent>open(e), agent(e,you), patient(e,c), chest(c), det(c) figure 2: parsing/realization for open the chest?
</prevsent>
<prevsent>the parsing module performs the syntactic analysis of command issued by the player, and constructs its semantic representation using thetag parser tulipa (kallmeyer et al, 2008) (illustrated in the figure 2 by ?).
</prevsent>
</prevsection>
<citsent citstr=" P07-1042 ">
the realization module works in the opposite direction, verbalizing the results of the execution of the command from the semantic representation using the tag surface re alizer geni (gardent and kow, 2007) (<papid> P07-1042 </papid>illustrated in the figure 2 by ?).</citsent>
<aftsection>
<nextsent>2.2 reference resolution and reference.
</nextsent>
<nextsent>generation the reference resolution (rr) module is responsible for mapping the semantic representations of definite and indefinite noun phrases and pronouns to individuals in the knowledge bases (illustrated in figure 3 by ?).
</nextsent>
<nextsent>the reference generation (rg)module performs the inverse task, that is it generates the semantic representation of noun phrase that uniquely identifies an individual in the knowledge bases (illustrated in the figure 3 by ?).
</nextsent>
<nextsent>the algorithms used for rr and rg are described in (koller et al, 2004).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1566">
<title id=" E09-1080.xml">unsupervised methods for head assignments </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in descriptive grammatical theories, the role of the head can range from the determinant of agreement or the locus of inflections, to the governor that selects the morphological form of its sister nodes or the constituent that is distributionally equivalent to its parent (corbett et al, 2006).
</prevsent>
<prevsent>in computational linguistics, heads mainly serve to select the lexical content on which the probability of production should depend (char niak, 1997; collins, 1999).
</prevsent>
</prevsection>
<citsent citstr=" D07-1096 ">
with the increased popularity of dependency parsing, head annotations have also become crucial level of syntactic information for transforming constituency treebanks to dependency structures (nivre et al, 2007)<papid> D07-1096 </papid>or richer syntactic representations (e.g., hockenmaier and steedman, 2007).<papid> J07-3004 </papid></citsent>
<aftsection>
<nextsent>for the wsj-section of the penn treebank, set of heuristic rules for assigning heads has emerged from the work of (magerman, 1995) <papid> P95-1037 </papid>and (collins, 1999) that has been employed in wide variety of studies and proven extremely useful, even in rather different applications from what the rules were originally intended for.</nextsent>
<nextsent>however, the rules are specific to english and the treebanks syntactic annotation, and do not offer much insights into how headedness can be learned in principle or in prac tice.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1567">
<title id=" E09-1080.xml">unsupervised methods for head assignments </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in descriptive grammatical theories, the role of the head can range from the determinant of agreement or the locus of inflections, to the governor that selects the morphological form of its sister nodes or the constituent that is distributionally equivalent to its parent (corbett et al, 2006).
</prevsent>
<prevsent>in computational linguistics, heads mainly serve to select the lexical content on which the probability of production should depend (char niak, 1997; collins, 1999).
</prevsent>
</prevsection>
<citsent citstr=" J07-3004 ">
with the increased popularity of dependency parsing, head annotations have also become crucial level of syntactic information for transforming constituency treebanks to dependency structures (nivre et al, 2007)<papid> D07-1096 </papid>or richer syntactic representations (e.g., hockenmaier and steedman, 2007).<papid> J07-3004 </papid></citsent>
<aftsection>
<nextsent>for the wsj-section of the penn treebank, set of heuristic rules for assigning heads has emerged from the work of (magerman, 1995) <papid> P95-1037 </papid>and (collins, 1999) that has been employed in wide variety of studies and proven extremely useful, even in rather different applications from what the rules were originally intended for.</nextsent>
<nextsent>however, the rules are specific to english and the treebanks syntactic annotation, and do not offer much insights into how headedness can be learned in principle or in prac tice.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1568">
<title id=" E09-1080.xml">unsupervised methods for head assignments </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in computational linguistics, heads mainly serve to select the lexical content on which the probability of production should depend (char niak, 1997; collins, 1999).
</prevsent>
<prevsent>with the increased popularity of dependency parsing, head annotations have also become crucial level of syntactic information for transforming constituency treebanks to dependency structures (nivre et al, 2007)<papid> D07-1096 </papid>or richer syntactic representations (e.g., hockenmaier and steedman, 2007).<papid> J07-3004 </papid></prevsent>
</prevsection>
<citsent citstr=" P95-1037 ">
for the wsj-section of the penn treebank, set of heuristic rules for assigning heads has emerged from the work of (magerman, 1995) <papid> P95-1037 </papid>and (collins, 1999) that has been employed in wide variety of studies and proven extremely useful, even in rather different applications from what the rules were originally intended for.</citsent>
<aftsection>
<nextsent>however, the rules are specific to english and the treebanks syntactic annotation, and do not offer much insights into how headedness can be learned in principle or in practice.
</nextsent>
<nextsent>moreover, the rules are heuristic and might still leave room for improvement with respect to recovering linguistic head assignment even on thepenn wsj corpus; in fact, we find that the head assignments according to the magerman-collinsrules correspond only in 85% of the cases to dependencies such as annotated in parc 700 dependency bank (see section 5).
</nextsent>
<nextsent>automatic methods for identifying heads are therefore of interest, both for practical and more fundamental linguistic reasons.
</nextsent>
<nextsent>in this paper we investigate possible ways of finding heads based on lexicalized tree structures that can be extracted from an available treebank.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1569">
<title id=" E09-1080.xml">unsupervised methods for head assignments </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>or should we assume that linguistic head assignments are based on the occurrence frequencies of the productive units they implywe present two new algorithms for unsupervised recovering of heads ? entropy minimization and greedy technique we call familiarity maximization?
</prevsent>
<prevsent>that can be seen as ways to opera tionalize these last two linguistic intuitions.
</prevsent>
</prevsection>
<citsent citstr=" P02-1017 ">
both algorithms are unsupervised, in the sense that they are trained on data without head annotations, but both take labeled phrase-structure trees as input.our work fits well with several recent approaches aimed at completely unsupervised learning of the key aspects of syntactic structure: lexical categories (schutze, 1993), phrase-structure (klein and manning, 2002; <papid> P02-1017 </papid>seginer, 2007), phrasal categories (borensztajn and zuidema,2007; reichart and rappoport, 2008) <papid> C08-1091 </papid>and dependencies (klein and manning, 2004).<papid> P04-1061 </papid></citsent>
<aftsection>
<nextsent>for the specific task addressed in this paper ? assigning heads in treebanks ? we only know of one earlier paper: chiang and bikel (2002).<papid> C02-1126 </papid></nextsent>
<nextsent>these authors investigated technique for identifying heads in constituency trees based on maximizing likelihood, using em, under tree insertion grammar (tig)model1.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1570">
<title id=" E09-1080.xml">unsupervised methods for head assignments </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>or should we assume that linguistic head assignments are based on the occurrence frequencies of the productive units they implywe present two new algorithms for unsupervised recovering of heads ? entropy minimization and greedy technique we call familiarity maximization?
</prevsent>
<prevsent>that can be seen as ways to opera tionalize these last two linguistic intuitions.
</prevsent>
</prevsection>
<citsent citstr=" C08-1091 ">
both algorithms are unsupervised, in the sense that they are trained on data without head annotations, but both take labeled phrase-structure trees as input.our work fits well with several recent approaches aimed at completely unsupervised learning of the key aspects of syntactic structure: lexical categories (schutze, 1993), phrase-structure (klein and manning, 2002; <papid> P02-1017 </papid>seginer, 2007), phrasal categories (borensztajn and zuidema,2007; reichart and rappoport, 2008) <papid> C08-1091 </papid>and dependencies (klein and manning, 2004).<papid> P04-1061 </papid></citsent>
<aftsection>
<nextsent>for the specific task addressed in this paper ? assigning heads in treebanks ? we only know of one earlier paper: chiang and bikel (2002).<papid> C02-1126 </papid></nextsent>
<nextsent>these authors investigated technique for identifying heads in constituency trees based on maximizing likelihood, using em, under tree insertion grammar (tig)model1.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1571">
<title id=" E09-1080.xml">unsupervised methods for head assignments </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>or should we assume that linguistic head assignments are based on the occurrence frequencies of the productive units they implywe present two new algorithms for unsupervised recovering of heads ? entropy minimization and greedy technique we call familiarity maximization?
</prevsent>
<prevsent>that can be seen as ways to opera tionalize these last two linguistic intuitions.
</prevsent>
</prevsection>
<citsent citstr=" P04-1061 ">
both algorithms are unsupervised, in the sense that they are trained on data without head annotations, but both take labeled phrase-structure trees as input.our work fits well with several recent approaches aimed at completely unsupervised learning of the key aspects of syntactic structure: lexical categories (schutze, 1993), phrase-structure (klein and manning, 2002; <papid> P02-1017 </papid>seginer, 2007), phrasal categories (borensztajn and zuidema,2007; reichart and rappoport, 2008) <papid> C08-1091 </papid>and dependencies (klein and manning, 2004).<papid> P04-1061 </papid></citsent>
<aftsection>
<nextsent>for the specific task addressed in this paper ? assigning heads in treebanks ? we only know of one earlier paper: chiang and bikel (2002).<papid> C02-1126 </papid></nextsent>
<nextsent>these authors investigated technique for identifying heads in constituency trees based on maximizing likelihood, using em, under tree insertion grammar (tig)model1.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1572">
<title id=" E09-1080.xml">unsupervised methods for head assignments </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>that can be seen as ways to opera tionalize these last two linguistic intuitions.
</prevsent>
<prevsent>both algorithms are unsupervised, in the sense that they are trained on data without head annotations, but both take labeled phrase-structure trees as input.our work fits well with several recent approaches aimed at completely unsupervised learning of the key aspects of syntactic structure: lexical categories (schutze, 1993), phrase-structure (klein and manning, 2002; <papid> P02-1017 </papid>seginer, 2007), phrasal categories (borensztajn and zuidema,2007; reichart and rappoport, 2008) <papid> C08-1091 </papid>and dependencies (klein and manning, 2004).<papid> P04-1061 </papid></prevsent>
</prevsection>
<citsent citstr=" C02-1126 ">
for the specific task addressed in this paper ? assigning heads in treebanks ? we only know of one earlier paper: chiang and bikel (2002).<papid> C02-1126 </papid></citsent>
<aftsection>
<nextsent>these authors investigated technique for identifying heads in constituency trees based on maximizing likelihood, using em, under tree insertion grammar (tig)model1.
</nextsent>
<nextsent>in this approach, headedness in some sense becomes state-split, allowing for grammars that more closely match empirical distributions over trees.
</nextsent>
<nextsent>the authors report somewhat disappointing results, however: the automatically induced head-annotations do not lead to significantly more accurate parsers than simple left most or rightmost head assignment schemes2.
</nextsent>
<nextsent>in section 2 we define the grammar model wewill use.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1573">
<title id=" E09-1080.xml">unsupervised methods for head assignments </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in section 3 we describe the head assignment algorithms.
</prevsent>
<prevsent>in section 4, 5 and 6 we 1the space over the possible head assignments that these authors consider ? essentially regular expressions over cfg rules ? is more restricted than in the current work where we consider larger domain of locality?.2however, the authors?
</prevsent>
</prevsection>
<citsent citstr=" P06-1055 ">
approach of using em for inducing latent information in treebanks has led to extremely accurate constituency parsers, that neither make use of nor produce headedness information; see (petrov et al, 2006) <papid> P06-1055 </papid>then describe our evaluations of these algorithms.</citsent>
<aftsection>
<nextsent>in this section we define lexicalised tree substitution grammars (ltsgs) and show how they can be read off unambiguously from head-annotated treebank.
</nextsent>
<nextsent>ltsgs are best defined as restriction of the more general probabilistic tree substitution grammars, which we describe first.
</nextsent>
<nextsent>2.1 tree substitution grammars.
</nextsent>
<nextsent>a tree substitution grammar (tsg) is 4-tuple vn, vt, s, ? where vn is the set of nonterminals; vt is the set of of terminals; ? vn is the startsymbol; and is the set of elementary trees, having root and internal nodes in vn and leaf nodes invnvt. two elementary trees ? and ? can be combined by means of the substitution operation ? ??
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1574">
<title id=" E09-1080.xml">unsupervised methods for head assignments </title>
<section> lexicalized tree grammars.  </section>
<citcontext>
<prevsection>
<prevsent>this method depends on the annotation of heads in the treebank,such as for instance provided for the penn tree bank by the magerman-collins head-percolation rules.
</prevsent>
<prevsent>we adopt the same constraint as used in this scheme, that each nonterminal node in every parse tree must have exactly one of its children annotated as head.
</prevsent>
</prevsection>
<citsent citstr=" P00-1058 ">
our method is similar to (chiang,2000), <papid> P00-1058 </papid>but is even simpler in ignoring the distinction between arguments and adjuncts (and thus the sister-adjunction operation).</citsent>
<aftsection>
<nextsent>figure 1 shows an example parse tree enriched with head-annotation: the suffix -h indicates that the specific node is the head of the production above it.
</nextsent>
<nextsent>s np nnp ms. nnp-h haag vp-h v-h plays np nnp-h elianti figure 1: parse tree of the sentence ms. haag plays elianti?
</nextsent>
<nextsent>annotated with head markers.once parse tree is annotated with head markers in such manner, we will be able to extract for every leaf its spine.
</nextsent>
<nextsent>starting from each lexical production we need to move upwards towards the root on path of head-marked nodes until we find the first internal node which is not marked as heador until we reach the root of the tree.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1577">
<title id=" E09-1080.xml">unsupervised methods for head assignments </title>
<section> head assignment algorithms.  </section>
<citcontext>
<prevsection>
<prevsent>in this section we will describe an entropy based algorithm, which aims at learning the simplest grammar fitting the data.
</prevsent>
<prevsent>specifically, we take supertagging?
</prevsent>
</prevsection>
<citsent citstr=" J99-2004 ">
perspective (bangalore and joshi, 1999) <papid> J99-2004 </papid>and aim at reducing the uncertainty about which elementary tree (supertag) to assign to given lexical item.</citsent>
<aftsection>
<nextsent>we achieve this by minimizing an objective function based on the general definition of entropy in information theory.the entropy measure that we are going to describe is calculated from the bag of lexicalized elementary trees extracted from given training corpus of head annotated parse trees.
</nextsent>
<nextsent>we definetl as discrete stochastic variable, taking as values the elements from the set of all the elementary trees having as lexical anchor {l1 , l2 , . . .
</nextsent>
<nextsent>, ln}.tl thus takes possible values with specific prob abilities; its entropy is then defined as: h(tl) = ? n? i=1 p(li) log2 p(li) (3) the most intuitive way to assign probabilities toeach elementary tree is considering its relative frequency in . if f(?)
</nextsent>
<nextsent>is the frequency of the fragment ? and f(l) is the total frequency of fragments with as anchor we will have: p(lj ) = f(lj ) f(lex(lj )) = f(lj ) n?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1578">
<title id=" E09-1080.xml">unsupervised methods for head assignments </title>
<section> implementation details.  </section>
<citcontext>
<prevsection>
<prevsent>this in general will ensure coverage, i.e. that all the sentences in thetest set can be successfully parsed, but still prioritizing lexicalized trees over cfg rules4.
</prevsent>
<prevsent>4.3 corpora.
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
the evaluations of the different models were carried out on the penn wall street journal corpus (marcus et al, 1993) <papid> J93-2004 </papid>for english, and the tiger treebank (brants et al, 2002) for german.</citsent>
<aftsection>
<nextsent>as gold standard head annotations corpora, we used the parc 700 dependency bank (king et al, 2003) and the tiger dependency bank (forst et al, 2004),which contain independent re annotations of extracts of the wsj and tiger treebanks.
</nextsent>
<nextsent>we evaluate the head annotations our algorithms find in two ways.
</nextsent>
<nextsent>first, we compare the head annotations to gold standard manual annotations3in our implementation, each cfg rule frequency is divided by factor 100.
</nextsent>
<nextsent>4in this paper, we prefer these simple heuristics over more elaborate techniques, as our goal is to compare the merits of the different head-assignment algorithms.of heads.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1579">
<title id=" E09-1080.xml">unsupervised methods for head assignments </title>
<section> results.  </section>
<citcontext>
<prevsection>
<prevsent>first, we compare the head annotations to gold standard manual annotations3in our implementation, each cfg rule frequency is divided by factor 100.
</prevsent>
<prevsent>4in this paper, we prefer these simple heuristics over more elaborate techniques, as our goal is to compare the merits of the different head-assignment algorithms.of heads.
</prevsent>
</prevsection>
<citsent citstr=" J04-4004 ">
second, we evaluate constituency parsing performance using an ltsg parser (trained on the various ltsgs), and state-of-the-art parser (bikel, 2004).<papid> J04-4004 </papid></citsent>
<aftsection>
<nextsent>5.1 gold standard head annotations.
</nextsent>
<nextsent>table 1 reports the performance of different algorithms against gold standard head annotations of the wsj and the tiger treebank.
</nextsent>
<nextsent>these annotations were obtained by converting the dependency structures of the parc corpus (700 sentences from section 23) and the tiger dependency bank (2000 sentences), into head annotations5.since the algorithm doesnt guarantee that the recovered head annotations always follow the onehead-per-node constraint, when evaluating the accuracy of head annotations of different algorithms,we exclude the cases in which in the gold corpus no head or multiple heads are assigned to the daughters of an internal node6, as well as cases in which an internal node has single daughter.in the evaluation against gold standard dependencies for the parc and tiger dependency banks, we find that the familiarity algorithm when run with postags and spine conversion obtains around 74% recall for english and 69% forgerman.
</nextsent>
<nextsent>the different scores of the random assignment for the two languages can be explained 5this procedure is not reported here for reasons of space,but it is available for other researchers (together with the extracted head assignments) at http://staff.science.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1582">
<title id=" E09-1082.xml">word lattices for multi source translation </title>
<section> approaches to multi-source.  </section>
<citcontext>
<prevsection>
<prevsent>indeed, several authors have seen the  dog barked very loudly big dog barked  loudly sub insert ? shift delete ? table 1: example minimum ter edit script.
</prevsent>
<prevsent>0 1thea 2big 3dog 4barked 5very?
</prevsent>
</prevsection>
<citsent citstr=" E06-1005 ">
6loudly figure 1: conversion of ter script from table 1 to confusion network.improvements in translation quality by performing multi-source translation using generic system combination techniques (matusov et al, 2006; <papid> E06-1005 </papid>paulik et al, 2007).<papid> W07-0727 </papid></citsent>
<aftsection>
<nextsent>one class of approaches to consensus decoding focuses on construction of confusion network or lattice1 from translation outputs, from which new sentences can be created using different reorderings or combinations of translation fragments (e.g., bangalore et al (2001); rosti et al (2007<papid> P07-1040 </papid>b)).</nextsent>
<nextsent>these methods differ in the types of lattices used, their means of creation, and scoring method usedto extract the best consensus output from the lat tice.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1583">
<title id=" E09-1082.xml">word lattices for multi source translation </title>
<section> approaches to multi-source.  </section>
<citcontext>
<prevsection>
<prevsent>indeed, several authors have seen the  dog barked very loudly big dog barked  loudly sub insert ? shift delete ? table 1: example minimum ter edit script.
</prevsent>
<prevsent>0 1thea 2big 3dog 4barked 5very?
</prevsent>
</prevsection>
<citsent citstr=" W07-0727 ">
6loudly figure 1: conversion of ter script from table 1 to confusion network.improvements in translation quality by performing multi-source translation using generic system combination techniques (matusov et al, 2006; <papid> E06-1005 </papid>paulik et al, 2007).<papid> W07-0727 </papid></citsent>
<aftsection>
<nextsent>one class of approaches to consensus decoding focuses on construction of confusion network or lattice1 from translation outputs, from which new sentences can be created using different reorderings or combinations of translation fragments (e.g., bangalore et al (2001); rosti et al (2007<papid> P07-1040 </papid>b)).</nextsent>
<nextsent>these methods differ in the types of lattices used, their means of creation, and scoring method usedto extract the best consensus output from the lat tice.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1584">
<title id=" E09-1082.xml">word lattices for multi source translation </title>
<section> approaches to multi-source.  </section>
<citcontext>
<prevsection>
<prevsent>0 1thea 2big 3dog 4barked 5very?
</prevsent>
<prevsent>6loudly figure 1: conversion of ter script from table 1 to confusion network.improvements in translation quality by performing multi-source translation using generic system combination techniques (matusov et al, 2006; <papid> E06-1005 </papid>paulik et al, 2007).<papid> W07-0727 </papid></prevsent>
</prevsection>
<citsent citstr=" P07-1040 ">
one class of approaches to consensus decoding focuses on construction of confusion network or lattice1 from translation outputs, from which new sentences can be created using different reorderings or combinations of translation fragments (e.g., bangalore et al (2001); rosti et al (2007<papid> P07-1040 </papid>b)).</citsent>
<aftsection>
<nextsent>these methods differ in the types of lattices used, their means of creation, and scoring method usedto extract the best consensus output from the lattice.
</nextsent>
<nextsent>the system used in this paper is variant of the one proposed in rosti et al (2007<papid> P07-1040 </papid>a), which we now describe in detail.</nextsent>
<nextsent>the first step informing lattice is to align the inputs.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1602">
<title id=" E09-1082.xml">word lattices for multi source translation </title>
<section> approaches to multi-source.  </section>
<citcontext>
<prevsection>
<prevsent>thicker lines indicate higher probability word arcs.
</prevsent>
<prevsent>when translating speech recognition output, previous work has shown that representing the ambiguity in the recognized text via confusion networks leads to better translations than simply translating the single best hypothesis of the speech recognition system (bertoldi et al, 2007).
</prevsent>
</prevsection>
<citsent citstr=" P08-1115 ">
the application of input lattices to other forms of input ambiguity has been limited to encoding input reorderings, word segmentation, or morphological segmentation, all showing improvements in translation quality (costa-jussa` et al, 2007; xu et al,2005; dyer et al, 2008).<papid> P08-1115 </papid></citsent>
<aftsection>
<nextsent>however, these applications encode the ambiguity arising from single input, while in this work we combine distinct inputs into more compact and expressive single input format.
</nextsent>
<nextsent>when given many monolingual inputs, we can apply ter and construct confusion network as in section 2.2.2 in this application of confusion networks, arc weights are calculated by summing votes from each input forgiven word, and normalizing all arcs leaving node to sum to 1.
</nextsent>
<nextsent>figure 3 shows an example of ter-derived input from iwslt data.
</nextsent>
<nextsent>because the decoder will handle reordering, we select the input with the lowest average ter against the other inputs to serve as the skeleton system, and do not create lattice with multiple skeletons.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1605">
<title id=" E09-1082.xml">word lattices for multi source translation </title>
<section> approaches to multi-source.  </section>
<citcontext>
<prevsection>
<prevsent>the problem becomes more complex when we consider cases of multi-lingual multi-source translation.
</prevsent>
<prevsent>we cannot easily apply ter across languages because there is no clear notion of an exact match between words.
</prevsent>
</prevsection>
<citsent citstr=" N03-1003 ">
matusov et al (2006) <papid> E06-1005 </papid>propose using statistical word alignment algorithm as more robust way of aligning (monolingual)outputs into confusion network for system com2barzilay and lee (2003) <papid> N03-1003 </papid>construct lattices over paraphrases using an iterative pairwise multiple sequence alignment (msa) algorithm.</citsent>
<aftsection>
<nextsent>unlike our approach, msa does not allow reordering of inputs.
</nextsent>
<nextsent>721bination.
</nextsent>
<nextsent>we take similar approach for multilingual lattice generation.
</nextsent>
<nextsent>our process consists of four steps: (i) align words for each of the n(n ? 1) pairs of inputs; (ii) choose an input (or many inputs) to be the lattice skeleton; (iii) extract all minimal consistent alignments between the skeleton and the other inputs; and (iv) add links to the lattice for each aligned phrase pair.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1606">
<title id=" E09-1082.xml">word lattices for multi source translation </title>
<section> approaches to multi-source.  </section>
<citcontext>
<prevsection>
<prevsent>our process consists of four steps: (i) align words for each of the n(n ? 1) pairs of inputs; (ii) choose an input (or many inputs) to be the lattice skeleton; (iii) extract all minimal consistent alignments between the skeleton and the other inputs; and (iv) add links to the lattice for each aligned phrase pair.
</prevsent>
<prevsent>a multi-parallel corpus such as europarl (koehn, 2005) is ideally suited for training this setup, as training data is available for each pair of input languages needed by the word aligner.
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
we used the giza++ word alignment tool (och andney, 2003) <papid> J03-1002 </papid>for aligning inputs, trained on portion of the europarl training data for each pair.</citsent>
<aftsection>
<nextsent>we select skeleton input based on which single-language translation system performs the best when translating development set.
</nextsent>
<nextsent>for our europarl test condition, this was french.
</nextsent>
<nextsent>we define minimal consistent alignment (mca) as member of the set of multi-wordalignment pairs that can be extracted from manyto-many word alignment between skeleton sentence and non-skeleton sentence with the following restrictions: (i) no word in or is used more than once in the set of mcas; (ii) words and phrases selected from cannot be aligned to null; and (iii) no smaller mca can be decomposed from given pair.
</nextsent>
<nextsent>this definition is similar to that of minimal translation units as described in quirk and menezes (2006), <papid> N06-1002 </papid>although they allow null words on either side.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1607">
<title id=" E09-1082.xml">word lattices for multi source translation </title>
<section> approaches to multi-source.  </section>
<citcontext>
<prevsection>
<prevsent>for our europarl test condition, this was french.
</prevsent>
<prevsent>we define minimal consistent alignment (mca) as member of the set of multi-wordalignment pairs that can be extracted from manyto-many word alignment between skeleton sentence and non-skeleton sentence with the following restrictions: (i) no word in or is used more than once in the set of mcas; (ii) words and phrases selected from cannot be aligned to null; and (iii) no smaller mca can be decomposed from given pair.
</prevsent>
</prevsection>
<citsent citstr=" N06-1002 ">
this definition is similar to that of minimal translation units as described in quirk and menezes (2006), <papid> N06-1002 </papid>although they allow null words on either side.</citsent>
<aftsection>
<nextsent>different word alignment approaches will result in different sets of mcas.
</nextsent>
<nextsent>for input lattices, we want sets of mcas with as many aligned wordsas possible, while minimising the average number of words in each pair in the set.
</nextsent>
<nextsent>experiments with giza++ on the europarl data showed thatthe grow-diag-final-and?
</nextsent>
<nextsent>word alignment sym metrization heuristic had the best balance between coverage and pair length: over 85% of skeleton words were part of non-null minimal pair, and the average length of each pair was roughly 1.5words.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1610">
<title id=" E09-1082.xml">word lattices for multi source translation </title>
<section> experiments: monolingual input.  </section>
<citcontext>
<prevsection>
<prevsent>we examine two datasets: the btec italian-english corpus (takezawa et al, 2002), and the multiple translation chinese to english (mtc) corpora,3 as used in past years?
</prevsent>
<prevsent>nist mt evaluations.
</prevsent>
</prevsection>
<citsent citstr=" P07-2045 ">
all of our translation experiments use the moses decoder (koehn et al, 2007), <papid> P07-2045 </papid>and are evaluated using bleu-4.</citsent>
<aftsection>
<nextsent>moses is phrase-based decoder with features for lexicalized reordering,distance-based reordering, phrase and word translation probabilities, phrase and word counts, and an n-gram language model.
</nextsent>
<nextsent>3.1 english to italian.
</nextsent>
<nextsent>we use the portion of the btec data made available for the italian-english translation task at iwslt 2007, consisting of approximately 24,000sentences.
</nextsent>
<nextsent>we also use the europarl english italian parallel corpus to supplement our training data with approximately 1.2 million out-of domain sentences.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1611">
<title id=" E09-1082.xml">word lattices for multi source translation </title>
<section> experiments: monolingual input.  </section>
<citcontext>
<prevsection>
<prevsent>724 input language test2006 test2007 french (fr) 29.72 30.21 spanish (es) 29.55 29.62 swedish (sv) 29.33 29.44 portuguese (pt) 28.75 28.79 danish (da) 27.20 27.48 greek (el) 26.93 26.78 italian (it) 26.82 26.51 german (de) 24.04 24.41 dutch (nl) 23.79 24.28 finnish (fi) 18.96 18.85 table 5: bleu scores for individual translation systems into english trained on europarl, from best to worst.verse is true for syscomb.
</prevsent>
<prevsent>given the robust performance of max when translation scores originated from the same translation model in english to italian, it is not surprising that it favors the case where all the outputs are scored by the same model (all tuned?).
</prevsent>
</prevsection>
<citsent citstr=" D07-1105 ">
on the other hand, diversity amongst the system outputs has been shown to be important to the performance of system combination techniques (macherey and och, 2007).<papid> D07-1105 </papid></citsent>
<aftsection>
<nextsent>this may give an indication as to why the self tuned data produced higher scores in consensus decoding ? the outputs will be more highly divergent due to their different tuning conditions.
</nextsent>
<nextsent>multilingual cases are the traditional realm ofmulti-source translation.
</nextsent>
<nextsent>we no longer have directly comparable translation models; instead each input language has separate set of rules for translating to the output language.
</nextsent>
<nextsent>however, the availability of (and demand for) multi-parallel corpora makes this form of multi-source translation of great practical use.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1612">
<title id=" E09-1082.xml">word lattices for multi source translation </title>
<section> experiments: multilingual input.  </section>
<citcontext>
<prevsection>
<prevsent>interestingly, the order of the languages chosen ite rates between the roman and germanic language families and includes greek early on.
</prevsent>
<prevsent>this supports our claim that diversity is important.
</prevsent>
</prevsection>
<citsent citstr=" D08-1078 ">
note though that finnish, which is also in separate language family, is selected last, most likely due to difficulties in word alignment and translation stemming from its morphological complexity (birch et al, 2008).<papid> D08-1078 </papid></citsent>
<aftsection>
<nextsent>this finding might also carry over to phrase-table triangulation (cohn and lapata, 2007), <papid> P07-1092 </papid>where multi-parallel data is used in training to augment standard translation 725 approach test2006 test2007 french only 29.72 30.21 french + swedish max 29.86 30.13 lattice 29.33 29.97 multi lattice 29.55 29.88 syscomb 31.32 31.77 french + swedish + spanish max 30.18 30.33 lattice 29.98 30.45 multi lattice 30.50 30.50 syscomb 33.77 33.87 6 languages.</nextsent>
<nextsent>max 28.37 28.33 lattice 30.22 30.91 multi lattice 30.59 30.59 syscomb 35.47 36.03 table 6: bleu scores for multi-source translation systems into english trained on europarl.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1613">
<title id=" E09-1082.xml">word lattices for multi source translation </title>
<section> experiments: multilingual input.  </section>
<citcontext>
<prevsection>
<prevsent>this supports our claim that diversity is important.
</prevsent>
<prevsent>note though that finnish, which is also in separate language family, is selected last, most likely due to difficulties in word alignment and translation stemming from its morphological complexity (birch et al, 2008).<papid> D08-1078 </papid></prevsent>
</prevsection>
<citsent citstr=" P07-1092 ">
this finding might also carry over to phrase-table triangulation (cohn and lapata, 2007), <papid> P07-1092 </papid>where multi-parallel data is used in training to augment standard translation 725 approach test2006 test2007 french only 29.72 30.21 french + swedish max 29.86 30.13 lattice 29.33 29.97 multi lattice 29.55 29.88 syscomb 31.32 31.77 french + swedish + spanish max 30.18 30.33 lattice 29.98 30.45 multi lattice 30.50 30.50 syscomb 33.77 33.87 6 languages.</citsent>
<aftsection>
<nextsent>max 28.37 28.33 lattice 30.22 30.91 multi lattice 30.59 30.59 syscomb 35.47 36.03 table 6: bleu scores for multi-source translation systems into english trained on europarl.
</nextsent>
<nextsent>single source french decoding is shown as baseline.
</nextsent>
<nextsent>system.we choose to evaluate translation performance at three combination levels: two languages (french and swedish), three languages(+spanish), and six languages (+danish, portuguese, italian).
</nextsent>
<nextsent>for each combination we apply max, syscomb, french skeleton lattice in put translation lattice, and monotone decoding over multiple skeleton lattices, multilattice.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1614">
<title id=" E12-1058.xml">the impact of spelling errors on patent search </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>query spelling correction queries which are submitted to standard web search engines differ from queries which are posed to patent search engines with respect to both length and language diversity.
</prevsent>
<prevsent>hence, research in the field of web search is concerned with suggesting reasonable alternatives to misspelled queries rather than correcting single words (li et al  2011).
</prevsent>
</prevsection>
<citsent citstr=" D07-1090 ">
since standard spelling correction dictionaries (e.g. aspell) are not able to capture the rich language used in web queries, large-scale knowledge sources such as wikipedia (li et al  2011), query logs (chen et al  2007), and large n-gram corpora (brants et al ., 2007) <papid> D07-1090 </papid>are employed.</citsent>
<aftsection>
<nextsent>it should be noted thatthe set of correctly written assignee names is unknown for the uspto patent corpus.
</nextsent>
<nextsent>moreover, spelling errors are modeled on the basis of language models (li et al  2011).
</nextsent>
<nextsent>okuno (2011) proposes generative model to encounter spelling errors, where the original query is expanded based on alternatives produced by small edit distance to the original query.
</nextsent>
<nextsent>this strategy correlates to the trivial query expansion set (cf.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1615">
<title id=" E12-1076.xml">midge generating image descriptions from computervision detections </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>results show that the generation system outperforms state-of-the-art systems, automatically generating some of the most natural image descriptions to date.
</prevsent>
<prevsent>it is becoming real possibility for intelligent systems to talk about the visual world.
</prevsent>
</prevsection>
<citsent citstr=" W11-0326 ">
new ways of mapping computervision to generated language have emerged in the past few years, with focus on pairing detections in an image to words (farhadi et al  2010; li et al  2011; <papid> W11-0326 </papid>kulkarni etal., 2011; yang et al  2011).<papid> D11-1041 </papid></citsent>
<aftsection>
<nextsent>the goal in connecting vision to language has varied: systems have started producing language that is descriptive and poetic (li et al  2011), <papid> W11-0326 </papid>summaries that add content where the computervision system does not (yang et al  2011), <papid> D11-1041 </papid>and captions copied directly from other images that are globally (farhadi et al  2010) and locally similar (ordonez et al  2011).a commonality between all of these approaches is that they aim to produce natural sounding descriptions from computervision de tections.</nextsent>
<nextsent>this commonality is our starting point: we aim to design system capable of producingnatural-sounding descriptions from computer vision detections that are flexible enough to become more descriptive and poetic, or include likely in the bus by the road with clear blue sky figure 1: example image with generated description.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1616">
<title id=" E12-1076.xml">midge generating image descriptions from computervision detections </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>results show that the generation system outperforms state-of-the-art systems, automatically generating some of the most natural image descriptions to date.
</prevsent>
<prevsent>it is becoming real possibility for intelligent systems to talk about the visual world.
</prevsent>
</prevsection>
<citsent citstr=" D11-1041 ">
new ways of mapping computervision to generated language have emerged in the past few years, with focus on pairing detections in an image to words (farhadi et al  2010; li et al  2011; <papid> W11-0326 </papid>kulkarni etal., 2011; yang et al  2011).<papid> D11-1041 </papid></citsent>
<aftsection>
<nextsent>the goal in connecting vision to language has varied: systems have started producing language that is descriptive and poetic (li et al  2011), <papid> W11-0326 </papid>summaries that add content where the computervision system does not (yang et al  2011), <papid> D11-1041 </papid>and captions copied directly from other images that are globally (farhadi et al  2010) and locally similar (ordonez et al  2011).a commonality between all of these approaches is that they aim to produce natural sounding descriptions from computervision de tections.</nextsent>
<nextsent>this commonality is our starting point: we aim to design system capable of producingnatural-sounding descriptions from computer vision detections that are flexible enough to become more descriptive and poetic, or include likely in the bus by the road with clear blue sky figure 1: example image with generated description.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1626">
<title id=" E12-1076.xml">midge generating image descriptions from computervision detections </title>
<section> generation.  </section>
<citcontext>
<prevsection>
<prevsent>table 2: example attribute classes and values.
</prevsent>
<prevsent>group adjectives into broader attribute classes,3 and the generation system uses these classes when deciding how to describe objects.
</prevsent>
</prevsection>
<citsent citstr=" P08-1119 ">
to group adjectives, we use bootstrapping technique (kozareva et al  2008) <papid> P08-1119 </papid>that learns which adjectives tend toco-occur, and groups these together to form an attribute class.</citsent>
<aftsection>
<nextsent>co-occurrence is computed usingcosine (distributional) similarity between adjectives, considering adjacent nouns as context (i.e.,jj nn constructions).
</nextsent>
<nextsent>contexts (nouns) for adjectives are weighted using pointwise mutual information and only the top 1000 nouns are selected for every adjective.
</nextsent>
<nextsent>some of the learned attribute classes are given in table 2.
</nextsent>
<nextsent>in the flickr corpus, we find that each attribute (color, size, etc.), rarely has more than single value in the final description, with the most common (color) co-occurring less than 2% of the time.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1627">
<title id=" E12-1076.xml">midge generating image descriptions from computervision detections </title>
<section> generation.  </section>
<citcontext>
<prevsection>
<prevsent>midge enforces this idea to select the most likely word for each attribute from the detections.
</prevsent>
<prevsent>in noun phrase headed by an object noun, np{nn noun}, the pre nominal adjective (jj v) foreach attribute is selected using maximum likelihood.
</prevsent>
</prevsection>
<citsent citstr=" C04-1096 ">
4.2.4 step 4: group plural show to generate natural-sounding spatial relations and modifiers for set of objects, as opposed to single object, is still an open problem (fu nakoshi et al  2004; <papid> C04-1096 </papid>gatt, 2006).</citsent>
<aftsection>
<nextsent>in this work, weuse simple method to group all same-type objects together, associate them to the plural form listed in the kb, discard the modifiers, andre turn spatial relations based on the first recognized 3what in computervision are called attributes are called values in nlg.
</nextsent>
<nextsent>a value like red belongs to color attribute, and we use this distinction in the system.
</nextsent>
<nextsent>member of the group.
</nextsent>
<nextsent>4.2.5 step 5: gather local subtrees around object nouns 1 2 np nn jj* dt{0,1} ? vp{vbz} np{nn n} 3 4 np vp{vb(g|n)} np{nn n} np pp{in} np{nn n} 5 6 pp np{nn n}in ? vp pp{in} vb(g|n|z) ? 7 vp np{nn n}vb(g|n|z) figure 9: initial subtree frames for generation, present tense declarative phrases.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1628">
<title id=" E12-1076.xml">midge generating image descriptions from computervision detections </title>
<section> generation.  </section>
<citcontext>
<prevsection>
<prevsent>the final string is then the one with the most words.
</prevsent>
<prevsent>we find that the second method produces descriptions that seem more natural and varied than the n-gram ranking method for our development set, and so use the longest string method in evaluation.
</prevsent>
</prevsection>
<citsent citstr=" P11-2041 ">
4.4.2 step 8: pre nominal modifier ordering to order sets of selected adjectives, we use the top-scoring pre nominal modifier ordering model discussed in mitchell et al (2011).<papid> P11-2041 </papid></citsent>
<aftsection>
<nextsent>this is an gram model constructed over noun phrases that were extracted from an automatically parsed version of the new york times portion of the gigaword corpus (graff and cieri, 2003).
</nextsent>
<nextsent>with this in place, blue clear sky becomes clear blue sky,wooden brown table becomes brown wooden table, etc.
</nextsent>
<nextsent>each set of sentences is generated with ?
</nextsent>
<nextsent>(likeli hood cutoff) set to .01 and ?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1631">
<title id=" E12-1076.xml">midge generating image descriptions from computervision detections </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>(observation count cutoff) set to 3.
</prevsent>
<prevsent>we compare the system againsthuman-written descriptions and two state-of-the art vision-to-language systems, the kulkarni et al  (2011) and yang et al (2011) <papid> D11-1041 </papid>systems.human judgments were collected using amazons mechanical turk (amazon, 2011).</prevsent>
</prevsection>
<citsent citstr=" J09-4008 ">
we follow recommended practices for evaluating annlg system (reiter and belz, 2009) <papid> J09-4008 </papid>and for running study on mechanical turk (callison-burch and dredze, 2010), using balanced design with each subject rating 3 descriptions from each sys tem.</citsent>
<aftsection>
<nextsent>subjects rated their level of agreement ona 5-point likert scale including neutral middle position, and since quality ratings are ordinal(points are not necessarily equidistant), we evaluate responses using non-parametric test.
</nextsent>
<nextsent>participants that took less than 3 minutes to answer all 60 questions and did not include human like rating for at least 1 of the 3 human-written descriptions were removed and replaced.
</nextsent>
<nextsent>it is important to note that this evaluation compares full generation sys tems; many factors are at play in each system that may also influence participants?
</nextsent>
<nextsent>perception, e.g.,sentence length (napoles et al  2011) <papid> W11-1611 </papid>and punctuation decisions.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1632">
<title id=" E12-1076.xml">midge generating image descriptions from computervision detections </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>participants that took less than 3 minutes to answer all 60 questions and did not include human like rating for at least 1 of the 3 human-written descriptions were removed and replaced.
</prevsent>
<prevsent>it is important to note that this evaluation compares full generation sys tems; many factors are at play in each system that may also influence participants?
</prevsent>
</prevsection>
<citsent citstr=" W11-1611 ">
perception, e.g.,sentence length (napoles et al  2011) <papid> W11-1611 </papid>and punctuation decisions.</citsent>
<aftsection>
<nextsent>the systems are evaluated on set of 840 images evaluated in the original kulkarni et al  (2011) system.
</nextsent>
<nextsent>participants were asked to judge the statements given in figure 12, from strongly disagree to strongly agree.
</nextsent>
<nextsent>754 grammaticality main aspects correctness order human likeness human 4 (3.77, 1.19) 4 (4.09, 0.97) 4 (3.81, 1.11) 4 (3.88, 1.05) 4 (3.88, 0.96) midge 3 (2.95, 1.42) 3 (2.86, 1.35) 3 (2.95, 1.34) 3 (2.92, 1.25) 3 (3.16, 1.17) kulkarni et al 2011 3 (2.83, 1.37) 3 (2.84, 1.33) 3 (2.76, 1.34) 3 (2.78, 1.23) 3 (3.13, 1.23) yang et al 2011 <papid> D11-1041 </papid>3 (2.95, 1.49) 2 (2.31, 1.30) 2 (2.46, 1.36) 2 (2.53, 1.26) 3 (2.97, 1.23) table 4: median scores for systems, mean and standard deviation in parentheses.</nextsent>
<nextsent>distance between points on the rating scale cannot be assumed to be equidistant, and so we analyze results using non-parametric test.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1634">
<title id=" E09-1046.xml">generating a non english subjectivity lexicon relations that matter </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>features, and whether comments made are positive or negative.
</prevsent>
<prevsent>another application is in political research, where public opinion could be assessed by analyzing user generated online data (blogs, discussion forums, etc.).most current methods for subjectivity identification relyon subjectivity lexicons, which list words that are usually associated with positive or negative sentiments or opinions (i.e., words withpolarity).
</prevsent>
</prevsection>
<citsent citstr=" W02-1011 ">
such lexicon can be used, e.g., to classify individual sentences or phrases as subjective or not, and as bearing positive or negative sentiments (pang et al, 2002; <papid> W02-1011 </papid>kim and hovy, 2004;<papid> C04-1200 </papid>wilson et al, 2005<papid> H05-1044 </papid>a).</citsent>
<aftsection>
<nextsent>for english, manually created subjectivity lexicons have been available fora while, but for many other languages such resources are still missing.
</nextsent>
<nextsent>we describe language-independent method for automatically bootstrapping subjectivity lexicon, and apply and evaluate it for the dutch language.
</nextsent>
<nextsent>the method starts with an english lexicon of positive and negative words, automatically translated into the target language (dutch in our case).
</nextsent>
<nextsent>a pagerank-like algorithm is applied to the dutch wordnet in order to filter and expand the set of words obtained through translation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1635">
<title id=" E09-1046.xml">generating a non english subjectivity lexicon relations that matter </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>features, and whether comments made are positive or negative.
</prevsent>
<prevsent>another application is in political research, where public opinion could be assessed by analyzing user generated online data (blogs, discussion forums, etc.).most current methods for subjectivity identification relyon subjectivity lexicons, which list words that are usually associated with positive or negative sentiments or opinions (i.e., words withpolarity).
</prevsent>
</prevsection>
<citsent citstr=" C04-1200 ">
such lexicon can be used, e.g., to classify individual sentences or phrases as subjective or not, and as bearing positive or negative sentiments (pang et al, 2002; <papid> W02-1011 </papid>kim and hovy, 2004;<papid> C04-1200 </papid>wilson et al, 2005<papid> H05-1044 </papid>a).</citsent>
<aftsection>
<nextsent>for english, manually created subjectivity lexicons have been available fora while, but for many other languages such resources are still missing.
</nextsent>
<nextsent>we describe language-independent method for automatically bootstrapping subjectivity lexicon, and apply and evaluate it for the dutch language.
</nextsent>
<nextsent>the method starts with an english lexicon of positive and negative words, automatically translated into the target language (dutch in our case).
</nextsent>
<nextsent>a pagerank-like algorithm is applied to the dutch wordnet in order to filter and expand the set of words obtained through translation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1636">
<title id=" E09-1046.xml">generating a non english subjectivity lexicon relations that matter </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>features, and whether comments made are positive or negative.
</prevsent>
<prevsent>another application is in political research, where public opinion could be assessed by analyzing user generated online data (blogs, discussion forums, etc.).most current methods for subjectivity identification relyon subjectivity lexicons, which list words that are usually associated with positive or negative sentiments or opinions (i.e., words withpolarity).
</prevsent>
</prevsection>
<citsent citstr=" H05-1044 ">
such lexicon can be used, e.g., to classify individual sentences or phrases as subjective or not, and as bearing positive or negative sentiments (pang et al, 2002; <papid> W02-1011 </papid>kim and hovy, 2004;<papid> C04-1200 </papid>wilson et al, 2005<papid> H05-1044 </papid>a).</citsent>
<aftsection>
<nextsent>for english, manually created subjectivity lexicons have been available fora while, but for many other languages such resources are still missing.
</nextsent>
<nextsent>we describe language-independent method for automatically bootstrapping subjectivity lexicon, and apply and evaluate it for the dutch language.
</nextsent>
<nextsent>the method starts with an english lexicon of positive and negative words, automatically translated into the target language (dutch in our case).
</nextsent>
<nextsent>a pagerank-like algorithm is applied to the dutch wordnet in order to filter and expand the set of words obtained through translation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1638">
<title id=" E09-1046.xml">generating a non english subjectivity lexicon relations that matter </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>we discuss experimental results in section 5 and conclude in section 6.
</prevsent>
<prevsent>creating subjectivity lexicons for languages other than english has only recently attracted attention of the research community.
</prevsent>
</prevsection>
<citsent citstr=" P07-1123 ">
(mihalcea et al, 2007)<papid> P07-1123 </papid>describes experiments with subjectivity classification for romanian.</citsent>
<aftsection>
<nextsent>the authors start with an english subjectivity lexicon with 6,856 entries, opinion finder (wiebe and riloff, 2005), and automatically translate it into romanian using two bilingual dictionaries, obtaining romanian lexicon with 4,983 entries.
</nextsent>
<nextsent>a manual evaluation of sample of 123 entries of this lexicon showed that 50% of the entries do indicate subjectivity.
</nextsent>
<nextsent>in (banea et al, 2008)<papid> L08-1086 </papid>different approach based on boo strapping was explored for roma nian.</nextsent>
<nextsent>the method starts with small seed set of 60 words, which is iteratively (1) expanded by adding synonyms from an online romanian dictionary, and (2) filtered by removing words which are not similar (at preset threshold) to the original seed, according to an lsa-based similarity measure computed on half-million word corpus of romanian.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1639">
<title id=" E09-1046.xml">generating a non english subjectivity lexicon relations that matter </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the authors start with an english subjectivity lexicon with 6,856 entries, opinion finder (wiebe and riloff, 2005), and automatically translate it into romanian using two bilingual dictionaries, obtaining romanian lexicon with 4,983 entries.
</prevsent>
<prevsent>a manual evaluation of sample of 123 entries of this lexicon showed that 50% of the entries do indicate subjectivity.
</prevsent>
</prevsection>
<citsent citstr=" L08-1086 ">
in (banea et al, 2008)<papid> L08-1086 </papid>different approach based on boo strapping was explored for roma nian.</citsent>
<aftsection>
<nextsent>the method starts with small seed set of 60 words, which is iteratively (1) expanded by adding synonyms from an online romanian dictionary, and (2) filtered by removing words which are not similar (at preset threshold) to the original seed, according to an lsa-based similarity measure computed on half-million word corpus of romanian.
</nextsent>
<nextsent>the lexicon obtained after 5iterations of the method was used for sentence level sentiment classification, indicating an 18% improvement over the lexicon of (mihalcea et al, 2007)<papid> P07-1123 </papid>.</nextsent>
<nextsent>both these approaches produce unordered sets of positive and negative words.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1642">
<title id=" E09-1046.xml">generating a non english subjectivity lexicon relations that matter </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>both these approaches produce unordered sets of positive and negative words.
</prevsent>
<prevsent>our method, on the other hand, assigns polarity scores towords and produces ranking of words by polarity, which provides more flexible experimental framework for applications that will use the lexicon.
</prevsent>
</prevsection>
<citsent citstr=" P07-1054 ">
esuli and sebastiani (esuli and sebastiani, 2007) <papid> P07-1054 </papid>apply an algorithm based on page rank torank synsets in english wordnet according to positive and negativite sentiments.</citsent>
<aftsection>
<nextsent>the authors view wordnet as graph where nodes are synsets and synsets are linked with the synsets of terms usedin their glosses (definitions).
</nextsent>
<nextsent>the algorithm is ini tial ized with positivity/negativity scores provided in sentiwordnet (esuli and sebastiani, 2006), an english sentiment lexicon.
</nextsent>
<nextsent>the weights are then distributed through the graph using an the algorithm similar to pagerank.
</nextsent>
<nextsent>authors conclude that larger initial seed sets result in better ranking produced by the method.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1662">
<title id=" E09-1029.xml">company oriented extractive summarization of financial news </title>
<section> data.  </section>
<citcontext>
<prevsection>
<prevsent>the total number of (company name, news collection) pairs is 46,444.the corpus is cleaned of html tags, embedded graphics and unrelated information (e.g., ads, frames) with set of manually devised rules.
</prevsent>
<prevsent>the filtering is not perfect but removes most of the noise.
</prevsent>
</prevsection>
<citsent citstr=" L08-1165 ">
each article is passed through language processing pipeline (described in (atserias et al, 2008)).<papid> L08-1165 </papid></citsent>
<aftsection>
<nextsent>sentence boundaries are identified by means of simple heuristics.
</nextsent>
<nextsent>the text is tokenized according to penn treebank style and each token lemmatized using wordnets morphological functions.
</nextsent>
<nextsent>part of speech tags and named entities (loc, per, org, misc) are identified by meansof publicly available named-entity tagger5 (cia ramita &amp; altun, 2006, <papid> W06-1670 </papid>supersense).</nextsent>
<nextsent>apart from that, all sentences which are shorter than 5 tokens and contain neither nouns nor verbs are sorted out.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1663">
<title id=" E09-1029.xml">company oriented extractive summarization of financial news </title>
<section> data.  </section>
<citcontext>
<prevsection>
<prevsent>sentence boundaries are identified by means of simple heuristics.
</prevsent>
<prevsent>the text is tokenized according to penn treebank style and each token lemmatized using wordnets morphological functions.
</prevsent>
</prevsection>
<citsent citstr=" W06-1670 ">
part of speech tags and named entities (loc, per, org, misc) are identified by meansof publicly available named-entity tagger5 (cia ramita &amp; altun, 2006, <papid> W06-1670 </papid>supersense).</citsent>
<aftsection>
<nextsent>apart from that, all sentences which are shorter than 5 tokens and contain neither nouns nor verbs are sorted out.
</nextsent>
<nextsent>we apply the latter filter as we are interested in textual information only.
</nextsent>
<nextsent>numeric information contained, e.g., in tables can be easily and more reliably obtained from the indices tables available online.
</nextsent>
<nextsent>in company-oriented summarization query expansion is crucial because, by default, our query contains only the symbol, that is the abbreviation ofthe name of the company.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1664">
<title id=" E09-1029.xml">company oriented extractive summarization of financial news </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the only information used in the rules is the presence or absence of certain key phrases provided by human expert who judged them to be influential factors potentially moving stockmarkets?.
</prevsent>
<prevsent>in this approach, training data is required to measure the usefulness of the keyphrasesfor each of the three classes.
</prevsent>
</prevsection>
<citsent citstr=" C08-1060 ">
more recently, lerman et al (2008) <papid> C08-1060 </papid>introduced forecasting system for prediction markets that combines news analysis with price trend analysis model.</citsent>
<aftsection>
<nextsent>this approach was shown to be successful for the forecasting of public opinion about political candidates in such prediction markets.
</nextsent>
<nextsent>our approach can be seen as complement to both these approaches, necessary especially for financial markets where the news typically cover many events, only some related to the company of interest.
</nextsent>
<nextsent>unsupervized summarization systems extract sentences whose relevance can be inferred fromthe inter-sentence relations in the document collection.
</nextsent>
<nextsent>in (radev et al, 2000), the centro id of the collection, i.e., the words with the highest tf*idf, is considered and the sentences which contain more words from the centro id are extracted.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1665">
<title id=" E12-3002.xml">cross lingual genre classification </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>my experiments show this method to perform equally well or better than full text translation combined with monolingual classification, while requiring fewer resources.
</prevsent>
<prevsent>automated text classification has become standard practice with applications in fields such as information retrieval and natural language processing.
</prevsent>
</prevsection>
<citsent citstr=" W02-1011 ">
the most common basis for text classification is by topic (joachims, 1998; sebastiani, 2002), but other classification criteria have evolved, including sentiment (pang et al 2002), <papid> W02-1011 </papid>authorship (de vel et al 2001; stamatatos et al 2000<papid> J00-4001 </papid>a), and author personality (oberlander and nowson, 2006), <papid> P06-2081 </papid>as well as categories relevant to filter algorithms (e.g., spam or inappropriate contents for minors).genre is another text characteristic, often described as orthogonal to topic.</citsent>
<aftsection>
<nextsent>it has been shown by biber (1988) and others after him, that the genre of text affects its formal properties.
</nextsent>
<nextsent>it is therefore possible to use cues (e.g., lexical, syntactic, structural) from text as features to predict its genre, which can then feed into information retrieval applications (karlgren and cutting,1994; <papid> C94-2174 </papid>kessler et al 1997; <papid> P97-1005 </papid>finn and kushmerick, 2006; freund et al 2006).</nextsent>
<nextsent>this is because users may want documents that serve particular communicative purpose, as well as being on particular topic.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1666">
<title id=" E12-3002.xml">cross lingual genre classification </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>my experiments show this method to perform equally well or better than full text translation combined with monolingual classification, while requiring fewer resources.
</prevsent>
<prevsent>automated text classification has become standard practice with applications in fields such as information retrieval and natural language processing.
</prevsent>
</prevsection>
<citsent citstr=" J00-4001 ">
the most common basis for text classification is by topic (joachims, 1998; sebastiani, 2002), but other classification criteria have evolved, including sentiment (pang et al 2002), <papid> W02-1011 </papid>authorship (de vel et al 2001; stamatatos et al 2000<papid> J00-4001 </papid>a), and author personality (oberlander and nowson, 2006), <papid> P06-2081 </papid>as well as categories relevant to filter algorithms (e.g., spam or inappropriate contents for minors).genre is another text characteristic, often described as orthogonal to topic.</citsent>
<aftsection>
<nextsent>it has been shown by biber (1988) and others after him, that the genre of text affects its formal properties.
</nextsent>
<nextsent>it is therefore possible to use cues (e.g., lexical, syntactic, structural) from text as features to predict its genre, which can then feed into information retrieval applications (karlgren and cutting,1994; <papid> C94-2174 </papid>kessler et al 1997; <papid> P97-1005 </papid>finn and kushmerick, 2006; freund et al 2006).</nextsent>
<nextsent>this is because users may want documents that serve particular communicative purpose, as well as being on particular topic.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1670">
<title id=" E12-3002.xml">cross lingual genre classification </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>my experiments show this method to perform equally well or better than full text translation combined with monolingual classification, while requiring fewer resources.
</prevsent>
<prevsent>automated text classification has become standard practice with applications in fields such as information retrieval and natural language processing.
</prevsent>
</prevsection>
<citsent citstr=" P06-2081 ">
the most common basis for text classification is by topic (joachims, 1998; sebastiani, 2002), but other classification criteria have evolved, including sentiment (pang et al 2002), <papid> W02-1011 </papid>authorship (de vel et al 2001; stamatatos et al 2000<papid> J00-4001 </papid>a), and author personality (oberlander and nowson, 2006), <papid> P06-2081 </papid>as well as categories relevant to filter algorithms (e.g., spam or inappropriate contents for minors).genre is another text characteristic, often described as orthogonal to topic.</citsent>
<aftsection>
<nextsent>it has been shown by biber (1988) and others after him, that the genre of text affects its formal properties.
</nextsent>
<nextsent>it is therefore possible to use cues (e.g., lexical, syntactic, structural) from text as features to predict its genre, which can then feed into information retrieval applications (karlgren and cutting,1994; <papid> C94-2174 </papid>kessler et al 1997; <papid> P97-1005 </papid>finn and kushmerick, 2006; freund et al 2006).</nextsent>
<nextsent>this is because users may want documents that serve particular communicative purpose, as well as being on particular topic.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1671">
<title id=" E12-3002.xml">cross lingual genre classification </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the most common basis for text classification is by topic (joachims, 1998; sebastiani, 2002), but other classification criteria have evolved, including sentiment (pang et al 2002), <papid> W02-1011 </papid>authorship (de vel et al 2001; stamatatos et al 2000<papid> J00-4001 </papid>a), and author personality (oberlander and nowson, 2006), <papid> P06-2081 </papid>as well as categories relevant to filter algorithms (e.g., spam or inappropriate contents for minors).genre is another text characteristic, often described as orthogonal to topic.</prevsent>
<prevsent>it has been shown by biber (1988) and others after him, that the genre of text affects its formal properties.</prevsent>
</prevsection>
<citsent citstr=" C94-2174 ">
it is therefore possible to use cues (e.g., lexical, syntactic, structural) from text as features to predict its genre, which can then feed into information retrieval applications (karlgren and cutting,1994; <papid> C94-2174 </papid>kessler et al 1997; <papid> P97-1005 </papid>finn and kushmerick, 2006; freund et al 2006).</citsent>
<aftsection>
<nextsent>this is because users may want documents that serve particular communicative purpose, as well as being on particular topic.
</nextsent>
<nextsent>for example, web search on the topic crocodiles?
</nextsent>
<nextsent>may return an encyclopedia entry, biological fact sheet, news report about attacks in australia, blog post about safari experience, fiction novel set in south africa, or poem about wildlife.
</nextsent>
<nextsent>a user may reject many of these, just because of their genre: blog posts, poems, novels, or news reports may not contain the kind or quality of information she is seeking.having classified indexed texts by genre would allow additional selection criteria to reflect this.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1672">
<title id=" E12-3002.xml">cross lingual genre classification </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the most common basis for text classification is by topic (joachims, 1998; sebastiani, 2002), but other classification criteria have evolved, including sentiment (pang et al 2002), <papid> W02-1011 </papid>authorship (de vel et al 2001; stamatatos et al 2000<papid> J00-4001 </papid>a), and author personality (oberlander and nowson, 2006), <papid> P06-2081 </papid>as well as categories relevant to filter algorithms (e.g., spam or inappropriate contents for minors).genre is another text characteristic, often described as orthogonal to topic.</prevsent>
<prevsent>it has been shown by biber (1988) and others after him, that the genre of text affects its formal properties.</prevsent>
</prevsection>
<citsent citstr=" P97-1005 ">
it is therefore possible to use cues (e.g., lexical, syntactic, structural) from text as features to predict its genre, which can then feed into information retrieval applications (karlgren and cutting,1994; <papid> C94-2174 </papid>kessler et al 1997; <papid> P97-1005 </papid>finn and kushmerick, 2006; freund et al 2006).</citsent>
<aftsection>
<nextsent>this is because users may want documents that serve particular communicative purpose, as well as being on particular topic.
</nextsent>
<nextsent>for example, web search on the topic crocodiles?
</nextsent>
<nextsent>may return an encyclopedia entry, biological fact sheet, news report about attacks in australia, blog post about safari experience, fiction novel set in south africa, or poem about wildlife.
</nextsent>
<nextsent>a user may reject many of these, just because of their genre: blog posts, poems, novels, or news reports may not contain the kind or quality of information she is seeking.having classified indexed texts by genre would allow additional selection criteria to reflect this.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1673">
<title id=" E12-3002.xml">cross lingual genre classification </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a user may reject many of these, just because of their genre: blog posts, poems, novels, or news reports may not contain the kind or quality of information she is seeking.having classified indexed texts by genre would allow additional selection criteria to reflect this.
</prevsent>
<prevsent>genre classification can also benefit language technology indirectly, where differences in thecues that correlate with genre may impact system performance.
</prevsent>
</prevsection>
<citsent citstr=" J11-2004 ">
for example, petrenz and webber (2011) <papid> J11-2004 </papid>found that within the new york times corpus (sandhaus, 2008), the word state shas higher likelihood of being verb in letters (approx.</citsent>
<aftsection>
<nextsent>20%) than in editorials (approx.
</nextsent>
<nextsent>2%).
</nextsent>
<nextsent>part-of-speech (pos) taggers or statistical machine translation (mt) systems could benefit from knowing such genre-based domain variation.
</nextsent>
<nextsent>kessler et al(1997) <papid> P97-1005 </papid>mention that parsing and word-sense disambiguation can also benefit from genre classification.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1677">
<title id=" E12-3002.xml">cross lingual genre classification </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>part-of-speech (pos) taggers or statistical machine translation (mt) systems could benefit from knowing such genre-based domain variation.
</prevsent>
<prevsent>kessler et al(1997) <papid> P97-1005 </papid>mention that parsing and word-sense disambiguation can also benefit from genre classification.</prevsent>
</prevsection>
<citsent citstr=" P09-1076 ">
webber (2009) <papid> P09-1076 </papid>found that different genres have different distribution of discourse relations, and goldstein et al(2007) showed that knowing the genre of text can also improve automated summarization algorithms, asgenre conventions dictate the location and structure of important information within document.</citsent>
<aftsection>
<nextsent>all the above work has been done within asingle language.
</nextsent>
<nextsent>here describe new approach to genre classification that is cross-lingual.
</nextsent>
<nextsent>cross-lingual genre classification (clgc) differs 11 from both poly-lingual and language-independent genre classification.
</nextsent>
<nextsent>clgc entails training genre classification model on set of labeled texts written in source language ls and using this model to predict the genres of texts written in the target language lt 6= ls . in poly-lingual classification, the training set is made up of texts from two or more languages = {ls1 , . . .
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1681">
<title id=" E12-3002.xml">cross lingual genre classification </title>
<section> prior work.  </section>
<citcontext>
<prevsection>
<prevsent>these include counts of function words such aswe?
</prevsent>
<prevsent>or therefore?, selected pos tag frequencies, punctuation cues, and other statistics derived from intuition or text analysis.
</prevsent>
</prevsection>
<citsent citstr=" E99-1019 ">
similarly language specific feature sets were later explored for mono-lingual genre classification experiments in german (wolters and kirsten, 1999) <papid> E99-1019 </papid>and russian (braslavski, 2004).in subsequent research, automatically generated feature sets have become more popular.</citsent>
<aftsection>
<nextsent>most of these tend to be language-independent and might work in mono-lingual genre classification tasks in languages other than english.
</nextsent>
<nextsent>examples are the word based approaches suggested by stamatatos et al(2000<papid> J00-4001 </papid>b) and freund et al(2006), the image features suggested by kim and ross (2008), the pos histogram frequency approach by feldman et al(2009), and the character n-gram approaches proposed by kanaris and stamatatos (2007) and sharoff et al(2010).</nextsent>
<nextsent>all of them were tested exclusively on english texts.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1684">
<title id=" E12-3002.xml">cross lingual genre classification </title>
<section> prior work.  </section>
<citcontext>
<prevsection>
<prevsent>using information gain scor esat every iteration to only retain the most predictive words and thus reduce noise, rigutini et al (2005) achieve considerable improvement overthe baseline accuracy, which is simple translation of the training instances and subsequent 12mono-lingual classification.
</prevsent>
<prevsent>they, too, were classifying texts by topics and used collection of english and italian news group messages.
</prevsent>
</prevsection>
<citsent citstr=" P09-1027 ">
similarly, researchers have used semi-supervised bootstrapping methods like co-training (wan, 2009)<papid> P09-1027 </papid>and other domain adaptation methods like structural component learning (prettenhofer and stein, 2010) to carry out cross-lingual text classification.</citsent>
<aftsection>
<nextsent>all of the approaches described above relyon mt, even if some try to keep translation to aminimum.
</nextsent>
<nextsent>this has several disadvantages however, as applications become dependent on parallel corpora, which may not be available for poorly-resourced languages.
</nextsent>
<nextsent>it also introduces problems due to word ambiguity and morphology, especially where single words are translated out of context.
</nextsent>
<nextsent>a different method is proposed by gliozzo and strapparava (2006), <papid> P06-1070 </papid>who use latent semantic analysis on combined collection of texts written in two languages.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1685">
<title id=" E12-3002.xml">cross lingual genre classification </title>
<section> prior work.  </section>
<citcontext>
<prevsection>
<prevsent>this has several disadvantages however, as applications become dependent on parallel corpora, which may not be available for poorly-resourced languages.
</prevsent>
<prevsent>it also introduces problems due to word ambiguity and morphology, especially where single words are translated out of context.
</prevsent>
</prevsection>
<citsent citstr=" P06-1070 ">
a different method is proposed by gliozzo and strapparava (2006), <papid> P06-1070 </papid>who use latent semantic analysis on combined collection of texts written in two languages.</citsent>
<aftsection>
<nextsent>the rationale is that named entities such as microsoft?
</nextsent>
<nextsent>or hiv?
</nextsent>
<nextsent>are identical in different languages withthe same writing system.
</nextsent>
<nextsent>using term correlation, the algorithm can identify semantically similar words in both languages.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1695">
<title id=" E12-3002.xml">cross lingual genre classification </title>
<section> approach.  </section>
<citcontext>
<prevsection>
<prevsent>i only use them to generate baseline results.
</prevsent>
<prevsent>the same restriction is applied to other types of prior knowledge, and do not assume supervised pos taggers, syntactic parsers, or other tools are available.
</prevsent>
</prevsection>
<citsent citstr=" E03-1009 ">
in future work however, may explore unsupervised methods, such as the pos induction methods of clark (2003), <papid> E03-1009 </papid>goldwater and griffiths (2007), <papid> P07-1094 </papid>or berg-kirkpatrick et al(2010), as they do not represent external knowledge.</citsent>
<aftsection>
<nextsent>there are few assumptions that must be made in order to carry out any meaningful experiments.
</nextsent>
<nextsent>first, some way to detect sentence and paragraph boundaries is expected.
</nextsent>
<nextsent>this can be simple rule based algorithm, or unsupervised methods, such as the punkt boundary detection system by kiss and strunk (2006).<papid> J06-4003 </papid></nextsent>
<nextsent>also, punctuation symbols and numerals are assumed to be identifiable assuch, although their exact semantic function is un known.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1696">
<title id=" E12-3002.xml">cross lingual genre classification </title>
<section> approach.  </section>
<citcontext>
<prevsection>
<prevsent>i only use them to generate baseline results.
</prevsent>
<prevsent>the same restriction is applied to other types of prior knowledge, and do not assume supervised pos taggers, syntactic parsers, or other tools are available.
</prevsent>
</prevsection>
<citsent citstr=" P07-1094 ">
in future work however, may explore unsupervised methods, such as the pos induction methods of clark (2003), <papid> E03-1009 </papid>goldwater and griffiths (2007), <papid> P07-1094 </papid>or berg-kirkpatrick et al(2010), as they do not represent external knowledge.</citsent>
<aftsection>
<nextsent>there are few assumptions that must be made in order to carry out any meaningful experiments.
</nextsent>
<nextsent>first, some way to detect sentence and paragraph boundaries is expected.
</nextsent>
<nextsent>this can be simple rule based algorithm, or unsupervised methods, such as the punkt boundary detection system by kiss and strunk (2006).<papid> J06-4003 </papid></nextsent>
<nextsent>also, punctuation symbols and numerals are assumed to be identifiable assuch, although their exact semantic function is un known.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1697">
<title id=" E12-3002.xml">cross lingual genre classification </title>
<section> approach.  </section>
<citcontext>
<prevsection>
<prevsent>there are few assumptions that must be made in order to carry out any meaningful experiments.
</prevsent>
<prevsent>first, some way to detect sentence and paragraph boundaries is expected.
</prevsent>
</prevsection>
<citsent citstr=" J06-4003 ">
this can be simple rule based algorithm, or unsupervised methods, such as the punkt boundary detection system by kiss and strunk (2006).<papid> J06-4003 </papid></citsent>
<aftsection>
<nextsent>also, punctuation symbols and numerals are assumed to be identifiable assuch, although their exact semantic function is unknown.
</nextsent>
<nextsent>for example, question mark will be identified as punctuation symbol, but its function (question cue; end of sentence) will not.
</nextsent>
<nextsent>lastly, sufficiently large, unlabeled set of texts in the target language is required.
</nextsent>
<nextsent>3.2 stable features.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1699">
<title id=" E12-1013.xml">computing lattice bleu oracle scores for machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in several softwares, an approximation of this search space can be out putted, either as n-best list containing the top hypotheses found by the decoder, oras phrase or word graph (lattice) which compactly encodes those hypotheses that have survived search space pruning.
</prevsent>
<prevsent>lattices usually contain much more hypotheses than n-best lists and better approximate the search space.
</prevsent>
</prevsection>
<citsent citstr=" W08-0305 ">
exploring the pbsmt search space is one of the few means to perform diagnostic analysis and to better understand the behavior of the system (turchi et al 2008; <papid> W08-0305 </papid>auli et al 2009).<papid> W09-0437 </papid></citsent>
<aftsection>
<nextsent>useful diagnostics are, for instance, provided by looking at the best (oracle) hypotheses contained in the search space, i.e, those hypotheses that have the highest quality score with respect to one or several references.
</nextsent>
<nextsent>such oracle hypotheses canbe used for failure analysis and to better understand the bottlenecks of existing translation systems (wisniewski et al 2010).<papid> D10-1091 </papid></nextsent>
<nextsent>indeed, the inability to faithfully reproduce reference translations can have many causes, such as scant iness of the translation table, insufficient expressiveness of reordering models, inadequate scoring function, non-literal references, over-pruned lattices,etc. oracle decoding has several other applications: for instance, in (liang et al 2006; <papid> P06-1096 </papid>chiang et al 2008) <papid> D08-1024 </papid>it is used as work-around to the problem of non-reachability of the reference in discriminative training of mt systems.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1700">
<title id=" E12-1013.xml">computing lattice bleu oracle scores for machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in several softwares, an approximation of this search space can be out putted, either as n-best list containing the top hypotheses found by the decoder, oras phrase or word graph (lattice) which compactly encodes those hypotheses that have survived search space pruning.
</prevsent>
<prevsent>lattices usually contain much more hypotheses than n-best lists and better approximate the search space.
</prevsent>
</prevsection>
<citsent citstr=" W09-0437 ">
exploring the pbsmt search space is one of the few means to perform diagnostic analysis and to better understand the behavior of the system (turchi et al 2008; <papid> W08-0305 </papid>auli et al 2009).<papid> W09-0437 </papid></citsent>
<aftsection>
<nextsent>useful diagnostics are, for instance, provided by looking at the best (oracle) hypotheses contained in the search space, i.e, those hypotheses that have the highest quality score with respect to one or several references.
</nextsent>
<nextsent>such oracle hypotheses canbe used for failure analysis and to better understand the bottlenecks of existing translation systems (wisniewski et al 2010).<papid> D10-1091 </papid></nextsent>
<nextsent>indeed, the inability to faithfully reproduce reference translations can have many causes, such as scant iness of the translation table, insufficient expressiveness of reordering models, inadequate scoring function, non-literal references, over-pruned lattices,etc. oracle decoding has several other applications: for instance, in (liang et al 2006; <papid> P06-1096 </papid>chiang et al 2008) <papid> D08-1024 </papid>it is used as work-around to the problem of non-reachability of the reference in discriminative training of mt systems.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1701">
<title id=" E12-1013.xml">computing lattice bleu oracle scores for machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>exploring the pbsmt search space is one of the few means to perform diagnostic analysis and to better understand the behavior of the system (turchi et al 2008; <papid> W08-0305 </papid>auli et al 2009).<papid> W09-0437 </papid></prevsent>
<prevsent>useful diagnostics are, for instance, provided by looking at the best (oracle) hypotheses contained in the search space, i.e, those hypotheses that have the highest quality score with respect to one or several references.</prevsent>
</prevsection>
<citsent citstr=" D10-1091 ">
such oracle hypotheses canbe used for failure analysis and to better understand the bottlenecks of existing translation systems (wisniewski et al 2010).<papid> D10-1091 </papid></citsent>
<aftsection>
<nextsent>indeed, the inability to faithfully reproduce reference translations can have many causes, such as scant iness of the translation table, insufficient expressiveness of reordering models, inadequate scoring function, non-literal references, over-pruned lattices,etc. oracle decoding has several other applications: for instance, in (liang et al 2006; <papid> P06-1096 </papid>chiang et al 2008) <papid> D08-1024 </papid>it is used as work-around to the problem of non-reachability of the reference in discriminative training of mt systems.</nextsent>
<nextsent>lattice reranking (li and khudanpur, 2009), <papid> N09-2003 </papid>promising way to improve mt systems, also relies on oracle decoding to build the training data for reranking algorithm.for sentence level metrics, finding oracle hypotheses in n-best lists is simple issue; how ever, solving this problem on lattices proves much more challenging, due to the number of embedded hypotheses, which prevents the use of brute force approaches.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1703">
<title id=" E12-1013.xml">computing lattice bleu oracle scores for machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>useful diagnostics are, for instance, provided by looking at the best (oracle) hypotheses contained in the search space, i.e, those hypotheses that have the highest quality score with respect to one or several references.
</prevsent>
<prevsent>such oracle hypotheses canbe used for failure analysis and to better understand the bottlenecks of existing translation systems (wisniewski et al 2010).<papid> D10-1091 </papid></prevsent>
</prevsection>
<citsent citstr=" P06-1096 ">
indeed, the inability to faithfully reproduce reference translations can have many causes, such as scant iness of the translation table, insufficient expressiveness of reordering models, inadequate scoring function, non-literal references, over-pruned lattices,etc. oracle decoding has several other applications: for instance, in (liang et al 2006; <papid> P06-1096 </papid>chiang et al 2008) <papid> D08-1024 </papid>it is used as work-around to the problem of non-reachability of the reference in discriminative training of mt systems.</citsent>
<aftsection>
<nextsent>lattice reranking (li and khudanpur, 2009), <papid> N09-2003 </papid>promising way to improve mt systems, also relies on oracle decoding to build the training data for reranking algorithm.for sentence level metrics, finding oracle hypotheses in n-best lists is simple issue; how ever, solving this problem on lattices proves much more challenging, due to the number of embedded hypotheses, which prevents the use of brute force approaches.</nextsent>
<nextsent>when using bleu, or rathersentence-level approximations thereof, the problem is in fact known to be np-hard (leusch et al., 2008).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1704">
<title id=" E12-1013.xml">computing lattice bleu oracle scores for machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>useful diagnostics are, for instance, provided by looking at the best (oracle) hypotheses contained in the search space, i.e, those hypotheses that have the highest quality score with respect to one or several references.
</prevsent>
<prevsent>such oracle hypotheses canbe used for failure analysis and to better understand the bottlenecks of existing translation systems (wisniewski et al 2010).<papid> D10-1091 </papid></prevsent>
</prevsection>
<citsent citstr=" D08-1024 ">
indeed, the inability to faithfully reproduce reference translations can have many causes, such as scant iness of the translation table, insufficient expressiveness of reordering models, inadequate scoring function, non-literal references, over-pruned lattices,etc. oracle decoding has several other applications: for instance, in (liang et al 2006; <papid> P06-1096 </papid>chiang et al 2008) <papid> D08-1024 </papid>it is used as work-around to the problem of non-reachability of the reference in discriminative training of mt systems.</citsent>
<aftsection>
<nextsent>lattice reranking (li and khudanpur, 2009), <papid> N09-2003 </papid>promising way to improve mt systems, also relies on oracle decoding to build the training data for reranking algorithm.for sentence level metrics, finding oracle hypotheses in n-best lists is simple issue; how ever, solving this problem on lattices proves much more challenging, due to the number of embedded hypotheses, which prevents the use of brute force approaches.</nextsent>
<nextsent>when using bleu, or rathersentence-level approximations thereof, the problem is in fact known to be np-hard (leusch et al., 2008).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1705">
<title id=" E12-1013.xml">computing lattice bleu oracle scores for machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>such oracle hypotheses canbe used for failure analysis and to better understand the bottlenecks of existing translation systems (wisniewski et al 2010).<papid> D10-1091 </papid></prevsent>
<prevsent>indeed, the inability to faithfully reproduce reference translations can have many causes, such as scant iness of the translation table, insufficient expressiveness of reordering models, inadequate scoring function, non-literal references, over-pruned lattices,etc. oracle decoding has several other applications: for instance, in (liang et al 2006; <papid> P06-1096 </papid>chiang et al 2008) <papid> D08-1024 </papid>it is used as work-around to the problem of non-reachability of the reference in discriminative training of mt systems.</prevsent>
</prevsection>
<citsent citstr=" N09-2003 ">
lattice reranking (li and khudanpur, 2009), <papid> N09-2003 </papid>promising way to improve mt systems, also relies on oracle decoding to build the training data for reranking algorithm.for sentence level metrics, finding oracle hypotheses in n-best lists is simple issue; how ever, solving this problem on lattices proves much more challenging, due to the number of embedded hypotheses, which prevents the use of brute force approaches.</citsent>
<aftsection>
<nextsent>when using bleu, or rathersentence-level approximations thereof, the problem is in fact known to be np-hard (leusch et al., 2008).
</nextsent>
<nextsent>this complexity stems from the fact that the contribution of given edge to the total modified n-gram precision can not be computed without looking at all other edges on the path.
</nextsent>
<nextsent>similar (or worse) complexity result are expected 120 for other metrics such as meteor (banerjee and lavie, 2005) or ter (snover et al 2006).
</nextsent>
<nextsent>the exact computation of oracles under corpus level metrics, such as bleu, poses supplementary combinatorial problems that will not be addressed in this work.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1707">
<title id=" E12-1013.xml">computing lattice bleu oracle scores for machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the exact computation of oracles under corpus level metrics, such as bleu, poses supplementary combinatorial problems that will not be addressed in this work.
</prevsent>
<prevsent>in this paper, we present two original methods for finding approximate oracle hypotheses on lattices.
</prevsent>
</prevsection>
<citsent citstr=" D08-1065 ">
the first one is based on linear approximation of the corpus bleu, that was originally designed for efficient minimum bayesian risk decoding on lattices (tromble et al 2008).<papid> D08-1065 </papid></citsent>
<aftsection>
<nextsent>the second one, based on integer linear programming, is an extension to lattices of recent work on failure analysis for phrase-based decoders (wisniewski et al 2010).<papid> D10-1091 </papid></nextsent>
<nextsent>in this framework, we study two decoding strategies: one based on generic ilp solver, and one, based on lagrangian relaxation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1711">
<title id=" E12-1013.xml">computing lattice bleu oracle scores for machine translation </title>
<section> preliminaries.  </section>
<citcontext>
<prevsection>
<prevsent>the decoder therefore aims at finding the path pi?
</prevsent>
<prevsent>that generates the hypothesis that best matches rf . for this task, only the output labels ei will matter, the other informations can be left aside.4 oracle decoding assumes the definition of measure of the similarity between reference and hypothesis.
</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
in this paper we will consider sentence-level approximations of the popular bleu score (papineni et al 2002).<papid> P02-1040 </papid></citsent>
<aftsection>
<nextsent>bleu is formally defined for two parallel corpora, = {ej}jj=1 and = {rj} j=1, each containing sentences as: n-bleu(e ,r) = bp ?
</nextsent>
<nextsent>( n?
</nextsent>
<nextsent>m=1 pm )1/n , (1) where bp = min(1, e1c1(r)/c1(e)) is the brevity penalty and pm = cm(e ,r)/cm(e) are clipped or modified m-gram precisions: cm(e) is the total number of wordm-grams in ; cm(e ,r)accumulates over sentences the number of grams in ej that also belong to rj . these counts are clipped, meaning that m-gram that appears times in and times in r, with   l, is only counted times.
</nextsent>
<nextsent>as it is well known, bleu performs compromise between precision, which is directly appears in equation (1), and recall, which is indirectly taken into account via the brevity penalty.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1715">
<title id=" E12-1013.xml">computing lattice bleu oracle scores for machine translation </title>
<section> existing algorithms.  </section>
<citcontext>
<prevsection>
<prevsent>onemay expect the most probable path to select frequent n-gram from the reference, thus augmenting n-bleu.
</prevsent>
<prevsent>3.2 partial bleu oracle (pb).
</prevsent>
</prevsection>
<citsent citstr=" W07-0414 ">
another approach is put forward in (dreyer et al., 2007) <papid> W07-0414 </papid>and used in (li and khudanpur, 2009): <papid> N09-2003 </papid>oracle translations are shortest paths in latticel, where the weight of each path pi is the sentence level log bleu(pi) score of the corresponding complete or partial hypothesis: log bleu(pi) = 1 4 ? m=1...4 log pm.</citsent>
<aftsection>
<nextsent>(4)here, the brevity penalty is ignored and gram precis ions are offset to avoid null counts: pm = (cm(epi, r) + 0.1)/(cm(epi) + 0.1).
</nextsent>
<nextsent>this approach has been reimplemented using the fst formalism by defining suitable semir ing.
</nextsent>
<nextsent>let each weight of the semi ring keep set of tuples accumulated up to the current state ofthe lattice.
</nextsent>
<nextsent>each tuple contains three words of recent history, partial hypothesis as well as current values of the length of the partial hypothesis, gram counts (4 numbers) and the sentence-level log bleu score defined by equation (4).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1720">
<title id=" E12-1013.xml">computing lattice bleu oracle scores for machine translation </title>
<section> linear bleu oracle (lb).  </section>
<citcontext>
<prevsection>
<prevsent>the (pb`,pb)-semiring, in which the equal length requirement also implies equal brevity penalties, is more conservative in recom bining hypotheses and should achieve final bleu that is least as good as that obtained with the (pb,pb)-semiring5.
</prevsent>
<prevsent>in this section, we propose new oracle based onthe linear approximation of the corpus bleu introduced in (tromble et al 2008).<papid> D08-1065 </papid></prevsent>
</prevsection>
<citsent citstr=" P10-2006 ">
while this approximation was earlier used for minimum bayes risk decoding in lattices (tromble et al 2008; <papid> D08-1065 </papid>blackwood et al 2010), <papid> P10-2006 </papid>we show here how it can also be used to approximately compute an oracle translation.given five real parameters 0...4 and word vocabulary ?, tromble et al(2008) <papid> D08-1065 </papid>showed that onecan approximate the corpus-bleu with its first order (linear) taylor expansion: lin bleu(pi) = 0 |epi|+ 4?</citsent>
<aftsection>
<nextsent>n=1 ? un cu(epi)u(r), (5) where cu(e) is the number of times the n-gram appears in e, and u(r) is an indicator variable testing the presence of in r.to exploit this approximation for oracle decoding, we construct four weighted fsts containing (final) state for each possible (n ? 1) 5see, however, experiments in section 6.
</nextsent>
<nextsent>gram, and all weighted transitions of the kind (n11 , : ? 1 /n ? n1 (r), ? 2 ), where arein ?, input word sequence n11 and output sequence n2 , are, respectively, the maximal prefix and suffix of an n-gram n1 .in supplement, we add auxiliary states corresponding to m-grams (m   ? 1), whose functional purpose is to help reach one of the main (n ? 1)-gram states.
</nextsent>
<nextsent>there are |?| n11 |?|1 ,   1, such supplementary states and their transitions are (k1 , k+1 : ? k+1 1 /0, ? k+1 1 ), = 1 . . .
</nextsent>
<nextsent>n2.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1723">
<title id=" E12-1013.xml">computing lattice bleu oracle scores for machine translation </title>
<section> oracles with n-gram clipping.  </section>
<citcontext>
<prevsection>
<prevsent>5.3 oracle decoding through lagrangian.
</prevsent>
<prevsent>relaxation (rlx) in this section, we introduce another method tosolve problem (7) without relying on an external ilp solver.
</prevsent>
</prevsection>
<citsent citstr=" D10-1001 ">
following (rush et al 2010; <papid> D10-1001 </papid>chang and collins, 2011), <papid> D11-1003 </papid>we propose an original method for oracle decoding based on lagrangianrelaxation.</citsent>
<aftsection>
<nextsent>this method relies on the idea of relaxing the clipping constraints: starting from an unconstrained problem, the counts clipping is enforced by incrementally strengthening the weight of paths satisfying the constraints.
</nextsent>
<nextsent>the oracle decoding problem with clipping constraints amounts to solving: arg min ???
</nextsent>
<nextsent>i=1 ? (8) s.t. ? ???(w) ? ?
</nextsent>
<nextsent>cw(r), ? where, by abusing the notations, also denotes the set of words in the reference.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1724">
<title id=" E12-1013.xml">computing lattice bleu oracle scores for machine translation </title>
<section> oracles with n-gram clipping.  </section>
<citcontext>
<prevsection>
<prevsent>5.3 oracle decoding through lagrangian.
</prevsent>
<prevsent>relaxation (rlx) in this section, we introduce another method tosolve problem (7) without relying on an external ilp solver.
</prevsent>
</prevsection>
<citsent citstr=" D11-1003 ">
following (rush et al 2010; <papid> D10-1001 </papid>chang and collins, 2011), <papid> D11-1003 </papid>we propose an original method for oracle decoding based on lagrangianrelaxation.</citsent>
<aftsection>
<nextsent>this method relies on the idea of relaxing the clipping constraints: starting from an unconstrained problem, the counts clipping is enforced by incrementally strengthening the weight of paths satisfying the constraints.
</nextsent>
<nextsent>the oracle decoding problem with clipping constraints amounts to solving: arg min ???
</nextsent>
<nextsent>i=1 ? (8) s.t. ? ???(w) ? ?
</nextsent>
<nextsent>cw(r), ? where, by abusing the notations, also denotes the set of words in the reference.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1726">
<title id=" E09-3008.xml">a comparison of merging strategies for translation of german compounds </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>i will explore and compare three markup schemes for compound parts, and eight merging algorithms that use different combinations of knowledge sources.
</prevsent>
<prevsent>splitting german compounds into their parts priorto translation has been suggested by many researchers.
</prevsent>
</prevsection>
<citsent citstr=" E03-1076 ">
koehn and knight (2003) <papid> E03-1076 </papid>presented an empirical splitting algorithm that is used to im prove translation from german to english.</citsent>
<aftsection>
<nextsent>they split all words in all possible places, and considered splitting option valid if all the parts are existing words in monolingual corpus.
</nextsent>
<nextsent>they allowed the addition of -s or -es at all splitting points.
</nextsent>
<nextsent>if there were several valid splitting options they chose one based on the number of splits, the geometric mean of part frequencies or based on alignment data.
</nextsent>
<nextsent>stymne (2008) extended this algorithm in number of ways, for instance by allowing more compound forms.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1727">
<title id=" E09-3008.xml">a comparison of merging strategies for translation of german compounds </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>this method can merge unseen compounds, and the tendency to merge too much is reduced by the restriction that pos-tags need to match.
</prevsent>
<prevsent>in addition coordinated compounds were handled by the algorithm.
</prevsent>
</prevsection>
<citsent citstr=" W08-0318 ">
this strategy resulted in improved scores on automatic metrics, which were confirmed by an error analy sis.koehn et al (2008) <papid> W08-0318 </papid>discussed treatment of hy phened compounds in translation into german by splitting at hyphens and treat the hyphen as separate token, marked by symbol.</citsent>
<aftsection>
<nextsent>the impact on translation results was small.
</nextsent>
<nextsent>there are also other ways of using compound processing to improve smt into german.
</nextsent>
<nextsent>popovic?
</nextsent>
<nextsent>et al (2006) suggested using compound splitting to improve alignment, or to merge english compounds prior to training.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1728">
<title id=" E09-3008.xml">a comparison of merging strategies for translation of german compounds </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>they marked split parts with symbol, and merged every word in the output which had this symbol with the next word.
</prevsent>
<prevsent>if morphs were misplaced in the translation output, they were merged anyway, possibly creating non-existent words.
</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
this system was worse than the baseline on bleu (papineni et al, 2002), <papid> P02-1040 </papid>but an error analysis showed some improvements.el-kahlout and oflazer (2006), discuss merging of morphs in turkish.</citsent>
<aftsection>
<nextsent>they also markmorphs with symbol, and in addition normalize affixes to standard form.
</nextsent>
<nextsent>in the merging 62 phase, surface forms were generated following morphographemic rules.
</nextsent>
<nextsent>they found that morphswere often translated out of order, and that merging based purely on symbols gave bad results.
</nextsent>
<nextsent>to reduce this risk, they constrained splitting to allow only morphologically correct splits, and by grouping some morphemes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1729">
<title id=" E09-3008.xml">a comparison of merging strategies for translation of german compounds </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>using list of compounds instead of words reduces the errors slightly.
</prevsent>
<prevsent>the translation system used is factored phrase based translation system.
</prevsent>
</prevsection>
<citsent citstr=" D07-1091 ">
in factored translation model other factors than surface form can be used, such as lemma or part-of-speech (koehnand hoang, 2007).<papid> D07-1091 </papid></citsent>
<aftsection>
<nextsent>in the current system part-of speech is used only as an output factor in the target language.
</nextsent>
<nextsent>besides the standard language model sequence model on part-of-speech is used, which can be expected to lead to better word order in the translation output.
</nextsent>
<nextsent>there are no input factors, so no tagging has to be performed prior to translation,only the training corpus needs to be tagged.
</nextsent>
<nextsent>in addition, the computational overhead is small.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1730">
<title id=" E09-3008.xml">a comparison of merging strategies for translation of german compounds </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>one possible benefit gained by using part-of-speech as an output factor is that ordering, both in general,and of compound parts, can be improved.
</prevsent>
<prevsent>this hypothesis is tested by trying two system setups, with and without the part-of-speech sequence model.in addition part-of-speech is used for post process 64 wlist wlist+head-pos clist symbol symbol+head-pos symbol+wlist pos-match pos-match+coord 2393 1656 2257 118 205 330 118 55 table 2: number of merging errors on the split reference corpus tokens types english baseline 15158429 63692 german baseline 14356051 184215 marked 15674728 93746 unmarked 15674728 81806 sep marked 17007929 81808 table 3: type and token counts for the 701157 sentence training corpus ing, both for upper casing german nouns and as knowledge source for compound merging.
</prevsent>
</prevsection>
<citsent citstr=" P07-2045 ">
the tools used are the moses toolkit (koehn et al., 2007) <papid> P07-2045 </papid>for decoding and training, giza++ for word alignment (och and ney, 2003), <papid> J03-1002 </papid>and srilm (stolcke, 2002) for language models.</citsent>
<aftsection>
<nextsent>a 5-gram model is used for surface form, and 7-gram model is used for part-of-speech.
</nextsent>
<nextsent>to tune feature weights minimum error rate training is used (och,2003), <papid> P03-1021 </papid>optimized against the neva metric (fors bom, 2003).</nextsent>
<nextsent>compound splitting is performed on the training corpus, prior to training.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1731">
<title id=" E09-3008.xml">a comparison of merging strategies for translation of german compounds </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>one possible benefit gained by using part-of-speech as an output factor is that ordering, both in general,and of compound parts, can be improved.
</prevsent>
<prevsent>this hypothesis is tested by trying two system setups, with and without the part-of-speech sequence model.in addition part-of-speech is used for post process 64 wlist wlist+head-pos clist symbol symbol+head-pos symbol+wlist pos-match pos-match+coord 2393 1656 2257 118 205 330 118 55 table 2: number of merging errors on the split reference corpus tokens types english baseline 15158429 63692 german baseline 14356051 184215 marked 15674728 93746 unmarked 15674728 81806 sep marked 17007929 81808 table 3: type and token counts for the 701157 sentence training corpus ing, both for upper casing german nouns and as knowledge source for compound merging.
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
the tools used are the moses toolkit (koehn et al., 2007) <papid> P07-2045 </papid>for decoding and training, giza++ for word alignment (och and ney, 2003), <papid> J03-1002 </papid>and srilm (stolcke, 2002) for language models.</citsent>
<aftsection>
<nextsent>a 5-gram model is used for surface form, and 7-gram model is used for part-of-speech.
</nextsent>
<nextsent>to tune feature weights minimum error rate training is used (och,2003), <papid> P03-1021 </papid>optimized against the neva metric (fors bom, 2003).</nextsent>
<nextsent>compound splitting is performed on the training corpus, prior to training.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1732">
<title id=" E09-3008.xml">a comparison of merging strategies for translation of german compounds </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>the tools used are the moses toolkit (koehn et al., 2007) <papid> P07-2045 </papid>for decoding and training, giza++ for word alignment (och and ney, 2003), <papid> J03-1002 </papid>and srilm (stolcke, 2002) for language models.</prevsent>
<prevsent>a 5-gram model is used for surface form, and 7-gram model is used for part-of-speech.</prevsent>
</prevsection>
<citsent citstr=" P03-1021 ">
to tune feature weights minimum error rate training is used (och,2003), <papid> P03-1021 </papid>optimized against the neva metric (fors bom, 2003).</citsent>
<aftsection>
<nextsent>compound splitting is performed on the training corpus, prior to training.
</nextsent>
<nextsent>merging is performed after translation, both for test, and incorporated into the tuning step.
</nextsent>
<nextsent>4.1 corpus.
</nextsent>
<nextsent>the system is trained and tested on the europarlcorpus (koehn, 2005).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1734">
<title id=" E09-3008.xml">a comparison of merging strategies for translation of german compounds </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>these metrics have disadvantages, for instance because the same weight is given to all tokens, both to complex compounds, and to function words such as und (and).
</prevsent>
<prevsent>bleu has been criticized, see e.g.
</prevsent>
</prevsection>
<citsent citstr=" D08-1064 ">
(callison-burch et al, 2006; chiang et al, 2008).<papid> D08-1064 </papid></citsent>
<aftsection>
<nextsent>table 4 and 5 shows the translation results using the different merging algorithms.
</nextsent>
<nextsent>for the systems with pos sequence models the baseline performs slightly better on bleu, than the best systems withmerging.
</nextsent>
<nextsent>without the pos sequence model, how ever, merging often leads to improvements, by upto 0.48 bleu points.
</nextsent>
<nextsent>for all systems it is advantageous to use the pos sequence model.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1735">
<title id=" E09-3008.xml">a comparison of merging strategies for translation of german compounds </title>
<section> conclusion and future work.  </section>
<citcontext>
<prevsection>
<prevsent>the overall best results were achieved by using matching for part-of-speech.
</prevsent>
<prevsent>one factor that affects merging, which was not explored in this work, is the quality of splitting.if splitting produces less erroneously split compounds than the current method, it is possible that merging also can produce better results, even though it was not clear from the error analysis thatbad splits were problem.
</prevsent>
</prevsection>
<citsent citstr=" P08-2064 ">
a number of more accurate splitting strategies have been suggested for different tasks, see e.g. alfonseca et al (2008),<papid> P08-2064 </papid>that could be explored in combination with merging for machine translation.</citsent>
<aftsection>
<nextsent>i have compared the performance of different merging strategies in one language, german.
</nextsent>
<nextsent>itwould be interesting to investigate these methods for other compounding languages as well.
</nextsent>
<nextsent>ialso want to explore translation between two compounding languages, where splitting and merging would be performed on both languages, not only on one language as in this study.
</nextsent>
<nextsent>68
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1736">
<title id=" E12-3003.xml">a comparative study of reinforcement learning techniques on dialogue management </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(2010) propose travel planning ads, that is able to learn dialogue policies using rl, building on top of existing handcrafted policies.
</prevsent>
<prevsent>this enables the designers of the system to provide prior knowledge and the system can then learn the details.
</prevsent>
</prevsection>
<citsent citstr=" W10-2706 ">
konstantopoulos (2010) <papid> W10-2706 </papid>proposes an affective ads which serves as museum guide.</citsent>
<aftsection>
<nextsent>it isable to adapt to each users personality by assessing his / her emotional state and current mood and also adapt its output to the users expertise level.the system itself has an emotional state that is affected by the user and affects its output.
</nextsent>
<nextsent>an example ads architecture is depicted in figure 1, where we can see several components trying to understand the users utterance and several others trying to express the systems response.
</nextsent>
<nextsent>the system first attempts to convert spoken input to text using the automatic speech recognition (asr) component and then tries toinfer the meaning using the natural language understanding (nlu) component.
</nextsent>
<nextsent>at the core liesthe dialogue manager (dm), component responsible for understanding what the users utterance means and deciding which action to take that will lead to achieving his / her goals.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1737">
<title id=" E12-3003.xml">a comparative study of reinforcement learning techniques on dialogue management </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>or historical data before making decision.
</prevsent>
<prevsent>after the system has decided what to say, it uses the referring expression generation (reg) component to create appropriate referring expressions,the natural language generation (nlg) component to create the textual form of the output andlast, the text to speech (tts) component to convert the text to spoken output.trying to make ads as human-like as possible researchers have focused on techniques that achieve adaptation, i.e. adjust to the current users personality, behaviour, mood, needs and to the environment in general.
</prevsent>
</prevsection>
<citsent citstr=" E09-1078 ">
examples include adaptive or trainable nlg (rieser and lemon, 2009), <papid> E09-1078 </papid>where the authors formulate their problem as statistical planning problem and use rl to finda policy according to which the system will decide how to present information.</citsent>
<aftsection>
<nextsent>another example is adaptive reg (janarthanam and lemon, 2009), <papid> W09-3916 </papid>where the authors again use rl to choose one of three strategies (jargon, tutorial, descrip tive) according to the users expertise level.</nextsent>
<nextsent>an example of adaptive tts is the work of boidin et al(2009), where the authors propose model that sorts paraphrases with respect to predictions of which sounds more natural.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1738">
<title id=" E12-3003.xml">a comparative study of reinforcement learning techniques on dialogue management </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>after the system has decided what to say, it uses the referring expression generation (reg) component to create appropriate referring expressions,the natural language generation (nlg) component to create the textual form of the output andlast, the text to speech (tts) component to convert the text to spoken output.trying to make ads as human-like as possible researchers have focused on techniques that achieve adaptation, i.e. adjust to the current users personality, behaviour, mood, needs and to the environment in general.
</prevsent>
<prevsent>examples include adaptive or trainable nlg (rieser and lemon, 2009), <papid> E09-1078 </papid>where the authors formulate their problem as statistical planning problem and use rl to finda policy according to which the system will decide how to present information.</prevsent>
</prevsection>
<citsent citstr=" W09-3916 ">
another example is adaptive reg (janarthanam and lemon, 2009), <papid> W09-3916 </papid>where the authors again use rl to choose one of three strategies (jargon, tutorial, descrip tive) according to the users expertise level.</citsent>
<aftsection>
<nextsent>an example of adaptive tts is the work of boidin et al(2009), where the authors propose model that sorts paraphrases with respect to predictions of which sounds more natural.
</nextsent>
<nextsent>jurccek et al (2010) propose rl algorithm to optimize ads parameters in general.
</nextsent>
<nextsent>last, many researcher shave used rl to achieve adaptive dialogue management (pietquin and hastie, 2011; gasic?
</nextsent>
<nextsent>et al 2010; cuayahuitl et al 2010).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1739">
<title id=" E09-1045.xml">an empirical study on class based word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>it seems that many wordsensedistinctions are too subtle to be captured by automatic systems with the current small volumes ofwordsense annotated examples.
</prevsent>
<prevsent>possibly, building class-based classifiers would allow to avoid the data sparseness problem of the word-based approach.
</prevsent>
</prevsection>
<citsent citstr=" W04-0811 ">
recently, using wn as sense repository, the organizers of the english all-words taskat senseval-3 reported an inter-annotation agreement of 72.5% (snyder and palmer, 2004).<papid> W04-0811 </papid></citsent>
<aftsection>
<nextsent>interestingly, this result is difficult to outperform by state-of-the-art sense-based wsd systems.thus, some research has been focused on deriving different word-sense groupings to overcome the fine grained distinctions of wn (hearst and schutze, 1993), (peters et al , 1998), (mihalceaand moldovan, 2001), (agirre and lopez dela calle, 2003), (navigli, 2006) <papid> P06-1014 </papid>and (snow et al , 2007).<papid> D07-1107 </papid></nextsent>
<nextsent>that is, they provide methods for grouping senses of the same word, thus producing coarser word sense groupings for better disambiguation.wikipedia3 has been also recently used to overcome some problems of automatic learning methods: excessively fine grained definition of meanings, lack of annotated data and strong domain dependence of existing annotated corpora.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1740">
<title id=" E09-1045.xml">an empirical study on class based word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>possibly, building class-based classifiers would allow to avoid the data sparseness problem of the word-based approach.
</prevsent>
<prevsent>recently, using wn as sense repository, the organizers of the english all-words taskat senseval-3 reported an inter-annotation agreement of 72.5% (snyder and palmer, 2004).<papid> W04-0811 </papid></prevsent>
</prevsection>
<citsent citstr=" P06-1014 ">
interestingly, this result is difficult to outperform by state-of-the-art sense-based wsd systems.thus, some research has been focused on deriving different word-sense groupings to overcome the fine grained distinctions of wn (hearst and schutze, 1993), (peters et al , 1998), (mihalceaand moldovan, 2001), (agirre and lopez dela calle, 2003), (navigli, 2006) <papid> P06-1014 </papid>and (snow et al , 2007).<papid> D07-1107 </papid></citsent>
<aftsection>
<nextsent>that is, they provide methods for grouping senses of the same word, thus producing coarser word sense groupings for better disambiguation.wikipedia3 has been also recently used to overcome some problems of automatic learning methods: excessively fine grained definition of meanings, lack of annotated data and strong domain dependence of existing annotated corpora.
</nextsent>
<nextsent>in this way, wikipedia provides new very large source of annotated data, constantly expanded (mihalcea, 2007).<papid> N07-1025 </papid></nextsent>
<nextsent>1http://www.senseval.org 2http://wordnet.princeton.edu 3http://www.wikipedia.org 389 in contrast, some research have been focused onusing predefined sets of sense-groupings for learning class-based classifiers for wsd (segond et al , 1997), (<papid> W97-0811 </papid>ciaramita and johnson, 2003), (<papid> W03-1022 </papid>villarejo et al , 2005), (curran, 2005) <papid> P05-1004 </papid>and (ciaramita and altun, 2006).<papid> W06-1670 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1741">
<title id=" E09-1045.xml">an empirical study on class based word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>possibly, building class-based classifiers would allow to avoid the data sparseness problem of the word-based approach.
</prevsent>
<prevsent>recently, using wn as sense repository, the organizers of the english all-words taskat senseval-3 reported an inter-annotation agreement of 72.5% (snyder and palmer, 2004).<papid> W04-0811 </papid></prevsent>
</prevsection>
<citsent citstr=" D07-1107 ">
interestingly, this result is difficult to outperform by state-of-the-art sense-based wsd systems.thus, some research has been focused on deriving different word-sense groupings to overcome the fine grained distinctions of wn (hearst and schutze, 1993), (peters et al , 1998), (mihalceaand moldovan, 2001), (agirre and lopez dela calle, 2003), (navigli, 2006) <papid> P06-1014 </papid>and (snow et al , 2007).<papid> D07-1107 </papid></citsent>
<aftsection>
<nextsent>that is, they provide methods for grouping senses of the same word, thus producing coarser word sense groupings for better disambiguation.wikipedia3 has been also recently used to overcome some problems of automatic learning methods: excessively fine grained definition of meanings, lack of annotated data and strong domain dependence of existing annotated corpora.
</nextsent>
<nextsent>in this way, wikipedia provides new very large source of annotated data, constantly expanded (mihalcea, 2007).<papid> N07-1025 </papid></nextsent>
<nextsent>1http://www.senseval.org 2http://wordnet.princeton.edu 3http://www.wikipedia.org 389 in contrast, some research have been focused onusing predefined sets of sense-groupings for learning class-based classifiers for wsd (segond et al , 1997), (<papid> W97-0811 </papid>ciaramita and johnson, 2003), (<papid> W03-1022 </papid>villarejo et al , 2005), (curran, 2005) <papid> P05-1004 </papid>and (ciaramita and altun, 2006).<papid> W06-1670 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1742">
<title id=" E09-1045.xml">an empirical study on class based word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>interestingly, this result is difficult to outperform by state-of-the-art sense-based wsd systems.thus, some research has been focused on deriving different word-sense groupings to overcome the fine grained distinctions of wn (hearst and schutze, 1993), (peters et al , 1998), (mihalceaand moldovan, 2001), (agirre and lopez dela calle, 2003), (navigli, 2006) <papid> P06-1014 </papid>and (snow et al , 2007).<papid> D07-1107 </papid></prevsent>
<prevsent>that is, they provide methods for grouping senses of the same word, thus producing coarser word sense groupings for better disambiguation.wikipedia3 has been also recently used to overcome some problems of automatic learning methods: excessively fine grained definition of meanings, lack of annotated data and strong domain dependence of existing annotated corpora.</prevsent>
</prevsection>
<citsent citstr=" N07-1025 ">
in this way, wikipedia provides new very large source of annotated data, constantly expanded (mihalcea, 2007).<papid> N07-1025 </papid></citsent>
<aftsection>
<nextsent>1http://www.senseval.org 2http://wordnet.princeton.edu 3http://www.wikipedia.org 389 in contrast, some research have been focused onusing predefined sets of sense-groupings for learning class-based classifiers for wsd (segond et al , 1997), (<papid> W97-0811 </papid>ciaramita and johnson, 2003), (<papid> W03-1022 </papid>villarejo et al , 2005), (curran, 2005) <papid> P05-1004 </papid>and (ciaramita and altun, 2006).<papid> W06-1670 </papid></nextsent>
<nextsent>that is, grouping senses of different words into the same explicit and comprehensive semantic class.most of the later approaches used the original lexico graphical files of wn (more recently called supersenses) as very coarse grained sense distinctions.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1743">
<title id=" E09-1045.xml">an empirical study on class based word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>that is, they provide methods for grouping senses of the same word, thus producing coarser word sense groupings for better disambiguation.wikipedia3 has been also recently used to overcome some problems of automatic learning methods: excessively fine grained definition of meanings, lack of annotated data and strong domain dependence of existing annotated corpora.
</prevsent>
<prevsent>in this way, wikipedia provides new very large source of annotated data, constantly expanded (mihalcea, 2007).<papid> N07-1025 </papid></prevsent>
</prevsection>
<citsent citstr=" W97-0811 ">
1http://www.senseval.org 2http://wordnet.princeton.edu 3http://www.wikipedia.org 389 in contrast, some research have been focused onusing predefined sets of sense-groupings for learning class-based classifiers for wsd (segond et al , 1997), (<papid> W97-0811 </papid>ciaramita and johnson, 2003), (<papid> W03-1022 </papid>villarejo et al , 2005), (curran, 2005) <papid> P05-1004 </papid>and (ciaramita and altun, 2006).<papid> W06-1670 </papid></citsent>
<aftsection>
<nextsent>that is, grouping senses of different words into the same explicit and comprehensive semantic class.most of the later approaches used the original lexico graphical files of wn (more recently called supersenses) as very coarse grained sense distinctions.
</nextsent>
<nextsent>however, not so much attention has been paid on learning class-based classifiers from other available sense groupings such as wordnet domains (magnini and cavaglia`, 2000), sumo labels (niles and pease, 2001), eurowordnetbase concepts (vossen et al , 1998), top concept ontology labels (alvez et al , 2008) or basic level concepts (izquierdo et al , 2007).
</nextsent>
<nextsent>obviously, these resources relate senses at some level of abstraction using different semantic criteria and properties that could be of interest for wsd.
</nextsent>
<nextsent>possibly, their combination could improve the overall results since they offer different semantic perspectives of the data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1744">
<title id=" E09-1045.xml">an empirical study on class based word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>that is, they provide methods for grouping senses of the same word, thus producing coarser word sense groupings for better disambiguation.wikipedia3 has been also recently used to overcome some problems of automatic learning methods: excessively fine grained definition of meanings, lack of annotated data and strong domain dependence of existing annotated corpora.
</prevsent>
<prevsent>in this way, wikipedia provides new very large source of annotated data, constantly expanded (mihalcea, 2007).<papid> N07-1025 </papid></prevsent>
</prevsection>
<citsent citstr=" W03-1022 ">
1http://www.senseval.org 2http://wordnet.princeton.edu 3http://www.wikipedia.org 389 in contrast, some research have been focused onusing predefined sets of sense-groupings for learning class-based classifiers for wsd (segond et al , 1997), (<papid> W97-0811 </papid>ciaramita and johnson, 2003), (<papid> W03-1022 </papid>villarejo et al , 2005), (curran, 2005) <papid> P05-1004 </papid>and (ciaramita and altun, 2006).<papid> W06-1670 </papid></citsent>
<aftsection>
<nextsent>that is, grouping senses of different words into the same explicit and comprehensive semantic class.most of the later approaches used the original lexico graphical files of wn (more recently called supersenses) as very coarse grained sense distinctions.
</nextsent>
<nextsent>however, not so much attention has been paid on learning class-based classifiers from other available sense groupings such as wordnet domains (magnini and cavaglia`, 2000), sumo labels (niles and pease, 2001), eurowordnetbase concepts (vossen et al , 1998), top concept ontology labels (alvez et al , 2008) or basic level concepts (izquierdo et al , 2007).
</nextsent>
<nextsent>obviously, these resources relate senses at some level of abstraction using different semantic criteria and properties that could be of interest for wsd.
</nextsent>
<nextsent>possibly, their combination could improve the overall results since they offer different semantic perspectives of the data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1745">
<title id=" E09-1045.xml">an empirical study on class based word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>that is, they provide methods for grouping senses of the same word, thus producing coarser word sense groupings for better disambiguation.wikipedia3 has been also recently used to overcome some problems of automatic learning methods: excessively fine grained definition of meanings, lack of annotated data and strong domain dependence of existing annotated corpora.
</prevsent>
<prevsent>in this way, wikipedia provides new very large source of annotated data, constantly expanded (mihalcea, 2007).<papid> N07-1025 </papid></prevsent>
</prevsection>
<citsent citstr=" P05-1004 ">
1http://www.senseval.org 2http://wordnet.princeton.edu 3http://www.wikipedia.org 389 in contrast, some research have been focused onusing predefined sets of sense-groupings for learning class-based classifiers for wsd (segond et al , 1997), (<papid> W97-0811 </papid>ciaramita and johnson, 2003), (<papid> W03-1022 </papid>villarejo et al , 2005), (curran, 2005) <papid> P05-1004 </papid>and (ciaramita and altun, 2006).<papid> W06-1670 </papid></citsent>
<aftsection>
<nextsent>that is, grouping senses of different words into the same explicit and comprehensive semantic class.most of the later approaches used the original lexico graphical files of wn (more recently called supersenses) as very coarse grained sense distinctions.
</nextsent>
<nextsent>however, not so much attention has been paid on learning class-based classifiers from other available sense groupings such as wordnet domains (magnini and cavaglia`, 2000), sumo labels (niles and pease, 2001), eurowordnetbase concepts (vossen et al , 1998), top concept ontology labels (alvez et al , 2008) or basic level concepts (izquierdo et al , 2007).
</nextsent>
<nextsent>obviously, these resources relate senses at some level of abstraction using different semantic criteria and properties that could be of interest for wsd.
</nextsent>
<nextsent>possibly, their combination could improve the overall results since they offer different semantic perspectives of the data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1746">
<title id=" E09-1045.xml">an empirical study on class based word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>that is, they provide methods for grouping senses of the same word, thus producing coarser word sense groupings for better disambiguation.wikipedia3 has been also recently used to overcome some problems of automatic learning methods: excessively fine grained definition of meanings, lack of annotated data and strong domain dependence of existing annotated corpora.
</prevsent>
<prevsent>in this way, wikipedia provides new very large source of annotated data, constantly expanded (mihalcea, 2007).<papid> N07-1025 </papid></prevsent>
</prevsection>
<citsent citstr=" W06-1670 ">
1http://www.senseval.org 2http://wordnet.princeton.edu 3http://www.wikipedia.org 389 in contrast, some research have been focused onusing predefined sets of sense-groupings for learning class-based classifiers for wsd (segond et al , 1997), (<papid> W97-0811 </papid>ciaramita and johnson, 2003), (<papid> W03-1022 </papid>villarejo et al , 2005), (curran, 2005) <papid> P05-1004 </papid>and (ciaramita and altun, 2006).<papid> W06-1670 </papid></citsent>
<aftsection>
<nextsent>that is, grouping senses of different words into the same explicit and comprehensive semantic class.most of the later approaches used the original lexico graphical files of wn (more recently called supersenses) as very coarse grained sense distinctions.
</nextsent>
<nextsent>however, not so much attention has been paid on learning class-based classifiers from other available sense groupings such as wordnet domains (magnini and cavaglia`, 2000), sumo labels (niles and pease, 2001), eurowordnetbase concepts (vossen et al , 1998), top concept ontology labels (alvez et al , 2008) or basic level concepts (izquierdo et al , 2007).
</nextsent>
<nextsent>obviously, these resources relate senses at some level of abstraction using different semantic criteria and properties that could be of interest for wsd.
</nextsent>
<nextsent>possibly, their combination could improve the overall results since they offer different semantic perspectives of the data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1749">
<title id=" E09-1045.xml">an empirical study on class based word sense disambiguation </title>
<section> class-based wsd.  </section>
<citcontext>
<prevsection>
<prevsent>semcor has been used for training while the corpora from the english all-words tasks of senseval-2 and senseval-3has been used for testing.
</prevsent>
<prevsent>we also considered semeval-2007 coarse grained task corpus for testing, but this dataset was discarded because this corpus is also annotated with clusters of word senses.
</prevsent>
</prevsection>
<citsent citstr=" H93-1061 ">
semcor (miller et al , 1993) <papid> H93-1061 </papid>is subset of the brown corpus plus the novel the red badge of courage, and it has been developed by the same group that created wordnet.</citsent>
<aftsection>
<nextsent>it contains 253 texts and around 700,000 running words, and more than200,000 are also lemmatized and sense-tagged according to princeton wordnet 1.6.senseval-27 english all-words corpus (here inafter se2) (palmer et al , 2001) consists on 5,000words of text from three wsj articles representing different domains from the penn treebank ii.
</nextsent>
<nextsent>the sense inventory used for tagging is wordnet 1.7.
</nextsent>
<nextsent>finally, senseval-38 english all-words cor-.
</nextsent>
<nextsent>pus (hereinafter se3) (snyder and palmer, 2004), <papid> W04-0811 </papid>is made up of 5,000 words, extracted from twowsj articles and one excerpt from the brown cor pus.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1751">
<title id=" E09-1045.xml">an empirical study on class based word sense disambiguation </title>
<section> class-based wsd.  </section>
<citcontext>
<prevsection>
<prevsent>7http://www.sle.sharp.co.uk/senseval2 8http://www.senseval.org/senseval3 3.3 feature types.
</prevsent>
<prevsent>we have defined set of features to represent the examples according to previous works in wsd and the nature of class-based wsd.
</prevsent>
</prevsection>
<citsent citstr=" P94-1013 ">
features widely used in the literature as in (yarowsky, 1994) <papid> P94-1013 </papid>have been selected.</citsent>
<aftsection>
<nextsent>these features are pieces of information that occur in the context of the target word, and can be organized as: local features: bigrams and trigrams that contain the target word, including part-of-speech (pos), lemmas or word-forms.topical features: word forms or lemmas appearing in windows around the target word.in particular, our systems use the following basic features: word forms and lemmas in window of 10 words around the target wordpos: the concatenation of the preced ing/following three/five pos bigrams and trigrams formed by lemmas and word-forms and obtained in window of 5 words.
</nextsent>
<nextsent>we use of all tokens regardless their pos to build bi/trigrams.
</nextsent>
<nextsent>the target word is replaced by in these features to increase the generalization of them for the semantic classifiers moreover, we also defined set of semantic features to explode different semantic resources in order to enrich the set of basic features: most frequent semantic class calculated over semcor, the most frequent semantic class for the target word.
</nextsent>
<nextsent>monosemous semantic classes semantic classes of the monosemous words arround the target word in window of size 5.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1754">
<title id=" E06-1045.xml">data driven generation of emphatic facial displays </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this is therefore task for which data-driven techniques are beneficial.
</prevsent>
<prevsent>in this paper, we address the task of selecting emphatic facial displays for the talking head in the comic1 multimodal dialogue system.
</prevsent>
</prevsection>
<citsent citstr=" P05-3012 ">
in the basic comic process for generating multimodal output (foster et al, 2005), <papid> P05-3012 </papid>facial displays are selected using simple rules based only on the pitch accents specified by the text generation system.</citsent>
<aftsection>
<nextsent>inorder to make more sophisticated and naturalistic selection of facial displays, we recorded single speaker reading set of sentences drawn fromthe comic domain, and annotated the facial displays that he used and the contexts in which he used them.
</nextsent>
<nextsent>we then created models based on the data from this corpus and used them to choose the facial displays for the comic talking head.
</nextsent>
<nextsent>1http://www.hcrc.ed.ac.uk/comic/ 353 the rest of this paper is arranged as follows.first, in section 2, we describe previous approaches to selecting non-verbal behaviour for embodied conversational agents.
</nextsent>
<nextsent>in section 3, wethen show how we collected and annotated corpus of facial displays, and give some generalisations about the range of displays found in the corpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1755">
<title id=" E06-1045.xml">data driven generation of emphatic facial displays </title>
<section> choosing non-verbal behaviour for.  </section>
<citcontext>
<prevsection>
<prevsent>the implementations vary as to how directly they use the human data.
</prevsent>
<prevsent>in some systems, motion specifications for the agent are created from scratch, using rules derived from studying human behaviour.
</prevsent>
</prevsection>
<citsent citstr=" P01-1016 ">
for the rea agent (cassell et al, 2001<papid> P01-1016 </papid>a), for example, gesturing behaviour was selected to perform particular communicative functions, using rules based on studies of typical north american non-verbal dis plays.</citsent>
<aftsection>
<nextsent>similarly, the greta agent (de carolis et al,2002) selected its per formative facial displays using hand-crafted rules to map from affective states to facial motions.
</nextsent>
<nextsent>such implementations do not make direct use of any recorded human motions; this means that they generate average behaviours from range of people, but it is difficult to adapt them to reproduce the behaviour of an individual.
</nextsent>
<nextsent>in contrast, other eca implementations have selected non-verbal behaviour based directly on motion-capture recordings of humans.
</nextsent>
<nextsent>stone et al (2004), for example, recorded an actor performing scripted output in the domain of the target system.they then segmented the recordings into coherent phrases and annotated them with the relevant semantic and pragmatic information, and combined the segments at run-time to produce complete performance specifications that were then played back on the agent.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1757">
<title id=" E06-1045.xml">data driven generation of emphatic facial displays </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>the predicted rankings from the cross-validationstudy differ from those in the human evalua tion: while the cross-validation gave the highest scores to the majority-choice models, the human judges actually showed an overall preference forthe weighted-choice models.
</prevsent>
<prevsent>this provides support for our hypothesis that humans would prefer generated output that reproduced more of the variation in the corpus, even if the choices made on specific sentences differ from those mode in the corpus.
</prevsent>
</prevsection>
<citsent citstr=" E06-1040 ">
when belz and reiter (2006) <papid> E06-1040 </papid>perform eda similar study comparing natural-language generation systems that used different text-planningstrategies, they also found similar results: automated measures tended to favour majority-choice strategies, while human judges preferred those that made weighted choices.</citsent>
<aftsection>
<nextsent>in general, this sort of automated measure will always tend to favour strategies that, on average, do not diverge far from what is found in the corpus, which indicates drawback to using such measures alone to evaluate generation systems where variation is expected.the current study also suggests further draw back to corpus-based evaluation: users may vary systematically amongst themselves in what theyprefer.
</nextsent>
<nextsent>all of the overall preference for weighted choice models came from the female subjects; 358
</nextsent>
<nextsent>                        ! #! $ %  &amp; &amp;  #  # (a) male subjects
</nextsent>
<nextsent>                           ! #$  % &amp;       $  $ (b) female subjects figure 6: gender influence on head-to-head preferences the male subjects did not express any significant preference either way, but had mild preference for the majority-choice models.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1759">
<title id=" E12-1049.xml">experimenting with distant supervision for emotion classification </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>text classification according to emotion and sentiment is well-established research area.
</prevsent>
<prevsent>inthis and other areas of text analysis and classification, recent years have seen rise in use of data from online sources and social media, as these provide very large, often freely available datasets (see e.g.
</prevsent>
</prevsection>
<citsent citstr=" D10-1124 ">
(eisenstein et al 2010; <papid> D10-1124 </papid>go et al 2009; pak and paroubek, 2010) amongst many others).</citsent>
<aftsection>
<nextsent>however, one of the challenges this poses is that of data annotation: given very large amounts of data, often consisting of very short texts, written in unconventional style and without accompanying meta data, audio/video signals or access to the author for disambiguation, how can we easily produce gold-standard labelling for training and/or for evaluation and test?
</nextsent>
<nextsent>one possible solution that is becoming popular is crowd-sourcing the labelling task, as the easy access to very large numbers of annotators provided by tools such as amazons mechanical turk can help with the problem of dataset size; however, this has its own attendant problems of annotator reliability (see e.g.
</nextsent>
<nextsent>(hsuehet al 2009)), <papid> W09-1904 </papid>and cannot directly help with the inherent problem of ambiguity ? using many annotators does not guarantee that they can understand or correctly assign the authors intended interpretation or emotional state.in this paper, we investigate different approach via distant supervision (see e.g.</nextsent>
<nextsent>(mintz et al 2009)).<papid> P09-1113 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1760">
<title id=" E12-1049.xml">experimenting with distant supervision for emotion classification </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, one of the challenges this poses is that of data annotation: given very large amounts of data, often consisting of very short texts, written in unconventional style and without accompanying meta data, audio/video signals or access to the author for disambiguation, how can we easily produce gold-standard labelling for training and/or for evaluation and test?
</prevsent>
<prevsent>one possible solution that is becoming popular is crowd-sourcing the labelling task, as the easy access to very large numbers of annotators provided by tools such as amazons mechanical turk can help with the problem of dataset size; however, this has its own attendant problems of annotator reliability (see e.g.
</prevsent>
</prevsection>
<citsent citstr=" W09-1904 ">
(hsuehet al 2009)), <papid> W09-1904 </papid>and cannot directly help with the inherent problem of ambiguity ? using many annotators does not guarantee that they can understand or correctly assign the authors intended interpretation or emotional state.in this paper, we investigate different approach via distant supervision (see e.g.</citsent>
<aftsection>
<nextsent>(mintz et al 2009)).<papid> P09-1113 </papid></nextsent>
<nextsent>by using conventional markers of emotional content within the texts themselves asa surrogate for explicit labels, we can quickly retrieve large subsets of (noisily) labelled data.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1761">
<title id=" E12-1049.xml">experimenting with distant supervision for emotion classification </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>one possible solution that is becoming popular is crowd-sourcing the labelling task, as the easy access to very large numbers of annotators provided by tools such as amazons mechanical turk can help with the problem of dataset size; however, this has its own attendant problems of annotator reliability (see e.g.
</prevsent>
<prevsent>(hsuehet al 2009)), <papid> W09-1904 </papid>and cannot directly help with the inherent problem of ambiguity ? using many annotators does not guarantee that they can understand or correctly assign the authors intended interpretation or emotional state.in this paper, we investigate different approach via distant supervision (see e.g.</prevsent>
</prevsection>
<citsent citstr=" P09-1113 ">
(mintz et al 2009)).<papid> P09-1113 </papid></citsent>
<aftsection>
<nextsent>by using conventional markers of emotional content within the texts themselves asa surrogate for explicit labels, we can quickly retrieve large subsets of (noisily) labelled data.
</nextsent>
<nextsent>this approach has the advantage of giving us direct access to the authors?
</nextsent>
<nextsent>own intended interpretation or emotional state, without relying on third party annotators.
</nextsent>
<nextsent>of course, the labels themselves may be noisy: ambiguous, vague or not havinga direct correspondence with the desired classification.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1762">
<title id=" E12-1049.xml">experimenting with distant supervision for emotion classification </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>own intended interpretation or emotional state, without relying on third party annotators.
</prevsent>
<prevsent>of course, the labels themselves may be noisy: ambiguous, vague or not havinga direct correspondence with the desired classification.
</prevsent>
</prevsection>
<citsent citstr=" P05-2008 ">
we therefore experiment with multiple such conventions with apparently similar meanings ? here, emoticons (following (read, 2005)) <papid> P05-2008 </papid>and twitter hash tags ? allowing us to examine the similarity of classifiers trained on independent labels but intended to detect the same underlyingclass.</citsent>
<aftsection>
<nextsent>we also investigate the precision and correspondence of particular labels with the desire demotion classes by testing on small set of man 482 ually labelled data.we show that the success of this approach depends on both the conventional markers chosen and the emotion classes themselves.
</nextsent>
<nextsent>some emotions are both reliably marked by different conventions and distinguishable from other emotions; this seems particularly true for happiness, sadness and anger, indicating that this approach can provide not only the basic distinction required for sentiment analysis but some more finer-grained information.
</nextsent>
<nextsent>others are either less distinguishable from short text messages, or less reliably marked.
</nextsent>
<nextsent>2.1 emotion and sentiment classification.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1763">
<title id=" E12-3001.xml">improving pronoun translation for statistical machine translation </title>
<section> identifying the pronoun antecedent;.  </section>
<citcontext>
<prevsection>
<prevsent>where multiple referring expressions refer to the same antecedent, they are said tobe coreferential.
</prevsent>
<prevsent>anaphora resolution and there lated task of coreference resolution have been the subject of considerable research within natural language processing (nlp).
</prevsent>
</prevsection>
<citsent citstr=" P10-1142 ">
excellent surveys are provided by strube (2007) and ng (2010).<papid> P10-1142 </papid>unresolved anaphora can add significant translation ambiguity, and their incorrect translation can significantly decrease readers ability to understand text.</citsent>
<aftsection>
<nextsent>accurate coreference in translation is therefore necessary in order to produce understandable and cohesive texts.
</nextsent>
<nextsent>this justifies recent interest (le nagard &amp; koehn, 2010; hardmeier &amp; federico, 2010) and motivates the work presented in this paper.
</nextsent>
<nextsent>2.2 pronominal coreference in english.
</nextsent>
<nextsent>whilst english makes some use of case, it lacks the grammatical gender found in other languages.for monolingual speakers, the relatively few different pronoun forms in english make sentences easy to generate: pronoun choice depends on the number and gender of the entity to which theyre fer.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1764">
<title id=" E12-3001.xml">improving pronoun translation for statistical machine translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>is used to refer to dog (masculine animate, singular) and svuj? to refer to castle (masculine inanimate, singular), both of which belong to the speaker.
</prevsent>
<prevsent>because pronoun may take large number of morphological forms in czech and because case is not checked in annotation projection, the method presented here for translating co referring pronouns does not guarantee their correct form.
</prevsent>
</prevsection>
<citsent citstr=" J94-4002 ">
early work on integrating anaphora resolution with machine translation includes the rule-based approaches of mitkov et al (1995) and lappin &amp; leass (1994) <papid> J94-4002 </papid>and the transfer-based approach of saggion &amp; carvalho (1994).</citsent>
<aftsection>
<nextsent>work in the 1990s culminated in the publication of special issue of machine translation on anaphora resolution (mitkov, 1999).
</nextsent>
<nextsent>work then appears to have beenon hold until papers were published by le na gard &amp; koehn (2010) and hardmeier &amp; federico(2010).
</nextsent>
<nextsent>this resurgence of interest follows advances since the 1990s which have made new approaches possible.
</nextsent>
<nextsent>the work described in this paper resembles thatof le nagard &amp; koehn (2010), with two main differences.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1765">
<title id=" E12-3001.xml">improving pronoun translation for statistical machine translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in contrast, hardmeier &amp; federico (2010) focuson english-german translation and model coreference using word dependency module integrated within the log-linear smt model as an additional feature function.
</prevsent>
<prevsent>annotation projection has been used elsewhere in smt.
</prevsent>
</prevsection>
<citsent citstr=" W08-0302 ">
gimpel &amp; smith (2008) <papid> W08-0302 </papid>use it to capturelongdistance phenomena within single sentence in the source-language text via the extraction of sentence-level contextual features, which are used to augment smt translation models and better predict phrase translation.</citsent>
<aftsection>
<nextsent>projection techniques have also been applied to multilingual word sense disambiguation whereby the sense of word may be determined in another language (diab, 2004; <papid> W04-1609 </papid>khapra et al  2009).<papid> D09-1048 </papid></nextsent>
<nextsent>4.1 overview.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1766">
<title id=" E12-3001.xml">improving pronoun translation for statistical machine translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>annotation projection has been used elsewhere in smt.
</prevsent>
<prevsent>gimpel &amp; smith (2008) <papid> W08-0302 </papid>use it to capturelongdistance phenomena within single sentence in the source-language text via the extraction of sentence-level contextual features, which are used to augment smt translation models and better predict phrase translation.</prevsent>
</prevsection>
<citsent citstr=" W04-1609 ">
projection techniques have also been applied to multilingual word sense disambiguation whereby the sense of word may be determined in another language (diab, 2004; <papid> W04-1609 </papid>khapra et al  2009).<papid> D09-1048 </papid></citsent>
<aftsection>
<nextsent>4.1 overview.
</nextsent>
<nextsent>i have followed le nagard &amp; koehn (2010) in using two-step approach to translation, with annotation projection incorporated as pre-processing 3 it stands on hill.
</nextsent>
<nextsent>the castle is old.
</nextsent>
<nextsent>hrad je star?.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1767">
<title id=" E12-3001.xml">improving pronoun translation for statistical machine translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>annotation projection has been used elsewhere in smt.
</prevsent>
<prevsent>gimpel &amp; smith (2008) <papid> W08-0302 </papid>use it to capturelongdistance phenomena within single sentence in the source-language text via the extraction of sentence-level contextual features, which are used to augment smt translation models and better predict phrase translation.</prevsent>
</prevsection>
<citsent citstr=" D09-1048 ">
projection techniques have also been applied to multilingual word sense disambiguation whereby the sense of word may be determined in another language (diab, 2004; <papid> W04-1609 </papid>khapra et al  2009).<papid> D09-1048 </papid></citsent>
<aftsection>
<nextsent>4.1 overview.
</nextsent>
<nextsent>i have followed le nagard &amp; koehn (2010) in using two-step approach to translation, with annotation projection incorporated as pre-processing 3 it stands on hill.
</nextsent>
<nextsent>the castle is old.
</nextsent>
<nextsent>hrad je star?.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1768">
<title id=" E12-3001.xml">improving pronoun translation for statistical machine translation </title>
<section> methodology.  </section>
<citcontext>
<prevsection>
<prevsent>they share the same 3-gram language model constructed from the target-side text ofthe parallel training corpus and the czech monolingual 2010 and 2011 news crawl corpora1.
</prevsent>
<prevsent>the language model was constructed using the srilm toolkit (stolcke, 2002) with interpolated kneser-ney discounting (kneser &amp; ney, 1995).
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
in addition, both systems are forced to use the same word alignments (constructed using giza++(och &amp; ney, 2003) <papid> J03-1002 </papid>in both language pair directions and using stemmed training data in which words are limited to the first four characters) inorder to mitigate the effects of czech word inflection on word alignment statistics.</citsent>
<aftsection>
<nextsent>this helps to ensure that the czech translation of the head of the antecedent remains constantin both steps of the two-step process.
</nextsent>
<nextsent>if this were to change it would defeat the purpose of pronoun annotation as different czech translations could result in different gender and/or number.
</nextsent>
<nextsent>the baseline system was trained using thepenn wall street journal corpus with no annotation, while the annotated system was trained with an annotated version of the same text (see table 1), with the target-language text being the same in both cases.
</nextsent>
<nextsent>the penn wall street journal corpus was annotated using the process described above, with the number and gender of the czech translation of the antecedent head obtained from the pcedt 2.0 alignment file.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1770">
<title id=" E12-3001.xml">improving pronoun translation for statistical machine translation </title>
<section> methodology.  </section>
<citcontext>
<prevsection>
<prevsent>early work focussed onthe development of techniques for anaphora resolution and their integration within machine translation (lappin &amp; leass, 1994; <papid> J94-4002 </papid>saggion &amp; carvalho, 1994; mitkov et al  1995), with little mention of evaluation.</prevsent>
<prevsent>in recent work, evaluation has become much more important.</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
both le na gard &amp; koehn (2010) and hardmeier &amp; federico (2010) consider and reject bleu (papineni et al 2002) <papid> P02-1040 </papid>as ill-suited for evaluating pronoun transla tion.</citsent>
<aftsection>
<nextsent>while hardmeier &amp; federico propose and 5 use strict recall and precision based metric for english german translation, found it unsuitable for english czech translation, given the highly inflective nature of czech.
</nextsent>
<nextsent>given the importance of evaluation to the goalof assessing the effectiveness of annotation projection for improving the translation of corefer ring pronouns, carried out two separate types of evaluation ? an automated evaluation which could be applied to the entire test set, and an indepth manual assessment that might provide more information, but could only be performed on subset of the test set.
</nextsent>
<nextsent>the automated evaluation is based on the fact that czech pronoun must agree in number and gender with its antecedent.
</nextsent>
<nextsent>thus one can count the number of pronouns in the translation output for which this agreement holds,rather than simply score the output against single reference translation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1771">
<title id=" E12-3001.xml">improving pronoun translation for statistical machine translation </title>
<section> total coreferential english pronouns trans-.  </section>
<citcontext>
<prevsection>
<prevsent>these include, but are not limited to, handling pronoun dropping in pro-drop languages, developing pronoun-specific automated evaluation metrics and addressing the problem of having only one reference translation for use with such metrics.
</prevsent>
<prevsent>in this regard, will be considering the use of paraphrase techniques to generate synthetic reference translations to augment an existing reference translation set.
</prevsent>
</prevsection>
<citsent citstr=" N06-1058 ">
initial efforts will focus on adapting the approach of kauchak &amp; barzilay (2006) <papid> N06-1058 </papid>and back translation methods for extracting paraphrases (bannard &amp;callison-burch;, 2005) to the more specific problem of pronoun variation.</citsent>
<aftsection>
<nextsent>acknowledgementsi would like to thank bonnie webber (univer sity of edinburgh) who supervised this project and marketa lopatkova?
</nextsent>
<nextsent>(charles university) who provided the much needed czech language assistance.
</nextsent>
<nextsent>i am very grateful to ondrej bojar (charles university) for his numerous helpful suggestion sand to the institute of formal and applied linguistics (charles university) for providing the pcedt 2.0 corpus.
</nextsent>
<nextsent>i would also like to thank wolodja wentland and the three anonymous reviewers for their feedback.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1772">
<title id=" E09-1041.xml">learning based named entity recognition for morphologically rich resource scarce languages </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in other words, bengali ner is complicated not only by the scarcity of annotated data, but also by the lack of an accurate pos tagger.
</prevsent>
<prevsent>one could imagine building bengali pos tagger using un 1see http://en.wikipedia.org/wiki/bengali language.
</prevsent>
</prevsection>
<citsent citstr=" E03-1009 ">
354 supervised induction techniques that have been successfully developed for english (e.g., schutze(1995), clark (2003)), <papid> E03-1009 </papid>including the recently proposed prototype-driven approach (haghighiand klein, 2006) <papid> N06-1041 </papid>and bayesian approach (gold water and griffiths, 2007).<papid> P07-1094 </papid></citsent>
<aftsection>
<nextsent>the majority of these approaches operate by clustering distributionally similar words, but they are unlikely to work well for bengali for two reasons.
</nextsent>
<nextsent>first, bengali is relatively free word order language, and hence the distributional information collected for bengali words may not be as reliable as that for english words.
</nextsent>
<nextsent>second, many closed-class words that typically appear in the distributional representation of an english word (e.g., prepositions and particles such as in?
</nextsent>
<nextsent>and to?)
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1773">
<title id=" E09-1041.xml">learning based named entity recognition for morphologically rich resource scarce languages </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in other words, bengali ner is complicated not only by the scarcity of annotated data, but also by the lack of an accurate pos tagger.
</prevsent>
<prevsent>one could imagine building bengali pos tagger using un 1see http://en.wikipedia.org/wiki/bengali language.
</prevsent>
</prevsection>
<citsent citstr=" N06-1041 ">
354 supervised induction techniques that have been successfully developed for english (e.g., schutze(1995), clark (2003)), <papid> E03-1009 </papid>including the recently proposed prototype-driven approach (haghighiand klein, 2006) <papid> N06-1041 </papid>and bayesian approach (gold water and griffiths, 2007).<papid> P07-1094 </papid></citsent>
<aftsection>
<nextsent>the majority of these approaches operate by clustering distributionally similar words, but they are unlikely to work well for bengali for two reasons.
</nextsent>
<nextsent>first, bengali is relatively free word order language, and hence the distributional information collected for bengali words may not be as reliable as that for english words.
</nextsent>
<nextsent>second, many closed-class words that typically appear in the distributional representation of an english word (e.g., prepositions and particles such as in?
</nextsent>
<nextsent>and to?)
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1774">
<title id=" E09-1041.xml">learning based named entity recognition for morphologically rich resource scarce languages </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in other words, bengali ner is complicated not only by the scarcity of annotated data, but also by the lack of an accurate pos tagger.
</prevsent>
<prevsent>one could imagine building bengali pos tagger using un 1see http://en.wikipedia.org/wiki/bengali language.
</prevsent>
</prevsection>
<citsent citstr=" P07-1094 ">
354 supervised induction techniques that have been successfully developed for english (e.g., schutze(1995), clark (2003)), <papid> E03-1009 </papid>including the recently proposed prototype-driven approach (haghighiand klein, 2006) <papid> N06-1041 </papid>and bayesian approach (gold water and griffiths, 2007).<papid> P07-1094 </papid></citsent>
<aftsection>
<nextsent>the majority of these approaches operate by clustering distributionally similar words, but they are unlikely to work well for bengali for two reasons.
</nextsent>
<nextsent>first, bengali is relatively free word order language, and hence the distributional information collected for bengali words may not be as reliable as that for english words.
</nextsent>
<nextsent>second, many closed-class words that typically appear in the distributional representation of an english word (e.g., prepositions and particles such as in?
</nextsent>
<nextsent>and to?)
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1775">
<title id=" E09-1041.xml">learning based named entity recognition for morphologically rich resource scarce languages </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>second, it is not clear how comprehensive these lists are.
</prevsent>
<prevsent>relying on comprehensive lists that comprise large portion of the names in the test set essentially reduces the ner problem to dictionary-lookup problem, which is arguably not very interesting from research perspective.in addition, many existing learning-based bengali ne recognizers have several common weaknesses.
</prevsent>
</prevsection>
<citsent citstr=" P07-2056 ">
first, they use as features pseudo-affixes, which are created by extracting the first and the last characters of word (where 1 ? ? 4)(e.g., dandapat et al  (2007)).<papid> P07-2056 </papid></citsent>
<aftsection>
<nextsent>while affixes encode essential grammatical information in bengali due to its morphological richness, this extraction method is arguably too ad-hoc and does not cover many useful affixes.
</nextsent>
<nextsent>second, they typically adopt pipe lined ner architecture, performing pos tagging prior toner and encoding the resulting not-so-accurate pos information as feature.in other words, errors in pos tagging are propagated to the ne recognizer via the pos feature, thus limiting its performance.motivated in part by these weaknesses, we investigate how to improve learning-based ne recognizer that does not relyon manually-constructedgazetteers.
</nextsent>
<nextsent>specifically, we investigate two learning architectures for our ner system.
</nextsent>
<nextsent>the first one is the aforementioned pipe lined architecture in which the ne recognizer uses as features the output of pos tagger that is trained independently of the recognizer.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1776">
<title id=" E09-1041.xml">learning based named entity recognition for morphologically rich resource scarce languages </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in sections 5 and 6, we train and evaluate pos tagger and an ne recognizer independently, augmenting the feature set typically used for these two tasks with our new knowledge sources.
</prevsent>
<prevsent>finally, we describe and evaluate our joint model in section 7.
</prevsent>
</prevsection>
<citsent citstr=" W99-0612 ">
cucerzan and yarowsky (1999) <papid> W99-0612 </papid>exploit morphological and contextual patterns to propose language-independent solution to ner.</citsent>
<aftsection>
<nextsent>they useaffixes based on the paradigm that named entities corresponding to particular class have similar morphological structure.
</nextsent>
<nextsent>their bootstrapping 355 approach is tested on romanian, english, greek, turkish, and hindi.
</nextsent>
<nextsent>the recall for hindi is the lowest (27.84%) among the five languages, suggesting that the lack of case information can significantly complicate the ner task.
</nextsent>
<nextsent>to investigate the role of gazette ers inner, mikheev et al  (1999) <papid> E99-1001 </papid>combine grammar rules with maximum entropy models and vary the gazetteersize.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1777">
<title id=" E09-1041.xml">learning based named entity recognition for morphologically rich resource scarce languages </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>their bootstrapping 355 approach is tested on romanian, english, greek, turkish, and hindi.
</prevsent>
<prevsent>the recall for hindi is the lowest (27.84%) among the five languages, suggesting that the lack of case information can significantly complicate the ner task.
</prevsent>
</prevsection>
<citsent citstr=" E99-1001 ">
to investigate the role of gazette ers inner, mikheev et al  (1999) <papid> E99-1001 </papid>combine grammar rules with maximum entropy models and vary the gazetteersize.</citsent>
<aftsection>
<nextsent>experimental results show that (1) the fscores for ne classes like person and organization are still high without gazette ers, ranging from 85% to 92%; and (2) small list of country name scan improve the low f-score for locations substantially.
</nextsent>
<nextsent>it is worth noting that their recognizer requires that the input data contain pos tags and simple semantic tags, whereas ours automatically acquires such linguistic information.
</nextsent>
<nextsent>in addition, their approach uses part of the dataset to extend the gazetteer.
</nextsent>
<nextsent>therefore, the resulting gazetteer list is specific to particular domain; on the other hand, our approach does not generate domain-specific list, since it makes use of wikipedia articles.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1778">
<title id=" E09-1041.xml">learning based named entity recognition for morphologically rich resource scarce languages </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in addition, their approach uses part of the dataset to extend the gazetteer.
</prevsent>
<prevsent>therefore, the resulting gazetteer list is specific to particular domain; on the other hand, our approach does not generate domain-specific list, since it makes use of wikipedia articles.
</prevsent>
</prevsection>
<citsent citstr=" E06-3004 ">
kozareva (2006) <papid> E06-3004 </papid>generates gazetteer lists for person and location names from unlabeled data using common patterns and graph exploration algorithm.</citsent>
<aftsection>
<nextsent>the location pattern is essentially preposition followed by capitalized context words.
</nextsent>
<nextsent>however, this approach is inadequate for morphologically-rich language like bengali, since prepositions are often realized as inflections.
</nextsent>
<nextsent>since bengali is morphologically productive, lot of grammatical information about bengali words is expressed via affixes.
</nextsent>
<nextsent>hence, these affixes could serve as useful features for training pos and ne taggers.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1779">
<title id=" E09-1041.xml">learning based named entity recognition for morphologically rich resource scarce languages </title>
<section> semantic class induction from.  </section>
<citcontext>
<prevsection>
<prevsent>in our experiments, we set this threshold to 50, and generate our vocabulary of 140k words from five years of articles taken from the bengali newspaper prothom alo.
</prevsent>
<prevsent>this enables us to induce 979 prefixes and 975 suffixes.
</prevsent>
</prevsection>
<citsent citstr=" D07-1074 ">
wikipedia wikipedia has recently been used as knowledge source for various language processing tasks, including taxonomy construction (ponzetto and strube, 2007a), coreference resolution (ponzetto and strube, 2007b), and english ner (e.g., bunescu and pasca (2006), cucerzan (2007), <papid> D07-1074 </papid>kazama and torisawa (2007), <papid> D07-1073 </papid>watanabe et al  (2007)).<papid> D07-1068 </papid></citsent>
<aftsection>
<nextsent>unlike previous work on using wikipedia for ner, our goal here is to (1) generate list of phrases and tokens that are potentially named entities from the 16914 articles in the bengali wikipedia3 and (2) heuristic ally annotate each ofthem with one of four classes, namely, per (person), org (organization), loc (location), or others (i.e., anything other than per, org and loc).
</nextsent>
<nextsent>4.1 generating an annotated list of phrases.
</nextsent>
<nextsent>we employ the steps below to generate our annotated list.
</nextsent>
<nextsent>generating and annotating the titles recall that each wikipedia article has been optionally assigned to one or more categories by its creatorand/or editors.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1780">
<title id=" E09-1041.xml">learning based named entity recognition for morphologically rich resource scarce languages </title>
<section> semantic class induction from.  </section>
<citcontext>
<prevsection>
<prevsent>in our experiments, we set this threshold to 50, and generate our vocabulary of 140k words from five years of articles taken from the bengali newspaper prothom alo.
</prevsent>
<prevsent>this enables us to induce 979 prefixes and 975 suffixes.
</prevsent>
</prevsection>
<citsent citstr=" D07-1073 ">
wikipedia wikipedia has recently been used as knowledge source for various language processing tasks, including taxonomy construction (ponzetto and strube, 2007a), coreference resolution (ponzetto and strube, 2007b), and english ner (e.g., bunescu and pasca (2006), cucerzan (2007), <papid> D07-1074 </papid>kazama and torisawa (2007), <papid> D07-1073 </papid>watanabe et al  (2007)).<papid> D07-1068 </papid></citsent>
<aftsection>
<nextsent>unlike previous work on using wikipedia for ner, our goal here is to (1) generate list of phrases and tokens that are potentially named entities from the 16914 articles in the bengali wikipedia3 and (2) heuristic ally annotate each ofthem with one of four classes, namely, per (person), org (organization), loc (location), or others (i.e., anything other than per, org and loc).
</nextsent>
<nextsent>4.1 generating an annotated list of phrases.
</nextsent>
<nextsent>we employ the steps below to generate our annotated list.
</nextsent>
<nextsent>generating and annotating the titles recall that each wikipedia article has been optionally assigned to one or more categories by its creatorand/or editors.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1781">
<title id=" E09-1041.xml">learning based named entity recognition for morphologically rich resource scarce languages </title>
<section> semantic class induction from.  </section>
<citcontext>
<prevsection>
<prevsent>in our experiments, we set this threshold to 50, and generate our vocabulary of 140k words from five years of articles taken from the bengali newspaper prothom alo.
</prevsent>
<prevsent>this enables us to induce 979 prefixes and 975 suffixes.
</prevsent>
</prevsection>
<citsent citstr=" D07-1068 ">
wikipedia wikipedia has recently been used as knowledge source for various language processing tasks, including taxonomy construction (ponzetto and strube, 2007a), coreference resolution (ponzetto and strube, 2007b), and english ner (e.g., bunescu and pasca (2006), cucerzan (2007), <papid> D07-1074 </papid>kazama and torisawa (2007), <papid> D07-1073 </papid>watanabe et al  (2007)).<papid> D07-1068 </papid></citsent>
<aftsection>
<nextsent>unlike previous work on using wikipedia for ner, our goal here is to (1) generate list of phrases and tokens that are potentially named entities from the 16914 articles in the bengali wikipedia3 and (2) heuristic ally annotate each ofthem with one of four classes, namely, per (person), org (organization), loc (location), or others (i.e., anything other than per, org and loc).
</nextsent>
<nextsent>4.1 generating an annotated list of phrases.
</nextsent>
<nextsent>we employ the steps below to generate our annotated list.
</nextsent>
<nextsent>generating and annotating the titles recall that each wikipedia article has been optionally assigned to one or more categories by its creatorand/or editors.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1782">
<title id=" E09-1041.xml">learning based named entity recognition for morphologically rich resource scarce languages </title>
<section> semantic class induction from.  </section>
<citcontext>
<prevsection>
<prevsent>similarly, if it has category whose name starts with cities of?
</prevsent>
<prevsent>or countries of,?
</prevsent>
</prevsection>
<citsent citstr=" J01-2001 ">
we 2the dependence on frequency and length is motivated by the observation that less frequent and shorter affixes are more likely to be erroneous (see goldsmith (2001)).<papid> J01-2001 </papid></citsent>
<aftsection>
<nextsent>3see http://bn.wikipedia.org.
</nextsent>
<nextsent>in our experiments, we used the bengali wikipedia dump obtained on october 22, 2007.
</nextsent>
<nextsent>356 ne class keywords per born,?
</nextsent>
<nextsent>died,?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1785">
<title id=" E09-1041.xml">learning based named entity recognition for morphologically rich resource scarce languages </title>
<section> part-of-speech tagging.  </section>
<citcontext>
<prevsection>
<prevsent>each instance is represented by set of linguistic features, as described next.
</prevsent>
<prevsent>7a detailed description of these pos tags can be found in http://shiva.iiit.ac.in/spsal2007/iiit tagset guidelines.pdf, and are omitted here due to space limitations.
</prevsent>
</prevsection>
<citsent citstr=" P06-2100 ">
this tagset and the penn treebank tagset differ in that (1) nouns do not have number feature; (2) verbs do not have tense feature; and (3) adjectives and adverbs are not subcategorized.features our feature set consists of (1) baseline features motivated by those used in dandapat et al (2007) <papid> P07-2056 </papid>bengali pos tagger and singh et al (2006) <papid> P06-2100 </papid>hindi pos tagger, as well as (2) features derived from our induced affixes and the wikipedia-induced list.</citsent>
<aftsection>
<nextsent>more specifically, the baseline feature set has (1) word unigrams, bi grams and trigrams; (2) pseudo-affix features that are created by taking the first three characters and the last three characters of the current word; and (3) binary feature that determines whether the current word is number.
</nextsent>
<nextsent>as far as our new features are concerned, we create one induced prefix feature and one induced suffix feature from both the current word and the previous word, as wellas two bigrams involving induced prefixes and induced suffixes.
</nextsent>
<nextsent>we also create three wiki features, including the wikipedia-induced ne tag of the current word and that of the previous word, as well as the combination of these two tags.
</nextsent>
<nextsent>note thatthe wikipedia-induced tag of word can be obtained by annotating the test sentence under consideration using the list generated from the bengali wikipedia (see section 4).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1786">
<title id=" E09-1041.xml">learning based named entity recognition for morphologically rich resource scarce languages </title>
<section> named entity recognition.  </section>
<citcontext>
<prevsection>
<prevsent>now, to train the ne recognizer, we create one training instance from each word in training text.
</prevsent>
<prevsent>the class value of an instance is the ne tag of the corresponding word, or others if the word is not part of an ne.
</prevsent>
</prevsection>
<citsent citstr=" I08-2077 ">
each instance is represented by set of linguistic features, as described next.features our feature set consists of (1) base line features motivated by those used in ekbal et al (2008) <papid> I08-2077 </papid>bengali ne recognizer, as well as (2) features derived from our induced affixes and the wikipedia-induced list.</citsent>
<aftsection>
<nextsent>more specifically, the baseline feature set has (1) word unigrams; (2) pseudo-affix features that are created by taking the first three characters and the last three characters of the current word; (3) binary feature that determines whether the current word is the first word of sentence; and (4) set of pos-related features,including the pos of the current word and its surrounding words, as well as pos bigrams formed from the current and surrounding words.
</nextsent>
<nextsent>our induced affixes and wiki features are incorporated into the baseline ne feature set in the same manner as in pos tagging.
</nextsent>
<nextsent>in essence, the feature tem 359 experiment p baseline 60.97 74.46 67.05 person 66.18 74.06 69.90 organization 29.81 44.93 35.84 location 52.62 80.40 63.61 baseline+induced affixes 60.45 73.30 66.26 person 65.70 72.61 69.02 organization 31.73 46.48 37.71 location 51.46 80.05 62.64 baseline+induced affixes+wiki 63.24 75.19 68.70 person 66.47 75.16 70.55 organization 30.77 43.84 36.16 location 60.06 79.69 68.50 table 6: 5-fold cross-validation results for ner plates employed by the ne recognizer are the top 12 templates in table 2 and those in table 5.
</nextsent>
<nextsent>learning algorithm we again use crf++ as our sequence learner for acquiring the recognizer.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1787">
<title id=" E09-1041.xml">learning based named entity recognition for morphologically rich resource scarce languages </title>
<section> a joint model for pos tagging and </section>
<citcontext>
<prevsection>
<prevsent>also, there is strong empirical support forthis argument: the ne recognizers, when given access to the correct pos tags, have f-scores ranging from 76-79%, which are 10% higher on average than those with pos tags that were automatically computed.
</prevsent>
<prevsent>consequently, we hypothesize that modeling pos tagging and ner jointly would yield better performance than learning the two tasks separately.
</prevsent>
</prevsection>
<citsent citstr=" N01-1006 ">
in fact, many approaches have been developed to jointly model pos tagging andnoun phrase chunking, including transformation based learning (ngai and florian, 2001), <papid> N01-1006 </papid>facto rial hmms (duh, 2005), <papid> P05-2004 </papid>and dynamic crfs (sutton et al , 2007).</citsent>
<aftsection>
<nextsent>some of these approaches are fairly sophisticated and also require intensive computations during inference.
</nextsent>
<nextsent>for instance, when jointly modeling pos tagging and chunking, sutton et al  (2007) reduce the number of pos tags from 45 to 5 when training facto rial dynamic crf on small dataset (with only 209 sentences) in order to reduce training and inference time.
</nextsent>
<nextsent>in contrast, we propose relatively simple model for jointly learning bengali pos tagging and ner, by exploiting the limited dependencies between the two tasks.
</nextsent>
<nextsent>specifically, we make the observation that most of the bengali words that are part of an ne are also proper nouns.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1788">
<title id=" E09-1041.xml">learning based named entity recognition for morphologically rich resource scarce languages </title>
<section> a joint model for pos tagging and </section>
<citcontext>
<prevsection>
<prevsent>also, there is strong empirical support forthis argument: the ne recognizers, when given access to the correct pos tags, have f-scores ranging from 76-79%, which are 10% higher on average than those with pos tags that were automatically computed.
</prevsent>
<prevsent>consequently, we hypothesize that modeling pos tagging and ner jointly would yield better performance than learning the two tasks separately.
</prevsent>
</prevsection>
<citsent citstr=" P05-2004 ">
in fact, many approaches have been developed to jointly model pos tagging andnoun phrase chunking, including transformation based learning (ngai and florian, 2001), <papid> N01-1006 </papid>facto rial hmms (duh, 2005), <papid> P05-2004 </papid>and dynamic crfs (sutton et al , 2007).</citsent>
<aftsection>
<nextsent>some of these approaches are fairly sophisticated and also require intensive computations during inference.
</nextsent>
<nextsent>for instance, when jointly modeling pos tagging and chunking, sutton et al  (2007) reduce the number of pos tags from 45 to 5 when training facto rial dynamic crf on small dataset (with only 209 sentences) in order to reduce training and inference time.
</nextsent>
<nextsent>in contrast, we propose relatively simple model for jointly learning bengali pos tagging and ner, by exploiting the limited dependencies between the two tasks.
</nextsent>
<nextsent>specifically, we make the observation that most of the bengali words that are part of an ne are also proper nouns.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1789">
<title id=" E12-1083.xml">structural and topical dimensions in multi task patent translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>ifwe consider domains as tasks, domain adaptation is special two-task case of multi-task learning.
</prevsent>
<prevsent>most previous work has concentrated on adapting unsupervised generative modules such as translation models or language models to new tasks.
</prevsent>
</prevsection>
<citsent citstr=" P07-1004 ">
for example, transductive approaches have used automatic translations of monolingual corpora for self-training modules of the generative smt pipeline (ueffing et al  2007; <papid> P07-1004 </papid>schwenk,2008; bertoldi and federico, 2009).<papid> W09-0432 </papid></citsent>
<aftsection>
<nextsent>other approaches have extracted parallel data from similar or comparable corpora (zhao et al  2004; <papid> C04-1059 </papid>snoveret al  2008).<papid> D08-1090 </papid></nextsent>
<nextsent>several approaches have been presented that train separate translation and language models on task-specific subsets of the data and combine them in different mixture models (fos ter and kuhn, 2007; <papid> W07-0717 </papid>koehn and schroeder, 2007; <papid> W07-0733 </papid>foster et al  2010).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1790">
<title id=" E12-1083.xml">structural and topical dimensions in multi task patent translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>ifwe consider domains as tasks, domain adaptation is special two-task case of multi-task learning.
</prevsent>
<prevsent>most previous work has concentrated on adapting unsupervised generative modules such as translation models or language models to new tasks.
</prevsent>
</prevsection>
<citsent citstr=" W09-0432 ">
for example, transductive approaches have used automatic translations of monolingual corpora for self-training modules of the generative smt pipeline (ueffing et al  2007; <papid> P07-1004 </papid>schwenk,2008; bertoldi and federico, 2009).<papid> W09-0432 </papid></citsent>
<aftsection>
<nextsent>other approaches have extracted parallel data from similar or comparable corpora (zhao et al  2004; <papid> C04-1059 </papid>snoveret al  2008).<papid> D08-1090 </papid></nextsent>
<nextsent>several approaches have been presented that train separate translation and language models on task-specific subsets of the data and combine them in different mixture models (fos ter and kuhn, 2007; <papid> W07-0717 </papid>koehn and schroeder, 2007; <papid> W07-0733 </papid>foster et al  2010).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1791">
<title id=" E12-1083.xml">structural and topical dimensions in multi task patent translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>most previous work has concentrated on adapting unsupervised generative modules such as translation models or language models to new tasks.
</prevsent>
<prevsent>for example, transductive approaches have used automatic translations of monolingual corpora for self-training modules of the generative smt pipeline (ueffing et al  2007; <papid> P07-1004 </papid>schwenk,2008; bertoldi and federico, 2009).<papid> W09-0432 </papid></prevsent>
</prevsection>
<citsent citstr=" C04-1059 ">
other approaches have extracted parallel data from similar or comparable corpora (zhao et al  2004; <papid> C04-1059 </papid>snoveret al  2008).<papid> D08-1090 </papid></citsent>
<aftsection>
<nextsent>several approaches have been presented that train separate translation and language models on task-specific subsets of the data and combine them in different mixture models (fos ter and kuhn, 2007; <papid> W07-0717 </papid>koehn and schroeder, 2007; <papid> W07-0733 </papid>foster et al  2010).</nextsent>
<nextsent>the latter kind of approach is applied in our work to multiple patent tasks.multi-task learning efforts in patent translation have so far been restricted to experimental combinations of translation and language models from different sets of ipc sections.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1792">
<title id=" E12-1083.xml">structural and topical dimensions in multi task patent translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>most previous work has concentrated on adapting unsupervised generative modules such as translation models or language models to new tasks.
</prevsent>
<prevsent>for example, transductive approaches have used automatic translations of monolingual corpora for self-training modules of the generative smt pipeline (ueffing et al  2007; <papid> P07-1004 </papid>schwenk,2008; bertoldi and federico, 2009).<papid> W09-0432 </papid></prevsent>
</prevsection>
<citsent citstr=" D08-1090 ">
other approaches have extracted parallel data from similar or comparable corpora (zhao et al  2004; <papid> C04-1059 </papid>snoveret al  2008).<papid> D08-1090 </papid></citsent>
<aftsection>
<nextsent>several approaches have been presented that train separate translation and language models on task-specific subsets of the data and combine them in different mixture models (fos ter and kuhn, 2007; <papid> W07-0717 </papid>koehn and schroeder, 2007; <papid> W07-0733 </papid>foster et al  2010).</nextsent>
<nextsent>the latter kind of approach is applied in our work to multiple patent tasks.multi-task learning efforts in patent translation have so far been restricted to experimental combinations of translation and language models from different sets of ipc sections.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1793">
<title id=" E12-1083.xml">structural and topical dimensions in multi task patent translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>for example, transductive approaches have used automatic translations of monolingual corpora for self-training modules of the generative smt pipeline (ueffing et al  2007; <papid> P07-1004 </papid>schwenk,2008; bertoldi and federico, 2009).<papid> W09-0432 </papid></prevsent>
<prevsent>other approaches have extracted parallel data from similar or comparable corpora (zhao et al  2004; <papid> C04-1059 </papid>snoveret al  2008).<papid> D08-1090 </papid></prevsent>
</prevsection>
<citsent citstr=" W07-0717 ">
several approaches have been presented that train separate translation and language models on task-specific subsets of the data and combine them in different mixture models (fos ter and kuhn, 2007; <papid> W07-0717 </papid>koehn and schroeder, 2007; <papid> W07-0733 </papid>foster et al  2010).</citsent>
<aftsection>
<nextsent>the latter kind of approach is applied in our work to multiple patent tasks.multi-task learning efforts in patent translation have so far been restricted to experimental combinations of translation and language models from different sets of ipc sections.
</nextsent>
<nextsent>forex ample, utiyama and isahara (2007) and tinsley et al (2010) investigate translation and language models trained on different sets of patent sections,with larger pools of parallel data improving results.
</nextsent>
<nextsent>ceausu et al (2011) find that language models always and translation model mostly benefit from larger pools of data from different sections.
</nextsent>
<nextsent>models trained on pooled patent data are used as baselines in our approach.the machine learning community has developed several different formalizations of the central idea of trading off optimality of parameter vectors for each task-specific model and closeness of these model parameters to the average parameter vector across models.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1794">
<title id=" E12-1083.xml">structural and topical dimensions in multi task patent translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>for example, transductive approaches have used automatic translations of monolingual corpora for self-training modules of the generative smt pipeline (ueffing et al  2007; <papid> P07-1004 </papid>schwenk,2008; bertoldi and federico, 2009).<papid> W09-0432 </papid></prevsent>
<prevsent>other approaches have extracted parallel data from similar or comparable corpora (zhao et al  2004; <papid> C04-1059 </papid>snoveret al  2008).<papid> D08-1090 </papid></prevsent>
</prevsection>
<citsent citstr=" W07-0733 ">
several approaches have been presented that train separate translation and language models on task-specific subsets of the data and combine them in different mixture models (fos ter and kuhn, 2007; <papid> W07-0717 </papid>koehn and schroeder, 2007; <papid> W07-0733 </papid>foster et al  2010).</citsent>
<aftsection>
<nextsent>the latter kind of approach is applied in our work to multiple patent tasks.multi-task learning efforts in patent translation have so far been restricted to experimental combinations of translation and language models from different sets of ipc sections.
</nextsent>
<nextsent>forex ample, utiyama and isahara (2007) and tinsley et al (2010) investigate translation and language models trained on different sets of patent sections,with larger pools of parallel data improving results.
</nextsent>
<nextsent>ceausu et al (2011) find that language models always and translation model mostly benefit from larger pools of data from different sections.
</nextsent>
<nextsent>models trained on pooled patent data are used as baselines in our approach.the machine learning community has developed several different formalizations of the central idea of trading off optimality of parameter vectors for each task-specific model and closeness of these model parameters to the average parameter vector across models.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1795">
<title id=" E12-1083.xml">structural and topical dimensions in multi task patent translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>models trained on pooled patent data are used as baselines in our approach.the machine learning community has developed several different formalizations of the central idea of trading off optimality of parameter vectors for each task-specific model and closeness of these model parameters to the average parameter vector across models.
</prevsent>
<prevsent>for example, starting from separate svm for each task, evgeniou and pontil (2004) present regularization method that trades off optimization of the task-specific parameter vectors and the distance of each svm tothe average svm.
</prevsent>
</prevsection>
<citsent citstr=" N09-1068 ">
equivalent formalizations replace parameter regularization by bayesian prior distributions on the parameters (finkel and manning, 2009) <papid> N09-1068 </papid>or by augmentation of the feature space with domain independent features (daume?,2007).</citsent>
<aftsection>
<nextsent>besides svms, several learning algorithms have been extended to the multi-task scenario in parameter regularization setting, e.g., perceptron-type algorithms (dredze et al  2010)or boosting (chapelle et al  2011).
</nextsent>
<nextsent>further variants include different formalizations of norms for parameter regularization, e.g., `1,2 regularization 819 (obozinski et al  2010) or `1,?
</nextsent>
<nextsent>regularization (quattoni et al  2009), where only the features that are most important across all tasks are kept inthe model.
</nextsent>
<nextsent>in our experiments, we apply parameter regularization for multi-task learning to minimum error rate training for patent translation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1796">
<title id=" E12-1083.xml">structural and topical dimensions in multi task patent translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>for the language pair german english we extracted total of 2,101,107 parallel titles, 291,716 parallel abstracts, and 735,667 parallel claims sections.
</prevsent>
<prevsent>the lack of directly translated descriptions poses serious limitation for patent translation, since this section constitutes the largest part of thedocument.
</prevsent>
</prevsection>
<citsent citstr=" C10-2010 ">
it is possible to obtain comparable descriptions from related patents that have been filed in different countries and are connected through the patent family id. we extracted 172,472 patents that were both filed with the uspto and the epo and contain an english and german description, respectively.for sentence alignment, we used the gargantua5 tool (braune and fraser, 2010) <papid> C10-2010 </papid>that filters sentence-length based alignment with ibmmodel-1 lexical word translation probabilities, estimated on parallel data obtained from the first 3http://www.ir-facility.org/ prototypes/marec 4a patent kind code indicates the document stage in the filing process, e.g., for applications and for granted patents, with publication levels from 1-9.</citsent>
<aftsection>
<nextsent>see http:// www.wipo.int/standards/en/part\_03.html.
</nextsent>
<nextsent>5http://gargantua.sourceforge.net pass alignment.
</nextsent>
<nextsent>this yields the parallel corpus listed in table 2 with high input-output ratios for claims, and much lower ratios for abstracts and descriptions, showing that claims exhibit natural parallelism due to their structure, while abstracts and descriptions are considerably less parallel.
</nextsent>
<nextsent>removing duplicates and adding parallel titles results in corpus of over 23 million parallel sentence pairs.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1797">
<title id=" E12-1083.xml">structural and topical dimensions in multi task patent translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>4 machine translation experiments.
</prevsent>
<prevsent>4.1 individual task baselines.
</prevsent>
</prevsection>
<citsent citstr=" P07-2045 ">
for our experiments we used the phrase-based, open-source smt toolkit moses6 (koehn et al  2007).<papid> P07-2045 </papid></citsent>
<aftsection>
<nextsent>for language modeling, we computed 5-gram models using irstlm7 (federico et al ., 2008) and queried the model with kenlm (heafield, 2011).<papid> W11-2123 </papid></nextsent>
<nextsent>bleu (papineni et al  2001)scores were computed up to 4-grams on lower cased data.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1798">
<title id=" E12-1083.xml">structural and topical dimensions in multi task patent translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>4.1 individual task baselines.
</prevsent>
<prevsent>for our experiments we used the phrase-based, open-source smt toolkit moses6 (koehn et al  2007).<papid> P07-2045 </papid></prevsent>
</prevsection>
<citsent citstr=" W11-2123 ">
for language modeling, we computed 5-gram models using irstlm7 (federico et al ., 2008) and queried the model with kenlm (heafield, 2011).<papid> W11-2123 </papid></citsent>
<aftsection>
<nextsent>bleu (papineni et al  2001)scores were computed up to 4-grams on lower cased data.
</nextsent>
<nextsent>europarl-v6 marec bleu oov bleu oov abstract 0.1726 14.40% 0.3721 3.00% claim 0.2301 15.80% 0.4711 4.20% title 0.0964 26.00% 0.3228 9.20%table 5: bleu scores and oov rate for europarl base line and marec model.
</nextsent>
<nextsent>table 5 shows first comparison of results ofmoses models trained on 500,000 parallel sentences from patent text sections balanced over ipc classes, against moses trained on 1.7 million sentences of parliament proceedings from europarl8 (koehn, 2005).
</nextsent>
<nextsent>the best result on each section is indicated in boldface.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1801">
<title id=" E12-1083.xml">structural and topical dimensions in multi task patent translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>9following duh et al (2010), we use the alignment model trained on the pooled dataset in the phrase extraction phase of the separate models.
</prevsent>
<prevsent>similarly, we use globally trained lexical reordering model.
</prevsent>
</prevsection>
<citsent citstr=" W05-0908 ">
10for assessing significance, we apply the approximate randomization method described in riezler and maxwell(2005).<papid> W05-0908 </papid></citsent>
<aftsection>
<nextsent>we consider pairwise differing results scoring pvalue smaller than 0.05 as significant; the assessment is repeated three times and the average value is taken.
</nextsent>
<nextsent>822 test train b d f h 0.5349 0.4475 0.5472 0.4746 0.4438 0.4523 0.4318 0.4109 0.4846 0.4736 0.5161 0.4847 0.4578 0.4734 0.4396 0.4248 0.5047 0.4257 0.5719 0.462 0.4134 0.4249 0.409 0.3845 0.47 0.4387 0.5106 0.5167 0.4344 0.4435 0.407 0.3917 0.4486 0.4458 0.4681 0.4531 0.4771 0.4591 0.4073 0.4028 0.4595 0.4588 0.4761 0.4655 0.4517 0.4909 0.422 0.4188 0.4935 0.4489 0.5239 0.4629 0.4414 0.4565 0.4748 0.4532 0.4628 0.4484 0.4914 0.4621 0.4421 0.4616 0.4588 0.4714 table 8: bleu scores for 300k individual ipc section models.
</nextsent>
<nextsent>a c e g a 0 0.1303 0.1317 0.1311 0.188 0.186 0.164 0.1906 0.1302 0 0.2388 0.1242 0.0974 0.0875 0.1417 0.1514 0.1317 0.2388 0 0.1992 0.311 0.3068 0.2506 0.2825 0.1311 0.1242 0.1992 0 0.1811 0.1808 0.1876 0.201 0.188 0.0974 0.311 0.1811 0 0.0921 0.2058 0.2025 0.186 0.0875 0.3068 0.1808 0.0921 0 0.1824 0.1743 0.164 0.1417 0.2506 0.1876 0.2056 0.1824 0 0.064 0.1906 0.1514 0.2825 0.201 0.2025 0.1743 0.064 0 table 9: pairwise a-distance for 300k ipc training sets.
</nextsent>
<nextsent>train test pooling mixture abstract-claim abstract 0.3703 0.3704 claim 0.4809 0.4834 claim-title claim 0.4799 0.4789 title 0.3269 0.328 title-abstract title 0.3311 0.3275 abstract 0.3643 0.366 table 10: mixture and pooling on text sections.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1802">
<title id=" E12-1083.xml">structural and topical dimensions in multi task patent translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in contrast to task pooling and task mixtures, the specific setting addressed by multi-task minimum error rate training is one in which the generative train test pooling mixture a-c 0.5271 0.5274 0.5664 0.5632 b-f 0.4696 0.4354 0.4859 0.4769 g-h 0.4735 0.4754 0.4634 0.467 table 11: mixture and pooling on ipc sections.
</prevsent>
<prevsent>smt pipeline is not adaptable.
</prevsent>
</prevsection>
<citsent citstr=" P03-1021 ">
such situations arise if there are not enough data to train translation models or language models on the new tasks.however, we assume that there are enough parallel data available to perform meta-parameter tuning by minimum error rate training (mert) (och, 2003; <papid> P03-1021 </papid>bertoldi et al  2009) for each task.</citsent>
<aftsection>
<nextsent>a generic algorithm for multi-task learning can be motivated as follows: multi-task learning aims to take advantage of commonalities shared among tasks by learning several independent but related tasks together.
</nextsent>
<nextsent>information is shared between tasks through joint representation and in 823 tuning test individual pooled average mmert mmert-average abstract 0.3721 0.362 0.3657?+ 0.3719+ 0.3685?+ claim 0.4711 0.4681 0.4749?+ 0.475?+ 0.4734?+ title 0.3228 0.3152 0.3326?+ 0.3268?+ 0.3325?+ table 12: multi-task tuning on text sections.
</nextsent>
<nextsent>tuning test individual pooled average mmert mmert-average 0.5187 0.5199 0.5213?+ 0.5195 0.5196 0.4877 0.4885 0.4908?+ 0.4911?+ 0.4921?+ 0.5214 0.5175 0.5199?+ 0.5218+ 0.5162?+ 0.4724 0.4730 0.4733 0.4736 0.4734 0.4666 0.4661 0.4679?+ 0.4669+ 0.4685?+ 0.4794 0.4801 0.4811?
</nextsent>
<nextsent>0.4821?+ 0.4830?+ 0.4596 0.4576 0.4607+ 0.4606+ 0.4610?+ 0.4573 0.4560 0.4578 0.4581+ 0.4581+ table 13: multi-task tuning on ipc sections.troduces an inductive bias.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1803">
<title id=" E12-1083.xml">structural and topical dimensions in multi task patent translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>w (t) avg[k]   0 then w(t)d [k] = min(w (t) avg[k], (t) [k] + ?) end if end for end for end for return w(t )1 , . . .
</prevsent>
<prevsent>, (t ) , (t ) avg figure 1: multi-task mert.
</prevsent>
</prevsection>
<citsent citstr=" P09-1054 ">
824 the weight updates and the clipping strategy can be motivated in framework of gradient descent optimization under `1-regularization (tsuruoka et al  2009).<papid> P09-1054 </papid></citsent>
<aftsection>
<nextsent>assuming mert as algorithmic minimizer11 of the loss function ld in equation 1, the weight update towards the average follows from the sub gradient of the `1 regular izer.
</nextsent>
<nextsent>since w(t)avg is taken as average over weights w(t1)d from the step before, the term (t)avg is constant with respect to w(t)d , leading to the following sub gradient (where sgn(x) = 1 if   0, sgn(x) = 1 if   0, and sgn(x) = 0 if = 0): ? w(t)r [k] ? d? d=1 ? ?
</nextsent>
<nextsent>w(t)d ? 1 d?
</nextsent>
<nextsent>s=1 w(t1)s ? ?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1805">
<title id=" E12-1083.xml">structural and topical dimensions in multi task patent translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the available features are very coarse and more likely to capture structural differences, such as sentence length, than the lexical differences that differentiate the semantic domains.
</prevsent>
<prevsent>we expect to see larger gains due to multi task learning for discriminatively trained smt models that involve very large numbers of features, especially when multi-task learning is donein framework that combines parameter regularization with feature selection (obozinski et al 2010).
</prevsent>
</prevsection>
<citsent citstr=" P06-1096 ">
in future work, we will explore combination of large-scale discriminative training (liang et al  2006) <papid> P06-1096 </papid>with multi-task learning for smt.</citsent>
<aftsection>
<nextsent>acknowledgments this work was supported in part by dfg grantcross-language learning-to-rank for patent re trieval?.
</nextsent>
<nextsent>826
</nextsent>


</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1806">
<title id=" E09-2009.xml">the software architecture for the first challenge on generating instructions in virtual environments </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this paper,we motivate and describe the software infrastructure that we developed to support this challenge.
</prevsent>
<prevsent>natural language generation (nlg) systems are notoriously hard to evaluate.
</prevsent>
</prevsection>
<citsent citstr=" P08-2050 ">
on the one hand,simply comparing system outputs to gold standard is not appropriate because there can be multiple generated outputs that are equally good, and finding metrics that account for this variability and produce results consistent with human judgments and task performance measures is difficult (belz and gatt, 2008; <papid> P08-2050 </papid>stent et al, 2005; foster, 2008).on the other hand, lab-based evaluations with human subjects to assess each aspect of the systems functionality are expensive and time-consuming.these characteristics make it hard to compare different systems and measure progress.give (generating instructions in virtual en vironments?)</citsent>
<aftsection>
<nextsent>(koller et al, 2007) is research challenge for the nlg community designed to provide new approach to nlg system evaluation.
</nextsent>
<nextsent>in the give scenario, users try to solve treasure hunt in virtual 3d world that they have not seen before.
</nextsent>
<nextsent>the computer has complete symbolic representation of the virtual environment.
</nextsent>
<nextsent>the challenge for the nlg system isto generate, in real time, natural-language instructions that will guide the users to the successful completion of their task (see fig.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1807">
<title id=" E06-1047.xml">parsing arabic dialects </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>we then proceed to discuss three ap proaches: sentence transduction, in which the lasentence to be parsed is turned into an msa sentence and then parsed with an msa parser (sec tion 5); treebank transduction, in which the msa treebank is turned into an la treebank (section 6); and grammar transduction, in which an msa grammar is turned into an la grammar which isthen used for parsing la (section 7).
</prevsent>
<prevsent>we summarize and discuss the results in section 8.
</prevsent>
</prevsection>
<citsent citstr=" W04-3207 ">
there has been fair amount of interest in parsing one language using another language, see forex ample (smith and smith, 2004; <papid> W04-3207 </papid>hwa et al, 2004)for recent work.</citsent>
<aftsection>
<nextsent>much of this work uses synchronized formalisms as do we in the grammar transduction approach.
</nextsent>
<nextsent>however, these approaches relyon parallel corpora.
</nextsent>
<nextsent>for msa and its dialects, there are no naturally occurring parallel corpora.
</nextsent>
<nextsent>it is this fact that has led us to investigate the use of explicit linguistic knowledge to complement machine learning.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1808">
<title id=" E06-1047.xml">parsing arabic dialects </title>
<section> sentence transduction.  </section>
<citcontext>
<prevsection>
<prevsent>the lattice is scored and decoded using the srilm toolkit with trigram language model trained on 54 million msa words from arabic gigaword (graff, 2003).
</prevsent>
<prevsent>the text used for language modeling was tokenized to match the tokenization of the arabic used in the atb and latb.
</prevsent>
</prevsection>
<citsent citstr=" N04-4038 ">
the tokenization was done using the asvm toolkit (diab et al, 2004).<papid> N04-4038 </papid></citsent>
<aftsection>
<nextsent>the 1-best path in the lattice is passed on to the bikel parser (bikel, 2002), which was trained on the msa training atb.
</nextsent>
<nextsent>finally, the terminal nodes in the resulting parse structure are replaced with the original la words.
</nextsent>
<nextsent>5.2 experimental results.
</nextsent>
<nextsent>table 1 describes the results of the sentence transduction path on the development corpus (dev) in different settings: using no pos tags in the input versus using gold pos tags in the input, and using slxun versus blxun.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1809">
<title id=" E06-1047.xml">parsing arabic dialects </title>
<section> grammar transduction.  </section>
<citcontext>
<prevsection>
<prevsent>moreover, because parsed msa translation is produced as byproduct, we can also think of this approach as being related to the sentence-transduction approach.
</prevsent>
<prevsent>7.1 preliminaries.
</prevsent>
</prevsection>
<citsent citstr=" P00-1058 ">
the parsing model used is essentially that of chiang (chiang, 2000), <papid> P00-1058 </papid>which is based on highly restricted version of tree-adjoining grammar.</citsent>
<aftsection>
<nextsent>in its present form, the formalism is tree-substitutiongrammar (schabes, 1990) with an additional operation called sister-adjunction (rambow et al,2001).<papid> J01-1004 </papid></nextsent>
<nextsent>because of space constraints, we omit discussion of the sister-adjunction operation in this paper.a tree-substitution grammar is set of elementary trees.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1810">
<title id=" E06-1047.xml">parsing arabic dialects </title>
<section> grammar transduction.  </section>
<citcontext>
<prevsection>
<prevsent>7.1 preliminaries.
</prevsent>
<prevsent>the parsing model used is essentially that of chiang (chiang, 2000), <papid> P00-1058 </papid>which is based on highly restricted version of tree-adjoining grammar.</prevsent>
</prevsection>
<citsent citstr=" J01-1004 ">
in its present form, the formalism is tree-substitutiongrammar (schabes, 1990) with an additional operation called sister-adjunction (rambow et al,2001).<papid> J01-1004 </papid></citsent>
<aftsection>
<nextsent>because of space constraints, we omit discussion of the sister-adjunction operation in this paper.a tree-substitution grammar is set of elementary trees.
</nextsent>
<nextsent>a frontier node labeled with nonterminal label is called substitution site.
</nextsent>
<nextsent>if an elementary tree has exactly one terminal symbol, that symbol is called its lexical anchor.
</nextsent>
<nextsent>a derivation starts with an elementary tree and proceeds by series of composition operations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1812">
<title id=" E06-1047.xml">parsing arabic dialects </title>
<section> grammar transduction.  </section>
<citcontext>
<prevsection>
<prevsent>7.2 an msa-dialect synchronous grammar.
</prevsent>
<prevsent>we now describe how we build our msa-dialect synchronous grammar.
</prevsent>
</prevsection>
<citsent citstr=" W00-1307 ">
as mentioned above, the msa side of the grammar is extracted from the atb in process described by chiang and others (chiang, 2000; <papid> P00-1058 </papid>xia et al, 2000; <papid> W00-1307 </papid>chen, 2001).</citsent>
<aftsection>
<nextsent>this process also gives us msa-only substitution probabilities (?
</nextsent>
<nextsent>| ?).we then apply various transformation rules (de scribed below) to the msa elementary trees to produce dialect grammar, at the same time assigning probabilities (??
</nextsent>
<nextsent>the synchronous substitution probabilities can then be estimated as: (?,??
</nextsent>
<nextsent>p (?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1813">
<title id=" E09-1072.xml">empirical evaluations of animacy annotation </title>
<section> human reference in swedish.  </section>
<citcontext>
<prevsection>
<prevsent>we may thus distinguish between purely type level annotation strategy and purely token level one.
</prevsent>
<prevsent>type level properties hold for lexemes and are context-independent, i.e., independent of the particular linguistic context, whereas token-level properties are determined in context and hold for referring expressions, rather than lex emes.
</prevsent>
</prevsection>
<citsent citstr=" W06-2933 ">
talbanken05 is swedish treebank which was created in the 1970s and which has recently been converted to dependency format (nivre etal., 2006<papid> W06-2933 </papid>b) and made freely available.</citsent>
<aftsection>
<nextsent>the written sections of the treebank consist of professional prose and student essays and amount to197,123 running tokens, spread over 11,431 sentences.
</nextsent>
<nextsent>figure 2 shows the labeled dependency graph of example (2), taken from talbanken05.
</nextsent>
<nextsent>(2) samma same erfarenhet experience gjorde made engelsmannen englishmen-def the same experience, the englishmen had?
</nextsent>
<nextsent>_ _ _ samma po kp erfarenhet nn _ gjorde vv pt engelsmannen nn dd|hh rootdt oo ss figure 2: dependency representation of example (2) from talbanken05.in addition to information on part-of-speech, dependency head and relation, and various morphosyntactic properties such as definite ness, the annotation expresses distinction for nominal elements between reference to human and nonhuman.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1815">
<title id=" E09-1072.xml">empirical evaluations of animacy annotation </title>
<section> human reference in swedish.  </section>
<citcontext>
<prevsection>
<prevsent>the annotation manual (teleman, 1974) states that mark able should be tagged as human(hh) if it may be replaced by the interrogative pronoun vem who?
</prevsent>
<prevsent>and be referred to by the personal pronouns han he?
</prevsent>
</prevsection>
<citsent citstr=" W04-0216 ">
or hon she?.there are clear similarities between the annotation for human reference found in talbanken05 and the annotation scheme for animacy discussed 631 anim conc nconc time place org hum inanimateotheranimate figure 1: animacy classification scheme (zaenen et al, 2004).<papid> W04-0216 </papid></citsent>
<aftsection>
<nextsent>above.
</nextsent>
<nextsent>the human/non-human contrast forms the central distinction in the animacy dimension and,in this respect, the annotation schemes do not conflict.
</nextsent>
<nextsent>if we compare the annotation found intal banken05 with the annotation proposed in zaenenet.
</nextsent>
<nextsent>al.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1817">
<title id=" E09-1072.xml">empirical evaluations of animacy annotation </title>
<section> lexical acquisition of animacy.  </section>
<citcontext>
<prevsection>
<prevsent>class.
</prevsent>
<prevsent>how ever, whereas most ner systems make extensive use of orthographic, morphological or contextual clues (titles, suffixes) and gazette ers, animacy for nouns is not signaled overtly in the same way.
</prevsent>
</prevsection>
<citsent citstr=" J01-3003 ">
following strategy in line with work on verb classification (merlo and stevenson, 2001;<papid> J01-3003 </papid>stevenson and joanis, 2003), <papid> W03-0410 </papid>we set out to classify common noun lemmas based on their morphosyntactic distribution in considerably larger corpus.</citsent>
<aftsection>
<nextsent>this is thus equivalent to treatment of animacy as lexical semantic property and the classification strategy is based on generalization of morphosyntactic behaviour of common nouns over large quantities of data.
</nextsent>
<nextsent>due to the small size of the talbanken05 treebank and the small amount of variation, this strategy was pursued for the acquisition of animacy information.
</nextsent>
<nextsent>in the animacy classification of common nouns we exploit well-documented correlations between morphosyntactic realization and semantic properties of nouns.
</nextsent>
<nextsent>for instance, animate nouns tend to be realized as agentive subjects, inanimate nouns do not (dahl and fraurud, 1996).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1818">
<title id=" E09-1072.xml">empirical evaluations of animacy annotation </title>
<section> lexical acquisition of animacy.  </section>
<citcontext>
<prevsection>
<prevsent>class.
</prevsent>
<prevsent>how ever, whereas most ner systems make extensive use of orthographic, morphological or contextual clues (titles, suffixes) and gazette ers, animacy for nouns is not signaled overtly in the same way.
</prevsent>
</prevsection>
<citsent citstr=" W03-0410 ">
following strategy in line with work on verb classification (merlo and stevenson, 2001;<papid> J01-3003 </papid>stevenson and joanis, 2003), <papid> W03-0410 </papid>we set out to classify common noun lemmas based on their morphosyntactic distribution in considerably larger corpus.</citsent>
<aftsection>
<nextsent>this is thus equivalent to treatment of animacy as lexical semantic property and the classification strategy is based on generalization of morphosyntactic behaviour of common nouns over large quantities of data.
</nextsent>
<nextsent>due to the small size of the talbanken05 treebank and the small amount of variation, this strategy was pursued for the acquisition of animacy information.
</nextsent>
<nextsent>in the animacy classification of common nouns we exploit well-documented correlations between morphosyntactic realization and semantic properties of nouns.
</nextsent>
<nextsent>for instance, animate nouns tend to be realized as agentive subjects, inanimate nouns do not (dahl and fraurud, 1996).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1823">
<title id=" E12-1060.xml">word sense induction for novel sense detection </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>word sense induction (wsi) is the task of automatically inducing the different senses of given word, generally in the form of an unsupervised learning task with senses represented as clusters of token instances.
</prevsent>
<prevsent>it contrasts with word sense disambiguation (wsd), where fixed sense inventory is assumed to exist, and token instances of given word are disambiguated relative to the sense inventory.
</prevsent>
</prevsection>
<citsent citstr=" D10-1012 ">
while wsi is intuitively appealing as task, there have been no real examples ofwsi being successfully deployed in end-user applications, other than work by schutze (1998) and navigli and crisafulli (2010) <papid> D10-1012 </papid>in an information retrieval context.</citsent>
<aftsection>
<nextsent>a key contribution of this paperis the successful application of wsi to the lexico graphical task of novel sense detection, i.e. identifying words which have taken on new senses over time.
</nextsent>
<nextsent>one of the key challenges in wsi is learning the appropriate sense granularity forgiven word, i.e. the number of senses that best captures the token occurrences of that word.
</nextsent>
<nextsent>building on the work of brody and lapata (2009) <papid> E09-1013 </papid>and others, we approach wsi via topic modelling ? using latent dirichlet allocation (lda: blei et al (2003)) and derivative approaches ? and use the topic model to determine the appropriate sense granularity.</nextsent>
<nextsent>topic modelling is an unsupervised approach to jointly learn topics ? in the form of multinomial probability distributions over words ? and per-document topic assignments ? in the form of multinomial probability distributions overtopics.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1824">
<title id=" E12-1060.xml">word sense induction for novel sense detection </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a key contribution of this paperis the successful application of wsi to the lexico graphical task of novel sense detection, i.e. identifying words which have taken on new senses over time.
</prevsent>
<prevsent>one of the key challenges in wsi is learning the appropriate sense granularity forgiven word, i.e. the number of senses that best captures the token occurrences of that word.
</prevsent>
</prevsection>
<citsent citstr=" E09-1013 ">
building on the work of brody and lapata (2009) <papid> E09-1013 </papid>and others, we approach wsi via topic modelling ? using latent dirichlet allocation (lda: blei et al (2003)) and derivative approaches ? and use the topic model to determine the appropriate sense granularity.</citsent>
<aftsection>
<nextsent>topic modelling is an unsupervised approach to jointly learn topics ? in the form of multinomial probability distributions over words ? and per-document topic assignments ? in the form of multinomial probability distributions overtopics.
</nextsent>
<nextsent>lda is appealing for wsi as it both assigns senses to words (in the form of topic alloca tion), and outputs representation of each senseas weighted list of words.
</nextsent>
<nextsent>lda offers solution to the question of sense granularity determination via non-parametric formulations, such as hierarchical dirichlet process (hdp: teh et al  (2006), yao and durme (2011)).<papid> W11-1102 </papid></nextsent>
<nextsent>our contributions in this paper are as follows.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1827">
<title id=" E12-1060.xml">word sense induction for novel sense detection </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>topic modelling is an unsupervised approach to jointly learn topics ? in the form of multinomial probability distributions over words ? and per-document topic assignments ? in the form of multinomial probability distributions overtopics.
</prevsent>
<prevsent>lda is appealing for wsi as it both assigns senses to words (in the form of topic alloca tion), and outputs representation of each senseas weighted list of words.
</prevsent>
</prevsection>
<citsent citstr=" W11-1102 ">
lda offers solution to the question of sense granularity determination via non-parametric formulations, such as hierarchical dirichlet process (hdp: teh et al  (2006), yao and durme (2011)).<papid> W11-1102 </papid></citsent>
<aftsection>
<nextsent>our contributions in this paper are as follows.
</nextsent>
<nextsent>we first establish the effectiveness of hdp forwsi over both the semeval-2007 and semeval2010wsi datasets (agirre and soroa, 2007; <papid> W07-2002 </papid>manandhar et al  2010), <papid> S10-1011 </papid>and show that the non parametric formulation is superior to standard lda formulation with oracle determination of sense granularity forgiven word.</nextsent>
<nextsent>we next demonstrate that our interpretation of hdp-basedwsi is superior to other topic model-based approaches to wsi, and indeed, better than the best published results for both semeval datasets.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1828">
<title id=" E12-1060.xml">word sense induction for novel sense detection </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>lda offers solution to the question of sense granularity determination via non-parametric formulations, such as hierarchical dirichlet process (hdp: teh et al  (2006), yao and durme (2011)).<papid> W11-1102 </papid></prevsent>
<prevsent>our contributions in this paper are as follows.</prevsent>
</prevsection>
<citsent citstr=" W07-2002 ">
we first establish the effectiveness of hdp forwsi over both the semeval-2007 and semeval2010wsi datasets (agirre and soroa, 2007; <papid> W07-2002 </papid>manandhar et al  2010), <papid> S10-1011 </papid>and show that the non parametric formulation is superior to standard lda formulation with oracle determination of sense granularity forgiven word.</citsent>
<aftsection>
<nextsent>we next demonstrate that our interpretation of hdp-basedwsi is superior to other topic model-based approaches to wsi, and indeed, better than the best published results for both semeval datasets.
</nextsent>
<nextsent>finally, we apply our method to the novel sense detection task based on dataset developed in this research, and achieve highly encouraging results.
</nextsent>
<nextsent>in topic modelling, documents are assumed to exhibit multiple topics, with each document having 591its own distribution over topics.
</nextsent>
<nextsent>words are generated in each document by first sampling topic from the documents topic distribution, then sampling word from that topic.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1829">
<title id=" E12-1060.xml">word sense induction for novel sense detection </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>lda offers solution to the question of sense granularity determination via non-parametric formulations, such as hierarchical dirichlet process (hdp: teh et al  (2006), yao and durme (2011)).<papid> W11-1102 </papid></prevsent>
<prevsent>our contributions in this paper are as follows.</prevsent>
</prevsection>
<citsent citstr=" S10-1011 ">
we first establish the effectiveness of hdp forwsi over both the semeval-2007 and semeval2010wsi datasets (agirre and soroa, 2007; <papid> W07-2002 </papid>manandhar et al  2010), <papid> S10-1011 </papid>and show that the non parametric formulation is superior to standard lda formulation with oracle determination of sense granularity forgiven word.</citsent>
<aftsection>
<nextsent>we next demonstrate that our interpretation of hdp-basedwsi is superior to other topic model-based approaches to wsi, and indeed, better than the best published results for both semeval datasets.
</nextsent>
<nextsent>finally, we apply our method to the novel sense detection task based on dataset developed in this research, and achieve highly encouraging results.
</nextsent>
<nextsent>in topic modelling, documents are assumed to exhibit multiple topics, with each document having 591its own distribution over topics.
</nextsent>
<nextsent>words are generated in each document by first sampling topic from the documents topic distribution, then sampling word from that topic.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1837">
<title id=" E12-1060.xml">word sense induction for novel sense detection </title>
<section> hdp+position+dependency: hdp with.  </section>
<citcontext>
<prevsection>
<prevsent>learns ), with extra positional word features.
</prevsent>
<prevsent>both positional word and dependency features.
</prevsent>
</prevsection>
<citsent citstr=" S10-1079 ">
we compare our models with two baselines from the semeval-2010 task: (1) baseline random ? randomly assign each test instance to oneof four senses; (2) baseline mfs ? most frequent sense baseline, assigning all test instances to one sense; and also benchmark system(uoy), in the form of the university of york system (korkontzelos and manandhar, 2010), <papid> S10-1079 </papid>which achieved the best overall wsd results in the original semeval-2010 task.</citsent>
<aftsection>
<nextsent>3.2 semeval-2010 results.
</nextsent>
<nextsent>the results of our experiments over the semeval 2010 dataset are summarised in table 1.
</nextsent>
<nextsent>593 system wsd (80%/20%)all verbs nouns baselines baseline random 0.57 0.66 0.51 baseline mfs 0.59 0.67 0.53 lda variable 0.64 0.69 0.60 fixed 0.63 0.68 0.59 fixed +position 0.63 0.68 0.60 hdp +position 0.68 0.72 0.65 +position+dependency 0.68 0.72 0.65 benchmark uoy 0.62 0.67 0.59 table 1: wsd f-score over the semeval-2010 dataset looking first at the results for lda, we see that the first lda approach (variable ) is very competitive, outperforming the benchmark system.
</nextsent>
<nextsent>in this approach, however, we assume perfect knowledge of the number of gold senses of each target word, meaning that the method isnt truly unsupervised.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1849">
<title id=" E12-1060.xml">word sense induction for novel sense detection </title>
<section> hdp+position+dependency: hdp with.  </section>
<citcontext>
<prevsection>
<prevsent>for the purposes of model comparison under identical training settings, however, it is appropriate to report on results for only the bnc.
</prevsent>
<prevsent>we experiment with both our original method(with both positional word and dependency features, and default parameter settings for hdp) without any parameter tuning, and the same method with the tuned parameter settings ofyvd, for direct comparability.
</prevsent>
</prevsection>
<citsent citstr=" W07-2037 ">
we present the results in table 3, including the results for the best performing system in the original semeval-2007 task (i2r: niu et al (2007)).<papid> W07-2037 </papid>the results are enlightening: with default parameter settings, our methodology is slightly be low the results of the other three models.</citsent>
<aftsection>
<nextsent>bear 8in creating the training dataset, each instance is made up of the sentence the target word occurs in, as we as one sentence to either side of that sentence, i.e. 3 sentences in total per instance.
</nextsent>
<nextsent>595in mind, however, that the two topic modelling based approaches were tuned extensively to thedataset.
</nextsent>
<nextsent>when we use the tuned hyperparameter settings of yvd, our results rise around 2.5% to surpass both topic modelling approaches, and marginally outperform the i2r system from the original task.
</nextsent>
<nextsent>recall that both bland yvd report higher results again using in-domain training data, so we would expect to see further gains again over the i2r system in following this path.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1850">
<title id=" E12-1060.xml">word sense induction for novel sense detection </title>
<section> hdp+position+dependency: hdp with.  </section>
<citcontext>
<prevsection>
<prevsent>these senses of swag and tweet are not included in many dictionaries or computational lexicons ? e.g., neither of these senses is listed in wordnet3.0 (fellbaum, 1998) ? yet appear to be in regular usage, particularly in text related to pop culture and online media.the manual identification of such new word senses is challenge in lexicography over and above identifying new words themselves, and is essential to keeping dictionaries up-to-date.moreover, lexicons that better reflect contemporary usage could benefit nlp applications that use sense inventories.
</prevsent>
<prevsent>the challenge of identifying changes in word sense has only recently been considered in computational linguistics.
</prevsent>
</prevsection>
<citsent citstr=" W09-0214 ">
for example, sagi et al (2009), <papid> W09-0214 </papid>cook and stevenson (2010), and gulordava and baroni (2011) <papid> W11-2508 </papid>propose type-based models of semantic change.</citsent>
<aftsection>
<nextsent>such models do not account for polysemy, and appear best-suited to identifying changes in predominant sense.
</nextsent>
<nextsent>bam man and crane (2011) use parallel latin?
</nextsent>
<nextsent>english corpus to induce word senses and build wsd system, which they then apply to study dia chronic variation in word senses.
</nextsent>
<nextsent>crucially, inthis token-based approach there is clear connection between word senses and tokens, making it possible to identify usages of specific sense.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1851">
<title id=" E12-1060.xml">word sense induction for novel sense detection </title>
<section> hdp+position+dependency: hdp with.  </section>
<citcontext>
<prevsection>
<prevsent>these senses of swag and tweet are not included in many dictionaries or computational lexicons ? e.g., neither of these senses is listed in wordnet3.0 (fellbaum, 1998) ? yet appear to be in regular usage, particularly in text related to pop culture and online media.the manual identification of such new word senses is challenge in lexicography over and above identifying new words themselves, and is essential to keeping dictionaries up-to-date.moreover, lexicons that better reflect contemporary usage could benefit nlp applications that use sense inventories.
</prevsent>
<prevsent>the challenge of identifying changes in word sense has only recently been considered in computational linguistics.
</prevsent>
</prevsection>
<citsent citstr=" W11-2508 ">
for example, sagi et al (2009), <papid> W09-0214 </papid>cook and stevenson (2010), and gulordava and baroni (2011) <papid> W11-2508 </papid>propose type-based models of semantic change.</citsent>
<aftsection>
<nextsent>such models do not account for polysemy, and appear best-suited to identifying changes in predominant sense.
</nextsent>
<nextsent>bam man and crane (2011) use parallel latin?
</nextsent>
<nextsent>english corpus to induce word senses and build wsd system, which they then apply to study dia chronic variation in word senses.
</nextsent>
<nextsent>crucially, inthis token-based approach there is clear connection between word senses and tokens, making it possible to identify usages of specific sense.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1853">
<title id=" E09-1011.xml">syntactic phrase reordering for englishtoarabic statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>obviously, the same reordering has to be applied to both training data and test data.
</prevsent>
<prevsent>despite the added complexity of parsing the data, this technique has shown improvements,especially when good parses of the source side exist.
</prevsent>
</prevsection>
<citsent citstr=" P05-1066 ">
it has been successfully applied to german-to english and chinese-to-english smt (collins et al., 2005; <papid> P05-1066 </papid>wang et al, 2007).<papid> D07-1077 </papid></citsent>
<aftsection>
<nextsent>in this paper, we propose the use of similar approach for english-to-arabic smt.
</nextsent>
<nextsent>unlike most other work on arabic translation, our work is inthe direction of the more morphologically complex language, which poses unique challenges.
</nextsent>
<nextsent>we propose set of syntactic reordering rules on the english source to align it better to the arabic target.
</nextsent>
<nextsent>the reordering rules exploit systematic differences between the syntax of arabic and the syntax of english; they specifically address two syntactic constructs.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1854">
<title id=" E09-1011.xml">syntactic phrase reordering for englishtoarabic statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>obviously, the same reordering has to be applied to both training data and test data.
</prevsent>
<prevsent>despite the added complexity of parsing the data, this technique has shown improvements,especially when good parses of the source side exist.
</prevsent>
</prevsection>
<citsent citstr=" D07-1077 ">
it has been successfully applied to german-to english and chinese-to-english smt (collins et al., 2005; <papid> P05-1066 </papid>wang et al, 2007).<papid> D07-1077 </papid></citsent>
<aftsection>
<nextsent>in this paper, we propose the use of similar approach for english-to-arabic smt.
</nextsent>
<nextsent>unlike most other work on arabic translation, our work is inthe direction of the more morphologically complex language, which poses unique challenges.
</nextsent>
<nextsent>we propose set of syntactic reordering rules on the english source to align it better to the arabic target.
</nextsent>
<nextsent>the reordering rules exploit systematic differences between the syntax of arabic and the syntax of english; they specifically address two syntactic constructs.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1855">
<title id=" E09-1011.xml">syntactic phrase reordering for englishtoarabic statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we show that using the reordering rules results in gains in the translation scores and study the effect of the training data size on those gains.
</prevsent>
<prevsent>this paper also investigates the effect of using morphological segmentation of the arabic target 86in combination with the reordering rules.
</prevsent>
</prevsection>
<citsent citstr=" N06-2013 ">
morphological segmentation has been shown to benefit arabic-to-english (habash and sadat, 2006) <papid> N06-2013 </papid>and english-to-arabic (badr et al, 2008) <papid> P08-2039 </papid>translation,although the gains tend to decrease with increasing training data size.</citsent>
<aftsection>
<nextsent>section 2 provides linguistic motivation for the paper.
</nextsent>
<nextsent>it describes the rich morphology of arabic, and its implications on smt.
</nextsent>
<nextsent>it also describes the syntax of the verb phrase and noun phrase in arabic, and how they differ from their english counterparts.
</nextsent>
<nextsent>in section 3, we describe some of the relevant previous work.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1857">
<title id=" E09-1011.xml">syntactic phrase reordering for englishtoarabic statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we show that using the reordering rules results in gains in the translation scores and study the effect of the training data size on those gains.
</prevsent>
<prevsent>this paper also investigates the effect of using morphological segmentation of the arabic target 86in combination with the reordering rules.
</prevsent>
</prevsection>
<citsent citstr=" P08-2039 ">
morphological segmentation has been shown to benefit arabic-to-english (habash and sadat, 2006) <papid> N06-2013 </papid>and english-to-arabic (badr et al, 2008) <papid> P08-2039 </papid>translation,although the gains tend to decrease with increasing training data size.</citsent>
<aftsection>
<nextsent>section 2 provides linguistic motivation for the paper.
</nextsent>
<nextsent>it describes the rich morphology of arabic, and its implications on smt.
</nextsent>
<nextsent>it also describes the syntax of the verb phrase and noun phrase in arabic, and how they differ from their english counterparts.
</nextsent>
<nextsent>in section 3, we describe some of the relevant previous work.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1858">
<title id=" E09-1011.xml">syntactic phrase reordering for englishtoarabic statistical machine translation </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>also, since arabic is morphologically richer language, translating into arabic poses unique issues that are not present in the opposite direction.
</prevsent>
<prevsent>the only works on english to-arabic smt that we are aware of are badr et al.
</prevsent>
</prevsection>
<citsent citstr=" N07-2037 ">
(2008), and sarikaya and deng (2007).<papid> N07-2037 </papid></citsent>
<aftsection>
<nextsent>badret al show that using segmentation and recombination as pre- and post-processing steps leadsto significant gains especially for smaller training data corpora.
</nextsent>
<nextsent>sarikaya and deng use jointmorphological-lexical language models to rerank the output of an english-to-arabic mt system.
</nextsent>
<nextsent>they use regular expression-based segmentation of the arabic so as not to run into recombination issues on the output side.
</nextsent>
<nextsent>similarly, for arabic-to-english, lee (2004),<papid> N04-4015 </papid>and habash and sadat (2006) <papid> N06-2013 </papid>show that various segmentation schemes lead to improvements that decrease with increasing parallel corpus size.they use trigram language model and the arabic morphological analyzer mada (habash andrambow, 2005) <papid> P05-1071 </papid>respectively, to segment the arabic side of their corpora.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1859">
<title id=" E09-1011.xml">syntactic phrase reordering for englishtoarabic statistical machine translation </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>sarikaya and deng use jointmorphological-lexical language models to rerank the output of an english-to-arabic mt system.
</prevsent>
<prevsent>they use regular expression-based segmentation of the arabic so as not to run into recombination issues on the output side.
</prevsent>
</prevsection>
<citsent citstr=" N04-4015 ">
similarly, for arabic-to-english, lee (2004),<papid> N04-4015 </papid>and habash and sadat (2006) <papid> N06-2013 </papid>show that various segmentation schemes lead to improvements that decrease with increasing parallel corpus size.they use trigram language model and the arabic morphological analyzer mada (habash andrambow, 2005) <papid> P05-1071 </papid>respectively, to segment the arabic side of their corpora.</citsent>
<aftsection>
<nextsent>other work on arabicto-english smt tries to address the word reordering problem.
</nextsent>
<nextsent>habash (2007) automatically learns syntactic reordering rules that are then applied to the arabic side of the parallel corpora.
</nextsent>
<nextsent>the words are aligned in sentence pair, then the arabic sentence is parsed to extract reordering rules based on how the constituents in the parse tree are reordered on the english side.
</nextsent>
<nextsent>no significant improvement is 88shown with reordering when compared to base line that uses non-lexicalized distance reordering model.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1861">
<title id=" E09-1011.xml">syntactic phrase reordering for englishtoarabic statistical machine translation </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>sarikaya and deng use jointmorphological-lexical language models to rerank the output of an english-to-arabic mt system.
</prevsent>
<prevsent>they use regular expression-based segmentation of the arabic so as not to run into recombination issues on the output side.
</prevsent>
</prevsection>
<citsent citstr=" P05-1071 ">
similarly, for arabic-to-english, lee (2004),<papid> N04-4015 </papid>and habash and sadat (2006) <papid> N06-2013 </papid>show that various segmentation schemes lead to improvements that decrease with increasing parallel corpus size.they use trigram language model and the arabic morphological analyzer mada (habash andrambow, 2005) <papid> P05-1071 </papid>respectively, to segment the arabic side of their corpora.</citsent>
<aftsection>
<nextsent>other work on arabicto-english smt tries to address the word reordering problem.
</nextsent>
<nextsent>habash (2007) automatically learns syntactic reordering rules that are then applied to the arabic side of the parallel corpora.
</nextsent>
<nextsent>the words are aligned in sentence pair, then the arabic sentence is parsed to extract reordering rules based on how the constituents in the parse tree are reordered on the english side.
</nextsent>
<nextsent>no significant improvement is 88shown with reordering when compared to base line that uses non-lexicalized distance reordering model.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1864">
<title id=" E09-1011.xml">syntactic phrase reordering for englishtoarabic statistical machine translation </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>popovic and ney (2006) use similar methods to reorder german by looking at thepos tags for german-to-english and german-to spanish.
</prevsent>
<prevsent>they show significant improvements on test set sentences that do get reordered as wellas those that dont, which is attributed to the improvement of the extracted phrases.
</prevsent>
</prevsection>
<citsent citstr=" C04-1073 ">
(xia and mccord, 2004) <papid> C04-1073 </papid>present similar approach, witha notable difference: the re-ordering rules are automatically learned from aligning parse trees for both the source and target sentences.</citsent>
<aftsection>
<nextsent>they reporta 10% relative gain for english-to-french translation.
</nextsent>
<nextsent>although target-side parsing is optional in this approach, it is needed to take full advantage of the approach.
</nextsent>
<nextsent>this is bigger issue whenno reliable parses are available for the target language, as is the case in this paper.
</nextsent>
<nextsent>more generally,the use of automatically-learned rules has the advantage of readily applicable to different language pairs.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1865">
<title id=" E09-1011.xml">syntactic phrase reordering for englishtoarabic statistical machine translation </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>the use of deterministic, pre-defined rules, however, has the advantage of being linguistically motivated, since differences between the two languages are addressed explicitly.
</prevsent>
<prevsent>moreover, the implementation of pre-defined transfer rules based on target-side parses is relatively easy and cheap to implement in different language pairs.generic approaches for translating from english to more morphologically complex languages have been proposed.
</prevsent>
</prevsection>
<citsent citstr=" D07-1091 ">
koehn and hoang (2007)<papid> D07-1091 </papid>propose factored translation models, which extend phrase-based statistical machine translation by allowing the integration of additional morphological features at the word level.</citsent>
<aftsection>
<nextsent>they demonstrate improvements for english-to-german andenglish-to-czech.
</nextsent>
<nextsent>tighter integration of features is claimed to allow for better modeling of the morphology and hence is better than using pre-processing and post-processing techniques.
</nextsent>
<nextsent>avramidis and koehn (2008) <papid> P08-1087 </papid>enrich the english side by adding feature to the factored model that models noun case agreement and verb person conjugation, and show that it leads to more grammatically correct output for english-to-greek and english-to-czech translation.</nextsent>
<nextsent>although factored models are well equipped for handling languages that differ in terms of morphology, they still usethe same distortion reordering model as phrase based mt system.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1866">
<title id=" E09-1011.xml">syntactic phrase reordering for englishtoarabic statistical machine translation </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>they demonstrate improvements for english-to-german andenglish-to-czech.
</prevsent>
<prevsent>tighter integration of features is claimed to allow for better modeling of the morphology and hence is better than using pre-processing and post-processing techniques.
</prevsent>
</prevsection>
<citsent citstr=" P08-1087 ">
avramidis and koehn (2008) <papid> P08-1087 </papid>enrich the english side by adding feature to the factored model that models noun case agreement and verb person conjugation, and show that it leads to more grammatically correct output for english-to-greek and english-to-czech translation.</citsent>
<aftsection>
<nextsent>although factored models are well equipped for handling languages that differ in terms of morphology, they still usethe same distortion reordering model as phrase based mt system.
</nextsent>
<nextsent>4.1 arabic segmentation and recombination.
</nextsent>
<nextsent>it has been shown previously work (badr et al,2008; <papid> P08-2039 </papid>habash and sadat, 2006) <papid> N06-2013 </papid>that morphological segmentation of arabic improves the translation performance for both arabic-to-english and english-to-arabic by addressing the problem of sparsity of the arabic side.</nextsent>
<nextsent>in this paper, we use segmented and non-segmented arabic on the target side, and study the effect of the combination of segmentation with reordering.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1874">
<title id=" E09-1011.xml">syntactic phrase reordering for englishtoarabic statistical machine translation </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>so, the real value of the egyptian pound ? value the egyptian the pound the real the vp reordering rule is independent.
</prevsent>
<prevsent>5.1 system description.
</prevsent>
</prevsection>
<citsent citstr=" N03-1033 ">
for the english source, we first tokenize using the stanford log-linear part-of-speech tagger (toutanova et al, 2003).<papid> N03-1033 </papid></citsent>
<aftsection>
<nextsent>we then proceed to split the data into smaller sentences and tagthem using ratnaparkhis maximum entropy tagger (ratnaparkhi, 1996).<papid> W96-0213 </papid></nextsent>
<nextsent>we parse the data using the collins parser (collins, 1997), <papid> P97-1003 </papid>and thentag person, location and organization names using the stanford named entity recognizer (finkel et al, 2005).<papid> P05-1045 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1875">
<title id=" E09-1011.xml">syntactic phrase reordering for englishtoarabic statistical machine translation </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>5.1 system description.
</prevsent>
<prevsent>for the english source, we first tokenize using the stanford log-linear part-of-speech tagger (toutanova et al, 2003).<papid> N03-1033 </papid></prevsent>
</prevsection>
<citsent citstr=" W96-0213 ">
we then proceed to split the data into smaller sentences and tagthem using ratnaparkhis maximum entropy tagger (ratnaparkhi, 1996).<papid> W96-0213 </papid></citsent>
<aftsection>
<nextsent>we parse the data using the collins parser (collins, 1997), <papid> P97-1003 </papid>and thentag person, location and organization names using the stanford named entity recognizer (finkel et al, 2005).<papid> P05-1045 </papid></nextsent>
<nextsent>on the arabic side, we normalize the data by changing final y? to y?, and changing the various forms of alif hamza to bare alif, since these characters are written inconsistently in some arabic sources.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1876">
<title id=" E09-1011.xml">syntactic phrase reordering for englishtoarabic statistical machine translation </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>for the english source, we first tokenize using the stanford log-linear part-of-speech tagger (toutanova et al, 2003).<papid> N03-1033 </papid></prevsent>
<prevsent>we then proceed to split the data into smaller sentences and tagthem using ratnaparkhis maximum entropy tagger (ratnaparkhi, 1996).<papid> W96-0213 </papid></prevsent>
</prevsection>
<citsent citstr=" P97-1003 ">
we parse the data using the collins parser (collins, 1997), <papid> P97-1003 </papid>and thentag person, location and organization names using the stanford named entity recognizer (finkel et al, 2005).<papid> P05-1045 </papid></citsent>
<aftsection>
<nextsent>on the arabic side, we normalize the data by changing final y? to y?, and changing the various forms of alif hamza to bare alif, since these characters are written inconsistently in some arabic sources.
</nextsent>
<nextsent>we then segment the data using mada according to the scheme explained in section 4.1.
</nextsent>
<nextsent>90the english source is aligned to the segmented arabic target using the standard moses (moses, 2007) configuration of giza++ (och and ney, 2000), <papid> P00-1056 </papid>which is ibm model 4, and decoding is done using the phrase based smt system moses.</nextsent>
<nextsent>we use maximum phrase length of 15 to account for the increase in length of the segmented arabic.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1877">
<title id=" E09-1011.xml">syntactic phrase reordering for englishtoarabic statistical machine translation </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>for the english source, we first tokenize using the stanford log-linear part-of-speech tagger (toutanova et al, 2003).<papid> N03-1033 </papid></prevsent>
<prevsent>we then proceed to split the data into smaller sentences and tagthem using ratnaparkhis maximum entropy tagger (ratnaparkhi, 1996).<papid> W96-0213 </papid></prevsent>
</prevsection>
<citsent citstr=" P05-1045 ">
we parse the data using the collins parser (collins, 1997), <papid> P97-1003 </papid>and thentag person, location and organization names using the stanford named entity recognizer (finkel et al, 2005).<papid> P05-1045 </papid></citsent>
<aftsection>
<nextsent>on the arabic side, we normalize the data by changing final y? to y?, and changing the various forms of alif hamza to bare alif, since these characters are written inconsistently in some arabic sources.
</nextsent>
<nextsent>we then segment the data using mada according to the scheme explained in section 4.1.
</nextsent>
<nextsent>90the english source is aligned to the segmented arabic target using the standard moses (moses, 2007) configuration of giza++ (och and ney, 2000), <papid> P00-1056 </papid>which is ibm model 4, and decoding is done using the phrase based smt system moses.</nextsent>
<nextsent>we use maximum phrase length of 15 to account for the increase in length of the segmented arabic.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1878">
<title id=" E09-1011.xml">syntactic phrase reordering for englishtoarabic statistical machine translation </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>on the arabic side, we normalize the data by changing final y? to y?, and changing the various forms of alif hamza to bare alif, since these characters are written inconsistently in some arabic sources.
</prevsent>
<prevsent>we then segment the data using mada according to the scheme explained in section 4.1.
</prevsent>
</prevsection>
<citsent citstr=" P00-1056 ">
90the english source is aligned to the segmented arabic target using the standard moses (moses, 2007) configuration of giza++ (och and ney, 2000), <papid> P00-1056 </papid>which is ibm model 4, and decoding is done using the phrase based smt system moses.</citsent>
<aftsection>
<nextsent>we use maximum phrase length of 15 to account for the increase in length of the segmented arabic.
</nextsent>
<nextsent>we also use lexicalized bidirectional reordering model conditioned on both the source and target sides, with distortion limit set to 6.
</nextsent>
<nextsent>we tune using ochs algorithm (och, 2003) <papid> P03-1021 </papid>to optimize weights for the distortion model, language model, phrase translation model and word penalty over the bleu metric (papineni et al, 2001).</nextsent>
<nextsent>for the segmented arabic experiments, we experiment with tuning using non-segmented arabicas reference.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1879">
<title id=" E09-1011.xml">syntactic phrase reordering for englishtoarabic statistical machine translation </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>we use maximum phrase length of 15 to account for the increase in length of the segmented arabic.
</prevsent>
<prevsent>we also use lexicalized bidirectional reordering model conditioned on both the source and target sides, with distortion limit set to 6.
</prevsent>
</prevsection>
<citsent citstr=" P03-1021 ">
we tune using ochs algorithm (och, 2003) <papid> P03-1021 </papid>to optimize weights for the distortion model, language model, phrase translation model and word penalty over the bleu metric (papineni et al, 2001).</citsent>
<aftsection>
<nextsent>for the segmented arabic experiments, we experiment with tuning using non-segmented arabicas reference.
</nextsent>
<nextsent>this is done by re combining the output before each tuning iteration is scored and has been shown by badr et. al (2008) <papid> P08-2039 </papid>to perform better than using segmented arabicas reference.</nextsent>
<nextsent>5.2 data used.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1882">
<title id=" E12-2008.xml">folheador browsing through portuguese semantic relations </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>its authors provide mnex4, an online interface for mindnet.
</prevsent>
<prevsent>after querying for pair of words, mnex provides allthe semantic relation paths between them, established by set of links that connect directly or indirectly one word to another.
</prevsent>
</prevsection>
<citsent citstr=" P98-1013 ">
it is also possible to view the definitions that originated the path.framenet (baker et al  1998) <papid> P98-1013 </papid>is manually built knowledge base structured on semantic frames that describe objects, states orevents.</citsent>
<aftsection>
<nextsent>there are several means for exploring framenet easily, including frame sql (sato,2003)5, which allows searching for frames, lexical units and relations in an integrated interface, and framegrapher6, graphical interface for the visualization of frame relations.
</nextsent>
<nextsent>for each frame, in both interfaces, textual definition, annotated sentences of the frame elements, lists of the frame relations, and lists with the lexical units in the frame are provided.
</nextsent>
<nextsent>reverb (fader et al  2011) <papid> D11-1142 </papid>is web-scale information extraction system that automatically acquires binary relations from text.</nextsent>
<nextsent>using reverb search7, web interface for reverb extractions, it is possible to obtain sets of relational triples where the predicate and/or the arguments contain givenstrings.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1883">
<title id=" E12-2008.xml">folheador browsing through portuguese semantic relations </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>there are several means for exploring framenet easily, including frame sql (sato,2003)5, which allows searching for frames, lexical units and relations in an integrated interface, and framegrapher6, graphical interface for the visualization of frame relations.
</prevsent>
<prevsent>for each frame, in both interfaces, textual definition, annotated sentences of the frame elements, lists of the frame relations, and lists with the lexical units in the frame are provided.
</prevsent>
</prevsection>
<citsent citstr=" D11-1142 ">
reverb (fader et al  2011) <papid> D11-1142 </papid>is web-scale information extraction system that automatically acquires binary relations from text.</citsent>
<aftsection>
<nextsent>using reverb search7, web interface for reverb extractions, it is possible to obtain sets of relational triples where the predicate and/or the arguments contain givenstrings.
</nextsent>
<nextsent>regarding that each of the former is optional, it is possible, for instance, to search for all triples with the predicate loves and first argument portuguese.
</nextsent>
<nextsent>search results include the matching triples, organised according to the name of the predicate, as well as the number of times each triple was extracted.
</nextsent>
<nextsent>the sentences where each triple was extracted from are as well provided.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1884">
<title id=" E12-2008.xml">folheador browsing through portuguese semantic relations </title>
<section> folheador.  </section>
<citcontext>
<prevsection>
<prevsent>the presented sentences (a subset of those returned by the previous service) will thus contain the related words connected by discriminating pattern for the relation they hold.
</prevsent>
<prevsent>figure 3.3 shows two sentences returned for the relation computa dor hiponimo de maquina.
</prevsent>
</prevsection>
<citsent citstr=" C92-2082 ">
these patterns, as those proposed by hearst (1992) <papid> C92-2082 </papid>and used in many projects since, may not be 100% reliable.</citsent>
<aftsection>
<nextsent>so, varra was designed toallow human users to classify the sentences according to whether the latter validate the relation, are just compatible with it, or not even that.in fact, people do not usually write definitions, especially when using common sense terms in ordinary discourse.
</nextsent>
<nextsent>thus, co-occurrence of semantically-related terms frequently indicates particular relation only implicitly.
</nextsent>
<nextsent>the choice of assessing sentences as good validators of asemantic relation is related to the task of automatically finding good illustrative examples for dictionaries, which is surprisingly complex task (rychly?
</nextsent>
<nextsent>et al  2008).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1885">
<title id=" E12-1032.xml">extending the entity based coherence model with multiple ranks </title>
<section> abstract </section>
<citcontext>
<prevsection>


</prevsection>
<citsent citstr=" J08-1001 ">
we extend the original entity-based coherence model (barzilay and lapata, 2008)<papid> J08-1001 </papid>by learning from more fine-grained coherence preferences in training data.</citsent>
<aftsection>
<nextsent>we associate multiple ranks with the set of permutations originating from the same source document, as opposed to the original pairwise rankings.
</nextsent>
<nextsent>we also study the effect of the permutations used in training, and the effect of the coreference component used in entity extraction.
</nextsent>
<nextsent>with no additional manual annotations required, our extended model is able to outperform the original model on two tasks: sentence ordering and summary coherence rating.
</nextsent>
<nextsent>coherence is important in well-written document; it helps make the text semantically meaningful and interpretable.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1891">
<title id=" E12-1032.xml">extending the entity based coherence model with multiple ranks </title>
<section> entity-based coherence model.  </section>
<citcontext>
<prevsection>
<prevsent>, pm(d)), where pt(d) is the probability of the transition in the entity grid, and is the number of transitions with length no more than predefined optimal transition length k. pt(d) is computed as the number of occurrences of in the entity grid of document d, divided by the total number of transitions of the same length in the entity grid.
</prevsent>
<prevsent>for entity extraction, barzilay and lapata (2008)<papid> J08-1001 </papid>had two conditions: coreference+ andcoreference?.</prevsent>
</prevsection>
<citsent citstr=" P02-1014 ">
in coreference+, entity coreference relations in the document were resolved by an automatic coreference resolution tool (ng and cardie, 2002), <papid> P02-1014 </papid>whereas in coreference?, nouns are simply clustered by string matching.</citsent>
<aftsection>
<nextsent>2.2 evaluation tasks.
</nextsent>
<nextsent>two evaluation tasks for barzilay and lapata(2008)<papid> J08-1001 </papid>s entity-based model are sentence ordering and summary coherence rating.in sentence ordering, set of random permutations is created for each source document, andthe learning procedure is conducted on this synthetic mixture of coherent and incoherent docu ments.</nextsent>
<nextsent>barzilay and lapata (2008)<papid> J08-1001 </papid>experimented on two datasets: news articles on the topic of earthquakes (earthquakes) and narratives on the topic of aviation accidents (accidents).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1896">
<title id=" E12-1032.xml">extending the entity based coherence model with multiple ranks </title>
<section> entity-based coherence model.  </section>
<citcontext>
<prevsection>
<prevsent>though the improvement was not significant, interestingly, short subsection in 316 their paper described their approach to extending pairwise rankings to longer rankings, by supplying the learner with rankings of all renderings as computed by ken dalls ?, which is one of our extensions considered in this paper.
</prevsent>
<prevsent>although filippova and strube simply discarded this idea because it hurt accuracies when tested on their data, we found it promising direction for further exploration.
</prevsent>
</prevsection>
<citsent citstr=" P10-1020 ">
cheung and penn (2010) <papid> P10-1020 </papid>adapted the standard entity-based coherence model to the same german corpus, but replaced the original linguistic dimension used by barzilay and lapata (2008)<papid> J08-1001 </papid>? grammatical role ? with topol ogi cal field information, and showed that for german text, such modification improves accuracy.for english text, two extensions have been proposed recently.</citsent>
<aftsection>
<nextsent>elsner and charniak (2011) <papid> P11-2022 </papid>augmented the original features used in the standardentity-based coherence model with large number of entity-specific features, and their extension significantly outperformed the standard model on two tasks: document discrimination (anothername for sentence ordering), and sentence insertion.</nextsent>
<nextsent>lin et al(2011) <papid> P11-1100 </papid>adapted the entity grid representation in the standard model into discourse role matrix, where additional discourse information about the document was encoded.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1900">
<title id=" E12-1032.xml">extending the entity based coherence model with multiple ranks </title>
<section> entity-based coherence model.  </section>
<citcontext>
<prevsection>
<prevsent>although filippova and strube simply discarded this idea because it hurt accuracies when tested on their data, we found it promising direction for further exploration.
</prevsent>
<prevsent>cheung and penn (2010) <papid> P10-1020 </papid>adapted the standard entity-based coherence model to the same german corpus, but replaced the original linguistic dimension used by barzilay and lapata (2008)<papid> J08-1001 </papid>? grammatical role ? with topol ogi cal field information, and showed that for german text, such modification improves accuracy.for english text, two extensions have been proposed recently.</prevsent>
</prevsection>
<citsent citstr=" P11-2022 ">
elsner and charniak (2011) <papid> P11-2022 </papid>augmented the original features used in the standardentity-based coherence model with large number of entity-specific features, and their extension significantly outperformed the standard model on two tasks: document discrimination (anothername for sentence ordering), and sentence insertion.</citsent>
<aftsection>
<nextsent>lin et al(2011) <papid> P11-1100 </papid>adapted the entity grid representation in the standard model into discourse role matrix, where additional discourse information about the document was encoded.</nextsent>
<nextsent>their extended model significantly improved ranking accuracies on the same two datasets used by barzilay and lapata (2008) <papid> J08-1001 </papid>as well as on the wall street journal corpus.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1901">
<title id=" E12-1032.xml">extending the entity based coherence model with multiple ranks </title>
<section> entity-based coherence model.  </section>
<citcontext>
<prevsection>
<prevsent>cheung and penn (2010) <papid> P10-1020 </papid>adapted the standard entity-based coherence model to the same german corpus, but replaced the original linguistic dimension used by barzilay and lapata (2008)<papid> J08-1001 </papid>? grammatical role ? with topol ogi cal field information, and showed that for german text, such modification improves accuracy.for english text, two extensions have been proposed recently.</prevsent>
<prevsent>elsner and charniak (2011) <papid> P11-2022 </papid>augmented the original features used in the standardentity-based coherence model with large number of entity-specific features, and their extension significantly outperformed the standard model on two tasks: document discrimination (anothername for sentence ordering), and sentence insertion.</prevsent>
</prevsection>
<citsent citstr=" P11-1100 ">
lin et al(2011) <papid> P11-1100 </papid>adapted the entity grid representation in the standard model into discourse role matrix, where additional discourse information about the document was encoded.</citsent>
<aftsection>
<nextsent>their extended model significantly improved ranking accuracies on the same two datasets used by barzilay and lapata (2008) <papid> J08-1001 </papid>as well as on the wall street journal corpus.</nextsent>
<nextsent>however, while enriching or modifying the original features used in the standard model is certainly direction for refinement of the model, it usually requires more training data or more sophisticated feature representation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1906">
<title id=" E12-1032.xml">extending the entity based coherence model with multiple ranks </title>
<section> dissimilarity metrics.  </section>
<citcontext>
<prevsection>
<prevsent>(2) it depends on the particular sentence ordering in permutation while remaining independent of the entities within the sentences; otherwise our multiple-rank model might be trained to fit particular probability distributions of entity transitions rather than true coherence preferences.
</prevsent>
<prevsent>in our work we use three different metrics: ken dalls ? distance, average continuity, and edit distance.
</prevsent>
</prevsection>
<citsent citstr=" P03-1069 ">
ken dalls ? distance: this metric has been widely used in evaluation of sentence ordering (lapata, 2003; <papid> P03-1069 </papid>lapata, 2006; <papid> J06-4002 </papid>bollegala et al 2006; <papid> P06-1049 </papid>madnani et al 2007)1.</citsent>
<aftsection>
<nextsent>it measures the disagreement between two orderings ? and pi interms of the number of inversions of adjacent sentences necessary to convert one ordering into an other.
</nextsent>
<nextsent>ken dalls ? distance is defined as ? = 2m n(n ? 1) ,where is the number of sentence inversions necessary to convert ? to pi.
</nextsent>
<nextsent>average continuity (ac): following zhang(2011), <papid> P11-3002 </papid>we use average continuity as the second dissimilarity metric.</nextsent>
<nextsent>it was first proposed1filippova and strube (2007) found that their performance dropped when using this metric for longer rankings; but they were using data in different language and with manual annotations, so its effect on our datasets is worth trying nonetheless.by bollegala et al(2006).<papid> P06-1049 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1907">
<title id=" E12-1032.xml">extending the entity based coherence model with multiple ranks </title>
<section> dissimilarity metrics.  </section>
<citcontext>
<prevsection>
<prevsent>(2) it depends on the particular sentence ordering in permutation while remaining independent of the entities within the sentences; otherwise our multiple-rank model might be trained to fit particular probability distributions of entity transitions rather than true coherence preferences.
</prevsent>
<prevsent>in our work we use three different metrics: ken dalls ? distance, average continuity, and edit distance.
</prevsent>
</prevsection>
<citsent citstr=" J06-4002 ">
ken dalls ? distance: this metric has been widely used in evaluation of sentence ordering (lapata, 2003; <papid> P03-1069 </papid>lapata, 2006; <papid> J06-4002 </papid>bollegala et al 2006; <papid> P06-1049 </papid>madnani et al 2007)1.</citsent>
<aftsection>
<nextsent>it measures the disagreement between two orderings ? and pi interms of the number of inversions of adjacent sentences necessary to convert one ordering into an other.
</nextsent>
<nextsent>ken dalls ? distance is defined as ? = 2m n(n ? 1) ,where is the number of sentence inversions necessary to convert ? to pi.
</nextsent>
<nextsent>average continuity (ac): following zhang(2011), <papid> P11-3002 </papid>we use average continuity as the second dissimilarity metric.</nextsent>
<nextsent>it was first proposed1filippova and strube (2007) found that their performance dropped when using this metric for longer rankings; but they were using data in different language and with manual annotations, so its effect on our datasets is worth trying nonetheless.by bollegala et al(2006).<papid> P06-1049 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1908">
<title id=" E12-1032.xml">extending the entity based coherence model with multiple ranks </title>
<section> dissimilarity metrics.  </section>
<citcontext>
<prevsection>
<prevsent>(2) it depends on the particular sentence ordering in permutation while remaining independent of the entities within the sentences; otherwise our multiple-rank model might be trained to fit particular probability distributions of entity transitions rather than true coherence preferences.
</prevsent>
<prevsent>in our work we use three different metrics: ken dalls ? distance, average continuity, and edit distance.
</prevsent>
</prevsection>
<citsent citstr=" P06-1049 ">
ken dalls ? distance: this metric has been widely used in evaluation of sentence ordering (lapata, 2003; <papid> P03-1069 </papid>lapata, 2006; <papid> J06-4002 </papid>bollegala et al 2006; <papid> P06-1049 </papid>madnani et al 2007)1.</citsent>
<aftsection>
<nextsent>it measures the disagreement between two orderings ? and pi interms of the number of inversions of adjacent sentences necessary to convert one ordering into an other.
</nextsent>
<nextsent>ken dalls ? distance is defined as ? = 2m n(n ? 1) ,where is the number of sentence inversions necessary to convert ? to pi.
</nextsent>
<nextsent>average continuity (ac): following zhang(2011), <papid> P11-3002 </papid>we use average continuity as the second dissimilarity metric.</nextsent>
<nextsent>it was first proposed1filippova and strube (2007) found that their performance dropped when using this metric for longer rankings; but they were using data in different language and with manual annotations, so its effect on our datasets is worth trying nonetheless.by bollegala et al(2006).<papid> P06-1049 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1909">
<title id=" E12-1032.xml">extending the entity based coherence model with multiple ranks </title>
<section> dissimilarity metrics.  </section>
<citcontext>
<prevsection>
<prevsent>it measures the disagreement between two orderings ? and pi interms of the number of inversions of adjacent sentences necessary to convert one ordering into an other.
</prevsent>
<prevsent>ken dalls ? distance is defined as ? = 2m n(n ? 1) ,where is the number of sentence inversions necessary to convert ? to pi.
</prevsent>
</prevsection>
<citsent citstr=" P11-3002 ">
average continuity (ac): following zhang(2011), <papid> P11-3002 </papid>we use average continuity as the second dissimilarity metric.</citsent>
<aftsection>
<nextsent>it was first proposed1filippova and strube (2007) found that their performance dropped when using this metric for longer rankings; but they were using data in different language and with manual annotations, so its effect on our datasets is worth trying nonetheless.by bollegala et al(2006).<papid> P06-1049 </papid></nextsent>
<nextsent>this metric estimates the quality of particular sentence ordering by the number of correctly arranged continuous sentences, compared to the reference order ing.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1945">
<title id=" E12-1032.xml">extending the entity based coherence model with multiple ranks </title>
<section> results.  </section>
<citcontext>
<prevsection>
<prevsent>when our model performs poorer than the baseline (using coreference?
</prevsent>
<prevsent>configuration), the difference is not significant, which suggests that our multiple-rank model with unsupervised sco reassignment via simple cosine matching can remain competitive with the standard model, which requires human annotations to obtain more fine grained coherence spectrum.
</prevsent>
</prevsection>
<citsent citstr=" N04-4001 ">
this observation is consistent with banko and vanderwende (2004)<papid> N04-4001 </papid>s discovery that human-generated summaries look quite extractive.</citsent>
<aftsection>
<nextsent>in this paper, we have extended the popular coherence model of barzilay and lapata (2008)<papid> J08-1001 </papid>by adopting multiple-rank learning approach.</nextsent>
<nextsent>this is inherently different from other extensions to this model, in which the focus is on enriching the set of features for entity-grid construction, whereas we simply keep their original feature set intact, and manipulate only their learning method ology.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1949">
<title id=" E12-1082.xml">composing extended topdown tree transducers </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>it is demonstrated thatthe new procedure is more widely applicable than the existing methods.
</prevsent>
<prevsent>in general, the result of the composition is an extended top-down tree transducer that is no longer linear or non deleting, but in number ofcases these properties can easily be recovered by post-processing step.
</prevsent>
</prevsection>
<citsent citstr=" P03-2041 ">
tree-based translation models such as synchronous tree substitution grammars (eisner, 2003; <papid> P03-2041 </papid>shieber, 2004) or multi bottom-up tree transducers (lilin, 1978; engelfriet et al 2009;maletti, 2010; <papid> N10-1130 </papid>maletti, 2011) are used for several aspects of syntax-based machine translation (knight and graehl, 2005).</citsent>
<aftsection>
<nextsent>here we consider the extended top-down tree transducer (xtop), which was studied in (arnold and dauchet, 1982; knight, 2007; graehl et al 2008; <papid> J08-3004 </papid>graehl et al 2009) and implemented in the toolkit tiburon (may and knight, 2006; may, 2010).</nextsent>
<nextsent>specifically, we investigate compositions of linear and non deleting xtops (ln-xtop).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1950">
<title id=" E12-1082.xml">composing extended topdown tree transducers </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>it is demonstrated thatthe new procedure is more widely applicable than the existing methods.
</prevsent>
<prevsent>in general, the result of the composition is an extended top-down tree transducer that is no longer linear or non deleting, but in number ofcases these properties can easily be recovered by post-processing step.
</prevsent>
</prevsection>
<citsent citstr=" N10-1130 ">
tree-based translation models such as synchronous tree substitution grammars (eisner, 2003; <papid> P03-2041 </papid>shieber, 2004) or multi bottom-up tree transducers (lilin, 1978; engelfriet et al 2009;maletti, 2010; <papid> N10-1130 </papid>maletti, 2011) are used for several aspects of syntax-based machine translation (knight and graehl, 2005).</citsent>
<aftsection>
<nextsent>here we consider the extended top-down tree transducer (xtop), which was studied in (arnold and dauchet, 1982; knight, 2007; graehl et al 2008; <papid> J08-3004 </papid>graehl et al 2009) and implemented in the toolkit tiburon (may and knight, 2006; may, 2010).</nextsent>
<nextsent>specifically, we investigate compositions of linear and non deleting xtops (ln-xtop).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1952">
<title id=" E12-1082.xml">composing extended topdown tree transducers </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in general, the result of the composition is an extended top-down tree transducer that is no longer linear or non deleting, but in number ofcases these properties can easily be recovered by post-processing step.
</prevsent>
<prevsent>tree-based translation models such as synchronous tree substitution grammars (eisner, 2003; <papid> P03-2041 </papid>shieber, 2004) or multi bottom-up tree transducers (lilin, 1978; engelfriet et al 2009;maletti, 2010; <papid> N10-1130 </papid>maletti, 2011) are used for several aspects of syntax-based machine translation (knight and graehl, 2005).</prevsent>
</prevsection>
<citsent citstr=" J08-3004 ">
here we consider the extended top-down tree transducer (xtop), which was studied in (arnold and dauchet, 1982; knight, 2007; graehl et al 2008; <papid> J08-3004 </papid>graehl et al 2009) and implemented in the toolkit tiburon (may and knight, 2006; may, 2010).</citsent>
<aftsection>
<nextsent>specifically, we investigate compositions of linear and non deleting xtops (ln-xtop).
</nextsent>
<nextsent>arnold and dauchet (1982) showed that ln-xtops compute class of transformations that is not closed under composition, so we cannot compose two arbitrary ln-xtops into single ln-xtop.
</nextsent>
<nextsent>however, we will show that ln-xtops can be composed into (not necessarily linear or nondeleting) xtop.
</nextsent>
<nextsent>to illustrate the use of ln-xtops in machine translation, we consider the following english sentence together with german reference translation: ? all authors were financially supported by the emmy noether project ma / 4959 / 1-1 of the german research foundation (dfg).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1954">
<title id=" E12-1082.xml">composing extended topdown tree transducers </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the newswire reported yesterday that the serbs have completed the negotiations.
</prevsent>
<prevsent>gestern [yesterday] berichtete [reported] die [the] nachrichtenagentur [newswire] die [the] serben [serbs] hatten [would have] die [the] verhandlungen [negotiations] beendet [completed].
</prevsent>
</prevsection>
<citsent citstr=" P01-1067 ">
the relation between them can be described (yamada and knight, 2001) <papid> P01-1067 </papid>by three operations: drop of the relative pronoun, movement of the participle to end of the clause, and word-to-wordtranslation.</citsent>
<aftsection>
<nextsent>figure 1 shows the first two operations, and figure 2 shows ln-xtop rules performing them.
</nextsent>
<nextsent>let us now informally describe the execution of an ln-xtop on the top rule of figure 2.
</nextsent>
<nextsent>in general, ln-xtops process an in put tree from the root towards the leaves usinga set of rules and states.
</nextsent>
<nextsent>the state in the left hand side of ? controls the particular operation of figure 1 [top].
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1955">
<title id=" E12-1082.xml">composing extended topdown tree transducers </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>once the operation has been performed, control is passed to states pnp and pvp,which use their own rules to process the remaining input subtree governed by the variable below them (see figure 2).
</prevsent>
<prevsent>in the same fashion, an lnxtop containing the bottom rule of figure 2 re orders the english verbal complex.in this way we model the word drop by an ln xtop and reordering by an ln-xtop . the syntactic properties of linearity and nondeletionyield nice algorithmic properties, and the mod 808 prc prel that y1 y2 ? pnp y1 pvp y2 c z1 vp z2 z3 z4 ? qnp z1 vp qva z2 qvp z4 qnp z3 figure 2: xtop rules for the operations of figure 1.
</prevsent>
</prevsection>
<citsent citstr=" P10-1108 ">
ular approach is desirable for better design and parametrization of the translation model (may et al., 2010).<papid> P10-1108 </papid></citsent>
<aftsection>
<nextsent>composition allows us to re combine those parts into one device modeling the whole translation.
</nextsent>
<nextsent>in particular, it gives all parts the chance to vote at the same time.
</nextsent>
<nextsent>this is especially important if pruning is used because it might otherwise exclude candidates that score low in one part but well in others (may et al 2010).<papid> P10-1108 </papid>because ln-xtop is not closed under composition, the composition ofm andn might be out side ln-xtop.</nextsent>
<nextsent>these cases have been identified by arnold and dauchet (1982) as infinitely overlapping cuts?, which occur when the right-handsides of and the left-hand sides of are unboundedly overlapping.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1962">
<title id=" E12-1055.xml">perplexity minimization for translation model domain adaptation in statistical machine translation </title>
<section> domain adaptation for translation.  </section>
<citcontext>
<prevsection>
<prevsent>ifwe can scale up the number of models whose contributions we weight, this reduces the need for apriori knowledge about the fitness1 of each potential training text, and opens new research opportunities, for instance experiments with clustered training data.
</prevsent>
<prevsent>models to motivate efforts in domain adaptation, let us review why additional training data can improve, but also decrease translation quality.adding more training data to translation system is easy to motivate through the data sparseness problem.
</prevsent>
</prevsection>
<citsent citstr=" W01-0504 ">
koehn and knight (2001) <papid> W01-0504 </papid>show that translation quality correlates strongly with how often word occurs in the training corpus.rare words or phrases pose problem in several stages of mt modelling, from word alignment to the computation of translation probabilities through maximum likelihood estimation.</citsent>
<aftsection>
<nextsent>unknown words are typically copied verbatim to the target text, which may be good strategy for named entities, but is often wrong otherwise.
</nextsent>
<nextsent>in general, more data allows for better word alignment, better estimation of translation probabilities, and for the consideration of more context (in phrase-based or syntactic smt).a second effect of additional data is not necessarily positive.
</nextsent>
<nextsent>translations are inherently ambiguous, and strong source of ambiguity is the 1we borrow this term from early evolutionary biology to emphasize that the question in domain adaptation is not how good?
</nextsent>
<nextsent>or bad?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1963">
<title id=" E12-1055.xml">perplexity minimization for translation model domain adaptation in statistical machine translation </title>
<section> domain adaptation for translation.  </section>
<citcontext>
<prevsection>
<prevsent>for smt, linear interpolation of translation models has been used in numerous systems.
</prevsent>
<prevsent>the approaches diverge in how they set the interpolation weights.
</prevsent>
</prevsection>
<citsent citstr=" P07-1092 ">
some authors use uniform weights (cohn and lapata, 2007), <papid> P07-1092 </papid>others empirically test different interpolation coefficients (finch and sumita, 2008; <papid> W08-0334 </papid>yasuda et al 2008; <papid> I08-2088 </papid>nakov and ng, 2009; axelrod et al 2011), others apply monolingual metrics to set the weights for tm interpolation (foster and kuhn, 2007; <papid> W07-0717 </papid>koehn et al 2010).<papid> W10-1715 </papid></citsent>
<aftsection>
<nextsent>there are reasons against all these approaches.
</nextsent>
<nextsent>uniform weights are easy to implement, but give little control.
</nextsent>
<nextsent>empirically, it has been shown that they often do not perform optimally (finch and sumita, 2008; <papid> W08-0334 </papid>yasuda et al 2008).<papid> I08-2088 </papid></nextsent>
<nextsent>an optimization of bleu scores on development set is promising, but slow and impractical.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1964">
<title id=" E12-1055.xml">perplexity minimization for translation model domain adaptation in statistical machine translation </title>
<section> domain adaptation for translation.  </section>
<citcontext>
<prevsection>
<prevsent>for smt, linear interpolation of translation models has been used in numerous systems.
</prevsent>
<prevsent>the approaches diverge in how they set the interpolation weights.
</prevsent>
</prevsection>
<citsent citstr=" W08-0334 ">
some authors use uniform weights (cohn and lapata, 2007), <papid> P07-1092 </papid>others empirically test different interpolation coefficients (finch and sumita, 2008; <papid> W08-0334 </papid>yasuda et al 2008; <papid> I08-2088 </papid>nakov and ng, 2009; axelrod et al 2011), others apply monolingual metrics to set the weights for tm interpolation (foster and kuhn, 2007; <papid> W07-0717 </papid>koehn et al 2010).<papid> W10-1715 </papid></citsent>
<aftsection>
<nextsent>there are reasons against all these approaches.
</nextsent>
<nextsent>uniform weights are easy to implement, but give little control.
</nextsent>
<nextsent>empirically, it has been shown that they often do not perform optimally (finch and sumita, 2008; <papid> W08-0334 </papid>yasuda et al 2008).<papid> I08-2088 </papid></nextsent>
<nextsent>an optimization of bleu scores on development set is promising, but slow and impractical.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1965">
<title id=" E12-1055.xml">perplexity minimization for translation model domain adaptation in statistical machine translation </title>
<section> domain adaptation for translation.  </section>
<citcontext>
<prevsection>
<prevsent>for smt, linear interpolation of translation models has been used in numerous systems.
</prevsent>
<prevsent>the approaches diverge in how they set the interpolation weights.
</prevsent>
</prevsection>
<citsent citstr=" I08-2088 ">
some authors use uniform weights (cohn and lapata, 2007), <papid> P07-1092 </papid>others empirically test different interpolation coefficients (finch and sumita, 2008; <papid> W08-0334 </papid>yasuda et al 2008; <papid> I08-2088 </papid>nakov and ng, 2009; axelrod et al 2011), others apply monolingual metrics to set the weights for tm interpolation (foster and kuhn, 2007; <papid> W07-0717 </papid>koehn et al 2010).<papid> W10-1715 </papid></citsent>
<aftsection>
<nextsent>there are reasons against all these approaches.
</nextsent>
<nextsent>uniform weights are easy to implement, but give little control.
</nextsent>
<nextsent>empirically, it has been shown that they often do not perform optimally (finch and sumita, 2008; <papid> W08-0334 </papid>yasuda et al 2008).<papid> I08-2088 </papid></nextsent>
<nextsent>an optimization of bleu scores on development set is promising, but slow and impractical.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1966">
<title id=" E12-1055.xml">perplexity minimization for translation model domain adaptation in statistical machine translation </title>
<section> domain adaptation for translation.  </section>
<citcontext>
<prevsection>
<prevsent>for smt, linear interpolation of translation models has been used in numerous systems.
</prevsent>
<prevsent>the approaches diverge in how they set the interpolation weights.
</prevsent>
</prevsection>
<citsent citstr=" W07-0717 ">
some authors use uniform weights (cohn and lapata, 2007), <papid> P07-1092 </papid>others empirically test different interpolation coefficients (finch and sumita, 2008; <papid> W08-0334 </papid>yasuda et al 2008; <papid> I08-2088 </papid>nakov and ng, 2009; axelrod et al 2011), others apply monolingual metrics to set the weights for tm interpolation (foster and kuhn, 2007; <papid> W07-0717 </papid>koehn et al 2010).<papid> W10-1715 </papid></citsent>
<aftsection>
<nextsent>there are reasons against all these approaches.
</nextsent>
<nextsent>uniform weights are easy to implement, but give little control.
</nextsent>
<nextsent>empirically, it has been shown that they often do not perform optimally (finch and sumita, 2008; <papid> W08-0334 </papid>yasuda et al 2008).<papid> I08-2088 </papid></nextsent>
<nextsent>an optimization of bleu scores on development set is promising, but slow and impractical.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1968">
<title id=" E12-1055.xml">perplexity minimization for translation model domain adaptation in statistical machine translation </title>
<section> domain adaptation for translation.  </section>
<citcontext>
<prevsection>
<prevsent>for smt, linear interpolation of translation models has been used in numerous systems.
</prevsent>
<prevsent>the approaches diverge in how they set the interpolation weights.
</prevsent>
</prevsection>
<citsent citstr=" W10-1715 ">
some authors use uniform weights (cohn and lapata, 2007), <papid> P07-1092 </papid>others empirically test different interpolation coefficients (finch and sumita, 2008; <papid> W08-0334 </papid>yasuda et al 2008; <papid> I08-2088 </papid>nakov and ng, 2009; axelrod et al 2011), others apply monolingual metrics to set the weights for tm interpolation (foster and kuhn, 2007; <papid> W07-0717 </papid>koehn et al 2010).<papid> W10-1715 </papid></citsent>
<aftsection>
<nextsent>there are reasons against all these approaches.
</nextsent>
<nextsent>uniform weights are easy to implement, but give little control.
</nextsent>
<nextsent>empirically, it has been shown that they often do not perform optimally (finch and sumita, 2008; <papid> W08-0334 </papid>yasuda et al 2008).<papid> I08-2088 </papid></nextsent>
<nextsent>an optimization of bleu scores on development set is promising, but slow and impractical.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1972">
<title id=" E12-1055.xml">perplexity minimization for translation model domain adaptation in statistical machine translation </title>
<section> domain adaptation for translation.  </section>
<citcontext>
<prevsection>
<prevsent>5unlike equation 1, equation 3 does not require that ( ? i) = 1.
</prevsent>
<prevsent>2.3 alternative paths.
</prevsent>
</prevsection>
<citsent citstr=" W07-0733 ">
a third method is using multiple translation models as alternative decoding paths (birch et al 2007), an idea which koehn and schroeder (2007) <papid> W07-0733 </papid>first used for domain adaptation.</citsent>
<aftsection>
<nextsent>this approach has the attractive theoretical property that adding new models is guaranteed to lead to equal or better performance, given the right weights.
</nextsent>
<nextsent>at best, model is beneficial with appropriate weights.
</nextsent>
<nextsent>at worst, we can set the feature weights so that the decoding paths of one model are never picked for the final translation.
</nextsent>
<nextsent>in practice, each translation model adds 5 features and thus 5 more dimensions to the weight space, which leads to longer search, search errors, and/or overfitting.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1975">
<title id=" E12-1055.xml">perplexity minimization for translation model domain adaptation in statistical machine translation </title>
<section> domain adaptation for translation.  </section>
<citcontext>
<prevsection>
<prevsent>in language modelling, perplexity is frequently used as quality measure for language models(chen and goodman, 1998).
</prevsent>
<prevsent>among other applications, language model perplexity has been used for domain adaptation (foster and kuhn, 2007).<papid> W07-0717 </papid></prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
for translation models, perplexity is most closely associated with em word alignment (brown et al., 1993) <papid> J93-2003 </papid>and has been used to evaluate different alignment algorithms (al-onaizan et al 1999).</citsent>
<aftsection>
<nextsent>we investigate translation model perplexity minimization as method to set model weigh tsin mixture modelling.
</nextsent>
<nextsent>for the purpose of optimization, the cross-entropy h(p), the perplexity 2h(p), and other derived measures are equivalent.
</nextsent>
<nextsent>the cross-entropy h(p) is defined as:6 6see (chen and goodman, 1998) for short discussion of the equation.
</nextsent>
<nextsent>in short, lower cross-entropy indicates that the model is better able to predict the development set.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1976">
<title id=" E12-1055.xml">perplexity minimization for translation model domain adaptation in statistical machine translation </title>
<section> domain adaptation for translation.  </section>
<citcontext>
<prevsection>
<prevsent>x,y p?(x, y) log2 p(x|y) (4) the phrase pairs (x, y) whose probability we measure, and their empirical probability p?
</prevsent>
<prevsent>need to be extracted from development set, whereas is the model probability.
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
to obtain the phrase pairs, we process the development set with the same word alignment and phrase extraction tools that we use for training, i.e. giza++ and heuristics for phrase extraction (och and ney, 2003).<papid> J03-1002 </papid></citsent>
<aftsection>
<nextsent>the objective function is the minimization of thecross-entropy, with the weight vector ? as argu ment: ??
</nextsent>
<nextsent>= argmin ? ?
</nextsent>
<nextsent>x,y p?(x, y) log2 p(x|y;?)
</nextsent>
<nextsent>(5) we can fill in equations 1 or 3 for p(x|y;?).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1977">
<title id=" E12-1055.xml">perplexity minimization for translation model domain adaptation in statistical machine translation </title>
<section> domain adaptation for translation.  </section>
<citcontext>
<prevsection>
<prevsent>both are convex in the domain 0.
</prevsent>
<prevsent>consequently, equation 4 is also convex be cause it is the weighted sum of convex functions.8there are tasks for which perplexity is known to be unreliable, e.g. for comparing models with different vocabularies.
</prevsent>
</prevsection>
<citsent citstr=" D10-1044 ">
however, such confounding factors do not affect the optimization algorithm, which works with fixed set of phrase pairs, and merely varies ?.our main technical contributions are as fol lows: additionally to perplexity optimization for linear interpolation, which was first applied by foster et al(2010), <papid> D10-1044 </papid>we propose perplexity optimization for weighted counts (equation 3), and modified implementation of linear interpolation.also, we independently perform perplexity minimization for all four features of the standard smt translation model: the phrase translation probabilities p(t|s) and p(s|t), and the lexical weights lex(t|s) and lex(s|t).</citsent>
<aftsection>
<nextsent>so far, we discussed mixture modelling for translation models, which is only subset of domain adaptation techniques in smt.
</nextsent>
<nextsent>mixture-modelling for language models is well established (foster and kuhn, 2007).<papid> W07-0717 </papid></nextsent>
<nextsent>language model adaptation serves the same purpose as translation model adaptation, i.e. skewing the probability distribution in favour of in-domain translations.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1981">
<title id=" E12-1055.xml">perplexity minimization for translation model domain adaptation in statistical machine translation </title>
<section> other domain adaptation techniques.  </section>
<citcontext>
<prevsection>
<prevsent>this means that lm adaptation may have similar effects as tm adaptation, and that the two are to some extent redundant.
</prevsent>
<prevsent>foster andkuhn (2007) <papid> W07-0717 </papid>find that both tm and lm adaptation are effective?, but that combined lm and tm adaptation is not better than lm adaptation on its own?.a second strand of research in domain adaptation is data selection, i.e. choosing subset of the training data that is considered more relevant for the task at hand.</prevsent>
</prevsection>
<citsent citstr=" C04-1059 ">
this has been done for language models using techniques from information retrieval (zhao et al 2004), <papid> C04-1059 </papid>or perplexity (lin etal., 1997; moore and lewis, 2010).<papid> P10-2041 </papid></citsent>
<aftsection>
<nextsent>data selection has also been proposed for translation models (axelrod et al 2011).
</nextsent>
<nextsent>note that for translation models, data selection offers an unattractivetrade-off between the data sparseness and the ambiguity problem, and that the optimal amount of data to select is hard to determine.our discussion of mixture-modelling is relatively coarse-grained, with 2-10 models beingcombined.
</nextsent>
<nextsent>matsoukas et al(2009) <papid> D09-1074 </papid>propose an approach where each sentence is weighted according to classifier, and foster et al(2010) <papid> D10-1044 </papid>extend this approach by weighting individual phrase pairs.</nextsent>
<nextsent>these more fine-grained methods need not be seen as alternatives to coarse-grained ones.foster et al(2010) <papid> D10-1044 </papid>combine the two, applying linear interpolation to combine the instance 542 weighted out-of-domain model with an in-domain model.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1982">
<title id=" E12-1055.xml">perplexity minimization for translation model domain adaptation in statistical machine translation </title>
<section> other domain adaptation techniques.  </section>
<citcontext>
<prevsection>
<prevsent>this means that lm adaptation may have similar effects as tm adaptation, and that the two are to some extent redundant.
</prevsent>
<prevsent>foster andkuhn (2007) <papid> W07-0717 </papid>find that both tm and lm adaptation are effective?, but that combined lm and tm adaptation is not better than lm adaptation on its own?.a second strand of research in domain adaptation is data selection, i.e. choosing subset of the training data that is considered more relevant for the task at hand.</prevsent>
</prevsection>
<citsent citstr=" P10-2041 ">
this has been done for language models using techniques from information retrieval (zhao et al 2004), <papid> C04-1059 </papid>or perplexity (lin etal., 1997; moore and lewis, 2010).<papid> P10-2041 </papid></citsent>
<aftsection>
<nextsent>data selection has also been proposed for translation models (axelrod et al 2011).
</nextsent>
<nextsent>note that for translation models, data selection offers an unattractivetrade-off between the data sparseness and the ambiguity problem, and that the optimal amount of data to select is hard to determine.our discussion of mixture-modelling is relatively coarse-grained, with 2-10 models beingcombined.
</nextsent>
<nextsent>matsoukas et al(2009) <papid> D09-1074 </papid>propose an approach where each sentence is weighted according to classifier, and foster et al(2010) <papid> D10-1044 </papid>extend this approach by weighting individual phrase pairs.</nextsent>
<nextsent>these more fine-grained methods need not be seen as alternatives to coarse-grained ones.foster et al(2010) <papid> D10-1044 </papid>combine the two, applying linear interpolation to combine the instance 542 weighted out-of-domain model with an in-domain model.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1983">
<title id=" E12-1055.xml">perplexity minimization for translation model domain adaptation in statistical machine translation </title>
<section> other domain adaptation techniques.  </section>
<citcontext>
<prevsection>
<prevsent>data selection has also been proposed for translation models (axelrod et al 2011).
</prevsent>
<prevsent>note that for translation models, data selection offers an unattractivetrade-off between the data sparseness and the ambiguity problem, and that the optimal amount of data to select is hard to determine.our discussion of mixture-modelling is relatively coarse-grained, with 2-10 models beingcombined.
</prevsent>
</prevsection>
<citsent citstr=" D09-1074 ">
matsoukas et al(2009) <papid> D09-1074 </papid>propose an approach where each sentence is weighted according to classifier, and foster et al(2010) <papid> D10-1044 </papid>extend this approach by weighting individual phrase pairs.</citsent>
<aftsection>
<nextsent>these more fine-grained methods need not be seen as alternatives to coarse-grained ones.foster et al(2010) <papid> D10-1044 </papid>combine the two, applying linear interpolation to combine the instance 542 weighted out-of-domain model with an in-domain model.</nextsent>
<nextsent>apart from measuring the performance of the approaches introduced in section 2, we want to investigate the following open research questions.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1988">
<title id=" E12-1055.xml">perplexity minimization for translation model domain adaptation in statistical machine translation </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>4.1 data and methods.
</prevsent>
<prevsent>in terms of tools and techniques used, we mostly adhere to the work flow described for the wmt 2011 baseline system9.
</prevsent>
</prevsection>
<citsent citstr=" P07-2045 ">
the main tools are moses (koehn et al 2007), <papid> P07-2045 </papid>srilm (stolcke, 2002), and giza++ (och and ney, 2003), <papid> J03-1002 </papid>with settings as described in the wmt 2011 guide.</citsent>
<aftsection>
<nextsent>we report two translation measures: bleu (papineni et al 2002) <papid> P02-1040 </papid>and meteor 1.3 (denkowski and lavie, 2011).<papid> W11-2107 </papid></nextsent>
<nextsent>all results are lower cased and tokenized, measured with five independent runs of mert (och and ney, 2003) <papid> J03-1002 </papid>and multeval (clark et al 2011) for re sampling and significance testing.we compare three baselines and four translation model mixture techniques.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1990">
<title id=" E12-1055.xml">perplexity minimization for translation model domain adaptation in statistical machine translation </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>in terms of tools and techniques used, we mostly adhere to the work flow described for the wmt 2011 baseline system9.
</prevsent>
<prevsent>the main tools are moses (koehn et al 2007), <papid> P07-2045 </papid>srilm (stolcke, 2002), and giza++ (och and ney, 2003), <papid> J03-1002 </papid>with settings as described in the wmt 2011 guide.</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
we report two translation measures: bleu (papineni et al 2002) <papid> P02-1040 </papid>and meteor 1.3 (denkowski and lavie, 2011).<papid> W11-2107 </papid></citsent>
<aftsection>
<nextsent>all results are lower cased and tokenized, measured with five independent runs of mert (och and ney, 2003) <papid> J03-1002 </papid>and multeval (clark et al 2011) for re sampling and significance testing.we compare three baselines and four translation model mixture techniques.</nextsent>
<nextsent>the three baselines are purely in-domain model, purely outof-domain model, and model trained on the concatenation of the two, which corresponds to equation 3 with uniform weights.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I1991">
<title id=" E12-1055.xml">perplexity minimization for translation model domain adaptation in statistical machine translation </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>in terms of tools and techniques used, we mostly adhere to the work flow described for the wmt 2011 baseline system9.
</prevsent>
<prevsent>the main tools are moses (koehn et al 2007), <papid> P07-2045 </papid>srilm (stolcke, 2002), and giza++ (och and ney, 2003), <papid> J03-1002 </papid>with settings as described in the wmt 2011 guide.</prevsent>
</prevsection>
<citsent citstr=" W11-2107 ">
we report two translation measures: bleu (papineni et al 2002) <papid> P02-1040 </papid>and meteor 1.3 (denkowski and lavie, 2011).<papid> W11-2107 </papid></citsent>
<aftsection>
<nextsent>all results are lower cased and tokenized, measured with five independent runs of mert (och and ney, 2003) <papid> J03-1002 </papid>and multeval (clark et al 2011) for re sampling and significance testing.we compare three baselines and four translation model mixture techniques.</nextsent>
<nextsent>the three baselines are purely in-domain model, purely outof-domain model, and model trained on the concatenation of the two, which corresponds to equation 3 with uniform weights.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2001">
<title id=" E09-1017.xml">predicting the fluency of text with shallow structural features case studies of machine translation and human written text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this finding suggests that developing dedicated, task-independent corpus of fluency judgments will be beneficial for further investigations of the problem.
</prevsent>
<prevsent>numerous natural language applications involve the task of producing fluent text.
</prevsent>
</prevsection>
<citsent citstr=" P98-1116 ">
this is core problem for surface realization in natural language generation (langkilde and knight, 1998; <papid> P98-1116 </papid>bangalore and rambow, 2000), <papid> C00-1007 </papid>as well as an important step in machine translation.</citsent>
<aftsection>
<nextsent>considerations of sentence fluency are also key in sentence simplification (siddharthan, 2003), sentence compression (jing, 2000; <papid> A00-1043 </papid>knight and marcu, 2002; clarke and lapata, 2006; <papid> P06-1048 </papid>mcdonald, 2006; <papid> E06-1038 </papid>turner and charniak, 2005; <papid> P05-1036 </papid>galley and mckeown, 2007), <papid> N07-1023 </papid>text re-generation for summarization (daume?</nextsent>
<nextsent>iii and marcu, 2004; barzilay and mckeown, 2005; <papid> J05-3002 </papid>wan et al, 2005) <papid> W05-1628 </papid>and headline generation (banko et al, 2000; <papid> P00-1041 </papid>zajic et al, 2007; soricut and marcu, 2007).despite its importance for these popular applications, the factors contributing to sentence level fluency have not been researched indepth.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2002">
<title id=" E09-1017.xml">predicting the fluency of text with shallow structural features case studies of machine translation and human written text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this finding suggests that developing dedicated, task-independent corpus of fluency judgments will be beneficial for further investigations of the problem.
</prevsent>
<prevsent>numerous natural language applications involve the task of producing fluent text.
</prevsent>
</prevsection>
<citsent citstr=" C00-1007 ">
this is core problem for surface realization in natural language generation (langkilde and knight, 1998; <papid> P98-1116 </papid>bangalore and rambow, 2000), <papid> C00-1007 </papid>as well as an important step in machine translation.</citsent>
<aftsection>
<nextsent>considerations of sentence fluency are also key in sentence simplification (siddharthan, 2003), sentence compression (jing, 2000; <papid> A00-1043 </papid>knight and marcu, 2002; clarke and lapata, 2006; <papid> P06-1048 </papid>mcdonald, 2006; <papid> E06-1038 </papid>turner and charniak, 2005; <papid> P05-1036 </papid>galley and mckeown, 2007), <papid> N07-1023 </papid>text re-generation for summarization (daume?</nextsent>
<nextsent>iii and marcu, 2004; barzilay and mckeown, 2005; <papid> J05-3002 </papid>wan et al, 2005) <papid> W05-1628 </papid>and headline generation (banko et al, 2000; <papid> P00-1041 </papid>zajic et al, 2007; soricut and marcu, 2007).despite its importance for these popular applications, the factors contributing to sentence level fluency have not been researched indepth.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2003">
<title id=" E09-1017.xml">predicting the fluency of text with shallow structural features case studies of machine translation and human written text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>numerous natural language applications involve the task of producing fluent text.
</prevsent>
<prevsent>this is core problem for surface realization in natural language generation (langkilde and knight, 1998; <papid> P98-1116 </papid>bangalore and rambow, 2000), <papid> C00-1007 </papid>as well as an important step in machine translation.</prevsent>
</prevsection>
<citsent citstr=" A00-1043 ">
considerations of sentence fluency are also key in sentence simplification (siddharthan, 2003), sentence compression (jing, 2000; <papid> A00-1043 </papid>knight and marcu, 2002; clarke and lapata, 2006; <papid> P06-1048 </papid>mcdonald, 2006; <papid> E06-1038 </papid>turner and charniak, 2005; <papid> P05-1036 </papid>galley and mckeown, 2007), <papid> N07-1023 </papid>text re-generation for summarization (daume?</citsent>
<aftsection>
<nextsent>iii and marcu, 2004; barzilay and mckeown, 2005; <papid> J05-3002 </papid>wan et al, 2005) <papid> W05-1628 </papid>and headline generation (banko et al, 2000; <papid> P00-1041 </papid>zajic et al, 2007; soricut and marcu, 2007).despite its importance for these popular applications, the factors contributing to sentence level fluency have not been researched indepth.</nextsent>
<nextsent>much more attention has been devoted to discourse-levelconstraints on adjacent sentences indicative of coherence and good text flow (lapata, 2003; <papid> P03-1069 </papid>barzilay and lapata, 2008; karamanis et al, to appear).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2004">
<title id=" E09-1017.xml">predicting the fluency of text with shallow structural features case studies of machine translation and human written text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>numerous natural language applications involve the task of producing fluent text.
</prevsent>
<prevsent>this is core problem for surface realization in natural language generation (langkilde and knight, 1998; <papid> P98-1116 </papid>bangalore and rambow, 2000), <papid> C00-1007 </papid>as well as an important step in machine translation.</prevsent>
</prevsection>
<citsent citstr=" P06-1048 ">
considerations of sentence fluency are also key in sentence simplification (siddharthan, 2003), sentence compression (jing, 2000; <papid> A00-1043 </papid>knight and marcu, 2002; clarke and lapata, 2006; <papid> P06-1048 </papid>mcdonald, 2006; <papid> E06-1038 </papid>turner and charniak, 2005; <papid> P05-1036 </papid>galley and mckeown, 2007), <papid> N07-1023 </papid>text re-generation for summarization (daume?</citsent>
<aftsection>
<nextsent>iii and marcu, 2004; barzilay and mckeown, 2005; <papid> J05-3002 </papid>wan et al, 2005) <papid> W05-1628 </papid>and headline generation (banko et al, 2000; <papid> P00-1041 </papid>zajic et al, 2007; soricut and marcu, 2007).despite its importance for these popular applications, the factors contributing to sentence level fluency have not been researched indepth.</nextsent>
<nextsent>much more attention has been devoted to discourse-levelconstraints on adjacent sentences indicative of coherence and good text flow (lapata, 2003; <papid> P03-1069 </papid>barzilay and lapata, 2008; karamanis et al, to appear).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2005">
<title id=" E09-1017.xml">predicting the fluency of text with shallow structural features case studies of machine translation and human written text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>numerous natural language applications involve the task of producing fluent text.
</prevsent>
<prevsent>this is core problem for surface realization in natural language generation (langkilde and knight, 1998; <papid> P98-1116 </papid>bangalore and rambow, 2000), <papid> C00-1007 </papid>as well as an important step in machine translation.</prevsent>
</prevsection>
<citsent citstr=" E06-1038 ">
considerations of sentence fluency are also key in sentence simplification (siddharthan, 2003), sentence compression (jing, 2000; <papid> A00-1043 </papid>knight and marcu, 2002; clarke and lapata, 2006; <papid> P06-1048 </papid>mcdonald, 2006; <papid> E06-1038 </papid>turner and charniak, 2005; <papid> P05-1036 </papid>galley and mckeown, 2007), <papid> N07-1023 </papid>text re-generation for summarization (daume?</citsent>
<aftsection>
<nextsent>iii and marcu, 2004; barzilay and mckeown, 2005; <papid> J05-3002 </papid>wan et al, 2005) <papid> W05-1628 </papid>and headline generation (banko et al, 2000; <papid> P00-1041 </papid>zajic et al, 2007; soricut and marcu, 2007).despite its importance for these popular applications, the factors contributing to sentence level fluency have not been researched indepth.</nextsent>
<nextsent>much more attention has been devoted to discourse-levelconstraints on adjacent sentences indicative of coherence and good text flow (lapata, 2003; <papid> P03-1069 </papid>barzilay and lapata, 2008; karamanis et al, to appear).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2006">
<title id=" E09-1017.xml">predicting the fluency of text with shallow structural features case studies of machine translation and human written text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>numerous natural language applications involve the task of producing fluent text.
</prevsent>
<prevsent>this is core problem for surface realization in natural language generation (langkilde and knight, 1998; <papid> P98-1116 </papid>bangalore and rambow, 2000), <papid> C00-1007 </papid>as well as an important step in machine translation.</prevsent>
</prevsection>
<citsent citstr=" P05-1036 ">
considerations of sentence fluency are also key in sentence simplification (siddharthan, 2003), sentence compression (jing, 2000; <papid> A00-1043 </papid>knight and marcu, 2002; clarke and lapata, 2006; <papid> P06-1048 </papid>mcdonald, 2006; <papid> E06-1038 </papid>turner and charniak, 2005; <papid> P05-1036 </papid>galley and mckeown, 2007), <papid> N07-1023 </papid>text re-generation for summarization (daume?</citsent>
<aftsection>
<nextsent>iii and marcu, 2004; barzilay and mckeown, 2005; <papid> J05-3002 </papid>wan et al, 2005) <papid> W05-1628 </papid>and headline generation (banko et al, 2000; <papid> P00-1041 </papid>zajic et al, 2007; soricut and marcu, 2007).despite its importance for these popular applications, the factors contributing to sentence level fluency have not been researched indepth.</nextsent>
<nextsent>much more attention has been devoted to discourse-levelconstraints on adjacent sentences indicative of coherence and good text flow (lapata, 2003; <papid> P03-1069 </papid>barzilay and lapata, 2008; karamanis et al, to appear).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2007">
<title id=" E09-1017.xml">predicting the fluency of text with shallow structural features case studies of machine translation and human written text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>numerous natural language applications involve the task of producing fluent text.
</prevsent>
<prevsent>this is core problem for surface realization in natural language generation (langkilde and knight, 1998; <papid> P98-1116 </papid>bangalore and rambow, 2000), <papid> C00-1007 </papid>as well as an important step in machine translation.</prevsent>
</prevsection>
<citsent citstr=" N07-1023 ">
considerations of sentence fluency are also key in sentence simplification (siddharthan, 2003), sentence compression (jing, 2000; <papid> A00-1043 </papid>knight and marcu, 2002; clarke and lapata, 2006; <papid> P06-1048 </papid>mcdonald, 2006; <papid> E06-1038 </papid>turner and charniak, 2005; <papid> P05-1036 </papid>galley and mckeown, 2007), <papid> N07-1023 </papid>text re-generation for summarization (daume?</citsent>
<aftsection>
<nextsent>iii and marcu, 2004; barzilay and mckeown, 2005; <papid> J05-3002 </papid>wan et al, 2005) <papid> W05-1628 </papid>and headline generation (banko et al, 2000; <papid> P00-1041 </papid>zajic et al, 2007; soricut and marcu, 2007).despite its importance for these popular applications, the factors contributing to sentence level fluency have not been researched indepth.</nextsent>
<nextsent>much more attention has been devoted to discourse-levelconstraints on adjacent sentences indicative of coherence and good text flow (lapata, 2003; <papid> P03-1069 </papid>barzilay and lapata, 2008; karamanis et al, to appear).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2008">
<title id=" E09-1017.xml">predicting the fluency of text with shallow structural features case studies of machine translation and human written text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this is core problem for surface realization in natural language generation (langkilde and knight, 1998; <papid> P98-1116 </papid>bangalore and rambow, 2000), <papid> C00-1007 </papid>as well as an important step in machine translation.</prevsent>
<prevsent>considerations of sentence fluency are also key in sentence simplification (siddharthan, 2003), sentence compression (jing, 2000; <papid> A00-1043 </papid>knight and marcu, 2002; clarke and lapata, 2006; <papid> P06-1048 </papid>mcdonald, 2006; <papid> E06-1038 </papid>turner and charniak, 2005; <papid> P05-1036 </papid>galley and mckeown, 2007), <papid> N07-1023 </papid>text re-generation for summarization (daume?</prevsent>
</prevsection>
<citsent citstr=" J05-3002 ">
iii and marcu, 2004; barzilay and mckeown, 2005; <papid> J05-3002 </papid>wan et al, 2005) <papid> W05-1628 </papid>and headline generation (banko et al, 2000; <papid> P00-1041 </papid>zajic et al, 2007; soricut and marcu, 2007).despite its importance for these popular applications, the factors contributing to sentence level fluency have not been researched indepth.</citsent>
<aftsection>
<nextsent>much more attention has been devoted to discourse-levelconstraints on adjacent sentences indicative of coherence and good text flow (lapata, 2003; <papid> P03-1069 </papid>barzilay and lapata, 2008; karamanis et al, to appear).</nextsent>
<nextsent>in many applications fluency is assessed in combination with other qualities.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2009">
<title id=" E09-1017.xml">predicting the fluency of text with shallow structural features case studies of machine translation and human written text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this is core problem for surface realization in natural language generation (langkilde and knight, 1998; <papid> P98-1116 </papid>bangalore and rambow, 2000), <papid> C00-1007 </papid>as well as an important step in machine translation.</prevsent>
<prevsent>considerations of sentence fluency are also key in sentence simplification (siddharthan, 2003), sentence compression (jing, 2000; <papid> A00-1043 </papid>knight and marcu, 2002; clarke and lapata, 2006; <papid> P06-1048 </papid>mcdonald, 2006; <papid> E06-1038 </papid>turner and charniak, 2005; <papid> P05-1036 </papid>galley and mckeown, 2007), <papid> N07-1023 </papid>text re-generation for summarization (daume?</prevsent>
</prevsection>
<citsent citstr=" W05-1628 ">
iii and marcu, 2004; barzilay and mckeown, 2005; <papid> J05-3002 </papid>wan et al, 2005) <papid> W05-1628 </papid>and headline generation (banko et al, 2000; <papid> P00-1041 </papid>zajic et al, 2007; soricut and marcu, 2007).despite its importance for these popular applications, the factors contributing to sentence level fluency have not been researched indepth.</citsent>
<aftsection>
<nextsent>much more attention has been devoted to discourse-levelconstraints on adjacent sentences indicative of coherence and good text flow (lapata, 2003; <papid> P03-1069 </papid>barzilay and lapata, 2008; karamanis et al, to appear).</nextsent>
<nextsent>in many applications fluency is assessed in combination with other qualities.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2012">
<title id=" E09-1017.xml">predicting the fluency of text with shallow structural features case studies of machine translation and human written text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this is core problem for surface realization in natural language generation (langkilde and knight, 1998; <papid> P98-1116 </papid>bangalore and rambow, 2000), <papid> C00-1007 </papid>as well as an important step in machine translation.</prevsent>
<prevsent>considerations of sentence fluency are also key in sentence simplification (siddharthan, 2003), sentence compression (jing, 2000; <papid> A00-1043 </papid>knight and marcu, 2002; clarke and lapata, 2006; <papid> P06-1048 </papid>mcdonald, 2006; <papid> E06-1038 </papid>turner and charniak, 2005; <papid> P05-1036 </papid>galley and mckeown, 2007), <papid> N07-1023 </papid>text re-generation for summarization (daume?</prevsent>
</prevsection>
<citsent citstr=" P00-1041 ">
iii and marcu, 2004; barzilay and mckeown, 2005; <papid> J05-3002 </papid>wan et al, 2005) <papid> W05-1628 </papid>and headline generation (banko et al, 2000; <papid> P00-1041 </papid>zajic et al, 2007; soricut and marcu, 2007).despite its importance for these popular applications, the factors contributing to sentence level fluency have not been researched indepth.</citsent>
<aftsection>
<nextsent>much more attention has been devoted to discourse-levelconstraints on adjacent sentences indicative of coherence and good text flow (lapata, 2003; <papid> P03-1069 </papid>barzilay and lapata, 2008; karamanis et al, to appear).</nextsent>
<nextsent>in many applications fluency is assessed in combination with other qualities.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2013">
<title id=" E09-1017.xml">predicting the fluency of text with shallow structural features case studies of machine translation and human written text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>considerations of sentence fluency are also key in sentence simplification (siddharthan, 2003), sentence compression (jing, 2000; <papid> A00-1043 </papid>knight and marcu, 2002; clarke and lapata, 2006; <papid> P06-1048 </papid>mcdonald, 2006; <papid> E06-1038 </papid>turner and charniak, 2005; <papid> P05-1036 </papid>galley and mckeown, 2007), <papid> N07-1023 </papid>text re-generation for summarization (daume?</prevsent>
<prevsent>iii and marcu, 2004; barzilay and mckeown, 2005; <papid> J05-3002 </papid>wan et al, 2005) <papid> W05-1628 </papid>and headline generation (banko et al, 2000; <papid> P00-1041 </papid>zajic et al, 2007; soricut and marcu, 2007).despite its importance for these popular applications, the factors contributing to sentence level fluency have not been researched indepth.</prevsent>
</prevsection>
<citsent citstr=" P03-1069 ">
much more attention has been devoted to discourse-levelconstraints on adjacent sentences indicative of coherence and good text flow (lapata, 2003; <papid> P03-1069 </papid>barzilay and lapata, 2008; karamanis et al, to appear).</citsent>
<aftsection>
<nextsent>in many applications fluency is assessed in combination with other qualities.
</nextsent>
<nextsent>for example, in machine translation evaluation, approaches suchas bleu (papineni et al, 2002) <papid> P02-1040 </papid>use n-gram overlap comparisons with model to judge overall goodness?, with higher n-grams meant to capture fluency considerations.</nextsent>
<nextsent>more sophisticated waysto compare system production and model involve the use of syntax, but even in these cases fluency is only indirectly assessed and the main advantage of the use of syntax is better estimation ofthe semantic overlap between model and an output.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2014">
<title id=" E09-1017.xml">predicting the fluency of text with shallow structural features case studies of machine translation and human written text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>much more attention has been devoted to discourse-levelconstraints on adjacent sentences indicative of coherence and good text flow (lapata, 2003; <papid> P03-1069 </papid>barzilay and lapata, 2008; karamanis et al, to appear).</prevsent>
<prevsent>in many applications fluency is assessed in combination with other qualities.</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
for example, in machine translation evaluation, approaches suchas bleu (papineni et al, 2002) <papid> P02-1040 </papid>use n-gram overlap comparisons with model to judge overall goodness?, with higher n-grams meant to capture fluency considerations.</citsent>
<aftsection>
<nextsent>more sophisticated waysto compare system production and model involve the use of syntax, but even in these cases fluency is only indirectly assessed and the main advantage of the use of syntax is better estimation ofthe semantic overlap between model and an output.
</nextsent>
<nextsent>similarly, the metrics proposed for text generation by (bangalore et al, 2000) (<papid> W00-1401 </papid>simple accuracy,generation accuracy) are based on string-edit distance from an ideal output.</nextsent>
<nextsent>in contrast, the work of (wan et al, 2005) <papid> W05-1628 </papid>and (mutton et al, 2007) <papid> P07-1044 </papid>directly sets as goalthe assessment of sentence-level fluency, regard less of content.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2016">
<title id=" E09-1017.xml">predicting the fluency of text with shallow structural features case studies of machine translation and human written text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for example, in machine translation evaluation, approaches suchas bleu (papineni et al, 2002) <papid> P02-1040 </papid>use n-gram overlap comparisons with model to judge overall goodness?, with higher n-grams meant to capture fluency considerations.</prevsent>
<prevsent>more sophisticated waysto compare system production and model involve the use of syntax, but even in these cases fluency is only indirectly assessed and the main advantage of the use of syntax is better estimation ofthe semantic overlap between model and an output.</prevsent>
</prevsection>
<citsent citstr=" W00-1401 ">
similarly, the metrics proposed for text generation by (bangalore et al, 2000) (<papid> W00-1401 </papid>simple accuracy,generation accuracy) are based on string-edit distance from an ideal output.</citsent>
<aftsection>
<nextsent>in contrast, the work of (wan et al, 2005) <papid> W05-1628 </papid>and (mutton et al, 2007) <papid> P07-1044 </papid>directly sets as goalthe assessment of sentence-level fluency, regard less of content.</nextsent>
<nextsent>in (wan et al, 2005) <papid> W05-1628 </papid>the main premise is that syntactic information from parser can more robustly capture fluency than language models, giving more direct indications of the degree of ungrammaticality.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2020">
<title id=" E09-1017.xml">predicting the fluency of text with shallow structural features case studies of machine translation and human written text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>more sophisticated waysto compare system production and model involve the use of syntax, but even in these cases fluency is only indirectly assessed and the main advantage of the use of syntax is better estimation ofthe semantic overlap between model and an output.
</prevsent>
<prevsent>similarly, the metrics proposed for text generation by (bangalore et al, 2000) (<papid> W00-1401 </papid>simple accuracy,generation accuracy) are based on string-edit distance from an ideal output.</prevsent>
</prevsection>
<citsent citstr=" P07-1044 ">
in contrast, the work of (wan et al, 2005) <papid> W05-1628 </papid>and (mutton et al, 2007) <papid> P07-1044 </papid>directly sets as goalthe assessment of sentence-level fluency, regard less of content.</citsent>
<aftsection>
<nextsent>in (wan et al, 2005) <papid> W05-1628 </papid>the main premise is that syntactic information from parser can more robustly capture fluency than language models, giving more direct indications of the degree of ungrammaticality.</nextsent>
<nextsent>the idea is extended in (mutton et al, 2007), <papid> P07-1044 </papid>where four parsers are used 139 and artificially generated sentences with varying level of fluency are evaluated with impressive success.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2025">
<title id=" E09-1017.xml">predicting the fluency of text with shallow structural features case studies of machine translation and human written text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in (wan et al, 2005) <papid> W05-1628 </papid>the main premise is that syntactic information from parser can more robustly capture fluency than language models, giving more direct indications of the degree of ungrammaticality.</prevsent>
<prevsent>the idea is extended in (mutton et al, 2007), <papid> P07-1044 </papid>where four parsers are used 139 and artificially generated sentences with varying level of fluency are evaluated with impressive success.</prevsent>
</prevsection>
<citsent citstr=" C08-1145 ">
the fluency models hold promise for actual improvements in machine translation output quality (zwarts and dras, 2008).<papid> C08-1145 </papid></citsent>
<aftsection>
<nextsent>in that work,only simple parser features are used for the prediction of fluency, but no actual syntactic properties of the sentences.
</nextsent>
<nextsent>but certainly, problems with sentence fluency are expected to be manifested in syntax.
</nextsent>
<nextsent>we would expect for example that syntactic tree features that capture common parse configurations and that are used in discriminative parsing (collins and koo, 2005; <papid> J05-1003 </papid>charniak and johnson, 2005; <papid> P05-1022 </papid>huang, 2008) <papid> P08-1067 </papid>should be useful for predicting sentence fluency as well.</nextsent>
<nextsent>indeed, early work has demonstrated that syntactic features, and branching properties in particular,are helpful features for automatically distinguishing human translations from machine translations (corston-oliver et al, 2001).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2026">
<title id=" E09-1017.xml">predicting the fluency of text with shallow structural features case studies of machine translation and human written text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in that work,only simple parser features are used for the prediction of fluency, but no actual syntactic properties of the sentences.
</prevsent>
<prevsent>but certainly, problems with sentence fluency are expected to be manifested in syntax.
</prevsent>
</prevsection>
<citsent citstr=" J05-1003 ">
we would expect for example that syntactic tree features that capture common parse configurations and that are used in discriminative parsing (collins and koo, 2005; <papid> J05-1003 </papid>charniak and johnson, 2005; <papid> P05-1022 </papid>huang, 2008) <papid> P08-1067 </papid>should be useful for predicting sentence fluency as well.</citsent>
<aftsection>
<nextsent>indeed, early work has demonstrated that syntactic features, and branching properties in particular,are helpful features for automatically distinguishing human translations from machine translations (corston-oliver et al, 2001).
</nextsent>
<nextsent>the exploration of branching properties of human and machine translations was motivated by the observations during failure analysis that mt system output tends tofavor right-branching structures over noun compounding.
</nextsent>
<nextsent>branching preference mismatch manifest themselves in the english output when translating from languages whose branching properties are radically different from english.
</nextsent>
<nextsent>accuracy close to 80% was achieved for distinguishing human translations from machine translations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2027">
<title id=" E09-1017.xml">predicting the fluency of text with shallow structural features case studies of machine translation and human written text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in that work,only simple parser features are used for the prediction of fluency, but no actual syntactic properties of the sentences.
</prevsent>
<prevsent>but certainly, problems with sentence fluency are expected to be manifested in syntax.
</prevsent>
</prevsection>
<citsent citstr=" P05-1022 ">
we would expect for example that syntactic tree features that capture common parse configurations and that are used in discriminative parsing (collins and koo, 2005; <papid> J05-1003 </papid>charniak and johnson, 2005; <papid> P05-1022 </papid>huang, 2008) <papid> P08-1067 </papid>should be useful for predicting sentence fluency as well.</citsent>
<aftsection>
<nextsent>indeed, early work has demonstrated that syntactic features, and branching properties in particular,are helpful features for automatically distinguishing human translations from machine translations (corston-oliver et al, 2001).
</nextsent>
<nextsent>the exploration of branching properties of human and machine translations was motivated by the observations during failure analysis that mt system output tends tofavor right-branching structures over noun compounding.
</nextsent>
<nextsent>branching preference mismatch manifest themselves in the english output when translating from languages whose branching properties are radically different from english.
</nextsent>
<nextsent>accuracy close to 80% was achieved for distinguishing human translations from machine translations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2028">
<title id=" E09-1017.xml">predicting the fluency of text with shallow structural features case studies of machine translation and human written text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in that work,only simple parser features are used for the prediction of fluency, but no actual syntactic properties of the sentences.
</prevsent>
<prevsent>but certainly, problems with sentence fluency are expected to be manifested in syntax.
</prevsent>
</prevsection>
<citsent citstr=" P08-1067 ">
we would expect for example that syntactic tree features that capture common parse configurations and that are used in discriminative parsing (collins and koo, 2005; <papid> J05-1003 </papid>charniak and johnson, 2005; <papid> P05-1022 </papid>huang, 2008) <papid> P08-1067 </papid>should be useful for predicting sentence fluency as well.</citsent>
<aftsection>
<nextsent>indeed, early work has demonstrated that syntactic features, and branching properties in particular,are helpful features for automatically distinguishing human translations from machine translations (corston-oliver et al, 2001).
</nextsent>
<nextsent>the exploration of branching properties of human and machine translations was motivated by the observations during failure analysis that mt system output tends tofavor right-branching structures over noun compounding.
</nextsent>
<nextsent>branching preference mismatch manifest themselves in the english output when translating from languages whose branching properties are radically different from english.
</nextsent>
<nextsent>accuracy close to 80% was achieved for distinguishing human translations from machine translations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2029">
<title id=" E09-1017.xml">predicting the fluency of text with shallow structural features case studies of machine translation and human written text </title>
<section> features.  </section>
<citcontext>
<prevsection>
<prevsent>perceived sentence fluency is influenced by manyfactors.
</prevsent>
<prevsent>the way the sentence fits in the context of surrounding sentences is one obvious factor (barzilay and lapata, 2008).
</prevsent>
</prevsection>
<citsent citstr=" P05-1065 ">
another well-knownfactor is vocabulary use: the presence of uncommon difficult words are known to pose problem sto readers and to render text less readable (collinsthompson and callan, 2004; schwarm and ostendorf, 2005).<papid> P05-1065 </papid></citsent>
<aftsection>
<nextsent>but these discourse- and vocabulary level features measure properties at granularities different from the sentence level.
</nextsent>
<nextsent>syntactic sentence level features have not been investigated as stand-alone class, as has been 140 done for the other types of features.
</nextsent>
<nextsent>this is why we constrain our study to syntactic features alone, and do not discuss discourse and language model features that have been extensively studied in prior work on coherence and readability.in our work, instead of looking at the syntactic structures present in the sentences, e.g. the syntactic rules used, we use surface statistics of phrase length and types of modification.
</nextsent>
<nextsent>the sentences were parsed with charniaks parser (char niak, 2000) <papid> A00-2018 </papid>in order to calculate these features.sentence length is the number of words in sen tence.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2030">
<title id=" E09-1017.xml">predicting the fluency of text with shallow structural features case studies of machine translation and human written text </title>
<section> features.  </section>
<citcontext>
<prevsection>
<prevsent>syntactic sentence level features have not been investigated as stand-alone class, as has been 140 done for the other types of features.
</prevsent>
<prevsent>this is why we constrain our study to syntactic features alone, and do not discuss discourse and language model features that have been extensively studied in prior work on coherence and readability.in our work, instead of looking at the syntactic structures present in the sentences, e.g. the syntactic rules used, we use surface statistics of phrase length and types of modification.
</prevsent>
</prevsection>
<citsent citstr=" A00-2018 ">
the sentences were parsed with charniaks parser (char niak, 2000) <papid> A00-2018 </papid>in order to calculate these features.sentence length is the number of words in sen tence.</citsent>
<aftsection>
<nextsent>evaluation metrics such as bleu (papineni et al, 2002) <papid> P02-1040 </papid>have built-in preference for shorter translations.</nextsent>
<nextsent>in general one would expect that shorter sentences are easier to read and thus are perceived as more fluent.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2033">
<title id=" E09-1017.xml">predicting the fluency of text with shallow structural features case studies of machine translation and human written text </title>
<section> applications to human written text.  </section>
<citcontext>
<prevsection>
<prevsent>wall street journal texts the goal we set out in the beginning of this paper was to derive predictive model of sentence fluency from data coming from mt evaluations.
</prevsent>
<prevsent>in the previous sections, we demonstrated that indeed structural features can enable us to perform this task very accurately in the context of machine translation.
</prevsent>
</prevsection>
<citsent citstr=" D08-1020 ">
but will the models conveniently trained on data from mt evaluation be at all capable to identify sentences in human-writtentext that are not fluent and are difficult to under standto answer this question, we performed an additional experiment on 30 wall street journal articles from the penn treebank that were previously used in experiments for assessing overall text quality (pitler and nenkova, 2008).<papid> D08-1020 </papid></citsent>
<aftsection>
<nextsent>the articles were chosen at random and comprised total of 290 sentences.
</nextsent>
<nextsent>one human assessor was asked to read each sentence and mark the ones that seemed dis fluent because they were hard to comprehend.
</nextsent>
<nextsent>these were sentences that needed to be read more than once in order to fully understand the information conveyed in them.
</nextsent>
<nextsent>there were 52such sentences.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2042">
<title id=" E06-3006.xml">a two stage approach to retrieving answers for howto questions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>without noticing this difference, treating the two parts equally in the retrieving process will get many noisy documents.
</prevsent>
<prevsent>many information requests seem to show such structure, with one part identifying specific topic and another part constraining the kind of information required about this topic (yin and power, 2005).
</prevsent>
</prevsection>
<citsent citstr=" E99-1034 ">
the second part is often omitted when selecting retrieval terms from the request to construct an effective query for an ir system, such as in picard (1999).<papid> E99-1034 </papid></citsent>
<aftsection>
<nextsent>the first point given above suggests that using cues such as first?
</nextsent>
<nextsent>and next?
</nextsent>
<nextsent>to expand the initial query may help in retrieving more relevant documents.
</nextsent>
<nextsent>expansion terms can be generated automatically by query expansion techniques.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2043">
<title id=" E06-3006.xml">a two stage approach to retrieving answers for howto questions </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>a research area that is closely related to ir is question answering (qa), the differences being a) the input of qa system is question rather than few key words; b) qa system aims to extract answers to question rather than retrieving relevant documents only.
</prevsent>
<prevsent>most qa systems do adopt two-stage architecture (if not consider the initial question analysis stage), i.e., perform ir with few content words extracted from the query to locate documents likely to contain an answer and then use information extraction (ie) to find the text snippets that match the question type (hovy et al, 2001; elworthy, 2000).
</prevsent>
</prevsection>
<citsent citstr=" W04-0505 ">
however, most question answering systems target factoid questions ? the research of non-factoid questions started only few years ago but limited to several kinds, such as definitional questions (xu et al, 2003) and questions asking for biographies (tsur et al, 2004).<papid> W04-0505 </papid></citsent>
<aftsection>
<nextsent>only few studies have addressed procedural questions.
</nextsent>
<nextsent>murdok and croft (2002) distinguish between task-oriented questions?
</nextsent>
<nextsent>(i.e., ask about process) and fact-oriented questions?
</nextsent>
<nextsent>(i.e., ask about fact) and present method to automatically classify questions into these two categories.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2044">
<title id=" E87-1030.xml">natural and simulated pointing </title>
<section> int roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>acord in-vestigates pointing actions with respect various two- dimensional objects, e.g. map of the planetary system (hanne, hoepelmann, and f~ihnrich 1986) and form for university registration (wetzel, hanne, and hoe- pelmann 1987).
</prevsent>
<prevsent>5.3 pointing actions in tagtilus.
</prevsent>
</prevsection>
<citsent citstr=" C86-1085 ">
one aim of xtra is the integration of (typed) ver-bal descriptions and pointing gestures (currently real-ized by mouse-clicks) for referent identification (kobsa et al 1986).<papid> C86-1085 </papid></citsent>
<aftsection>
<nextsent>the user should be able to efficiently refer to objects on the screen, even when s/he uses underspecified descriptions and/or imprecise pointing gestures (allgayer, reddig 1986).
</nextsent>
<nextsent>hence the process of specifying referents is speeded up and requires less knowledge of specialist terms.
</nextsent>
<nextsent>the deictic component of xtra (called tac- tilus) is completely implemented ona symbol ics lisp machine (allgayer 1086).
</nextsent>
<nextsent>it offers four types of point-ing gestures which differ inaccuracy.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2048">
<title id=" E12-1023.xml">large margin learning of sub modular summarization models </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this paper we focus on extractive multi-document summarization, where the final summary is subset of the sentences from multiple input documents.
</prevsent>
<prevsent>in this way, extractive summarization avoids the hard problem of generating well-formed natural-language sentences, since only existing sentences from the input documents are presented as part of the summary.
</prevsent>
</prevsection>
<citsent citstr=" N10-1134 ">
a current state-of-the-art method for document summarization was recently proposed by lin andbilmes (2010), <papid> N10-1134 </papid>using sub modular scoring function based on inter-sentence similarity.</citsent>
<aftsection>
<nextsent>on the one hand, this scoring function rewards summaries that are similar to many sentences in the original documents (i.e. promotes coverage).
</nextsent>
<nextsent>on the other hand, it penalizes summaries that contain sentences that are similar to each other (i.e. discourages redundancy).
</nextsent>
<nextsent>while obtaining the exact summary that optimizes the objective is computationally hard, they show that greedy algorithm is guaranteed to compute good approximation.
</nextsent>
<nextsent>however, their work does not address how to select good inter-sentence similarity measure,leaving this problem as well as selecting an appropriate trade-off between coverage and redundancy to manual tuning.to overcome this problem, we propose supervised learning method that can learn boththe similarity measure as well as the coverage/reduncancy trade-off from training data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2051">
<title id=" E12-1023.xml">large margin learning of sub modular summarization models </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>starting with unsupervised methods, one of the widely known approaches is maximal marginal relevance (mmr) (carbonell and goldstein, 1998).
</prevsent>
<prevsent>it uses greedy approach for selection and considers the trade-off between relevance and redundancy.
</prevsent>
</prevsection>
<citsent citstr=" W00-0405 ">
later it was extended (goldstein et al 2000) <papid> W00-0405 </papid>to support multi document settings by incorporating additional information available in this case.</citsent>
<aftsection>
<nextsent>good results can be achieved by reformulating this as knapsack packing problem and solving it using dynamic programing (mcdonald, 2007).
</nextsent>
<nextsent>alternatively, wecan use annotated phrases as textual units and select subset that covers most concepts present in the input (filatova and hatzivassiloglou, 2004)(<papid> W04-1017 </papid>which can also be achieved by our coverage scoring function if it is extended with appropriate fea tures).a popular stochastic graph-based summarization method is lexrank (erkan and radev, 2004).</nextsent>
<nextsent>it computes sentence importance based on the concept of eigenvector centrality in graph of sentence similarities.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2052">
<title id=" E12-1023.xml">large margin learning of sub modular summarization models </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>later it was extended (goldstein et al 2000) <papid> W00-0405 </papid>to support multi document settings by incorporating additional information available in this case.</prevsent>
<prevsent>good results can be achieved by reformulating this as knapsack packing problem and solving it using dynamic programing (mcdonald, 2007).</prevsent>
</prevsection>
<citsent citstr=" W04-1017 ">
alternatively, wecan use annotated phrases as textual units and select subset that covers most concepts present in the input (filatova and hatzivassiloglou, 2004)(<papid> W04-1017 </papid>which can also be achieved by our coverage scoring function if it is extended with appropriate fea tures).a popular stochastic graph-based summarization method is lexrank (erkan and radev, 2004).</citsent>
<aftsection>
<nextsent>it computes sentence importance based on the concept of eigenvector centrality in graph of sentence similarities.
</nextsent>
<nextsent>similarly, text rank (mihalcea and tarau, 2004) <papid> W04-3252 </papid>is also graph based ranking system for identification of important sentences in document by using sentence similarity and page rank (brin and page, 1998).</nextsent>
<nextsent>sentence extraction can also be implemented using other graph based scoring approaches (mihalcea,2004) <papid> P04-3020 </papid>such as hits (kleinberg, 1999) and positional power functions.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2053">
<title id=" E12-1023.xml">large margin learning of sub modular summarization models </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>alternatively, wecan use annotated phrases as textual units and select subset that covers most concepts present in the input (filatova and hatzivassiloglou, 2004)(<papid> W04-1017 </papid>which can also be achieved by our coverage scoring function if it is extended with appropriate fea tures).a popular stochastic graph-based summarization method is lexrank (erkan and radev, 2004).</prevsent>
<prevsent>it computes sentence importance based on the concept of eigenvector centrality in graph of sentence similarities.</prevsent>
</prevsection>
<citsent citstr=" W04-3252 ">
similarly, text rank (mihalcea and tarau, 2004) <papid> W04-3252 </papid>is also graph based ranking system for identification of important sentences in document by using sentence similarity and page rank (brin and page, 1998).</citsent>
<aftsection>
<nextsent>sentence extraction can also be implemented using other graph based scoring approaches (mihalcea,2004) <papid> P04-3020 </papid>such as hits (kleinberg, 1999) and positional power functions.</nextsent>
<nextsent>graph based method scan also be paired with clustering such as in col labsum (wan et al 2007).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2054">
<title id=" E12-1023.xml">large margin learning of sub modular summarization models </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>it computes sentence importance based on the concept of eigenvector centrality in graph of sentence similarities.
</prevsent>
<prevsent>similarly, text rank (mihalcea and tarau, 2004) <papid> W04-3252 </papid>is also graph based ranking system for identification of important sentences in document by using sentence similarity and page rank (brin and page, 1998).</prevsent>
</prevsection>
<citsent citstr=" P04-3020 ">
sentence extraction can also be implemented using other graph based scoring approaches (mihalcea,2004) <papid> P04-3020 </papid>such as hits (kleinberg, 1999) and positional power functions.</citsent>
<aftsection>
<nextsent>graph based method scan also be paired with clustering such as in col labsum (wan et al 2007).
</nextsent>
<nextsent>this approach first uses clustering to obtain document clusters andthen uses graph based algorithm for sentence selection which includes inter and intra-document sentence similarities.
</nextsent>
<nextsent>another clustering-based algorithm (nomoto and matsumoto, 2001) is adiversity-based extension of mmr that finds diversity by clustering and then proceeds to reduce redundancy by selecting representative for each cluster.
</nextsent>
<nextsent>the manually tuned sentence pairwise model (lin and bilmes, 2010; <papid> N10-1134 </papid>lin and bilmes, 2011) <papid> P11-1052 </papid>wetook inspiration from is based on budgeted sub modular optimization.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2057">
<title id=" E12-1023.xml">large margin learning of sub modular summarization models </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>this approach first uses clustering to obtain document clusters andthen uses graph based algorithm for sentence selection which includes inter and intra-document sentence similarities.
</prevsent>
<prevsent>another clustering-based algorithm (nomoto and matsumoto, 2001) is adiversity-based extension of mmr that finds diversity by clustering and then proceeds to reduce redundancy by selecting representative for each cluster.
</prevsent>
</prevsection>
<citsent citstr=" P11-1052 ">
the manually tuned sentence pairwise model (lin and bilmes, 2010; <papid> N10-1134 </papid>lin and bilmes, 2011) <papid> P11-1052 </papid>wetook inspiration from is based on budgeted sub modular optimization.</citsent>
<aftsection>
<nextsent>a summary is produced by maximizing an objective function that includes coverage and redundancy terms.
</nextsent>
<nextsent>coverage is defined as the sum of sentence similarities between the selected summary and the rest of the sentences, while redundancy is the sum of pairwiseintra-summary sentence similarities.
</nextsent>
<nextsent>another approach based on submodularity (qazvinian et al 2010) <papid> C10-1101 </papid>relies on extracting important keyphrasesfrom citation sentences forgiven paper and using them to build the summary.</nextsent>
<nextsent>in the supervised setting, several early methods(kupiec et al 1995) made independent binary decisions whether to include particular sentence in the summary or not.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2058">
<title id=" E12-1023.xml">large margin learning of sub modular summarization models </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>a summary is produced by maximizing an objective function that includes coverage and redundancy terms.
</prevsent>
<prevsent>coverage is defined as the sum of sentence similarities between the selected summary and the rest of the sentences, while redundancy is the sum of pairwiseintra-summary sentence similarities.
</prevsent>
</prevsection>
<citsent citstr=" C10-1101 ">
another approach based on submodularity (qazvinian et al 2010) <papid> C10-1101 </papid>relies on extracting important keyphrasesfrom citation sentences forgiven paper and using them to build the summary.</citsent>
<aftsection>
<nextsent>in the supervised setting, several early methods(kupiec et al 1995) made independent binary decisions whether to include particular sentence in the summary or not.
</nextsent>
<nextsent>this ignores dependencies between sentences and can result in high redundancy.
</nextsent>
<nextsent>the same problem arises when usinglearning-to-rank approaches such as ranking support vector machines, support vector regression and gradient boosted decision trees to select the most relevant sentences for the summary (metzler and kanungo, 2008).
</nextsent>
<nextsent>introducing some dependencies can improve the performance.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2059">
<title id=" E12-1023.xml">large margin learning of sub modular summarization models </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>they use the diversified retrieval method proposed in yue and joachims (2008) for document summarization.
</prevsent>
<prevsent>moreover, they assume that sub topic labels are available so that additional constraints for diversity, coverage and balance canbe added to the structural svm learning problem.
</prevsent>
</prevsection>
<citsent citstr=" W09-1801 ">
in contrast, our approach does not require the knowledge of sub topics (thus allowing us to apply it to wider range of tasks) and avoids adding additional constraints (simplifying the algorithm).furthermore, it can use different sub modular objective functions, for example word coverage and sentence pairwise models described later in this paper.another closely related work also takes max margin discriminative learning approach in the structural svm framework (berg-kirkpatrick et al., 2011) or by using mira (martins and smith, 2009) <papid> W09-1801 </papid>to learn the parameters for summarizinga set of documents.</citsent>
<aftsection>
<nextsent>however, they do not consider sub modular functions, but instead solve an integer linear program (ilp) or an approximation thereof.
</nextsent>
<nextsent>the ilp encodes compression model where arbitrary parts of the parse treesof sentences in the summary can be cut and removed.
</nextsent>
<nextsent>this allows them to select parts of sentences and yet preserve some gramatical structure.
</nextsent>
<nextsent>their work focuses on learning particular compression model based on ilp inference, while our work explores learning general and large class of sentence selection models using submod ular optimization.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2092">
<title id=" E12-1023.xml">large margin learning of sub modular summarization models </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>our coverage features are motivated from the approach of yue and joachims(2008).
</prevsent>
<prevsent>in contrast, the hand-tuned pairwise base line uses only tfidf weighted cosine similarity between sentences using all words, following the approach in lin and bilmes (2010).<papid> N10-1134 </papid></prevsent>
</prevsection>
<citsent citstr=" N03-1020 ">
the resulting summaries are evaluated using rouge version 1.5.5 (lin and hovy, 2003).<papid> N03-1020 </papid></citsent>
<aftsection>
<nextsent>we selected the rouge-1 measure because it was used by lin and bilmes (2010) <papid> N10-1134 </papid>and because it is one of the commonly used performance scores in recent work.</nextsent>
<nextsent>however, our learning method applies to other performance measures as well.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2116">
<title id=" E12-2014.xml">a support platform for event detection using social intelligence </title>
<section> system details.  </section>
<citcontext>
<prevsection>
<prevsent>in future versions of the system we therefore aim to use query expansion to generate noisy versions of query terms to retrieve additional relevant tweets.
</prevsent>
<prevsent>we subsequently intend to perform lexical normalisation to evaluate the precision of the returned data.
</prevsent>
</prevsection>
<citsent citstr=" P11-1038 ">
the present lexical normalisation used by our system is the dictionary lookup method of hanand baldwin (2011) <papid> P11-1038 </papid>which normalises noisy tokens only when the normalised form is known with high confidence (e.g. you for u).</citsent>
<aftsection>
<nextsent>ultimately,however, we are interested in performing context sensitive lexical normalisation, based on reim plementation of the method of han and baldwin (2011).<papid> P11-1038 </papid></nextsent>
<nextsent>this method will allow us to target wider variety of noisy tokens such as typos (e.g. earthquak earthquake?), abbreviations (e.g. lv love?), phonetic substitutions (e.g. b4 before?)</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2118">
<title id=" E12-2014.xml">a support platform for event detection using social intelligence </title>
<section> system details.  </section>
<citcontext>
<prevsection>
<prevsent>j (wj |loci)to avoid zero probabilities, we only consider tokens that occur at least twice in the training data,and ignore unseen words.
</prevsent>
<prevsent>a probability is calculated for the most-probable location by normalising over the scores for each loci.
</prevsent>
</prevsection>
<citsent citstr=" D11-1141 ">
we employ the method of ritter et al(2011) <papid> D11-1141 </papid>totokenise messages, and use token unigrams as features, including any hash tags, but ignoring twitter mentions, urls and purely numeric tokens.</citsent>
<aftsection>
<nextsent>we 6alternatively, we could consider hybrid approach of user- and message-level geo location prediction, especially for users where we have sufficient training data, which we plan to incorporate into future version of the system.
</nextsent>
<nextsent>71 ll l l l l l 10000 20000 30000 40000 0.15 0.20 0.25 0.30 0.35 0.40 feature number predict ion acc urac figure 2: accuracy of geo location prediction, for varying numbers of features based on information gain also experimented with included the named entity predictions of the ritter et al(2011) <papid> D11-1141 </papid>method into our system, but found that it had no impact on predictive accuracy.</nextsent>
<nextsent>finally, we apply feature selection to the data, based on information gain (yang and pedersen, 1997).to evaluate the geo location prediction module, we use the user-level geo location dataset of cheng et al(2010), based on the lower 48 states of the usa.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2122">
<title id=" E12-1020.xml">compensating for annotation errors in training a relation extractor </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>if so, classify it into one of the types.
</prevsent>
<prevsent>supervised learning treats rdc as classification problem and solves it with supervised machine learning algorithms such as maxent and svm.
</prevsent>
</prevsection>
<citsent citstr=" P11-1053 ">
there are two commonly used learning strategies (sun et al  2011).<papid> P11-1053 </papid></citsent>
<aftsection>
<nextsent>given an annotated corpus, one could apply flat learning strategy, which trains single multiclass classifier on training examples labeled as one of the relation types or not-a-relation, and apply it to determine its type or output not-a relation for each candidate relation mention during testing.
</nextsent>
<nextsent>the examples of each type are the relation mentions that are tagged as instances of that type, and the not-a-relation examples are constructed from pairs of entities that appear in the same sentence but are not tagged as any of the types.
</nextsent>
<nextsent>alternatively, one could apply hierarchical learning strategy, which trains two classifiers, binary classifier rd for relation detection and the other multi-class classifier rc for relation classification.
</nextsent>
<nextsent>rd is trained by grouping tagged relation mentions of all types as positive instances and using all the not-a-relation cases (same as described above) as negative examples.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2124">
<title id=" E12-1020.xml">compensating for annotation errors in training a relation extractor </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>rc is trained on the annotated examples with their tagged types.
</prevsent>
<prevsent>during testing, rd is applied first to identify whether an example expresses some relation, then rc is applied to determine the most likely type only if it is detected as correct by rd. state-of-the-art supervised methods for relation extraction also differ from each other on data representation.
</prevsent>
</prevsection>
<citsent citstr=" P05-1053 ">
given relation mention, feature-based methods (miller et al  2000; kambhatla, 2004; boschee et al  2005; grishman et al  2005; zhou et al  2005; <papid> P05-1053 </papid>jiang and zhai, 2007; <papid> N07-1015 </papid>sun et al  2011) <papid> P11-1053 </papid>extract rich list of structural, lexical, syntactic and semantic features to represent it; in contrast, the kernel based methods (zelenko et al  2003; bunescu and mooney, 2005<papid> H05-1091 </papid>a; bunescu and mooney, 2005<papid> H05-1091 </papid>b; zhao and grishman, 2005; <papid> P05-1052 </papid>zhang et al  2006<papid> P06-1104 </papid>a; zhang et al  2006<papid> P06-1104 </papid>b; zhou et al  2007; <papid> D07-1076 </papid>qian et al  2008) <papid> C08-1088 </papid>represent each instance with an object such as augmented token sequences or parse tree, and used carefully designed kernel function, e.g. sub sequence kernel (bunescu and mooney, 2005<papid> H05-1091 </papid>b) or convolution tree kernel (collins and duffy, 2001), to calculate their similarity.</citsent>
<aftsection>
<nextsent>these objects are usually augmented with features such as semantic features.
</nextsent>
<nextsent>in this paper, we use the hierarchical learning strategy since it simplifies the problem by letting us focus on relation detection only.
</nextsent>
<nextsent>the relation classification stage remains unchanged and we will show that it benefits from improved detection.
</nextsent>
<nextsent>for experiments on both relation detection and relation classification, we use svm4 (vapnik 1998) as the learning algorithm since it can be extended to support transductive inference as discussed in section 4.3.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2125">
<title id=" E12-1020.xml">compensating for annotation errors in training a relation extractor </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>rc is trained on the annotated examples with their tagged types.
</prevsent>
<prevsent>during testing, rd is applied first to identify whether an example expresses some relation, then rc is applied to determine the most likely type only if it is detected as correct by rd. state-of-the-art supervised methods for relation extraction also differ from each other on data representation.
</prevsent>
</prevsection>
<citsent citstr=" N07-1015 ">
given relation mention, feature-based methods (miller et al  2000; kambhatla, 2004; boschee et al  2005; grishman et al  2005; zhou et al  2005; <papid> P05-1053 </papid>jiang and zhai, 2007; <papid> N07-1015 </papid>sun et al  2011) <papid> P11-1053 </papid>extract rich list of structural, lexical, syntactic and semantic features to represent it; in contrast, the kernel based methods (zelenko et al  2003; bunescu and mooney, 2005<papid> H05-1091 </papid>a; bunescu and mooney, 2005<papid> H05-1091 </papid>b; zhao and grishman, 2005; <papid> P05-1052 </papid>zhang et al  2006<papid> P06-1104 </papid>a; zhang et al  2006<papid> P06-1104 </papid>b; zhou et al  2007; <papid> D07-1076 </papid>qian et al  2008) <papid> C08-1088 </papid>represent each instance with an object such as augmented token sequences or parse tree, and used carefully designed kernel function, e.g. sub sequence kernel (bunescu and mooney, 2005<papid> H05-1091 </papid>b) or convolution tree kernel (collins and duffy, 2001), to calculate their similarity.</citsent>
<aftsection>
<nextsent>these objects are usually augmented with features such as semantic features.
</nextsent>
<nextsent>in this paper, we use the hierarchical learning strategy since it simplifies the problem by letting us focus on relation detection only.
</nextsent>
<nextsent>the relation classification stage remains unchanged and we will show that it benefits from improved detection.
</nextsent>
<nextsent>for experiments on both relation detection and relation classification, we use svm4 (vapnik 1998) as the learning algorithm since it can be extended to support transductive inference as discussed in section 4.3.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2127">
<title id=" E12-1020.xml">compensating for annotation errors in training a relation extractor </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>rc is trained on the annotated examples with their tagged types.
</prevsent>
<prevsent>during testing, rd is applied first to identify whether an example expresses some relation, then rc is applied to determine the most likely type only if it is detected as correct by rd. state-of-the-art supervised methods for relation extraction also differ from each other on data representation.
</prevsent>
</prevsection>
<citsent citstr=" H05-1091 ">
given relation mention, feature-based methods (miller et al  2000; kambhatla, 2004; boschee et al  2005; grishman et al  2005; zhou et al  2005; <papid> P05-1053 </papid>jiang and zhai, 2007; <papid> N07-1015 </papid>sun et al  2011) <papid> P11-1053 </papid>extract rich list of structural, lexical, syntactic and semantic features to represent it; in contrast, the kernel based methods (zelenko et al  2003; bunescu and mooney, 2005<papid> H05-1091 </papid>a; bunescu and mooney, 2005<papid> H05-1091 </papid>b; zhao and grishman, 2005; <papid> P05-1052 </papid>zhang et al  2006<papid> P06-1104 </papid>a; zhang et al  2006<papid> P06-1104 </papid>b; zhou et al  2007; <papid> D07-1076 </papid>qian et al  2008) <papid> C08-1088 </papid>represent each instance with an object such as augmented token sequences or parse tree, and used carefully designed kernel function, e.g. sub sequence kernel (bunescu and mooney, 2005<papid> H05-1091 </papid>b) or convolution tree kernel (collins and duffy, 2001), to calculate their similarity.</citsent>
<aftsection>
<nextsent>these objects are usually augmented with features such as semantic features.
</nextsent>
<nextsent>in this paper, we use the hierarchical learning strategy since it simplifies the problem by letting us focus on relation detection only.
</nextsent>
<nextsent>the relation classification stage remains unchanged and we will show that it benefits from improved detection.
</nextsent>
<nextsent>for experiments on both relation detection and relation classification, we use svm4 (vapnik 1998) as the learning algorithm since it can be extended to support transductive inference as discussed in section 4.3.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2133">
<title id=" E12-1020.xml">compensating for annotation errors in training a relation extractor </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>rc is trained on the annotated examples with their tagged types.
</prevsent>
<prevsent>during testing, rd is applied first to identify whether an example expresses some relation, then rc is applied to determine the most likely type only if it is detected as correct by rd. state-of-the-art supervised methods for relation extraction also differ from each other on data representation.
</prevsent>
</prevsection>
<citsent citstr=" P05-1052 ">
given relation mention, feature-based methods (miller et al  2000; kambhatla, 2004; boschee et al  2005; grishman et al  2005; zhou et al  2005; <papid> P05-1053 </papid>jiang and zhai, 2007; <papid> N07-1015 </papid>sun et al  2011) <papid> P11-1053 </papid>extract rich list of structural, lexical, syntactic and semantic features to represent it; in contrast, the kernel based methods (zelenko et al  2003; bunescu and mooney, 2005<papid> H05-1091 </papid>a; bunescu and mooney, 2005<papid> H05-1091 </papid>b; zhao and grishman, 2005; <papid> P05-1052 </papid>zhang et al  2006<papid> P06-1104 </papid>a; zhang et al  2006<papid> P06-1104 </papid>b; zhou et al  2007; <papid> D07-1076 </papid>qian et al  2008) <papid> C08-1088 </papid>represent each instance with an object such as augmented token sequences or parse tree, and used carefully designed kernel function, e.g. sub sequence kernel (bunescu and mooney, 2005<papid> H05-1091 </papid>b) or convolution tree kernel (collins and duffy, 2001), to calculate their similarity.</citsent>
<aftsection>
<nextsent>these objects are usually augmented with features such as semantic features.
</nextsent>
<nextsent>in this paper, we use the hierarchical learning strategy since it simplifies the problem by letting us focus on relation detection only.
</nextsent>
<nextsent>the relation classification stage remains unchanged and we will show that it benefits from improved detection.
</nextsent>
<nextsent>for experiments on both relation detection and relation classification, we use svm4 (vapnik 1998) as the learning algorithm since it can be extended to support transductive inference as discussed in section 4.3.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2134">
<title id=" E12-1020.xml">compensating for annotation errors in training a relation extractor </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>rc is trained on the annotated examples with their tagged types.
</prevsent>
<prevsent>during testing, rd is applied first to identify whether an example expresses some relation, then rc is applied to determine the most likely type only if it is detected as correct by rd. state-of-the-art supervised methods for relation extraction also differ from each other on data representation.
</prevsent>
</prevsection>
<citsent citstr=" P06-1104 ">
given relation mention, feature-based methods (miller et al  2000; kambhatla, 2004; boschee et al  2005; grishman et al  2005; zhou et al  2005; <papid> P05-1053 </papid>jiang and zhai, 2007; <papid> N07-1015 </papid>sun et al  2011) <papid> P11-1053 </papid>extract rich list of structural, lexical, syntactic and semantic features to represent it; in contrast, the kernel based methods (zelenko et al  2003; bunescu and mooney, 2005<papid> H05-1091 </papid>a; bunescu and mooney, 2005<papid> H05-1091 </papid>b; zhao and grishman, 2005; <papid> P05-1052 </papid>zhang et al  2006<papid> P06-1104 </papid>a; zhang et al  2006<papid> P06-1104 </papid>b; zhou et al  2007; <papid> D07-1076 </papid>qian et al  2008) <papid> C08-1088 </papid>represent each instance with an object such as augmented token sequences or parse tree, and used carefully designed kernel function, e.g. sub sequence kernel (bunescu and mooney, 2005<papid> H05-1091 </papid>b) or convolution tree kernel (collins and duffy, 2001), to calculate their similarity.</citsent>
<aftsection>
<nextsent>these objects are usually augmented with features such as semantic features.
</nextsent>
<nextsent>in this paper, we use the hierarchical learning strategy since it simplifies the problem by letting us focus on relation detection only.
</nextsent>
<nextsent>the relation classification stage remains unchanged and we will show that it benefits from improved detection.
</nextsent>
<nextsent>for experiments on both relation detection and relation classification, we use svm4 (vapnik 1998) as the learning algorithm since it can be extended to support transductive inference as discussed in section 4.3.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2142">
<title id=" E12-1020.xml">compensating for annotation errors in training a relation extractor </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>rc is trained on the annotated examples with their tagged types.
</prevsent>
<prevsent>during testing, rd is applied first to identify whether an example expresses some relation, then rc is applied to determine the most likely type only if it is detected as correct by rd. state-of-the-art supervised methods for relation extraction also differ from each other on data representation.
</prevsent>
</prevsection>
<citsent citstr=" D07-1076 ">
given relation mention, feature-based methods (miller et al  2000; kambhatla, 2004; boschee et al  2005; grishman et al  2005; zhou et al  2005; <papid> P05-1053 </papid>jiang and zhai, 2007; <papid> N07-1015 </papid>sun et al  2011) <papid> P11-1053 </papid>extract rich list of structural, lexical, syntactic and semantic features to represent it; in contrast, the kernel based methods (zelenko et al  2003; bunescu and mooney, 2005<papid> H05-1091 </papid>a; bunescu and mooney, 2005<papid> H05-1091 </papid>b; zhao and grishman, 2005; <papid> P05-1052 </papid>zhang et al  2006<papid> P06-1104 </papid>a; zhang et al  2006<papid> P06-1104 </papid>b; zhou et al  2007; <papid> D07-1076 </papid>qian et al  2008) <papid> C08-1088 </papid>represent each instance with an object such as augmented token sequences or parse tree, and used carefully designed kernel function, e.g. sub sequence kernel (bunescu and mooney, 2005<papid> H05-1091 </papid>b) or convolution tree kernel (collins and duffy, 2001), to calculate their similarity.</citsent>
<aftsection>
<nextsent>these objects are usually augmented with features such as semantic features.
</nextsent>
<nextsent>in this paper, we use the hierarchical learning strategy since it simplifies the problem by letting us focus on relation detection only.
</nextsent>
<nextsent>the relation classification stage remains unchanged and we will show that it benefits from improved detection.
</nextsent>
<nextsent>for experiments on both relation detection and relation classification, we use svm4 (vapnik 1998) as the learning algorithm since it can be extended to support transductive inference as discussed in section 4.3.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2143">
<title id=" E12-1020.xml">compensating for annotation errors in training a relation extractor </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>rc is trained on the annotated examples with their tagged types.
</prevsent>
<prevsent>during testing, rd is applied first to identify whether an example expresses some relation, then rc is applied to determine the most likely type only if it is detected as correct by rd. state-of-the-art supervised methods for relation extraction also differ from each other on data representation.
</prevsent>
</prevsection>
<citsent citstr=" C08-1088 ">
given relation mention, feature-based methods (miller et al  2000; kambhatla, 2004; boschee et al  2005; grishman et al  2005; zhou et al  2005; <papid> P05-1053 </papid>jiang and zhai, 2007; <papid> N07-1015 </papid>sun et al  2011) <papid> P11-1053 </papid>extract rich list of structural, lexical, syntactic and semantic features to represent it; in contrast, the kernel based methods (zelenko et al  2003; bunescu and mooney, 2005<papid> H05-1091 </papid>a; bunescu and mooney, 2005<papid> H05-1091 </papid>b; zhao and grishman, 2005; <papid> P05-1052 </papid>zhang et al  2006<papid> P06-1104 </papid>a; zhang et al  2006<papid> P06-1104 </papid>b; zhou et al  2007; <papid> D07-1076 </papid>qian et al  2008) <papid> C08-1088 </papid>represent each instance with an object such as augmented token sequences or parse tree, and used carefully designed kernel function, e.g. sub sequence kernel (bunescu and mooney, 2005<papid> H05-1091 </papid>b) or convolution tree kernel (collins and duffy, 2001), to calculate their similarity.</citsent>
<aftsection>
<nextsent>these objects are usually augmented with features such as semantic features.
</nextsent>
<nextsent>in this paper, we use the hierarchical learning strategy since it simplifies the problem by letting us focus on relation detection only.
</nextsent>
<nextsent>the relation classification stage remains unchanged and we will show that it benefits from improved detection.
</nextsent>
<nextsent>for experiments on both relation detection and relation classification, we use svm4 (vapnik 1998) as the learning algorithm since it can be extended to support transductive inference as discussed in section 4.3.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2153">
<title id=" E12-1020.xml">compensating for annotation errors in training a relation extractor </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>it shows 5 fold cross validation results.
</prevsent>
<prevsent>relation annotation, annotating more documents with single-pass offers advantages over annotating less data with high quality assurance (dual passes and adjudication).
</prevsent>
</prevsection>
<citsent citstr=" W10-1808 ">
dligach et al (2010) <papid> W10-1808 </papid>studied wsd annotation from cost-effectiveness viewpoint.</citsent>
<aftsection>
<nextsent>they showed empirically that, with same amount of annotation dollars spent, single-annotation is better than dual-annotation and adjudication.
</nextsent>
<nextsent>the common practice for quality control of wsd annotation is similar to relation annotation.
</nextsent>
<nextsent>however, the task of wsd annotation is very different from relation annotation.
</nextsent>
<nextsent>wsd requires that every example must be assigned some tag, whereas that is not required for relation tagging.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2154">
<title id=" E12-1020.xml">compensating for annotation errors in training a relation extractor </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>we share similar assumption with li and liu (2003) but we use different method to select negative examples since the false negative examples show very skewed distribution, as described in section 5.2.
</prevsent>
<prevsent>transductive svm was introduced by vapnik (1998) and later refined in joachims (1999).
</prevsent>
</prevsection>
<citsent citstr=" I05-1036 ">
a few related methods were studied on the subtask of relation classification (the second stage of the hierarchical learning scheme) in zhang (2005).<papid> I05-1036 </papid></citsent>
<aftsection>
<nextsent>chan and roth (2011) <papid> P11-1056 </papid>observed the similar phenomenon that ace annotators rarely duplicate relation link for coreferential mentions.</nextsent>
<nextsent>they use an evaluation scheme to avoid being penalized by the relation mentions which are not annotated because of this behavior.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2155">
<title id=" E12-1020.xml">compensating for annotation errors in training a relation extractor </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>transductive svm was introduced by vapnik (1998) and later refined in joachims (1999).
</prevsent>
<prevsent>a few related methods were studied on the subtask of relation classification (the second stage of the hierarchical learning scheme) in zhang (2005).<papid> I05-1036 </papid></prevsent>
</prevsection>
<citsent citstr=" P11-1056 ">
chan and roth (2011) <papid> P11-1056 </papid>observed the similar phenomenon that ace annotators rarely duplicate relation link for coreferential mentions.</citsent>
<aftsection>
<nextsent>they use an evaluation scheme to avoid being penalized by the relation mentions which are not annotated because of this behavior.
</nextsent>
<nextsent>we analyzed snapshot of the ace 2005 relation annotation and found that each single pass annotation missed around 18-28% of relation mentions and contains around 10% spurious mentions.
</nextsent>
<nextsent>a detailed analysis showed that it is possible to find some of the false negatives, and that most spurious cases are actually correct examples from system builders perspective.
</nextsent>
<nextsent>by automatically purifying negative examples and applying transductive inference on suspicious examples, we can train relation classifier whose performance is comparable to classifier trained on the dual annotated and adjudicated data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2156">
<title id=" E12-1067.xml">probabilistic hierarchical clustering of morphological paradigms </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>applying morphological segmentation mitigates data sparsity by tackling the issue with out-of-vocabulary (oov) words.in this paper, we propose paradigmatic approach.
</prevsent>
<prevsent>a morphological paradigm is pair(stemlist, suffixlist) such that each concatenation of stem+suffix (where stem ? stem list and suffix ? suffixlist) is valid word form.
</prevsent>
</prevsection>
<citsent citstr=" J01-2001 ">
the learning of morphological paradigms is not novel as there has already been existing work in this area such as goldsmith (2001), <papid> J01-2001 </papid>snover et al (2002), monson et al (2009), can and manandhar (2009) and dreyer and eisner (2011).<papid> D11-1057 </papid></citsent>
<aftsection>
<nextsent>however, none of these existing approaches address learning of the hierarchical structure of paradigms.hierarchical organisation of words help capture morphological similarities between words in compact structure by factoring these similarities through stems, suffixes or prefixes.
</nextsent>
<nextsent>our inference algorithm simultaneously infers latent variables (i.e. the morphemes) along with their hierarchicalorganisation.
</nextsent>
<nextsent>most hierarchical clustering algorithms are single-pass, where once the hierarchical structure is built, the structure does not change further.
</nextsent>
<nextsent>the paper is structured as follows: section 2 gives the related work, section 3 describes the probabilistic hierarchical clustering scheme, section 4 explains the morphological segmentation model by embedding it into the clustering scheme and describes the inference algorithm along with how the morphological segmentation is performed, section 5 presents the experiment settings along with the evaluation scores, and finally section 6 presents discussion with comparison with other systems that participated in morpho challenge 2009 and 2010 .
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2157">
<title id=" E12-1067.xml">probabilistic hierarchical clustering of morphological paradigms </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>applying morphological segmentation mitigates data sparsity by tackling the issue with out-of-vocabulary (oov) words.in this paper, we propose paradigmatic approach.
</prevsent>
<prevsent>a morphological paradigm is pair(stemlist, suffixlist) such that each concatenation of stem+suffix (where stem ? stem list and suffix ? suffixlist) is valid word form.
</prevsent>
</prevsection>
<citsent citstr=" D11-1057 ">
the learning of morphological paradigms is not novel as there has already been existing work in this area such as goldsmith (2001), <papid> J01-2001 </papid>snover et al (2002), monson et al (2009), can and manandhar (2009) and dreyer and eisner (2011).<papid> D11-1057 </papid></citsent>
<aftsection>
<nextsent>however, none of these existing approaches address learning of the hierarchical structure of paradigms.hierarchical organisation of words help capture morphological similarities between words in compact structure by factoring these similarities through stems, suffixes or prefixes.
</nextsent>
<nextsent>our inference algorithm simultaneously infers latent variables (i.e. the morphemes) along with their hierarchicalorganisation.
</nextsent>
<nextsent>most hierarchical clustering algorithms are single-pass, where once the hierarchical structure is built, the structure does not change further.
</nextsent>
<nextsent>the paper is structured as follows: section 2 gives the related work, section 3 describes the probabilistic hierarchical clustering scheme, section 4 explains the morphological segmentation model by embedding it into the clustering scheme and describes the inference algorithm along with how the morphological segmentation is performed, section 5 presents the experiment settings along with the evaluation scores, and finally section 6 presents discussion with comparison with other systems that participated in morpho challenge 2009 and 2010 .
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2159">
<title id=" E12-1067.xml">probabilistic hierarchical clustering of morphological paradigms </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>dreyer and eisner (2011) <papid> D11-1057 </papid>propose an infinite diriclet mixture model for capturing paradigms.</prevsent>
<prevsent>however, they do not address learning of hierarchy.</prevsent>
</prevsection>
<citsent citstr=" W06-3209 ">
the method proposed in chan (2006) <papid> W06-3209 </papid>also learns within hierarchical structure where latent dirichlet allocation (lda) is used to findstem-suffix matrices.</citsent>
<aftsection>
<nextsent>however, their work is supervised, as true morphological analyses of words are provided to the system.
</nextsent>
<nextsent>in contrast, our proposed method is fully unsupervised.
</nextsent>
<nextsent>the hierarchical clustering proposed in this work is different from existing hierarchical clustering algorithms in two aspects:?
</nextsent>
<nextsent>it is not single-pass as the hierarchical structure changes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2160">
<title id=" E12-1067.xml">probabilistic hierarchical clustering of morphological paradigms </title>
<section> experiments &amp; results.  </section>
<citcontext>
<prevsection>
<prevsent>base.2 74.93 49.81 59.84 pm-union3 55.68 62.33 58.82 lignos4 83.49 45.00 58.48 prob.
</prevsent>
<prevsent>clustering (multiple) 57.08 57.58 57.33 pm-mimic3 53.13 59.01 55.91 morphonet5 65.08 47.82 55.13 rali-cof6 68.32 46.45 55.30 canman7 58.52 44.82 50.76 1 virpioja et al (2009).
</prevsent>
</prevsection>
<citsent citstr=" W02-0603 ">
2 creutz and lagus (2002).<papid> W02-0603 </papid></citsent>
<aftsection>
<nextsent>3 monson et al (2009).
</nextsent>
<nextsent>4 lignos et al (2009).
</nextsent>
<nextsent>5 bernhard (2009).
</nextsent>
<nextsent>table 3: comparison with other unsupervised systems that participated in morpho challenge 2009 for en glish.we compare our system with the other participant systems in morpho challenge 2010.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2164">
<title id=" E06-2030.xml">developments in affect detection in edrama </title>
<section> metaphorical expression of affect.  </section>
<citcontext>
<prevsection>
<prevsent>a theoretically intriguing spontane 205 ous creative elaboration of the pizza?
</prevsent>
<prevsent>metaphor.
</prevsent>
</prevsection>
<citsent citstr=" C02-2021 ">
our developing approach to metaphor handling in the affect detection module is partly to lookfor stock metaphorical phraseology and straight forward variants of it, and partly to use simple version of the more open-ended, reasoning-based techniques taken from the att-meta project (barnden et al, 2002; <papid> C02-2021 </papid>2003; 2004).</citsent>
<aftsection>
<nextsent>att-meta includes general-purpose reasoning engine, and can potentially be used to reason about emotion in relation to other factors in situation.
</nextsent>
<nextsent>in turn,the realities of metaphor usage in e-drama sessions are contributing to our basic research on metaphor processing.
</nextsent>
<nextsent>we have implemented limited degree of affect detection in an automated actor by means of pat tern-matching, robust parsing and some semanticanalysis.
</nextsent>
<nextsent>although there is considerable distance to go in terms of the practical affect detection that we plan to implement, the already implemented detection is able to cause reasonably appropriate contributions by the automated character.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2165">
<title id=" E06-1052.xml">investigating a generic paraphrase based approach for relation extraction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this set very high idealized upper bound for the recall of the paraphrase-based approach for this dataset.
</prevsent>
<prevsent>yet, obtaining high cover age in practice would require effective paraphrase acquisition and lexical-syntactic template matching.
</prevsent>
</prevsection>
<citsent citstr=" W04-3206 ">
next, we implemented prototype that utilizes state-of-the-art method for learning entailment relations from the web (szpektor et al, 2004), <papid> W04-3206 </papid>the minipar dependency parser (lin, 1998) and syntactic matching module.</citsent>
<aftsection>
<nextsent>as expected, the performance of the implemented system wasmuch lower than the ideal upper bound, yet obtaining quite reasonable practical results given its unsupervised nature.
</nextsent>
<nextsent>the contributions of our investigation follow 409the dual goal set above.
</nextsent>
<nextsent>to the best of our knowledge, this is the first comprehensive evaluation that measures directly the performance of unsupervised paraphrase acquisition relative to standard application dataset.
</nextsent>
<nextsent>it is also the first evaluation ofa generic paraphrase-based approach for the standard resetting.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2166">
<title id=" E06-1052.xml">investigating a generic paraphrase based approach for relation extraction </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>information extraction (ie) and its sub field relation extraction (re) are traditionally performed in supervised manner, identifying the different ways to express specific information or relation.
</prevsent>
<prevsent>given that annotated data is expensive to produce, unsupervised or weakly supervised methods have been proposed for ie andre.
</prevsent>
</prevsection>
<citsent citstr=" C00-2136 ">
yangarber et al (2000) <papid> C00-2136 </papid>and stevenson and greenwood (2005) <papid> P05-1047 </papid>define methods for automatic acquisition of predicate-argument structures thatare similar to set of seed relations, which represent specific scenario.</citsent>
<aftsection>
<nextsent>yangarber et al (2000) <papid> C00-2136 </papid>approach was evaluated in two ways: (1) manually mapping the discovered patterns into an ie system and running full muc-style evaluation; (2) using the learned patterns to perform document filtering at the scenario level.</nextsent>
<nextsent>stevenson and greenwood (2005) <papid> P05-1047 </papid>evaluated their method through document and sentence filtering at the scenario level.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2168">
<title id=" E06-1052.xml">investigating a generic paraphrase based approach for relation extraction </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>information extraction (ie) and its sub field relation extraction (re) are traditionally performed in supervised manner, identifying the different ways to express specific information or relation.
</prevsent>
<prevsent>given that annotated data is expensive to produce, unsupervised or weakly supervised methods have been proposed for ie andre.
</prevsent>
</prevsection>
<citsent citstr=" P05-1047 ">
yangarber et al (2000) <papid> C00-2136 </papid>and stevenson and greenwood (2005) <papid> P05-1047 </papid>define methods for automatic acquisition of predicate-argument structures thatare similar to set of seed relations, which represent specific scenario.</citsent>
<aftsection>
<nextsent>yangarber et al (2000) <papid> C00-2136 </papid>approach was evaluated in two ways: (1) manually mapping the discovered patterns into an ie system and running full muc-style evaluation; (2) using the learned patterns to perform document filtering at the scenario level.</nextsent>
<nextsent>stevenson and greenwood (2005) <papid> P05-1047 </papid>evaluated their method through document and sentence filtering at the scenario level.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2174">
<title id=" E06-1052.xml">investigating a generic paraphrase based approach for relation extraction </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>yangarber et al (2000) <papid> C00-2136 </papid>approach was evaluated in two ways: (1) manually mapping the discovered patterns into an ie system and running full muc-style evaluation; (2) using the learned patterns to perform document filtering at the scenario level.</prevsent>
<prevsent>stevenson and greenwood (2005) <papid> P05-1047 </papid>evaluated their method through document and sentence filtering at the scenario level.</prevsent>
</prevsection>
<citsent citstr=" P03-1029 ">
sudo et al (2003) <papid> P03-1029 </papid>extract dependency subtrees within relevant documents as ie patterns.</citsent>
<aftsection>
<nextsent>the goalof the algorithm is event extraction, though performance is measured by counting argument entities rather than counting events directly.
</nextsent>
<nextsent>hasegawa et al (2004) <papid> P04-1053 </papid>performs unsupervised hierarchical clustering over simple set of fea tures.</nextsent>
<nextsent>the algorithm does not extract entity pairs forgiven relation from set of documents but rather classifies all relations in large corpus.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2175">
<title id=" E06-1052.xml">investigating a generic paraphrase based approach for relation extraction </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>sudo et al (2003) <papid> P03-1029 </papid>extract dependency subtrees within relevant documents as ie patterns.</prevsent>
<prevsent>the goalof the algorithm is event extraction, though performance is measured by counting argument entities rather than counting events directly.</prevsent>
</prevsection>
<citsent citstr=" P04-1053 ">
hasegawa et al (2004) <papid> P04-1053 </papid>performs unsupervised hierarchical clustering over simple set of fea tures.</citsent>
<aftsection>
<nextsent>the algorithm does not extract entity pairs forgiven relation from set of documents but rather classifies all relations in large corpus.
</nextsent>
<nextsent>this approach is more similar to text mining tasks than to classic ie problems.
</nextsent>
<nextsent>to conclude, several unsupervised approaches learn relevant ie templates for complete scenario, but without identifying their relevance toeach specific relation within the scenario.
</nextsent>
<nextsent>accordingly, the evaluations of these works either did not address the direct applicability for re or evaluated it only after further manual postprocessing.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2176">
<title id=" E06-1052.xml">investigating a generic paraphrase based approach for relation extraction </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>learning from local corpus is bounded by the corpus scope, which is usually domain specific (both works above processed news domain corpora).
</prevsent>
<prevsent>to cover broader range of domains several works utilized the web, while requiring several manually provided examples for each input relation, e.g.
</prevsent>
</prevsection>
<citsent citstr=" P02-1006 ">
(ravichandran and hovy, 2002).<papid> P02-1006 </papid></citsent>
<aftsection>
<nextsent>taking step further, the tease algorithm (szpektor et al, 2004) <papid> W04-3206 </papid>provides completely unsupervised method for acquiring entailment relations from the web forgiven input relation (see section 5.1).most of these works did not evaluate their results in terms of application coverage.</nextsent>
<nextsent>lin andpantel (2001) compared their results to human generated paraphrases.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2181">
<title id=" E06-1052.xml">investigating a generic paraphrase based approach for relation extraction </title>
<section> assumed configuration for re.  </section>
<citcontext>
<prevsection>
<prevsent>table 1 presents list of generic syntactic phenomena that are known in the literature to relate to linguistic variability.
</prevsent>
<prevsent>aphenomenon which deserves few words of explanation is the transparent head noun?
</prevsent>
</prevsection>
<citsent citstr=" J86-3002 ">
(grishman et al, 1986; <papid> J86-3002 </papid>fillmore et al, 2002).</citsent>
<aftsection>
<nextsent>a transparent noun n1 typically occurs in constructs ofthe form n1 preposition n2?
</nextsent>
<nextsent>for which the syntactic relation involving n1, which is the head of the np, applies to n2, the modifier.
</nextsent>
<nextsent>in the example in table 1, fragment?
</nextsent>
<nextsent>is the transparent head noun while the relation activate?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2184">
<title id=" E09-3009.xml">a generalized vector space model for text retrieval based on semantic relatedness </title>
<section> a gvsm model based on semantic </section>
<citcontext>
<prevsection>
<prevsent>essentially, spe is the harmonic mean of the two depths normalized to the maximum thesaurus depth.
</prevsent>
<prevsent>the harmonic mean offers lower upper bound than the average of depths and we think is more realistic estimation of the paths depth.
</prevsent>
</prevsection>
<citsent citstr=" J06-1003 ">
scm and spe capture the two most important parameters of measuring semantic relatedness between terms (budanitsky and hirst, 2006), <papid> J06-1003 </papid>namely path length and senses depth in the used thesaurus.</citsent>
<aftsection>
<nextsent>we combine these two measures naturally towards defining the semantic relatedness between two terms.
</nextsent>
<nextsent>definition 3 given word thesaurus o, pair of terms = (t1, t2), and all pairs of senses =(s1i, s2j), where s1i, s2j senses of t1,t2 respectively.
</nextsent>
<nextsent>the semantic relatedness of (sr(t,s,o)) is defined as max{scm(s, o)spe(s, o)}.
</nextsent>
<nextsent>sr between two terms ti, tj where ti ? tj ? and /?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2185">
<title id=" E09-3009.xml">a generalized vector space model for text retrieval based on semantic relatedness </title>
<section> a gvsm model based on semantic </section>
<citcontext>
<prevsection>
<prevsent>3.2 semantic networks from word thesauri.
</prevsent>
<prevsent>in order to construct semantic network for pairof terms t1 and t2 and combination of their respective senses, i.e., s1 and s2, we adopted the network construction method that we introduced in (tsatsaronis et al , 2007).
</prevsent>
</prevsection>
<citsent citstr=" C04-1162 ">
this method was preferred against other related methods, like the one introduced in (mihalcea et al , 2004), <papid> C04-1162 </papid>since it embeds all the available semantic information existing in wordnet, even edges that cross pos, thus offering richer semantic representation.</citsent>
<aftsection>
<nextsent>according to the adopted semantic network construction model, each semantic edge type is given differentweight.
</nextsent>
<nextsent>the intuition behind edge types?
</nextsent>
<nextsent>weighting is that certain types provide stronger semantic connections than others.
</nextsent>
<nextsent>the frequency of occurrence of the different edge types in wordnet 2.0, is used to define the edge types?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2186">
<title id=" E09-3009.xml">a generalized vector space model for text retrieval based on semantic relatedness </title>
<section> a gvsm model based on semantic </section>
<citcontext>
<prevsection>
<prevsent>alternatively, wsd algorithm could have disam biguated the two terms, given the text fragments where the two terms occurred.
</prevsent>
<prevsent>though interesting, this prospect is neither addressed, nor examined in this work.
</prevsent>
</prevsection>
<citsent citstr=" P06-1013 ">
still, it is in our next plans and part of our future work to embed in our model some ofthe interesting wsd approaches, like knowledge based (sinha and mihalcea, 2007; brody et al , 2006), <papid> P06-1013 </papid>corpus-based (mihalcea and csomai, 2005; <papid> P05-3014 </papid>mccarthy et al , 2004), <papid> P04-1036 </papid>or combinations with very high accuracy (montoyo et al , 2005).</citsent>
<aftsection>
<nextsent>3.5 the gvsm model.
</nextsent>
<nextsent>in equation 2, which captures the document-query similarity in the gvsm model, the orthogonality between terms ti and tj is expressed by the inner product of the respective term vectors ~ti~tj . recall that ~ti and ~tj are in reality unknown.
</nextsent>
<nextsent>we estimate their inner product by equation 3, where si and sj are the senses of terms ti and tj respectively, maximizing scm ? spe.
</nextsent>
<nextsent>~ti~tj = sr((ti, tj), (si, sj), o) (3) since in our model we assume that each term can be semantically related with any other term, and 2the sign of the algorithm is not considered at this step.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2187">
<title id=" E09-3009.xml">a generalized vector space model for text retrieval based on semantic relatedness </title>
<section> a gvsm model based on semantic </section>
<citcontext>
<prevsection>
<prevsent>alternatively, wsd algorithm could have disam biguated the two terms, given the text fragments where the two terms occurred.
</prevsent>
<prevsent>though interesting, this prospect is neither addressed, nor examined in this work.
</prevsent>
</prevsection>
<citsent citstr=" P05-3014 ">
still, it is in our next plans and part of our future work to embed in our model some ofthe interesting wsd approaches, like knowledge based (sinha and mihalcea, 2007; brody et al , 2006), <papid> P06-1013 </papid>corpus-based (mihalcea and csomai, 2005; <papid> P05-3014 </papid>mccarthy et al , 2004), <papid> P04-1036 </papid>or combinations with very high accuracy (montoyo et al , 2005).</citsent>
<aftsection>
<nextsent>3.5 the gvsm model.
</nextsent>
<nextsent>in equation 2, which captures the document-query similarity in the gvsm model, the orthogonality between terms ti and tj is expressed by the inner product of the respective term vectors ~ti~tj . recall that ~ti and ~tj are in reality unknown.
</nextsent>
<nextsent>we estimate their inner product by equation 3, where si and sj are the senses of terms ti and tj respectively, maximizing scm ? spe.
</nextsent>
<nextsent>~ti~tj = sr((ti, tj), (si, sj), o) (3) since in our model we assume that each term can be semantically related with any other term, and 2the sign of the algorithm is not considered at this step.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2188">
<title id=" E09-3009.xml">a generalized vector space model for text retrieval based on semantic relatedness </title>
<section> a gvsm model based on semantic </section>
<citcontext>
<prevsection>
<prevsent>alternatively, wsd algorithm could have disam biguated the two terms, given the text fragments where the two terms occurred.
</prevsent>
<prevsent>though interesting, this prospect is neither addressed, nor examined in this work.
</prevsent>
</prevsection>
<citsent citstr=" P04-1036 ">
still, it is in our next plans and part of our future work to embed in our model some ofthe interesting wsd approaches, like knowledge based (sinha and mihalcea, 2007; brody et al , 2006), <papid> P06-1013 </papid>corpus-based (mihalcea and csomai, 2005; <papid> P05-3014 </papid>mccarthy et al , 2004), <papid> P04-1036 </papid>or combinations with very high accuracy (montoyo et al , 2005).</citsent>
<aftsection>
<nextsent>3.5 the gvsm model.
</nextsent>
<nextsent>in equation 2, which captures the document-query similarity in the gvsm model, the orthogonality between terms ti and tj is expressed by the inner product of the respective term vectors ~ti~tj . recall that ~ti and ~tj are in reality unknown.
</nextsent>
<nextsent>we estimate their inner product by equation 3, where si and sj are the senses of terms ti and tj respectively, maximizing scm ? spe.
</nextsent>
<nextsent>~ti~tj = sr((ti, tj), (si, sj), o) (3) since in our model we assume that each term can be semantically related with any other term, and 2the sign of the algorithm is not considered at this step.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2189">
<title id=" E09-3009.xml">a generalized vector space model for text retrieval based on semantic relatedness </title>
<section> experimental evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>a kind of gold standard?
</prevsent>
<prevsent>ranking of related word pairs (i.e., from the most related words to the most irrelevant) has thus been created, against which computer programs can test their ability on measuring semantic relatedness between words.
</prevsent>
</prevsection>
<citsent citstr=" J98-1006 ">
we compared our measure against ten known measures of semantic relatedness: (hs) hirst and st-onge (1998), (jc) jiang and conrath (1997), (lc) leacock et al  (1998), (<papid> J98-1006 </papid>l) lin (1998), (r) resnik (1995), (js) jarmasz and szpakowicz (2003), (gm) gabrilovich and markovitch (2007), (f) finkelstein et al  (2002), (hr) ) and (sp) strube and ponzetto (2006).</citsent>
<aftsection>
<nextsent>in table 1 the results of sr and the ten compared measures are shown.the reported numbers are the spearman correlation of the measures?
</nextsent>
<nextsent>rankings with the gold standard (human judgements).
</nextsent>
<nextsent>the correlations for the three datasets show thatsr performs better than any other measure of semantic relatedness, besides the case of (hr) in the m&c; dataset.
</nextsent>
<nextsent>it surpasses hr though in the r&g; and the 353-c dataset.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2191">
<title id=" E09-3009.xml">a generalized vector space model for text retrieval based on semantic relatedness </title>
<section> future work.  </section>
<citcontext>
<prevsection>
<prevsent>thus, the similarity between query and document may be computed by weighting the similarity in the terms space and the senses?
</prevsent>
<prevsent>space.
</prevsent>
</prevsection>
<citsent citstr=" P08-1017 ">
finally, we should also examine the perspective of applying the proposed measure of semantic relatedness in query expansion technique, similarly to the work of fang (2008).<papid> P08-1017 </papid></citsent>
<aftsection>
<nextsent>in this paper we presented new measure of semantic relatedness and expanded the standard vsm to embed the semantic relatedness between pairs of terms into new gvsm model.
</nextsent>
<nextsent>the semantic relatedness measure takes into account all of the semantic links offered by wordnet.
</nextsent>
<nextsent>it considers wordnet as graph, weighs edges depending on their type and depth and computes the maximum relatedness between any two nodes,connected via one or more paths.
</nextsent>
<nextsent>the comparison to well known measures gives promising results.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2192">
<title id=" E12-1004.xml">entailment above the word level in distributional semantics </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>on the other hand, fs has provided sophisticated models of sentence meaning,but it has been largely limited to hand-coded models that do not scale up to real-life challenges by learning from data.given these complementary strengths, we naturally ask if ds and fs can address each others limitations.
</prevsent>
<prevsent>two recent strands of research are bringing ds closer to meeting core fs challenges.
</prevsent>
</prevsection>
<citsent citstr=" D10-1115 ">
one strand attempts to model compo sitionality with ds methods, representing both primitive and composed linguistic expressions as distributional vectors (baroni and zamparelli,2010; <papid> D10-1115 </papid>grefenstette and sadrzadeh, 2011; <papid> D11-1129 </papid>guevara, 2010; <papid> W10-2805 </papid>mitchell and lapata, 2010).</citsent>
<aftsection>
<nextsent>the other strand attempts to reformulate fss notion of logical inference in terms that ds can capture (erk, 2009; <papid> W09-3711 </papid>geffet and dagan, 2005; kotlerman et al  2010; zhitomirsky-geffet and dagan, 2010).</nextsent>
<nextsent>in keeping with the lexical emphasis of ds, this strand has focused on inference at theword level, or lexical entailment, that is, discovering from distributional vectors of hyponyms (dog) that they entail their hypernyms (animal).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2193">
<title id=" E12-1004.xml">entailment above the word level in distributional semantics </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>on the other hand, fs has provided sophisticated models of sentence meaning,but it has been largely limited to hand-coded models that do not scale up to real-life challenges by learning from data.given these complementary strengths, we naturally ask if ds and fs can address each others limitations.
</prevsent>
<prevsent>two recent strands of research are bringing ds closer to meeting core fs challenges.
</prevsent>
</prevsection>
<citsent citstr=" D11-1129 ">
one strand attempts to model compo sitionality with ds methods, representing both primitive and composed linguistic expressions as distributional vectors (baroni and zamparelli,2010; <papid> D10-1115 </papid>grefenstette and sadrzadeh, 2011; <papid> D11-1129 </papid>guevara, 2010; <papid> W10-2805 </papid>mitchell and lapata, 2010).</citsent>
<aftsection>
<nextsent>the other strand attempts to reformulate fss notion of logical inference in terms that ds can capture (erk, 2009; <papid> W09-3711 </papid>geffet and dagan, 2005; kotlerman et al  2010; zhitomirsky-geffet and dagan, 2010).</nextsent>
<nextsent>in keeping with the lexical emphasis of ds, this strand has focused on inference at theword level, or lexical entailment, that is, discovering from distributional vectors of hyponyms (dog) that they entail their hypernyms (animal).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2194">
<title id=" E12-1004.xml">entailment above the word level in distributional semantics </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>on the other hand, fs has provided sophisticated models of sentence meaning,but it has been largely limited to hand-coded models that do not scale up to real-life challenges by learning from data.given these complementary strengths, we naturally ask if ds and fs can address each others limitations.
</prevsent>
<prevsent>two recent strands of research are bringing ds closer to meeting core fs challenges.
</prevsent>
</prevsection>
<citsent citstr=" W10-2805 ">
one strand attempts to model compo sitionality with ds methods, representing both primitive and composed linguistic expressions as distributional vectors (baroni and zamparelli,2010; <papid> D10-1115 </papid>grefenstette and sadrzadeh, 2011; <papid> D11-1129 </papid>guevara, 2010; <papid> W10-2805 </papid>mitchell and lapata, 2010).</citsent>
<aftsection>
<nextsent>the other strand attempts to reformulate fss notion of logical inference in terms that ds can capture (erk, 2009; <papid> W09-3711 </papid>geffet and dagan, 2005; kotlerman et al  2010; zhitomirsky-geffet and dagan, 2010).</nextsent>
<nextsent>in keeping with the lexical emphasis of ds, this strand has focused on inference at theword level, or lexical entailment, that is, discovering from distributional vectors of hyponyms (dog) that they entail their hypernyms (animal).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2195">
<title id=" E12-1004.xml">entailment above the word level in distributional semantics </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>two recent strands of research are bringing ds closer to meeting core fs challenges.
</prevsent>
<prevsent>one strand attempts to model compo sitionality with ds methods, representing both primitive and composed linguistic expressions as distributional vectors (baroni and zamparelli,2010; <papid> D10-1115 </papid>grefenstette and sadrzadeh, 2011; <papid> D11-1129 </papid>guevara, 2010; <papid> W10-2805 </papid>mitchell and lapata, 2010).</prevsent>
</prevsection>
<citsent citstr=" W09-3711 ">
the other strand attempts to reformulate fss notion of logical inference in terms that ds can capture (erk, 2009; <papid> W09-3711 </papid>geffet and dagan, 2005; kotlerman et al  2010; zhitomirsky-geffet and dagan, 2010).</citsent>
<aftsection>
<nextsent>in keeping with the lexical emphasis of ds, this strand has focused on inference at theword level, or lexical entailment, that is, discovering from distributional vectors of hyponyms (dog) that they entail their hypernyms (animal).
</nextsent>
<nextsent>this paper brings these two strands of research together by demonstrating two ways in which the distributional vectors of composite expressions bear on inference.
</nextsent>
<nextsent>here we focus on phrasal vectors harvested directly from the corpus rather than obtained compositionally.
</nextsent>
<nextsent>in first experiment, we exploit the entailment properties of class of composite expressions, namely adjective-noun constructions (ans), to harvest training data for an entailment recognizer.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2196">
<title id=" E12-1004.xml">entailment above the word level in distributional semantics </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>since semantically similar words tend to share similar contexts, ds has been very successful in tasks that require quantifying semantic similarity among words, such as synonym detection and concept clustering (turney and pan tel, 2010).
</prevsent>
<prevsent>recently, there has been flurry of interest in ds to model meaning composition: how can we derive the ds representation of composite phrase from that of its constituents?
</prevsent>
</prevsection>
<citsent citstr=" W03-1812 ">
although the general focus in the area is to perform algebraic operations on word semantic vectors (mitchelland lapata, 2010), some researchers have also directly examined the corpus contexts of phrases.for example, baldwin et al (2003) <papid> W03-1812 </papid>studied vector extraction for phrases because they were interested in the decomposability of multiword expressions.</citsent>
<aftsection>
<nextsent>baroni and zamparelli (2010) <papid> D10-1115 </papid>and guevara (2010) <papid> W10-2805 </papid>look at corpus-harvested phrase vectors to learn composition functions that should derive such composite vectors automatically.</nextsent>
<nextsent>baroni and zamparelli, in particular, showed qualitatively that directly corpus-harvested vectors for an constructions are meaningful; for example,the vector of young husband has nearest neighbors small son, small daughter and mistress.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2199">
<title id=" E12-1004.xml">entailment above the word level in distributional semantics </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>each semantic domain has its own entailment relation |=a. the entailment relation |=s among sentences isthe logical notion just described, whereas the entailment relations |=n and |=qp among nouns and quantifier phrases are the inclusion relations among sets of entities and sets of sets of entities respectively.
</prevsent>
<prevsent>our results in section 5 show that ds needs to treat |=n and |=qp differently as well.empirical, corpus-based perspectives on entailment until recently, the corpus-based research tradition has studied entailment mostly atthe word level, with applied goals such as classifying lexical relations and building taxonomic wordnet-like resources automatically.
</prevsent>
</prevsection>
<citsent citstr=" C92-2082 ">
the most popular approach, first adopted by hearst (1992), <papid> C92-2082 </papid>extracts lexical relations from patterns in large corpora.</citsent>
<aftsection>
<nextsent>for instance, from the pattern n1 such as n2 one learns that n2 |=n1 (from insects suchas beetles, derive beetles |= insects).
</nextsent>
<nextsent>several studies have refined and extended this approach (pan tel and ravichandran, 2004; <papid> N04-1041 </papid>snow et al  2005; snow et al  2006; <papid> P06-1101 </papid>turney, 2008).<papid> C08-1114 </papid>while empirically very successful, the pattern based method is mostly limited to single content words (or frequent content-word phrases).</nextsent>
<nextsent>we are interested in entailment between phrases, where it is not obvious how to use lexico-syntactic patterns and cope with data sparsity.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2200">
<title id=" E12-1004.xml">entailment above the word level in distributional semantics </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>the most popular approach, first adopted by hearst (1992), <papid> C92-2082 </papid>extracts lexical relations from patterns in large corpora.</prevsent>
<prevsent>for instance, from the pattern n1 such as n2 one learns that n2 |=n1 (from insects suchas beetles, derive beetles |= insects).</prevsent>
</prevsection>
<citsent citstr=" N04-1041 ">
several studies have refined and extended this approach (pan tel and ravichandran, 2004; <papid> N04-1041 </papid>snow et al  2005; snow et al  2006; <papid> P06-1101 </papid>turney, 2008).<papid> C08-1114 </papid>while empirically very successful, the pattern based method is mostly limited to single content words (or frequent content-word phrases).</citsent>
<aftsection>
<nextsent>we are interested in entailment between phrases, where it is not obvious how to use lexico-syntactic patterns and cope with data sparsity.
</nextsent>
<nextsent>for instance, it seems hard to find pattern that frequently connects one qp to another it entails, as in all beetles pattern many beetles.
</nextsent>
<nextsent>hence, we aim to find more general method and investigate whether ds vectors(whether corpus-harvested or compositionally de rived) encode the information needed to account for phrasal entailment in way that can be captured and generalized to unseen phrase pairs.rather recently, the study of sentential entailment has taken an empirical turn, thanks to the development of benchmarks for entailment systems.
</nextsent>
<nextsent>the fs definition of entailment has been modified by taking common sense into account.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2201">
<title id=" E12-1004.xml">entailment above the word level in distributional semantics </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>the most popular approach, first adopted by hearst (1992), <papid> C92-2082 </papid>extracts lexical relations from patterns in large corpora.</prevsent>
<prevsent>for instance, from the pattern n1 such as n2 one learns that n2 |=n1 (from insects suchas beetles, derive beetles |= insects).</prevsent>
</prevsection>
<citsent citstr=" P06-1101 ">
several studies have refined and extended this approach (pan tel and ravichandran, 2004; <papid> N04-1041 </papid>snow et al  2005; snow et al  2006; <papid> P06-1101 </papid>turney, 2008).<papid> C08-1114 </papid>while empirically very successful, the pattern based method is mostly limited to single content words (or frequent content-word phrases).</citsent>
<aftsection>
<nextsent>we are interested in entailment between phrases, where it is not obvious how to use lexico-syntactic patterns and cope with data sparsity.
</nextsent>
<nextsent>for instance, it seems hard to find pattern that frequently connects one qp to another it entails, as in all beetles pattern many beetles.
</nextsent>
<nextsent>hence, we aim to find more general method and investigate whether ds vectors(whether corpus-harvested or compositionally de rived) encode the information needed to account for phrasal entailment in way that can be captured and generalized to unseen phrase pairs.rather recently, the study of sentential entailment has taken an empirical turn, thanks to the development of benchmarks for entailment systems.
</nextsent>
<nextsent>the fs definition of entailment has been modified by taking common sense into account.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2202">
<title id=" E12-1004.xml">entailment above the word level in distributional semantics </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>the most popular approach, first adopted by hearst (1992), <papid> C92-2082 </papid>extracts lexical relations from patterns in large corpora.</prevsent>
<prevsent>for instance, from the pattern n1 such as n2 one learns that n2 |=n1 (from insects suchas beetles, derive beetles |= insects).</prevsent>
</prevsection>
<citsent citstr=" C08-1114 ">
several studies have refined and extended this approach (pan tel and ravichandran, 2004; <papid> N04-1041 </papid>snow et al  2005; snow et al  2006; <papid> P06-1101 </papid>turney, 2008).<papid> C08-1114 </papid>while empirically very successful, the pattern based method is mostly limited to single content words (or frequent content-word phrases).</citsent>
<aftsection>
<nextsent>we are interested in entailment between phrases, where it is not obvious how to use lexico-syntactic patterns and cope with data sparsity.
</nextsent>
<nextsent>for instance, it seems hard to find pattern that frequently connects one qp to another it entails, as in all beetles pattern many beetles.
</nextsent>
<nextsent>hence, we aim to find more general method and investigate whether ds vectors(whether corpus-harvested or compositionally de rived) encode the information needed to account for phrasal entailment in way that can be captured and generalized to unseen phrase pairs.rather recently, the study of sentential entailment has taken an empirical turn, thanks to the development of benchmarks for entailment systems.
</nextsent>
<nextsent>the fs definition of entailment has been modified by taking common sense into account.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2203">
<title id=" E12-1004.xml">entailment above the word level in distributional semantics </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>would most likely infer that ? is also true.
</prevsent>
<prevsent>entailment systems have been compared under this new perspective in various evaluation campaigns,the best known being the recognizing textual entailment (rte) initiative (dagan et al  2009).
</prevsent>
</prevsection>
<citsent citstr=" W07-1412 ">
most rte systems are based on advanced nlp components, machine learning techniques, and/or syntactic transformations (zanzotto et al  2007; <papid> W07-1412 </papid>kouleykov and magnini, 2005).</citsent>
<aftsection>
<nextsent>a few systems exploit deep fs analysis (bos and markert, 2006;chambers et al  2007).<papid> W07-1427 </papid></nextsent>
<nextsent>in particular, the fs results about qp properties that affect entailment have been exploited by chambers et al who complement core broad-coverage system with natural logic module to trade lower recall for higherprecision.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2204">
<title id=" E12-1004.xml">entailment above the word level in distributional semantics </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>entailment systems have been compared under this new perspective in various evaluation campaigns,the best known being the recognizing textual entailment (rte) initiative (dagan et al  2009).
</prevsent>
<prevsent>most rte systems are based on advanced nlp components, machine learning techniques, and/or syntactic transformations (zanzotto et al  2007; <papid> W07-1412 </papid>kouleykov and magnini, 2005).</prevsent>
</prevsection>
<citsent citstr=" W07-1427 ">
a few systems exploit deep fs analysis (bos and markert, 2006;chambers et al  2007).<papid> W07-1427 </papid></citsent>
<aftsection>
<nextsent>in particular, the fs results about qp properties that affect entailment have been exploited by chambers et al who complement core broad-coverage system with natural logic module to trade lower recall for higherprecision.
</nextsent>
<nextsent>for instance, they exploit the monotonicity properties of no that cause the following reversal in entailment direction: some beetles |= some insects but no insects |= no beetles.to investigate entailment step by step, we address here much simpler and clearer type of entailment than the more complex notion takenup by the rte community.
</nextsent>
<nextsent>while rte is outside our present scope, we do focus on qp entailment as natural logic does.
</nextsent>
<nextsent>however, our evaluation differs from chambers et al s, since we relyon general-purpose ds vectors as our only resource, and we look at phrase pairs with different quantifiers but the same noun.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2206">
<title id=" E12-1004.xml">entailment above the word level in distributional semantics </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>qps, of course, have many well-known semantic properties besides en tailment; we leave their analysis to future study.
</prevsent>
<prevsent>entailment in ds erk (2009) <papid> W09-3711 </papid>suggests that it may not be possible to induce lexical entailment directly from vector space representation, but itis possible to encode the relation in this space after it has been derived through other means.</prevsent>
</prevsection>
<citsent citstr=" C04-1146 ">
on the other hand, recent studies (geffet and dagan, 25 2005; kotlerman et al  2010; weeds et al  2004) <papid> C04-1146 </papid>have pursued the intuition that entailment is the asymmetric ability of one term to substitute?</citsent>
<aftsection>
<nextsent>for another.
</nextsent>
<nextsent>for example, baseball contexts are also sport contexts but not vice versa, hence baseball is narrower?
</nextsent>
<nextsent>than sport and baseball |=sport.
</nextsent>
<nextsent>on this view, entailment between vectors corresponds to inclusion of contexts or features, and can be captured by asymmetric measures of distribution similarity.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2207">
<title id=" E12-1004.xml">entailment above the word level in distributional semantics </title>
<section> data and methods.  </section>
<citcontext>
<prevsection>
<prevsent>we use phrases of interest as general term to refer to both multiword phrases and single words, and more precisely to: those an and qn sequences that are in the datasets (see next subsections), the adjectives, quantifiers and nouns contained in those sequences, and the most frequent (9.8k) nouns and (8.1k) adjectives in the corpus.
</prevsent>
<prevsent>the first step is to count the content words (more precisely, the most frequent 9.8knouns, 8.1k adjectives, and 9.6k verbs in the cor pus) that occur in the same sentence as phrases of interest.
</prevsent>
</prevsection>
<citsent citstr=" J90-1003 ">
in the second step, following standard practice, the co-occurrence counts are converted into pointwise mutual information (pmi) scores (church and hanks, 1990).<papid> J90-1003 </papid></citsent>
<aftsection>
<nextsent>the result of this step is sparse matrix (with both positive and negative entries) with 48k rows (one per phrase of interest) and 27k columns (one per content word).
</nextsent>
<nextsent>3.2 the an |= dataset.
</nextsent>
<nextsent>to characterize entailment between nouns using their semantic vectors, we need data exemplifying which noun entails which.
</nextsent>
<nextsent>this section introduces one cheap way to collect such training data set exploiting semantic vectors for composed expressions, namely an sequences.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2208">
<title id=" E12-1004.xml">entailment above the word level in distributional semantics </title>
<section> data and methods.  </section>
<citcontext>
<prevsection>
<prevsent>of course, such unsupervised data would be slightly noisy, especially because some of the most frequent adjectives are not restrictive.
</prevsent>
<prevsent>to collect cleaner data and to be sure that weare really examining the phenomenon of entailment, we took mere few moments of manual effort to select the 256 restrictive adjectives from the most frequent 300 adjectives in the corpus.
</prevsent>
</prevsection>
<citsent citstr=" W11-2501 ">
we then took the cartesian product of these 256 adjectives with the 200 concrete nouns in the bless dataset (baroni and lenci, 2011).<papid> W11-2501 </papid></citsent>
<aftsection>
<nextsent>those nouns were chosen to avoid highly polysemous words.
</nextsent>
<nextsent>from the cartesian product, we obtain total of 1246 an sequences, such as big cat, that occur more than 100 times in the corpus.
</nextsent>
<nextsent>thesean sequences encompass 190 of the 256 adjec 26 tives and 128 of the 200 nouns.
</nextsent>
<nextsent>the process results in 1246 positive instances of an |= entailment, which we use as training data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2209">
<title id=" E09-1052.xml">a logic of semantic representations for shallow parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>representing semantics as logical form that supports automated inference and model construction is vital for deeper language engineering tasks, such as dialogue systems.
</prevsent>
<prevsent>logical forms can be obtained from hand-crafted deep grammars (butt et al, 1999; copestake and flickinger, 2000) butthis lacks robustness: not all words and constructions are covered and by design ill-formed phrases fail to parse.
</prevsent>
</prevsection>
<citsent citstr=" D07-1071 ">
there has thus been trend recently towards robust wide-coverage semantic construction (e.g., (bos et al, 2004; zettlemoyer and collins, 2007)).<papid> D07-1071 </papid></citsent>
<aftsection>
<nextsent>but there are certain semantic phenomena that these robust approaches dont capture reliably, including quantifier scope, optional arguments, and long-distance dependencies (for instance, clark et al (2004) report that the parser used by bos et al (2004) yields 63% accuracy on object extraction; e.g., the man that imet.
</nextsent>
<nextsent>forcing robust parser to make decision about these phenomena can therefore be error-prone.
</nextsent>
<nextsent>depending on the application, it may be preferable to give the parser the option to leave semantic decision open when its not sufficientlyinformedi.e., to compute partial semantic representation and to complete it later, using information extraneous to the parser.in this paper, we focus on an approach to semantic representation that supports this strategy: robust minimal recur sion semantics (rmrs, copestake (2007<papid> W07-1210 </papid>a)).</nextsent>
<nextsent>rmrs is designed to support under specification of lexical information, scope,and predicate-argument structure.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2210">
<title id=" E09-1052.xml">a logic of semantic representations for shallow parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>but there are certain semantic phenomena that these robust approaches dont capture reliably, including quantifier scope, optional arguments, and long-distance dependencies (for instance, clark et al (2004) report that the parser used by bos et al (2004) yields 63% accuracy on object extraction; e.g., the man that imet.
</prevsent>
<prevsent>forcing robust parser to make decision about these phenomena can therefore be error-prone.
</prevsent>
</prevsection>
<citsent citstr=" W07-1210 ">
depending on the application, it may be preferable to give the parser the option to leave semantic decision open when its not sufficientlyinformedi.e., to compute partial semantic representation and to complete it later, using information extraneous to the parser.in this paper, we focus on an approach to semantic representation that supports this strategy: robust minimal recur sion semantics (rmrs, copestake (2007<papid> W07-1210 </papid>a)).</citsent>
<aftsection>
<nextsent>rmrs is designed to support under specification of lexical information, scope,and predicate-argument structure.
</nextsent>
<nextsent>it is an emerging standard for representing partial semantics,and has been applied in several implemented systems.
</nextsent>
<nextsent>for instance, copestake (2003) and frank (2004) <papid> C04-1185 </papid>use it to specify semantic components to shallow parsers ranging indepth from pos taggers to chunk parsers and intermediate parsers such as rasp (briscoe et al, 2006).<papid> P06-4020 </papid></nextsent>
<nextsent>mrs analyses (copestake et al, 2005) derived from deep grammars, such as the english resource grammar(erg, (copestake and flickinger, 2000)) are special cases of rmrs.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2211">
<title id=" E09-1052.xml">a logic of semantic representations for shallow parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>rmrs is designed to support under specification of lexical information, scope,and predicate-argument structure.
</prevsent>
<prevsent>it is an emerging standard for representing partial semantics,and has been applied in several implemented systems.
</prevsent>
</prevsection>
<citsent citstr=" C04-1185 ">
for instance, copestake (2003) and frank (2004) <papid> C04-1185 </papid>use it to specify semantic components to shallow parsers ranging indepth from pos taggers to chunk parsers and intermediate parsers such as rasp (briscoe et al, 2006).<papid> P06-4020 </papid></citsent>
<aftsection>
<nextsent>mrs analyses (copestake et al, 2005) derived from deep grammars, such as the english resource grammar(erg, (copestake and flickinger, 2000)) are special cases of rmrs.
</nextsent>
<nextsent>but rmrs, unlike mrs andre lated formalisms like dominance constraints (egget al, 2001), is able to express semantic information in the absence of full predicate argument structure and lexical subcategorisation.
</nextsent>
<nextsent>the key contribution we make is to cast rmrs, for the first time, as logic with well-defined model theory.
</nextsent>
<nextsent>previously, no such model theory existed, and so rmrs had to be used in some what ad-hoc manner that left open exactly what any given rmrs representation actually means.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2212">
<title id=" E09-1052.xml">a logic of semantic representations for shallow parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>rmrs is designed to support under specification of lexical information, scope,and predicate-argument structure.
</prevsent>
<prevsent>it is an emerging standard for representing partial semantics,and has been applied in several implemented systems.
</prevsent>
</prevsection>
<citsent citstr=" P06-4020 ">
for instance, copestake (2003) and frank (2004) <papid> C04-1185 </papid>use it to specify semantic components to shallow parsers ranging indepth from pos taggers to chunk parsers and intermediate parsers such as rasp (briscoe et al, 2006).<papid> P06-4020 </papid></citsent>
<aftsection>
<nextsent>mrs analyses (copestake et al, 2005) derived from deep grammars, such as the english resource grammar(erg, (copestake and flickinger, 2000)) are special cases of rmrs.
</nextsent>
<nextsent>but rmrs, unlike mrs andre lated formalisms like dominance constraints (egget al, 2001), is able to express semantic information in the absence of full predicate argument structure and lexical subcategorisation.
</nextsent>
<nextsent>the key contribution we make is to cast rmrs, for the first time, as logic with well-defined model theory.
</nextsent>
<nextsent>previously, no such model theory existed, and so rmrs had to be used in some what ad-hoc manner that left open exactly what any given rmrs representation actually means.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2215">
<title id=" E12-1044.xml">coordination structure analysis using dual decomposition </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>has as many as five possible structures (resnik, 1999).
</prevsent>
<prevsent>therefore, grammatical approach is not sufficient to disambiguate coordination structures.
</prevsent>
</prevsection>
<citsent citstr=" C04-1204 ">
in fact, the stanford parser (klein and manning, 2003) and enju (miyao and tsujii, 2004) <papid> C04-1204 </papid>fail to disambiguate sentence ama freshman advertising and marketing major.</citsent>
<aftsection>
<nextsent>table 1 shows the output from them and the correct coordination structure.
</nextsent>
<nextsent>the coordination structure above is obvious to humans because there is symmetry of conjuncts(-ing) in the sentence.
</nextsent>
<nextsent>coordination structures of ten have such structural and semantic symmetry of conjuncts.
</nextsent>
<nextsent>one approach is to capture local symmetry of conjuncts.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2217">
<title id=" E12-1044.xml">coordination structure analysis using dual decomposition </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>finally, we show experimental results that demonstrate the effectiveness of our approach.
</prevsent>
<prevsent>we compare three methods: coordination structure analysis with alignment-based local features, hpsg parsing, and the dual-decomposition-based approach that combines both.
</prevsent>
</prevsection>
<citsent citstr=" P07-1086 ">
many previous studies for coordination disambiguation have focused on particular type of npcoordination (hogan, 2007).<papid> P07-1086 </papid></citsent>
<aftsection>
<nextsent>resnik (1999) disambiguated coordination structures by using semantic similarity of the conjuncts in taxonomy.
</nextsent>
<nextsent>he dealt with two kinds of patterns, [n0 n1 and n2 n3] and [n1 and n2 n3], where ni are all nouns.he detected coordination structures based on similarity of form, meaning and conceptual association between n1 and n2 and between n1 and n3.nakov and hearst (2005) <papid> H05-1105 </papid>used the web as training set and applied it to task that is similar to resniks.in terms of integrating coordination disambiguation with an existing parsing model, our approach resembles the approach by hogan (2007).<papid> P07-1086 </papid>she detected noun phrase coordinations by finding symmetry in conjunct structure and the dependency between the lexical heads of the conjuncts.</nextsent>
<nextsent>they are used to rerank the n-best outputs of the bikel parser (2004), whereas two models interact with each other in our method.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2218">
<title id=" E12-1044.xml">coordination structure analysis using dual decomposition </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>many previous studies for coordination disambiguation have focused on particular type of npcoordination (hogan, 2007).<papid> P07-1086 </papid></prevsent>
<prevsent>resnik (1999) disambiguated coordination structures by using semantic similarity of the conjuncts in taxonomy.</prevsent>
</prevsection>
<citsent citstr=" H05-1105 ">
he dealt with two kinds of patterns, [n0 n1 and n2 n3] and [n1 and n2 n3], where ni are all nouns.he detected coordination structures based on similarity of form, meaning and conceptual association between n1 and n2 and between n1 and n3.nakov and hearst (2005) <papid> H05-1105 </papid>used the web as training set and applied it to task that is similar to resniks.in terms of integrating coordination disambiguation with an existing parsing model, our approach resembles the approach by hogan (2007).<papid> P07-1086 </papid>she detected noun phrase coordinations by finding symmetry in conjunct structure and the dependency between the lexical heads of the conjuncts.</citsent>
<aftsection>
<nextsent>they are used to rerank the n-best outputs of the bikel parser (2004), whereas two models interact with each other in our method.
</nextsent>
<nextsent>shimbo and hara (2007) <papid> D07-1064 </papid>proposed analignment-based method for detecting and dis ambiguating non-nested coordination structures.</nextsent>
<nextsent>they disambiguated coordination structures based on the edit distance between two conjuncts.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2220">
<title id=" E12-1044.xml">coordination structure analysis using dual decomposition </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>he dealt with two kinds of patterns, [n0 n1 and n2 n3] and [n1 and n2 n3], where ni are all nouns.he detected coordination structures based on similarity of form, meaning and conceptual association between n1 and n2 and between n1 and n3.nakov and hearst (2005) <papid> H05-1105 </papid>used the web as training set and applied it to task that is similar to resniks.in terms of integrating coordination disambiguation with an existing parsing model, our approach resembles the approach by hogan (2007).<papid> P07-1086 </papid>she detected noun phrase coordinations by finding symmetry in conjunct structure and the dependency between the lexical heads of the conjuncts.</prevsent>
<prevsent>they are used to rerank the n-best outputs of the bikel parser (2004), whereas two models interact with each other in our method.</prevsent>
</prevsection>
<citsent citstr=" D07-1064 ">
shimbo and hara (2007) <papid> D07-1064 </papid>proposed analignment-based method for detecting and dis ambiguating non-nested coordination structures.</citsent>
<aftsection>
<nextsent>they disambiguated coordination structures based on the edit distance between two conjuncts.
</nextsent>
<nextsent>hara et al(2009) <papid> P09-1109 </papid>extended the method, dealing with nested coordinations as well.</nextsent>
<nextsent>we used their method as one of the two sub-models.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2221">
<title id=" E12-1044.xml">coordination structure analysis using dual decomposition </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>shimbo and hara (2007) <papid> D07-1064 </papid>proposed analignment-based method for detecting and dis ambiguating non-nested coordination structures.</prevsent>
<prevsent>they disambiguated coordination structures based on the edit distance between two conjuncts.</prevsent>
</prevsection>
<citsent citstr=" P09-1109 ">
hara et al(2009) <papid> P09-1109 </papid>extended the method, dealing with nested coordinations as well.</citsent>
<aftsection>
<nextsent>we used their method as one of the two sub-models.
</nextsent>
<nextsent>3.1 coordination structure analysis with.
</nextsent>
<nextsent>alignment-based local features coordination structure analysis with alignment based local features (hara et al 2009) <papid> P09-1109 </papid>is hybrid approach to coordination disambiguation that combines simple grammar to ensure consistent global structure of coordinations in sentence,and features based on sequence alignment to capture local symmetry of conjuncts.</nextsent>
<nextsent>in this section, we describe the method briefly.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2235">
<title id=" E12-1044.xml">coordination structure analysis using dual decomposition </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>right,conjunct
</prevsent>
<prevsent>? coord_right_schema ? coord_left_schema figure 3: construction of coordination in enju composed into efficiently solvable sub-problems.
</prevsent>
</prevsection>
<citsent citstr=" D10-1001 ">
it is becoming popular in the nlp community and has been shown to work effectively on several nlp tasks (rush et al 2010).<papid> D10-1001 </papid></citsent>
<aftsection>
<nextsent>we consider an optimization problem argmax (f(x) + g(x)) (2) which is difficult to solve (e.g. np-hard), while argmaxx f(x) and argmaxx g(x) are effectively solvable.
</nextsent>
<nextsent>in dual decomposition, we solve min max x,y (f(x) + g(y) + u(x? y)) instead of the original problem.to find the minimum value, we can use sub gradient method (rush et al 2010).<papid> D10-1001 </papid></nextsent>
<nextsent>the subgra dient method is given in table 4.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2242">
<title id=" E12-1044.xml">coordination structure analysis using dual decomposition </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>5 experiments.
</prevsent>
<prevsent>5.1 test/training data.
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
we trained the alignment-based coordination analysis model on both the geni corpus (kim et al 2003) and the wall street jour al rtion of the penn treebank (marcus et al 1993), <papid> J93-2004 </papid>and evaluated the performance of our method on (i)the genia corpus and (ii) the wall street journal portion of the penn tre bank.</citsent>
<aftsection>
<nextsent>more precisely, we used hpsg treebank onverted from the penn treebank and genia, and further extracted thetraining/test data for ordin ati structure analysis with alignment-based features usi the annotation in the reebank.
</nextsent>
<nextsent>table 5 shows the corpus used in the experiments.
</nextsent>
<nextsent>the wall street journal portion of the penn treebank in the test set ha 2317 sentences from wsj articles, and there are 1356 coordinations in the sentences, while the genia corpus in thetest set has 1764 sentences from medline abstracts, and there are 1848 coordinations in thesentences.
</nextsent>
<nextsent>coor in ations are further subcatego coord wsj genia np 63.7 66.3 vp 13.8 11.4 adjp 6.8 9.6 11.4 6.0 pp 2.4 5.1 others 1.9 1.5 table 6: the percentage of each conjunct type (%) of each test set rized into phrase types such as np coordination or pp coordination.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2271">
<title id=" E09-1071.xml">using lexical and relational similarity to classify semantic relations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for example, the noun pairs (steel,knife) and (paper,cup) are similar because in both cases the relation n2 is made of n1 frequently holds between their members.
</prevsent>
<prevsent>analogi cal tasks are distinct from (but not unrelated to) other kinds of relation extraction?
</prevsent>
</prevsection>
<citsent citstr=" J06-3003 ">
tasks where each data item is tied to specific sentence context (e.g., girju et al (2007)).one such relational reasoning task is the problem of compound noun interpretation, which has received great deal of attention in recent years (girju et al, 2005; turney, 2006; <papid> J06-3003 </papid>butnariu and veale, 2008).</citsent>
<aftsection>
<nextsent>in english (and other languages), the process of producing new lexical items through compounding is very frequent andvery productive.
</nextsent>
<nextsent>furthermore, the noun-noun relation expressed by given compound is not explicit in its surface form: steel knife may be knife made from steel but kitchen knife is most likely to be knife used in kitchen, not knife made from kitchen.
</nextsent>
<nextsent>the assumption made by similarity-based interpretation methods is that the likely meaning of novel compound can be predicted by comparing it to previously seen compounds whose meanings are known.
</nextsent>
<nextsent>this is natural framework for computational techniques;there is also empirical evidence for similarity based interpretation in human compound processing (ryder, 1994; devereux and costello, 2007).<papid> W07-0612 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2272">
<title id=" E09-1071.xml">using lexical and relational similarity to classify semantic relations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>furthermore, the noun-noun relation expressed by given compound is not explicit in its surface form: steel knife may be knife made from steel but kitchen knife is most likely to be knife used in kitchen, not knife made from kitchen.
</prevsent>
<prevsent>the assumption made by similarity-based interpretation methods is that the likely meaning of novel compound can be predicted by comparing it to previously seen compounds whose meanings are known.
</prevsent>
</prevsection>
<citsent citstr=" W07-0612 ">
this is natural framework for computational techniques;there is also empirical evidence for similarity based interpretation in human compound processing (ryder, 1994; devereux and costello, 2007).<papid> W07-0612 </papid></citsent>
<aftsection>
<nextsent>this paper presents an approach to relational reasoning based on combining information about two kinds of similarity between word pairs: lexical similarity and relational similarity.
</nextsent>
<nextsent>the assumptions underlying these two models of similarity are sketched in section 2.
</nextsent>
<nextsent>in section 3 we describe how these models can be implemented for statistical machine learning with kernel methods.we present new flexible and efficient kernel based framework for classification with relational similarity.
</nextsent>
<nextsent>in sections 4 and 5 we apply our methods to compound interpretation task and demonstrate that combining models of lexical and relational similarity can give state-of-the-art results on compound noun interpretation task, surpassing the performance attained by either model taken alone.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2275">
<title id=" E09-1071.xml">using lexical and relational similarity to classify semantic relations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>by tailoring the choice of kernel to the task at hand, the user can use prior knowledge and intuition to improve classification performance.
</prevsent>
<prevsent>one useful property of kernels is that any sum or linear combination of kernel functions is itself valid kernel.
</prevsent>
</prevsection>
<citsent citstr=" P05-1050 ">
theoretical analyses (cristianiniet al, 2001; joachims et al, 2001) and empirical investigations (e.g., gliozzo et al (2005)) <papid> P05-1050 </papid>have shown that combining kernels in this way can have beneficial effect when the component kernels capture different views?</citsent>
<aftsection>
<nextsent>of the data while individually attaining similar levels of discriminative performance.
</nextsent>
<nextsent>in the experiments described below, we make use of this insight to integrate lexical and relational information for semantic classification of compound nouns.
</nextsent>
<nextsent>3.2 lexical kernels.
</nextsent>
<nextsent>o? seaghdha and copestake (2008) demonstrate how standard techniques for distributional similarity can be implemented in kernel framework.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2276">
<title id=" E09-1071.xml">using lexical and relational similarity to classify semantic relations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>gap-weighted string kernels implicitly compute the similarity between two strings s, as an inner product ??(s), ?(t)?.
</prevsent>
<prevsent>lodhi et al (2002) present an efficient dynamic programming algorithm that evaluates this kernel in o(l|s||t|) time without explicitly representing the feature vectors ?(s), ?(t).
</prevsent>
</prevsection>
<citsent citstr=" C08-1114 ">
an alternative embedding is that used by turney (2008) <papid> C08-1114 </papid>in his pair class system (see section 6).</citsent>
<aftsection>
<nextsent>for the pair class embedding pc , an n-word context [01 words]n1|2 [03 words]n1|2 [01 words] containing target words n1, n2 is mapped onto the 2n2 patterns produced by substituting zero or more of the context words with wild card ?.unlike the patterns used by the gap-weighted embedding these are not truly discontinuous, as each wild card must match exactly one word.
</nextsent>
<nextsent>623 3.4 kernels on sets.
</nextsent>
<nextsent>string kernels afford way of comparing individual contexts.
</nextsent>
<nextsent>in order to compute the relational similarity of two pairs, however, we do not want to associate each pair with single context but rather with the set of contexts in which they appear together.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2279">
<title id=" E09-1071.xml">using lexical and relational similarity to classify semantic relations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>two corpora were usedto extract co-occurrence information: the written component of the bnc (burnard, 1995) and the google web 1t 5-gram corpus (brants and franz, 2006).
</prevsent>
<prevsent>for each noun appearing as compound constituent in the dataset, we estimate cooccurrence distribution based on the nouns in coordinative constructions.
</prevsent>
</prevsection>
<citsent citstr=" P06-4020 ">
conjunctions are identified in the bnc by first parsing the corpus withrasp (briscoe et al, 2006) <papid> P06-4020 </papid>and extracting instances of the conj grammatical relation.</citsent>
<aftsection>
<nextsent>as the 5-gram corpus does not contain full sentences it cannot be parsed, so regular expressions were used to extract coordinations.
</nextsent>
<nextsent>in each corpus, the set of co-occurring terms is restricted to the 10,000 most frequent conjuncts in that corpus so that each constituent distribution is represented with 10,000 dimensional vector.
</nextsent>
<nextsent>the probability vector for the compound is created by appending the two constituent vectors, each scaled by 0.5 to weight both 2http://www.csie.ntu.edu.tw/cjlin/ libsvmconstituents equally and ensure that the new vector sums to 1.
</nextsent>
<nextsent>to perform classification with these features we use the jensen-shannon kernel (3).3 4.4 relational features.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2280">
<title id=" E09-1071.xml">using lexical and relational similarity to classify semantic relations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>turney et al (2003) suggest combining various information sources for solving sat analogy problems.
</prevsent>
<prevsent>however, previous work on compound interpretation has generally used either lexical similarity or relational similarity but not both in combination.
</prevsent>
</prevsection>
<citsent citstr=" I05-1082 ">
previously proposed lexical models include the wordnet-based methods of kim and baldwin (2005) <papid> I05-1082 </papid>and girju et al (2005), and the distributional model of o?</citsent>
<aftsection>
<nextsent>seaghdha and copestake (2008).
</nextsent>
<nextsent>the idea of using relational similarity to understand compounds goes back at least as far as lebowitz?
</nextsent>
<nextsent>(1988) researcher system, which processed patent abstracts in an incremental fashion and associated an unseen compound withthe relation expressed in context where the constituents previously occurred.turney (2006) <papid> J06-3003 </papid>describes method (latent relational analysis) that extracts sub sequence patterns for noun pairs from large corpus, using query expansion to increase the recall of the search and feature selection and dimensionality reduction to reduce the complexity of the feature space.</nextsent>
<nextsent>lra performs well on ana logical tasks including compound interpretation, but has very substantial resource requirements.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2285">
<title id=" E09-1071.xml">using lexical and relational similarity to classify semantic relations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>implementing the full pair class algorithm on our corpus yields 46.2%accuracy, 44.9% f-score, which is again significantly worse than all results for the gap-weighted model with   1.
</prevsent>
<prevsent>in nlp, there has not been widespread use of set representations for data items, and hence set classification techniques have received little attention.
</prevsent>
</prevsection>
<citsent citstr=" H05-1092 ">
notable exceptions include rosario and hearst (2005) <papid> H05-1092 </papid>and bunescu and mooney (2007), <papid> P07-1073 </papid>who tackle relation classification and extraction tasks by considering the set of contexts in which the members of candidate relation argument pair co-occur.</citsent>
<aftsection>
<nextsent>while this gives set representation foreach pair, both sets of authors apply classification methods at the level of individual set members rather than directly comparing sets.
</nextsent>
<nextsent>thereis also close connection between the multino mial probability model we have proposed and the pervasive bag of words (or bag of n-grams) representation.
</nextsent>
<nextsent>distributional kernels based on gap weighted feature embedding extend these models by using bags of discontinuous n-grams and down weighting gappy subsequences.a number of set kernels other than those discussed here have been proposed in the machine learning literature, though none of these proposals have explicitly addressed the problem of comparing sets of strings or other structured objects, and many are suitable only for comparing sets of small cardinality.
</nextsent>
<nextsent>kondor and jebara (2003) take adistributional approach similar to ours, fitting mul tivariate normal distributions to the feature space mappings of setsa andb and comparing the mappings with the bhattacharrya vector inner product.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2286">
<title id=" E09-1071.xml">using lexical and relational similarity to classify semantic relations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>implementing the full pair class algorithm on our corpus yields 46.2%accuracy, 44.9% f-score, which is again significantly worse than all results for the gap-weighted model with   1.
</prevsent>
<prevsent>in nlp, there has not been widespread use of set representations for data items, and hence set classification techniques have received little attention.
</prevsent>
</prevsection>
<citsent citstr=" P07-1073 ">
notable exceptions include rosario and hearst (2005) <papid> H05-1092 </papid>and bunescu and mooney (2007), <papid> P07-1073 </papid>who tackle relation classification and extraction tasks by considering the set of contexts in which the members of candidate relation argument pair co-occur.</citsent>
<aftsection>
<nextsent>while this gives set representation foreach pair, both sets of authors apply classification methods at the level of individual set members rather than directly comparing sets.
</nextsent>
<nextsent>thereis also close connection between the multino mial probability model we have proposed and the pervasive bag of words (or bag of n-grams) representation.
</nextsent>
<nextsent>distributional kernels based on gap weighted feature embedding extend these models by using bags of discontinuous n-grams and down weighting gappy subsequences.a number of set kernels other than those discussed here have been proposed in the machine learning literature, though none of these proposals have explicitly addressed the problem of comparing sets of strings or other structured objects, and many are suitable only for comparing sets of small cardinality.
</nextsent>
<nextsent>kondor and jebara (2003) take adistributional approach similar to ours, fitting mul tivariate normal distributions to the feature space mappings of setsa andb and comparing the mappings with the bhattacharrya vector inner product.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2290">
<title id=" E09-1063.xml">bilingually motivated domain adapted word segmentation for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we evaluate the performance of our segmentation approach on pb-smt tasks from two domains and demonstrate that our approach scores consistently among the best results across different data conditions.
</prevsent>
<prevsent>state-of-the-art statistical machine translation(smt) requires certain amount of bilingual corpora as training data in order to achieve competitive results.
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
the only assumption of most current statistical models (brown et al, 1993; <papid> J93-2003 </papid>vogel et al, 1996; <papid> C96-2141 </papid>deng and byrne, 2005) <papid> H05-1022 </papid>is that the aligned sentences in such corpora should be segmented into sequences of tokens that are meant to be words.</citsent>
<aftsection>
<nextsent>therefore, for languages where word boundaries are not ortho graphically marked, tools which segment sentence into words are required.
</nextsent>
<nextsent>however, this segmentation is normally perform edas preprocessing step using various word seg menters.
</nextsent>
<nextsent>moreover, most of these segment ers are usually trained on manually segmented domain specific corpus, which is not adapted for the specific translation task at hand given that the manual segmentation is performed in monolingual context.
</nextsent>
<nextsent>consequently, such segment ers cannot produce consistently good results when used across different domains.a substantial amount of research has been carried out to address the problems of word segmentation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2291">
<title id=" E09-1063.xml">bilingually motivated domain adapted word segmentation for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we evaluate the performance of our segmentation approach on pb-smt tasks from two domains and demonstrate that our approach scores consistently among the best results across different data conditions.
</prevsent>
<prevsent>state-of-the-art statistical machine translation(smt) requires certain amount of bilingual corpora as training data in order to achieve competitive results.
</prevsent>
</prevsection>
<citsent citstr=" C96-2141 ">
the only assumption of most current statistical models (brown et al, 1993; <papid> J93-2003 </papid>vogel et al, 1996; <papid> C96-2141 </papid>deng and byrne, 2005) <papid> H05-1022 </papid>is that the aligned sentences in such corpora should be segmented into sequences of tokens that are meant to be words.</citsent>
<aftsection>
<nextsent>therefore, for languages where word boundaries are not ortho graphically marked, tools which segment sentence into words are required.
</nextsent>
<nextsent>however, this segmentation is normally perform edas preprocessing step using various word seg menters.
</nextsent>
<nextsent>moreover, most of these segment ers are usually trained on manually segmented domain specific corpus, which is not adapted for the specific translation task at hand given that the manual segmentation is performed in monolingual context.
</nextsent>
<nextsent>consequently, such segment ers cannot produce consistently good results when used across different domains.a substantial amount of research has been carried out to address the problems of word segmentation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2292">
<title id=" E09-1063.xml">bilingually motivated domain adapted word segmentation for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we evaluate the performance of our segmentation approach on pb-smt tasks from two domains and demonstrate that our approach scores consistently among the best results across different data conditions.
</prevsent>
<prevsent>state-of-the-art statistical machine translation(smt) requires certain amount of bilingual corpora as training data in order to achieve competitive results.
</prevsent>
</prevsection>
<citsent citstr=" H05-1022 ">
the only assumption of most current statistical models (brown et al, 1993; <papid> J93-2003 </papid>vogel et al, 1996; <papid> C96-2141 </papid>deng and byrne, 2005) <papid> H05-1022 </papid>is that the aligned sentences in such corpora should be segmented into sequences of tokens that are meant to be words.</citsent>
<aftsection>
<nextsent>therefore, for languages where word boundaries are not ortho graphically marked, tools which segment sentence into words are required.
</nextsent>
<nextsent>however, this segmentation is normally perform edas preprocessing step using various word seg menters.
</nextsent>
<nextsent>moreover, most of these segment ers are usually trained on manually segmented domain specific corpus, which is not adapted for the specific translation task at hand given that the manual segmentation is performed in monolingual context.
</nextsent>
<nextsent>consequently, such segment ers cannot produce consistently good results when used across different domains.a substantial amount of research has been carried out to address the problems of word segmentation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2293">
<title id=" E09-1063.xml">bilingually motivated domain adapted word segmentation for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>moreover, most of these segment ers are usually trained on manually segmented domain specific corpus, which is not adapted for the specific translation task at hand given that the manual segmentation is performed in monolingual context.
</prevsent>
<prevsent>consequently, such segment ers cannot produce consistently good results when used across different domains.a substantial amount of research has been carried out to address the problems of word segmentation.
</prevsent>
</prevsection>
<citsent citstr=" P08-1115 ">
however, most research focuses on combining various segment ers either in smt training or decoding (dyer et al, 2008; <papid> P08-1115 </papid>zhang et al, 2008).<papid> W08-0335 </papid></citsent>
<aftsection>
<nextsent>one important yet often neglected fact is that the optimal segmentation of the source (target) language is dependent on the target (source) language itself, its domain and its genre.
</nextsent>
<nextsent>segmentation considered to be good?
</nextsent>
<nextsent>from monolingual point of view may be un adapted for training alignment models or pb-smt decoding (ma et al, 2007).<papid> P07-1039 </papid>the resulting segmentation will consequently influence the performance of an smt system.in this paper, we propose bilingually motivated automatically domain-adapted approach for smt.</nextsent>
<nextsent>we utilise small bilingual corpus withthe relevant language segmented into basic writing units (e.g. characters for chinese or kana for japanese).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2294">
<title id=" E09-1063.xml">bilingually motivated domain adapted word segmentation for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>moreover, most of these segment ers are usually trained on manually segmented domain specific corpus, which is not adapted for the specific translation task at hand given that the manual segmentation is performed in monolingual context.
</prevsent>
<prevsent>consequently, such segment ers cannot produce consistently good results when used across different domains.a substantial amount of research has been carried out to address the problems of word segmentation.
</prevsent>
</prevsection>
<citsent citstr=" W08-0335 ">
however, most research focuses on combining various segment ers either in smt training or decoding (dyer et al, 2008; <papid> P08-1115 </papid>zhang et al, 2008).<papid> W08-0335 </papid></citsent>
<aftsection>
<nextsent>one important yet often neglected fact is that the optimal segmentation of the source (target) language is dependent on the target (source) language itself, its domain and its genre.
</nextsent>
<nextsent>segmentation considered to be good?
</nextsent>
<nextsent>from monolingual point of view may be un adapted for training alignment models or pb-smt decoding (ma et al, 2007).<papid> P07-1039 </papid>the resulting segmentation will consequently influence the performance of an smt system.in this paper, we propose bilingually motivated automatically domain-adapted approach for smt.</nextsent>
<nextsent>we utilise small bilingual corpus withthe relevant language segmented into basic writing units (e.g. characters for chinese or kana for japanese).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2296">
<title id=" E09-1063.xml">bilingually motivated domain adapted word segmentation for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>one important yet often neglected fact is that the optimal segmentation of the source (target) language is dependent on the target (source) language itself, its domain and its genre.
</prevsent>
<prevsent>segmentation considered to be good?
</prevsent>
</prevsection>
<citsent citstr=" P07-1039 ">
from monolingual point of view may be un adapted for training alignment models or pb-smt decoding (ma et al, 2007).<papid> P07-1039 </papid>the resulting segmentation will consequently influence the performance of an smt system.in this paper, we propose bilingually motivated automatically domain-adapted approach for smt.</citsent>
<aftsection>
<nextsent>we utilise small bilingual corpus withthe relevant language segmented into basic writing units (e.g. characters for chinese or kana for japanese).
</nextsent>
<nextsent>our approach consists of using the output from an existing statistical word alignerto obtain set of candidate words?.
</nextsent>
<nextsent>we evaluate the reliability of these candidates using simple metrics based on co-occurrence frequencies, similar to those used in associative approaches toword alignment (melamed, 2000).<papid> J00-2004 </papid></nextsent>
<nextsent>we then modify the segmentation of the respective sentence sin the parallel corpus according to these candidate words; these modified sentences are then given back to the word aligner, which produces new alignments.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2297">
<title id=" E09-1063.xml">bilingually motivated domain adapted word segmentation for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we utilise small bilingual corpus withthe relevant language segmented into basic writing units (e.g. characters for chinese or kana for japanese).
</prevsent>
<prevsent>our approach consists of using the output from an existing statistical word alignerto obtain set of candidate words?.
</prevsent>
</prevsection>
<citsent citstr=" J00-2004 ">
we evaluate the reliability of these candidates using simple metrics based on co-occurrence frequencies, similar to those used in associative approaches toword alignment (melamed, 2000).<papid> J00-2004 </papid></citsent>
<aftsection>
<nextsent>we then modify the segmentation of the respective sentence sin the parallel corpus according to these candidate words; these modified sentences are then given back to the word aligner, which produces new alignments.
</nextsent>
<nextsent>we evaluate the validity of our approach by measuring the influence of the segmentation process on chinese-to-english machine translation (mt) tasks in two different domains.the remainder of this paper is organised as fol 549 lows.
</nextsent>
<nextsent>in section 2, we study the influence of word segmentation on pb-smt across differentdomains.
</nextsent>
<nextsent>section 3 describes the working mechanism of our bilingually motivated word segmentation approach.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2300">
<title id=" E09-1063.xml">bilingually motivated domain adapted word segmentation for statistical machine translation </title>
<section> the influence of word segmentation.  </section>
<citcontext>
<prevsection>
<prevsent>section 8 concludes and gives avenues for future work.
</prevsent>
<prevsent>on smt: pilot investigation the monolingual word segmentation step intra ditional smt systems has substantial impact on the performance of such systems.
</prevsent>
</prevsection>
<citsent citstr=" W08-0336 ">
a considerable amount of recent research has focused on the influence of word segmentation on smt (ma et al, 2007; <papid> P07-1039 </papid>chang et al, 2008; <papid> W08-0336 </papid>zhang et al, 2008); <papid> W08-0335 </papid>however, most explorations focused on the impact of various segmentation guidelines and the mechanisms of the segment ers themselves.</citsent>
<aftsection>
<nextsent>a current research interest concerns consistency of performance across different domains.
</nextsent>
<nextsent>from our experiments, we show that monolingual segmenterscannot produce consistently good results when applied to new domain.
</nextsent>
<nextsent>our pilot investigation into the influence ofword segmentation on smt involves three off the-shelf chinese word segment ers includingictclas (ict) olympic version1, ldc segmenter2 and stanford segmenter version 2006-05 113.
</nextsent>
<nextsent>both ictclas and stanford segmenters.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2302">
<title id=" E09-1063.xml">bilingually motivated domain adapted word segmentation for statistical machine translation </title>
<section> the influence of word segmentation.  </section>
<citcontext>
<prevsection>
<prevsent>our pilot investigation into the influence ofword segmentation on smt involves three off the-shelf chinese word segment ers includingictclas (ict) olympic version1, ldc segmenter2 and stanford segmenter version 2006-05 113.
</prevsent>
<prevsent>both ictclas and stanford segmenters.
</prevsent>
</prevsection>
<citsent citstr=" W03-1730 ">
utilise machine learning techniques, with hidden markov models for ict (zhang et al, 2003) <papid> W03-1730 </papid>and conditional random fields for the stanford segmenter (tseng et al, 2005).<papid> I05-3027 </papid></citsent>
<aftsection>
<nextsent>both segmentation models were trained on news domain data with named entity recognition functionality.
</nextsent>
<nextsent>theldc segmenter is dictionary-based with word frequency information to help disambiguation, bothof which are collected from data in the news domain.
</nextsent>
<nextsent>we used chinese character-based and manual segment ations as contrastive segmentations.
</nextsent>
<nextsent>the experiments were carried out on range of data sizes from news and dialogue domains using state-of-the-art phrase-based smt (pb-smt) 1http://ictclas.org/index.html 2http://www.ldc.upenn.edu/projects/ chinese 3http://nlp.stanford.edu/software/ segmenter.shtmlsystemmoses (koehn et al, 2007).<papid> P07-2045 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2303">
<title id=" E09-1063.xml">bilingually motivated domain adapted word segmentation for statistical machine translation </title>
<section> the influence of word segmentation.  </section>
<citcontext>
<prevsection>
<prevsent>our pilot investigation into the influence ofword segmentation on smt involves three off the-shelf chinese word segment ers includingictclas (ict) olympic version1, ldc segmenter2 and stanford segmenter version 2006-05 113.
</prevsent>
<prevsent>both ictclas and stanford segmenters.
</prevsent>
</prevsection>
<citsent citstr=" I05-3027 ">
utilise machine learning techniques, with hidden markov models for ict (zhang et al, 2003) <papid> W03-1730 </papid>and conditional random fields for the stanford segmenter (tseng et al, 2005).<papid> I05-3027 </papid></citsent>
<aftsection>
<nextsent>both segmentation models were trained on news domain data with named entity recognition functionality.
</nextsent>
<nextsent>theldc segmenter is dictionary-based with word frequency information to help disambiguation, bothof which are collected from data in the news domain.
</nextsent>
<nextsent>we used chinese character-based and manual segment ations as contrastive segmentations.
</nextsent>
<nextsent>the experiments were carried out on range of data sizes from news and dialogue domains using state-of-the-art phrase-based smt (pb-smt) 1http://ictclas.org/index.html 2http://www.ldc.upenn.edu/projects/ chinese 3http://nlp.stanford.edu/software/ segmenter.shtmlsystemmoses (koehn et al, 2007).<papid> P07-2045 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2304">
<title id=" E09-1063.xml">bilingually motivated domain adapted word segmentation for statistical machine translation </title>
<section> the influence of word segmentation.  </section>
<citcontext>
<prevsection>
<prevsent>theldc segmenter is dictionary-based with word frequency information to help disambiguation, bothof which are collected from data in the news domain.
</prevsent>
<prevsent>we used chinese character-based and manual segment ations as contrastive segmentations.
</prevsent>
</prevsection>
<citsent citstr=" P07-2045 ">
the experiments were carried out on range of data sizes from news and dialogue domains using state-of-the-art phrase-based smt (pb-smt) 1http://ictclas.org/index.html 2http://www.ldc.upenn.edu/projects/ chinese 3http://nlp.stanford.edu/software/ segmenter.shtmlsystemmoses (koehn et al, 2007).<papid> P07-2045 </papid></citsent>
<aftsection>
<nextsent>the performance of pb-smt system is measured with bleu score (papineni et al, 2002).<papid> P02-1040 </papid>we firstly measure the influence of word segmentation on in-domain data with respect to the three abovementioned segment ers, namely un data from the nist 2006 evaluation campaign.</nextsent>
<nextsent>ascan be seen from table 1, using monolingual segment ers achieves consistently better smt performance than character-based segmentation (cs) on different data sizes, which means character-based segmentation is not good enough for this domain where the vocabulary tends to be large.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2305">
<title id=" E09-1063.xml">bilingually motivated domain adapted word segmentation for statistical machine translation </title>
<section> the influence of word segmentation.  </section>
<citcontext>
<prevsection>
<prevsent>we used chinese character-based and manual segment ations as contrastive segmentations.
</prevsent>
<prevsent>the experiments were carried out on range of data sizes from news and dialogue domains using state-of-the-art phrase-based smt (pb-smt) 1http://ictclas.org/index.html 2http://www.ldc.upenn.edu/projects/ chinese 3http://nlp.stanford.edu/software/ segmenter.shtmlsystemmoses (koehn et al, 2007).<papid> P07-2045 </papid></prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
the performance of pb-smt system is measured with bleu score (papineni et al, 2002).<papid> P02-1040 </papid>we firstly measure the influence of word segmentation on in-domain data with respect to the three abovementioned segment ers, namely un data from the nist 2006 evaluation campaign.</citsent>
<aftsection>
<nextsent>ascan be seen from table 1, using monolingual segment ers achieves consistently better smt performance than character-based segmentation (cs) on different data sizes, which means character-based segmentation is not good enough for this domain where the vocabulary tends to be large.
</nextsent>
<nextsent>we can also observe that the ict and stanford segmenter consistently outperform the ldc segmenter.
</nextsent>
<nextsent>even using 3m sentence pairs for training, the differences between them are still statistically significant (p   0.05) using approximate random isation (noreen, 1989) for significance testing.
</nextsent>
<nextsent>40k 160k 640k 3m cs 8.33 12.47 14.40 17.80 ict 10.17 14.85 17.20 20.50 ldc 9.37 13.88 15.86 19.59 stanford 10.45 15.26 16.94 20.64 table 1: word segmentation on nist datasets however, when tested on out-of-domain data,i.e. iwslt data in the dialogue domain, there sults seem to be more difficult to predict.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2311">
<title id=" E09-1063.xml">bilingually motivated domain adapted word segmentation for statistical machine translation </title>
<section> experimental setting.  </section>
<citcontext>
<prevsection>
<prevsent>the intrinsic quality of word segmentation is normally evaluated against manually segmented gold-standard corpus using f-score.
</prevsent>
<prevsent>while this approach can give direct evaluation of the quality of the word segmentation, it is faced with several limitations.
</prevsent>
</prevsection>
<citsent citstr=" J96-3004 ">
first of all, it is really difficult to build reliable and objective gold-standard given the fact that there is only 70% agreement between native speakers on this task (sproat et al, 1996).<papid> J96-3004 </papid>second, an increase in f-score does not necessarily imply an improvement in translation quality.</citsent>
<aftsection>
<nextsent>ithas been shown that f-score has very weak correlation with smt translation quality in terms of bleu score (zhang et al, 2008).<papid> W08-0335 </papid></nextsent>
<nextsent>consequently, we chose to extrinsic ally evaluate the performance of our approach via the chinese english translation task, i.e. we measure the influence of the segmentation process on the final translation out put.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2314">
<title id=" E09-1063.xml">bilingually motivated domain adapted word segmentation for statistical machine translation </title>
<section> experimental setting.  </section>
<citcontext>
<prevsection>
<prevsent>ithas been shown that f-score has very weak correlation with smt translation quality in terms of bleu score (zhang et al, 2008).<papid> W08-0335 </papid></prevsent>
<prevsent>consequently, we chose to extrinsic ally evaluate the performance of our approach via the chinese english translation task, i.e. we measure the influence of the segmentation process on the final translation out put.</prevsent>
</prevsection>
<citsent citstr=" W05-0909 ">
the quality of the translation output is mainly evaluated using bleu, with nist (doddington, 2002) and meteor (banerjee and lavie, 2005) <papid> W05-0909 </papid>as complementary metrics.</citsent>
<aftsection>
<nextsent>5.2 data.
</nextsent>
<nextsent>the data we used in our experiments are fromtwo different domains, namely news and travel dialogues.
</nextsent>
<nextsent>for the news domain, we trained our system using portion of un data for nist2006 evaluation campaign.
</nextsent>
<nextsent>the system was developed on ldc multiple-translation chinese (mtc) corpus and tested on mtc part 2, which was also used as test set for nist 2002 evaluation campaign.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2315">
<title id=" E09-1063.xml">bilingually motivated domain adapted word segmentation for statistical machine translation </title>
<section> experimental setting.  </section>
<citcontext>
<prevsection>
<prevsent>the various statistics for the corpora are shown in table 3.
</prevsent>
<prevsent>5.3 baseline system.
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
we conducted experiments using different seg menters with standard log-linear pb-smt model: giza++ implementation of ibm word alignment model 4 (och and ney, 2003), <papid> J03-1002 </papid>the refinement and phrase-extraction heuristics described in (koehn et al, 2003), <papid> N03-1017 </papid>minimum-error rate training (och, 2003), <papid> P03-1021 </papid>5-gram language model with kneser-ney smoothing trained with srilm (stolcke, 2002) on the english side of the training data, and moses (koehn et al, 2007; <papid> P07-2045 </papid>dyeret al, 2008) <papid> P08-1115 </papid>to translate both single best segmentation and word lattices.</citsent>
<aftsection>
<nextsent>6.1 results.
</nextsent>
<nextsent>the initial word alignments are obtained using the baseline configuration described above by segmenting the chinese sentences into characters.
</nextsent>
<nextsent>from these we build bilingual 1-to-n dictionary, and the training corpus is updated by grouping the characters in the dictionaries into single word,using the method presented in section 3.4.
</nextsent>
<nextsent>as previously mentioned, this process can be repeated several times.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2316">
<title id=" E09-1063.xml">bilingually motivated domain adapted word segmentation for statistical machine translation </title>
<section> experimental setting.  </section>
<citcontext>
<prevsection>
<prevsent>the various statistics for the corpora are shown in table 3.
</prevsent>
<prevsent>5.3 baseline system.
</prevsent>
</prevsection>
<citsent citstr=" N03-1017 ">
we conducted experiments using different seg menters with standard log-linear pb-smt model: giza++ implementation of ibm word alignment model 4 (och and ney, 2003), <papid> J03-1002 </papid>the refinement and phrase-extraction heuristics described in (koehn et al, 2003), <papid> N03-1017 </papid>minimum-error rate training (och, 2003), <papid> P03-1021 </papid>5-gram language model with kneser-ney smoothing trained with srilm (stolcke, 2002) on the english side of the training data, and moses (koehn et al, 2007; <papid> P07-2045 </papid>dyeret al, 2008) <papid> P08-1115 </papid>to translate both single best segmentation and word lattices.</citsent>
<aftsection>
<nextsent>6.1 results.
</nextsent>
<nextsent>the initial word alignments are obtained using the baseline configuration described above by segmenting the chinese sentences into characters.
</nextsent>
<nextsent>from these we build bilingual 1-to-n dictionary, and the training corpus is updated by grouping the characters in the dictionaries into single word,using the method presented in section 3.4.
</nextsent>
<nextsent>as previously mentioned, this process can be repeated several times.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2317">
<title id=" E09-1063.xml">bilingually motivated domain adapted word segmentation for statistical machine translation </title>
<section> experimental setting.  </section>
<citcontext>
<prevsection>
<prevsent>the various statistics for the corpora are shown in table 3.
</prevsent>
<prevsent>5.3 baseline system.
</prevsent>
</prevsection>
<citsent citstr=" P03-1021 ">
we conducted experiments using different seg menters with standard log-linear pb-smt model: giza++ implementation of ibm word alignment model 4 (och and ney, 2003), <papid> J03-1002 </papid>the refinement and phrase-extraction heuristics described in (koehn et al, 2003), <papid> N03-1017 </papid>minimum-error rate training (och, 2003), <papid> P03-1021 </papid>5-gram language model with kneser-ney smoothing trained with srilm (stolcke, 2002) on the english side of the training data, and moses (koehn et al, 2007; <papid> P07-2045 </papid>dyeret al, 2008) <papid> P08-1115 </papid>to translate both single best segmentation and word lattices.</citsent>
<aftsection>
<nextsent>6.1 results.
</nextsent>
<nextsent>the initial word alignments are obtained using the baseline configuration described above by segmenting the chinese sentences into characters.
</nextsent>
<nextsent>from these we build bilingual 1-to-n dictionary, and the training corpus is updated by grouping the characters in the dictionaries into single word,using the method presented in section 3.4.
</nextsent>
<nextsent>as previously mentioned, this process can be repeated several times.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2322">
<title id=" E09-1063.xml">bilingually motivated domain adapted word segmentation for statistical machine translation </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>the above experiments relyon giza++ to perform word alignment.
</prevsent>
<prevsent>we next show that our approach is not dependent on the word aligner given that we have conservative reliability estimation procedure.
</prevsent>
</prevsection>
<citsent citstr=" N06-4004 ">
table 11 shows the results obtained on the iwslt dataset using the mttk alignment tool (deng and byrne, 2005; <papid> H05-1022 </papid>deng and byrne, 2006).<papid> N06-4004 </papid></citsent>
<aftsection>
<nextsent>iwslt06 iwslt07 cs 21.04 31.41 ict 20.48 31.11 ldc 20.79 30.51 stanford 17.84 29.35 bs-singlebest 19.22 29.75 bs-wordlattice 21.76 31.75 table 11: bs on iwslt datasets using mttk
</nextsent>
<nextsent>(xu et al, 2004) <papid> W04-1118 </papid>were the first to question the use of word segmentation in smt and showed that the segmentation proposed by word alignments can beused in smt to achieve competitive results compared to using monolingual segmenters.</nextsent>
<nextsent>our approach differs from theirs in two aspects.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2323">
<title id=" E09-1063.xml">bilingually motivated domain adapted word segmentation for statistical machine translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>table 11 shows the results obtained on the iwslt dataset using the mttk alignment tool (deng and byrne, 2005; <papid> H05-1022 </papid>deng and byrne, 2006).<papid> N06-4004 </papid></prevsent>
<prevsent>iwslt06 iwslt07 cs 21.04 31.41 ict 20.48 31.11 ldc 20.79 30.51 stanford 17.84 29.35 bs-singlebest 19.22 29.75 bs-wordlattice 21.76 31.75 table 11: bs on iwslt datasets using mttk</prevsent>
</prevsection>
<citsent citstr=" W04-1118 ">
(xu et al, 2004) <papid> W04-1118 </papid>were the first to question the use of word segmentation in smt and showed that the segmentation proposed by word alignments can beused in smt to achieve competitive results compared to using monolingual segmenters.</citsent>
<aftsection>
<nextsent>our approach differs from theirs in two aspects.
</nextsent>
<nextsent>firstly, (xu et al, 2004) <papid> W04-1118 </papid>use word align ers to reconstruct (monolingual) chinese dictionary and reuse this dictionary to segment chinese sentences as other monolingual segmenters.</nextsent>
<nextsent>our approach features the use of bilingual dictionary and conducts adifferent segmentation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2330">
<title id=" E06-2004.xml">dude a dialogue and understanding development environment mapping business process models to information state update dialogue systems </title>
<section> introduction: business process.  </section>
<citcontext>
<prevsection>
<prevsent>rather than having to laboriously navigate series of multiple-choiceoptions.
</prevsent>
<prevsent>this sort of how may help you??
</prevsent>
</prevsection>
<citsent citstr=" A00-2028 ">
system is easily within current dialogue system expertise(walker et al, 2000), <papid> A00-2028 </papid>but has not seen widespread commercial deployment.</citsent>
<aftsection>
<nextsent>another possibility opened up by the use of dialogue technology is the personalization of the dialogue with the customer.
</nextsent>
<nextsent>by interacting witha model of the customers preferences dialogue interface is able to recommend appropriate services for the customer (moore et al, 2004), as well as modify its interaction style.
</nextsent>
<nextsent>dude targets development of flexible and robust isu dialogue systems from bpms and databases.
</nextsent>
<nextsent>its main components are:  graphical business process modelling tool (graham technology plc, 2005) (java)  dipper generic dialogue manager (bos et al, 2003) (java or prolog)  mysql databases  development gui (java), see section 2.2 the spoken dialogue systems produced by dude all run using the open agent architecture (oaa) (cheyer and martin, 2001) and employ the following agents in addition to dipper:  grammatical framework (gf) parser (ranta, 2004) (java)  bpm agent (java) and database agent (java)  htk speech recognizer (young, 1995) using atk (or alternatively nuance)  festival2 speech synthesizer (taylor et al, 1998) we now highlight generic dialogue management, the dude developer gui, and the use of gf.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2331">
<title id=" E06-2004.xml">dude a dialogue and understanding development environment mapping business process models to information state update dialogue systems </title>
<section> dude: development environment.  </section>
<citcontext>
<prevsection>
<prevsent>using bpms and related authoring tools to specify dialogue interactions addresses this problem and requires the development of domain general dialogue managers, where bpms represent application-specific information.
</prevsent>
<prevsent>100 we have developed generic dialogue manager (dm) using dipper.
</prevsent>
</prevsection>
<citsent citstr=" E06-2009 ">
the core dm rules cover mixed initiative dialogue for multiple tasks (e.g. bpm with several sub-processes), explicit and implicit confirmation, help, restart, repeat, and quit commands, and presentation and refinement of database query results.this is domain-neutral abstraction of the isu dialogue managers implemented for the flights and talk systems (moore et al, 2004; lemon et al, 2006).<papid> E06-2009 </papid></citsent>
<aftsection>
<nextsent>the key point here is that the dm consults the bpm to determine what task-based steps to take next (e.g. ask for cinema name), when appropriate.
</nextsent>
<nextsent>domain-general aspects of dialogue (e.g. confirmation and clarificationstrategies) are handled by the core dm.
</nextsent>
<nextsent>values for constraints on transitions and branching in the bpm (e.g. present insurance option if the user is business-class)are compiled into domain-specific parts of the information state.
</nextsent>
<nextsent>we use an xml format for bpms, and compile them into finite state machines (the bpm agent) consulted by dipper for task-based dialogue control.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2332">
<title id=" E09-1054.xml">lattice parsing to integrate speech recognition and rule based machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>tomita (1986) introduced the concept of word lattice parsing for the purpose of speech recognition and used an lr parser.
</prevsent>
<prevsent>next, paeseler (1988)used chart parser to process word-lattices.
</prevsent>
</prevsection>
<citsent citstr=" P08-1115 ">
how ever, to the best of our knowledge, the specific method for chart parsing word graph introduced in this paper has not been previously used for coupling purposes.recent studies point out the importance of utilizing word graphs in speech tasks (dyer et al, 2008).<papid> P08-1115 </papid></citsent>
<aftsection>
<nextsent>previous work on language modeling can be classified according to whether system uses purely statistical methods or whether it uses themin combination with syntactic methods.
</nextsent>
<nextsent>in this paper, the focus is on systems that contain syntactic approaches.
</nextsent>
<nextsent>in general, these language modeling approaches try to parse the asr output in word lattice format in order to choose the most probable hypothesis.
</nextsent>
<nextsent>chow and roukos (1989) used unification-based cyk parser for the purpose of speech understanding.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2333">
<title id=" E09-1054.xml">lattice parsing to integrate speech recognition and rule based machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in general, these language modeling approaches try to parse the asr output in word lattice format in order to choose the most probable hypothesis.
</prevsent>
<prevsent>chow and roukos (1989) used unification-based cyk parser for the purpose of speech understanding.
</prevsent>
</prevsection>
<citsent citstr=" C90-2011 ">
chien et al (1990) <papid> C90-2011 </papid>and weber (1994) utilized probabilistic context free grammars in conjunction with unification grammars tochart-parse word-lattice.</citsent>
<aftsection>
<nextsent>there are various differences between the work of chien et al (1990) <papid> C90-2011 </papid>and weber (1994) and the work presented in this paper.</nextsent>
<nextsent>first, in the previously mentioned studies, the chart is populated with the same word graph that comes from the speech recognizer without any pruning, whereas in our approach the word graph is reduced to an acceptable size.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2337">
<title id=" E09-1054.xml">lattice parsing to integrate speech recognition and rule based machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>otherwise, the efficiency becomes big challenge because the search space introduced by chart with over thousands of initial edges can easily be beyond current practical limits.
</prevsent>
<prevsent>another important difference inour approach is the modification of the chart parsing algorithm to eliminate spurious parses.ney (1991) deals with the use of probabilistic cyk parser for conti nous speech recognition task.
</prevsent>
</prevsection>
<citsent citstr=" J95-2002 ">
stolcke (1995) <papid> J95-2002 </papid>summarizes extensively their approach to utilize probabilistic earley parsing.chappelier et al (1999) gives an overview of different approaches to integrate linguistic models into speech recognition systems.</citsent>
<aftsection>
<nextsent>they also research various techniques of producing sets of hypotheses that contain more semantic?
</nextsent>
<nextsent>variability than the commonly used ones.
</nextsent>
<nextsent>some of the recent studies about structural language modeling extract list of n-best hypotheses using ann-gram and then apply structural methods to decide on the best hypothesis (chelba, 2000; roark, 2001).<papid> J01-2004 </papid></nextsent>
<nextsent>this contrasts with the approach presented in this study where, instead of single sentence,the word-lattice is parsed.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2338">
<title id=" E09-1054.xml">lattice parsing to integrate speech recognition and rule based machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>they also research various techniques of producing sets of hypotheses that contain more semantic?
</prevsent>
<prevsent>variability than the commonly used ones.
</prevsent>
</prevsection>
<citsent citstr=" J01-2004 ">
some of the recent studies about structural language modeling extract list of n-best hypotheses using ann-gram and then apply structural methods to decide on the best hypothesis (chelba, 2000; roark, 2001).<papid> J01-2004 </papid></citsent>
<aftsection>
<nextsent>this contrasts with the approach presented in this study where, instead of single sentence,the word-lattice is parsed.
</nextsent>
<nextsent>parsing all sentence hypotheses simultaneously enables reduction in the number of edges produced during the parsing process.
</nextsent>
<nextsent>this is because the shared word hypotheses are processed only once compared to the nbest list approach, where the shared words are processed each time they occur in hypothesis.
</nextsent>
<nextsent>similar to the current work, other studies parse the whole word-lattice without extracting list (hall, 2005).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2340">
<title id=" E09-1054.xml">lattice parsing to integrate speech recognition and rule based machine translation </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>another common source of error in parsing is because of un normalized text.
</prevsent>
<prevsent>table 3: wer taken from hall and johnson(2003) for various language models on hub-1 lattices in addition to our approach presented in the fifth row.
</prevsent>
</prevsection>
<citsent citstr=" P01-1017 ">
model wer charniak parser (charniak, 2001) <papid> P01-1017 </papid>11.8 attention shifting 11.9 (hall and johnson, 2004) <papid> P04-1006 </papid>pcfg (hall, 2005) 12.0 a* decoding (xu et al, 2002) <papid> P02-1025 </papid>12.3 n-best word graph (this study) 12.6 pcfg (roark, 2001) <papid> J01-2004 </papid>12.7 pcfg (hall and johnson, 2004) <papid> P04-1006 </papid>13.0 40m-word trigram 13.7 (hall and johnson, 2003) pcfg (hall and johnson, 2003) 15.5</citsent>
<aftsection>
<nextsent>the primary aim of this research was to propose new and efficient method for integrating an sr system with an mt system employing chart parser.
</nextsent>
<nextsent>the main idea is to populate the initial chart parser with the word graph that comes out of the sr component.this paper presents an attempt to blend statistical sr systems with rule-based mt systems.
</nextsent>
<nextsent>thegoal of the final assembly of these two components was to achieve an enhanced speech translation (st) system.
</nextsent>
<nextsent>specifically, we propose to parse the word graph generated by the sr module inside the rule-based parser.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2341">
<title id=" E09-1054.xml">lattice parsing to integrate speech recognition and rule based machine translation </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>another common source of error in parsing is because of un normalized text.
</prevsent>
<prevsent>table 3: wer taken from hall and johnson(2003) for various language models on hub-1 lattices in addition to our approach presented in the fifth row.
</prevsent>
</prevsection>
<citsent citstr=" P04-1006 ">
model wer charniak parser (charniak, 2001) <papid> P01-1017 </papid>11.8 attention shifting 11.9 (hall and johnson, 2004) <papid> P04-1006 </papid>pcfg (hall, 2005) 12.0 a* decoding (xu et al, 2002) <papid> P02-1025 </papid>12.3 n-best word graph (this study) 12.6 pcfg (roark, 2001) <papid> J01-2004 </papid>12.7 pcfg (hall and johnson, 2004) <papid> P04-1006 </papid>13.0 40m-word trigram 13.7 (hall and johnson, 2003) pcfg (hall and johnson, 2003) 15.5</citsent>
<aftsection>
<nextsent>the primary aim of this research was to propose new and efficient method for integrating an sr system with an mt system employing chart parser.
</nextsent>
<nextsent>the main idea is to populate the initial chart parser with the word graph that comes out of the sr component.this paper presents an attempt to blend statistical sr systems with rule-based mt systems.
</nextsent>
<nextsent>thegoal of the final assembly of these two components was to achieve an enhanced speech translation (st) system.
</nextsent>
<nextsent>specifically, we propose to parse the word graph generated by the sr module inside the rule-based parser.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2343">
<title id=" E09-1054.xml">lattice parsing to integrate speech recognition and rule based machine translation </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>another common source of error in parsing is because of un normalized text.
</prevsent>
<prevsent>table 3: wer taken from hall and johnson(2003) for various language models on hub-1 lattices in addition to our approach presented in the fifth row.
</prevsent>
</prevsection>
<citsent citstr=" P02-1025 ">
model wer charniak parser (charniak, 2001) <papid> P01-1017 </papid>11.8 attention shifting 11.9 (hall and johnson, 2004) <papid> P04-1006 </papid>pcfg (hall, 2005) 12.0 a* decoding (xu et al, 2002) <papid> P02-1025 </papid>12.3 n-best word graph (this study) 12.6 pcfg (roark, 2001) <papid> J01-2004 </papid>12.7 pcfg (hall and johnson, 2004) <papid> P04-1006 </papid>13.0 40m-word trigram 13.7 (hall and johnson, 2003) pcfg (hall and johnson, 2003) 15.5</citsent>
<aftsection>
<nextsent>the primary aim of this research was to propose new and efficient method for integrating an sr system with an mt system employing chart parser.
</nextsent>
<nextsent>the main idea is to populate the initial chart parser with the word graph that comes out of the sr component.this paper presents an attempt to blend statistical sr systems with rule-based mt systems.
</nextsent>
<nextsent>thegoal of the final assembly of these two components was to achieve an enhanced speech translation (st) system.
</nextsent>
<nextsent>specifically, we propose to parse the word graph generated by the sr module inside the rule-based parser.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2347">
<title id=" E09-1014.xml">human evaluation of a german surface realisation ranker </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>an important component of research on surface realisation (the task of generating strings for agiven abstract representation) is evaluation, especially if we want to be able to compare across systems.
</prevsent>
<prevsent>there is consensus that exact match with respect to an actually observed corpus sentence is too strict metric and that bleu score measured against corpus sentences can only give rough impression of the quality of the system output.
</prevsent>
</prevsection>
<citsent citstr=" W00-1401 ">
it is unclear, however, what kind of metric would bemost suitable for the evaluation of string realisations, so that, as result, there have been range of automatic metrics applied including inter alia exact match, string edit distance, nist ssa, bleu,nist, rouge, generation string accuracy, generation tree accuracy, word accuracy (bangalore et al., 2000; <papid> W00-1401 </papid>callaway, 2003; nakanishi et al, 2005; <papid> W05-1510 </papid>velldal and oepen, 2006; <papid> W06-1661 </papid>belz and reiter, 2006).<papid> E06-1040 </papid>it is not always clear how appropriate these metrics are, especially at the level of individual sen tences.</citsent>
<aftsection>
<nextsent>using automatic evaluation metrics cannot be avoided, but ideally, metric for the evaluation of realisation rankers would rank alternative realisations in the same way as native speakers of the language for which the surface realisation system is developed, and not only globally, but also at the level of individual sentences.
</nextsent>
<nextsent>another major consideration in evaluation iswhat to take as the gold standard.
</nextsent>
<nextsent>the easiest option is to take the original corpus string that was used to produce the abstract representation from which we generate.
</nextsent>
<nextsent>however, there may well be other realisations of the same input that are as suitable in the given context.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2348">
<title id=" E09-1014.xml">human evaluation of a german surface realisation ranker </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>an important component of research on surface realisation (the task of generating strings for agiven abstract representation) is evaluation, especially if we want to be able to compare across systems.
</prevsent>
<prevsent>there is consensus that exact match with respect to an actually observed corpus sentence is too strict metric and that bleu score measured against corpus sentences can only give rough impression of the quality of the system output.
</prevsent>
</prevsection>
<citsent citstr=" W05-1510 ">
it is unclear, however, what kind of metric would bemost suitable for the evaluation of string realisations, so that, as result, there have been range of automatic metrics applied including inter alia exact match, string edit distance, nist ssa, bleu,nist, rouge, generation string accuracy, generation tree accuracy, word accuracy (bangalore et al., 2000; <papid> W00-1401 </papid>callaway, 2003; nakanishi et al, 2005; <papid> W05-1510 </papid>velldal and oepen, 2006; <papid> W06-1661 </papid>belz and reiter, 2006).<papid> E06-1040 </papid>it is not always clear how appropriate these metrics are, especially at the level of individual sen tences.</citsent>
<aftsection>
<nextsent>using automatic evaluation metrics cannot be avoided, but ideally, metric for the evaluation of realisation rankers would rank alternative realisations in the same way as native speakers of the language for which the surface realisation system is developed, and not only globally, but also at the level of individual sentences.
</nextsent>
<nextsent>another major consideration in evaluation iswhat to take as the gold standard.
</nextsent>
<nextsent>the easiest option is to take the original corpus string that was used to produce the abstract representation from which we generate.
</nextsent>
<nextsent>however, there may well be other realisations of the same input that are as suitable in the given context.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2349">
<title id=" E09-1014.xml">human evaluation of a german surface realisation ranker </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>an important component of research on surface realisation (the task of generating strings for agiven abstract representation) is evaluation, especially if we want to be able to compare across systems.
</prevsent>
<prevsent>there is consensus that exact match with respect to an actually observed corpus sentence is too strict metric and that bleu score measured against corpus sentences can only give rough impression of the quality of the system output.
</prevsent>
</prevsection>
<citsent citstr=" W06-1661 ">
it is unclear, however, what kind of metric would bemost suitable for the evaluation of string realisations, so that, as result, there have been range of automatic metrics applied including inter alia exact match, string edit distance, nist ssa, bleu,nist, rouge, generation string accuracy, generation tree accuracy, word accuracy (bangalore et al., 2000; <papid> W00-1401 </papid>callaway, 2003; nakanishi et al, 2005; <papid> W05-1510 </papid>velldal and oepen, 2006; <papid> W06-1661 </papid>belz and reiter, 2006).<papid> E06-1040 </papid>it is not always clear how appropriate these metrics are, especially at the level of individual sen tences.</citsent>
<aftsection>
<nextsent>using automatic evaluation metrics cannot be avoided, but ideally, metric for the evaluation of realisation rankers would rank alternative realisations in the same way as native speakers of the language for which the surface realisation system is developed, and not only globally, but also at the level of individual sentences.
</nextsent>
<nextsent>another major consideration in evaluation iswhat to take as the gold standard.
</nextsent>
<nextsent>the easiest option is to take the original corpus string that was used to produce the abstract representation from which we generate.
</nextsent>
<nextsent>however, there may well be other realisations of the same input that are as suitable in the given context.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2350">
<title id=" E09-1014.xml">human evaluation of a german surface realisation ranker </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>an important component of research on surface realisation (the task of generating strings for agiven abstract representation) is evaluation, especially if we want to be able to compare across systems.
</prevsent>
<prevsent>there is consensus that exact match with respect to an actually observed corpus sentence is too strict metric and that bleu score measured against corpus sentences can only give rough impression of the quality of the system output.
</prevsent>
</prevsection>
<citsent citstr=" E06-1040 ">
it is unclear, however, what kind of metric would bemost suitable for the evaluation of string realisations, so that, as result, there have been range of automatic metrics applied including inter alia exact match, string edit distance, nist ssa, bleu,nist, rouge, generation string accuracy, generation tree accuracy, word accuracy (bangalore et al., 2000; <papid> W00-1401 </papid>callaway, 2003; nakanishi et al, 2005; <papid> W05-1510 </papid>velldal and oepen, 2006; <papid> W06-1661 </papid>belz and reiter, 2006).<papid> E06-1040 </papid>it is not always clear how appropriate these metrics are, especially at the level of individual sen tences.</citsent>
<aftsection>
<nextsent>using automatic evaluation metrics cannot be avoided, but ideally, metric for the evaluation of realisation rankers would rank alternative realisations in the same way as native speakers of the language for which the surface realisation system is developed, and not only globally, but also at the level of individual sentences.
</nextsent>
<nextsent>another major consideration in evaluation iswhat to take as the gold standard.
</nextsent>
<nextsent>the easiest option is to take the original corpus string that was used to produce the abstract representation from which we generate.
</nextsent>
<nextsent>however, there may well be other realisations of the same input that are as suitable in the given context.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2353">
<title id=" E09-1055.xml">treebank grammar techniques for non projective dependency parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>one of the key choices in dependency parsing is about the class of candidate structures for this prediction.
</prevsent>
<prevsent>many parsers are confined to projective structures, in which the yield of syntactic head is required to be continuous.
</prevsent>
</prevsection>
<citsent citstr=" C96-1058 ">
a major benefit of this choice is computational efficiency: an exhaustive search over all projective structures can be done in cubic, greedy parsing in linear time (eisner, 1996; <papid> C96-1058 </papid>nivre, 2003).</citsent>
<aftsection>
<nextsent>a major drawback of the restriction to projective dependency structures is potential loss inaccuracy.
</nextsent>
<nextsent>for example, around 23% of the analyses in the prague dependency treebank of czech (hajic?
</nextsent>
<nextsent>et al , 2001) are non projective, and for german and dutch treebanks, the proportion of non-projective structures is even higher (havelka, 2007).<papid> P07-1077 </papid>the problem of non-projective dependency parsing under the joint requirement of accuracy and efficiency has only recently been addressed in theliterature.</nextsent>
<nextsent>some authors propose to solve it by techniques for recovering non-projectivity from the out put of projective parser in post-processing step (hall and novk, 2005; nivre and nilsson, 2005), <papid> P05-1013 </papid>others extend projective parsers by heuristics that allow at least certain non-projective construction sto be parsed (attardi, 2006; <papid> W06-2922 </papid>nivre, 2007).<papid> N07-1050 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2354">
<title id=" E09-1055.xml">treebank grammar techniques for non projective dependency parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a major drawback of the restriction to projective dependency structures is potential loss inaccuracy.
</prevsent>
<prevsent>for example, around 23% of the analyses in the prague dependency treebank of czech (hajic?
</prevsent>
</prevsection>
<citsent citstr=" P07-1077 ">
et al , 2001) are non projective, and for german and dutch treebanks, the proportion of non-projective structures is even higher (havelka, 2007).<papid> P07-1077 </papid>the problem of non-projective dependency parsing under the joint requirement of accuracy and efficiency has only recently been addressed in theliterature.</citsent>
<aftsection>
<nextsent>some authors propose to solve it by techniques for recovering non-projectivity from the out put of projective parser in post-processing step (hall and novk, 2005; nivre and nilsson, 2005), <papid> P05-1013 </papid>others extend projective parsers by heuristics that allow at least certain non-projective construction sto be parsed (attardi, 2006; <papid> W06-2922 </papid>nivre, 2007).<papid> N07-1050 </papid></nextsent>
<nextsent>mcdonald et al  (2005) <papid> H05-1066 </papid>formulate dependency parsing as the search for the most probable spanning tree over the full set of all possible dependencies.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2355">
<title id=" E09-1055.xml">treebank grammar techniques for non projective dependency parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for example, around 23% of the analyses in the prague dependency treebank of czech (hajic?
</prevsent>
<prevsent>et al , 2001) are non projective, and for german and dutch treebanks, the proportion of non-projective structures is even higher (havelka, 2007).<papid> P07-1077 </papid>the problem of non-projective dependency parsing under the joint requirement of accuracy and efficiency has only recently been addressed in theliterature.</prevsent>
</prevsection>
<citsent citstr=" P05-1013 ">
some authors propose to solve it by techniques for recovering non-projectivity from the out put of projective parser in post-processing step (hall and novk, 2005; nivre and nilsson, 2005), <papid> P05-1013 </papid>others extend projective parsers by heuristics that allow at least certain non-projective construction sto be parsed (attardi, 2006; <papid> W06-2922 </papid>nivre, 2007).<papid> N07-1050 </papid></citsent>
<aftsection>
<nextsent>mcdonald et al  (2005) <papid> H05-1066 </papid>formulate dependency parsing as the search for the most probable spanning tree over the full set of all possible dependencies.</nextsent>
<nextsent>however, this approach is limited to probability models with strong independence assumptions.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2356">
<title id=" E09-1055.xml">treebank grammar techniques for non projective dependency parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for example, around 23% of the analyses in the prague dependency treebank of czech (hajic?
</prevsent>
<prevsent>et al , 2001) are non projective, and for german and dutch treebanks, the proportion of non-projective structures is even higher (havelka, 2007).<papid> P07-1077 </papid>the problem of non-projective dependency parsing under the joint requirement of accuracy and efficiency has only recently been addressed in theliterature.</prevsent>
</prevsection>
<citsent citstr=" W06-2922 ">
some authors propose to solve it by techniques for recovering non-projectivity from the out put of projective parser in post-processing step (hall and novk, 2005; nivre and nilsson, 2005), <papid> P05-1013 </papid>others extend projective parsers by heuristics that allow at least certain non-projective construction sto be parsed (attardi, 2006; <papid> W06-2922 </papid>nivre, 2007).<papid> N07-1050 </papid></citsent>
<aftsection>
<nextsent>mcdonald et al  (2005) <papid> H05-1066 </papid>formulate dependency parsing as the search for the most probable spanning tree over the full set of all possible dependencies.</nextsent>
<nextsent>however, this approach is limited to probability models with strong independence assumptions.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2357">
<title id=" E09-1055.xml">treebank grammar techniques for non projective dependency parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for example, around 23% of the analyses in the prague dependency treebank of czech (hajic?
</prevsent>
<prevsent>et al , 2001) are non projective, and for german and dutch treebanks, the proportion of non-projective structures is even higher (havelka, 2007).<papid> P07-1077 </papid>the problem of non-projective dependency parsing under the joint requirement of accuracy and efficiency has only recently been addressed in theliterature.</prevsent>
</prevsection>
<citsent citstr=" N07-1050 ">
some authors propose to solve it by techniques for recovering non-projectivity from the out put of projective parser in post-processing step (hall and novk, 2005; nivre and nilsson, 2005), <papid> P05-1013 </papid>others extend projective parsers by heuristics that allow at least certain non-projective construction sto be parsed (attardi, 2006; <papid> W06-2922 </papid>nivre, 2007).<papid> N07-1050 </papid></citsent>
<aftsection>
<nextsent>mcdonald et al  (2005) <papid> H05-1066 </papid>formulate dependency parsing as the search for the most probable spanning tree over the full set of all possible dependencies.</nextsent>
<nextsent>however, this approach is limited to probability models with strong independence assumptions.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2358">
<title id=" E09-1055.xml">treebank grammar techniques for non projective dependency parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>et al , 2001) are non projective, and for german and dutch treebanks, the proportion of non-projective structures is even higher (havelka, 2007).<papid> P07-1077 </papid>the problem of non-projective dependency parsing under the joint requirement of accuracy and efficiency has only recently been addressed in theliterature.</prevsent>
<prevsent>some authors propose to solve it by techniques for recovering non-projectivity from the out put of projective parser in post-processing step (hall and novk, 2005; nivre and nilsson, 2005), <papid> P05-1013 </papid>others extend projective parsers by heuristics that allow at least certain non-projective construction sto be parsed (attardi, 2006; <papid> W06-2922 </papid>nivre, 2007).<papid> N07-1050 </papid></prevsent>
</prevsection>
<citsent citstr=" H05-1066 ">
mcdonald et al  (2005) <papid> H05-1066 </papid>formulate dependency parsing as the search for the most probable spanning tree over the full set of all possible dependencies.</citsent>
<aftsection>
<nextsent>however, this approach is limited to probability models with strong independence assumptions.
</nextsent>
<nextsent>exhaustive non projective dependency parsing with more powerful models is intractable (mcdonald and satta, 2007), <papid> W07-2216 </papid>and one has to resort to approximation algorithms (mcdonald and pereira, 2006).<papid> E06-1011 </papid>in this paper, we propose to attack non-projective dependency parsing in principled way, using polynomial chart-parsing algorithms developed for mildly context-sensitive grammar formalisms.</nextsent>
<nextsent>this proposal is motivated by the observation that most dependency structures required for the analysis of natural language are very nearly projective, differing only minimally from the best projective approximation (kuhlmann and nivre, 2006), <papid> P06-2066 </papid>andby the close link between such mildly non-project ive?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2359">
<title id=" E09-1055.xml">treebank grammar techniques for non projective dependency parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>mcdonald et al  (2005) <papid> H05-1066 </papid>formulate dependency parsing as the search for the most probable spanning tree over the full set of all possible dependencies.</prevsent>
<prevsent>however, this approach is limited to probability models with strong independence assumptions.</prevsent>
</prevsection>
<citsent citstr=" W07-2216 ">
exhaustive non projective dependency parsing with more powerful models is intractable (mcdonald and satta, 2007), <papid> W07-2216 </papid>and one has to resort to approximation algorithms (mcdonald and pereira, 2006).<papid> E06-1011 </papid>in this paper, we propose to attack non-projective dependency parsing in principled way, using polynomial chart-parsing algorithms developed for mildly context-sensitive grammar formalisms.</citsent>
<aftsection>
<nextsent>this proposal is motivated by the observation that most dependency structures required for the analysis of natural language are very nearly projective, differing only minimally from the best projective approximation (kuhlmann and nivre, 2006), <papid> P06-2066 </papid>andby the close link between such mildly non-project ive?</nextsent>
<nextsent>dependency structures on the one hand, and grammar formalisms with mildly context-sensitive generative capacity on the other (kuhlmann andmhl, 2007).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2360">
<title id=" E09-1055.xml">treebank grammar techniques for non projective dependency parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>mcdonald et al  (2005) <papid> H05-1066 </papid>formulate dependency parsing as the search for the most probable spanning tree over the full set of all possible dependencies.</prevsent>
<prevsent>however, this approach is limited to probability models with strong independence assumptions.</prevsent>
</prevsection>
<citsent citstr=" E06-1011 ">
exhaustive non projective dependency parsing with more powerful models is intractable (mcdonald and satta, 2007), <papid> W07-2216 </papid>and one has to resort to approximation algorithms (mcdonald and pereira, 2006).<papid> E06-1011 </papid>in this paper, we propose to attack non-projective dependency parsing in principled way, using polynomial chart-parsing algorithms developed for mildly context-sensitive grammar formalisms.</citsent>
<aftsection>
<nextsent>this proposal is motivated by the observation that most dependency structures required for the analysis of natural language are very nearly projective, differing only minimally from the best projective approximation (kuhlmann and nivre, 2006), <papid> P06-2066 </papid>andby the close link between such mildly non-project ive?</nextsent>
<nextsent>dependency structures on the one hand, and grammar formalisms with mildly context-sensitive generative capacity on the other (kuhlmann andmhl, 2007).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2361">
<title id=" E09-1055.xml">treebank grammar techniques for non projective dependency parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, this approach is limited to probability models with strong independence assumptions.
</prevsent>
<prevsent>exhaustive non projective dependency parsing with more powerful models is intractable (mcdonald and satta, 2007), <papid> W07-2216 </papid>and one has to resort to approximation algorithms (mcdonald and pereira, 2006).<papid> E06-1011 </papid>in this paper, we propose to attack non-projective dependency parsing in principled way, using polynomial chart-parsing algorithms developed for mildly context-sensitive grammar formalisms.</prevsent>
</prevsection>
<citsent citstr=" P06-2066 ">
this proposal is motivated by the observation that most dependency structures required for the analysis of natural language are very nearly projective, differing only minimally from the best projective approximation (kuhlmann and nivre, 2006), <papid> P06-2066 </papid>andby the close link between such mildly non-project ive?</citsent>
<aftsection>
<nextsent>dependency structures on the one hand, and grammar formalisms with mildly context-sensitive generative capacity on the other (kuhlmann andmhl, 2007).
</nextsent>
<nextsent>furthermore, as pointed out by mcdonald and satta (2007), <papid> W07-2216 </papid>chart-parsing algorithms are amenable to augmentation by non-local information such as arity constraints and markovization,and therefore should allow for more predictive statistical models than those used by current systems for non-projective dependency parsing.</nextsent>
<nextsent>hence,mildly non-projective dependency parsing promises to be both efficient and accurate.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2367">
<title id=" E09-1055.xml">treebank grammar techniques for non projective dependency parsing </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>however, our algorithm may fai leven in cases where binarization exist sour notion of adjacency is not strong enough to capture all binarizable cases.
</prevsent>
<prevsent>this raises the question about the practical relevance of our technique.
</prevsent>
</prevsection>
<citsent citstr=" W06-2920 ">
in order to get at least preliminary answer to this question, we extracted lcfrs productions from the data used in the 2006 conll shared task on data-driven dependency parsing (buchholz and marsi, 2006), <papid> W06-2920 </papid>and evaluated how large portion of these productions could be binarized using our algorithm.</citsent>
<aftsection>
<nextsent>the results are given in table 1.
</nextsent>
<nextsent>since it is easy to see that our algorithm always succeeds on context-free productions (productions where each nonterminal has fan-out 1), we evaluated our algorithm on the 102 687 productions with higher fan-out.
</nextsent>
<nextsent>out of these, only 24 (0.02%) could not be binarized using our technique.
</nextsent>
<nextsent>we take this number as an indicator for the usefulness of our result.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2369">
<title id=" E12-1041.xml">lexical surprisal as a general predictor of reading time </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>improbable (surprising?)
</prevsent>
<prevsent>events carry more information than expected ones, so that surprisal is inversely related to probability, through logarithmic function.
</prevsent>
</prevsection>
<citsent citstr=" N01-1021 ">
in the context of sentence processing, if w1, ..., wt1 denotes the sentence so far, then the cognitive effort required for processing the next word, wt, is assumed to be proportional to its surprisal: effort(t) ? surprisal(wt) = ? log(p (wt|w1, ..., wt1)) (1)different theoretical groundings for this relationship have been proposed (hale, 2001; <papid> N01-1021 </papid>levy 2008; smith and levy, 2008).</citsent>
<aftsection>
<nextsent>smith and levy derive it by taking scale free assumption: any linguistic unit can be subdivided into smaller entities (e.g., sentence is comprised of words, word of phonemes), so that time to process the whole will equal the sum of processing times for each part.
</nextsent>
<nextsent>since the probability of the whole can be expressed as the product of the probabilities of the subunits, the function relating probability and effort must be logarithmic.
</nextsent>
<nextsent>levy (2008), on the other hand, grounds surprisal in its information theoretical context, describing difficulty encountered in on-line sentence processing as result of the need to update probability distribution over possible parses, being directly proportional to the difference between the previous and updated distributions.
</nextsent>
<nextsent>by expressing the difference between these in terms of relative entropy, levy shows that difficulty at each newly encountered word should be equal to its surprisal.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2371">
<title id=" E12-1041.xml">lexical surprisal as a general predictor of reading time </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>although markov models theoretically limit the amount of prior information that is relevant for prediction of the next step, theyare often used in linguistic context as an approximation to the full conditional probability.
</prevsent>
<prevsent>the effect of bigram probability (or forward transitional probability) has been repeatedly observed (e.g. mcdonald and shill cock, 2003), and smit hand levy (2008) report an effect of lexical sur prisal as estimated by trigram model on rts for the dundee corpus (a collection of newspaper texts with eye-tracking data from ten participants; kennedy and pynte, 2005).
</prevsent>
</prevsection>
<citsent citstr=" W10-2009 ">
phrase structure grammars (psgs) have also been amply used as language models (boston et al ., 2008; brouwer et al  2010; <papid> W10-2009 </papid>demberg and keller, 2008; hale, 2001; <papid> N01-1021 </papid>levy, 2008).</citsent>
<aftsection>
<nextsent>psgscan combine statistical exposure effects with explicit syntactic rules, by annotating norms with their respective probabilities, which can be estimated from occurrence counts in text corpora.
</nextsent>
<nextsent>information about hierarchical sentence structure can thus be included in the models.
</nextsent>
<nextsent>in this way,brouwer et al trained probabilistic context free grammar (pcfg) on 204,000 sentences extracted from dutch newspapers to estimate lexical surprisal (using an earley-stolcke parser; stolcke, 1995), <papid> J95-2002 </papid>showing that it could account forthe noun phrase coordination bias previously described and explained by frazier (1987) in terms of minimal-attachment preference of the human parser.</nextsent>
<nextsent>in contrast, demberg and keller used texts from naturalistic source (the dundee corpus) as the experimental stimuli, thus evaluating surprisalas wide-coverage account of processing diffi culty.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2374">
<title id=" E12-1041.xml">lexical surprisal as a general predictor of reading time </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>psgscan combine statistical exposure effects with explicit syntactic rules, by annotating norms with their respective probabilities, which can be estimated from occurrence counts in text corpora.
</prevsent>
<prevsent>information about hierarchical sentence structure can thus be included in the models.
</prevsent>
</prevsection>
<citsent citstr=" J95-2002 ">
in this way,brouwer et al trained probabilistic context free grammar (pcfg) on 204,000 sentences extracted from dutch newspapers to estimate lexical surprisal (using an earley-stolcke parser; stolcke, 1995), <papid> J95-2002 </papid>showing that it could account forthe noun phrase coordination bias previously described and explained by frazier (1987) in terms of minimal-attachment preference of the human parser.</citsent>
<aftsection>
<nextsent>in contrast, demberg and keller used texts from naturalistic source (the dundee corpus) as the experimental stimuli, thus evaluating surprisalas wide-coverage account of processing difficulty.
</nextsent>
<nextsent>they also employed psg, trained on one-million-word language sample from the wall street journal (part of the penn treebank ii, marcus et al  1993).<papid> J93-2004 </papid></nextsent>
<nextsent>using roarks (2001) incremental parser, they found significant effects of unlexi calized surprisal on rts (see also boston et al for similar approach and results for german texts).however, they failed to find an effect for lexicalized surprisal, over and above forward transitional probability.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2375">
<title id=" E12-1041.xml">lexical surprisal as a general predictor of reading time </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this way,brouwer et al trained probabilistic context free grammar (pcfg) on 204,000 sentences extracted from dutch newspapers to estimate lexical surprisal (using an earley-stolcke parser; stolcke, 1995), <papid> J95-2002 </papid>showing that it could account forthe noun phrase coordination bias previously described and explained by frazier (1987) in terms of minimal-attachment preference of the human parser.</prevsent>
<prevsent>in contrast, demberg and keller used texts from naturalistic source (the dundee corpus) as the experimental stimuli, thus evaluating surprisalas wide-coverage account of processing diffi culty.</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
they also employed psg, trained on one-million-word language sample from the wall street journal (part of the penn treebank ii, marcus et al  1993).<papid> J93-2004 </papid></citsent>
<aftsection>
<nextsent>using roarks (2001) incremental parser, they found significant effects of unlexi calized surprisal on rts (see also boston et al for similar approach and results for german texts).however, they failed to find an effect for lexicalized surprisal, over and above forward transitional probability.
</nextsent>
<nextsent>roark et al (2009) <papid> D09-1034 </papid>also looked at the 399 effects of syntactic and lexical surprisal, using rtdata for short narrative texts.</nextsent>
<nextsent>however, their estimates of these two surprisal values differ from those described above: in order to tease apart semantic and syntactic effects, they used demberg and kellers lexicalized surprisal as total surprisal measure, which they decompose into syntactic and lexical components.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2376">
<title id=" E12-1041.xml">lexical surprisal as a general predictor of reading time </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>they also employed psg, trained on one-million-word language sample from the wall street journal (part of the penn treebank ii, marcus et al  1993).<papid> J93-2004 </papid></prevsent>
<prevsent>using roarks (2001) incremental parser, they found significant effects of unlexi calized surprisal on rts (see also boston et al for similar approach and results for german texts).however, they failed to find an effect for lexicalized surprisal, over and above forward transitional probability.</prevsent>
</prevsection>
<citsent citstr=" D09-1034 ">
roark et al (2009) <papid> D09-1034 </papid>also looked at the 399 effects of syntactic and lexical surprisal, using rtdata for short narrative texts.</citsent>
<aftsection>
<nextsent>however, their estimates of these two surprisal values differ from those described above: in order to tease apart semantic and syntactic effects, they used demberg and kellers lexicalized surprisal as total surprisal measure, which they decompose into syntactic and lexical components.
</nextsent>
<nextsent>their results show significant effects of both syntactic and lexical surprisal, although the latter was found to hold only for closed class words.
</nextsent>
<nextsent>lack of wider effect was attributed to data sparsity: the models were trained on the relatively small brown corpus (over one million words from 500 samples of american english text), so that surprisal estimates for the less frequent content words would not have been accurate enough.using the same training and experimental language samples as demberg and keller (2008), and only un lexicalized surprisal estimates, frank (2009) and frank and bod (2011) focused on comparing different language models, including various n-gram models, psgs and recurrent networks (rnn).
</nextsent>
<nextsent>the latter were found to be the better predictors of rts, and psgs could not explain any variance in rt over and above the rnns, suggesting that human processing relies on linear rather than hierarchical representations.summing up, the only models taking into account actual words that have been consistently shown to simulate human behaviour with naturalistic text samples are bigram models.1 possible limitation in previous studies can be found in the stimuli employed.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2380">
<title id=" E12-1041.xml">lexical surprisal as a general predictor of reading time </title>
<section> models.  </section>
<citcontext>
<prevsection>
<prevsent>in addition, the randomly selected sentences did not make up coherent texts (in contrast, roark et al  2009, <papid> D09-1034 </papid>employed short stories), so that they were independent from each other, both for the models and the readers.</prevsent>
<prevsent>2.3 part-of-speech tagging.</prevsent>
</prevsection>
<citsent citstr=" P03-1054 ">
in order to produce pos-based surprisal estimates, versions of both the training and experimental texts with their words replaced by pos were developed: the bnc sentences were parsed by the stanford parser, version 1.6.7 (klein and manning, 2003), <papid> P03-1054 </papid>whilst the experimental texts were tagged by an automatic tagger (tsuruokaand tsujii, 2005), <papid> H05-1059 </papid>with posterior review and correction by hand following the penn treebank project guidelines (santorini, 1991).</citsent>
<aftsection>
<nextsent>by training language models and subsequently running them on the pos versions of the texts, un lexicalized surprisal values were estimated.
</nextsent>
<nextsent>2.4 phrase-structure grammars.
</nextsent>
<nextsent>the treebank formed by the parsed bnc sentences served as training data for roarks (2001) incremental parser.
</nextsent>
<nextsent>following frank and bod(2011), range of grammars was induced, differing in the features of the tree structure upon which rule probabilities were conditioned.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2381">
<title id=" E12-1041.xml">lexical surprisal as a general predictor of reading time </title>
<section> models.  </section>
<citcontext>
<prevsection>
<prevsent>in addition, the randomly selected sentences did not make up coherent texts (in contrast, roark et al  2009, <papid> D09-1034 </papid>employed short stories), so that they were independent from each other, both for the models and the readers.</prevsent>
<prevsent>2.3 part-of-speech tagging.</prevsent>
</prevsection>
<citsent citstr=" H05-1059 ">
in order to produce pos-based surprisal estimates, versions of both the training and experimental texts with their words replaced by pos were developed: the bnc sentences were parsed by the stanford parser, version 1.6.7 (klein and manning, 2003), <papid> P03-1054 </papid>whilst the experimental texts were tagged by an automatic tagger (tsuruokaand tsujii, 2005), <papid> H05-1059 </papid>with posterior review and correction by hand following the penn treebank project guidelines (santorini, 1991).</citsent>
<aftsection>
<nextsent>by training language models and subsequently running them on the pos versions of the texts, un lexicalized surprisal values were estimated.
</nextsent>
<nextsent>2.4 phrase-structure grammars.
</nextsent>
<nextsent>the treebank formed by the parsed bnc sentences served as training data for roarks (2001) incremental parser.
</nextsent>
<nextsent>following frank and bod(2011), range of grammars was induced, differing in the features of the tree structure upon which rule probabilities were conditioned.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2388">
<title id=" E12-3005.xml">whats in a name entity type variation across two biomedical sub domains </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>more specifically, researchers have concentrated mainly on named entity recognition, mapping them to concepts in curated databases (krallinger et al 2008) and extracting simple binary relations between entities.
</prevsent>
<prevsent>recently, an increasing number of resources that facilitate the training of systems to extract more detailed information have become available,e.g., pennbioie (kulick et al 2004), gene tag (tanabe et al 2005), bio infer (pyysalo et al., 2007), genia (kim et al 2008), grec(thompson et al 2009) and meta knowledge genia (thompson et al 2011).
</prevsent>
</prevsection>
<citsent citstr=" W11-1802 ">
moreover, several other annotated corpora have been developed for shared task purposes, such as bio creative i, ii, iii (arighi et al 2011) and bionlp shared tasks 2009 and 2011 (cohen et al 2009; kim et al 2011).<papid> W11-1802 </papid>many of the tools currently used for biomedical language processing were trained and evaluated on such popular corpora, most of which consist of documents from the molecular biology sub domain.</citsent>
<aftsection>
<nextsent>however, previous studies (discussed in section 2) have established that different biomedical sublanguages exhibit linguistic variations.
</nextsent>
<nextsent>it follows that tools which were developed and evaluated on corpora derived from one sub domain might not always perform as well on corpora from other subdomains.
</nextsent>
<nextsent>understanding these linguistic variations is essential to the process of adapt ating natural language processing tools to new do mains.in this paper, we highlight the variations between biomedical sublanguages by focussing on the different types of named entities (nes) thatare relevant to them.
</nextsent>
<nextsent>we show that the frequencies of different named entity types vary enough to allow classifier for scientific sub domains to be built based upon them.the study is performed on open access journal articles present in the uk pubmed central1 (ukpmc) (mcentyre et al 2010), an article database that extends the functionality of the original pubmed central (pmc) repository2.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2389">
<title id=" E12-3005.xml">whats in a name entity type variation across two biomedical sub domains </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>for instance, sager et al(1987) worked on pharmacological literature and lipid metabolism,whereas friedman et al(2002) analysed the properties of clinical and biomolecular sublanguages.other studies have investigated the differences between general and biomedical languages by focussing on specific linguistic aspects,such as verb-argument relations and pronominal anaphora.
</prevsent>
<prevsent>for instance, wattarujeekrit et al (2004) analysed the predicate-argument structures of 30 verbs used in biomedical articles.
</prevsent>
</prevsection>
<citsent citstr=" J05-1004 ">
their results suggest that, in certain cases, significant difference exists in the predicate frames compared to those obtained from analysing news articles inthe propbank project (palmer et al 2005).<papid> J05-1004 </papid></citsent>
<aftsection>
<nextsent>similarly, based on the genia and pennbioie corpora, cohen et al(2008) performed study of argument realisation with respect to the nominal isation and alternation of biomedical verbs.
</nextsent>
<nextsent>they concluded that there is high occurrence of these phenomena in this semantically restricted do main, and underline that this sublanguage model applies only to biomedical language.
</nextsent>
<nextsent>taking different angle, nguyen and kim (2008) <papid> C08-1079 </papid>examined the differences in the use of pronouns by studying general domains (muc and ace) and one biomedical domain (genia).</nextsent>
<nextsent>they observed that compared to the muc and ace corpora, the genia corpus has significantly more occurrences of neutral and third-person pronouns, whilst first and second person pronouns are non-existent.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2390">
<title id=" E12-3005.xml">whats in a name entity type variation across two biomedical sub domains </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>similarly, based on the genia and pennbioie corpora, cohen et al(2008) performed study of argument realisation with respect to the nominal isation and alternation of biomedical verbs.
</prevsent>
<prevsent>they concluded that there is high occurrence of these phenomena in this semantically restricted do main, and underline that this sublanguage model applies only to biomedical language.
</prevsent>
</prevsection>
<citsent citstr=" C08-1079 ">
taking different angle, nguyen and kim (2008) <papid> C08-1079 </papid>examined the differences in the use of pronouns by studying general domains (muc and ace) and one biomedical domain (genia).</citsent>
<aftsection>
<nextsent>they observed that compared to the muc and ace corpora, the genia corpus has significantly more occurrences of neutral and third-person pronouns, whilst first and second person pronouns are non-existent.
</nextsent>
<nextsent>verspoor et al(2009) measured lexical and structural variation in biomedical open access journals and subscription-based journals, concluding that there are no significant differences between them.
</nextsent>
<nextsent>therefore, model trained on one of these sources can be used successfully on the other, as long as the subject matter is maintained.
</nextsent>
<nextsent>furthermore, they compared mouse genomics corpus with two reference corpora, one composed of newswire texts and another of general biomedical articles.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2391">
<title id=" E12-1015.xml">character based pivot translation for under resourced languages and domains </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>data-driven approaches have been extremely successful in most areas of natural language processing (nlp) and can be considered the main paradigm in application-oriented research and development.
</prevsent>
<prevsent>research in machine translation is atypical example with the dominance of statistical models over the last decade.
</prevsent>
</prevsection>
<citsent citstr=" P07-2045 ">
this is even enforced due to the availability of tool boxes such asmoses (koehn et al 2007) <papid> P07-2045 </papid>which make it possible to build translation engines within days oreven hours for any language pair provided that appropriate training data is available.</citsent>
<aftsection>
<nextsent>however, this reliance on training data is also the most severe limitation of statistical approaches.
</nextsent>
<nextsent>resources inlarge quantities are only available for few languages and domains.
</nextsent>
<nextsent>in the case of smt, the dilemma is even more apparent as parallel corpora are rare and usually quite sparse.
</nextsent>
<nextsent>some languages can be considered lucky, for example, because of political situations that lead to the production of freely available translated material on large scale.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2392">
<title id=" E12-1015.xml">character based pivot translation for under resourced languages and domains </title>
<section> pivot models.  </section>
<citcontext>
<prevsection>
<prevsent>a condition is that we have at least some training data for the translation between pivot and the resource-poor language.
</prevsent>
<prevsent>however, for the original task (source-to-target translation) we do not require any data resources except for purposes of comparison.we will explore various models for the translation between the resource-poor language and the pivot language and most of them are not compatible with standard phrase-based translation models.
</prevsent>
</prevsection>
<citsent citstr=" P07-1092 ">
hence, triangulation methods (cohn and la pata, 2007) <papid> P07-1092 </papid>for combining phrase tables are not applicable in our case.</citsent>
<aftsection>
<nextsent>instead, we explore cascaded approach (also called transfer method?
</nextsent>
<nextsent>(wu and wang, 2009)) <papid> P09-1018 </papid>in which we translate the input text in two steps using linear interpolation for rescoring n-best lists.</nextsent>
<nextsent>following the method described in (utiyama and isahara, 2007)<papid> N07-1061 </papid>and (wu and wang, 2009), <papid> P09-1018 </papid>we use the best hypotheses from the translation of source sentences to pivot sentences and combine them with thetop hypotheses for translating these pivot sentences to target sentences t: t?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2393">
<title id=" E12-1015.xml">character based pivot translation for under resourced languages and domains </title>
<section> pivot models.  </section>
<citcontext>
<prevsection>
<prevsent>hence, triangulation methods (cohn and la pata, 2007) <papid> P07-1092 </papid>for combining phrase tables are not applicable in our case.</prevsent>
<prevsent>instead, we explore cascaded approach (also called transfer method?</prevsent>
</prevsection>
<citsent citstr=" P09-1018 ">
(wu and wang, 2009)) <papid> P09-1018 </papid>in which we translate the input text in two steps using linear interpolation for rescoring n-best lists.</citsent>
<aftsection>
<nextsent>following the method described in (utiyama and isahara, 2007)<papid> N07-1061 </papid>and (wu and wang, 2009), <papid> P09-1018 </papid>we use the best hypotheses from the translation of source sentences to pivot sentences and combine them with thetop hypotheses for translating these pivot sentences to target sentences t: t?</nextsent>
<nextsent>argmax l?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2394">
<title id=" E12-1015.xml">character based pivot translation for under resourced languages and domains </title>
<section> pivot models.  </section>
<citcontext>
<prevsection>
<prevsent>instead, we explore cascaded approach (also called transfer method?
</prevsent>
<prevsent>(wu and wang, 2009)) <papid> P09-1018 </papid>in which we translate the input text in two steps using linear interpolation for rescoring n-best lists.</prevsent>
</prevsection>
<citsent citstr=" N07-1061 ">
following the method described in (utiyama and isahara, 2007)<papid> N07-1061 </papid>and (wu and wang, 2009), <papid> P09-1018 </papid>we use the best hypotheses from the translation of source sentences to pivot sentences and combine them with thetop hypotheses for translating these pivot sentences to target sentences t: t?</citsent>
<aftsection>
<nextsent>argmax l?
</nextsent>
<nextsent>k=1 sp hsp (s, p) + (1?
</nextsent>
<nextsent>?)pt hpt (p, t) where hxyk are feature functions for model xy with appropriate weights xyk . 1 basically, this.
</nextsent>
<nextsent>means that we simply add the scores and, similar to related work, we assume that the feature weights can be set independently for each model using minimum error rate training (mert) (och, 1note, that we do not require the same feature functions in both models even though the formula above implies this for simplicity of representation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2396">
<title id=" E12-1015.xml">character based pivot translation for under resourced languages and domains </title>
<section> character-based smt.  </section>
<citcontext>
<prevsection>
<prevsent>spelling conventions can still be quite different but those differences are often very consistent.
</prevsent>
<prevsent>the bosnian macedonian example also shows that we do not have to require any alphabetic overlap in order to obtain character-level similarities.regularities between such closely related languages can be captured below the word level.
</prevsent>
</prevsection>
<citsent citstr=" W07-0705 ">
wecan also assume more or less monotonic relation between the two languages which motivates the idea of translation models over character grams treating translation as transliteration task(vilar et al 2007).<papid> W07-0705 </papid></citsent>
<aftsection>
<nextsent>conceptually it is straightforward to think of phrase-based models on the character level.
</nextsent>
<nextsent>sequences of characters can be used instead of word n-grams for both, translation and language models.
</nextsent>
<nextsent>training can proceed with the same tools and approaches.
</nextsent>
<nextsent>the basic task is to 2note that different samples may still include common sentences.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2398">
<title id=" E12-1015.xml">character based pivot translation for under resourced languages and domains </title>
<section> character-based smt.  </section>
<citcontext>
<prevsection>
<prevsent>an alternative is to use models designed for transliteration or related character-level transformation tasks.
</prevsent>
<prevsent>many approaches are based on transducer models that resemble string edit operations such as insertions, deletions and substitutions (ristad and yianilos, 1998).
</prevsent>
</prevsection>
<citsent citstr=" N07-1047 ">
weighted finite state transducers (wfsts) can be trained on unaligned pairs of character sequences and have been shown to be very effective for transliteration tasks or letter-to-phoneme conversions (jiampoja marn et al 2007).<papid> N07-1047 </papid></citsent>
<aftsection>
<nextsent>the training procedure usually employs an expectation maximization (em) procedure and the resulting transducer can be used tofind the viterbi alignment between characters according to the best sequence of edit operations applied to transform one string into the other.
</nextsent>
<nextsent>extensions to this model are possible, for example the use of many-to-many alignments which have been shown to be very effective in letter-to-phoneme alignment tasks (jiampojamarn et al 2007).<papid> N07-1047 </papid>one advantage of the edit-distance-based transducer models is that the alignments they predict are strictly monotonic and cannot easily be confused by spurious relations between characters over longer distances.</nextsent>
<nextsent>long distance alignments are only possible in connection with series of insertions and deletions that usually in crease the alignment costs in such way that they are avoided if possible.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2400">
<title id=" E12-1015.xml">character based pivot translation for under resourced languages and domains </title>
<section> character-based smt.  </section>
<citcontext>
<prevsection>
<prevsent>however, the size of the vocabulary in character-level model is very small (several orders of magnitude smaller than on the word level) and this may cause serious confusion of the word alignment model that very much relies on context-independent lexical translation probabilities.
</prevsent>
<prevsent>hence, for character alignment, the lexical evidence is much less reliable without their context.it is certainly possible to find compromise between word-level and character-level models in order to generalize below word boundaries but avoiding alignment problems as discussed above.
</prevsent>
</prevsection>
<citsent citstr=" D10-1015 ">
morpheme-based translation models have been explored in several studies with similar motivations as in our approach, better generalization from sparse training data (fishel and kirik, 2010; luong et al 2010).<papid> D10-1015 </papid></citsent>
<aftsection>
<nextsent>however, these approaches have the drawback that they require proper morphological analyses.
</nextsent>
<nextsent>data-driven techniques exist even for morphology, but their use in smt still needs to be shown (fishel, 2009).
</nextsent>
<nextsent>the situation is comparable to the problems of integrating linguistically motivated phrases into phrase based smt (koehn et al 2003).
</nextsent>
<nextsent>instead we opt for more general approach to extend context to facilitate, especially, the alignment step.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2401">
<title id=" E12-1015.xml">character based pivot translation for under resourced languages and domains </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>all other models use phrases over character sequences.
</prevsent>
<prevsent>the wfst x:y models use weighted finite state transducers for character alignment with units that are at most and characters long, respectively.
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
other models use viterbi alignments created by ibm model 4 using giza++ (och and ney, 2003) <papid> J03-1002 </papid>between characters (ibm char ) or bigrams (ibm bigram ).</citsent>
<aftsection>
<nextsent>lcsr refers to the averaged longest common sub sequence ratio between system translations and references.
</nextsent>
<nextsent>results are significantly better (p   0.01++,   0.05+) or worse (p   0.01??,   0.05?)
</nextsent>
<nextsent>than the word-based baseline.
</nextsent>
<nextsent>mk-bs mk-bg gl-es ca-es model bleu % unk bleu % unk bleu % unk bleu % unk word-based 14.22 17.83% 14.77 5.29% 43.22 10.18% 59.34 3.80% char ? wfst1:1 21.74++ 1.50% 16.04++ 0.77% 50.24++ 1.17% 62.87++ 0.45% char ? wfst2:2 19.19++ 2.05% 15.32 0.96% 50.59++ 1.28% 59.84 0.47% char ? ibmchar 24.15++ 1.30% 17.12++ 0.80% 51.18++ 1.38% 64.35 ++ 0.59% char ? ibm bigram 24.82++ 1.00% 17.28++ 0.77% 50.70++ 1.36% 65.14++ 0.48% table 3: translating from the source language to related pivot language.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2403">
<title id=" E12-1015.xml">character based pivot translation for under resourced languages and domains </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>there is wide range of pivot language approaches to machine translation and number of strategies have been proposed.
</prevsent>
<prevsent>one of them is often called triangulation and usually refers to the combination of phrase tables (cohn and lapata, 2007).<papid> P07-1092 </papid></prevsent>
</prevsection>
<citsent citstr=" P07-1108 ">
phrase translation probabilities are merged and lexical weights are estimated by bridging word alignment models (wu and wang, 2007; <papid> P07-1108 </papid>bertoldi et al 2008).</citsent>
<aftsection>
<nextsent>cascaded translation via pivot languages are discussed by (utiyamaand isahara, 2007) <papid> N07-1061 </papid>and are frequently used by various researchers (de gispert and marino, 2006; koehn et al 2009; wu and wang, 2009) <papid> P09-1018 </papid>and commercial systems such as google translate.</nextsent>
<nextsent>a third strategy is to generate or augment datasets with the help of pivot models.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2407">
<title id=" E12-1015.xml">character based pivot translation for under resourced languages and domains </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>a third strategy is to generate or augment datasets with the help of pivot models.
</prevsent>
<prevsent>this is, for example, explored by (de gispert and marino, 2006) and (wu and wang, 2009) <papid> P09-1018 </papid>(who call it the synthetic method).</prevsent>
</prevsection>
<citsent citstr=" C10-1027 ">
pivoting has also been used for paraphrasing and lexical adaptation (bannard and callison-burch, 2005; crego et al 2010).<papid> C10-1027 </papid></citsent>
<aftsection>
<nextsent>(nakov and ng, 2009) <papid> D09-1141 </papid>investigate pivot languages for resource-poor languages (but only when translating from the resource-poor language).</nextsent>
<nextsent>they also use transliteration for adapting models to new (related) language.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2408">
<title id=" E12-1015.xml">character based pivot translation for under resourced languages and domains </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>this is, for example, explored by (de gispert and marino, 2006) and (wu and wang, 2009) <papid> P09-1018 </papid>(who call it the synthetic method).</prevsent>
<prevsent>pivoting has also been used for paraphrasing and lexical adaptation (bannard and callison-burch, 2005; crego et al 2010).<papid> C10-1027 </papid></prevsent>
</prevsection>
<citsent citstr=" D09-1141 ">
(nakov and ng, 2009) <papid> D09-1141 </papid>investigate pivot languages for resource-poor languages (but only when translating from the resource-poor language).</citsent>
<aftsection>
<nextsent>they also use transliteration for adapting models to new (related) language.
</nextsent>
<nextsent>character-level smt has been used for transliteration (matthews, 2007; tiedemann and nabende, 2009) and also for the translation between closely related languages (vi lar et al 2007; <papid> W07-0705 </papid>tiedemann, 2009a).</nextsent>
<nextsent>in this paper, we have discussed possibilities to translate via pivot languages on the characterlevel.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2411">
<title id=" E89-1040.xml">an approach to sentence level anaphora in machine translation </title>
<section> wh-movement.  </section>
<citcontext>
<prevsection>
<prevsent>- 306- acknowledgements the work we report here hscl its beginnings in work within the eurotra framework.
</prevsent>
<prevsent>mimo however is not  the  official eurotra system.
</prevsent>
</prevsection>
<citsent citstr=" C88-1008 ">
it differs in many critical respects from e.g bech &amp; nygaard (1988).<papid> C88-1008 </papid></citsent>
<aftsection>
<nextsent>mimo is the result of the joint effort of essex, utrecht and dominique petitpierre from issco, geneve.
</nextsent>
<nextsent>the research reported in this paper was supported by the european community, the dti (depart- ment of trade and industry) and the nbbi (nederlands bureau voor bibliotheekwezen informatieverzorging).
</nextsent>
<nextsent>s shieber, 1986: an introduction to unification based ap-proaches to grammar.
</nextsent>
<nextsent>csli 1988.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2412">
<title id=" E12-1016.xml">does more data always yield better translations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, references are not used at any stage within the translation system for obtaining the hypotheses.
</prevsent>
<prevsent>note that although we are not able to achieve such an improvement without an oracle, this result restates the bss problem as an interesting approach not only for reducing computational effort but also for significantly boosting performance.
</prevsent>
</prevsection>
<citsent citstr=" P07-2045 ">
to our knowledge, no previous work has quantified the room of improvement in which bss techniques could incur.in order to assess the performance of the different bss techniques, translation results are obtained by using standard state-of-the-art smt system (koehn et al  2007).<papid> P07-2045 </papid></citsent>
<aftsection>
<nextsent>the most recent literature defines the smt problem (papineni et al  1998; och and ney, 2002) <papid> P02-1038 </papid>as follows: given an input sentence from certain source language, the purpose is to find an output sentence e?</nextsent>
<nextsent>in certain target language such that e?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2413">
<title id=" E12-1016.xml">does more data always yield better translations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>note that although we are not able to achieve such an improvement without an oracle, this result restates the bss problem as an interesting approach not only for reducing computational effort but also for significantly boosting performance.
</prevsent>
<prevsent>to our knowledge, no previous work has quantified the room of improvement in which bss techniques could incur.in order to assess the performance of the different bss techniques, translation results are obtained by using standard state-of-the-art smt system (koehn et al  2007).<papid> P07-2045 </papid></prevsent>
</prevsection>
<citsent citstr=" P02-1038 ">
the most recent literature defines the smt problem (papineni et al  1998; och and ney, 2002) <papid> P02-1038 </papid>as follows: given an input sentence from certain source language, the purpose is to find an output sentence e?</citsent>
<aftsection>
<nextsent>in certain target language such that e?
</nextsent>
<nextsent>= argmax k?
</nextsent>
<nextsent>k=1 khk(f , e) (1) where hk(f , e) is score function representing an important feature for the translation of into e, as for example the language model of the target language, reordering model or several translation models.
</nextsent>
<nextsent>k are the log-linear combination weights.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2414">
<title id=" E12-1016.xml">does more data always yield better translations </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>finally, the main results of thework and several future work directions are discussed in section 6.
</prevsent>
<prevsent>training data selection has been receiving an increasing amount of attention within the smt community.
</prevsent>
</prevsection>
<citsent citstr=" W10-1718 ">
for instance, in (li et al  2010;<papid> W10-1718 </papid>gasco?</citsent>
<aftsection>
<nextsent>et al  2010) several bss techniques, similar to those analysed in this paper, have been applied for training mt systems when there are large training corpora available.
</nextsent>
<nextsent>however, neither such techniques have been formalised, nor its performance thoroughly analysed.
</nextsent>
<nextsent>a similar approach that gives weights to different sub corpora was proposed in (matsoukas et al  2009).<papid> D09-1074 </papid>in (lu et al  2007), information retrieval methods are used in order to produce different sub models which are then weighted according to the sentence to be translated.</nextsent>
<nextsent>in such work, authors define the baseline as the result obtained training only with the corpus that share the same do main of the test.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2415">
<title id=" E12-1016.xml">does more data always yield better translations </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>et al  2010) several bss techniques, similar to those analysed in this paper, have been applied for training mt systems when there are large training corpora available.
</prevsent>
<prevsent>however, neither such techniques have been formalised, nor its performance thoroughly analysed.
</prevsent>
</prevsection>
<citsent citstr=" D09-1074 ">
a similar approach that gives weights to different sub corpora was proposed in (matsoukas et al  2009).<papid> D09-1074 </papid>in (lu et al  2007), information retrieval methods are used in order to produce different sub models which are then weighted according to the sentence to be translated.</citsent>
<aftsection>
<nextsent>in such work, authors define the baseline as the result obtained training only with the corpus that share the same do main of the test.
</nextsent>
<nextsent>afterwards they claim that they are able to improve baseline translation quality by adding new sentences retrieved with their method.
</nextsent>
<nextsent>however, they neither compare their technique with random sentence selection, nor with model trained with all the corpora.
</nextsent>
<nextsent>although the techniques that are applied forbss are often very similar to those applied for active learning (al), both problems are essentially different.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2416">
<title id=" E12-1016.xml">does more data always yield better translations </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>some works have applied sentence selection in small scale al frameworks.
</prevsent>
<prevsent>these works extend the training corpora at most with 5000 sentences.
</prevsent>
</prevsection>
<citsent citstr=" D10-1061 ">
in (ananthakrishnan et al  2010), <papid> D10-1061 </papid>sentences are selected by means of discriminative techniques.</citsent>
<aftsection>
<nextsent>in (haffari et al  2009) <papid> N09-1047 </papid>technique is proposed for increasing the counts of phrases that are considered infrequent.</nextsent>
<nextsent>both works significantly differ from the current work not only on the framework, but also on the scale of the experiments, the 153proposed techniques and the obtained improvements.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2417">
<title id=" E12-1016.xml">does more data always yield better translations </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>these works extend the training corpora at most with 5000 sentences.
</prevsent>
<prevsent>in (ananthakrishnan et al  2010), <papid> D10-1061 </papid>sentences are selected by means of discriminative techniques.</prevsent>
</prevsection>
<citsent citstr=" N09-1047 ">
in (haffari et al  2009) <papid> N09-1047 </papid>technique is proposed for increasing the counts of phrases that are considered infrequent.</citsent>
<aftsection>
<nextsent>both works significantly differ from the current work not only on the framework, but also on the scale of the experiments, the 153proposed techniques and the obtained improvements.
</nextsent>
<nextsent>similar ideas applied to adaptation problems have been proposed in (moore and lewis, 2010; <papid> P10-2041 </papid>axelrod et al  2011).<papid> D11-1033 </papid></nextsent>
<nextsent>as discussed in section 2, bss has inherently attached many meaningful links with al techniques.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2418">
<title id=" E12-1016.xml">does more data always yield better translations </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in (haffari et al  2009) <papid> N09-1047 </papid>technique is proposed for increasing the counts of phrases that are considered infrequent.</prevsent>
<prevsent>both works significantly differ from the current work not only on the framework, but also on the scale of the experiments, the 153proposed techniques and the obtained improvements.</prevsent>
</prevsection>
<citsent citstr=" P10-2041 ">
similar ideas applied to adaptation problems have been proposed in (moore and lewis, 2010; <papid> P10-2041 </papid>axelrod et al  2011).<papid> D11-1033 </papid></citsent>
<aftsection>
<nextsent>as discussed in section 2, bss has inherently attached many meaningful links with al techniques.
</nextsent>
<nextsent>selecting samples for learning our models, incurs in well-known difficulty in al, the so-called sample bias problem (dasgupta, 2009).
</nextsent>
<nextsent>this problem, which is spread to the bss case, is summarised as the distortion introduced by the active strategy into the probability distribution underlying the training corpus.
</nextsent>
<nextsent>this bias forces the training algorithm to learn distorted probability model which can significantly differ from the actual one.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2419">
<title id=" E12-1016.xml">does more data always yield better translations </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in (haffari et al  2009) <papid> N09-1047 </papid>technique is proposed for increasing the counts of phrases that are considered infrequent.</prevsent>
<prevsent>both works significantly differ from the current work not only on the framework, but also on the scale of the experiments, the 153proposed techniques and the obtained improvements.</prevsent>
</prevsection>
<citsent citstr=" D11-1033 ">
similar ideas applied to adaptation problems have been proposed in (moore and lewis, 2010; <papid> P10-2041 </papid>axelrod et al  2011).<papid> D11-1033 </papid></citsent>
<aftsection>
<nextsent>as discussed in section 2, bss has inherently attached many meaningful links with al techniques.
</nextsent>
<nextsent>selecting samples for learning our models, incurs in well-known difficulty in al, the so-called sample bias problem (dasgupta, 2009).
</nextsent>
<nextsent>this problem, which is spread to the bss case, is summarised as the distortion introduced by the active strategy into the probability distribution underlying the training corpus.
</nextsent>
<nextsent>this bias forces the training algorithm to learn distorted probability model which can significantly differ from the actual one.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2421">
<title id=" E12-1016.xml">does more data always yield better translations </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>|s| stands for number of sentences,|w | for number of running words, and |v | for vocabulary size.
</prevsent>
<prevsent>subset language |s| |w | |v | train english 77.2k 1.71m 29.9k french 1.99m 48k dev 08 english 2.1k 49.8k 8.7k french 55.4k 7.7k test 09 english 2.5k 65.6k 8.9k french 72.5k 10.6k test 10 english 2.5k 62k 8.9k french 70.5k 10.3k table 3: news commentary corpus main figures.
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
giza++ (och and ney, 2003).<papid> J03-1002 </papid></citsent>
<aftsection>
<nextsent>the language model used was 5-gram with modified kneser ney smoothing (kneser and ney, 1995), built with srilm toolkit (stolcke, 2002).
</nextsent>
<nextsent>the loglinear combination weights in eq.
</nextsent>
<nextsent>(1) were optimised using minimum error rate training (ochand ney, 2002) <papid> P02-1038 </papid>on the corresponding development sets.</nextsent>
<nextsent>experiments were carried out on two corpora: ted (paul et al  2010) and news commentary (nc) (callison-burch et al  2010).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2423">
<title id=" E12-1016.xml">does more data always yield better translations </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>in contrast, bss tackles with the problem of how to select samples from large pool of training data, regardless of whether such pool of data is in-domain or out-of-domain.
</prevsent>
<prevsent>hence, inone case we can assume to have fairly welles timated translation model, which is to be adapted, whereas in bss we still have full control over the estimation of such model and need not to aim at specific domain, although it might often be so.
</prevsent>
</prevsection>
<citsent citstr=" P07-1034 ">
bss is related with instance weighting (jiangand zhai, 2007; <papid> P07-1034 </papid>foster et al  2010).<papid> D10-1044 </papid></citsent>
<aftsection>
<nextsent>adaptation and bss can be considered to be orthogonal (yet complementary) problems under the instance weighting paradigm.
</nextsent>
<nextsent>in such case, instance weighting can be considered to span complete paradigmatic space between both.
</nextsent>
<nextsent>at one end, there is sample selection (bss for smt), while at the other end there is adaptation.
</nextsent>
<nextsent>for instance, itis quite common to confront the adaptation problem by extracting different phrase-tables from different corpora, and then interpol ate such tables.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2424">
<title id=" E12-1016.xml">does more data always yield better translations </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>in contrast, bss tackles with the problem of how to select samples from large pool of training data, regardless of whether such pool of data is in-domain or out-of-domain.
</prevsent>
<prevsent>hence, inone case we can assume to have fairly welles timated translation model, which is to be adapted, whereas in bss we still have full control over the estimation of such model and need not to aim at specific domain, although it might often be so.
</prevsent>
</prevsection>
<citsent citstr=" D10-1044 ">
bss is related with instance weighting (jiangand zhai, 2007; <papid> P07-1034 </papid>foster et al  2010).<papid> D10-1044 </papid></citsent>
<aftsection>
<nextsent>adaptation and bss can be considered to be orthogonal (yet complementary) problems under the instance weighting paradigm.
</nextsent>
<nextsent>in such case, instance weighting can be considered to span complete paradigmatic space between both.
</nextsent>
<nextsent>at one end, there is sample selection (bss for smt), while at the other end there is adaptation.
</nextsent>
<nextsent>for instance, itis quite common to confront the adaptation problem by extracting different phrase-tables from different corpora, and then interpol ate such tables.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2425">
<title id=" E09-1018.xml">em works for pronoun anaphora resolution </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we believe it is of interest for two reasons.
</prevsent>
<prevsent>first, virtually all ofits parameters are learned via the expectation maximization algorithm (em).
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
while em has worked quite well for few tasks, notably machine translations (starting with the ibm models 1-5 (brown et al, 1993), <papid> J93-2003 </papid>it has not had success inmost others, such as part-of-speech tagging (meri aldo, 1991), named-entity recognition (collinsand singer, 1999) <papid> W99-0613 </papid>and context-free-grammar induction (numerous attempts, too many to mention).</citsent>
<aftsection>
<nextsent>thus understanding the abilities and limitations of em is very much topic of interest.
</nextsent>
<nextsent>we present this work as positive data-point in this ongoing discussion.
</nextsent>
<nextsent>secondly, and perhaps more importantly, is the systems performance.
</nextsent>
<nextsent>remarkably, there are very few systems for actually doing pronoun anaphora available on the web.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2426">
<title id=" E09-1018.xml">em works for pronoun anaphora resolution </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we believe it is of interest for two reasons.
</prevsent>
<prevsent>first, virtually all ofits parameters are learned via the expectation maximization algorithm (em).
</prevsent>
</prevsection>
<citsent citstr=" W99-0613 ">
while em has worked quite well for few tasks, notably machine translations (starting with the ibm models 1-5 (brown et al, 1993), <papid> J93-2003 </papid>it has not had success inmost others, such as part-of-speech tagging (meri aldo, 1991), named-entity recognition (collinsand singer, 1999) <papid> W99-0613 </papid>and context-free-grammar induction (numerous attempts, too many to mention).</citsent>
<aftsection>
<nextsent>thus understanding the abilities and limitations of em is very much topic of interest.
</nextsent>
<nextsent>we present this work as positive data-point in this ongoing discussion.
</nextsent>
<nextsent>secondly, and perhaps more importantly, is the systems performance.
</nextsent>
<nextsent>remarkably, there are very few systems for actually doing pronoun anaphora available on the web.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2427">
<title id=" E09-1018.xml">em works for pronoun anaphora resolution </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>the literature on pronominal anaphora is quite large, and we cannot hope to do justice to it here.
</prevsent>
<prevsent>rather we limit ourselves to particular papers and systems that have had the greatest impact on, and similarity to, ours.
</prevsent>
</prevsection>
<citsent citstr=" W05-0612 ">
probably the closest approach to our own is cherry and bergsma (2005), <papid> W05-0612 </papid>which also present san em approach to pronoun resolution, and obtains quite successful results.</citsent>
<aftsection>
<nextsent>our work improves upon theirs in several dimensions.
</nextsent>
<nextsent>firstly, they do not distinguish antecedents of non-reflexive pronouns based on syntax (for instance, subjects and objects).
</nextsent>
<nextsent>both previous work (cf.
</nextsent>
<nextsent>tetreault (2001) <papid> J01-4003 </papid>discussed below) and our present results find these distinctions extremely helpful.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2428">
<title id=" E09-1018.xml">em works for pronoun anaphora resolution </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>firstly, they do not distinguish antecedents of non-reflexive pronouns based on syntax (for instance, subjects and objects).
</prevsent>
<prevsent>both previous work (cf.
</prevsent>
</prevsection>
<citsent citstr=" J01-4003 ">
tetreault (2001) <papid> J01-4003 </papid>discussed below) and our present results find these distinctions extremely helpful.</citsent>
<aftsection>
<nextsent>secondly, their system relies on separate preprocessing stage to classify non-anaphoric pronouns, and mark the gender of certain nps (mr., mrs.and some first names).
</nextsent>
<nextsent>this allows the incorporation of external data and learning systems, but conversely, it requires these decisions to be made sequentially.
</nextsent>
<nextsent>our system classifies non-anaphoric pronouns jointly, and learns gender without an external database.
</nextsent>
<nextsent>next, they only handle third person pronouns, while we handle first and second as well.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2430">
<title id=" E09-1018.xml">em works for pronoun anaphora resolution </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>their emrequires careful initialization ? sufficiently careful that the em version only performs 0.4% better than the initial ized program alone.
</prevsent>
<prevsent>(we can say nothing about relative performance of their system vs. ours since we have been able to access neither their data nor code.)
</prevsent>
</prevsection>
<citsent citstr=" N04-4009 ">
a quite different unsupervised approach is kehler et al (2004<papid> N04-4009 </papid>a), which uses self-training of adiscriminative system, initial ized with some con 148servative number and gender heuristics.</citsent>
<aftsection>
<nextsent>the system uses the conventional ranking approach, applying maximum-entropy classifier to pairs of pronoun and potential antecedent and selecting the best antecedent.
</nextsent>
<nextsent>in each iteration of self-training,the system labels the training corpus and its decisions are treated as input for the next training phase.
</nextsent>
<nextsent>the system improves substantially over ahobbs baseline.
</nextsent>
<nextsent>in comparison to ours, their feature set is quite similar, while their learning approach is rather different.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2432">
<title id=" E09-1018.xml">em works for pronoun anaphora resolution </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>the system improves substantially over ahobbs baseline.
</prevsent>
<prevsent>in comparison to ours, their feature set is quite similar, while their learning approach is rather different.
</prevsent>
</prevsection>
<citsent citstr=" P07-1107 ">
in addition, their system does not classify non-anaphoric pronouns, third paper that has significantly influenced our work is that of (haghighi and klein, 2007).<papid> P07-1107 </papid></citsent>
<aftsection>
<nextsent>this is the first paper to treat all noun phrase (np) anaphora using generative model.
</nextsent>
<nextsent>the success they achieve directly inspired our work.
</nextsent>
<nextsent>there are, however, many differences between their approach and ours.
</nextsent>
<nextsent>the most obvious is our use of em rather than theirs of gibbs sampling.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2433">
<title id=" E09-1018.xml">em works for pronoun anaphora resolution </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>more generally it follows from this that the system only works (or at least works with the accuracy they achieve) when the input data is so marked.
</prevsent>
<prevsent>these markings not only render the non-anaphoric pronoun situation moot, but also significantly restrict the choice of possible antecedent.
</prevsent>
</prevsection>
<citsent citstr=" J98-2001 ">
only perhaps one in four or five nps are mark able (poesio and vieira, 1998).<papid> J98-2001 </papid></citsent>
<aftsection>
<nextsent>there are also several papers which treat coference as an unsupervised clustering problem (cardie and wagstaff, 1999; <papid> W99-0611 </papid>angheluta et al, 2004).</nextsent>
<nextsent>in this literature there is no generative model at all, and thus this work is only loosely connected to the above models.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2434">
<title id=" E09-1018.xml">em works for pronoun anaphora resolution </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>these markings not only render the non-anaphoric pronoun situation moot, but also significantly restrict the choice of possible antecedent.
</prevsent>
<prevsent>only perhaps one in four or five nps are mark able (poesio and vieira, 1998).<papid> J98-2001 </papid></prevsent>
</prevsection>
<citsent citstr=" W99-0611 ">
there are also several papers which treat coference as an unsupervised clustering problem (cardie and wagstaff, 1999; <papid> W99-0611 </papid>angheluta et al, 2004).</citsent>
<aftsection>
<nextsent>in this literature there is no generative model at all, and thus this work is only loosely connected to the above models.
</nextsent>
<nextsent>another key paper is (ge et al, 1998).<papid> W98-1119 </papid></nextsent>
<nextsent>the data annotated for the ge research is used here for testing and development data.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2435">
<title id=" E09-1018.xml">em works for pronoun anaphora resolution </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>there are also several papers which treat coference as an unsupervised clustering problem (cardie and wagstaff, 1999; <papid> W99-0611 </papid>angheluta et al, 2004).</prevsent>
<prevsent>in this literature there is no generative model at all, and thus this work is only loosely connected to the above models.</prevsent>
</prevsection>
<citsent citstr=" W98-1119 ">
another key paper is (ge et al, 1998).<papid> W98-1119 </papid></citsent>
<aftsection>
<nextsent>the data annotated for the ge research is used here for testing and development data.
</nextsent>
<nextsent>also, there are many overlaps between their formulation of the problem and ours.
</nextsent>
<nextsent>for one thing, their model is generative, although they do not note this fact, and (with the partial exception we are about to mention) they obtain their probabilities from hand annotated data rather than using em.
</nextsent>
<nextsent>lastly, they learn their gender information (the probability of that pronoun will have particular gender given its antecedent) using truncated em procedure.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2440">
<title id=" E09-1018.xml">em works for pronoun anaphora resolution </title>
<section> the generative model.  </section>
<citcontext>
<prevsection>
<prevsent>we first guess that the non-anaphoric pronouns is it?
</prevsent>
<prevsent>according to p(it?|non-anaphoric).2actually, as in most previous work, we only consider referents realized by nps.
</prevsent>
</prevsection>
<citsent citstr=" P02-1011 ">
for more general approaches see byron (2002).<papid> P02-1011 </papid></citsent>
<aftsection>
<nextsent>and then generate the governor/relation according to p(governor/relation|non-anaphoric-it); lastly we generate any other non-anaphoricpronouns and their governor with fixed probability p(other).
</nextsent>
<nextsent>(strictly speaking, this is mathematically invalid, since we do not bother to normalize over all the alternatives; good topic for future research would be exploring what happens when we make this part of the model truly generative.)
</nextsent>
<nextsent>one inelegant part of the model is the needto scale the p(governor/rel|antecedent) probabilities.
</nextsent>
<nextsent>we smooth them using kneser-ney smoothing, but even then their dynamic range (a factor of 106) greatly exceeds those of the other parameters.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2443">
<title id=" E09-1018.xml">em works for pronoun anaphora resolution </title>
<section> definition of correctness.  </section>
<citcontext>
<prevsection>
<prevsent>we evaluate all programs according to mitkovs resolution etiquette?
</prevsent>
<prevsent>scoring metric (also used in cherry and bergsma (2005)), <papid> W05-0612 </papid>which is defined as follows: if is the number of non-anaphoric pronouns correctly identified, the number of anaphoric pronouns correctly linked to their antecedent, and the total number of pronouns, then pronoun-anaphora programs percentage correct is n+ap . most papers dealing with pronoun coreference use this simple ratio, or the variant that ignores non-anaphoric pronouns.</prevsent>
</prevsection>
<citsent citstr=" P06-1006 ">
it has appeared under number of names: success (yang et al, 2006), <papid> P06-1006 </papid>accuracy (kehler et al, 2004<papid> N04-4009 </papid>a; angheluta et al, 2004) and success rate (tetreault, 2001).<papid> J01-4003 </papid></citsent>
<aftsection>
<nextsent>the other occasionally-used metric is the muc score restricted to pronouns, but this has well-known problems (bagga and baldwin, 1998).to make the definition perfectly concrete, how ever, we must resolve few special cases.
</nextsent>
<nextsent>one is the case in which pronoun correctly says that it is co referent with another pronoun y. how ever, the program mis identifies the antecedent of y. in this case (sometimes called error chaining (walker, 1989)), <papid> P89-1031 </papid>both and are to be scored aswrong, as they both end up in the wrong coreferential chain.</nextsent>
<nextsent>we believe this is, in fact, the standard (mitkov, personal communication), although there are few papers (tetreault, 2001; <papid> J01-4003 </papid>yang et al., 2006) <papid> P06-1006 </papid>which do the opposite and many which simply do not discuss this case.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2447">
<title id=" E09-1018.xml">em works for pronoun anaphora resolution </title>
<section> definition of correctness.  </section>
<citcontext>
<prevsection>
<prevsent>it has appeared under number of names: success (yang et al, 2006), <papid> P06-1006 </papid>accuracy (kehler et al, 2004<papid> N04-4009 </papid>a; angheluta et al, 2004) and success rate (tetreault, 2001).<papid> J01-4003 </papid></prevsent>
<prevsent>the other occasionally-used metric is the muc score restricted to pronouns, but this has well-known problems (bagga and baldwin, 1998).to make the definition perfectly concrete, how ever, we must resolve few special cases.</prevsent>
</prevsection>
<citsent citstr=" P89-1031 ">
one is the case in which pronoun correctly says that it is co referent with another pronoun y. how ever, the program mis identifies the antecedent of y. in this case (sometimes called error chaining (walker, 1989)), <papid> P89-1031 </papid>both and are to be scored aswrong, as they both end up in the wrong coreferential chain.</citsent>
<aftsection>
<nextsent>we believe this is, in fact, the standard (mitkov, personal communication), although there are few papers (tetreault, 2001; <papid> J01-4003 </papid>yang et al., 2006) <papid> P06-1006 </papid>which do the opposite and many which simply do not discuss this case.</nextsent>
<nextsent>one more issue arises in the case of system attempting to perform complete np anaphora3.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2452">
<title id=" E09-1018.xml">em works for pronoun anaphora resolution </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>ge marked every personal pronoun and allnoun phrases that were co referent with these pronouns.
</prevsent>
<prevsent>we used section 0 as our development set, and section 1 for testing.
</prevsent>
</prevsection>
<citsent citstr=" P05-1022 ">
we re parsed the sentences using the charniak and johnson parser (charniak and johnson, 2005) <papid> P05-1022 </papid>rather than using the gold-parses that ge marked up.</citsent>
<aftsection>
<nextsent>we hope thereby to make the results closer to those user will experience.
</nextsent>
<nextsent>(generally the gold trees perform about 0.005 higher than the machine parsed ver sion.)
</nextsent>
<nextsent>the test set has 1119 personal pronouns of which 246 are non-anaphoric.
</nextsent>
<nextsent>our selection of this dataset, rather than the widely used muc-6corpus, is motivated by this large number of pro nouns.we compared our results to four currently available anaphora programs from the web.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2455">
<title id=" E09-1018.xml">em works for pronoun anaphora resolution </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>we received four leads: javarap, open-nlp, bart and guitar.
</prevsent>
<prevsent>of course, these systems represent the best available work, not the state of the art.
</prevsent>
</prevsection>
<citsent citstr=" P04-1017 ">
we presume that more recent supervised systems (kehler et al,2004<papid> N04-4009 </papid>b; yang et al, 2004; <papid> P04-1017 </papid>yang et al, 2006) <papid> P06-1006 </papid>per 3of course our system does not attempt np coreference resolution, nor does javarap.</citsent>
<aftsection>
<nextsent>the other three comparison systems do.
</nextsent>
<nextsent>153form better.
</nextsent>
<nextsent>unfortunately, we were unable to obtain comparison unsupervised learning system at all.
</nextsent>
<nextsent>only one of the four is explicitly aimedat personal-pronoun anaphora ? rap (resolution of anaphora procedure) (lappin and leass, 1994).<papid> J94-4002 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2457">
<title id=" E09-1018.xml">em works for pronoun anaphora resolution </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>153form better.
</prevsent>
<prevsent>unfortunately, we were unable to obtain comparison unsupervised learning system at all.
</prevsent>
</prevsection>
<citsent citstr=" J94-4002 ">
only one of the four is explicitly aimedat personal-pronoun anaphora ? rap (resolution of anaphora procedure) (lappin and leass, 1994).<papid> J94-4002 </papid></citsent>
<aftsection>
<nextsent>it is non-statistical system originally implemented in prolog.
</nextsent>
<nextsent>the version we used is javarap, later re implementation in java (long qiu and chua, 2004).
</nextsent>
<nextsent>it only handles third person pronouns.
</nextsent>
<nextsent>the other three are more general in that they handle all np anaphora.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2458">
<title id=" E09-1018.xml">em works for pronoun anaphora resolution </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>opennlp (morton et al, 2005) uses amaximum-entropy classifier to rank potential antecedents for pronouns.
</prevsent>
<prevsent>however despite being the best-performing (on pronouns) of the existing systems, there is remarkable lack of published information on its innards.
</prevsent>
</prevsection>
<citsent citstr=" P08-4003 ">
bart (versley et al, 2008) <papid> P08-4003 </papid>also uses maximum-entropy model, based on soon et al (2001).<papid> J01-4004 </papid></citsent>
<aftsection>
<nextsent>the bart system also provides more sophisticated feature set than is available in the basic model, including tree-kernel features and avariety of web-based knowledge sources.
</nextsent>
<nextsent>unfortunately we were not able to get the basic version working.
</nextsent>
<nextsent>more precisely we were able to run the program, but the results we got were substantially lower than any of the other models and we believe that the program as shipped is not working prop erly.some of these systems provide their own preprocessing tools.
</nextsent>
<nextsent>however, these were bypassed, so that all systems ran on the charniak parse trees (with gold sentence segmentation).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2459">
<title id=" E09-1018.xml">em works for pronoun anaphora resolution </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>opennlp (morton et al, 2005) uses amaximum-entropy classifier to rank potential antecedents for pronouns.
</prevsent>
<prevsent>however despite being the best-performing (on pronouns) of the existing systems, there is remarkable lack of published information on its innards.
</prevsent>
</prevsection>
<citsent citstr=" J01-4004 ">
bart (versley et al, 2008) <papid> P08-4003 </papid>also uses maximum-entropy model, based on soon et al (2001).<papid> J01-4004 </papid></citsent>
<aftsection>
<nextsent>the bart system also provides more sophisticated feature set than is available in the basic model, including tree-kernel features and avariety of web-based knowledge sources.
</nextsent>
<nextsent>unfortunately we were not able to get the basic version working.
</nextsent>
<nextsent>more precisely we were able to run the program, but the results we got were substantially lower than any of the other models and we believe that the program as shipped is not working prop erly.some of these systems provide their own preprocessing tools.
</nextsent>
<nextsent>however, these were bypassed, so that all systems ran on the charniak parse trees (with gold sentence segmentation).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2460">
<title id=" E12-2005.xml">swan  scientific writing assistant a tool for helping scholars to write reader friendly manuscripts </title>
<section> the swan tool.  </section>
<citcontext>
<prevsection>
<prevsent>in addition to these two main evaluation modes, the tool also includes manual fluidity assessment exercise where the writer goes through given text passage, sentence by sentence, to see whether the next sentence can be predicted from the previous sentences.
</prevsent>
<prevsent>4.2 implementation and external libraries.
</prevsent>
</prevsection>
<citsent citstr=" N03-1033 ">
the tool is desktop application written in java.it uses external libraries for natural language processing from stanford, namely stanford pos tagger (toutanova et al 2003) <papid> N03-1033 </papid>and stanford parser (klein and manning, 2003).<papid> P03-1054 </papid></citsent>
<aftsection>
<nextsent>this is one of themost accurate and robust parsers available and implemented in java, as is the rest of our system.
</nextsent>
<nextsent>other external libraries include apache tika3, which we use in extracting textual content from files.
</nextsent>
<nextsent>jfreechart4 is used in generating graphs 2http://pdfbox.apache.org/ 3http://tika.apache.org/ 4http://www.jfree.org/jfreechart/ and xstream5 in saving and loading inputs and results.
</nextsent>
<nextsent>since its release in june 2011, the tool hasbeen used in scientific writing classes in doctoral schools in france, finland, and singapore, as well as in 16 research institutes from a*star (agency for science technology and research).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2461">
<title id=" E12-2005.xml">swan  scientific writing assistant a tool for helping scholars to write reader friendly manuscripts </title>
<section> the swan tool.  </section>
<citcontext>
<prevsection>
<prevsent>in addition to these two main evaluation modes, the tool also includes manual fluidity assessment exercise where the writer goes through given text passage, sentence by sentence, to see whether the next sentence can be predicted from the previous sentences.
</prevsent>
<prevsent>4.2 implementation and external libraries.
</prevsent>
</prevsection>
<citsent citstr=" P03-1054 ">
the tool is desktop application written in java.it uses external libraries for natural language processing from stanford, namely stanford pos tagger (toutanova et al 2003) <papid> N03-1033 </papid>and stanford parser (klein and manning, 2003).<papid> P03-1054 </papid></citsent>
<aftsection>
<nextsent>this is one of themost accurate and robust parsers available and implemented in java, as is the rest of our system.
</nextsent>
<nextsent>other external libraries include apache tika3, which we use in extracting textual content from files.
</nextsent>
<nextsent>jfreechart4 is used in generating graphs 2http://pdfbox.apache.org/ 3http://tika.apache.org/ 4http://www.jfree.org/jfreechart/ and xstream5 in saving and loading inputs and results.
</nextsent>
<nextsent>since its release in june 2011, the tool hasbeen used in scientific writing classes in doctoral schools in france, finland, and singapore, as well as in 16 research institutes from a*star (agency for science technology and research).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2462">
<title id=" E12-1059.xml">uby  a largescale unified lexical semantic resource based on lmf </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>lexical-semantic resources (lsrs) are the foundation of many nlp tasks such as word sense disambiguation, semantic role labeling, question answering and information extraction.
</prevsent>
<prevsent>they are needed on large scale in different languages.the growing demand for resources is met neither by the largest single expert-constructed resources (ecrs), such as wordnet and framenet, whose coverage is limited, nor by collaboratively constructed resources (ccrs), such as wikipedia and wiktionary, which encode lexical-semantic knowledge in less systematic form than ecrs, because they are lacking expert supervision.previously, there have been several independent efforts of combining existing lsrs to enhance their coverage w.r.t. their breadth and depth, i.e.
</prevsent>
</prevsection>
<citsent citstr=" P10-1023 ">
(i) the number of lexical items, and (ii) the types of lexical-semantic information contained (shi and mihalcea, 2005; johansson and nugues, 2007; navigli and ponzetto, 2010<papid> P10-1023 </papid>b; meyer and gurevych, 2011).</citsent>
<aftsection>
<nextsent>as these efforts often targeted particular applications, they focused on aligning selected, specialized information types.
</nextsent>
<nextsent>to our knowledge, no single work focused on model inga wide range of ecrs and ccrs in multiple languages and large variety of information types in standardized format.
</nextsent>
<nextsent>frequently, the presented model is not easily scalable to accommodate anopen set of lsrs in multiple languages and the information mined automatically from corpora.
</nextsent>
<nextsent>the previous work also lacked the aspects of lexicon format standardization and api access.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2464">
<title id=" E12-1059.xml">uby  a largescale unified lexical semantic resource based on lmf </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we model the lexical-semanticinformation down to fine-grained level of information (e.g. syntactic frames) and employ standardized definitions of linguistic information types from isocat.
</prevsent>
<prevsent>(2) we present uby, large scale lsr implementing the uby-lmf model.
</prevsent>
</prevsection>
<citsent citstr=" P98-1013 ">
uby currently contains nine resources in two languages: english wordnet (wn, fellbaum(1998), wiktionary2 (wkt-en), wikipedia3 (wp en), framenet (fn, baker et al(1998)), <papid> P98-1013 </papid>andverbnet (vn, kipper et al(2008)); german wiktionary (wkt-de), wikipedia (wp-de), and germanet (gn, kunze and lemnitzer (2002)), and the english and german entries of omegawiki4 (ow), referred to as ow-en and ow-de.</citsent>
<aftsection>
<nextsent>ow,a novel ccr, is inherently multilingual ? its basic structure are multilingual synsets, which are avaluable addition to our multilingual uby.
</nextsent>
<nextsent>essential to uby are the nine pairwise sense alignments between resources, which we provide to enable resource interoperability on the sense level, e.g. by providing access to the often complementary information for sense in different resources.
</nextsent>
<nextsent>(3) we present java-api which offers unified access to the information contained in uby.we will make the uby-lmf model, there source uby and the api freely available to the research community.5 this will make it easy for the nlp community to utilize uby in variety of tasks in the future.
</nextsent>
<nextsent>the work presented in this paper concerns standardization of lsrs, large-scale integration thereof at the representational level, and the unified access to lexical-semantic information in the integrated resources.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2465">
<title id=" E12-1059.xml">uby  a largescale unified lexical semantic resource based on lmf </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the work presented in this paper concerns standardization of lsrs, large-scale integration thereof at the representational level, and the unified access to lexical-semantic information in the integrated resources.
</prevsent>
<prevsent>standardization of resources.
</prevsent>
</prevsection>
<citsent citstr=" C10-1052 ">
previous work includes models for representing lexical information relative to ontologies (buitelaar et al 2009; mccrae et al 2011), and standardized single wordnets (english, german and italian wordnets)in the iso standard lmf (soria et al 2009; henrich and hinrichs, 2010; <papid> C10-1052 </papid>toral et al 2010).</citsent>
<aftsection>
<nextsent>2http://www.wiktionary.org/ 3http://www.wikipedia.org/ 4http://www.omegawiki.org/ 5http://www.ukp.tu-darmstadt.de/data/ubymccrae et al(2011) propose lemon, conceptual model for lexicalizing ontologies as an extension of the lexinfo model (buitelaar et al 2009).
</nextsent>
<nextsent>lemon provides an lmf-implementation in the web ontology language (owl), which is similar to uby-lmf, as it also uses dcsfrom isocat, but diverges further from the standard (e.g. by removing structural elements such as the predicative representation class).
</nextsent>
<nextsent>whilewe focus on modeling lexical-semantic information comprehensively and at fine-grained level,the goal of lemon is to support the linking between ontologies and lexicons.
</nextsent>
<nextsent>this goal entailsa task-targeted application: domain-specific lexicons are extracted from ontology specifications and merged with existing lsrs on demand.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2468">
<title id=" E12-1059.xml">uby  a largescale unified lexical semantic resource based on lmf </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>similarly,de melo and weikum (2009) create multilingual wordnet by integrating wordnets, bilingual dictionaries and information from parallel corpora.
</prevsent>
<prevsent>none of these resources integrate lexical semantic information, such as syntactic subcategorization or semantic roles.
</prevsent>
</prevsection>
<citsent citstr=" P11-2063 ">
mcfate and forbus (2011) <papid> P11-2063 </papid>present nulex, syntactic lexicon automatically compiled fromwn, wkt-en and vn.</citsent>
<aftsection>
<nextsent>as their goal is to create an open-license resource to enhance syntactic parsing, they enrich verbs and nouns in wn with inflection information from wkt-en and syntactic frames from vn.
</nextsent>
<nextsent>thus, they only use small part of the lexical information present in wkt-en.padro?
</nextsent>
<nextsent>et al(2011) present their work on lexicon merging within the panacea project.
</nextsent>
<nextsent>onegoal of panacea is to create lexical resource development platform that supports large-scale lexical acquisition and can be used to combine existing lexicons with automatically acquired ones.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2471">
<title id=" E12-1059.xml">uby  a largescale unified lexical semantic resource based on lmf </title>
<section> uby ? data model.  </section>
<citcontext>
<prevsection>
<prevsent>we have designed ubylmf8 as model of the union of various heterogeneous resources, namely wn, gn, fn, and vn on the one hand and ccrs on the other hand.
</prevsent>
<prevsent>two design principles guided our development of uby-lmf: first, to preserve the information available in the original resources and to uniformly represent it in uby-lmf.
</prevsent>
</prevsection>
<citsent citstr=" W09-3021 ">
second, to beable to extend uby in the future by further languages, resources, and types of linguistic information, in particular, alignments between different lsrs.wordnets, fn and vn are largely complementary regarding the information types they provide,see, e.g. baker and fellbaum (2009).<papid> W09-3021 </papid></citsent>
<aftsection>
<nextsent>accordingly, they use different organizational units to represent this information.
</nextsent>
<nextsent>wordnets, such as wn and gn, primarily contain information on lexical-semantic relations, such as synonymy, anduse synsets (groups of lexemes that are synony mous) as organizational units.
</nextsent>
<nextsent>fn focuses on groups of lexemes that evoke the same prototypical situation (so-called semantic frames, fillmore (1982)) involving semantic roles (so-called frameelements).
</nextsent>
<nextsent>vn, large-scale verb lexicon, is organized in levin-style verb classes (levin, 1993)(groups of verbs that share the same syntactical ternations and semantic roles) and provides rich subcategorization frames including semantic roles and specification of semantic predicates.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2472">
<title id=" E12-1059.xml">uby  a largescale unified lexical semantic resource based on lmf </title>
<section> uby ? population with information.  </section>
<citcontext>
<prevsection>
<prevsent>in the following, we (i) describe how we converted already existing sense alignments of resources into lmf, and (ii) present framework to infer alignments automatically for any pair of resources.
</prevsent>
<prevsent>existing alignments.
</prevsent>
</prevsection>
<citsent citstr=" W11-0122 ">
previous work on sense alignment yielded several alignments, such as wnwp-en (niemann and gurevych, 2011), <papid> W11-0122 </papid>wnwkt-en (meyer and gurevych, 2011) and vnfn (palmer, 2009).</citsent>
<aftsection>
<nextsent>we converted these alignments into uby-lmf by creating sense axis instance for each pair of aligned senses.
</nextsent>
<nextsent>this involved mapping the sense ids from the proprietary alignment files to the corresponding sense ids in uby.
</nextsent>
<nextsent>in addition, we integrated the sense alignments already present in ow and wp.
</nextsent>
<nextsent>some ow entries provide links to the corresponding wp page.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2473">
<title id=" E12-1059.xml">uby  a largescale unified lexical semantic resource based on lmf </title>
<section> uby ? population with information.  </section>
<citcontext>
<prevsection>
<prevsent>we can expect that these links have high quality, as they were entered manually by users and are subject to community control.
</prevsent>
<prevsent>therefore, we straightforwardly imported them into uby.alignment framework.
</prevsent>
</prevsection>
<citsent citstr=" P06-1014 ">
automatically creating new alignments is difficult because of word ambiguities, different granularities of senses, or language specific conceptualizations (navigli, 2006).<papid> P06-1014 </papid></citsent>
<aftsection>
<nextsent>to support this task for large number of resources across languages, we have designed flexible alignment framework based on the state-of-the-art method of niemann and gurevych(2011).<papid> W11-0122 </papid></nextsent>
<nextsent>the framework is generic in order to allow alignments between different kinds of entities as found in different resources, e.g. wn synsets, fn frames or wp articles.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2485">
<title id=" E12-1026.xml">adapting translation models to translation ese improves smt </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>knownas translation ese, translated texts (in any lan guage) constitute genre, or dialect, of the target language, which reflects both artifacts ofthe translation process and traces of the original language from which the texts were translated.
</prevsent>
<prevsent>among the better-known properties of translation ese are simplification and explicit ation (baker, 1993, 1995, 1996): translated texts tend to be shorter, to have lower type/token ratio, and to use certain discourse markers more frequently than original texts.
</prevsent>
</prevsection>
<citsent citstr=" P11-1132 ">
incidentally, translated texts are so markedly different from original ones that automatic classification can identify them with very high accuracy (van halteren, 2008; baroni and bernard ini, 2006; ilisei et al  2010; koppel and ordan, 2011).<papid> P11-1132 </papid></citsent>
<aftsection>
<nextsent>contemporary statistical machine translation(smt) systems use parallel corpora to train translation models that reflect source- and target language phrase correspondences.
</nextsent>
<nextsent>typically, smt systems ignore the direction of translation used to produce those corpora.
</nextsent>
<nextsent>given the unique properties of translation ese, however, it is reason able to assume that this direction may affect the quality of the translation.
</nextsent>
<nextsent>recently, kurokawa et al (2009) showed that this is indeed the case.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2486">
<title id=" E12-1026.xml">adapting translation models to translation ese improves smt </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this is achieved by casting the problem in the framework of domain adaptation: we use domain-adaptationtechniques to direct the smt system toward producing output that better reflects the properties of translationese.
</prevsent>
<prevsent>we show that smt systems adapted to translation ese produce better translations than vanilla systems trained on exactly the same resources.
</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
we confirm these findings using an automatic evaluation metric, bleu (papineniet al  2002), <papid> P02-1040 </papid>as well as through qualitative analysis of the results.</citsent>
<aftsection>
<nextsent>our departure point is the results of kurokawa et al (2009), which we successfully replicate in section 3.
</nextsent>
<nextsent>first (section 4), we explain why translation quality improves when the parallel corpus is translated in the right?
</nextsent>
<nextsent>direction.
</nextsent>
<nextsent>we do so by showing that the subset of the corpus that was translated in the direction of the translation task (the right?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2487">
<title id=" E12-1026.xml">adapting translation models to translation ese improves smt </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>we indeed replicate these results here (section 3), and view them as baseline.
</prevsent>
<prevsent>additionally, we show that the ? portion is also important forma chine translation and thus should not be discarded.using information-theory measures, and in particular cross-entropy, we gain statistically significant improvements in translation quality beyond the results of kurokawa et al (2009).
</prevsent>
</prevsection>
<citsent citstr=" D11-1034 ">
furthermore, we eliminate the need to (manually or au tomatically) detect the direction of translation of the parallel corpus.lembersky et al (2011)<papid> D11-1034 </papid>also investigate the relations between translation ese and machine trans lation.</citsent>
<aftsection>
<nextsent>focusing on the language model (lm), they show that lms trained on translated texts yield better translation quality than lms compiled from original texts.
</nextsent>
<nextsent>they also show that perplexity is good discriminator between original and translated texts.
</nextsent>
<nextsent>our current work is closely related to research in domain-adaptation.
</nextsent>
<nextsent>in typical domain adaptation scenario, system is trained on large corpus of general?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2490">
<title id=" E12-1026.xml">adapting translation models to translation ese improves smt </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>most existing adaptation methods focus on selecting in-domain data from general domain corpus.
</prevsent>
<prevsent>in particular, perplexity is used to score the sentences in the general-domain corpus according to an in-domain language model.
</prevsent>
</prevsection>
<citsent citstr=" D11-1033 ">
gao et al (2002) and moore and lewis (2010) apply this method to language modeling, while foster 256 et al (2010) and axelrod et al (2011) <papid> D11-1033 </papid>use it on the translation model.</citsent>
<aftsection>
<nextsent>moore and lewis (2010)suggest slightly different approach, using cross entropy difference as ranking function.
</nextsent>
<nextsent>domain adaptation methods are usually applied at the corpus level, while we focus on an adaptation of the phrase table used for smt.
</nextsent>
<nextsent>in this sense, our work follows foster et al (2010), who weigh out-of-domain phrase pairs according to their relevance to the target domain.
</nextsent>
<nextsent>they use multiple features that help distinguish between phrase pairs in the general domain and those in the specific domain.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2491">
<title id=" E12-1026.xml">adapting translation models to translation ese improves smt </title>
<section> experimental setup.  </section>
<citcontext>
<prevsection>
<prevsent>the hansard is abi lingual french english corpus comprising approximately 80% english-original texts and 20% french-original texts.
</prevsent>
<prevsent>crucially, each sentence pair in the corpus is annotated with the direction of translation.
</prevsent>
</prevsection>
<citsent citstr=" P07-2045 ">
both english and french are lower cased and tokenized using moses (koehn et al 2007).<papid> P07-2045 </papid></citsent>
<aftsection>
<nextsent>sentences longer than 80 words are discarded.
</nextsent>
<nextsent>to address the effect of the corpus size, we compile six subsets of different sizes (250k, 500k, 750k, 1m, 1.25m and 1.5m parallel sentences) from each portion (english-originaland french-original) of the corpus.
</nextsent>
<nextsent>additionally, we use the devtest section of the hansard corpus to randomly select french-original andenglish-original sentences that are used for tuning (1,000 sentences each) and evaluation (5,000sentences each).
</nextsent>
<nextsent>french-to-english mt systems are tuned and tested on french-original sentences and english-to-french systems on english original ones.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2493">
<title id=" E12-1026.xml">adapting translation models to translation ese improves smt </title>
<section> experimental setup.  </section>
<citcontext>
<prevsection>
<prevsent>french-to-english mt systems are tuned and tested on french-original sentences and english-to-french systems on english original ones.
</prevsent>
<prevsent>to replicate the results of kurokawa et al  (2009) and set up baseline, we train twelve french-to-english and twelve english-to-french phrase-based (pb-) smt systems using the moses toolkit (koehn et al  2007), <papid> P07-2045 </papid>each trained on different subset of the corpus.</prevsent>
</prevsection>
<citsent citstr=" P00-1056 ">
we usegiza++ (och and ney, 2000) <papid> P00-1056 </papid>with grow-diag final alignment, and extract phrases of length up to 10 words.</citsent>
<aftsection>
<nextsent>we prune the resulting phrase table sas in johnson et al (2007), <papid> D07-1103 </papid>using at most 30 translations per source phrase and discarding singleton phrase pairs.we construct english and french 5-gram language models from the english and french subsections of the europarl-v6 corpus (koehn, 2005), using interpolated modified kneser-ney discounting (chen, 1998) and no cut-off on all n-grams.</nextsent>
<nextsent>europarl consists of large number of subsets translated from various languages, andis therefore unlikely to be biased towards specific source language.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2494">
<title id=" E12-1026.xml">adapting translation models to translation ese improves smt </title>
<section> experimental setup.  </section>
<citcontext>
<prevsection>
<prevsent>to replicate the results of kurokawa et al  (2009) and set up baseline, we train twelve french-to-english and twelve english-to-french phrase-based (pb-) smt systems using the moses toolkit (koehn et al  2007), <papid> P07-2045 </papid>each trained on different subset of the corpus.</prevsent>
<prevsent>we usegiza++ (och and ney, 2000) <papid> P00-1056 </papid>with grow-diag final alignment, and extract phrases of length up to 10 words.</prevsent>
</prevsection>
<citsent citstr=" D07-1103 ">
we prune the resulting phrase table sas in johnson et al (2007), <papid> D07-1103 </papid>using at most 30 translations per source phrase and discarding singleton phrase pairs.we construct english and french 5-gram language models from the english and french subsections of the europarl-v6 corpus (koehn, 2005), using interpolated modified kneser-ney discounting (chen, 1998) and no cut-off on all n-grams.</citsent>
<aftsection>
<nextsent>europarl consists of large number of subsets translated from various languages, andis therefore unlikely to be biased towards specific source language.
</nextsent>
<nextsent>the reordering model used in all mt systems is trained on the union ofthe 1.5m french-original and the 1.5m english original subsets, using msd-bidirectional-fe reordering.
</nextsent>
<nextsent>we use the mert algorithm (och, 2003) <papid> P03-1021 </papid>for tuning and bleu (papineni et al  2002) <papid> P02-1040 </papid>as our evaluation metric.</nextsent>
<nextsent>we test the statistical significance of the differences between the results using the bootstrap re sampling method (koehn, 2004).<papid> W04-3250 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2495">
<title id=" E12-1026.xml">adapting translation models to translation ese improves smt </title>
<section> experimental setup.  </section>
<citcontext>
<prevsection>
<prevsent>europarl consists of large number of subsets translated from various languages, andis therefore unlikely to be biased towards specific source language.
</prevsent>
<prevsent>the reordering model used in all mt systems is trained on the union ofthe 1.5m french-original and the 1.5m english original subsets, using msd-bidirectional-fe reordering.
</prevsent>
</prevsection>
<citsent citstr=" P03-1021 ">
we use the mert algorithm (och, 2003) <papid> P03-1021 </papid>for tuning and bleu (papineni et al  2002) <papid> P02-1040 </papid>as our evaluation metric.</citsent>
<aftsection>
<nextsent>we test the statistical significance of the differences between the results using the bootstrap re sampling method (koehn, 2004).<papid> W04-3250 </papid></nextsent>
<nextsent>a word on notation: we use english-original?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2497">
<title id=" E12-1026.xml">adapting translation models to translation ese improves smt </title>
<section> experimental setup.  </section>
<citcontext>
<prevsection>
<prevsent>the reordering model used in all mt systems is trained on the union ofthe 1.5m french-original and the 1.5m english original subsets, using msd-bidirectional-fe reordering.
</prevsent>
<prevsent>we use the mert algorithm (och, 2003) <papid> P03-1021 </papid>for tuning and bleu (papineni et al  2002) <papid> P02-1040 </papid>as our evaluation metric.</prevsent>
</prevsection>
<citsent citstr=" W04-3250 ">
we test the statistical significance of the differences between the results using the bootstrap re sampling method (koehn, 2004).<papid> W04-3250 </papid></citsent>
<aftsection>
<nextsent>a word on notation: we use english-original?
</nextsent>
<nextsent>(eo) and french-original?
</nextsent>
<nextsent>(fo) to refer to the subsets of the corpus that are translated from english to french and from french to english, respectively.
</nextsent>
<nextsent>the translation tasks are english-to french (e2f) and french-to-english (f2e).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2498">
<title id=" E12-1026.xml">adapting translation models to translation ese improves smt </title>
<section> analysis of the phrase tables.  </section>
<citcontext>
<prevsection>
<prevsent>given source phrase and phrase table with translations of whose probabilities are p(t|s), the entropy of is: h(s) = ? ?
</prevsent>
<prevsent>tt p(t|s)?
</prevsent>
</prevsection>
<citsent citstr=" W11-2132 ">
log2p(t|s) (1) there are two major flavors of the phrase table entropy metric: lambert et al (2011) <papid> W11-2132 </papid>calculate the average entropy over all translation options for each source phrase (henceforth, phrase table entropy or ptent), whereas koehn et al (2009) search through all possible segment ations of the source sentence to find the optimal covering set of test sentences that minimizes the average entropy of the source phrases in the covering set (hence forth, covering set entropy or covent).we also propose metric that assesses the quality of the source side of phrase table.</citsent>
<aftsection>
<nextsent>the metric finds the minimal covering set of given text in the source language using source phrases from particular phrase table, and outputs the average length of phrase in the covering set (henceforth, covering set average length or covlen).
</nextsent>
<nextsent>lembersky et al (2011)<papid> D11-1034 </papid>show that perplexity distinguishes well between translated and original texts.</nextsent>
<nextsent>moreover, perplexity reflects the degree of relatedness?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2502">
<title id=" E12-1026.xml">adapting translation models to translation ese improves smt </title>
<section> conclusion.  </section>
<citcontext>
<prevsection>
<prevsent>it is completely generic and can be applied to any language pair, domain or corpus.this work can be extended in various directions.
</prevsent>
<prevsent>we plan to further explore the use of two phrase tables, one for each direction-determined subset of the parallel corpus.
</prevsent>
</prevsection>
<citsent citstr=" W07-0717 ">
specifically, we will interpol ate the translation models as in foster and kuhn (2007), <papid> W07-0717 </papid>including maximum posteriori combination (bacchiani et al  2006).</citsent>
<aftsection>
<nextsent>we also plan to up weight the ? subset of the parallel corpus and train single phrase table on the con catenated corpus.
</nextsent>
<nextsent>finally, we intend to extend this work by combining the translation-model adaptation we present here with the language-model adaptation suggested by lembersky et al (2011)<papid> D11-1034 </papid>in unified system that is more tuned to generating translationese.</nextsent>
<nextsent>acknowledgments we are grateful to cyril goutte, george foster and pierre isabelle for providing us with an annotated version of the hansard corpus.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2504">
<title id=" E12-1085.xml">managing uncertainty in semantic tagging </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>traditionally, semantic tagging relies on the tacit assumption that various uses of polysemouswords can be sorted into discrete senses; understanding or using an unfamiliar word be then like looking it up in dictionary.
</prevsent>
<prevsent>when building dictionary entry forgiven word, the lexicographer sorts number of its occurrences into discrete senses present (or emerging) in his/her mental lexicon, which is supposed to be shared by all speakers of the same language.
</prevsent>
</prevsection>
<citsent citstr=" W97-0206 ">
the assumed common mental representation of words meaning should make it easy for other humans to assign random occurrences of the word to one of the pre-defined senses (fellbaum et al 1997).<papid> W97-0206 </papid></citsent>
<aftsection>
<nextsent>this assumption seems to be falsified by the inter annotator agreement (iaa, sometimes ita) constantly reported much lower in semantic than in morphological or syntactic annotation, as well as by the general divergence of opinion on which value of which iaa measure indicates reliable annotation.
</nextsent>
<nextsent>in some projects (e.g. ontonotes (hovy et al 2006)), <papid> N06-2015 </papid>the percentage of agreements between two annotators is used, but number of more complex measures are available (for comprehensive survey see (artstein and poesio, 2008)).<papid> J08-4004 </papid></nextsent>
<nextsent>consequently, using different measures for iaa makes the reported iaa values incomparable across different projects.even skilled lexicographers have trouble selecting one discrete sense for concordance (kr ishnamurthy and nicholls, 2000), and, more tosay, when the tagging performance of lexicog raphers and ordinary annotators (students) was 840compared, the experiment showed that the mental representations of words semantics differ foreach group (fellbaum et al 1997), <papid> W97-0206 </papid>and cf.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2505">
<title id=" E12-1085.xml">managing uncertainty in semantic tagging </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the assumed common mental representation of words meaning should make it easy for other humans to assign random occurrences of the word to one of the pre-defined senses (fellbaum et al 1997).<papid> W97-0206 </papid></prevsent>
<prevsent>this assumption seems to be falsified by the inter annotator agreement (iaa, sometimes ita) constantly reported much lower in semantic than in morphological or syntactic annotation, as well as by the general divergence of opinion on which value of which iaa measure indicates reliable annotation.</prevsent>
</prevsection>
<citsent citstr=" N06-2015 ">
in some projects (e.g. ontonotes (hovy et al 2006)), <papid> N06-2015 </papid>the percentage of agreements between two annotators is used, but number of more complex measures are available (for comprehensive survey see (artstein and poesio, 2008)).<papid> J08-4004 </papid></citsent>
<aftsection>
<nextsent>consequently, using different measures for iaa makes the reported iaa values incomparable across different projects.even skilled lexicographers have trouble selecting one discrete sense for concordance (kr ishnamurthy and nicholls, 2000), and, more tosay, when the tagging performance of lexicog raphers and ordinary annotators (students) was 840compared, the experiment showed that the mental representations of words semantics differ foreach group (fellbaum et al 1997), <papid> W97-0206 </papid>and cf.</nextsent>
<nextsent>(jorgensen, 1990).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2506">
<title id=" E12-1085.xml">managing uncertainty in semantic tagging </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the assumed common mental representation of words meaning should make it easy for other humans to assign random occurrences of the word to one of the pre-defined senses (fellbaum et al 1997).<papid> W97-0206 </papid></prevsent>
<prevsent>this assumption seems to be falsified by the inter annotator agreement (iaa, sometimes ita) constantly reported much lower in semantic than in morphological or syntactic annotation, as well as by the general divergence of opinion on which value of which iaa measure indicates reliable annotation.</prevsent>
</prevsection>
<citsent citstr=" J08-4004 ">
in some projects (e.g. ontonotes (hovy et al 2006)), <papid> N06-2015 </papid>the percentage of agreements between two annotators is used, but number of more complex measures are available (for comprehensive survey see (artstein and poesio, 2008)).<papid> J08-4004 </papid></citsent>
<aftsection>
<nextsent>consequently, using different measures for iaa makes the reported iaa values incomparable across different projects.even skilled lexicographers have trouble selecting one discrete sense for concordance (kr ishnamurthy and nicholls, 2000), and, more tosay, when the tagging performance of lexicog raphers and ordinary annotators (students) was 840compared, the experiment showed that the mental representations of words semantics differ foreach group (fellbaum et al 1997), <papid> W97-0206 </papid>and cf.</nextsent>
<nextsent>(jorgensen, 1990).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2508">
<title id=" E12-1085.xml">managing uncertainty in semantic tagging </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>intuitively, the finer thegranularity of word entry is, the more opportunities for inter annotator disagreement there are and the lower iaa can be expected.
</prevsent>
<prevsent>brown et al proved this hypothesis experimentally (brown et al., 2010).
</prevsent>
</prevsection>
<citsent citstr=" J05-1004 ">
also, the annotators are less confident in their decisions, when they have many options to choose from (fellbaum et al(1998) reported drop in subjective annotators confidence in words with 8+ senses).despite all the known issues in semantic tagging, the major lexical resources (wordnet (fell baum, 1998), framenet (ruppenhofer et al 2010), propbank (palmer et al 2005) <papid> J05-1004 </papid>and the word-sense part of ontonotes (weischedel et al 2011)) are still maintained and their annotation schemes are adopted for creating new manually annotated data (e.g. masc, the manually annotated sub corpus (ide et al 2008)).<papid> L08-1105 </papid></citsent>
<aftsection>
<nextsent>more to say, these resources are not only used in wsd and semantic labeling, but also in research directions that in their turn do not relyon the idea of an inventory of discrete senses any more, e.g. in distributional semantics (erk, 2010) <papid> W10-2803 </papid>and recognizing textual entailment (e.g.</nextsent>
<nextsent>(zanzotto et al 2009) and (aharon et al 2010)).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2509">
<title id=" E12-1085.xml">managing uncertainty in semantic tagging </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>intuitively, the finer thegranularity of word entry is, the more opportunities for inter annotator disagreement there are and the lower iaa can be expected.
</prevsent>
<prevsent>brown et al proved this hypothesis experimentally (brown et al., 2010).
</prevsent>
</prevsection>
<citsent citstr=" L08-1105 ">
also, the annotators are less confident in their decisions, when they have many options to choose from (fellbaum et al(1998) reported drop in subjective annotators confidence in words with 8+ senses).despite all the known issues in semantic tagging, the major lexical resources (wordnet (fell baum, 1998), framenet (ruppenhofer et al 2010), propbank (palmer et al 2005) <papid> J05-1004 </papid>and the word-sense part of ontonotes (weischedel et al 2011)) are still maintained and their annotation schemes are adopted for creating new manually annotated data (e.g. masc, the manually annotated sub corpus (ide et al 2008)).<papid> L08-1105 </papid></citsent>
<aftsection>
<nextsent>more to say, these resources are not only used in wsd and semantic labeling, but also in research directions that in their turn do not relyon the idea of an inventory of discrete senses any more, e.g. in distributional semantics (erk, 2010) <papid> W10-2803 </papid>and recognizing textual entailment (e.g.</nextsent>
<nextsent>(zanzotto et al 2009) and (aharon et al 2010)).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2510">
<title id=" E12-1085.xml">managing uncertainty in semantic tagging </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>brown et al proved this hypothesis experimentally (brown et al., 2010).
</prevsent>
<prevsent>also, the annotators are less confident in their decisions, when they have many options to choose from (fellbaum et al(1998) reported drop in subjective annotators confidence in words with 8+ senses).despite all the known issues in semantic tagging, the major lexical resources (wordnet (fell baum, 1998), framenet (ruppenhofer et al 2010), propbank (palmer et al 2005) <papid> J05-1004 </papid>and the word-sense part of ontonotes (weischedel et al 2011)) are still maintained and their annotation schemes are adopted for creating new manually annotated data (e.g. masc, the manually annotated sub corpus (ide et al 2008)).<papid> L08-1105 </papid></prevsent>
</prevsection>
<citsent citstr=" W10-2803 ">
more to say, these resources are not only used in wsd and semantic labeling, but also in research directions that in their turn do not relyon the idea of an inventory of discrete senses any more, e.g. in distributional semantics (erk, 2010) <papid> W10-2803 </papid>and recognizing textual entailment (e.g.</citsent>
<aftsection>
<nextsent>(zanzotto et al 2009) and (aharon et al 2010)).
</nextsent>
<nextsent>it is remarkable fact that, to the best of our knowledge, there is no measure that would relate granularity, reliability of the annotation (derived from iaa) and the resulting information gain.therefore it is impossible to say where the optimum for granularity and iaa lies.
</nextsent>
<nextsent>2.1 semantic tagging vs. morphological or.
</nextsent>
<nextsent>syntactic analysis manual semantic tagging is in many respects similar to morphological tagging and syntactic analysis: human annotators are trained to sort certain elements occurring in running text according to reference source.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2511">
<title id=" E12-1085.xml">managing uncertainty in semantic tagging </title>
<section> approaches to semantic tagging.  </section>
<citcontext>
<prevsection>
<prevsent>there is, nevertheless, substantial difference: whereas morphologically or syntactically annotated data exist separately from the reference (tagset, annotation guide, annotation scheme), semantically tagged resource can be regarded both as corpus of texts disambiguated according to an attached inventory of semantic categories and as lexicon with links to example concordancesfor each semantic category.
</prevsent>
<prevsent>so, in semantically tagged resources, the data and the reference are intertwined.
</prevsent>
</prevsection>
<citsent citstr=" H93-1061 ">
such double-faced semantic resources have also been called semantic concor dances (miller et al 1993<papid> H93-1061 </papid>a).</citsent>
<aftsection>
<nextsent>for instance, one ofthe earlier versions of wordnet, the largest lexical resource for english, was used in the semantic concordance semcor (miller et al 1993<papid> H93-1061 </papid>b).</nextsent>
<nextsent>more recent lexical resources have been built as semantic concordances from the very beginning (propbank (palmer et al 2005), <papid> J05-1004 </papid>ontonotes word senses (weischedel et al 2011)).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2516">
<title id=" E12-1085.xml">managing uncertainty in semantic tagging </title>
<section> approaches to semantic tagging.  </section>
<citcontext>
<prevsection>
<prevsent>even in difficult cases, there are hardly more thantwo options of interpretation.
</prevsent>
<prevsent>data manually annotated for morphology or surface syntax are reliable enough to train syntactic parsers with an accuracy above 80 % (e.g.
</prevsent>
</prevsection>
<citsent citstr=" J11-1005 ">
(zhang and clark, 2011; <papid> J11-1005 </papid>mcdonald et al 2006)).<papid> W06-2932 </papid></citsent>
<aftsection>
<nextsent>on the other hand, semantic tagging actually employs different tagset for each word lemma.
</nextsent>
<nextsent>even within the same part of speech, individual words require individual descriptions.
</nextsent>
<nextsent>possible similarities among them come into relief ex post rather than that they could be imposed on the lexicographers from the beginning.
</nextsent>
<nextsent>when assigning senses to concordances, the annotator often has to select among more than two relevant options.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2517">
<title id=" E12-1085.xml">managing uncertainty in semantic tagging </title>
<section> approaches to semantic tagging.  </section>
<citcontext>
<prevsection>
<prevsent>even in difficult cases, there are hardly more thantwo options of interpretation.
</prevsent>
<prevsent>data manually annotated for morphology or surface syntax are reliable enough to train syntactic parsers with an accuracy above 80 % (e.g.
</prevsent>
</prevsection>
<citsent citstr=" W06-2932 ">
(zhang and clark, 2011; <papid> J11-1005 </papid>mcdonald et al 2006)).<papid> W06-2932 </papid></citsent>
<aftsection>
<nextsent>on the other hand, semantic tagging actually employs different tagset for each word lemma.
</nextsent>
<nextsent>even within the same part of speech, individual words require individual descriptions.
</nextsent>
<nextsent>possible similarities among them come into relief ex post rather than that they could be imposed on the lexicographers from the beginning.
</nextsent>
<nextsent>when assigning senses to concordances, the annotator often has to select among more than two relevant options.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2518">
<title id=" E12-1085.xml">managing uncertainty in semantic tagging </title>
<section> approaches to semantic tagging.  </section>
<citcontext>
<prevsection>
<prevsent>when wordnet (fellbaum, 1998) was created, both iaa and subjective confidence measurements serv edas an informal feedback to lexicographers (fell baum et al (1998), p. 200).
</prevsent>
<prevsent>in general, wordnet has been considered resource too fine-grainedfor most annotations (and applications).
</prevsent>
</prevsection>
<citsent citstr=" P06-1014 ">
navigli (2006) <papid> P06-1014 </papid>developed method of reducing the granularity of wordnet by mapping the synsets to senses in more coarse-grained dictionary.</citsent>
<aftsection>
<nextsent>amanual, more coarse-grained grouping of wordnet senses has been performed in ontonotes (weischedel et al 2011).
</nextsent>
<nextsent>the ontonotes 90 % solution (hovy et al 2006) <papid> N06-2015 </papid>actually means such degree of granularity that enables 90-%-iaa.</nextsent>
<nextsent>ontonotes is reaction to the traditionally poor iaa in wordnet annotated corpora, caused by thehigh granularity of senses.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2525">
<title id=" E12-1085.xml">managing uncertainty in semantic tagging </title>
<section> approaches to semantic tagging.  </section>
<citcontext>
<prevsection>
<prevsent>was not found to be sufficiently well-defined to be work able basic unit of meaning (p. 116).
</prevsent>
<prevsent>on the other hand, even non-experts seem to agree reasonably well when judging the similarity of use of word in different contexts (rumshisky et al 2009).
</prevsent>
</prevsection>
<citsent citstr=" P09-1002 ">
erk et al(2009) <papid> P09-1002 </papid>showed promising annotation results with scheme that allowed the annotators graded judgments of similarity between two words or between word and its definition.</citsent>
<aftsection>
<nextsent>verbs are the most challenging part of speech.we see two major causes: vagueness and coercion.
</nextsent>
<nextsent>we neglect ambiguity, since it has proved to be rare in our experience.
</nextsent>
<nextsent>cpa and pdev our current work focuses on english verbs.it has been inspired by the manual corpus pattern analysis method (cpa) (hanks, forthcoming) and its implementation, the pattern dictionary of english verbs (pdev) (hanks and pustejovsky, 2005).
</nextsent>
<nextsent>pdev is semantic concordance built on yet different principle than framenet,wordnet, propbank or ontonotes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2528">
<title id=" E12-1085.xml">managing uncertainty in semantic tagging </title>
<section> 10 0.054 </section>
<citcontext>
<prevsection>
<prevsent>the problem is that annotators often cannot consistently make the fine-grained distinctions proposed by trained lexicographers, which is particularly serious for verbs, because verbs generally tend to be polyse mous rather than homonymous.
</prevsent>
<prevsent>a few approaches have been suggested in the literature that address the problem of the fine-grained semantic distinctions by (automatic) measuring sense distinguishability.
</prevsent>
</prevsection>
<citsent citstr=" P04-1039 ">
diab (2004)<papid> P04-1039 </papid>computes sense perplexity using the entropy function as characteristic of training data.</citsent>
<aftsection>
<nextsent>she also compares the sense distributions to obtain sense distributional correlation, which can serve as avery good direct indicator of performance ratio?, especially together with sense context conf usability (another indicator observed in the training data).
</nextsent>
<nextsent>resnik and yarowsky (1999) introduced the communicative/semantic distance between the predicted sense and the correct?
</nextsent>
<nextsent>sense.then they use it for evaluation metric that provides partial credit for incorrectly classified instances.
</nextsent>
<nextsent>cohn (2003) introduces the concept of (non-uniform) misclassification costs.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2529">
<title id=" E09-1066.xml">syntactic and semantic kernels for short text pair categorization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>from these studies, it emerges that when the categorization task is linguistically complex, syntax and semantics may play relevant role.
</prevsent>
<prevsent>in this perspective, the study of the automatic detection of relationships between short texts is particularly interesting.
</prevsent>
</prevsection>
<citsent citstr=" W07-1401 ">
typical examples of such relations are given in (giampiccolo et al, 2007) <papid> W07-1401 </papid>or those holding between question and answer, e.g.</citsent>
<aftsection>
<nextsent>(hovy et al., 2002; <papid> C02-1042 </papid>punyakanok et al, 2004; lin and katz, 2003), i.e. if text fragment correctly responds to question.</nextsent>
<nextsent>in question answering, the latter problem is mostly tackled by using different heuristics and classifiers, which aim at extracting the best answers (chen et al, 2006; <papid> P06-1136 </papid>collins-thompson et al., 2004).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2530">
<title id=" E09-1066.xml">syntactic and semantic kernels for short text pair categorization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this perspective, the study of the automatic detection of relationships between short texts is particularly interesting.
</prevsent>
<prevsent>typical examples of such relations are given in (giampiccolo et al, 2007) <papid> W07-1401 </papid>or those holding between question and answer, e.g.</prevsent>
</prevsection>
<citsent citstr=" C02-1042 ">
(hovy et al., 2002; <papid> C02-1042 </papid>punyakanok et al, 2004; lin and katz, 2003), i.e. if text fragment correctly responds to question.</citsent>
<aftsection>
<nextsent>in question answering, the latter problem is mostly tackled by using different heuristics and classifiers, which aim at extracting the best answers (chen et al, 2006; <papid> P06-1136 </papid>collins-thompson et al., 2004).</nextsent>
<nextsent>however, for definitional questions, amore effective approach would be to test if correct relationship between the answer and the query holds.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2531">
<title id=" E09-1066.xml">syntactic and semantic kernels for short text pair categorization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>typical examples of such relations are given in (giampiccolo et al, 2007) <papid> W07-1401 </papid>or those holding between question and answer, e.g.</prevsent>
<prevsent>(hovy et al., 2002; <papid> C02-1042 </papid>punyakanok et al, 2004; lin and katz, 2003), i.e. if text fragment correctly responds to question.</prevsent>
</prevsection>
<citsent citstr=" P06-1136 ">
in question answering, the latter problem is mostly tackled by using different heuristics and classifiers, which aim at extracting the best answers (chen et al, 2006; <papid> P06-1136 </papid>collins-thompson et al., 2004).</citsent>
<aftsection>
<nextsent>however, for definitional questions, amore effective approach would be to test if correct relationship between the answer and the query holds.
</nextsent>
<nextsent>this, in turns, depends on the structure ofthe two text fragments.
</nextsent>
<nextsent>designing language models to capture such relation is too complex since probabilistic models suffer from (i) computational complexity issues, e.g. for the processing of large bayesian networks, (ii) problems ineffectively estimating and smoothing probabilities and (iii) high sensitive ness to irrelevant features and processing errors.
</nextsent>
<nextsent>in contrast, discriminative models such as support vector machines (svms) have theoretically been shown to be robust to noise and irrelevant features (vapnik, 1995).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2532">
<title id=" E09-1066.xml">syntactic and semantic kernels for short text pair categorization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>moreover, such learning approach supports the use of kernel methods which allow for an efficient and effective representation of structured data.
</prevsent>
<prevsent>svms and kernel methods have recently been applied to natural language tasks with promising results, e.g.
</prevsent>
</prevsection>
<citsent citstr=" P02-1034 ">
(collins and duffy, 2002; <papid> P02-1034 </papid>kudo and matsumoto, 2003; <papid> P03-1004 </papid>cumby and roth, 2003; shen et al, 2003; <papid> W03-1012 </papid>moschitti and bejan, 2004; culotta and sorensen, 2004; <papid> P04-1054 </papid>kudo et al, 2005; <papid> P05-1024 </papid>toutanova et al, 2004; <papid> W04-3222 </papid>kazama and torisawa, 2005; <papid> H05-1018 </papid>zhang et al, 2006; <papid> N06-1037 </papid>moschitti et al, 2006).<papid> W06-2909 </papid></citsent>
<aftsection>
<nextsent>in particular, in question classification, tree kernels, e.g.
</nextsent>
<nextsent>(zhang and lee, 2003), have shown accuracy comparable to the best models, e.g.
</nextsent>
<nextsent>(li and roth, 2005).
</nextsent>
<nextsent>moreover, (shen and lapata, 2007; <papid> D07-1002 </papid>moschitti et al, 2007; <papid> P07-1098 </papid>surdeanu et al, 2008; <papid> P08-1082 </papid>chali and joty, 5762008) have shown that shallow semantic information in the form of predicate argument structures (pass) (jackendoff, 1990; johnson and fillmore,2000) <papid> A00-2008 </papid>improves the automatic detection of correct answers to target question.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2533">
<title id=" E09-1066.xml">syntactic and semantic kernels for short text pair categorization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>moreover, such learning approach supports the use of kernel methods which allow for an efficient and effective representation of structured data.
</prevsent>
<prevsent>svms and kernel methods have recently been applied to natural language tasks with promising results, e.g.
</prevsent>
</prevsection>
<citsent citstr=" P03-1004 ">
(collins and duffy, 2002; <papid> P02-1034 </papid>kudo and matsumoto, 2003; <papid> P03-1004 </papid>cumby and roth, 2003; shen et al, 2003; <papid> W03-1012 </papid>moschitti and bejan, 2004; culotta and sorensen, 2004; <papid> P04-1054 </papid>kudo et al, 2005; <papid> P05-1024 </papid>toutanova et al, 2004; <papid> W04-3222 </papid>kazama and torisawa, 2005; <papid> H05-1018 </papid>zhang et al, 2006; <papid> N06-1037 </papid>moschitti et al, 2006).<papid> W06-2909 </papid></citsent>
<aftsection>
<nextsent>in particular, in question classification, tree kernels, e.g.
</nextsent>
<nextsent>(zhang and lee, 2003), have shown accuracy comparable to the best models, e.g.
</nextsent>
<nextsent>(li and roth, 2005).
</nextsent>
<nextsent>moreover, (shen and lapata, 2007; <papid> D07-1002 </papid>moschitti et al, 2007; <papid> P07-1098 </papid>surdeanu et al, 2008; <papid> P08-1082 </papid>chali and joty, 5762008) have shown that shallow semantic information in the form of predicate argument structures (pass) (jackendoff, 1990; johnson and fillmore,2000) <papid> A00-2008 </papid>improves the automatic detection of correct answers to target question.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2534">
<title id=" E09-1066.xml">syntactic and semantic kernels for short text pair categorization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>moreover, such learning approach supports the use of kernel methods which allow for an efficient and effective representation of structured data.
</prevsent>
<prevsent>svms and kernel methods have recently been applied to natural language tasks with promising results, e.g.
</prevsent>
</prevsection>
<citsent citstr=" W03-1012 ">
(collins and duffy, 2002; <papid> P02-1034 </papid>kudo and matsumoto, 2003; <papid> P03-1004 </papid>cumby and roth, 2003; shen et al, 2003; <papid> W03-1012 </papid>moschitti and bejan, 2004; culotta and sorensen, 2004; <papid> P04-1054 </papid>kudo et al, 2005; <papid> P05-1024 </papid>toutanova et al, 2004; <papid> W04-3222 </papid>kazama and torisawa, 2005; <papid> H05-1018 </papid>zhang et al, 2006; <papid> N06-1037 </papid>moschitti et al, 2006).<papid> W06-2909 </papid></citsent>
<aftsection>
<nextsent>in particular, in question classification, tree kernels, e.g.
</nextsent>
<nextsent>(zhang and lee, 2003), have shown accuracy comparable to the best models, e.g.
</nextsent>
<nextsent>(li and roth, 2005).
</nextsent>
<nextsent>moreover, (shen and lapata, 2007; <papid> D07-1002 </papid>moschitti et al, 2007; <papid> P07-1098 </papid>surdeanu et al, 2008; <papid> P08-1082 </papid>chali and joty, 5762008) have shown that shallow semantic information in the form of predicate argument structures (pass) (jackendoff, 1990; johnson and fillmore,2000) <papid> A00-2008 </papid>improves the automatic detection of correct answers to target question.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2535">
<title id=" E09-1066.xml">syntactic and semantic kernels for short text pair categorization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>moreover, such learning approach supports the use of kernel methods which allow for an efficient and effective representation of structured data.
</prevsent>
<prevsent>svms and kernel methods have recently been applied to natural language tasks with promising results, e.g.
</prevsent>
</prevsection>
<citsent citstr=" P04-1054 ">
(collins and duffy, 2002; <papid> P02-1034 </papid>kudo and matsumoto, 2003; <papid> P03-1004 </papid>cumby and roth, 2003; shen et al, 2003; <papid> W03-1012 </papid>moschitti and bejan, 2004; culotta and sorensen, 2004; <papid> P04-1054 </papid>kudo et al, 2005; <papid> P05-1024 </papid>toutanova et al, 2004; <papid> W04-3222 </papid>kazama and torisawa, 2005; <papid> H05-1018 </papid>zhang et al, 2006; <papid> N06-1037 </papid>moschitti et al, 2006).<papid> W06-2909 </papid></citsent>
<aftsection>
<nextsent>in particular, in question classification, tree kernels, e.g.
</nextsent>
<nextsent>(zhang and lee, 2003), have shown accuracy comparable to the best models, e.g.
</nextsent>
<nextsent>(li and roth, 2005).
</nextsent>
<nextsent>moreover, (shen and lapata, 2007; <papid> D07-1002 </papid>moschitti et al, 2007; <papid> P07-1098 </papid>surdeanu et al, 2008; <papid> P08-1082 </papid>chali and joty, 5762008) have shown that shallow semantic information in the form of predicate argument structures (pass) (jackendoff, 1990; johnson and fillmore,2000) <papid> A00-2008 </papid>improves the automatic detection of correct answers to target question.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2536">
<title id=" E09-1066.xml">syntactic and semantic kernels for short text pair categorization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>moreover, such learning approach supports the use of kernel methods which allow for an efficient and effective representation of structured data.
</prevsent>
<prevsent>svms and kernel methods have recently been applied to natural language tasks with promising results, e.g.
</prevsent>
</prevsection>
<citsent citstr=" P05-1024 ">
(collins and duffy, 2002; <papid> P02-1034 </papid>kudo and matsumoto, 2003; <papid> P03-1004 </papid>cumby and roth, 2003; shen et al, 2003; <papid> W03-1012 </papid>moschitti and bejan, 2004; culotta and sorensen, 2004; <papid> P04-1054 </papid>kudo et al, 2005; <papid> P05-1024 </papid>toutanova et al, 2004; <papid> W04-3222 </papid>kazama and torisawa, 2005; <papid> H05-1018 </papid>zhang et al, 2006; <papid> N06-1037 </papid>moschitti et al, 2006).<papid> W06-2909 </papid></citsent>
<aftsection>
<nextsent>in particular, in question classification, tree kernels, e.g.
</nextsent>
<nextsent>(zhang and lee, 2003), have shown accuracy comparable to the best models, e.g.
</nextsent>
<nextsent>(li and roth, 2005).
</nextsent>
<nextsent>moreover, (shen and lapata, 2007; <papid> D07-1002 </papid>moschitti et al, 2007; <papid> P07-1098 </papid>surdeanu et al, 2008; <papid> P08-1082 </papid>chali and joty, 5762008) have shown that shallow semantic information in the form of predicate argument structures (pass) (jackendoff, 1990; johnson and fillmore,2000) <papid> A00-2008 </papid>improves the automatic detection of correct answers to target question.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2537">
<title id=" E09-1066.xml">syntactic and semantic kernels for short text pair categorization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>moreover, such learning approach supports the use of kernel methods which allow for an efficient and effective representation of structured data.
</prevsent>
<prevsent>svms and kernel methods have recently been applied to natural language tasks with promising results, e.g.
</prevsent>
</prevsection>
<citsent citstr=" W04-3222 ">
(collins and duffy, 2002; <papid> P02-1034 </papid>kudo and matsumoto, 2003; <papid> P03-1004 </papid>cumby and roth, 2003; shen et al, 2003; <papid> W03-1012 </papid>moschitti and bejan, 2004; culotta and sorensen, 2004; <papid> P04-1054 </papid>kudo et al, 2005; <papid> P05-1024 </papid>toutanova et al, 2004; <papid> W04-3222 </papid>kazama and torisawa, 2005; <papid> H05-1018 </papid>zhang et al, 2006; <papid> N06-1037 </papid>moschitti et al, 2006).<papid> W06-2909 </papid></citsent>
<aftsection>
<nextsent>in particular, in question classification, tree kernels, e.g.
</nextsent>
<nextsent>(zhang and lee, 2003), have shown accuracy comparable to the best models, e.g.
</nextsent>
<nextsent>(li and roth, 2005).
</nextsent>
<nextsent>moreover, (shen and lapata, 2007; <papid> D07-1002 </papid>moschitti et al, 2007; <papid> P07-1098 </papid>surdeanu et al, 2008; <papid> P08-1082 </papid>chali and joty, 5762008) have shown that shallow semantic information in the form of predicate argument structures (pass) (jackendoff, 1990; johnson and fillmore,2000) <papid> A00-2008 </papid>improves the automatic detection of correct answers to target question.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2538">
<title id=" E09-1066.xml">syntactic and semantic kernels for short text pair categorization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>moreover, such learning approach supports the use of kernel methods which allow for an efficient and effective representation of structured data.
</prevsent>
<prevsent>svms and kernel methods have recently been applied to natural language tasks with promising results, e.g.
</prevsent>
</prevsection>
<citsent citstr=" H05-1018 ">
(collins and duffy, 2002; <papid> P02-1034 </papid>kudo and matsumoto, 2003; <papid> P03-1004 </papid>cumby and roth, 2003; shen et al, 2003; <papid> W03-1012 </papid>moschitti and bejan, 2004; culotta and sorensen, 2004; <papid> P04-1054 </papid>kudo et al, 2005; <papid> P05-1024 </papid>toutanova et al, 2004; <papid> W04-3222 </papid>kazama and torisawa, 2005; <papid> H05-1018 </papid>zhang et al, 2006; <papid> N06-1037 </papid>moschitti et al, 2006).<papid> W06-2909 </papid></citsent>
<aftsection>
<nextsent>in particular, in question classification, tree kernels, e.g.
</nextsent>
<nextsent>(zhang and lee, 2003), have shown accuracy comparable to the best models, e.g.
</nextsent>
<nextsent>(li and roth, 2005).
</nextsent>
<nextsent>moreover, (shen and lapata, 2007; <papid> D07-1002 </papid>moschitti et al, 2007; <papid> P07-1098 </papid>surdeanu et al, 2008; <papid> P08-1082 </papid>chali and joty, 5762008) have shown that shallow semantic information in the form of predicate argument structures (pass) (jackendoff, 1990; johnson and fillmore,2000) <papid> A00-2008 </papid>improves the automatic detection of correct answers to target question.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2539">
<title id=" E09-1066.xml">syntactic and semantic kernels for short text pair categorization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>moreover, such learning approach supports the use of kernel methods which allow for an efficient and effective representation of structured data.
</prevsent>
<prevsent>svms and kernel methods have recently been applied to natural language tasks with promising results, e.g.
</prevsent>
</prevsection>
<citsent citstr=" N06-1037 ">
(collins and duffy, 2002; <papid> P02-1034 </papid>kudo and matsumoto, 2003; <papid> P03-1004 </papid>cumby and roth, 2003; shen et al, 2003; <papid> W03-1012 </papid>moschitti and bejan, 2004; culotta and sorensen, 2004; <papid> P04-1054 </papid>kudo et al, 2005; <papid> P05-1024 </papid>toutanova et al, 2004; <papid> W04-3222 </papid>kazama and torisawa, 2005; <papid> H05-1018 </papid>zhang et al, 2006; <papid> N06-1037 </papid>moschitti et al, 2006).<papid> W06-2909 </papid></citsent>
<aftsection>
<nextsent>in particular, in question classification, tree kernels, e.g.
</nextsent>
<nextsent>(zhang and lee, 2003), have shown accuracy comparable to the best models, e.g.
</nextsent>
<nextsent>(li and roth, 2005).
</nextsent>
<nextsent>moreover, (shen and lapata, 2007; <papid> D07-1002 </papid>moschitti et al, 2007; <papid> P07-1098 </papid>surdeanu et al, 2008; <papid> P08-1082 </papid>chali and joty, 5762008) have shown that shallow semantic information in the form of predicate argument structures (pass) (jackendoff, 1990; johnson and fillmore,2000) <papid> A00-2008 </papid>improves the automatic detection of correct answers to target question.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2540">
<title id=" E09-1066.xml">syntactic and semantic kernels for short text pair categorization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>moreover, such learning approach supports the use of kernel methods which allow for an efficient and effective representation of structured data.
</prevsent>
<prevsent>svms and kernel methods have recently been applied to natural language tasks with promising results, e.g.
</prevsent>
</prevsection>
<citsent citstr=" W06-2909 ">
(collins and duffy, 2002; <papid> P02-1034 </papid>kudo and matsumoto, 2003; <papid> P03-1004 </papid>cumby and roth, 2003; shen et al, 2003; <papid> W03-1012 </papid>moschitti and bejan, 2004; culotta and sorensen, 2004; <papid> P04-1054 </papid>kudo et al, 2005; <papid> P05-1024 </papid>toutanova et al, 2004; <papid> W04-3222 </papid>kazama and torisawa, 2005; <papid> H05-1018 </papid>zhang et al, 2006; <papid> N06-1037 </papid>moschitti et al, 2006).<papid> W06-2909 </papid></citsent>
<aftsection>
<nextsent>in particular, in question classification, tree kernels, e.g.
</nextsent>
<nextsent>(zhang and lee, 2003), have shown accuracy comparable to the best models, e.g.
</nextsent>
<nextsent>(li and roth, 2005).
</nextsent>
<nextsent>moreover, (shen and lapata, 2007; <papid> D07-1002 </papid>moschitti et al, 2007; <papid> P07-1098 </papid>surdeanu et al, 2008; <papid> P08-1082 </papid>chali and joty, 5762008) have shown that shallow semantic information in the form of predicate argument structures (pass) (jackendoff, 1990; johnson and fillmore,2000) <papid> A00-2008 </papid>improves the automatic detection of correct answers to target question.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2541">
<title id=" E09-1066.xml">syntactic and semantic kernels for short text pair categorization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(zhang and lee, 2003), have shown accuracy comparable to the best models, e.g.
</prevsent>
<prevsent>(li and roth, 2005).
</prevsent>
</prevsection>
<citsent citstr=" D07-1002 ">
moreover, (shen and lapata, 2007; <papid> D07-1002 </papid>moschitti et al, 2007; <papid> P07-1098 </papid>surdeanu et al, 2008; <papid> P08-1082 </papid>chali and joty, 5762008) have shown that shallow semantic information in the form of predicate argument structures (pass) (jackendoff, 1990; johnson and fillmore,2000) <papid> A00-2008 </papid>improves the automatic detection of correct answers to target question.</citsent>
<aftsection>
<nextsent>in particular,in (moschitti et al, 2007) <papid> P07-1098 </papid>kernels for the processing of pass (in propbank1 format (kingsbury and palmer, 2002)) extracted from question/answer pairs were proposed.</nextsent>
<nextsent>however, the relatively high kernel computational complexity and the limited improvement on bag-of-words (bow) produced by this approach do not make the use of such technique practical for real world applications.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2543">
<title id=" E09-1066.xml">syntactic and semantic kernels for short text pair categorization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(zhang and lee, 2003), have shown accuracy comparable to the best models, e.g.
</prevsent>
<prevsent>(li and roth, 2005).
</prevsent>
</prevsection>
<citsent citstr=" P07-1098 ">
moreover, (shen and lapata, 2007; <papid> D07-1002 </papid>moschitti et al, 2007; <papid> P07-1098 </papid>surdeanu et al, 2008; <papid> P08-1082 </papid>chali and joty, 5762008) have shown that shallow semantic information in the form of predicate argument structures (pass) (jackendoff, 1990; johnson and fillmore,2000) <papid> A00-2008 </papid>improves the automatic detection of correct answers to target question.</citsent>
<aftsection>
<nextsent>in particular,in (moschitti et al, 2007) <papid> P07-1098 </papid>kernels for the processing of pass (in propbank1 format (kingsbury and palmer, 2002)) extracted from question/answer pairs were proposed.</nextsent>
<nextsent>however, the relatively high kernel computational complexity and the limited improvement on bag-of-words (bow) produced by this approach do not make the use of such technique practical for real world applications.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2545">
<title id=" E09-1066.xml">syntactic and semantic kernels for short text pair categorization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(zhang and lee, 2003), have shown accuracy comparable to the best models, e.g.
</prevsent>
<prevsent>(li and roth, 2005).
</prevsent>
</prevsection>
<citsent citstr=" P08-1082 ">
moreover, (shen and lapata, 2007; <papid> D07-1002 </papid>moschitti et al, 2007; <papid> P07-1098 </papid>surdeanu et al, 2008; <papid> P08-1082 </papid>chali and joty, 5762008) have shown that shallow semantic information in the form of predicate argument structures (pass) (jackendoff, 1990; johnson and fillmore,2000) <papid> A00-2008 </papid>improves the automatic detection of correct answers to target question.</citsent>
<aftsection>
<nextsent>in particular,in (moschitti et al, 2007) <papid> P07-1098 </papid>kernels for the processing of pass (in propbank1 format (kingsbury and palmer, 2002)) extracted from question/answer pairs were proposed.</nextsent>
<nextsent>however, the relatively high kernel computational complexity and the limited improvement on bag-of-words (bow) produced by this approach do not make the use of such technique practical for real world applications.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2547">
<title id=" E09-1066.xml">syntactic and semantic kernels for short text pair categorization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(zhang and lee, 2003), have shown accuracy comparable to the best models, e.g.
</prevsent>
<prevsent>(li and roth, 2005).
</prevsent>
</prevsection>
<citsent citstr=" A00-2008 ">
moreover, (shen and lapata, 2007; <papid> D07-1002 </papid>moschitti et al, 2007; <papid> P07-1098 </papid>surdeanu et al, 2008; <papid> P08-1082 </papid>chali and joty, 5762008) have shown that shallow semantic information in the form of predicate argument structures (pass) (jackendoff, 1990; johnson and fillmore,2000) <papid> A00-2008 </papid>improves the automatic detection of correct answers to target question.</citsent>
<aftsection>
<nextsent>in particular,in (moschitti et al, 2007) <papid> P07-1098 </papid>kernels for the processing of pass (in propbank1 format (kingsbury and palmer, 2002)) extracted from question/answer pairs were proposed.</nextsent>
<nextsent>however, the relatively high kernel computational complexity and the limited improvement on bag-of-words (bow) produced by this approach do not make the use of such technique practical for real world applications.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2560">
<title id=" E09-1066.xml">syntactic and semantic kernels for short text pair categorization </title>
<section> shallow semantic kernels.  </section>
<citcontext>
<prevsection>
<prevsent>the extraction of semantic representations from text is very complex task.
</prevsent>
<prevsent>for it, traditionally used models are based on lexical similarity and tends to neglect lexical dependencies.
</prevsent>
</prevsection>
<citsent citstr=" P08-2029 ">
recently, work such as (shen and lapata, 2007; <papid> D07-1002 </papid>surdeanu et al, 2008; <papid> P08-1082 </papid>moschitti et al, 2007; <papid> P07-1098 </papid>moschitti and quarteroni, 2008; <papid> P08-2029 </papid>chali and joty, 2008), <papid> P08-2003 </papid>uses pasto consider such dependencies but only the latter three researches attempt to completely exploit pas with shallow semantic tree kernels(sstks).</citsent>
<aftsection>
<nextsent>unfortunately, these kernels result computational expensive for real world applications.
</nextsent>
<nextsent>in the remainder of this section, we present ournew kernel for pass and compare it with the previous sstk.
</nextsent>
<nextsent>pas a1 disorder rel characterize a0 fear (a) pas r-a0 that rel causes a1 anxiety (b) figure 2: predicate argument structure trees associated with the sentence: panic disorder is characterized by unexpected and intense fear that causes anxiety.?.
</nextsent>
<nextsent>pas rel characterize a0 fear pas rel characterize pas a1 rel a0 pas a1 rel characterize pas rel characterize a0 figure 3: some of the tree substructures useful to capture shallow semantic properties.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2561">
<title id=" E09-1066.xml">syntactic and semantic kernels for short text pair categorization </title>
<section> shallow semantic kernels.  </section>
<citcontext>
<prevsection>
<prevsent>the extraction of semantic representations from text is very complex task.
</prevsent>
<prevsent>for it, traditionally used models are based on lexical similarity and tends to neglect lexical dependencies.
</prevsent>
</prevsection>
<citsent citstr=" P08-2003 ">
recently, work such as (shen and lapata, 2007; <papid> D07-1002 </papid>surdeanu et al, 2008; <papid> P08-1082 </papid>moschitti et al, 2007; <papid> P07-1098 </papid>moschitti and quarteroni, 2008; <papid> P08-2029 </papid>chali and joty, 2008), <papid> P08-2003 </papid>uses pasto consider such dependencies but only the latter three researches attempt to completely exploit pas with shallow semantic tree kernels(sstks).</citsent>
<aftsection>
<nextsent>unfortunately, these kernels result computational expensive for real world applications.
</nextsent>
<nextsent>in the remainder of this section, we present ournew kernel for pass and compare it with the previous sstk.
</nextsent>
<nextsent>pas a1 disorder rel characterize a0 fear (a) pas r-a0 that rel causes a1 anxiety (b) figure 2: predicate argument structure trees associated with the sentence: panic disorder is characterized by unexpected and intense fear that causes anxiety.?.
</nextsent>
<nextsent>pas rel characterize a0 fear pas rel characterize pas a1 rel a0 pas a1 rel characterize pas rel characterize a0 figure 3: some of the tree substructures useful to capture shallow semantic properties.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2565">
<title id=" E09-1066.xml">syntactic and semantic kernels for short text pair categorization </title>
<section> shallow semantic kernels.  </section>
<citcontext>
<prevsection>
<prevsent>3.4 srk vs. sstk.
</prevsent>
<prevsent>a comparison between sstk and srk suggests the following points: first, although the computational complexity of srk is larger than the one of sstk, we will show in the experiment section that the running time (for both training and test ing) is much lower.
</prevsent>
</prevsection>
<citsent citstr=" E06-1015 ">
the worse case is not really informative since as shown in (moschitti, 2006), <papid> E06-1015 </papid>we can design fast algorithm with linear average running time (we use such algorithm for sstk).</citsent>
<aftsection>
<nextsent>second, although srk uses trees with only three levels, in eq.1, the function ?
</nextsent>
<nextsent>(defined togive 1 or 0 if the heads match or not) can be substituted by any kernel function.
</nextsent>
<nextsent>thus, ? can recursively be an srk (and evaluate nested pass(moschitti et al, 2007)) <papid> P07-1098 </papid>or any other potential kernel (over the arguments).</nextsent>
<nextsent>the very interesting aspect is that the efficient algorithm that we provide (eqs 2, 3 and 4) can be accordingly modified to efficiently evaluate new kernels obtained with the ? substitution2.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2569">
<title id=" E09-1066.xml">syntactic and semantic kernels for short text pair categorization </title>
<section> conclusion.  </section>
<citcontext>
<prevsection>
<prevsent>the results on two different question/answerclassification corpora suggest that (a) srk for processing pass is more efficient and effective than previous models, (b) kernels based on pas, pos tag sequences and syntactic parse trees improve on bow on both datasets and (c) on the trec data the improvement is remarkably high, e.g. about 61%.
</prevsent>
<prevsent>promising future work concerns the definition of kernel on the entire argument information (e.g. by means of lexical similarity between all the words of two arguments) and the design of discourse kernel to exploit the relational information gathered from different sentence pairs.
</prevsent>
</prevsection>
<citsent citstr=" P06-1051 ">
a closer relationship between questions and answers can be exploited with models presented in (moschitti and zanzotto, 2007; zanzotto and moschitti, 2006).<papid> P06-1051 </papid></citsent>
<aftsection>
<nextsent>also the use of pas derived from framenet and propbank (giuglea and moschitti, 2006) <papid> P06-1117 </papid>appears to be an interesting research line.</nextsent>
<nextsent>acknowledgments would like to thank silvia quarteroni for her work on extracting linguistic structures.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2571">
<title id=" E09-1066.xml">syntactic and semantic kernels for short text pair categorization </title>
<section> conclusion.  </section>
<citcontext>
<prevsection>
<prevsent>promising future work concerns the definition of kernel on the entire argument information (e.g. by means of lexical similarity between all the words of two arguments) and the design of discourse kernel to exploit the relational information gathered from different sentence pairs.
</prevsent>
<prevsent>a closer relationship between questions and answers can be exploited with models presented in (moschitti and zanzotto, 2007; zanzotto and moschitti, 2006).<papid> P06-1051 </papid></prevsent>
</prevsection>
<citsent citstr=" P06-1117 ">
also the use of pas derived from framenet and propbank (giuglea and moschitti, 2006) <papid> P06-1117 </papid>appears to be an interesting research line.</citsent>
<aftsection>
<nextsent>acknowledgments would like to thank silvia quarteroni for her work on extracting linguistic structures.
</nextsent>
<nextsent>this work has been partially supported by the european commission - luna project, contract n. 33549.
</nextsent>
<nextsent>583
</nextsent>

</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2573">
<title id=" E09-1076.xml">flexible answer typing with discriminative preference ranking </title>
<section> methods.  </section>
<citcontext>
<prevsection>
<prevsent>because our partial ordering does not attempt to rank candidates for one question in relation to candidates for another.moreover, no rank constraints are generated between a(i,j) and a(i,k) nor b(i,j) and b(i,k) because the partial ordering does not include orderings between two candidates of the same class.
</prevsent>
<prevsent>given two appropriate candidates to the question what city hosted the 1988 winter olympics??, new york and calgary, rank constraints will not be created for the pair (new york, calgary).
</prevsent>
</prevsection>
<citsent citstr=" E06-1050 ">
we begin with the work of pinchak and lin (2006) <papid> E06-1050 </papid>in which question contexts (dependency tree paths involving the wh-word) are extracted from the question and matched against those found in corpus of text.</citsent>
<aftsection>
<nextsent>the basic idea is that words that are appropriate as answers will appear in place of thewh-word in these contexts when found in the corpus.
</nextsent>
<nextsent>for example, the question what city hosted the 1988 winter olympics??
</nextsent>
<nextsent>will have as one of the question contexts hosted olympics.?
</nextsent>
<nextsent>wethen consult corpus to discover what replacements for were actually mentioned and smooth the resulting distribution.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2732">
<title id=" E09-1076.xml">flexible answer typing with discriminative preference ranking </title>
<section> prior work.  </section>
<citcontext>
<prevsection>
<prevsent>those same muc types arethen assigned by named-entity tagger to identify appropriate candidate answers.
</prevsent>
<prevsent>because of the potential for unanticipated types, ittycheriah et al (2000), (2001) include phrase type as catch-allclass that is used when no other class is appropriate.
</prevsent>
</prevsection>
<citsent citstr=" C02-1150 ">
although the classifier and named-entity tagger are shown to be among the components with figure 4: precision-recall of appropriate (rbf kernel) 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 recall 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 r c s o correctness model appropriateness model combined model reduced model the lowest error rate in their qa system, it is not clear how much benefit is obtained from using relatively coarse-grained set of classes.the approach of li and roth (2002) <papid> C02-1150 </papid>is similar in that it uses learning for answer type de tection.</citsent>
<aftsection>
<nextsent>they make use of multi-class learning with sparse network of winnows (snow) and two-layer class hierarchy comprising total of fifty possible answer types.
</nextsent>
<nextsent>these finer-grained classes are of more use when computing notion of appropriateness, although one major drawback is that no entity tagger is discussed that can identify these types in text.
</nextsent>
<nextsent>li and roth (2002) <papid> C02-1150 </papid>also relyon rigid set of classes and so run the risk of encountering new question of an unseen type.</nextsent>
<nextsent>pinchak and lin (2006) <papid> E06-1050 </papid>present an alternative in which the probability of term being appropriate to question is computed directly.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2765">
<title id=" E09-1076.xml">flexible answer typing with discriminative preference ranking </title>
<section> prior work.  </section>
<citcontext>
<prevsection>
<prevsent>although learning is applied in many qatasks, very few qa systems rely solely on learning.
</prevsent>
<prevsent>compositional approaches, in which multiple distinct qa techniques are combined, also show promise for improving qa performance.
</prevsent>
</prevsection>
<citsent citstr=" P08-1082 ">
echihabi et al (2003) use three separate answer extraction agents and combine the output scores with maximum entropy re-ranker.surdeanu et al (2008) <papid> P08-1082 </papid>explore preference ranking for advice or how to?</citsent>
<aftsection>
<nextsent>questions in which unique correct answer is preferred over all other candidates.
</nextsent>
<nextsent>their focus is on complex-answer questions in addition to the use of collection of user-generated answers rather than answer typing.
</nextsent>
<nextsent>however, their use of preference ranking mirrors the techniques we describe here in which the relative difference between two candidates at different ranks is more important than the individual candidates.
</nextsent>
<nextsent>we have introduced means of flexible answer typing with discriminative preference rank learning.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2766">
<title id=" E09-1004.xml">contextual phrase level polarity analysis using lexical affect scoring and syntactic ngrams </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>as in earlier approaches, we also use lexicon to set priors, but we explore new uses of dictionary of affect in language (dal) (whissel, 1989) extended using wordnet (fellbaum, 1998).
</prevsent>
<prevsent>we augment this approach with n-gram analysis to capture the effect of context.
</prevsent>
</prevsection>
<citsent citstr=" H05-1044 ">
we present system for classification of neutral versus positive versus negative and positive versus negative polarity (as is also done by (wilson et al,2005)).<papid> H05-1044 </papid></citsent>
<aftsection>
<nextsent>our approach is novel in the use of following features:?
</nextsent>
<nextsent>lexical scores derived from daland extended through wordnet: the dictionary of affect has been widely used to aid in interpretation of emotion in speech (hirschberg 1we assign polarity to phrases based on wiebe (wiebe et al., 2005); the polarity of all examples shown here is drawn from ann notations in the mpqa corpus.
</nextsent>
<nextsent>clearly the assignment of polarity chosen in this corpus depends on general cultural norms.
</nextsent>
<nextsent>24et al, 2005).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2767">
<title id=" E09-1004.xml">contextual phrase level polarity analysis using lexical affect scoring and syntactic ngrams </title>
<section> literature survey.  </section>
<citcontext>
<prevsection>
<prevsent>experimental set-up and results are presented in section 6.
</prevsent>
<prevsent>we conclude with section 7 where we also look at future directions for this research.
</prevsent>
</prevsection>
<citsent citstr=" P02-1053 ">
the task of sentiment analysis has evolved from document level analysis (e.g., (turney., 2002);(<papid> P02-1053 </papid>pang and lee, 2004)) <papid> P04-1035 </papid>to sentence level analysis (e.g., (hu and liu., 2004); (kim and hovy., 2004); (<papid> C04-1200 </papid>yu and hatzivassiloglou, 2003)).<papid> W03-1017 </papid></citsent>
<aftsection>
<nextsent>these researchers first set priors on words using prior polarity lexicon.
</nextsent>
<nextsent>when classifying sentiment at the sentence level, other types of clues are also used, including averaging of word polarities or models for learning sentence sentiment.
</nextsent>
<nextsent>research on contextual phrasal level sentiment analysis was pioneered by nasukawa and yi (2003), who used manually developed patterns to identify sentiment.
</nextsent>
<nextsent>their approach had high precision, but low recall.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2768">
<title id=" E09-1004.xml">contextual phrase level polarity analysis using lexical affect scoring and syntactic ngrams </title>
<section> literature survey.  </section>
<citcontext>
<prevsection>
<prevsent>experimental set-up and results are presented in section 6.
</prevsent>
<prevsent>we conclude with section 7 where we also look at future directions for this research.
</prevsent>
</prevsection>
<citsent citstr=" P04-1035 ">
the task of sentiment analysis has evolved from document level analysis (e.g., (turney., 2002);(<papid> P02-1053 </papid>pang and lee, 2004)) <papid> P04-1035 </papid>to sentence level analysis (e.g., (hu and liu., 2004); (kim and hovy., 2004); (<papid> C04-1200 </papid>yu and hatzivassiloglou, 2003)).<papid> W03-1017 </papid></citsent>
<aftsection>
<nextsent>these researchers first set priors on words using prior polarity lexicon.
</nextsent>
<nextsent>when classifying sentiment at the sentence level, other types of clues are also used, including averaging of word polarities or models for learning sentence sentiment.
</nextsent>
<nextsent>research on contextual phrasal level sentiment analysis was pioneered by nasukawa and yi (2003), who used manually developed patterns to identify sentiment.
</nextsent>
<nextsent>their approach had high precision, but low recall.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2769">
<title id=" E09-1004.xml">contextual phrase level polarity analysis using lexical affect scoring and syntactic ngrams </title>
<section> literature survey.  </section>
<citcontext>
<prevsection>
<prevsent>experimental set-up and results are presented in section 6.
</prevsent>
<prevsent>we conclude with section 7 where we also look at future directions for this research.
</prevsent>
</prevsection>
<citsent citstr=" C04-1200 ">
the task of sentiment analysis has evolved from document level analysis (e.g., (turney., 2002);(<papid> P02-1053 </papid>pang and lee, 2004)) <papid> P04-1035 </papid>to sentence level analysis (e.g., (hu and liu., 2004); (kim and hovy., 2004); (<papid> C04-1200 </papid>yu and hatzivassiloglou, 2003)).<papid> W03-1017 </papid></citsent>
<aftsection>
<nextsent>these researchers first set priors on words using prior polarity lexicon.
</nextsent>
<nextsent>when classifying sentiment at the sentence level, other types of clues are also used, including averaging of word polarities or models for learning sentence sentiment.
</nextsent>
<nextsent>research on contextual phrasal level sentiment analysis was pioneered by nasukawa and yi (2003), who used manually developed patterns to identify sentiment.
</nextsent>
<nextsent>their approach had high precision, but low recall.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2770">
<title id=" E09-1004.xml">contextual phrase level polarity analysis using lexical affect scoring and syntactic ngrams </title>
<section> literature survey.  </section>
<citcontext>
<prevsection>
<prevsent>experimental set-up and results are presented in section 6.
</prevsent>
<prevsent>we conclude with section 7 where we also look at future directions for this research.
</prevsent>
</prevsection>
<citsent citstr=" W03-1017 ">
the task of sentiment analysis has evolved from document level analysis (e.g., (turney., 2002);(<papid> P02-1053 </papid>pang and lee, 2004)) <papid> P04-1035 </papid>to sentence level analysis (e.g., (hu and liu., 2004); (kim and hovy., 2004); (<papid> C04-1200 </papid>yu and hatzivassiloglou, 2003)).<papid> W03-1017 </papid></citsent>
<aftsection>
<nextsent>these researchers first set priors on words using prior polarity lexicon.
</nextsent>
<nextsent>when classifying sentiment at the sentence level, other types of clues are also used, including averaging of word polarities or models for learning sentence sentiment.
</nextsent>
<nextsent>research on contextual phrasal level sentiment analysis was pioneered by nasukawa and yi (2003), who used manually developed patterns to identify sentiment.
</nextsent>
<nextsent>their approach had high precision, but low recall.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2773">
<title id=" E09-1004.xml">contextual phrase level polarity analysis using lexical affect scoring and syntactic ngrams </title>
<section> literature survey.  </section>
<citcontext>
<prevsection>
<prevsent>wilson et al, (2005) <papid> H05-1044 </papid>also explore contextual phrasal level sentiment analysis, using machine learning approach that is closer to the one we present.</prevsent>
<prevsent>both of these researchers also follow the traditional approach and first set priors on words using prior polarity lexicon.</prevsent>
</prevsection>
<citsent citstr=" W03-1014 ">
wil sonet al (2005) <papid> H05-1044 </papid>use lexicon of over 8000 subjectivity clues, gathered from three sources ((riloffand wiebe, 2003); (<papid> W03-1014 </papid>hatzivassiloglou and mckeown, 1997) <papid> P97-1023 </papid>and the general inquirer2).</citsent>
<aftsection>
<nextsent>words that were not tagged as positive or negative were manually labeled.
</nextsent>
<nextsent>yi et al (2003) acquired words from gi, daland wordnet.
</nextsent>
<nextsent>from dal, only words whose pleasant ness score is one standard deviation away from the mean were used.
</nextsent>
<nextsent>nasukawa as well as other researchers (kamps and marx, 2002)) also manually tag words with prior polarities.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2774">
<title id=" E09-1004.xml">contextual phrase level polarity analysis using lexical affect scoring and syntactic ngrams </title>
<section> literature survey.  </section>
<citcontext>
<prevsection>
<prevsent>wilson et al, (2005) <papid> H05-1044 </papid>also explore contextual phrasal level sentiment analysis, using machine learning approach that is closer to the one we present.</prevsent>
<prevsent>both of these researchers also follow the traditional approach and first set priors on words using prior polarity lexicon.</prevsent>
</prevsection>
<citsent citstr=" P97-1023 ">
wil sonet al (2005) <papid> H05-1044 </papid>use lexicon of over 8000 subjectivity clues, gathered from three sources ((riloffand wiebe, 2003); (<papid> W03-1014 </papid>hatzivassiloglou and mckeown, 1997) <papid> P97-1023 </papid>and the general inquirer2).</citsent>
<aftsection>
<nextsent>words that were not tagged as positive or negative were manually labeled.
</nextsent>
<nextsent>yi et al (2003) acquired words from gi, daland wordnet.
</nextsent>
<nextsent>from dal, only words whose pleasant ness score is one standard deviation away from the mean were used.
</nextsent>
<nextsent>nasukawa as well as other researchers (kamps and marx, 2002)) also manually tag words with prior polarities.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2778">
<title id=" E09-1004.xml">contextual phrase level polarity analysis using lexical affect scoring and syntactic ngrams </title>
<section> the polarity classification framework.  </section>
<citcontext>
<prevsection>
<prevsent>therefore, we decided to back off to only binary n-gram features as part of our feature vector.
</prevsent>
<prevsent>3we use the stanford tagger to assign parts of speech tags to sentences.
</prevsent>
</prevsection>
<citsent citstr=" W00-1308 ">
(toutanova and manning, 2000) <papid> W00-1308 </papid>5.4 syntactic features.</citsent>
<aftsection>
<nextsent>in this section, we show how we can combine thedal scores with syntactic constituents.
</nextsent>
<nextsent>this process involves two steps.
</nextsent>
<nextsent>first, we chunk each sentence to its syntactic constituents (np, vp, pp, jjp, and other) using crf chunker.4 if the marked-up subjective phrase does not contain complete chunks (i.e., it partially overlaps with other chunks), we expand the subjective phrase to include the chunks that it overlaps with.
</nextsent>
<nextsent>we term this expanded phrase as the target phrase, see figure 1.second, each chunk in sentence is then assigned 2-d ae space score as defined by cowie et al, (2001) by adding the individual ae space scores of all the words in the chunk and then normalizing it by the number of words.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2779">
<title id=" E09-1004.xml">contextual phrase level polarity analysis using lexical affect scoring and syntactic ngrams </title>
<section> conclusion and future work.  </section>
<citcontext>
<prevsection>
<prevsent>and just?, which mostly occur as preposition and an adverb, respectively.
</prevsent>
<prevsent>also, in our state machine, we havent accounted for the impact of connectives such as but?
</prevsent>
</prevsection>
<citsent citstr=" C90-3018 ">
or although?;we propose drawing on work in argumentative orientation to do so ((anscombre and ducrot, 1983); (elhadad and mckeown, 1990)).<papid> C90-3018 </papid></citsent>
<aftsection>
<nextsent>for future work, it would be interesting to do subjectivity and intensity classification using the same scheme and features.
</nextsent>
<nextsent>particularly, for the task of subjectivity analysis, we speculate that the imagery score might be useful for tagging chunks with subjective?
</nextsent>
<nextsent>and objective?
</nextsent>
<nextsent>instead of positive, negative, and neutral.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2780">
<title id=" E12-2012.xml">maltoptimizer an optimization tool for malt parser </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>from this perspective, components that relyon machine learning have an advantage, since they can be quickly adapted to new settings provided that we can find suitable training data.
</prevsent>
<prevsent>however, such components may require careful feature selection and parameter tuning in order togive optimal performance, task that can be difficult for application developers without specialized knowledge of each component.
</prevsent>
</prevsection>
<citsent citstr=" W06-2920 ">
a typical example is malt parser (nivre et al 2006), widely used transition-based dependency parser with state-of-the-art performance for many languages, as demonstrated in the conll shared tasks on multilingual dependency parsing (buchholz and marsi, 2006; <papid> W06-2920 </papid>nivre et al 2007).<papid> D07-1096 </papid></citsent>
<aftsection>
<nextsent>malt parser is an open-source system that offers wide range of parameters for optimization.
</nextsent>
<nextsent>it implements nine different transition-based parsing algorithms, each with its own specific parameters, and it has an expressive specification language that allows the user to define arbitrarily complex feature models.
</nextsent>
<nextsent>finally, any combination of parsing algorithm and feature model can be combined with number of different machine learning algorithms available in libsvm (chang and lin, 2001) and liblinear (fan et al 2008).
</nextsent>
<nextsent>just running the system with default settings when training new parser is therefore very likely to result in sub optimal performance.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2781">
<title id=" E12-2012.xml">maltoptimizer an optimization tool for malt parser </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>from this perspective, components that relyon machine learning have an advantage, since they can be quickly adapted to new settings provided that we can find suitable training data.
</prevsent>
<prevsent>however, such components may require careful feature selection and parameter tuning in order togive optimal performance, task that can be difficult for application developers without specialized knowledge of each component.
</prevsent>
</prevsection>
<citsent citstr=" D07-1096 ">
a typical example is malt parser (nivre et al 2006), widely used transition-based dependency parser with state-of-the-art performance for many languages, as demonstrated in the conll shared tasks on multilingual dependency parsing (buchholz and marsi, 2006; <papid> W06-2920 </papid>nivre et al 2007).<papid> D07-1096 </papid></citsent>
<aftsection>
<nextsent>malt parser is an open-source system that offers wide range of parameters for optimization.
</nextsent>
<nextsent>it implements nine different transition-based parsing algorithms, each with its own specific parameters, and it has an expressive specification language that allows the user to define arbitrarily complex feature models.
</nextsent>
<nextsent>finally, any combination of parsing algorithm and feature model can be combined with number of different machine learning algorithms available in libsvm (chang and lin, 2001) and liblinear (fan et al 2008).
</nextsent>
<nextsent>just running the system with default settings when training new parser is therefore very likely to result in sub optimal performance.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2782">
<title id=" E12-2012.xml">maltoptimizer an optimization tool for malt parser </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this way, we hope to cater for users 58 without specific knowledge of malt parser, who can use the tool for black box optimization, as well as expert users, who can use it interactively to speed up optimization.
</prevsent>
<prevsent>experiments on number of datasets show that using maltoptimizer for completely automatic optimization gives consistent and often substantial improvements over the default settings for maltparser.the importance of feature selection and parameter optimization has been demonstrated for many nlp tasks (kool et al 2000; daelemans et al 2003), and there are general optimization tools for machine learning, such as paramsearch (van den bosch, 2004).
</prevsent>
</prevsection>
<citsent citstr=" C10-1093 ">
in addition, nilsson and nugues (2010) <papid> C10-1093 </papid>has explored automatic feature selection specifically for malt parser, but maltoptimizer isthe first system that implements complete customized optimization process for this system.in the rest of the paper, we describe the optimization process implemented in maltoptimizer(section 2), report experiments (section 3), out line the demonstration (section 4), and conclude(section 5).</citsent>
<aftsection>
<nextsent>a more detailed description of malt optimizer with additional experimental results can be found in ballesteros and nivre (2012).
</nextsent>
<nextsent>maltoptimizer is written in java and implements an optimization procedure for malt parser based on the heuristics described in nivre and hall (2010).
</nextsent>
<nextsent>the system takes as input training set, consisting of sentences annotated with dependency trees in conll data format,1 and outputs an optimized malt parser configuration together with an estimate of the final parsing accuracy.the evaluation metric that is used for optimization by default is the labeled attachment score(las) excluding punctuation, that is, the percentage of non-punctuation tokens that are assigned the correct head and the correct label (buchholz and marsi, 2006), <papid> W06-2920 </papid>but other options are available.for efficiency reasons, maltoptimizer only explores linear multiclass svms in liblinear.</nextsent>
<nextsent>2.1 phase 1: data analysis.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2784">
<title id=" E12-2012.xml">maltoptimizer an optimization tool for malt parser </title>
<section> the maltoptimizer system.  </section>
<citcontext>
<prevsection>
<prevsent>the user is given the opportunity to edit this option file and may also choose to stop the process and continue with manual optimization.
</prevsent>
<prevsent>2.2 phase 2: parsing algorithm selection.
</prevsent>
</prevsection>
<citsent citstr=" J08-4003 ">
malt parser implements three groups of transition based parsing algorithms:3 (i) nivres algorithms(nivre, 2003; nivre, 2008), <papid> J08-4003 </papid>ii) covingtons algorithms (covington, 2001; nivre, 2008), <papid> J08-4003 </papid>and (iii)3recent versions of malt parser contains additional algorithms that are currently not handled by maltoptimizer.</citsent>
<aftsection>
<nextsent>59 figure 1: decision tree for best projective algorithm.figure 2: decision tree for best non-projective algorithm (+pp for pseudo-projective parsing).
</nextsent>
<nextsent>stack algorithms (nivre, 2009; <papid> P09-1040 </papid>nivre et al 2009) <papid> W09-3811 </papid>both the covington group and the stack group contain algorithms that can handle non-projective dependency trees, and any projective algorithm can be combined with pseudo-projective parsing to recover non-projective dependencies in postprocessing (nivre and nilsson, 2005).<papid> P05-1013 </papid></nextsent>
<nextsent>in phase 2, maltoptimizer explores the parsing algorithms implemented in malt parser, based on the data characteristics inferred in the first phase.in particular, if there are no non-projective dependencies in the training set, then only projective algorithms are explored, including the arc-eager and arc-standard versions of nivres algorithm, the projective version of covingtons projective parsing algorithm and the projective stack algorithm.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2787">
<title id=" E12-2012.xml">maltoptimizer an optimization tool for malt parser </title>
<section> the maltoptimizer system.  </section>
<citcontext>
<prevsection>
<prevsent>malt parser implements three groups of transition based parsing algorithms:3 (i) nivres algorithms(nivre, 2003; nivre, 2008), <papid> J08-4003 </papid>ii) covingtons algorithms (covington, 2001; nivre, 2008), <papid> J08-4003 </papid>and (iii)3recent versions of malt parser contains additional algorithms that are currently not handled by maltoptimizer.</prevsent>
<prevsent>59 figure 1: decision tree for best projective algorithm.figure 2: decision tree for best non-projective algorithm (+pp for pseudo-projective parsing).</prevsent>
</prevsection>
<citsent citstr=" P09-1040 ">
stack algorithms (nivre, 2009; <papid> P09-1040 </papid>nivre et al 2009) <papid> W09-3811 </papid>both the covington group and the stack group contain algorithms that can handle non-projective dependency trees, and any projective algorithm can be combined with pseudo-projective parsing to recover non-projective dependencies in postprocessing (nivre and nilsson, 2005).<papid> P05-1013 </papid></citsent>
<aftsection>
<nextsent>in phase 2, maltoptimizer explores the parsing algorithms implemented in malt parser, based on the data characteristics inferred in the first phase.in particular, if there are no non-projective dependencies in the training set, then only projective algorithms are explored, including the arc-eager and arc-standard versions of nivres algorithm, the projective version of covingtons projective parsing algorithm and the projective stack algorithm.
</nextsent>
<nextsent>the system follows decision tree considering the characteristics of each algorithm, which is shown in figure 1.on the other hand, if the training set contains substantial amount of non-projective dependencies, maltoptimizer instead tests the non projective versions of covingtons algorithm and the stack algorithm (including lazy and an eager variant), and projective algorithms in combination with pseudo-projective parsing.
</nextsent>
<nextsent>the system then follows the decision tree shown in figure 2.if the number of trees containing non projective arcs is small but not zero, the system tests both projective algorithms and non projective algorithms, following the decision trees in figure 1 and figure 2 and picking the algorithm that gives the best results after traversing both.
</nextsent>
<nextsent>once the system has finished testing each of the algorithms with default settings, maltoptimizertunes some specific parameters of the best performing algorithm and creates new option file for the best configuration so far.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2788">
<title id=" E12-2012.xml">maltoptimizer an optimization tool for malt parser </title>
<section> the maltoptimizer system.  </section>
<citcontext>
<prevsection>
<prevsent>malt parser implements three groups of transition based parsing algorithms:3 (i) nivres algorithms(nivre, 2003; nivre, 2008), <papid> J08-4003 </papid>ii) covingtons algorithms (covington, 2001; nivre, 2008), <papid> J08-4003 </papid>and (iii)3recent versions of malt parser contains additional algorithms that are currently not handled by maltoptimizer.</prevsent>
<prevsent>59 figure 1: decision tree for best projective algorithm.figure 2: decision tree for best non-projective algorithm (+pp for pseudo-projective parsing).</prevsent>
</prevsection>
<citsent citstr=" W09-3811 ">
stack algorithms (nivre, 2009; <papid> P09-1040 </papid>nivre et al 2009) <papid> W09-3811 </papid>both the covington group and the stack group contain algorithms that can handle non-projective dependency trees, and any projective algorithm can be combined with pseudo-projective parsing to recover non-projective dependencies in postprocessing (nivre and nilsson, 2005).<papid> P05-1013 </papid></citsent>
<aftsection>
<nextsent>in phase 2, maltoptimizer explores the parsing algorithms implemented in malt parser, based on the data characteristics inferred in the first phase.in particular, if there are no non-projective dependencies in the training set, then only projective algorithms are explored, including the arc-eager and arc-standard versions of nivres algorithm, the projective version of covingtons projective parsing algorithm and the projective stack algorithm.
</nextsent>
<nextsent>the system follows decision tree considering the characteristics of each algorithm, which is shown in figure 1.on the other hand, if the training set contains substantial amount of non-projective dependencies, maltoptimizer instead tests the non projective versions of covingtons algorithm and the stack algorithm (including lazy and an eager variant), and projective algorithms in combination with pseudo-projective parsing.
</nextsent>
<nextsent>the system then follows the decision tree shown in figure 2.if the number of trees containing non projective arcs is small but not zero, the system tests both projective algorithms and non projective algorithms, following the decision trees in figure 1 and figure 2 and picking the algorithm that gives the best results after traversing both.
</nextsent>
<nextsent>once the system has finished testing each of the algorithms with default settings, maltoptimizertunes some specific parameters of the best performing algorithm and creates new option file for the best configuration so far.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2789">
<title id=" E12-2012.xml">maltoptimizer an optimization tool for malt parser </title>
<section> the maltoptimizer system.  </section>
<citcontext>
<prevsection>
<prevsent>malt parser implements three groups of transition based parsing algorithms:3 (i) nivres algorithms(nivre, 2003; nivre, 2008), <papid> J08-4003 </papid>ii) covingtons algorithms (covington, 2001; nivre, 2008), <papid> J08-4003 </papid>and (iii)3recent versions of malt parser contains additional algorithms that are currently not handled by maltoptimizer.</prevsent>
<prevsent>59 figure 1: decision tree for best projective algorithm.figure 2: decision tree for best non-projective algorithm (+pp for pseudo-projective parsing).</prevsent>
</prevsection>
<citsent citstr=" P05-1013 ">
stack algorithms (nivre, 2009; <papid> P09-1040 </papid>nivre et al 2009) <papid> W09-3811 </papid>both the covington group and the stack group contain algorithms that can handle non-projective dependency trees, and any projective algorithm can be combined with pseudo-projective parsing to recover non-projective dependencies in postprocessing (nivre and nilsson, 2005).<papid> P05-1013 </papid></citsent>
<aftsection>
<nextsent>in phase 2, maltoptimizer explores the parsing algorithms implemented in malt parser, based on the data characteristics inferred in the first phase.in particular, if there are no non-projective dependencies in the training set, then only projective algorithms are explored, including the arc-eager and arc-standard versions of nivres algorithm, the projective version of covingtons projective parsing algorithm and the projective stack algorithm.
</nextsent>
<nextsent>the system follows decision tree considering the characteristics of each algorithm, which is shown in figure 1.on the other hand, if the training set contains substantial amount of non-projective dependencies, maltoptimizer instead tests the non projective versions of covingtons algorithm and the stack algorithm (including lazy and an eager variant), and projective algorithms in combination with pseudo-projective parsing.
</nextsent>
<nextsent>the system then follows the decision tree shown in figure 2.if the number of trees containing non projective arcs is small but not zero, the system tests both projective algorithms and non projective algorithms, following the decision trees in figure 1 and figure 2 and picking the algorithm that gives the best results after traversing both.
</nextsent>
<nextsent>once the system has finished testing each of the algorithms with default settings, maltoptimizertunes some specific parameters of the best performing algorithm and creates new option file for the best configuration so far.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2795">
<title id=" E09-3010.xml">aligning medical domain ontologies for clinical query extraction </title>
<section> evaluation strategies.  </section>
<citcontext>
<prevsection>
<prevsent>resulting alignments to reference alignments (absolute comparison) or to each other (relative comparison).
</prevsent>
<prevsent>21http://oaei.ontologymatching.org 6.2 clinical query extraction.
</prevsent>
</prevsection>
<citsent citstr=" W08-0625 ">
we conceive of the clinical query extraction process as use case that shows the benefits of semantic integration by means of ontology align-ments.clinical query extraction, (oezden wenner-berg et al, 2008; buitelaar et al, 2008) <papid> W08-0625 </papid>is the process of predicting patterns for typical clinical queries given domain ontologies and corpora.</citsent>
<aftsection>
<nextsent>it is motivated by the fact that when developing search systems for healthcare professionals, it is necessary to know what kind of information they search for in their daily working tasks.
</nextsent>
<nextsent>as inter-views with clinicians are not always possible, alternative solutions become necessary to obtain this information.
</nextsent>
<nextsent>clinical query extraction is technique to semi-automatically predict possible clinical que-ries without having to depend on clinical inter-views.
</nextsent>
<nextsent>it requires domain corpora (i.e. disease, anatomy and radiology) and domain ontologies to be able to process statistically most relevant concepts in the ontologies and the relations that hold between them.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2796">
<title id=" E09-3010.xml">aligning medical domain ontologies for clinical query extraction </title>
<section> evaluation strategies.  </section>
<citcontext>
<prevsection>
<prevsent>it requires domain corpora (i.e. disease, anatomy and radiology) and domain ontologies to be able to process statistically most relevant concepts in the ontologies and the relations that hold between them.
</prevsent>
<prevsent>consequently, concept-relation-concept triplets are identified, for which the assumption is that the statistically most rele-vant triplets are more likely to occur in clinical queries.clinical query extraction can be viewed as special case of term/relation extraction.
</prevsent>
</prevsection>
<citsent citstr=" E99-1003 ">
related approaches from the medical domain are re-ported by bourigault and jacquemin (1999) <papid> E99-1003 </papid>and le moigno et al (2002).the identification of query patterns (i.e. the concept-relation-concept triplets) starts with the construction of domain corpora from related web resources such as wikipedia22 and pub-med23.</citsent>
<aftsection>
<nextsent>as next, use case relevant parts from do-main ontologies are extracted.
</nextsent>
<nextsent>the frequency of the concepts from the extracted sub-ontologies in the domain corpora versus the frequencies in domain independent corpus determines the do-main specificity of the concepts.
</nextsent>
<nextsent>this statistical term/concept profiling can be viewed as function that takes the domain (sub)ontologies and the corpora as input and re-turns the partially weighted domain ontologies as output, where the terms/concepts are ranked ac-cording to their weights.
</nextsent>
<nextsent>an example query pat-tern can look like: 22 http://www.wikipedia.org/23 http://www.ncbi.nlm.nih.gov/pubmed/ 85 [anatomical structure] located_in [anatomical structure] and [[radiology image]modality] is_about [anatomical structure] and [[radiology image]modality] shows_ symptom [disease symptom] the clinical query extraction approach, as il-lustrated so far, builds on using domain ontolo-gies, however on using them independently.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2797">
<title id=" E09-1008.xml">correcting automatic translations through collaborations between mt and monolingual target language users </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>finally, the statistics of user actions may be useful for translation evaluation.
</prevsent>
<prevsent>they may be informative features for developing automatic metrics for sentence-level evaluations (kulesza and shieber, 2004).
</prevsent>
</prevsection>
<citsent citstr=" W00-0507 ">
while there have been many successful computer aided translation systems both for research and as commercial products (bowker, 2002; langlais et al., 2000), <papid> W00-0507 </papid>collaborative translation has not been as widely explored.</citsent>
<aftsection>
<nextsent>previous efforts such as derivtool (deneefe et al, 2005) <papid> P05-3025 </papid>and linear (callison-burch, 2005) placed stronger emphasis on improving mt. they elicited more in-depth interactions between the users and the mt systems phrase tables.</nextsent>
<nextsent>these approaches may be more appropriate for users who are mt researchers themselves.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2798">
<title id=" E09-1008.xml">correcting automatic translations through collaborations between mt and monolingual target language users </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>they may be informative features for developing automatic metrics for sentence-level evaluations (kulesza and shieber, 2004).
</prevsent>
<prevsent>while there have been many successful computer aided translation systems both for research and as commercial products (bowker, 2002; langlais et al., 2000), <papid> W00-0507 </papid>collaborative translation has not been as widely explored.</prevsent>
</prevsection>
<citsent citstr=" P05-3025 ">
previous efforts such as derivtool (deneefe et al, 2005) <papid> P05-3025 </papid>and linear (callison-burch, 2005) placed stronger emphasis on improving mt. they elicited more in-depth interactions between the users and the mt systems phrase tables.</citsent>
<aftsection>
<nextsent>these approaches may be more appropriate for users who are mt researchers themselves.
</nextsent>
<nextsent>in contrast, our approach focuses on providing intuitive visualization of variety of information sources for users who may not be mt savvy.
</nextsent>
<nextsent>by tracking the types of information they consulted, the portions of translations they selected to modify, and the portions of the source 66 table 7: some examples of translations corrected by the participants and their scores.
</nextsent>
<nextsent>score translation mt 0.34 he is being discovered almost hit an arm in the pile of books on the desktop, just like frightened horse as lieju wangbangbian almost pengfan the piano stool.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2799">
<title id=" E09-1008.xml">correcting automatic translations through collaborations between mt and monolingual target language users </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>said bandais spokeswoman kasumi nakanishi.
</prevsent>
<prevsent>text they attempted to understand, we may alter the design of our translation model.
</prevsent>
</prevsection>
<citsent citstr=" H01-1033 ">
our objective is also related to that of cross-language information retrieval (resnik et al, 2001).<papid> H01-1033 </papid></citsent>
<aftsection>
<nextsent>this work can be seen as providing the next step in helping users to gain some understanding of the information in the documents once they are retrieved.
</nextsent>
<nextsent>by facilitating better collaborations between mt and target-language readers, we can naturally increase human annotated data for exploring alternative mt models.
</nextsent>
<nextsent>this form of symbiosis is akin to the paradigm proposed by von ahn and dabbish (2004).
</nextsent>
<nextsent>they designed interactive games in which the player generated data could be used to improve image tagging and other classification tasks (von ahn, 2006).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2800">
<title id=" E12-1010.xml">answer sentence retrieval by matching dependency paths acquired from question answer sentence pairs </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>more recently, significant amount of research is dedicated to query alteration approaches.
</prevsent>
<prevsent>(cui et al 2002), for example, assume that if queries containing one term often result in the selection of documents containing another term, then strong relationship between the two terms exist.
</prevsent>
</prevsection>
<citsent citstr=" J10-3010 ">
in their approach, query terms and document terms are linked via sessions in which users click on documents that are presented as results for the query.(riezler and liu, 2010) <papid> J10-3010 </papid>apply statistical machine translation model to parallel data consisting of user queries and snippets from clicked web documents and in such way extract contextual expansion terms from the query rewrites.we see our work as addressing the same fundamental problem, but shifting focus from queryterm/document term mismatch to mismatches observed between the grammatical structure of natural language queries and relevant text pieces.</citsent>
<aftsection>
<nextsent>in order to achieve this we analyze the queries?
</nextsent>
<nextsent>and the relevant contents?
</nextsent>
<nextsent>syntactic structure by using dependency paths.
</nextsent>
<nextsent>especially in qa there is strong tradition of using dependency structures: (lin and pan tel, 2001) present an unsupervised algorithm to automatically discover inference rules (essentially paraphrases) from text.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2801">
<title id=" E12-1010.xml">answer sentence retrieval by matching dependency paths acquired from question answer sentence pairs </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the candidate whose paths show the highest matching score is selected.
</prevsent>
<prevsent>(shen and klakow, 2006) also describe method that is primarily based on similarity scores between dependency relation pairs.
</prevsent>
</prevsection>
<citsent citstr=" W06-3807 ">
how ever, their algorithm computes the similarity of paths between key phrases, not between words.furthermore, it takes relations in path not as independent from each other, but acknowledges that they form sequence, by comparing two paths with the help of an adaptation of the dynamic time warping algorithm (rabiner et al  1991).(molla, 2006) <papid> W06-3807 </papid>presents an approach for the acquisition of question answering rules by applying graph manipulation methods.</citsent>
<aftsection>
<nextsent>questions are represented as dependency graphs, which are extended with information from answer sentences.these combined graphs can then be used to identify answers.
</nextsent>
<nextsent>finally, in (wang et al  2007), <papid> D07-1003 </papid>quasi-synchronous grammar (smith and eisner,2006) <papid> W06-3104 </papid>is used to model relations between questions and answer sentences.</nextsent>
<nextsent>in this paper we describe an algorithm that learns possible syntactic answer sentence formulations for syntactic question classes from set of example question/answer sentence pairs.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2802">
<title id=" E12-1010.xml">answer sentence retrieval by matching dependency paths acquired from question answer sentence pairs </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>how ever, their algorithm computes the similarity of paths between key phrases, not between words.furthermore, it takes relations in path not as independent from each other, but acknowledges that they form sequence, by comparing two paths with the help of an adaptation of the dynamic time warping algorithm (rabiner et al  1991).(molla, 2006) <papid> W06-3807 </papid>presents an approach for the acquisition of question answering rules by applying graph manipulation methods.</prevsent>
<prevsent>questions are represented as dependency graphs, which are extended with information from answer sentences.these combined graphs can then be used to identify answers.</prevsent>
</prevsection>
<citsent citstr=" D07-1003 ">
finally, in (wang et al  2007), <papid> D07-1003 </papid>quasi-synchronous grammar (smith and eisner,2006) <papid> W06-3104 </papid>is used to model relations between questions and answer sentences.</citsent>
<aftsection>
<nextsent>in this paper we describe an algorithm that learns possible syntactic answer sentence formulations for syntactic question classes from set of example question/answer sentence pairs.
</nextsent>
<nextsent>unlike the related work described above, it acknowledges that a) valid answer sentences syntax might be very different for the questions syntax and b) several valid answer sentence structures, which might be completely independent from each other, can exist for one and the same question.
</nextsent>
<nextsent>to illustrate this consider the question whenwas alaska purchased??
</nextsent>
<nextsent>the following four sentences all answer the given question, but only the first sentence is straightforward reformulation of the question: 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2803">
<title id=" E12-1010.xml">answer sentence retrieval by matching dependency paths acquired from question answer sentence pairs </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>how ever, their algorithm computes the similarity of paths between key phrases, not between words.furthermore, it takes relations in path not as independent from each other, but acknowledges that they form sequence, by comparing two paths with the help of an adaptation of the dynamic time warping algorithm (rabiner et al  1991).(molla, 2006) <papid> W06-3807 </papid>presents an approach for the acquisition of question answering rules by applying graph manipulation methods.</prevsent>
<prevsent>questions are represented as dependency graphs, which are extended with information from answer sentences.these combined graphs can then be used to identify answers.</prevsent>
</prevsection>
<citsent citstr=" W06-3104 ">
finally, in (wang et al  2007), <papid> D07-1003 </papid>quasi-synchronous grammar (smith and eisner,2006) <papid> W06-3104 </papid>is used to model relations between questions and answer sentences.</citsent>
<aftsection>
<nextsent>in this paper we describe an algorithm that learns possible syntactic answer sentence formulations for syntactic question classes from set of example question/answer sentence pairs.
</nextsent>
<nextsent>unlike the related work described above, it acknowledges that a) valid answer sentences syntax might be very different for the questions syntax and b) several valid answer sentence structures, which might be completely independent from each other, can exist for one and the same question.
</nextsent>
<nextsent>to illustrate this consider the question whenwas alaska purchased??
</nextsent>
<nextsent>the following four sentences all answer the given question, but only the first sentence is straightforward reformulation of the question: 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2804">
<title id=" E12-1010.xml">answer sentence retrieval by matching dependency paths acquired from question answer sentence pairs </title>
<section> the acquisition of alaska by the united.  </section>
<citcontext>
<prevsection>
<prevsent>relevant textual content this is piece of text that is relevant to the user query in that it contains the information the user is searching for.
</prevsent>
<prevsent>in this paper, we use sentences extracted from the aquaint corpus (graff, 2002) that contain the answer to the given trec question.in total, the data available to us for our experiments consists of 8,830 question/answer sentence pairs.
</prevsent>
</prevsection>
<citsent citstr=" L08-1307 ">
this data is publicly available, see (kaisser and lowe, 2008).<papid> L08-1307 </papid></citsent>
<aftsection>
<nextsent>the algorithm described in this paper has three main steps:phrase alignment key phrases from the question are paired with phrases from the answer sentences.
</nextsent>
<nextsent>pattern creation the dependency structures of queries and answer sentences are analyzed and patterns are extracted.
</nextsent>
<nextsent>pattern evaluation the patterns discovered in the last step are evaluated and confidence score is assigned to each.
</nextsent>
<nextsent>the acquired patterns can then be used during retrieval, where question is matched against the antecedents describing the syntax of the question.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2805">
<title id=" E12-1010.xml">answer sentence retrieval by matching dependency paths acquired from question answer sentence pairs </title>
<section> the acquisition of alaska by the united.  </section>
<citcontext>
<prevsection>
<prevsent>in our approach, this is two step process.
</prevsent>
<prevsent>first we align on word level, then the output of the word alignment process is used to identify and align phrases.
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
word alignment is important in many fields of nlp, e.g. machine translation (mt) where words in parallel, bilingual corpora need to be aligned, see (och andney, 2003) <papid> J03-1002 </papid>for comparison of various statistical alignment models.</citsent>
<aftsection>
<nextsent>in our case however weare dealing with monolingual alignment problem which enables us to exploit clues not available for bilingual alignment: first of all, we can expect many query words to be present in the answer sentence, either with the exact same surface appearance or in some morphological variant.
</nextsent>
<nextsent>secondly,there are tools available that tell us how semantically related two words are, most notably wordnet (miller et al  1993).
</nextsent>
<nextsent>for these reasons we implemented bespoke alignment strategy, tailored towards our problem description.
</nextsent>
<nextsent>this method is described in detail in (kaisser, 2009).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2806">
<title id=" E12-1010.xml">answer sentence retrieval by matching dependency paths acquired from question answer sentence pairs </title>
<section> pattern creation.  </section>
<citcontext>
<prevsection>
<prevsent>in step 1 and 2 key phrases from the question are aligned to the corresponding phrases in the answer sentence, see section 4 of this paper.
</prevsent>
<prevsent>step 3 is concerned with retrieving the parse tree for the answer sentence.
</prevsent>
</prevsection>
<citsent citstr=" P03-1054 ">
in our implementation all answer sentences in the training set have for performance reasons been parsed beforehand with the stanford parser (klein and manning, 2003<papid> P03-1054 </papid>b; klein and manning, 2003<papid> P03-1054 </papid>a), so at this point they are simply loaded from file.</citsent>
<aftsection>
<nextsent>step 4 is the key step in our algorithm.
</nextsent>
<nextsent>from the previous steps, weknow where the key constituents from the question as well as the answer are located in the answer sentence.
</nextsent>
<nextsent>this enables us to compute the dependency paths in the answer sentences?
</nextsent>
<nextsent>parse tree that connect the answer with the key constituents.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2810">
<title id=" E12-1010.xml">answer sentence retrieval by matching dependency paths acquired from question answer sentence pairs </title>
<section> pattern evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>for each created pattern, at least one matching example must exists: the sentence that was 92 used to create it in the first place.
</prevsent>
<prevsent>however, we do not know how precise each pattern is. to this end, an additional processing step between pattern creation and application is needed: pattern evaluation.
</prevsent>
</prevsection>
<citsent citstr=" P02-1006 ">
similar approaches to ours have been described in the relevant literature, many of them concerned with bootstrapping, starting with (ravichandran and hovy, 2002).<papid> P02-1006 </papid></citsent>
<aftsection>
<nextsent>the general purpose of this step is to use the available data about questions and their correct answers to evaluate how often each created pattern returns correct or an incorrect result.
</nextsent>
<nextsent>this data is stored with each pattern and the result of the equation, often called pattern precision, can be used during retrieval stage.
</nextsent>
<nextsent>pattern precision in our case is defined as: = #correct + 1 #correct + #incorrect + 2 (1)we use lucene to retrieve the top 100 paragraphs from the aquaint corpus by issuing query that consists of the querys keywords andall non-stop words in the answer.
</nextsent>
<nextsent>then, all patterns are loaded whose antecedent matches the query that is currently being processed.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2811">
<title id=" E09-1025.xml">inference rules and their application to recognizing textual entailment </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>textual inference plays an important role in many natural language processing (nlp) tasks.
</prevsent>
<prevsent>in recent years, the recognizing textual entailment (rte) (dagan et al, 2006) challenge, which focuses on detecting semantic inference, has attracted lot of attention.
</prevsent>
</prevsection>
<citsent citstr=" W07-1409 ">
given text (several sentences) and hypothesis (one sentence), the goal is to detect if can be inferred from t. studies such as (clark et al, 2007)<papid> W07-1409 </papid>attest that lexical substitution (e.g. synonyms, antonyms) or simple syntactic variation account for the entailment only in small number of pairs.</citsent>
<aftsection>
<nextsent>thus, one essential issue is to identify more complex expressions which, inappropriate contexts, convey thesame (or similar) meaning.
</nextsent>
<nextsent>however, more generally, we are also interested in pairs of expressions in which only uni-directional inference relation holds1.1we will use the term inference rule to stand for such concept; the two expressions can be actual paraphrases if there lation is bi-directional typical example is the following rte pair in which accelerate to in is used as an alternative formulation for reach speed of in t.t: the high-speed train, scheduled for trial run on tuesday, is able to reach maximum speed of up to 430 kilometers per hour, or 119 meters per second.
</nextsent>
<nextsent>h: the train accelerates to 430 kilometers per hour.
</nextsent>
<nextsent>one way to deal with textual inference is through rule representation, for example wrote ? is author of y. however, manually building collections of inference rules is time-consuming and it is unlikely that humans can exhaustively enumerate all the rules encoding the knowledge needed in reasoning with natural language.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2813">
<title id=" E09-1025.xml">inference rules and their application to recognizing textual entailment </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for the second aspect, we focus on applying these rules to the rte task.
</prevsent>
<prevsent>in particular, we usea structure representation derived from the dependency parse trees of and h, which aims to capture the essential information they convey.
</prevsent>
</prevsection>
<citsent citstr=" P01-1008 ">
the rest of the paper is organized as follows: section 2 introduces the inference rule collection2another line of work on acquiring paraphrases uses comparable corpora, for instance (barzilay and mckeown, 2001), (<papid> P01-1008 </papid>pang et al, 2003) <papid> N03-1024 </papid>211 we use, based on the discovery of inference rules from text (henceforth dirt) algorithm and discusses previous work on applying it to the rtetask.</citsent>
<aftsection>
<nextsent>section 3 focuses on the rule collection itself and on the methods in which we use an external lexical resource to extend and refine it.
</nextsent>
<nextsent>section 4 discusses the application of the rules for the rte data, describing the structure representation we use to identify the appropriate context for the rule application.
</nextsent>
<nextsent>the experimental results will be presented in section 5, followed by an error analysis and discussions in section 6.
</nextsent>
<nextsent>finally section 7 will conclude the paper and point out future work directions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2814">
<title id=" E09-1025.xml">inference rules and their application to recognizing textual entailment </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for the second aspect, we focus on applying these rules to the rte task.
</prevsent>
<prevsent>in particular, we usea structure representation derived from the dependency parse trees of and h, which aims to capture the essential information they convey.
</prevsent>
</prevsection>
<citsent citstr=" N03-1024 ">
the rest of the paper is organized as follows: section 2 introduces the inference rule collection2another line of work on acquiring paraphrases uses comparable corpora, for instance (barzilay and mckeown, 2001), (<papid> P01-1008 </papid>pang et al, 2003) <papid> N03-1024 </papid>211 we use, based on the discovery of inference rules from text (henceforth dirt) algorithm and discusses previous work on applying it to the rtetask.</citsent>
<aftsection>
<nextsent>section 3 focuses on the rule collection itself and on the methods in which we use an external lexical resource to extend and refine it.
</nextsent>
<nextsent>section 4 discusses the application of the rules for the rte data, describing the structure representation we use to identify the appropriate context for the rule application.
</nextsent>
<nextsent>the experimental results will be presented in section 5, followed by an error analysis and discussions in section 6.
</nextsent>
<nextsent>finally section 7 will conclude the paper and point out future work directions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2815">
<title id=" E09-1025.xml">inference rules and their application to recognizing textual entailment </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>the experimental results will be presented in section 5, followed by an error analysis and discussions in section 6.
</prevsent>
<prevsent>finally section 7 will conclude the paper and point out future work directions.
</prevsent>
</prevsection>
<citsent citstr=" W04-3206 ">
a number of automatically acquired inference rule/paraphrase collections are available, such as (szpektor et al, 2004), (<papid> W04-3206 </papid>sekine, 2005).<papid> I05-5011 </papid></citsent>
<aftsection>
<nextsent>in our work we use the dirt collection because it is the largest one available and it has relatively good accuracy (in the 50% range for top generated paraphrases, (szpektor et al, 2007)).<papid> P07-1058 </papid></nextsent>
<nextsent>in this section,we describe the dirt algorithm for acquiring inference rules.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2816">
<title id=" E09-1025.xml">inference rules and their application to recognizing textual entailment </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>the experimental results will be presented in section 5, followed by an error analysis and discussions in section 6.
</prevsent>
<prevsent>finally section 7 will conclude the paper and point out future work directions.
</prevsent>
</prevsection>
<citsent citstr=" I05-5011 ">
a number of automatically acquired inference rule/paraphrase collections are available, such as (szpektor et al, 2004), (<papid> W04-3206 </papid>sekine, 2005).<papid> I05-5011 </papid></citsent>
<aftsection>
<nextsent>in our work we use the dirt collection because it is the largest one available and it has relatively good accuracy (in the 50% range for top generated paraphrases, (szpektor et al, 2007)).<papid> P07-1058 </papid></nextsent>
<nextsent>in this section,we describe the dirt algorithm for acquiring inference rules.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2817">
<title id=" E09-1025.xml">inference rules and their application to recognizing textual entailment </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>finally section 7 will conclude the paper and point out future work directions.
</prevsent>
<prevsent>a number of automatically acquired inference rule/paraphrase collections are available, such as (szpektor et al, 2004), (<papid> W04-3206 </papid>sekine, 2005).<papid> I05-5011 </papid></prevsent>
</prevsection>
<citsent citstr=" P07-1058 ">
in our work we use the dirt collection because it is the largest one available and it has relatively good accuracy (in the 50% range for top generated paraphrases, (szpektor et al, 2007)).<papid> P07-1058 </papid></citsent>
<aftsection>
<nextsent>in this section,we describe the dirt algorithm for acquiring inference rules.
</nextsent>
<nextsent>following that, we will overview the rte systems which take dirt as an external knowledge resource.
</nextsent>
<nextsent>2.1 discovery of inference rules from text.
</nextsent>
<nextsent>the dirt algorithm has been introduced by (lin and pantel, 2001) and it is based on what is called the extended distributional hypothesis.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2819">
<title id=" E09-1025.xml">inference rules and their application to recognizing textual entailment </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>table 1 gives few examples of rules present in dirt4.
</prevsent>
<prevsent>current work on inference rules focuses on making such resources more precise.
</prevsent>
</prevsection>
<citsent citstr=" P08-1078 ">
(basili etal., 2007) and (szpektor et al, 2008) <papid> P08-1078 </papid>propose attaching selectional preferences to inference rules.</citsent>
<aftsection>
<nextsent>these are semantic classes which correspond to the anchor values of an inference rule and have the role of making precise the context in which therule can be applied 5.
</nextsent>
<nextsent>this aspect is very important and we plan to address it in our future work.
</nextsent>
<nextsent>however in this paper we investigate the first and more basic issue: how to successfully use rules in their current form.
</nextsent>
<nextsent>4for simplification, in the rest of the paper we will omit giving the dependency relations in pattern.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2821">
<title id=" E09-1025.xml">inference rules and their application to recognizing textual entailment </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>(iftene and balahur-dobrescu, 2007) use dirtin more relaxed manner.
</prevsent>
<prevsent>a dirt rule is employed in the system if at least one of the anchors match in and h, i.e. they use them as unaryrules.
</prevsent>
</prevsection>
<citsent citstr=" W07-1414 ">
however, the detailed analysis of the system that they provide shows that the dirt component is the least relevant one (adding 0.4% of precision).in (marsi et al, 2007), <papid> W07-1414 </papid>the focus is on the usefulness of dirt.</citsent>
<aftsection>
<nextsent>in their system paraphrase substitution step is added on top of system based on tree alignment algorithm.
</nextsent>
<nextsent>the basic paraphrase substitution method follows three steps.
</nextsent>
<nextsent>initially, the two patterns of rule are matched in and (instantiations of the anchors , do not haveto match).
</nextsent>
<nextsent>the text tree is transformed by applying the paraphrase substitution.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2824">
<title id=" E09-1025.xml">inference rules and their application to recognizing textual entailment </title>
<section> applying dirt on rte.  </section>
<citcontext>
<prevsection>
<prevsent>figure 1 shows the ts of of the following positive example,t for their discovery of ulcer-causing bacteria, australian doctors robin warren and barry marshall have received the 2005 nobel prize in physiology or medicine.
</prevsent>
<prevsent>h robin warren was awarded nobel prize.
</prevsent>
</prevsection>
<citsent citstr=" W06-3907 ">
notice that, in order to match the inference rules with two anchors, the number of the dependency 7see (nairn et al, 2006) <papid> W06-3907 </papid>for detailed analysis of these aspects.</citsent>
<aftsection>
<nextsent>8here we also use minipar for the reason of consistence 215 figure 1: dependency structure of text.
</nextsent>
<nextsent>tree skeleton in bold paths contained in ts should also be two.
</nextsent>
<nextsent>in practice, among all the 800 t-h pairs of the rte 2 test set, we successfully extracted tree skeleton sin 296 text pairs, i.e., 37% of the test data is covered by this step and results on other datasets are similar.
</nextsent>
<nextsent>applying dirt on ts dependency representations like the tree skeleton have been explored by many researchers, e.g.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2825">
<title id=" E09-1025.xml">inference rules and their application to recognizing textual entailment </title>
<section> applying dirt on rte.  </section>
<citcontext>
<prevsection>
<prevsent>in practice, among all the 800 t-h pairs of the rte 2 test set, we successfully extracted tree skeleton sin 296 text pairs, i.e., 37% of the test data is covered by this step and results on other datasets are similar.
</prevsent>
<prevsent>applying dirt on ts dependency representations like the tree skeleton have been explored by many researchers, e.g.
</prevsent>
</prevsection>
<citsent citstr=" P06-1051 ">
(zanzotto and moschitti, 2006) <papid> P06-1051 </papid>have utilized tree kernel method to calculate the similarity between and h, and (wang and neumann, 2007) chose sub sequence kernel to reduce the computational complexity.</citsent>
<aftsection>
<nextsent>however, the focus of this paper is to evaluate the application of inference rules on rte, instead of exploring methods of tackling the task itself.
</nextsent>
<nextsent>therefore, we performed straightforward matching algorithm to apply the inference rules on top of the tree skeleton structure.
</nextsent>
<nextsent>given tree skeletons of and h, we check if the two left dependency paths, the two right ones or the two root nodes contain the patterns of rule.
</nextsent>
<nextsent>in the example above, the rule obj???
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2826">
<title id=" E09-1025.xml">inference rules and their application to recognizing textual entailment </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>this aspect together with an error analysis we performed are the focus of section 7.the last column (dirt+id+wn) gives the precision we obtain if we simply decide pair is true entailment if we have an inference rule matched in it (irrespective of the values of the anchors or of the existence of tree skeletons).
</prevsent>
<prevsent>as expected, only identifying the patterns of rule in pair irrespective of tree skeletons does not give any indication of the entailment value of the pair.9the rte task is considered to be difficult.
</prevsent>
</prevsection>
<citsent citstr=" W07-1401 ">
the average accuracy of the systems in the rte-3 challenge is around 61% (giampiccolo et al, 2007) <papid> W07-1401 </papid>216 rte set dirtts dirt + wnts idts dirt + id + wnts dirt + id + wn rte2 49/69.38 94/67.02 45/66.66 130/65.38 673/50.07 rte3 42/69.04 70/70.00 29/79.31 93/72.05 661/55.06 table 4: coverage/precision with various rule collections rte set bow main rte2 (85 pairs) 51.76% 60.00% rte3 (64 pairs) 54.68% 62.50% table 5: precision on the covered rte data rte set (800 pairs) bow main &amp; bow rte2 56.87% 57.75% rte3 61.12% 61.75% table 6: precision on full rte data 5.2 results on the entire data.</citsent>
<aftsection>
<nextsent>at last, we also integrate our method with bag of words baseline, which calculates the ratio of overlapping words in and h. for the pairs thatour method covers, we overrule the baselines decision.
</nextsent>
<nextsent>the results are shown in table 6 (main stands for the dirt + id + wnts configuration).
</nextsent>
<nextsent>on the full dataset, the improvement is still small due to the low coverage of our method, however on the pairs that are covered by our method (ta ble 5), there is significant improvement over the overlap baseline.
</nextsent>
<nextsent>in this section we take closer look at the data in order to better understand how does our method of combining tree skeletons and inference rules work.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2827">
<title id=" E06-1049.xml">a machine learning approach to extract temporal information from texts in swedish and generate animated 3d scenes </title>
<section> extraction of temporal information.  </section>
<citcontext>
<prevsection>


</prevsection>
<citsent citstr=" W01-1301 ">
and scene visualization carsim is program that generates 3d scenes from narratives describing road accidents (johansson etal., 2005; dupuy et al, 2001).<papid> W01-1301 </papid></citsent>
<aftsection>
<nextsent>it considers authentic texts, generally collected from web sitesof swedish newspapers or transcribed from hand written accounts by victims of accidents.
</nextsent>
<nextsent>one of carsims key features is that it animates the generated scene to visualize events described in the narrative.
</nextsent>
<nextsent>the text below, newspaper article with its translation into english, illustrates the goals and challenges of it.
</nextsent>
<nextsent>we bracketed the entities, time expressions, and events and we annotated them with identifiers, denoted respectively oi, tj , and ek: en {bussolycka}e1 sdra afghanistan krvdee2 {p? torsdagen}t1 {20 ddsoffer}o1 . ytterligare {39 personer}o2 skadadese3 olyckane4.busseno3 {var p?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2828">
<title id=" E06-1049.xml">a machine learning approach to extract temporal information from texts in swedish and generate animated 3d scenes </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>a basic observation is that the temporal order of events is related to their narrative order.
</prevsent>
<prevsent>dowty (1986) investigated it and formulated atemporal discourse interpretation principle to interpret the advance of narrative time in sequence of sentences.
</prevsent>
</prevsection>
<citsent citstr=" E95-1035 ">
lascarides and asher (1993) described complex logical framework to deal with events in simple past and pluperfect sentences.hitzeman et al (1995) <papid> E95-1035 </papid>proposed constraint based approach taking into account tense, aspect,temporal adverbials, and rhetorical structure to analyze discourse.recently, groups have used machine learning techniques to determine temporal relations.they trained automatically classifiers on hand annotated corpora.</citsent>
<aftsection>
<nextsent>mani et al (2003) <papid> N03-2019 </papid>achieved the best results so far by using decision trees toorder partially events of successive clauses in english texts.</nextsent>
<nextsent>boguraev and ando (2005) is another example of it for english and li et al (2004) <papid> P04-1074 </papid>for chinese.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2829">
<title id=" E06-1049.xml">a machine learning approach to extract temporal information from texts in swedish and generate animated 3d scenes </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>dowty (1986) investigated it and formulated atemporal discourse interpretation principle to interpret the advance of narrative time in sequence of sentences.
</prevsent>
<prevsent>lascarides and asher (1993) described complex logical framework to deal with events in simple past and pluperfect sentences.hitzeman et al (1995) <papid> E95-1035 </papid>proposed constraint based approach taking into account tense, aspect,temporal adverbials, and rhetorical structure to analyze discourse.recently, groups have used machine learning techniques to determine temporal relations.they trained automatically classifiers on hand annotated corpora.</prevsent>
</prevsection>
<citsent citstr=" N03-2019 ">
mani et al (2003) <papid> N03-2019 </papid>achieved the best results so far by using decision trees toorder partially events of successive clauses in english texts.</citsent>
<aftsection>
<nextsent>boguraev and ando (2005) is another example of it for english and li et al (2004) <papid> P04-1074 </papid>for chinese.</nextsent>
<nextsent>information several schemes have been proposed to annotate temporal information in texts, see setzer and gaizauskas (2002), inter alia.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2830">
<title id=" E06-1049.xml">a machine learning approach to extract temporal information from texts in swedish and generate animated 3d scenes </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>lascarides and asher (1993) described complex logical framework to deal with events in simple past and pluperfect sentences.hitzeman et al (1995) <papid> E95-1035 </papid>proposed constraint based approach taking into account tense, aspect,temporal adverbials, and rhetorical structure to analyze discourse.recently, groups have used machine learning techniques to determine temporal relations.they trained automatically classifiers on hand annotated corpora.</prevsent>
<prevsent>mani et al (2003) <papid> N03-2019 </papid>achieved the best results so far by using decision trees toorder partially events of successive clauses in english texts.</prevsent>
</prevsection>
<citsent citstr=" P04-1074 ">
boguraev and ando (2005) is another example of it for english and li et al (2004) <papid> P04-1074 </papid>for chinese.</citsent>
<aftsection>
<nextsent>information several schemes have been proposed to annotate temporal information in texts, see setzer and gaizauskas (2002), inter alia.
</nextsent>
<nextsent>many of them were incompatible or incomplete and in an effort to reconcile and unify the field, ingria and pustejovsky (2002) introduced the xml-based time markup language (timeml).
</nextsent>
<nextsent>timeml is specification language whose goal is to capture most aspects of temporal relations between events in discourses.
</nextsent>
<nextsent>it is based on allens (1984) relations and variation ofvendlers (1967) classification of verbs.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2831">
<title id=" E06-1049.xml">a machine learning approach to extract temporal information from texts in swedish and generate animated 3d scenes </title>
<section> event ordering.  </section>
<citcontext>
<prevsection>
<prevsent>we apply each tree to sequences of events where it decides the order between two of the events in each sequence.
</prevsent>
<prevsent>if e1, ..., en are the events in the sequence they appear in the text, the trees correspond to the following functions: fdt1(ei, ei+1) ? trel(ei, ei+1) fdt2(ei, ei+1, ei+2) ? trel(ei, ei+1) fdt3(ei, ei+1, ei+2) ? trel(ei+1, ei+2) fdt4(ei, ei+1, ei+2) ? trel(ei, ei+2) fdt5(ei, ei+1, ei+2, ei+3) ? trel(ei, ei+3)the possible output values of the trees are: simultaneous, after, before, is_included, includes, and none.
</prevsent>
</prevsection>
<citsent citstr=" W01-1311 ">
these values correspond to the relations described by setzer and gaizauskas (2001).<papid> W01-1311 </papid>the first decision tree should capture more general relations between two adjacent events with out the need of context.</citsent>
<aftsection>
<nextsent>decision trees dt2 anddt3 extend the context by one event to the left respectively one event to the right.
</nextsent>
<nextsent>they should capture more specific phenomena.
</nextsent>
<nextsent>however, they are not always applicable as we never apply decision 388 tree when there is time expression between any of the events involved.
</nextsent>
<nextsent>in effect, time expressionsreanchor?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2835">
<title id=" E06-1046.xml">edit machines for robust multimodal language processing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we also present an approach where the edit operations are trained from data usinga noisy channel model paradigm.
</prevsent>
<prevsent>we evaluate and compare the performance of the hand-crafted and learned edit machines in the context of multimodal conversational system (match).
</prevsent>
</prevsection>
<citsent citstr=" P02-1048 ">
over the years, there have been several multimodal systems that allow input and/or output to be conveyed over multiple channels such as speech, graphics, and gesture, for example, put that there (bolt, 1980), cubricon (neal and shapiro, 1991), quick set (cohen et al, 1998), smartkom (wahlster, 2002), match (johnston etal., 2002).<papid> P02-1048 </papid></citsent>
<aftsection>
<nextsent>multimodal integration and interpretation for such interfaces is elegantly expressed using multimodal grammars (johnston and bangalore, 2000).<papid> C00-1054 </papid></nextsent>
<nextsent>these grammars support composite multimodal inputs by aligning speech input (words) and gesture input (represented as sequences of gesture symbols) while expressing the relation between the speech and gesture input and their combined semantic representation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2836">
<title id=" E06-1046.xml">edit machines for robust multimodal language processing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we evaluate and compare the performance of the hand-crafted and learned edit machines in the context of multimodal conversational system (match).
</prevsent>
<prevsent>over the years, there have been several multimodal systems that allow input and/or output to be conveyed over multiple channels such as speech, graphics, and gesture, for example, put that there (bolt, 1980), cubricon (neal and shapiro, 1991), quick set (cohen et al, 1998), smartkom (wahlster, 2002), match (johnston etal., 2002).<papid> P02-1048 </papid></prevsent>
</prevsection>
<citsent citstr=" C00-1054 ">
multimodal integration and interpretation for such interfaces is elegantly expressed using multimodal grammars (johnston and bangalore, 2000).<papid> C00-1054 </papid></citsent>
<aftsection>
<nextsent>these grammars support composite multimodal inputs by aligning speech input (words) and gesture input (represented as sequences of gesture symbols) while expressing the relation between the speech and gesture input and their combined semantic representation.
</nextsent>
<nextsent>in (bangalore and johnston, 2000; johnston and bangalore, 2005), we have shown that such grammar scan be compiled into finite-state transducers enabling effective processing of lattice input from speech and gesture recognition and mutual compensation for errors and ambiguities.however, like other approaches based on handcrafted grammars, multimodal grammars can be brittle with respect to extra-grammatical, erroneous and dis fluent input.
</nextsent>
<nextsent>for speech recognition, corpus-driven stochastic language model (slm)with smoothing or combination of grammar based and   -gram model (bangalore and johnston, 2004; <papid> N04-1005 </papid>wang et al, 2002) can be built in order to overcome the brittleness of grammar-basedlanguage model.</nextsent>
<nextsent>although the corpus-driven language model might recognize users utterance correctly, the recognized utterance may not be assigned semantic representation by the multimodal grammar if the utterance is not part of the grammar.there have been two main approaches to improving robustness of the understanding component in the spoken language understanding litera ture.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2837">
<title id=" E06-1046.xml">edit machines for robust multimodal language processing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these grammars support composite multimodal inputs by aligning speech input (words) and gesture input (represented as sequences of gesture symbols) while expressing the relation between the speech and gesture input and their combined semantic representation.
</prevsent>
<prevsent>in (bangalore and johnston, 2000; johnston and bangalore, 2005), we have shown that such grammar scan be compiled into finite-state transducers enabling effective processing of lattice input from speech and gesture recognition and mutual compensation for errors and ambiguities.however, like other approaches based on handcrafted grammars, multimodal grammars can be brittle with respect to extra-grammatical, erroneous and dis fluent input.
</prevsent>
</prevsection>
<citsent citstr=" N04-1005 ">
for speech recognition, corpus-driven stochastic language model (slm)with smoothing or combination of grammar based and   -gram model (bangalore and johnston, 2004; <papid> N04-1005 </papid>wang et al, 2002) can be built in order to overcome the brittleness of grammar-basedlanguage model.</citsent>
<aftsection>
<nextsent>although the corpus-driven language model might recognize users utterance correctly, the recognized utterance may not be assigned semantic representation by the multimodal grammar if the utterance is not part of the grammar.there have been two main approaches to improving robustness of the understanding component in the spoken language understanding literature.
</nextsent>
<nextsent>first, parsing-based approach attempts to recover partial parses from the parse chart when the input cannot be parsed in its entirety due to noise, in order to construct (partial) semantic representation (dowding et al, 1993; <papid> P93-1008 </papid>allen et al, 2001; ward, 1991).</nextsent>
<nextsent>second, classification-based approach views the problem of understanding as extracting certain bits of information from the input.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2838">
<title id=" E06-1046.xml">edit machines for robust multimodal language processing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for speech recognition, corpus-driven stochastic language model (slm)with smoothing or combination of grammar based and   -gram model (bangalore and johnston, 2004; <papid> N04-1005 </papid>wang et al, 2002) can be built in order to overcome the brittleness of grammar-basedlanguage model.</prevsent>
<prevsent>although the corpus-driven language model might recognize users utterance correctly, the recognized utterance may not be assigned semantic representation by the multimodal grammar if the utterance is not part of the grammar.there have been two main approaches to improving robustness of the understanding component in the spoken language understanding litera ture.</prevsent>
</prevsection>
<citsent citstr=" P93-1008 ">
first, parsing-based approach attempts to recover partial parses from the parse chart when the input cannot be parsed in its entirety due to noise, in order to construct (partial) semantic representation (dowding et al, 1993; <papid> P93-1008 </papid>allen et al, 2001; ward, 1991).</citsent>
<aftsection>
<nextsent>second, classification-based approach views the problem of understanding as extracting certain bits of information from the input.
</nextsent>
<nextsent>it attempts to classify the utterance and identifies sub strings of the input as slot-filler values to construct frame-like semantic representation.
</nextsent>
<nextsent>both approaches have shortcomings.
</nextsent>
<nextsent>although in the first approach, the grammar can encode richer semantic representations, the method for combining the fragmented parses is quite ad hoc.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2839">
<title id=" E06-1046.xml">edit machines for robust multimodal language processing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>above) to the closest language the system can understand(grammar:?
</prevsent>
<prevsent>above).
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
to this end, we adopt techniques from statistical machine translation (brown et al, 1993; <papid> J93-2003 </papid>och and ney, 2003) <papid> J03-1002 </papid>and use statistical alignment to learn the edit patterns.</citsent>
<aftsection>
<nextsent>here we evaluate these different techniques on data from thematchmultimodal conversational system (john ston et al, 2002) <papid> P02-1048 </papid>but the same techniques are more broadly applicable to spoken language systems in general whether uni modal or multimodal.the layout of the paper is as follows.</nextsent>
<nextsent>in sections 2 and 3, we briefly describe the match application and the finite-state approach to multimodal language understanding.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2840">
<title id=" E06-1046.xml">edit machines for robust multimodal language processing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>above) to the closest language the system can understand(grammar:?
</prevsent>
<prevsent>above).
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
to this end, we adopt techniques from statistical machine translation (brown et al, 1993; <papid> J93-2003 </papid>och and ney, 2003) <papid> J03-1002 </papid>and use statistical alignment to learn the edit patterns.</citsent>
<aftsection>
<nextsent>here we evaluate these different techniques on data from thematchmultimodal conversational system (john ston et al, 2002) <papid> P02-1048 </papid>but the same techniques are more broadly applicable to spoken language systems in general whether uni modal or multimodal.the layout of the paper is as follows.</nextsent>
<nextsent>in sections 2 and 3, we briefly describe the match application and the finite-state approach to multimodal language understanding.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2856">
<title id=" E06-1046.xml">edit machines for robust multimodal language processing </title>
<section> robust understanding.  </section>
<citcontext>
<prevsection>
<prevsent>forex ample, in (1), 6 7fi,ffi  is the predicate and !+#8% (9%+*)fi :-/,0+ ,9!12)  3&amp; ,)!8)fi is the set of arguments to the predicate.we determine the predicate ( ;fl  ) for = token multimodal utterance (  @a ) by maximizing the posterior probability as shown in equation 2.
</prevsent>
<prevsent>;  cb d$e9f&ghd;$i k e,l ;nm-  ? ano (2)we view the problem of identifying and extracting arguments from multimodal input as problem of associating each token of the input witha specific tag that encodes the label of the argument and the span of the argument.
</prevsent>
</prevsection>
<citsent citstr=" W95-0107 ">
these tags are drawn from tagset which is constructed by 363 extending each argument label by three additional symbols p,qsrtqvu , following (ramshaw and marcus, 1995).<papid> W95-0107 </papid></citsent>
<aftsection>
<nextsent>these symbols correspond to cases when token is inside ( ) an argument span, out side ( ) an argument span or at the boundary of two argument spans ( ) (see table 1).
</nextsent>
<nextsent>user cheap thai upper west side utterance argument  price  cheap  /price 6 cuisine  annotation thai  /cuisine w place  upper west side  /place  iob cheap price   thai cuisine   encoding upper place   west place   side place  table 1: the i,o,b encoding for argument extraction.
</nextsent>
<nextsent>given this encoding, the problem of extracting the arguments is search for the most likely sequence of tags ( z[  ) given the input multimodal utterance   as shown in equation (3).
</nextsent>
<nextsent>we approximate the posterior probability e,l z\m  ? ao using independence assumptions as shown in equation (4).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2862">
<title id=" E09-1031.xml">tblimproved non deterministic segmentation and pos tagging for a chinese parser </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>compared to the pku tokenizer tagger, we improve segmentation f-score from 94.18% to 96.74%, tagged wordf-score from 84.63% to 92.44%, segmented sentence accuracy from 47.15% to 65.06% and tagged sentence accuracy from 14.07% to 31.47%.
</prevsent>
<prevsent>word segmentation and tagging are the necessary initial steps for almost any language processing system, and chinese parsers are no exception.
</prevsent>
</prevsection>
<citsent citstr=" W03-1719 ">
however, automatic chinese word segmentation and tagging has been recognized as very difficult task (sproat and emerson, 2003), <papid> W03-1719 </papid>for the following reasons: first, chinese text provides few cues for word boundaries (xia, 2000; wu, 2003) and part-of speech (pos) information.</citsent>
<aftsection>
<nextsent>with the exception of punctuation marks, chinese does not have word delimiters such as the whit espace used in english text, and unlike other languages without white spaces such as japanese, chinese lacks morphological inflections that could provide cues for word boundaries and pos information.
</nextsent>
<nextsent>in fact, the lackof word boundary marks and morphological inflection contributes not only to mistakes in machine processing of chinese; it has also been identified as factor for parsing miscues in chinese childrens reading behavior (chang et al, 1992).second, in addition to the two problems described above, segmentation and tagging also suffer from the fact that the notion of word is very unclear in chinese (xu, 1997; packard, 2000; hsu, 2002).
</nextsent>
<nextsent>while the word is an intuitive and salient notion in english, it is by no means clear notion in chinese.
</nextsent>
<nextsent>instead, for historical reasons, the intuitive and clear notion in chinese language and culture is the character rather thanthe word.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2863">
<title id=" E09-1031.xml">tblimproved non deterministic segmentation and pos tagging for a chinese parser </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>3http://www.icl.pku.edu.cn/icl res/critical to overall quality of the systems output.
</prevsent>
<prevsent>however, even though pkus tokenizertagger is one of the state-of-art systems, its performance is not satisfactory for the chinese lfg.
</prevsent>
</prevsection>
<citsent citstr=" C02-1145 ">
this becomes clear from small-scale evaluation in which the system was tested on set of 101 gold sentences chosen from the chinese treebank 5 (ctb5) (xue et al, 2002; <papid> C02-1145 </papid>xue et al, 2005).</citsent>
<aftsection>
<nextsent>these 101 sentences are 10-20 words long and all of them are chosen from xinhua sources 4.based on the deterministic segmentation and tagging results produced by pkus tokenizer-tagger, the chinese lfg can only parse 80 out of the 101 sentences.
</nextsent>
<nextsent>among the 80 sentences that are parsed, 66 received full parses and 14 received fragmented parses.
</nextsent>
<nextsent>among the 21 completely failed sentences, 20 sentences failed due to segmentation and tagging mistakes.
</nextsent>
<nextsent>this simple test shows that in order for the deep chinese grammar to be practically useful, the performance of the tokenizer-tagger must be improved.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2864">
<title id=" E09-1031.xml">tblimproved non deterministic segmentation and pos tagging for a chinese parser </title>
<section> fst5 rules for the improvement of.  </section>
<citcontext>
<prevsection>
<prevsent>since there are large amounts of training data that are close to the segmentation and tagging standard the grammar developer wants to use, the idea of inducing fst rules rather than hand-crafting them comes quite naturally.
</prevsent>
<prevsent>the easiest way to do this is to apply transformation-based learning (tbl) to the combined problem of chinese segmentation and pos tagging, since the cascade of transformational rules learned in tbl training run can straightforwardly be translated into cascade of fst rules.
</prevsent>
</prevsection>
<citsent citstr=" J95-4004 ">
3.2.1 transformation-based learning and ?-tbl tbl is machine learning approach that has been employed to solve number of problems in natural language processing; most famously, it has been used for part-of-speech tagging (brill, 1995).<papid> J95-4004 </papid>tbl is supervised learning approach, since it relies on gold-annotated training data.</citsent>
<aftsection>
<nextsent>in addition, it relies on set of templates of transformationalrules; learning consists in finding sequence of in stantiations of these templates that minimizes the number of errors in more or less naive base-line output with respect to the gold-annotated training data.
</nextsent>
<nextsent>the first attempts to employ tbl to solve the problem of chinese word segmentation go back to palmer (1997) <papid> P97-1041 </papid>and hockenmaier and brew (1998).in more recent work, tbl was used for the adaption of the output of statistical general purpose?</nextsent>
<nextsent>segmenter to standards that vary depending on the application that requires sentence segmentation (gao et al, 2004).<papid> P04-1059 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2865">
<title id=" E09-1031.xml">tblimproved non deterministic segmentation and pos tagging for a chinese parser </title>
<section> fst5 rules for the improvement of.  </section>
<citcontext>
<prevsection>
<prevsent>3.2.1 transformation-based learning and ?-tbl tbl is machine learning approach that has been employed to solve number of problems in natural language processing; most famously, it has been used for part-of-speech tagging (brill, 1995).<papid> J95-4004 </papid>tbl is supervised learning approach, since it relies on gold-annotated training data.</prevsent>
<prevsent>in addition, it relies on set of templates of transformationalrules; learning consists in finding sequence of in stantiations of these templates that minimizes the number of errors in more or less naive base-line output with respect to the gold-annotated training data.</prevsent>
</prevsection>
<citsent citstr=" P97-1041 ">
the first attempts to employ tbl to solve the problem of chinese word segmentation go back to palmer (1997) <papid> P97-1041 </papid>and hockenmaier and brew (1998).in more recent work, tbl was used for the adaption of the output of statistical general purpose?</citsent>
<aftsection>
<nextsent>segmenter to standards that vary depending on the application that requires sentence segmentation (gao et al, 2004).<papid> P04-1059 </papid></nextsent>
<nextsent>tbl approaches tothe combined problem of segmenting and pos tagging chinese sentences are reported in florian and ngai (2001) <papid> W01-0701 </papid>and fung et al (2004).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2866">
<title id=" E09-1031.xml">tblimproved non deterministic segmentation and pos tagging for a chinese parser </title>
<section> fst5 rules for the improvement of.  </section>
<citcontext>
<prevsection>
<prevsent>in addition, it relies on set of templates of transformationalrules; learning consists in finding sequence of in stantiations of these templates that minimizes the number of errors in more or less naive base-line output with respect to the gold-annotated training data.
</prevsent>
<prevsent>the first attempts to employ tbl to solve the problem of chinese word segmentation go back to palmer (1997) <papid> P97-1041 </papid>and hockenmaier and brew (1998).in more recent work, tbl was used for the adaption of the output of statistical general purpose?</prevsent>
</prevsection>
<citsent citstr=" P04-1059 ">
segmenter to standards that vary depending on the application that requires sentence segmentation (gao et al, 2004).<papid> P04-1059 </papid></citsent>
<aftsection>
<nextsent>tbl approaches tothe combined problem of segmenting and pos tagging chinese sentences are reported in florian and ngai (2001) <papid> W01-0701 </papid>and fung et al (2004).</nextsent>
<nextsent>several implementations of the tbl approach are freely available on the web, the most well known being the so-called brill tagger, fntbl, which allows for multi-dimensional tbl, and ?-tbl (lager, 1999).<papid> W99-0705 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2867">
<title id=" E09-1031.xml">tblimproved non deterministic segmentation and pos tagging for a chinese parser </title>
<section> fst5 rules for the improvement of.  </section>
<citcontext>
<prevsection>
<prevsent>the first attempts to employ tbl to solve the problem of chinese word segmentation go back to palmer (1997) <papid> P97-1041 </papid>and hockenmaier and brew (1998).in more recent work, tbl was used for the adaption of the output of statistical general purpose?</prevsent>
<prevsent>segmenter to standards that vary depending on the application that requires sentence segmentation (gao et al, 2004).<papid> P04-1059 </papid></prevsent>
</prevsection>
<citsent citstr=" W01-0701 ">
tbl approaches tothe combined problem of segmenting and pos tagging chinese sentences are reported in florian and ngai (2001) <papid> W01-0701 </papid>and fung et al (2004).</citsent>
<aftsection>
<nextsent>several implementations of the tbl approach are freely available on the web, the most well known being the so-called brill tagger, fntbl, which allows for multi-dimensional tbl, and ?-tbl (lager, 1999).<papid> W99-0705 </papid></nextsent>
<nextsent>among these, we chose ?-tbl for our experiments because (like fntbl) it is completely flexible as to whether sample is word, character or anything else and (un like fntbl) it allows for the induction of optional rules.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2869">
<title id=" E09-1031.xml">tblimproved non deterministic segmentation and pos tagging for a chinese parser </title>
<section> fst5 rules for the improvement of.  </section>
<citcontext>
<prevsection>
<prevsent>segmenter to standards that vary depending on the application that requires sentence segmentation (gao et al, 2004).<papid> P04-1059 </papid></prevsent>
<prevsent>tbl approaches tothe combined problem of segmenting and pos tagging chinese sentences are reported in florian and ngai (2001) <papid> W01-0701 </papid>and fung et al (2004).</prevsent>
</prevsection>
<citsent citstr=" W99-0705 ">
several implementations of the tbl approach are freely available on the web, the most well known being the so-called brill tagger, fntbl, which allows for multi-dimensional tbl, and ?-tbl (lager, 1999).<papid> W99-0705 </papid></citsent>
<aftsection>
<nextsent>among these, we chose ?-tbl for our experiments because (like fntbl) it is completely flexible as to whether sample is word, character or anything else and (un like fntbl) it allows for the induction of optional rules.
</nextsent>
<nextsent>probably due to its flexibility, ?-tbl has been used (albeit on small scale for the most part) for tasks as diverse as pos tagging, map tasks, and machine translation.
</nextsent>
<nextsent>3.2.2 experiment set-upwe started out with corpus of thirty gold segmented and -tagged daily editions of the xinhua daily, which were provided by the institute of computational linguistics at beijing university.
</nextsent>
<nextsent>three daily editions, which comprise 5,054sentences with 129,377 words and 213,936 characters, were set aside for testing purposes; there maining 27 editions were used for training.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2875">
<title id=" E09-1031.xml">tblimproved non deterministic segmentation and pos tagging for a chinese parser </title>
<section> comparison to related work and.  </section>
<citcontext>
<prevsection>
<prevsent>evaluation measures for the sentence level have not been given inany publication that we are aware of, probably because segment ers and pos taggers are rarely considered as pre-processing modules for parsers, but also because the figures for measures like sentence accuracy are strikingly low.for systems that perform only word segmentation, we find the following results in the literature:(gao et al, 2004), <papid> P04-1059 </papid>who use tbl to adapt general purpose?</prevsent>
<prevsent>segmenter to varying standards, report an sf of 95.5% on pku data and an sf of 90.4% on ctb data.</prevsent>
</prevsection>
<citsent citstr=" I05-3027 ">
(tseng et al, 2005) <papid> I05-3027 </papid>achieve an sf of 95.0%, 95.3% and 86.3% on pku data from the sighan bakeoff 2005, pku data from the sighan bakeoff 2003 and ctb data from the sighan bakeoff 2003 respectively.</citsent>
<aftsection>
<nextsent>finally, (zhang et al, 2006) <papid> P06-2123 </papid>report an sf of 94.8% on pku data.for systems that perform both word segmentation and pos tagging, the following results were published: florian and ngai (2001) <papid> W01-0701 </papid>report an sf of 93.55% and ta of 88.86% on ctb data.</nextsent>
<nextsent>ng and low (2004) <papid> W04-3236 </papid>report an sf of 95.2% and ta of 91.9% on ctb data.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2876">
<title id=" E09-1031.xml">tblimproved non deterministic segmentation and pos tagging for a chinese parser </title>
<section> comparison to related work and.  </section>
<citcontext>
<prevsection>
<prevsent>segmenter to varying standards, report an sf of 95.5% on pku data and an sf of 90.4% on ctb data.
</prevsent>
<prevsent>(tseng et al, 2005) <papid> I05-3027 </papid>achieve an sf of 95.0%, 95.3% and 86.3% on pku data from the sighan bakeoff 2005, pku data from the sighan bakeoff 2003 and ctb data from the sighan bakeoff 2003 respectively.</prevsent>
</prevsection>
<citsent citstr=" P06-2123 ">
finally, (zhang et al, 2006) <papid> P06-2123 </papid>report an sf of 94.8% on pku data.for systems that perform both word segmentation and pos tagging, the following results were published: florian and ngai (2001) <papid> W01-0701 </papid>report an sf of 93.55% and ta of 88.86% on ctb data.</citsent>
<aftsection>
<nextsent>ng and low (2004) <papid> W04-3236 </papid>report an sf of 95.2% and ta of 91.9% on ctb data.</nextsent>
<nextsent>finally, zhang and clark (2008) <papid> P08-1101 </papid>achieve an sf of 95.90% and tf of 91.34% by 10-fold cross validation using ctb data.last but not least, there are parsers that operate on characters rather than words and who perform segmentation and pos tagging as part of the parsing process.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2879">
<title id=" E09-1031.xml">tblimproved non deterministic segmentation and pos tagging for a chinese parser </title>
<section> comparison to related work and.  </section>
<citcontext>
<prevsection>
<prevsent>(tseng et al, 2005) <papid> I05-3027 </papid>achieve an sf of 95.0%, 95.3% and 86.3% on pku data from the sighan bakeoff 2005, pku data from the sighan bakeoff 2003 and ctb data from the sighan bakeoff 2003 respectively.</prevsent>
<prevsent>finally, (zhang et al, 2006) <papid> P06-2123 </papid>report an sf of 94.8% on pku data.for systems that perform both word segmentation and pos tagging, the following results were published: florian and ngai (2001) <papid> W01-0701 </papid>report an sf of 93.55% and ta of 88.86% on ctb data.</prevsent>
</prevsection>
<citsent citstr=" W04-3236 ">
ng and low (2004) <papid> W04-3236 </papid>report an sf of 95.2% and ta of 91.9% on ctb data.</citsent>
<aftsection>
<nextsent>finally, zhang and clark (2008) <papid> P08-1101 </papid>achieve an sf of 95.90% and tf of 91.34% by 10-fold cross validation using ctb data.last but not least, there are parsers that operate on characters rather than words and who perform segmentation and pos tagging as part of the parsing process.</nextsent>
<nextsent>among these, we would like to mention luo (2003), <papid> W03-1025 </papid>who reports an sf 96.0% on chinese treebank (ctb) data, and (fung etal., 2004), who achieve word segmentation pre cision/recall performance of 93/94%?.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2880">
<title id=" E09-1031.xml">tblimproved non deterministic segmentation and pos tagging for a chinese parser </title>
<section> comparison to related work and.  </section>
<citcontext>
<prevsection>
<prevsent>finally, (zhang et al, 2006) <papid> P06-2123 </papid>report an sf of 94.8% on pku data.for systems that perform both word segmentation and pos tagging, the following results were published: florian and ngai (2001) <papid> W01-0701 </papid>report an sf of 93.55% and ta of 88.86% on ctb data.</prevsent>
<prevsent>ng and low (2004) <papid> W04-3236 </papid>report an sf of 95.2% and ta of 91.9% on ctb data.</prevsent>
</prevsection>
<citsent citstr=" P08-1101 ">
finally, zhang and clark (2008) <papid> P08-1101 </papid>achieve an sf of 95.90% and tf of 91.34% by 10-fold cross validation using ctb data.last but not least, there are parsers that operate on characters rather than words and who perform segmentation and pos tagging as part of the parsing process.</citsent>
<aftsection>
<nextsent>among these, we would like to mention luo (2003), <papid> W03-1025 </papid>who reports an sf 96.0% on chinese treebank (ctb) data, and (fung etal., 2004), who achieve word segmentation pre cision/recall performance of 93/94%?.</nextsent>
<nextsent>both the sf and the tf results achieved by our pku onetag + non-deterministic rule set?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2881">
<title id=" E09-1031.xml">tblimproved non deterministic segmentation and pos tagging for a chinese parser </title>
<section> comparison to related work and.  </section>
<citcontext>
<prevsection>
<prevsent>ng and low (2004) <papid> W04-3236 </papid>report an sf of 95.2% and ta of 91.9% on ctb data.</prevsent>
<prevsent>finally, zhang and clark (2008) <papid> P08-1101 </papid>achieve an sf of 95.90% and tf of 91.34% by 10-fold cross validation using ctb data.last but not least, there are parsers that operate on characters rather than words and who perform segmentation and pos tagging as part of the parsing process.</prevsent>
</prevsection>
<citsent citstr=" W03-1025 ">
among these, we would like to mention luo (2003), <papid> W03-1025 </papid>who reports an sf 96.0% on chinese treebank (ctb) data, and (fung etal., 2004), who achieve word segmentation pre cision/recall performance of 93/94%?.</citsent>
<aftsection>
<nextsent>both the sf and the tf results achieved by our pku onetag + non-deterministic rule set?
</nextsent>
<nextsent>setup, whose out put is slightly ambiguous, compare favorably with all the results mentioned, and even the results achieved by our pku one tag + deterministic rule set?
</nextsent>
<nextsent>setup are competitive.
</nextsent>
<nextsent>the idea of carrying some ambiguity from one processing step into the next in order not to prune good solutions is not new.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2882">
<title id=" E09-1031.xml">tblimproved non deterministic segmentation and pos tagging for a chinese parser </title>
<section> conclusions and future work.  </section>
<citcontext>
<prevsection>
<prevsent>of words per corr.
</prevsent>
<prevsent>tagged sent.
</prevsent>
</prevsection>
<citsent citstr=" P06-1088 ">
9.58 13.20 15.11 16.33 table 1: evaluation figures achieved by four different systems on the 5,054 sentences of our test setand curran et al (2006) <papid> P06-1088 </papid>show the benefits of using multi-tagger rather than single-tagger for an induced ccg for english.</citsent>
<aftsection>
<nextsent>however, to our knowledge, this idea has not made its way intothe field of chinese parsing so far.
</nextsent>
<nextsent>chinese parsing systems either pass on single segmentation and pos tagging analysis to the parser proper or they are character-based, i.e. segmentation and tagging are part of the parsing process.
</nextsent>
<nextsent>although several treebank-induced character-based parsers for chinese have achieved promising results, this approach is impractical in the development of hand-crafted deep grammar like the chinese lfg.
</nextsent>
<nextsent>we therefore believe that the development of multi-tokenizer-tagger?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2883">
<title id=" E09-1042.xml">weakly supervised partofspeech tagging for morphologically rich resource scarce languages </title>
<section> abstract </section>
<citcontext>
<prevsection>


</prevsection>
<citsent citstr=" P07-1094 ">
this paper examines unsupervised approaches to part-of-speech (pos) tagging for morphologically-rich, resource-scarcelanguages, with an emphasis on goldwater and griffithss (2007) <papid> P07-1094 </papid>fully-bayesianapproach originally developed for english pos tagging.</citsent>
<aftsection>
<nextsent>we argue that existing unsupervised pos taggers unrealistically assume as input perfect pos lexicon, and consequently, we proposea weakly supervised fully-bayesian approach to pos tagging, which relaxes the unrealistic assumption by automatically acquiring the lexicon from small amount of pos-tagged data.
</nextsent>
<nextsent>since such relaxation comes at the expense of drop in tagging accuracy, we propose two extension sto the bayesian framework and demonstrate that they are effective in improving fully-bayesian pos tagger for bengali, our representative morphologically rich, resource-scarce language.
</nextsent>
<nextsent>unsupervised pos tagging requires neither manual encoding of tagging heuristics nor the availability of data labeled with pos information.
</nextsent>
<nextsent>rather, an unsupervised pos tagger operates byonly assuming as input pos lexicon, which consists of list of possible pos tags for each word.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2886">
<title id=" E09-1042.xml">weakly supervised partofspeech tagging for morphologically rich resource scarce languages </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>finally, we conclude in section 6.
</prevsent>
<prevsent>with the notable exception of synder et al .s (2008), (2009) recent work on unsupervised multilingual pos tagging, existing approaches to unsupervised pos tagging have been developed and tested primarily on english data.
</prevsent>
</prevsection>
<citsent citstr=" J94-2001 ">
for instance,merialdo (1994) <papid> J94-2001 </papid>uses maximum likelihood estimation to train trigram hmm.</citsent>
<aftsection>
<nextsent>schutze (1995) and clark (2000) <papid> W00-0717 </papid>apply syntactic clustering and dimensionality reduction in knowledge-free setting to obtain meaningful clusters.</nextsent>
<nextsent>haghighi and klein (2006) <papid> N06-1041 </papid>develop prototype-driven approach, which requires just few prototype examples for each pos tag and exploits these labeled words to constrain the labels of their distributionally similar words.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2887">
<title id=" E09-1042.xml">weakly supervised partofspeech tagging for morphologically rich resource scarce languages </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>with the notable exception of synder et al .s (2008), (2009) recent work on unsupervised multilingual pos tagging, existing approaches to unsupervised pos tagging have been developed and tested primarily on english data.
</prevsent>
<prevsent>for instance,merialdo (1994) <papid> J94-2001 </papid>uses maximum likelihood estimation to train trigram hmm.</prevsent>
</prevsection>
<citsent citstr=" W00-0717 ">
schutze (1995) and clark (2000) <papid> W00-0717 </papid>apply syntactic clustering and dimensionality reduction in knowledge-free setting to obtain meaningful clusters.</citsent>
<aftsection>
<nextsent>haghighi and klein (2006) <papid> N06-1041 </papid>develop prototype-driven approach, which requires just few prototype examples for each pos tag and exploits these labeled words to constrain the labels of their distributionally similar words.</nextsent>
<nextsent>smith and eisner (2005) <papid> P05-1044 </papid>train an unsupervised pos tagger using contrastive estimation, which seeks to move probability mass to positive example from its neighbors (i.e., negative examples are created by perturbing e).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2888">
<title id=" E09-1042.xml">weakly supervised partofspeech tagging for morphologically rich resource scarce languages </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>for instance,merialdo (1994) <papid> J94-2001 </papid>uses maximum likelihood estimation to train trigram hmm.</prevsent>
<prevsent>schutze (1995) and clark (2000) <papid> W00-0717 </papid>apply syntactic clustering and dimensionality reduction in knowledge-free setting to obtain meaningful clusters.</prevsent>
</prevsection>
<citsent citstr=" N06-1041 ">
haghighi and klein (2006) <papid> N06-1041 </papid>develop prototype-driven approach, which requires just few prototype examples for each pos tag and exploits these labeled words to constrain the labels of their distributionally similar words.</citsent>
<aftsection>
<nextsent>smith and eisner (2005) <papid> P05-1044 </papid>train an unsupervised pos tagger using contrastive estimation, which seeks to move probability mass to positive example from its neighbors (i.e., negative examples are created by perturbing e).</nextsent>
<nextsent>wang and schuurmans (2005) improve an unsupervised hmm-based tagger by constraining the learned structure to maintain appropriate marginal tag probabilities and using word similarities to smooth the lexical parameters.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2889">
<title id=" E09-1042.xml">weakly supervised partofspeech tagging for morphologically rich resource scarce languages </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>schutze (1995) and clark (2000) <papid> W00-0717 </papid>apply syntactic clustering and dimensionality reduction in knowledge-free setting to obtain meaningful clusters.</prevsent>
<prevsent>haghighi and klein (2006) <papid> N06-1041 </papid>develop prototype-driven approach, which requires just few prototype examples for each pos tag and exploits these labeled words to constrain the labels of their distributionally similar words.</prevsent>
</prevsection>
<citsent citstr=" P05-1044 ">
smith and eisner (2005) <papid> P05-1044 </papid>train an unsupervised pos tagger using contrastive estimation, which seeks to move probability mass to positive example from its neighbors (i.e., negative examples are created by perturbing e).</citsent>
<aftsection>
<nextsent>wang and schuurmans (2005) improve an unsupervised hmm-based tagger by constraining the learned structure to maintain appropriate marginal tag probabilities and using word similarities to smooth the lexical parameters.
</nextsent>
<nextsent>as mentioned before, goldwater and griffiths (2007) <papid> P07-1094 </papid>have recently proposed an unsupervisedfully-bayesian pos tagging framework that operates by integrating over the possible parameter values instead of fixing set of parameter values for unsupervised sequence learning.</nextsent>
<nextsent>importantly,this bayesian approach facilitates the incorporation of sparse priors that result in more practical distribution of tokens to lexical categories (john son, 2007).<papid> D07-1031 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2891">
<title id=" E09-1042.xml">weakly supervised partofspeech tagging for morphologically rich resource scarce languages </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>wang and schuurmans (2005) improve an unsupervised hmm-based tagger by constraining the learned structure to maintain appropriate marginal tag probabilities and using word similarities to smooth the lexical parameters.
</prevsent>
<prevsent>as mentioned before, goldwater and griffiths (2007) <papid> P07-1094 </papid>have recently proposed an unsupervisedfully-bayesian pos tagging framework that operates by integrating over the possible parameter values instead of fixing set of parameter values for unsupervised sequence learning.</prevsent>
</prevsection>
<citsent citstr=" D07-1031 ">
importantly,this bayesian approach facilitates the incorporation of sparse priors that result in more practical distribution of tokens to lexical categories (john son, 2007).<papid> D07-1031 </papid></citsent>
<aftsection>
<nextsent>similar to goldwater and griffiths(2007) <papid> P07-1094 </papid>and johnson (2007), <papid> D07-1031 </papid>toutanova and johnson (2007) <papid> D07-1031 </papid>also use bayesian inference for postagging.</nextsent>
<nextsent>however, their work departs from existing bayesian approaches to pos tagging in that they (1) introduce new sparse prior on the distribution over tags for each word, (2) extend the latent dirichlet allocation model, and (3) explicitly model ambiguity class.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2896">
<title id=" E09-1042.xml">weakly supervised partofspeech tagging for morphologically rich resource scarce languages </title>
<section> a fully bayesian approach </section>
<citcontext>
<prevsection>
<prevsent>(1) the question, then, is: which priors on ? would allow the acquisition of skewed distributions?
</prevsent>
<prevsent>to 2when given good parameter initializations, however, emcan find good parameter values for an hmm-based pos tagger.
</prevsent>
</prevsection>
<citsent citstr=" P08-1085 ">
see goldberg et al  (2008) <papid> P08-1085 </papid>for details.</citsent>
<aftsection>
<nextsent>answer this question, recall that in pos tagging, ? is composed of set of tag transition distributions and output distributions.
</nextsent>
<nextsent>each such distribution is multinomial (i.e., each trial produces exactly one of some finite number of possible outcomes).
</nextsent>
<nextsent>for multinomial with outcomes, k-dimensional dirichlet distribution, which is conjugate to themultinomial, is natural choice of prior.
</nextsent>
<nextsent>for simplicity, we assume that distribution in ? is drawn from symmetric dirichlet with certain hyperparameter (see teh et al  (2006) for details).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2897">
<title id=" E09-1042.xml">weakly supervised partofspeech tagging for morphologically rich resource scarce languages </title>
<section> two extensions.  </section>
<citcontext>
<prevsection>
<prevsent>4.1 induced suffix emission.
</prevsent>
<prevsent>for morphologically-rich languages like bengali,a lot of grammatical information (e.g., pos) is expressed via suffixes.
</prevsent>
</prevsection>
<citsent citstr=" E03-1009 ">
in fact, several approaches to unsupervised pos induction for morphologically rich languages have exploited the observation that some suffixes can only be associated with small number of pos tags (e.g., clark (2003), <papid> E03-1009 </papid>dasgupta and ng (2007)).<papid> D07-1023 </papid></citsent>
<aftsection>
<nextsent>to exploit suffixes in hmm based pos tagging, one can (1) convert the word based pos lexicon to suffix-based pos lexicon, which lists the possible pos tags for each suffix; and then (2) have the hmm emit suffixes rather than words, subject to the constraints in the suffix based pos lexicon.
</nextsent>
<nextsent>such suffix-based hmm, however, may suffer from over-generalization.
</nextsent>
<nextsent>to prevent over-generalization and at the same time exploit suffixes, we propose as our first extension to g&gs; framework hybrid approach to word/suffix emission: word is emitted if it is present in the word-based pos lexicon; otherwise, its suffix is emitted.
</nextsent>
<nextsent>in other words, our approach imposes suffix-based constraints on the tagging of words that are unseen w.r.t. the word-based pos lexicon.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2898">
<title id=" E09-1042.xml">weakly supervised partofspeech tagging for morphologically rich resource scarce languages </title>
<section> two extensions.  </section>
<citcontext>
<prevsection>
<prevsent>4.1 induced suffix emission.
</prevsent>
<prevsent>for morphologically-rich languages like bengali,a lot of grammatical information (e.g., pos) is expressed via suffixes.
</prevsent>
</prevsection>
<citsent citstr=" D07-1023 ">
in fact, several approaches to unsupervised pos induction for morphologically rich languages have exploited the observation that some suffixes can only be associated with small number of pos tags (e.g., clark (2003), <papid> E03-1009 </papid>dasgupta and ng (2007)).<papid> D07-1023 </papid></citsent>
<aftsection>
<nextsent>to exploit suffixes in hmm based pos tagging, one can (1) convert the word based pos lexicon to suffix-based pos lexicon, which lists the possible pos tags for each suffix; and then (2) have the hmm emit suffixes rather than words, subject to the constraints in the suffix based pos lexicon.
</nextsent>
<nextsent>such suffix-based hmm, however, may suffer from over-generalization.
</nextsent>
<nextsent>to prevent over-generalization and at the same time exploit suffixes, we propose as our first extension to g&gs; framework hybrid approach to word/suffix emission: word is emitted if it is present in the word-based pos lexicon; otherwise, its suffix is emitted.
</nextsent>
<nextsent>in other words, our approach imposes suffix-based constraints on the tagging of words that are unseen w.r.t. the word-based pos lexicon.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2899">
<title id=" E09-1042.xml">weakly supervised partofspeech tagging for morphologically rich resource scarce languages </title>
<section> two extensions.  </section>
<citcontext>
<prevsection>
<prevsent>constructing suffix-based pos lexicon next, we construct suffix-based pos lexicon.
</prevsent>
<prevsent>for each word in the original word-based pos lexicon, we (1) use the induced suffix list obtained in the previous step to identify the longest-matching suffix of w, and then (2) assign all the pos tags associated with to this suffix.
</prevsent>
</prevsection>
<citsent citstr=" J01-2001 ">
incorporating suffix-based output distributions finally, we extend our trigram model by intro duc 3the dependence on frequency and length is motivated by the observation that less frequent and shorter affixes are more likely to be erroneous (see goldsmith (2001)).<papid> J01-2001 </papid></citsent>
<aftsection>
<nextsent>366 (ti|ti,w, ?, ?) ?
</nextsent>
<nextsent>n(ti,wi) + ? nti + wti?
</nextsent>
<nextsent>n(ti2,ti1,ti) + ? n(ti2,ti1) + t?
</nextsent>
<nextsent>n(ti1,ti,ti+1) + i(ti2 = ti1 = ti = ti+1) + ? n(ti1,ti) + i(ti2 = ti1 = ti) + t?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2901">
<title id=" E89-1036.xml">a descriptive framework for translating speakers meaning </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the whole process is performed in unification-based framework.
</prevsent>
<prevsent>in devising machine translation system of telephone dialogues, one of the problems how to adequately translate tile underlying meaning of the source utterance, or the speaker intention, into the target language.
</prevsent>
</prevsection>
<citsent citstr=" C88-2142 ">
such concern is rarely observed in conventional machine translation research, which has focused on st ic ly rammat ica t rans la ion ivorced from consideration of the speaker itu ation and intentions (tsujii and nagao 1988).<papid> C88-2142 </papid></citsent>
<aftsection>
<nextsent>however, in dialogue, smoothness of communication depends on perceiving the speaker intention.
</nextsent>
<nextsent>especially when dealing with different language family pairs such as japanese and english, it is necessary to have methodology of treating language-specific communication strategies in universal framework.
</nextsent>
<nextsent>although the input of our machine translation system is spoken dialogue, here we leave aside the issues of speech processing and limit our discussion to inguistic processing.
</nextsent>
<nextsent>extra- grammatical sentence patterns uch as intra- sentential correction, stammering, and inversion are not treated either.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2902">
<title id=" E09-1061.xml">translation as weighted deduction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>most translation algorithms do the same thing: dynamic programming search over space of weighted rules (2).
</prevsent>
<prevsent>fortunately, we neednot search far for modular descriptions of dynamic programming algorithms.
</prevsent>
</prevsection>
<citsent citstr=" P83-1021 ">
deductive logic(pereira and warren, 1983), <papid> P83-1021 </papid>extended with semi rings (goodman, 1999), <papid> J99-4004 </papid>is an established formalism used in parsing.</citsent>
<aftsection>
<nextsent>it is occasionally usedto describe formally syntactic translation models, but these treatments tend to be brief (chiang, 2007; venugopal et al, 2007; <papid> N07-1063 </papid>dyer et al, 2008; <papid> P08-1115 </papid>melamed, 2004).<papid> P04-1083 </papid></nextsent>
<nextsent>we apply weighted deduction much more thoroughly, first extending it to phrase based models and showing that the set of search strategies used by these models have surprisingly different implications for model and search error(3), and search error(4).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2903">
<title id=" E09-1061.xml">translation as weighted deduction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>most translation algorithms do the same thing: dynamic programming search over space of weighted rules (2).
</prevsent>
<prevsent>fortunately, we neednot search far for modular descriptions of dynamic programming algorithms.
</prevsent>
</prevsection>
<citsent citstr=" J99-4004 ">
deductive logic(pereira and warren, 1983), <papid> P83-1021 </papid>extended with semi rings (goodman, 1999), <papid> J99-4004 </papid>is an established formalism used in parsing.</citsent>
<aftsection>
<nextsent>it is occasionally usedto describe formally syntactic translation models, but these treatments tend to be brief (chiang, 2007; venugopal et al, 2007; <papid> N07-1063 </papid>dyer et al, 2008; <papid> P08-1115 </papid>melamed, 2004).<papid> P04-1083 </papid></nextsent>
<nextsent>we apply weighted deduction much more thoroughly, first extending it to phrase based models and showing that the set of search strategies used by these models have surprisingly different implications for model and search error(3), and search error(4).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2904">
<title id=" E09-1061.xml">translation as weighted deduction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>fortunately, we neednot search far for modular descriptions of dynamic programming algorithms.
</prevsent>
<prevsent>deductive logic(pereira and warren, 1983), <papid> P83-1021 </papid>extended with semi rings (goodman, 1999), <papid> J99-4004 </papid>is an established formalism used in parsing.</prevsent>
</prevsection>
<citsent citstr=" N07-1063 ">
it is occasionally usedto describe formally syntactic translation models, but these treatments tend to be brief (chiang, 2007; venugopal et al, 2007; <papid> N07-1063 </papid>dyer et al, 2008; <papid> P08-1115 </papid>melamed, 2004).<papid> P04-1083 </papid></citsent>
<aftsection>
<nextsent>we apply weighted deduction much more thoroughly, first extending it to phrase based models and showing that the set of search strategies used by these models have surprisingly different implications for model and search error(3), and search error(4).
</nextsent>
<nextsent>we then show how it can be used to analyze common translation problems such as non local parameterizations (5), alignment, and novel model design (6).
</nextsent>
<nextsent>finally, we show that it leads to simple analysis of cube pruning (chiang, 2007), an important approximate search algorithm (7).
</nextsent>
<nextsent>a translation model consists of two distinct elements: an unweighted ruleset, and parameterization (lopez, 2008).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2906">
<title id=" E09-1061.xml">translation as weighted deduction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>fortunately, we neednot search far for modular descriptions of dynamic programming algorithms.
</prevsent>
<prevsent>deductive logic(pereira and warren, 1983), <papid> P83-1021 </papid>extended with semi rings (goodman, 1999), <papid> J99-4004 </papid>is an established formalism used in parsing.</prevsent>
</prevsection>
<citsent citstr=" P08-1115 ">
it is occasionally usedto describe formally syntactic translation models, but these treatments tend to be brief (chiang, 2007; venugopal et al, 2007; <papid> N07-1063 </papid>dyer et al, 2008; <papid> P08-1115 </papid>melamed, 2004).<papid> P04-1083 </papid></citsent>
<aftsection>
<nextsent>we apply weighted deduction much more thoroughly, first extending it to phrase based models and showing that the set of search strategies used by these models have surprisingly different implications for model and search error(3), and search error(4).
</nextsent>
<nextsent>we then show how it can be used to analyze common translation problems such as non local parameterizations (5), alignment, and novel model design (6).
</nextsent>
<nextsent>finally, we show that it leads to simple analysis of cube pruning (chiang, 2007), an important approximate search algorithm (7).
</nextsent>
<nextsent>a translation model consists of two distinct elements: an unweighted ruleset, and parameterization (lopez, 2008).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2908">
<title id=" E09-1061.xml">translation as weighted deduction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>fortunately, we neednot search far for modular descriptions of dynamic programming algorithms.
</prevsent>
<prevsent>deductive logic(pereira and warren, 1983), <papid> P83-1021 </papid>extended with semi rings (goodman, 1999), <papid> J99-4004 </papid>is an established formalism used in parsing.</prevsent>
</prevsection>
<citsent citstr=" P04-1083 ">
it is occasionally usedto describe formally syntactic translation models, but these treatments tend to be brief (chiang, 2007; venugopal et al, 2007; <papid> N07-1063 </papid>dyer et al, 2008; <papid> P08-1115 </papid>melamed, 2004).<papid> P04-1083 </papid></citsent>
<aftsection>
<nextsent>we apply weighted deduction much more thoroughly, first extending it to phrase based models and showing that the set of search strategies used by these models have surprisingly different implications for model and search error(3), and search error(4).
</nextsent>
<nextsent>we then show how it can be used to analyze common translation problems such as non local parameterizations (5), alignment, and novel model design (6).
</nextsent>
<nextsent>finally, we show that it leads to simple analysis of cube pruning (chiang, 2007), an important approximate search algorithm (7).
</nextsent>
<nextsent>a translation model consists of two distinct elements: an unweighted ruleset, and parameterization (lopez, 2008).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2910">
<title id=" E09-1061.xml">translation as weighted deduction </title>
<section> translation as deduction.  </section>
<citcontext>
<prevsection>
<prevsent>for the first part of the discussion that follows, we consider deductive logics purely over unweighted rulesets.
</prevsent>
<prevsent>as way to introduce deductive logic, we consider the cky algorithm for context-free parsing, common example that we will revisit in 6.2.
</prevsent>
</prevsection>
<citsent citstr=" P96-1021 ">
it is also relevant since it can form the basis of decoder for inversion transduction grammar (wu, 1996).<papid> P96-1021 </papid></citsent>
<aftsection>
<nextsent>in the discussion that follows, we usea,b, and to denote arbitrary nonterminal symbols, to denote the start nonterminal symbol, and to denote terminal symbol.
</nextsent>
<nextsent>cky works on grammars in chomsky normal form: all rules are either binary as in a?
</nextsent>
<nextsent>bc, or unary as in a?
</nextsent>
<nextsent>a. the number of possible binary-branchingparses of sentence is defined by the catalan number, an exponential combinatoric function (churchand patil, 1982), <papid> J82-3004 </papid>so dynamic programming is crucial for efficiency.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2911">
<title id=" E09-1061.xml">translation as weighted deduction </title>
<section> translation as deduction.  </section>
<citcontext>
<prevsection>
<prevsent>cky works on grammars in chomsky normal form: all rules are either binary as in a?
</prevsent>
<prevsent>bc, or unary as in a?
</prevsent>
</prevsection>
<citsent citstr=" J82-3004 ">
a. the number of possible binary-branchingparses of sentence is defined by the catalan number, an exponential combinatoric function (churchand patil, 1982), <papid> J82-3004 </papid>so dynamic programming is crucial for efficiency.</citsent>
<aftsection>
<nextsent>cky computes all parses incubic time by reusing subparses.
</nextsent>
<nextsent>to parse sentence a1...ak , we compute set of items in the form [a, k, k?], wherea is nonterminal category, 1the true noisy channel parameterization p(f |e) ? p(e) would require marginalization over d, and is intractable for most models.
</nextsent>
<nextsent>k and k?
</nextsent>
<nextsent>are both inte gers in the range [0, n].
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2916">
<title id=" E09-1061.xml">translation as weighted deduction </title>
<section> translation as deduction.  </section>
<citcontext>
<prevsection>
<prevsent>are both in range [0,k].
</prevsent>
<prevsent>likewise, the number of possible inference rules that can fire is o(g3k3).
</prevsent>
</prevsection>
<citsent citstr=" N04-1033 ">
3.1 simple deductive decoder for our first example of translation logic we consider simple case: monotone decoding (marino et al, 2006; zens and ney, 2004).<papid> N04-1033 </papid></citsent>
<aftsection>
<nextsent>here, rewrite rules are applied strictly from left to right on the source sentence.
</nextsent>
<nextsent>despite its simplicity, the search 533 space can be very largein the limit there could be translation for every possible segmentation of the sentence, so there are exponentially many possible derivations.
</nextsent>
<nextsent>fortunately, we know that monotone decoding can easily be cast as dynamic programming problem.
</nextsent>
<nextsent>for any position iin the source sentence f1...fi , we can freely combine any partial derivation covering f1...fi on its left with any partial derivation covering fi+1...fi on its right to yield complete derivation.in our deductive program for monotone decoding, an item simply encodes the index of the right most word that has been rewritten.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2919">
<title id=" E09-1061.xml">translation as weighted deduction </title>
<section> translation as deduction.  </section>
<citcontext>
<prevsection>
<prevsent>the first uncovered words strategy (fduw) is described by tillman and ney (2003) and zens and ney (2004), <papid> N04-1033 </papid>who call it the ibm constraint.6 it requires at least one of the left most uncovered words to be covered by new phrase.</prevsent>
<prevsent>items in this strategy contain the index of the rightmost covered word and vector ? [1, i]d of the left most uncovered words (figure 1).</prevsent>
</prevsection>
<citsent citstr=" H05-1021 ">
its complexity is o(di ( d+1 )), which is roughly exponential in d.there are additional variants, such as the maximum jump strategy (mjd), polynomial-time strategy described by kumar and byrne (2005), <papid> H05-1021 </papid>and possibly others.</citsent>
<aftsection>
<nextsent>we lack space to describe allof them, but simply depicting the strategies as logics permits us to make some simple analyses.first, it should be clear that these reordering strategies define overlapping but not identical search spaces: for most values of it is impossible to find d?
</nextsent>
<nextsent>such that any of the other strategies would be identical (except for degenerate cases 3moore and quirk (2007) give nice description of mdd.
</nextsent>
<nextsent>4we do not know if wld is documented anywhere, but from inspection it is used in moses (koehn et al, 2007).<papid> P07-2045 </papid></nextsent>
<nextsent>this was confirmed by philipp koehn and hieu hoang (p.c.).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2920">
<title id=" E09-1061.xml">translation as weighted deduction </title>
<section> translation as deduction.  </section>
<citcontext>
<prevsection>
<prevsent>we lack space to describe allof them, but simply depicting the strategies as logics permits us to make some simple analyses.first, it should be clear that these reordering strategies define overlapping but not identical search spaces: for most values of it is impossible to find d?
</prevsent>
<prevsent>such that any of the other strategies would be identical (except for degenerate cases 3moore and quirk (2007) give nice description of mdd.
</prevsent>
</prevsection>
<citsent citstr=" P07-2045 ">
4we do not know if wld is documented anywhere, but from inspection it is used in moses (koehn et al, 2007).<papid> P07-2045 </papid></citsent>
<aftsection>
<nextsent>this was confirmed by philipp koehn and hieu hoang (p.c.).
</nextsent>
<nextsent>5when phrase covers the first uncovered word in the source sentence, the new first uncovered word may be further along in the sentence (if the phrase completely filled gap), or just past the end of the phrase (otherwise).
</nextsent>
<nextsent>6we could not identify this strategy in the ibm patents.
</nextsent>
<nextsent>534 (1) item form: [i, {0, 1}i ] goal: [i ? [i ? d, i], 1i ] rule: [i??, ] r(fi+1...fi?/ej ...ej?)
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2921">
<title id=" E09-1061.xml">translation as weighted deduction </title>
<section> translation as deduction.  </section>
<citcontext>
<prevsection>
<prevsent>furthermore, it should be clear that the strategy can have significant impact on decoding speed and pruning strategies (7).
</prevsent>
<prevsent>for example, mdd is more complex than wld, and we expect implementations of the former to require more pruning and suffer from more search errors, while the latter would suffer from more model errors since its space of possible reorderings is smaller.we emphasize that many other translation models can be described this way.
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
logics for the ibm models (brown et al, 1993) <papid> J93-2003 </papid>would be similar to our logics for phrase-based models.</citsent>
<aftsection>
<nextsent>syntax-based translation logics are similar to parsing logics; afew examples already appear in the literature (chi ang, 2007; venugopal et al, 2007; <papid> N07-1063 </papid>dyer et al, 2008; <papid> P08-1115 </papid>melamed, 2004).<papid> P04-1083 </papid></nextsent>
<nextsent>for simplicity, we will use the monotone logic for the remainder of our examples, but all of them generalize to more complex logics.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2928">
<title id=" E09-1061.xml">translation as weighted deduction </title>
<section> adding local parameterizations via.  </section>
<citcontext>
<prevsection>
<prevsent>p(c) = max(p(c), (p(a1)?
</prevsent>
<prevsent>p(al))) (3) if for every a` that is an item, we replace p(a`) recursively with this expression, we end up with maximization over product of rule probabilities.
</prevsent>
</prevsection>
<citsent citstr=" P08-1024 ">
applying this to logic monotone, the result will be maximization (over all possible derivations d) of the algebraic expression in equation 1.we might also want to calculate the total probability of all possible derivations, which is useful for parameter estimation (blunsom et al, 2008).<papid> P08-1024 </papid></citsent>
<aftsection>
<nextsent>we can do this using the following equation.
</nextsent>
<nextsent>p(c) = p(c) + (p(a1)?
</nextsent>
<nextsent>p(al)) (4) 535 equations 3 and 4 are quite similar.
</nextsent>
<nextsent>this suggestsa useful generalization: semiring-weighted deduction (goodman, 1999).<papid> J99-4004 </papid>7 semi ring a,?,consists of domain a, multiplicative operator ? and an additive operator ?.8 in equation 3 we use the viterbi semi ring ?[0, 1],?,max?, while in equation 4 we use the inside semi ring ?[0, 1],?,+?.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2934">
<title id=" E09-1061.xml">translation as weighted deduction </title>
<section> adding local parameterizations via.  </section>
<citcontext>
<prevsection>
<prevsent>the general form of equations 3 and 4 can be written for weights ? a. w(c)?= w(a1)?
</prevsent>
<prevsent>w(a`) (5)many quantities can be computed simply by using the appropriate semiring.
</prevsent>
</prevsection>
<citsent citstr=" P02-1001 ">
goodman (1999) <papid> J99-4004 </papid>describes semi rings for the viterbi derivation, k-bestviterbi derivations, derivation forest, and number of paths.9 eisner (2002) <papid> P02-1001 </papid>describes the expectation semi ring for parameter learning.</citsent>
<aftsection>
<nextsent>gimpel and smith (2009) describe approximation semi rings for approximate summing in (usually intractable) models.
</nextsent>
<nextsent>in parsing, the boolean semi ring ?{ ,?},?,??
</nextsent>
<nextsent>is used to determine gram mati cality of an input string.
</nextsent>
<nextsent>in translation it is relevant for alignment (6.1).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2935">
<title id=" E09-1061.xml">translation as weighted deduction </title>
<section> adding non-local parameterizations.  </section>
<citcontext>
<prevsection>
<prevsent>this means thatf is nonmonotonicit does not display the optimal substructure property on partial derivations,which is required for dynamic programming (cor men et al, 2001).
</prevsent>
<prevsent>the logics still work for somesemirings (e.g. boolean), but not others.
</prevsent>
</prevsection>
<citsent citstr=" H05-1036 ">
therefore, non-local parameterizations break semi ring weighted deduction, because we can no longer use7general weighted deduction subsumes semi ring weighted deduction (eisner et al, 2005; <papid> H05-1036 </papid>eisner and blatz, 2006; nederhof, 2003), <papid> J03-1006 </papid>but semiring-weighted deduction covers all translation models we are aware of, so it is good first step in applying weighted deduction to translation.8see goodman (1999) <papid> J99-4004 </papid>for additional conditions on semir ings used in this framework.</citsent>
<aftsection>
<nextsent>9eisner and blatz (2006) give an alternate strategy for the best derivation.
</nextsent>
<nextsent>the same logic under all semirings.
</nextsent>
<nextsent>we need new logics; for this we will use logic programming transform called the product transform (cohen et al, 2008).we first define logic for the non-local parameterization.
</nextsent>
<nextsent>the logic for an n-gram language model generates sequence e1...eq by generating each new word given the past n?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2936">
<title id=" E09-1061.xml">translation as weighted deduction </title>
<section> adding non-local parameterizations.  </section>
<citcontext>
<prevsection>
<prevsent>this means thatf is nonmonotonicit does not display the optimal substructure property on partial derivations,which is required for dynamic programming (cor men et al, 2001).
</prevsent>
<prevsent>the logics still work for somesemirings (e.g. boolean), but not others.
</prevsent>
</prevsection>
<citsent citstr=" J03-1006 ">
therefore, non-local parameterizations break semi ring weighted deduction, because we can no longer use7general weighted deduction subsumes semi ring weighted deduction (eisner et al, 2005; <papid> H05-1036 </papid>eisner and blatz, 2006; nederhof, 2003), <papid> J03-1006 </papid>but semiring-weighted deduction covers all translation models we are aware of, so it is good first step in applying weighted deduction to translation.8see goodman (1999) <papid> J99-4004 </papid>for additional conditions on semir ings used in this framework.</citsent>
<aftsection>
<nextsent>9eisner and blatz (2006) give an alternate strategy for the best derivation.
</nextsent>
<nextsent>the same logic under all semirings.
</nextsent>
<nextsent>we need new logics; for this we will use logic programming transform called the product transform (cohen et al, 2008).we first define logic for the non-local parameterization.
</nextsent>
<nextsent>the logic for an n-gram language model generates sequence e1...eq by generating each new word given the past n?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2943">
<title id=" E09-1061.xml">translation as weighted deduction </title>
<section> other uses of the product transform.  </section>
<citcontext>
<prevsection>
<prevsent>6.1 alignment.
</prevsent>
<prevsent>in the alignment problem (sometimes called constrained decoding or forced decoding), we are given reference target sentence r1, ..., rj , and we require the translation model to generate only derivations that produce that sentence.
</prevsent>
</prevsection>
<citsent citstr=" P06-1096 ">
alignment is often used in training both generative and discriminative models (brown et al, 1993; <papid> J93-2003 </papid>blunsom et al, 2008; <papid> P08-1024 </papid>liang et al, 2006).<papid> P06-1096 </papid></citsent>
<aftsection>
<nextsent>our approach to alignment is similar to the one for language modeling.
</nextsent>
<nextsent>first, we implement logic requiring an 537 input to be identical to the reference.
</nextsent>
<nextsent>item form: [j] goal: [j ] rule: [j] [j + 1] ej+1 = rj+1 (logic recognize)the logic only reaches its goal if the input is identical to the reference.
</nextsent>
<nextsent>in fact, partial derivations must produce prefix of the reference.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2946">
<title id=" E09-1061.xml">translation as weighted deduction </title>
<section> other uses of the product transform.  </section>
<citcontext>
<prevsection>
<prevsent>6.2 translation model design.
</prevsent>
<prevsent>a motivation for many syntax-based translation models is to use target-side syntax as language model (charniak et al, 2003).
</prevsent>
</prevsection>
<citsent citstr=" N04-1021 ">
och et al (2004) <papid> N04-1021 </papid>showed that simply parsing the -best outputs of phrase-based model did not work; to obtain the full power of language model, we needto integrate it into the search process.</citsent>
<aftsection>
<nextsent>most approaches to this problem focus on synchronous grammars, but it is possible to integrate the target side language model with phrase-based translation model.
</nextsent>
<nextsent>as an exercise, we integrate cky with the output of logic monotone-generate.
</nextsent>
<nextsent>the constraint is that the indices of the cky items unify with the items of the translation logic, which form word lattice.
</nextsent>
<nextsent>note that this logic retains the rules in the basic monotone logic, which are not depicted (figure 3).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2955">
<title id=" E09-1061.xml">translation as weighted deduction </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>however,while their weighted deductive formalism is general, they focus on concerns relevant to parsing, such as boolean semi rings and cyclicity.
</prevsent>
<prevsent>our work focuses on concerns common for translation, including general view of non-local parameteriza tions and cube pruning.
</prevsent>
</prevsection>
<citsent citstr=" P07-1019 ">
13huang and chiang (2007) <papid> P07-1019 </papid>give an informal example, but do not elaborate on it.</citsent>
<aftsection>
<nextsent>we have described general framework that synthesizes and extends deductive parsing and semir ing parsing, and adapts it to translation.
</nextsent>
<nextsent>our goal has been to show that logics make an attractive shorthand for description, analysis, and construction of translation models.
</nextsent>
<nextsent>for instance, we have shown that it is quite easy to mechanically construct search spaces using non-local features, andto create exotic models.
</nextsent>
<nextsent>we showed that different flavors of phrase-based models should suffer from quite different types of error, problem thatto our knowledge was heretofore unknown.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2956">
<title id=" E12-1022.xml">dualsum a topic model based approach for update summarization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>most existing approaches handle it as redundancy removal problem, with the goal of producing summary of collection that is as dissimilar as possible from either collection or from summary of collec tiona.
</prevsent>
<prevsent>a problem with this approach is that it can easily classify as redundant sentences in which novel information is mixed with existing information (from collection a).
</prevsent>
</prevsection>
<citsent citstr=" N09-1041 ">
furthermore, while this approach can identify sentences that contain novel information, it cannot model explicitly what the novel information is. recently, bayesian models have successfully been applied to multi-document summarization showing state-of-the-art results in summarization competitions (haghighi and vanderwende, 2009; <papid> N09-1041 </papid>jin et al  2010).</citsent>
<aftsection>
<nextsent>these approaches offer clear and rigorous probabilistic interpretations that many other techniques lack.
</nextsent>
<nextsent>furthermore, they have the advantage of operating in unsupervised settings, which can be used in real-world scenarios, across domains and languages.
</nextsent>
<nextsent>to our best knowledge,previous work has not used this approach for up date summarization.in this article, we propose novel non parametric bayesian approach for update summarization.
</nextsent>
<nextsent>our approach, which is variation of latent 214 dirichlet allocation (lda) (blei et al  2003),aims to learn to distinguish between common information and novel information.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2962">
<title id=" E12-1022.xml">dualsum a topic model based approach for update summarization </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>usually, smoothing factor ? is applied on the candidate distribution in order to avoid the divergence to be undefined1.this objective function selects the most representative sentences of the collection, and at thesame time it also diversifies the generated summary by penalizing redundancy.
</prevsent>
<prevsent>since the problem of finding the subset of sentences from collection that minimizes the kl divergence is np-complete, greedy algorithm is often used inpractice2.
</prevsent>
</prevsection>
<citsent citstr=" W11-0507 ">
some variations of this objective function can be considered, such as penalizing sentences that contain document-specific topics (ma son and charniak, 2011) <papid> W11-0507 </papid>or rewarding sentences appearing closer to the beginning of the docu ment.wang et al (2009) <papid> P09-2075 </papid>propose bayesian approach for summarization that does not use klfor reranking.</citsent>
<aftsection>
<nextsent>in their model, bayesian sentence based topic models, every sentence in document is assumed to be associated to unique latent topic.
</nextsent>
<nextsent>once the model parameters have been calculated, summary is generated by choosing the sentence with the highest probability for each topic.
</nextsent>
<nextsent>while hierarchical topic modeling approaches have shown remarkable effectiveness in learning the latent topics of document collections, they are not designed to capture the novel information in collection with respect to another one, which is the primary focus of update summarization.
</nextsent>
<nextsent>2.2 update summarization.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2963">
<title id=" E12-1022.xml">dualsum a topic model based approach for update summarization </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>usually, smoothing factor ? is applied on the candidate distribution in order to avoid the divergence to be undefined1.this objective function selects the most representative sentences of the collection, and at thesame time it also diversifies the generated summary by penalizing redundancy.
</prevsent>
<prevsent>since the problem of finding the subset of sentences from collection that minimizes the kl divergence is np-complete, greedy algorithm is often used inpractice2.
</prevsent>
</prevsection>
<citsent citstr=" P09-2075 ">
some variations of this objective function can be considered, such as penalizing sentences that contain document-specific topics (ma son and charniak, 2011) <papid> W11-0507 </papid>or rewarding sentences appearing closer to the beginning of the docu ment.wang et al (2009) <papid> P09-2075 </papid>propose bayesian approach for summarization that does not use klfor reranking.</citsent>
<aftsection>
<nextsent>in their model, bayesian sentence based topic models, every sentence in document is assumed to be associated to unique latent topic.
</nextsent>
<nextsent>once the model parameters have been calculated, summary is generated by choosing the sentence with the highest probability for each topic.
</nextsent>
<nextsent>while hierarchical topic modeling approaches have shown remarkable effectiveness in learning the latent topics of document collections, they are not designed to capture the novel information in collection with respect to another one, which is the primary focus of update summarization.
</nextsent>
<nextsent>2.2 update summarization.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2966">
<title id=" E12-1022.xml">dualsum a topic model based approach for update summarization </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>2in our experiments, we follow the same approach as in(haghighi and vanderwende, 2009) <papid> N09-1041 </papid>by greedily adding sentences to summary so long as they decrease kl divergence.</prevsent>
<prevsent>215to collection as the base collection and to collection as the update collection.update summarization is related to novelty detection which can be defined as the problem of determining whether document contains new information given an existing collection (soboroffand harman, 2005).</prevsent>
</prevsection>
<citsent citstr=" N09-2029 ">
thus, while the goal of novelty detection is to determine whether some information is new, the goal of update summarization is to extract and synthesize the novel information.update summarization is also related to contrastive summarization, i.e. the problem of jointly generating summaries for two entities in order to highlight their differences (lerman and mcdonald, 2009).<papid> N09-2029 </papid></citsent>
<aftsection>
<nextsent>the primary difference here is that update summarization aims to extract novel or updated information in the update collection with respect to the base collection.the most common approach for update summarization is to apply normal multi-documentsummarizer, with some added functionality to remove sentences that are redundant with respect to collection a. this can be achieved using simple filtering rules (fisher and roark, 2008), maximal marginal relevance (boudin et al  2008), <papid> C08-2006 </papid>or more complex graph-based algorithms (shen and li, 2010; wenjie et al  2008).</nextsent>
<nextsent>the goal here is to boost sentences in that bring out completely novel information.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2967">
<title id=" E12-1022.xml">dualsum a topic model based approach for update summarization </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>215to collection as the base collection and to collection as the update collection.update summarization is related to novelty detection which can be defined as the problem of determining whether document contains new information given an existing collection (soboroffand harman, 2005).
</prevsent>
<prevsent>thus, while the goal of novelty detection is to determine whether some information is new, the goal of update summarization is to extract and synthesize the novel information.update summarization is also related to contrastive summarization, i.e. the problem of jointly generating summaries for two entities in order to highlight their differences (lerman and mcdonald, 2009).<papid> N09-2029 </papid></prevsent>
</prevsection>
<citsent citstr=" C08-2006 ">
the primary difference here is that update summarization aims to extract novel or updated information in the update collection with respect to the base collection.the most common approach for update summarization is to apply normal multi-documentsummarizer, with some added functionality to remove sentences that are redundant with respect to collection a. this can be achieved using simple filtering rules (fisher and roark, 2008), maximal marginal relevance (boudin et al  2008), <papid> C08-2006 </papid>or more complex graph-based algorithms (shen and li, 2010; wenjie et al  2008).</citsent>
<aftsection>
<nextsent>the goal here is to boost sentences in that bring out completely novel information.
</nextsent>
<nextsent>one problem with this approach is that it is likely to discard as redundant sentences in containing novel information if it is mixed with known information from collection a.another approach is to introduce specific features intended to capture the novelty in collection b. for example, comparing collections and b, fastsum derives features for the collection such as number of named entities in the sentence that already occurred in the old cluster or the number of new content words in the sentence not already mentioned in the old cluster that are subsequently used to train support vector machine classifier(schilder et al  2008).
</nextsent>
<nextsent>a limitation with this approach is there are no large training sets available and, the more features it has, the more it is affected by the sparsity of the training data.
</nextsent>
<nextsent>3.1 model formulation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2976">
<title id=" E06-2025.xml">theoretical evaluation of estimation methods for data oriented parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>either has no daughters or the same daughter nodes as i. in tree bank.
</prevsent>
<prevsent>the first such method is now known as dop1?
</prevsent>
</prevsection>
<citsent citstr=" E93-1006 ">
(bod, 1993).<papid> E93-1006 </papid></citsent>
<aftsection>
<nextsent>in combination with some heuristic constraints on the allowed subtrees, it has been remarkably successful on small treebanks.
</nextsent>
<nextsent>despite this empirical success, (johnson,2002) <papid> J02-1005 </papid>argued that it is inadequate because it is biased and inconsistent.</nextsent>
<nextsent>his criticism spearheaded number of other methods, including (bonnema et al, 1999; bod, 2003; <papid> E03-1005 </papid>simaan and buratto, 2003; zollmann and simaan, 2005), and will be the starting point of our analysis.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2977">
<title id=" E06-2025.xml">theoretical evaluation of estimation methods for data oriented parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(bod, 1993).<papid> E93-1006 </papid></prevsent>
<prevsent>in combination with some heuristic constraints on the allowed subtrees, it has been remarkably successful on small tree banks.</prevsent>
</prevsection>
<citsent citstr=" J02-1005 ">
despite this empirical success, (johnson,2002) <papid> J02-1005 </papid>argued that it is inadequate because it is biased and inconsistent.</citsent>
<aftsection>
<nextsent>his criticism spearheaded number of other methods, including (bonnema et al, 1999; bod, 2003; <papid> E03-1005 </papid>simaan and buratto, 2003; zollmann and simaan, 2005), and will be the starting point of our analysis.</nextsent>
<nextsent>as it turns out,the dop1 method really is biased and inconsistent, but not for the reasons johnson gives, and it really is inadequate, but not because it is biased and inconsistent.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2978">
<title id=" E06-2025.xml">theoretical evaluation of estimation methods for data oriented parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in combination with some heuristic constraints on the allowed subtrees, it has been remarkably successful on small treebanks.
</prevsent>
<prevsent>despite this empirical success, (johnson,2002) <papid> J02-1005 </papid>argued that it is inadequate because it is biased and inconsistent.</prevsent>
</prevsection>
<citsent citstr=" E03-1005 ">
his criticism spearheaded number of other methods, including (bonnema et al, 1999; bod, 2003; <papid> E03-1005 </papid>simaan and buratto, 2003; zollmann and simaan, 2005), and will be the starting point of our analysis.</citsent>
<aftsection>
<nextsent>as it turns out,the dop1 method really is biased and inconsistent, but not for the reasons johnson gives, and it really is inadequate, but not because it is biased and inconsistent.
</nextsent>
<nextsent>in this note, we further show that alternative methods that have been proposed, only partly remedy the problems with dop1, leaving weight estimation as an important open problem.
</nextsent>
<nextsent>the dop model and stsg formalism are described in detail elsewhere, for instance in (bod, 1998).
</nextsent>
<nextsent>the main difference with pcfgs is that multiple derivations, using elementary trees with variety of sizes, can yield the same parse tree.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2995">
<title id=" E89-1000.xml">current issues in computational lexical semantics </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>first, clear notion of semantic well-formedness will be necessary in order to characterize theory of possible word meaning.
</prevsent>
<prevsent>this may entail idealizing the notion of lexical meaning away from other semantic influences.
</prevsent>
</prevsection>
<citsent citstr=" T87-1006 ">
for instance, this might suggest that discourse and pragmatic factors should be handled ifferently or separately from the semantic ontributions of lexical items in composition (contra \[hobbs, 1987\].<papid> T87-1006 </papid></citsent>
<aftsection>
<nextsent>although this is not necessary assumption and may in fact be wrong, it may help narrow our focus on what is important for lexical semantic descriptions.
</nextsent>
<nextsent>secondly, lexical semantics must look for representations that are richer than thematic role descriptions \[fillmore, 1968\].
</nextsent>
<nextsent>as argued in \[levin and rappaport, 1986\], named roles are useful, at best, for establishing fairly general mapping strategies to the syntactic structures in language.
</nextsent>
<nextsent>the distinctions possible with  theta  roles are much too coarse-grained to provide useful semantic interpretation of sentence.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2996">
<title id=" E06-3001.xml">whats there to talk about a multimodal model of referring behavior in the presence of shared visual information </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>together this work suggests that the interlocu tors?
</prevsent>
<prevsent>shared visual context has major impact on their patterns of referring behavior.
</prevsent>
</prevsection>
<citsent citstr=" P87-1022 ">
yet, number of discourse-based models of reference primarily relyon linguistic information without regard to the surrounding visual environment (e.g., see brennan et al, 1987; <papid> P87-1022 </papid>hobbs, 1978; poesio et al., 2004; <papid> J04-3003 </papid>strube, 1998; <papid> P98-2204 </papid>tetreault, 2005).</citsent>
<aftsection>
<nextsent>recently, multi-modal models have emerged that integrate visual information into the resolution process.
</nextsent>
<nextsent>however, many of these models are restricted by their simplifying assumption of communication via command language.
</nextsent>
<nextsent>thus, their approaches apply to explicit interaction techniques but do not necessarily support more general communication in the presence of shared visual information (e.g., see chai et al, 2005; huls et al, 1995; <papid> J95-1003 </papid>kehler, 2000).</nextsent>
<nextsent>it is the goal of the work presented in this paper to explore the performance of language based models of reference resolution in contexts where speakers share common visual space.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2997">
<title id=" E06-3001.xml">whats there to talk about a multimodal model of referring behavior in the presence of shared visual information </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>together this work suggests that the interlocu tors?
</prevsent>
<prevsent>shared visual context has major impact on their patterns of referring behavior.
</prevsent>
</prevsection>
<citsent citstr=" J04-3003 ">
yet, number of discourse-based models of reference primarily relyon linguistic information without regard to the surrounding visual environment (e.g., see brennan et al, 1987; <papid> P87-1022 </papid>hobbs, 1978; poesio et al., 2004; <papid> J04-3003 </papid>strube, 1998; <papid> P98-2204 </papid>tetreault, 2005).</citsent>
<aftsection>
<nextsent>recently, multi-modal models have emerged that integrate visual information into the resolution process.
</nextsent>
<nextsent>however, many of these models are restricted by their simplifying assumption of communication via command language.
</nextsent>
<nextsent>thus, their approaches apply to explicit interaction techniques but do not necessarily support more general communication in the presence of shared visual information (e.g., see chai et al, 2005; huls et al, 1995; <papid> J95-1003 </papid>kehler, 2000).</nextsent>
<nextsent>it is the goal of the work presented in this paper to explore the performance of language based models of reference resolution in contexts where speakers share common visual space.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I2998">
<title id=" E06-3001.xml">whats there to talk about a multimodal model of referring behavior in the presence of shared visual information </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>together this work suggests that the interlocu tors?
</prevsent>
<prevsent>shared visual context has major impact on their patterns of referring behavior.
</prevsent>
</prevsection>
<citsent citstr=" P98-2204 ">
yet, number of discourse-based models of reference primarily relyon linguistic information without regard to the surrounding visual environment (e.g., see brennan et al, 1987; <papid> P87-1022 </papid>hobbs, 1978; poesio et al., 2004; <papid> J04-3003 </papid>strube, 1998; <papid> P98-2204 </papid>tetreault, 2005).</citsent>
<aftsection>
<nextsent>recently, multi-modal models have emerged that integrate visual information into the resolution process.
</nextsent>
<nextsent>however, many of these models are restricted by their simplifying assumption of communication via command language.
</nextsent>
<nextsent>thus, their approaches apply to explicit interaction techniques but do not necessarily support more general communication in the presence of shared visual information (e.g., see chai et al, 2005; huls et al, 1995; <papid> J95-1003 </papid>kehler, 2000).</nextsent>
<nextsent>it is the goal of the work presented in this paper to explore the performance of language based models of reference resolution in contexts where speakers share common visual space.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3000">
<title id=" E06-3001.xml">whats there to talk about a multimodal model of referring behavior in the presence of shared visual information </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>recently, multi-modal models have emerged that integrate visual information into the resolution process.
</prevsent>
<prevsent>however, many of these models are restricted by their simplifying assumption of communication via command language.
</prevsent>
</prevsection>
<citsent citstr=" J95-1003 ">
thus, their approaches apply to explicit interaction techniques but do not necessarily support more general communication in the presence of shared visual information (e.g., see chai et al, 2005; huls et al, 1995; <papid> J95-1003 </papid>kehler, 2000).</citsent>
<aftsection>
<nextsent>it is the goal of the work presented in this paper to explore the performance of language based models of reference resolution in contexts where speakers share common visual space.
</nextsent>
<nextsent>in particular, we examine three basic hypotheses 7regarding the likely impact of linguistic and visual salience on referring behavior.
</nextsent>
<nextsent>the first hypothesis suggests that visual information is disregarded and that linguistic context provides sufficient information to describe referring behavior.
</nextsent>
<nextsent>the second hypothesis suggests that visual salience overrides any linguistic salience in governing referring behavior.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3001">
<title id=" E06-3001.xml">whats there to talk about a multimodal model of referring behavior in the presence of shared visual information </title>
<section> motivation.  </section>
<citcontext>
<prevsection>
<prevsent>second, such model can be applied to the development of range of technologies to support distributed group collaboration and mediated communication.
</prevsent>
<prevsent>finally, such model can be used to provide deeper theoretical understanding of how humans make use of various forms of shared visual information in their every day communication.
</prevsent>
</prevsection>
<citsent citstr=" P05-3022 ">
the development of an integrated multi-modal model of referring behavior can improve the performance of state-of-the-art computational models of communication currently used to support conversational interactions with an intelligent agent (allen et al, 2005; <papid> P05-3022 </papid>devault et al, 2005; <papid> P05-3001 </papid>gorniak &amp; roy, 2004).</citsent>
<aftsection>
<nextsent>many of these models relyon discourse state and prior linguistic contributions to successfully resolve references in given utterance.
</nextsent>
<nextsent>however, recent technological advances have created opportunities for human human and human-agent interactions in wide variety of contexts that include visual objects of interest.
</nextsent>
<nextsent>such systems may benefit from data driven model of how collaborative pairs adapt their language in the presence (or absence) of shared visual information.
</nextsent>
<nextsent>a successful computational model of referring behavior in the presence of visual information could enable agents to emulate many elements of more natural and realistic human conversational behavior.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3002">
<title id=" E06-3001.xml">whats there to talk about a multimodal model of referring behavior in the presence of shared visual information </title>
<section> motivation.  </section>
<citcontext>
<prevsection>
<prevsent>second, such model can be applied to the development of range of technologies to support distributed group collaboration and mediated communication.
</prevsent>
<prevsent>finally, such model can be used to provide deeper theoretical understanding of how humans make use of various forms of shared visual information in their every day communication.
</prevsent>
</prevsection>
<citsent citstr=" P05-3001 ">
the development of an integrated multi-modal model of referring behavior can improve the performance of state-of-the-art computational models of communication currently used to support conversational interactions with an intelligent agent (allen et al, 2005; <papid> P05-3022 </papid>devault et al, 2005; <papid> P05-3001 </papid>gorniak &amp; roy, 2004).</citsent>
<aftsection>
<nextsent>many of these models relyon discourse state and prior linguistic contributions to successfully resolve references in given utterance.
</nextsent>
<nextsent>however, recent technological advances have created opportunities for human human and human-agent interactions in wide variety of contexts that include visual objects of interest.
</nextsent>
<nextsent>such systems may benefit from data driven model of how collaborative pairs adapt their language in the presence (or absence) of shared visual information.
</nextsent>
<nextsent>a successful computational model of referring behavior in the presence of visual information could enable agents to emulate many elements of more natural and realistic human conversational behavior.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3003">
<title id=" E06-3001.xml">whats there to talk about a multimodal model of referring behavior in the presence of shared visual information </title>
<section> background and related work.  </section>
<citcontext>
<prevsection>
<prevsent>having computational description of these processes can provide insight into why they occur, can expose implicit and possibly inadequate simplifying assumptions underlying existing 8 theoretical models, and can serve as guide for future empirical research.
</prevsent>
<prevsent>a review of the computational linguistics literature reveals number of discourse models that describe referring behaviors in written, and to lesser extent, spoken discourse (for recent review see tetreault, 2005).
</prevsent>
</prevsection>
<citsent citstr=" J95-2003 ">
these include models based primarily on world knowledge (e.g., hobbs et al, 1993), syntax-based methods (hobbs, 1978), and those that integrate combination of syntax, semantics and discourse structure (e.g., grosz et al, 1995; <papid> J95-2003 </papid>strube, 1998; <papid> P98-2204 </papid>tetreault, 2001).<papid> J01-4003 </papid></citsent>
<aftsection>
<nextsent>the majority of these models are salience-based approaches where entities are ranked according to their grammatical function, number of prior mentions, prosodic markers, etc. in typical language-based models of reference resolution, the licensed referents are introduced through utterances in the prior linguistic context.
</nextsent>
<nextsent>consider the following example drawn from the puzzle corpus1 whereby helper?
</nextsent>
<nextsent>describes to worker?
</nextsent>
<nextsent>how to construct an arrangement of colored blocks so they match solution only the helper has visual access to: (1) helper: take the dark red piece.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3007">
<title id=" E06-3001.xml">whats there to talk about a multimodal model of referring behavior in the presence of shared visual information </title>
<section> background and related work.  </section>
<citcontext>
<prevsection>
<prevsent>having computational description of these processes can provide insight into why they occur, can expose implicit and possibly inadequate simplifying assumptions underlying existing 8 theoretical models, and can serve as guide for future empirical research.
</prevsent>
<prevsent>a review of the computational linguistics literature reveals number of discourse models that describe referring behaviors in written, and to lesser extent, spoken discourse (for recent review see tetreault, 2005).
</prevsent>
</prevsection>
<citsent citstr=" J01-4003 ">
these include models based primarily on world knowledge (e.g., hobbs et al, 1993), syntax-based methods (hobbs, 1978), and those that integrate combination of syntax, semantics and discourse structure (e.g., grosz et al, 1995; <papid> J95-2003 </papid>strube, 1998; <papid> P98-2204 </papid>tetreault, 2001).<papid> J01-4003 </papid></citsent>
<aftsection>
<nextsent>the majority of these models are salience-based approaches where entities are ranked according to their grammatical function, number of prior mentions, prosodic markers, etc. in typical language-based models of reference resolution, the licensed referents are introduced through utterances in the prior linguistic context.
</nextsent>
<nextsent>consider the following example drawn from the puzzle corpus1 whereby helper?
</nextsent>
<nextsent>describes to worker?
</nextsent>
<nextsent>how to construct an arrangement of colored blocks so they match solution only the helper has visual access to: (1) helper: take the dark red piece.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3010">
<title id=" E06-3001.xml">whats there to talk about a multimodal model of referring behavior in the presence of shared visual information </title>
<section> preliminary model overviews.  </section>
<citcontext>
<prevsection>
<prevsent>task condition corpus statistics dialogues contributions words pronouns no shared visual information 10 218 1181 30 shared visual information 10 174 938 39 total 20 392 2119 69 table 1.
</prevsent>
<prevsent>overview of the data used.
</prevsent>
</prevsection>
<citsent citstr=" J86-3001 ">
the models evaluated in this paper are based on centering theory (grosz et al, 1995; <papid> J95-2003 </papid>grosz &amp; sidner, 1986) <papid> J86-3001 </papid>and the algorithms devised by brennan and colleagues (1987) and adapted by tetreault (2001).<papid> J01-4003 </papid></citsent>
<aftsection>
<nextsent>we examine language-only model based on tetreaults left-right centering (lrc) model, visual-only model that uses measure of visual salience to rank the objects in the visual field as possible referential anchors, and an integrated model that balances the visual information along with the linguistic information to generate ranked list of possible anchors.
</nextsent>
<nextsent>5.1 the language-only model.
</nextsent>
<nextsent>we chose the lrc algorithm (tetreault, 2001) <papid> J01-4003 </papid>to serve as the basis for our language-only model.</nextsent>
<nextsent>it has been shown to farewell on task-oriented spoken dialogues (tetreault, 2005) and was easily adapted to the puzzle corpus data.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3014">
<title id=" E09-1069.xml">deterministic shift reduce parsing for unification based grammars by using default unification </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>therefore, it is developed by using default unification, which almost always succeeds in unification by overwriting inconsistent constraints ingram mars.
</prevsent>
<prevsent>over the last few decades, probabilistic unification-based grammar parsing has been investigated intensively.
</prevsent>
</prevsection>
<citsent citstr=" J97-4005 ">
previous studies (abney, 1997; <papid> J97-4005 </papid>johnson et al, 1999; kaplan et al, 2004; malouf and van noord, 2004; miyao and tsujii, 2005; riezler et al, 2000) defined probabilistic model of unification-based grammars, including head-driven phrase structure grammar (hpsg), lexical functional grammar (lfg) and combina tory categorial grammar (ccg), as maximum entropy model (berger et al, 1996).</citsent>
<aftsection>
<nextsent>geman and johnson (geman and johnson, 2002) and miyao and tsujii (miyao and tsujii, 2002) proposed feature forest, which is dynamic programming algorithm for estimating the probabilities of all possible parse candidates.
</nextsent>
<nextsent>a feature forest can estimate the model parameters without unpacking the parse forest, i.e., the chart and its edges.
</nextsent>
<nextsent>feature forests have been used successfully for probabilistic hpsg and ccg (clark and curran, 2004b; miyao and tsujii, 2005), and its parsing is empirically known to be fast and accurate, especially with super tagging (clark and curran, 2004a; ninomiya et al, 2007; ninomiya et al, 2006).
</nextsent>
<nextsent>both estimation and parsing with the packed parse forest, however, have several inherent problems deriving from the restriction of locality.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3015">
<title id=" E12-3007.xml">discourse type clustering using pos ngram profiles and high dimensional embed dings </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>to cluster the latter, n-gram pos (part-of-speech) tag profiles were extracted (section 2.3).
</prevsent>
<prevsent>pos-tags were chosen because of their expected relation to discourse types.
</prevsent>
</prevsection>
<citsent citstr=" C94-2174 ">
several authors have used pos-tags among other features for various text classification tasks,such as biber (1988) for text type detection, karlgren and cutting (1994) <papid> C94-2174 </papid>and malrieu and rastier 1sequence type is an appropriate name, because it refers to text passage type.</citsent>
<aftsection>
<nextsent>however, it will be further mentioned as discourse types, frequent french term.
</nextsent>
<nextsent>in english, standard term is: discourse modes.
</nextsent>
<nextsent>(2001) for genre classification, and palmer et al(2007) <papid> P07-1113 </papid>for situation entity classification.</nextsent>
<nextsent>the latter is an essential component of english discourse modes (smith, 2009).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3016">
<title id=" E12-3007.xml">discourse type clustering using pos ngram profiles and high dimensional embed dings </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, it will be further mentioned as discourse types, frequent french term.
</prevsent>
<prevsent>in english, standard term is: discourse modes.
</prevsent>
</prevsection>
<citsent citstr=" P07-1113 ">
(2001) for genre classification, and palmer et al(2007) <papid> P07-1113 </papid>for situation entity classification.</citsent>
<aftsection>
<nextsent>the latter is an essential component of english discourse modes (smith, 2009).
</nextsent>
<nextsent>moreover, previous work in discourse type detection has shown dependency between pos-tags and these types (cocco et al, 2011).in this paper, k-means algorithm with high dimensional embed dings and fuzzy clustering algorithm were applied on uni-, bi- and trigram pos-tag profiles (section 2.4) and results were evaluated (section 2.5).
</nextsent>
<nextsent>finally, results are given in section 3.
</nextsent>
<nextsent>2.1 expert assessment.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3017">
<title id=" E09-1064.xml">evaluating the inferential utility of lexical semantic resources </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>further, our results stress the need to include auxiliary information regarding the lexical and logical contexts in which lexical inference is valid, as well as its prior validity likelihood.
</prevsent>
<prevsent>lexical information plays major role in semantic inference, as the meaning of one term is often inferred form another.
</prevsent>
</prevsection>
<citsent citstr=" W07-1401 ">
lexical-semantic resources, which provide the needed knowledge for lexical inference, are commonly utilized by applied inference systems (giampiccolo et al, 2007) <papid> W07-1401 </papid>and applications such as information retrieval and question answering (shah and croft, 2004; pascaand harabagiu, 2001).</citsent>
<aftsection>
<nextsent>beyond wordnet (fell baum, 1998), wide range of resources has been developed and utilized, including extensions to wordnet (moldovan andrus, 2001; <papid> P01-1052 </papid>snow et al,2006) <papid> P06-1101 </papid>and resources based on automatic distributional similarity methods (lin, 1998; <papid> P98-2127 </papid>pant eland lin, 2002).</nextsent>
<nextsent>recently, wikipedia is emerging as source for extracting semantic relationships (suchanek et al, 2007; kazama and torisawa, 2007).<papid> D07-1073 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3018">
<title id=" E09-1064.xml">evaluating the inferential utility of lexical semantic resources </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>lexical information plays major role in semantic inference, as the meaning of one term is often inferred form another.
</prevsent>
<prevsent>lexical-semantic resources, which provide the needed knowledge for lexical inference, are commonly utilized by applied inference systems (giampiccolo et al, 2007) <papid> W07-1401 </papid>and applications such as information retrieval and question answering (shah and croft, 2004; pascaand harabagiu, 2001).</prevsent>
</prevsection>
<citsent citstr=" P01-1052 ">
beyond wordnet (fell baum, 1998), wide range of resources has been developed and utilized, including extensions to wordnet (moldovan andrus, 2001; <papid> P01-1052 </papid>snow et al,2006) <papid> P06-1101 </papid>and resources based on automatic distributional similarity methods (lin, 1998; <papid> P98-2127 </papid>pant eland lin, 2002).</citsent>
<aftsection>
<nextsent>recently, wikipedia is emerging as source for extracting semantic relationships (suchanek et al, 2007; kazama and torisawa, 2007).<papid> D07-1073 </papid></nextsent>
<nextsent>as of today, only partial comparative picture is available regarding the actual utility and limitations of available resources for lexical-semantic inference.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3019">
<title id=" E09-1064.xml">evaluating the inferential utility of lexical semantic resources </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>lexical information plays major role in semantic inference, as the meaning of one term is often inferred form another.
</prevsent>
<prevsent>lexical-semantic resources, which provide the needed knowledge for lexical inference, are commonly utilized by applied inference systems (giampiccolo et al, 2007) <papid> W07-1401 </papid>and applications such as information retrieval and question answering (shah and croft, 2004; pascaand harabagiu, 2001).</prevsent>
</prevsection>
<citsent citstr=" P06-1101 ">
beyond wordnet (fell baum, 1998), wide range of resources has been developed and utilized, including extensions to wordnet (moldovan andrus, 2001; <papid> P01-1052 </papid>snow et al,2006) <papid> P06-1101 </papid>and resources based on automatic distributional similarity methods (lin, 1998; <papid> P98-2127 </papid>pant eland lin, 2002).</citsent>
<aftsection>
<nextsent>recently, wikipedia is emerging as source for extracting semantic relationships (suchanek et al, 2007; kazama and torisawa, 2007).<papid> D07-1073 </papid></nextsent>
<nextsent>as of today, only partial comparative picture is available regarding the actual utility and limitations of available resources for lexical-semantic inference.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3020">
<title id=" E09-1064.xml">evaluating the inferential utility of lexical semantic resources </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>lexical information plays major role in semantic inference, as the meaning of one term is often inferred form another.
</prevsent>
<prevsent>lexical-semantic resources, which provide the needed knowledge for lexical inference, are commonly utilized by applied inference systems (giampiccolo et al, 2007) <papid> W07-1401 </papid>and applications such as information retrieval and question answering (shah and croft, 2004; pascaand harabagiu, 2001).</prevsent>
</prevsection>
<citsent citstr=" P98-2127 ">
beyond wordnet (fell baum, 1998), wide range of resources has been developed and utilized, including extensions to wordnet (moldovan andrus, 2001; <papid> P01-1052 </papid>snow et al,2006) <papid> P06-1101 </papid>and resources based on automatic distributional similarity methods (lin, 1998; <papid> P98-2127 </papid>pant eland lin, 2002).</citsent>
<aftsection>
<nextsent>recently, wikipedia is emerging as source for extracting semantic relationships (suchanek et al, 2007; kazama and torisawa, 2007).<papid> D07-1073 </papid></nextsent>
<nextsent>as of today, only partial comparative picture is available regarding the actual utility and limitations of available resources for lexical-semantic inference.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3021">
<title id=" E09-1064.xml">evaluating the inferential utility of lexical semantic resources </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>lexical-semantic resources, which provide the needed knowledge for lexical inference, are commonly utilized by applied inference systems (giampiccolo et al, 2007) <papid> W07-1401 </papid>and applications such as information retrieval and question answering (shah and croft, 2004; pascaand harabagiu, 2001).</prevsent>
<prevsent>beyond wordnet (fell baum, 1998), wide range of resources has been developed and utilized, including extensions to wordnet (moldovan andrus, 2001; <papid> P01-1052 </papid>snow et al,2006) <papid> P06-1101 </papid>and resources based on automatic distributional similarity methods (lin, 1998; <papid> P98-2127 </papid>pant eland lin, 2002).</prevsent>
</prevsection>
<citsent citstr=" D07-1073 ">
recently, wikipedia is emerging as source for extracting semantic relationships (suchanek et al, 2007; kazama and torisawa, 2007).<papid> D07-1073 </papid></citsent>
<aftsection>
<nextsent>as of today, only partial comparative picture is available regarding the actual utility and limitations of available resources for lexical-semantic inference.
</nextsent>
<nextsent>works that do provide quantitative information regarding resources utility have focused on few particular resources (kouylekov and magnini, 2006; roth and sammons, 2007) <papid> W07-1418 </papid>and evaluated their impact on specific system.</nextsent>
<nextsent>most often, works which utilized lexical resources donot provide information about their isolated contribution; rather, they only report overall performance for systems in which lexical resources serve as components.our paper provides step towards clarifying this picture.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3022">
<title id=" E09-1064.xml">evaluating the inferential utility of lexical semantic resources </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>recently, wikipedia is emerging as source for extracting semantic relationships (suchanek et al, 2007; kazama and torisawa, 2007).<papid> D07-1073 </papid></prevsent>
<prevsent>as of today, only partial comparative picture is available regarding the actual utility and limitations of available resources for lexical-semantic inference.</prevsent>
</prevsection>
<citsent citstr=" W07-1418 ">
works that do provide quantitative information regarding resources utility have focused on few particular resources (kouylekov and magnini, 2006; roth and sammons, 2007) <papid> W07-1418 </papid>and evaluated their impact on specific system.</citsent>
<aftsection>
<nextsent>most often, works which utilized lexical resources donot provide information about their isolated contribution; rather, they only report overall performance for systems in which lexical resources serve as components.our paper provides step towards clarifying this picture.
</nextsent>
<nextsent>we propose system- and application-independent evaluation methodology that isolates resources?
</nextsent>
<nextsent>performance, and systematically apply it to seven prominent lexical semantic resources.
</nextsent>
<nextsent>the evaluation and analysis methodology is specified within the textual entailment framework, which has become popular in recent years for modeling practical semantic inference in generic manner (dagan and glickman, 2004).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3024">
<title id=" E09-1064.xml">evaluating the inferential utility of lexical semantic resources </title>
<section> sub-sentential textual entailment.  </section>
<citcontext>
<prevsection>
<prevsent>2.1 entailment of sub-sentential hypotheses.
</prevsent>
<prevsent>we first seek definition that would capture the entailment relationship between text and sub sentential hypothesis.
</prevsent>
</prevsection>
<citsent citstr=" W06-1621 ">
a similar goal was addressed in (glickman et al, 2006), <papid> W06-1621 </papid>who defined the notion of lexical reference to model the fact that in order to entail hypothesis, the text has to entail each non-compositional lexical element within it.</citsent>
<aftsection>
<nextsent>we suggest that slight adaptation of their definition is suitable to capture the notion of entailment for any sub-sentential hypotheses, including compositional ones:definition 1 sub-sentential hypothesis is entailed by text if there is an explicit or implied reference in to possible meaning of h. for example, the sentence crude steel output is likely to fall in 2000?
</nextsent>
<nextsent>entails the sub-sentential hypotheses production, steel production and steel output decrease.
</nextsent>
<nextsent>glickman et al, achieving good inter-annotatoragreement, empirically found that almost all non compositional terms in an entailed sentential hypothesis are indeed referenced in the entailing text.
</nextsent>
<nextsent>this finding suggests that the above definition is consistent with the original definition of textual entailment for sentential hypotheses and can thus model compositional entailment inferences.we use this definition in our annotation methodology described in section 3.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3025">
<title id=" E09-1064.xml">evaluating the inferential utility of lexical semantic resources </title>
<section> experimental setting.  </section>
<citcontext>
<prevsection>
<prevsent>the kappa scores for inter-annotator agreement were 0.74 and 0.64 for judging h?
</prevsent>
<prevsent>andh, respectively.
</prevsent>
</prevsection>
<citsent citstr=" P07-1058 ">
these figures correspond to substantial agreement (landis and koch, 1997) and are comparable with related semantic annotations (szpektor et al, 2007; <papid> P07-1058 </papid>bhagat et al, 2007).<papid> D07-1017 </papid></citsent>
<aftsection>
<nextsent>4.2 lexical-semantic resources.
</nextsent>
<nextsent>we evaluated the following resources: wordnet (wnd): there is no clear agreement regarding which set of wordnet relations is useful for entailment inference.
</nextsent>
<nextsent>we therefore took conservative approach using only synonymy and hyponymy rules, which typically comply with the lexical entailment relation and are commonly used by textual entailment systems, e.g.
</nextsent>
<nextsent>(herrera et al, 2005; bos and markert, 2006).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3026">
<title id=" E09-1064.xml">evaluating the inferential utility of lexical semantic resources </title>
<section> experimental setting.  </section>
<citcontext>
<prevsection>
<prevsent>the kappa scores for inter-annotator agreement were 0.74 and 0.64 for judging h?
</prevsent>
<prevsent>andh, respectively.
</prevsent>
</prevsection>
<citsent citstr=" D07-1017 ">
these figures correspond to substantial agreement (landis and koch, 1997) and are comparable with related semantic annotations (szpektor et al, 2007; <papid> P07-1058 </papid>bhagat et al, 2007).<papid> D07-1017 </papid></citsent>
<aftsection>
<nextsent>4.2 lexical-semantic resources.
</nextsent>
<nextsent>we evaluated the following resources: wordnet (wnd): there is no clear agreement regarding which set of wordnet relations is useful for entailment inference.
</nextsent>
<nextsent>we therefore took conservative approach using only synonymy and hyponymy rules, which typically comply with the lexical entailment relation and are commonly used by textual entailment systems, e.g.
</nextsent>
<nextsent>(herrera et al, 2005; bos and markert, 2006).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3033">
<title id=" E09-1064.xml">evaluating the inferential utility of lexical semantic resources </title>
<section> results and analysis.  </section>
<citcontext>
<prevsection>
<prevsent>somewhat indicative, yet mostly indirect, information about rules?
</prevsent>
<prevsent>priors is contained in some resources.
</prevsent>
</prevsection>
<citsent citstr=" H93-1061 ">
this includes sense ranks in wordnet,semcor statistics (miller et al, 1993), <papid> H93-1061 </papid>and similarity scores and rankings in lins resources.</citsent>
<aftsection>
<nextsent>inference systems often incorporated this information,typically as top-k or threshold-based filters (pan tel and lin, 2003; <papid> N03-4011 </papid>roth and sammons, 2007).<papid> W07-1418 </papid></nextsent>
<nextsent>by empirically assessing the effect of several such filters in our setting, we found that this type of data is indeed informative in the sense that precision increases as the threshold rises.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3034">
<title id=" E09-1064.xml">evaluating the inferential utility of lexical semantic resources </title>
<section> results and analysis.  </section>
<citcontext>
<prevsection>
<prevsent>priors is contained in some resources.
</prevsent>
<prevsent>this includes sense ranks in wordnet,semcor statistics (miller et al, 1993), <papid> H93-1061 </papid>and similarity scores and rankings in lins resources.</prevsent>
</prevsection>
<citsent citstr=" N03-4011 ">
inference systems often incorporated this information,typically as top-k or threshold-based filters (pan tel and lin, 2003; <papid> N03-4011 </papid>roth and sammons, 2007).<papid> W07-1418 </papid></citsent>
<aftsection>
<nextsent>by empirically assessing the effect of several such filters in our setting, we found that this type of data is indeed informative in the sense that precision increases as the threshold rises.
</nextsent>
<nextsent>yet, no specific filters were found to improve results in terms of f1 score (where recall is measured relatively tothe yield of the unfiltered resource) due to significant drop in relative recall.
</nextsent>
<nextsent>for example, lin 564 prox loses more than 40% of its recall when only the top-50 rules for each hypothesis are exploited,and using only the first sense of wnd costs there source over 60% of its recall.
</nextsent>
<nextsent>we thus suggest abetter strategy might be to combine the prior information with context matching scores in orderto obtain overall likelihood scores for rule applications, as in (szpektor et al, 2008).<papid> P08-1078 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3037">
<title id=" E09-1064.xml">evaluating the inferential utility of lexical semantic resources </title>
<section> results and analysis.  </section>
<citcontext>
<prevsection>
<prevsent>yet, no specific filters were found to improve results in terms of f1 score (where recall is measured relatively tothe yield of the unfiltered resource) due to significant drop in relative recall.
</prevsent>
<prevsent>for example, lin 564 prox loses more than 40% of its recall when only the top-50 rules for each hypothesis are exploited,and using only the first sense of wnd costs there source over 60% of its recall.
</prevsent>
</prevsection>
<citsent citstr=" P08-1078 ">
we thus suggest abetter strategy might be to combine the prior information with context matching scores in orderto obtain overall likelihood scores for rule applications, as in (szpektor et al, 2008).<papid> P08-1078 </papid></citsent>
<aftsection>
<nextsent>furthermore,resources should include explicit information regarding the prior likelihoods of of their rules.
</nextsent>
<nextsent>5.5 operative conclusions.
</nextsent>
<nextsent>our findings highlight the currently limited recall of available resources for lexical inference.
</nextsent>
<nextsent>the higher recall of lins resources indicates that many more entailment relationships can be acquired, particularly when considering distributional evidence.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3038">
<title id=" E09-1064.xml">evaluating the inferential utility of lexical semantic resources </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>for instance, the existence of country implies the existence of its flag.
</prevsent>
<prevsent>yet, the meaning of flag is typically not implied by country.
</prevsent>
</prevsection>
<citsent citstr=" N07-1071 ">
previous works assessing rule application via human annotation include (pantel et al, 2007; <papid> N07-1071 </papid>szpektor et al, 2007), <papid> P07-1058 </papid>which evaluate acquisition methods for lexical-syntactic rules.</citsent>
<aftsection>
<nextsent>they posed an additional question to the annotators asking them to filter out invalid contexts.
</nextsent>
<nextsent>in our methodology implicit context matching for the full hypothesis was applied instead.
</nextsent>
<nextsent>other related instance-based evaluations (giuliano and gliozzo, 2007; <papid> D07-1026 </papid>connor and roth, 2007) performed lexical substitutions, but did not handle the non-substitutable cases.</nextsent>
<nextsent>this paper provides several methodological and empirical contributions.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3040">
<title id=" E09-1064.xml">evaluating the inferential utility of lexical semantic resources </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>they posed an additional question to the annotators asking them to filter out invalid contexts.
</prevsent>
<prevsent>in our methodology implicit context matching for the full hypothesis was applied instead.
</prevsent>
</prevsection>
<citsent citstr=" D07-1026 ">
other related instance-based evaluations (giuliano and gliozzo, 2007; <papid> D07-1026 </papid>connor and roth, 2007) performed lexical substitutions, but did not handle the non-substitutable cases.</citsent>
<aftsection>
<nextsent>this paper provides several methodological and empirical contributions.
</nextsent>
<nextsent>we presented novel evaluation methodology for the utility of lexical semantic resources for semantic inference.
</nextsent>
<nextsent>to thatend we proposed definitions for entailment at sub sentential levels, addressing gap in the textual entailment framework.
</nextsent>
<nextsent>our evaluation and analysis provide first quantitative comparative assessment of the isolated utility of range of prominent potential resources for entailment rules.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3041">
<title id=" E09-1057.xml">language independent bilingual terminology extraction from a multilingual parallel corpus </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>comparisons with standard terminology extraction programs show an improvement of up to 20% for bilingual terminology extraction and competitive results (85% to90% accuracy) for monolingual terminology extraction, and reveal that the linguistically based alignment module is particularly well suited for the extraction of complex multiword terms.
</prevsent>
<prevsent>automatic term recognition (atr) systems are usually categorized into two main families.
</prevsent>
</prevsection>
<citsent citstr=" C94-2167 ">
on the one hand, the linguistically-based or rule-based approaches use linguistic information such as pos tags, chunk information, etc. to filter out stop words and restrict candidate terms to predefined syntactic patterns (ananiadou, 1994), (<papid> C94-2167 </papid>dagan and church, 1994).<papid> A94-1006 </papid></citsent>
<aftsection>
<nextsent>on the other hand, the statistical corpus-based approaches select n-gram sequences as candidate terms that are filtered by means of statistical measures.
</nextsent>
<nextsent>more recent atr systems use hybrid approaches that combine both linguistic and statistical information (frantzi and ananiadou, 1999).
</nextsent>
<nextsent>most bilingual terminology extraction systems first identify candidate terms in the source language based on predefined source patterns, and then select translation candidates for these terms in the target language (kupiec, 1993).<papid> P93-1003 </papid>we present an alternative approach that generates candidate terms directly from the aligned words and phrases in our parallel corpus.</nextsent>
<nextsent>in second step, we use frequency information of general purpose corpus and the n-gram frequencies of the automotive corpus to determine the term specificity.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3042">
<title id=" E09-1057.xml">language independent bilingual terminology extraction from a multilingual parallel corpus </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>comparisons with standard terminology extraction programs show an improvement of up to 20% for bilingual terminology extraction and competitive results (85% to90% accuracy) for monolingual terminology extraction, and reveal that the linguistically based alignment module is particularly well suited for the extraction of complex multiword terms.
</prevsent>
<prevsent>automatic term recognition (atr) systems are usually categorized into two main families.
</prevsent>
</prevsection>
<citsent citstr=" A94-1006 ">
on the one hand, the linguistically-based or rule-based approaches use linguistic information such as pos tags, chunk information, etc. to filter out stop words and restrict candidate terms to predefined syntactic patterns (ananiadou, 1994), (<papid> C94-2167 </papid>dagan and church, 1994).<papid> A94-1006 </papid></citsent>
<aftsection>
<nextsent>on the other hand, the statistical corpus-based approaches select n-gram sequences as candidate terms that are filtered by means of statistical measures.
</nextsent>
<nextsent>more recent atr systems use hybrid approaches that combine both linguistic and statistical information (frantzi and ananiadou, 1999).
</nextsent>
<nextsent>most bilingual terminology extraction systems first identify candidate terms in the source language based on predefined source patterns, and then select translation candidates for these terms in the target language (kupiec, 1993).<papid> P93-1003 </papid>we present an alternative approach that generates candidate terms directly from the aligned words and phrases in our parallel corpus.</nextsent>
<nextsent>in second step, we use frequency information of general purpose corpus and the n-gram frequencies of the automotive corpus to determine the term specificity.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3043">
<title id=" E09-1057.xml">language independent bilingual terminology extraction from a multilingual parallel corpus </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>on the other hand, the statistical corpus-based approaches select n-gram sequences as candidate terms that are filtered by means of statistical measures.
</prevsent>
<prevsent>more recent atr systems use hybrid approaches that combine both linguistic and statistical information (frantzi and ananiadou, 1999).
</prevsent>
</prevsection>
<citsent citstr=" P93-1003 ">
most bilingual terminology extraction systems first identify candidate terms in the source language based on predefined source patterns, and then select translation candidates for these terms in the target language (kupiec, 1993).<papid> P93-1003 </papid>we present an alternative approach that generates candidate terms directly from the aligned words and phrases in our parallel corpus.</citsent>
<aftsection>
<nextsent>in second step, we use frequency information of general purpose corpus and the n-gram frequencies of the automotive corpus to determine the term specificity.
</nextsent>
<nextsent>our approach is more flexible in the sense that we do not first generate candidate terms based on language-dependent predefined pos patterns (e.g. for french, n, prep n, and nadj are typical patterns), but immediately link linguistically motivated phrases in our parallel corpus based on lexical correspondences and syntactic similarity.this article reports on the term extraction experiments for 3 language pairs, i.e. french-dutch, french-english and french-italian.
</nextsent>
<nextsent>the focus was on the extraction of automative lexicons.the remainder of this paper is organized as fol lows: section 2 describes the corpus.
</nextsent>
<nextsent>in section 3 we present our linguistically-based sub-sentential alignment system and in section 4 we describe how we generate and filter our list of candidateterms.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3044">
<title id=" E09-1057.xml">language independent bilingual terminology extraction from a multilingual parallel corpus </title>
<section> corpus.  </section>
<citcontext>
<prevsection>
<prevsent># sentence pairs # words french italian 364,221 6,408,693 french english 363,651 7,305,151 french dutch 364,311 7,100,585table 1: number of sentence pairs and total number of words in the three parallel corpora 2.1 preprocessing.
</prevsent>
<prevsent>we pos-tagged and lemmatized the french, english and italian corpora with the freely availabletreetagger tool (schmid, 1994) and we used tadpole (van den bosch et al, 2007) to annotate the dutch corpus.
</prevsent>
</prevsection>
<citsent citstr=" C08-1067 ">
in next step, chunk information was added by rule-based language-independent chunker (macken et al, 2008) <papid> C08-1067 </papid>that contains distituency rules, which implies that chunk boundaries are added between two pos codes that cannot occur in the same constituent.</citsent>
<aftsection>
<nextsent>2.2 test and development corpus.
</nextsent>
<nextsent>as we presume that sentence length has an impact on the alignment performance, and thus on term extraction, we created three test sets with varying sentence lengths.
</nextsent>
<nextsent>we distinguished short sentences (2-7 words), medium-length sentences (8 19 words) and long sentences (  19 words).
</nextsent>
<nextsent>each test corpus contains approximately 9,000 words; the number of sentence pairs per test set can be found in table 2.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3046">
<title id=" E09-1057.xml">language independent bilingual terminology extraction from a multilingual parallel corpus </title>
<section> sub-sentential alignment module.  </section>
<citcontext>
<prevsection>
<prevsent>3.4 evaluation.
</prevsent>
<prevsent>to test our alignment module, we manually indicated all translational correspondences in the three test corpora.
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
we used the evaluation methodology of och and ney (2003) <papid> J03-1002 </papid>to evaluate the systems performance.</citsent>
<aftsection>
<nextsent>they distinguished sure alignments (s) and possible alignments (p) and introduced the following redefined precision and recall measures (where refers to the set of alignments): precision = |a ? | |a| , recall = |a ? s| |s| (1) and the alignment error rate (aer): aer(s, ;a) = 1?
</nextsent>
<nextsent>|a ? |+ |a ? s| |a|+ |s| (2) 498 table 3 shows the alignment results for the three language pairs.
</nextsent>
<nextsent>(macken et al, 2008) <papid> C08-1067 </papid>showed that the results for french-english were competitive to state-of-the-art alignment systems.</nextsent>
<nextsent>short medium long r p e r italian .99 .93 .04 .95 .89 .08 .95 .89 .07 english .97 .91 .06 .95 .85 .10 .92 .85 .12 dutch .96 .83 .11 .87 .73 .20 .87 .67 .24table 3: precision (p), recall (r) and alignment error rate (e) for our sub-sentential alignment system evaluated on french-italian, french-english and french-dutchas expected, the results show that the alignment quality is closely related to the similarity between languages.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3049">
<title id=" E09-1057.xml">language independent bilingual terminology extraction from a multilingual parallel corpus </title>
<section> term extraction module.  </section>
<citcontext>
<prevsection>
<prevsent>4.1.1 log-likelihood measure the log-likehood measure (ll) should allow us to detect single word terms that are distinctive enough to be kept in our bilingual lexicon (daille, 1995).
</prevsent>
<prevsent>this metric considers word frequencies weighted over two different corpora (in our case technical automotive corpus and the more general purpose corpus le monde1), in order to assign high ll-values to words having much higher or lower frequencies than expected.
</prevsent>
</prevsection>
<citsent citstr=" W00-0901 ">
we implemented the formula for both the expected values and the log-likelihood values as described by (rayson and garside, 2000).<papid> W00-0901 </papid>manual inspection of the log-likelihood figures confirmed our hypothesis that more domain specific terms in our corpus were assigned highll-values.</citsent>
<aftsection>
<nextsent>we experimentally defined the threshold for log-likelihood values corresponding to distinctive terms on our development corpus.
</nextsent>
<nextsent>example (4) shows some translation pairs which are filtered out by applying the ll threshold.
</nextsent>
<nextsent>(4) fr: cependant ? en: however ? it: tuttavia ? du: echter fr: choix ? en: choice ? it: scelta ? du: keuze fr: continuer ? en: continue ? it: continuare ? du: verdergaan fr: cadre ? en: frame ? it: cornice ? du: frame (erroneous filtering) fr: allege ment ? en: lightening ? it: alleggerire ? du: verlichten (erroneous filtering) 4.1.2 mutual expectation measure the mutual expectation measure as described by dias and kaalep (2003) is used to measure the degree of cohesiveness between words in text.this way, candidate multiword terms whose components do not occur together more often than expected by chance get filtered out.
</nextsent>
<nextsent>in first step, we have calculated all n-gram frequencies (up to 8-grams) for our four automotive corpora and then used these frequencies to derive the normalised 1http://catalog.elra.info/product info.phpproducts id=438 500 expectation (ne) values for all multiword entries, as specified by the formula of dias and kaalep: ne = prob(n? gram) 1 ? prob(n? 1?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3050">
<title id=" E09-1057.xml">language independent bilingual terminology extraction from a multilingual parallel corpus </title>
<section> term extraction module.  </section>
<citcontext>
<prevsection>
<prevsent>anchor chunk approach multi term ok nok may ok nok may fr-en sing 86.5% 12% 1.5% 77% 21% 2% mult 83% 14.5% 2.5% 47% 51% 2% total 84.5% 13.5% 2 % 64% 34% 2% fr-it sing 84.5% 15% 0.5% 85% 14% 1% mult 72% 27% 1.0% 65% 34% 1% total 77.5% 22% 1% 76.5% 22.5% 1% fr-du sing 75% 22% 3% 64.5% 33% 2.5% mult 84% 14% 2% 49.5% 49.5% 1% total 79.5% 20% 2.5% 58% 40% 2% table 7: precision figures for our term extraction system and for sdl multi term extract 4.2.3 comparison with monolingual terminology extraction in order to have insights in the performance ofour terminology extraction module, without considering the validity of the bilingual terminology pairs, we contrasted our extracted english terms with state-of-the art monolingual terminology systems.
</prevsent>
<prevsent>as we want to include both single words and multiword terms in our technical automotive lexicon, we only considered atr systems which extract both categories.
</prevsent>
</prevsection>
<citsent citstr=" L08-1281 ">
we used the implementation for these systems from (zhang et al, 2008) <papid> L08-1281 </papid>which is freely available at1.</citsent>
<aftsection>
<nextsent>we compared our system against 5 other atr systems: 1.
</nextsent>
<nextsent>baseline system (simple term frequency).
</nextsent>
<nextsent>2.
</nextsent>
<nextsent>weirdness algorithm (ahmad et al, 2007).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3051">
<title id=" E12-2011.xml">automatically generated customizable online dictionaries </title>
<section> generating proto-dictionaries ?.  </section>
<citcontext>
<prevsection>
<prevsent>in the first step the lemmatized versions of each input text have been created by means of morhological analysis and disambiguation3.
</prevsent>
<prevsent>in the second step parallel corpora have been created.
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
we used hun align (varga et al 2005) for sentence alignment.in the next step word alignment has been performed with giza++ (och and ney, 2003).<papid> J03-1002 </papid></citsent>
<aftsection>
<nextsent>during word alignment giza++ builds dictionary file that stores translation candidates, i.e. source and target language lemmata along with their translation probabilities.
</nextsent>
<nextsent>we used this dictionary file as the starting point to create the proto dictionaries.
</nextsent>
<nextsent>in the fourth step the proto-dictionaries have been created.
</nextsent>
<nextsent>only the most likely translation candidates were kept on the basis of some suit able heuristics, which has been developed while evaluating the results manually.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3052">
<title id=" E12-2001.xml">language resources factory case study on the acquisition of translation memories </title>
<section> web services and work flows.  </section>
<citcontext>
<prevsection>
<prevsent>some wss, e.g. dependency parsers, require amore complex representation that cannot be handled by the to.
</prevsent>
<prevsent>therefore, more expressive format has been adopted for these.
</prevsent>
</prevsection>
<citsent citstr=" W07-1501 ">
the graph annotation format (graf) (ide and suderman, 2007) <papid> W07-1501 </papid>is xml representation of graph that allows different levels of annotation using feature?</citsent>
<aftsection>
<nextsent>value?
</nextsent>
<nextsent>paradigm.
</nextsent>
<nextsent>this system allows different in-house formats to be easily encapsulated in this container-based format.
</nextsent>
<nextsent>on the other hand, grafcan be used as pivot format between other formats (ide and bunt, 2010), <papid> W10-1840 </papid>e.g. there is software to convert graf to uima and gate formats (ide and suderman, 2009) <papid> W09-3004 </papid>and it can be used to merge data represented in graph.both to and graf address syntactic interoperability while semantic interoperability is still an open topic.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3053">
<title id=" E12-2001.xml">language resources factory case study on the acquisition of translation memories </title>
<section> web services and work flows.  </section>
<citcontext>
<prevsection>
<prevsent>paradigm.
</prevsent>
<prevsent>this system allows different in-house formats to be easily encapsulated in this container-based format.
</prevsent>
</prevsection>
<citsent citstr=" W10-1840 ">
on the other hand, grafcan be used as pivot format between other formats (ide and bunt, 2010), <papid> W10-1840 </papid>e.g. there is software to convert graf to uima and gate formats (ide and suderman, 2009) <papid> W09-3004 </papid>and it can be used to merge data represented in graph.both to and graf address syntactic interoperability while semantic interoperability is still an open topic.</citsent>
<aftsection>
<nextsent>12http://panacea-lr.eu/en/ info-for-professionals/documents/ 3 6 evaluation.
</nextsent>
<nextsent>the evaluation of the factory is based on its features and usability requirements.
</nextsent>
<nextsent>a binary scheme (yes/no) is used to check whether each requirement is fulfilled or not.
</nextsent>
<nextsent>the quality of the tools is not altered as they are deployed as wsswithout any modification.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3054">
<title id=" E12-2001.xml">language resources factory case study on the acquisition of translation memories </title>
<section> web services and work flows.  </section>
<citcontext>
<prevsection>
<prevsent>paradigm.
</prevsent>
<prevsent>this system allows different in-house formats to be easily encapsulated in this container-based format.
</prevsent>
</prevsection>
<citsent citstr=" W09-3004 ">
on the other hand, grafcan be used as pivot format between other formats (ide and bunt, 2010), <papid> W10-1840 </papid>e.g. there is software to convert graf to uima and gate formats (ide and suderman, 2009) <papid> W09-3004 </papid>and it can be used to merge data represented in graph.both to and graf address syntactic interoperability while semantic interoperability is still an open topic.</citsent>
<aftsection>
<nextsent>12http://panacea-lr.eu/en/ info-for-professionals/documents/ 3 6 evaluation.
</nextsent>
<nextsent>the evaluation of the factory is based on its features and usability requirements.
</nextsent>
<nextsent>a binary scheme (yes/no) is used to check whether each requirement is fulfilled or not.
</nextsent>
<nextsent>the quality of the tools is not altered as they are deployed as wsswithout any modification.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3055">
<title id=" E09-3004.xml">finding word substitutions using a distributional similarity baseline and immediate context overlap </title>
<section> lloyds ? buy ? hbos.  </section>
<citcontext>
<prevsection>
<prevsent>ran and hovy, 2002).
</prevsent>
<prevsent>the most significant contexts for an input seed are extracted as features and those features used to discover words related to the input (under the assumption that words appearing in at least one significant context are similar to the seed word).
</prevsent>
</prevsection>
<citsent citstr=" C92-2082 ">
there is also non-distributional strand of this approach: it uses hearst-like patterns(hearst, 1992) <papid> C92-2082 </papid>which are supposed to indicate the presence of two terms in certain relation - most often hyponymy or meronymy (see chklovski and pantel, 2004).<papid> W04-3205 </papid></citsent>
<aftsection>
<nextsent>2.
</nextsent>
<nextsent>the feature vector approach (e.g. lin and.
</nextsent>
<nextsent>pantel, 2001).
</nextsent>
<nextsent>this method fully embraces the definition of distributional similarity by making the assumption that two words appearing in similar sets of features must be related.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3056">
<title id=" E09-3004.xml">finding word substitutions using a distributional similarity baseline and immediate context overlap </title>
<section> lloyds ? buy ? hbos.  </section>
<citcontext>
<prevsection>
<prevsent>ran and hovy, 2002).
</prevsent>
<prevsent>the most significant contexts for an input seed are extracted as features and those features used to discover words related to the input (under the assumption that words appearing in at least one significant context are similar to the seed word).
</prevsent>
</prevsection>
<citsent citstr=" W04-3205 ">
there is also non-distributional strand of this approach: it uses hearst-like patterns(hearst, 1992) <papid> C92-2082 </papid>which are supposed to indicate the presence of two terms in certain relation - most often hyponymy or meronymy (see chklovski and pantel, 2004).<papid> W04-3205 </papid></citsent>
<aftsection>
<nextsent>2.
</nextsent>
<nextsent>the feature vector approach (e.g. lin and.
</nextsent>
<nextsent>pantel, 2001).
</nextsent>
<nextsent>this method fully embraces the definition of distributional similarity by making the assumption that two words appearing in similar sets of features must be related.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3057">
<title id=" E09-3004.xml">finding word substitutions using a distributional similarity baseline and immediate context overlap </title>
<section> lloyds ? buy ? hbos.  </section>
<citcontext>
<prevsection>
<prevsent>some work has thus focused on re-rankingstrategies (see geffet and dagan, 2004 and gef fet and dagan, 2005, who improve the output of distributional similarity system for an entailment task using web-based feature inclusion check, and comment that their filtering produces better outputs than cutting off the similarity pairs with the lowest ranking.)
</prevsent>
<prevsent>29 2.2 extraction systems.
</prevsent>
</prevsection>
<citsent citstr=" W04-3206 ">
prominent entailment rule acquisition systems include dirt (lin and pantel, 2001), which uses distributional similarity on 1 gb corpus to identify semantically similar words and expressions,and tease (szpektor et al, 2004), <papid> W04-3206 </papid>which extracts entailment relations from the web forgiven word by computing characteristic contexts for that word.recently, systems that combine both pattern based and feature vector approaches have also been presented.</citsent>
<aftsection>
<nextsent>lin et al (2003) and pantel and ravichandran (2004) <papid> N04-1041 </papid>have proposed to classify the output of systems based on feature vectors using lexico-syntactic patterns, respectively in order to remove antonyms from related words list and to name clusters of related terms.</nextsent>
<nextsent>even more related to our work, mirkin et al (2006) <papid> P06-2075 </papid>integrate both approaches by constructing features for the output of both pattern-based and vector-based systems, and by filtering incorrect entries with supervised svm classifier.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3059">
<title id=" E09-3004.xml">finding word substitutions using a distributional similarity baseline and immediate context overlap </title>
<section> lloyds ? buy ? hbos.  </section>
<citcontext>
<prevsection>
<prevsent>29 2.2 extraction systems.
</prevsent>
<prevsent>prominent entailment rule acquisition systems include dirt (lin and pantel, 2001), which uses distributional similarity on 1 gb corpus to identify semantically similar words and expressions,and tease (szpektor et al, 2004), <papid> W04-3206 </papid>which extracts entailment relations from the web forgiven word by computing characteristic contexts for that word.recently, systems that combine both pattern based and feature vector approaches have also been presented.</prevsent>
</prevsection>
<citsent citstr=" N04-1041 ">
lin et al (2003) and pantel and ravichandran (2004) <papid> N04-1041 </papid>have proposed to classify the output of systems based on feature vectors using lexico-syntactic patterns, respectively in order to remove antonyms from related words list and to name clusters of related terms.</citsent>
<aftsection>
<nextsent>even more related to our work, mirkin et al (2006) <papid> P06-2075 </papid>integrate both approaches by constructing features for the output of both pattern-based and vector-based systems, and by filtering incorrect entries with supervised svm classifier.</nextsent>
<nextsent>(thepattern-based approach uses set of manually constructed patterns applied to web search.)in the same vein, geffet and dagan (2005) filter the result of pattern-based system using feature vectors.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3060">
<title id=" E09-3004.xml">finding word substitutions using a distributional similarity baseline and immediate context overlap </title>
<section> lloyds ? buy ? hbos.  </section>
<citcontext>
<prevsection>
<prevsent>prominent entailment rule acquisition systems include dirt (lin and pantel, 2001), which uses distributional similarity on 1 gb corpus to identify semantically similar words and expressions,and tease (szpektor et al, 2004), <papid> W04-3206 </papid>which extracts entailment relations from the web forgiven word by computing characteristic contexts for that word.recently, systems that combine both pattern based and feature vector approaches have also been presented.</prevsent>
<prevsent>lin et al (2003) and pantel and ravichandran (2004) <papid> N04-1041 </papid>have proposed to classify the output of systems based on feature vectors using lexico-syntactic patterns, respectively in order to remove antonyms from related words list and to name clusters of related terms.</prevsent>
</prevsection>
<citsent citstr=" P06-2075 ">
even more related to our work, mirkin et al (2006) <papid> P06-2075 </papid>integrate both approaches by constructing features for the output of both pattern-based and vector-based systems, and by filtering incorrect entries with supervised svm classifier.</citsent>
<aftsection>
<nextsent>(thepattern-based approach uses set of manually constructed patterns applied to web search.)in the same vein, geffet and dagan (2005) filter the result of pattern-based system using feature vectors.
</nextsent>
<nextsent>they get their features out of an 18 million word corpus augmented by web search.their idea is that for any pair of potentially similar words, the features of the entailed one should comprise all the features of the entailing one.
</nextsent>
<nextsent>the main difference between our work and the last two quoted papers is that we add new layerof verification: we extract pairs of verbs using automatically derived semantic patterns, perform afirst stage of filtering using the semantic signatures of each word and apply final stage of filtering relying on surface substitutability, which we name immediate context overlap?
</nextsent>
<nextsent>method.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3061">
<title id=" E09-3004.xml">finding word substitutions using a distributional similarity baseline and immediate context overlap </title>
<section> lloyds ? buy ? hbos.  </section>
<citcontext>
<prevsection>
<prevsent>the evaluation of kb or ontology extraction systems is typically done by presenting human judges with subset of extracted data and asking them to annotate it according to certain correctness criteria.
</prevsent>
<prevsent>for entailment systems, the annotation usually relies on two tests: whether the meaning of one word entails the other one in some senses of those words, and whether the judges can come upwith contexts in which the words are directly substitutable.
</prevsent>
</prevsection>
<citsent citstr=" P07-1058 ">
szpektor et al (2007) <papid> P07-1058 </papid>point out the difficulties in applying those criteria.</citsent>
<aftsection>
<nextsent>they note thelow inter-annotator agreements obtained in previous studies and propose new evaluation method based on precise judgement questions applied to set of relevant contexts.
</nextsent>
<nextsent>using their methods, they evaluate the dirt (lin and pantel, 2001) and tease (szpektor et al, 2004) <papid> W04-3206 </papid>algorithms and obtain upper bound precis ions of 44% and 38% respectively on 646 entailment rules for 30 transitive verbs.</nextsent>
<nextsent>we follow here their methodology to check the results obtained via the traditional annotation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3063">
<title id=" E09-3004.xml">finding word substitutions using a distributional similarity baseline and immediate context overlap </title>
<section> the data.  </section>
<citcontext>
<prevsection>
<prevsent>using their methods, they evaluate the dirt (lin and pantel, 2001) and tease (szpektor et al, 2004) <papid> W04-3206 </papid>algorithms and obtain upper bound precis ions of 44% and 38% respectively on 646 entailment rules for 30 transitive verbs.</prevsent>
<prevsent>we follow here their methodology to check the results obtained via the traditional annotation.</prevsent>
</prevsection>
<citsent citstr=" P06-4020 ">
the corpus used for our distributional similarity baseline consists of subset of wikipedia totalling 500 mb in size, parsed first with rasp2(briscoe et al, 2006) <papid> P06-4020 </papid>and then into robust minimal recur sion semantics form (rmrs, copestake, 2004) using rasp-to-rmrs converter.</citsent>
<aftsection>
<nextsent>the rmrs representation consists of trees (or tree fragments when complete parse is not possible) which comprise, for each phrase in the sentence, semantic head and its arguments.
</nextsent>
<nextsent>for instance, in the sentence lloyds rescues failing bank?, three subtrees can be extracted: lemma:rescue arg:arg1 var:lloyds which indicates that lloyds? is subject of the head rescue?, lemma:rescue arg:arg2 var:bank which indicates that bank?
</nextsent>
<nextsent>is object of the head rescue?, and lemma:failing arg:arg1 var:bank which indicates that the argument of failing?
</nextsent>
<nextsent>is bank?.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3068">
<title id=" E09-3004.xml">finding word substitutions using a distributional similarity baseline and immediate context overlap </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>annotations).
</prevsent>
<prevsent>we argue, though, that the issue of producing an acceptable english sentence is generation problem separate from the extraction task.
</prevsent>
</prevsection>
<citsent citstr=" P06-1057 ">
some systems, in fact,are dedicated to related problems, such as identifying whether the senses of two synonyms are the same in particular lexical context (see dagan et al., 2006).<papid> P06-1057 </papid></citsent>
<aftsection>
<nextsent>as far as our needs are concerned in the task of kb querying, we only require accurate searching capabilities as opposed to generationalcapabilities: the expansion of search terms to include impossible strings is not problem in terms of result.looking at the immediate context overlaps returned for each pair by the system, we find that the overlap (the similarity) can be situated at various linguistic layers: ? in the semantics of the verbs object: anew album?
</nextsent>
<nextsent>is something that one would frequently record?
</nextsent>
<nextsent>or release?.
</nextsent>
<nextsent>the phrase boosts the similarity score between record?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3073">
<title id=" E09-1095.xml">growing finely discriminating taxonomies from seeds of varying quality and size </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>an evaluation of the key ideas is then presented in section 5, to determine which seed yields the highest quality taxonomy once bootstrapping is completed.
</prevsent>
<prevsent>the paper then concludes with some final remarks in section 6.
</prevsent>
</prevsection>
<citsent citstr=" P88-1027 ">
simple pattern-matching techniques can be surprisingly effective for the extraction of lexico-se mantic relations from text when those relations are expressed using relatively stable and unambiguous syntagmatic patterns (ahlswede and evens, 1988).<papid> P88-1027 </papid></citsent>
<aftsection>
<nextsent>for instance, the work of hearst (1992) <papid> C92-2082 </papid>typifies this surgical approach to relation extraction, in which system fishes in large text for particular word sequences that strongly suggest semantic relationship such as hyper nymy or, in the case of charniak and berland (1999), the part-whole relation.</nextsent>
<nextsent>such efforts offer high precision but can exhibit low recall on mod erate-sized corpora, and extract just tiny (but very useful) subset of the semantic content of text.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3074">
<title id=" E09-1095.xml">growing finely discriminating taxonomies from seeds of varying quality and size </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the paper then concludes with some final remarks in section 6.
</prevsent>
<prevsent>simple pattern-matching techniques can be surprisingly effective for the extraction of lexico-se mantic relations from text when those relations are expressed using relatively stable and unambiguous syntagmatic patterns (ahlswede and evens, 1988).<papid> P88-1027 </papid></prevsent>
</prevsection>
<citsent citstr=" C92-2082 ">
for instance, the work of hearst (1992) <papid> C92-2082 </papid>typifies this surgical approach to relation extraction, in which system fishes in large text for particular word sequences that strongly suggest semantic relationship such as hyper nymy or, in the case of charniak and berland (1999), the part-whole relation.</citsent>
<aftsection>
<nextsent>such efforts offer high precision but can exhibit low recall on mod erate-sized corpora, and extract just tiny (but very useful) subset of the semantic content of text.
</nextsent>
<nextsent>the knowitall system of etzioni et al .
</nextsent>
<nextsent>(2004) employs the same generic patterns as hearst (e.g., nps such as np1, np2, ??), and more besides, to extract whole range of facts that can be exploited for web-based question-an swering.
</nextsent>
<nextsent>cimiano and wenderoth (2007) <papid> P07-1112 </papid>also use range of hearst-like patterns to find text sequences in web-text that are indicative of the lex ico-semantic properties of words; in particular, these authors use phrases like to * new noun?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3076">
<title id=" E09-1095.xml">growing finely discriminating taxonomies from seeds of varying quality and size </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the knowitall system of etzioni et al .
</prevsent>
<prevsent>(2004) employs the same generic patterns as hearst (e.g., nps such as np1, np2, ??), and more besides, to extract whole range of facts that can be exploited for web-based question-an swering.
</prevsent>
</prevsection>
<citsent citstr=" P07-1112 ">
cimiano and wenderoth (2007) <papid> P07-1112 </papid>also use range of hearst-like patterns to find text sequences in web-text that are indicative of the lex ico-semantic properties of words; in particular, these authors use phrases like to * new noun?</citsent>
<aftsection>
<nextsent>and the purpose of noun is to *?
</nextsent>
<nextsent>to identify the formal (isa), agentive (made by) and telic (used for) roles of nouns.
</nextsent>
<nextsent>snow, jurafsky and ng (2004) use supervised learning techniques to acquire those syntagmatic patterns that prove most useful for extracting hypernym relations from text.
</nextsent>
<nextsent>they train their system using pairs of wordnet terms that exemplify the hypernym relation; these are used to identify specific sentences in corpora that are most likely to express the relation in lexical terms.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3077">
<title id=" E09-1095.xml">growing finely discriminating taxonomies from seeds of varying quality and size </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>kashyap et al ., (2005) experiment with bootstrapping approach to growing concept taxonomies in the medical domain.
</prevsent>
<prevsent>a gold standard taxonomy provides terms that are used to retrieve documents which are then hierarchically clustered; cohesiveness measures are used to yield taxonomy of terms that can then further drive the retrieval and clustering cycle.
</prevsent>
</prevsection>
<citsent citstr=" P08-1119 ">
kozareva et al  (2008) <papid> P08-1119 </papid>use bootstrapping approach that extends the fixed-pattern approach of hearst (1992) <papid> C92-2082 </papid>in two intriguing ways.</citsent>
<aftsection>
<nextsent>first, they use doubly-anchored retrieval pattern of the form nouncat such as noun exam ple and *?
</nextsent>
<nextsent>to ground the retrieval relative to known example of hypernymy, so that any values extracted for the wild card * are likely to be coordinate terms of noun example and even more likely to be good examples of nouncat.
</nextsent>
<nextsent>secondly, they construct graph of terms that co-occur within this pattern to determine which terms are supported by others, and by how much.
</nextsent>
<nextsent>these authors also use two kinds of bootstrapping: the first variation, dubbed reckless, uses the candidates extracted from the double-anchored pattern (via *) as exemplars (nounexample) for successive retrieval cycles; the second variation first checks whether candidate is sufficiently supported to be used as an exemplar in future retrieval cycles.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3084">
<title id=" E09-1095.xml">growing finely discriminating taxonomies from seeds of varying quality and size </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>as result, we obtain finely discriminating taxonomy based on categories that are explicitly annotated with the properties (adjcat) that they bequeath to their members.
</prevsent>
<prevsent>these categories have an obvious descriptive and organizational utility, but of kind that one is unlikely to find in conventional resources like wordnet and wikipedia.
</prevsent>
</prevsection>
<citsent citstr=" P07-1008 ">
kozareva et al  (2008) <papid> P08-1119 </papid>test their approach on relatively simple and objective categories like states, countries (both 836closed sets), singers and fish (both open, the former more so than the latter), but not on complex categories in which members are tied both to general category, like food, and to stereotypical property, like sweet (veale and hao, 2007).<papid> P07-1008 </papid></citsent>
<aftsection>
<nextsent>by validating membership in these complex categories using wordnet-based heuristics, we can hang these categories and members on specific wordnet senses, and thus enrich wordnet with this additional taxonomic structure.
</nextsent>
<nextsent>a fine-grained taxonomy can be viewed as set of triples tijk =  ci, dj, pk , where ci denotes child of the parent term pk that possesses the discriminating property dj; in effect, each such triple expresses that ci is specialization of the complex taxonym dj-pk.
</nextsent>
<nextsent>thus, the belief that cola is carbonated-drink is expressed by the triple  cola, carbonated, drink .
</nextsent>
<nextsent>from this triple we can identify other categorizations of cola (such as treat and refreshment) via the web query carbonated * such as cola?, or we can identify other similarly fizzy drinks via the query carbonated drinks such as *?.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3088">
<title id=" E09-1095.xml">growing finely discriminating taxonomies from seeds of varying quality and size </title>
<section> empirical evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>both the wordnet and concept net seeds achieve comparable accuracies of 68% and 67% respectively after 5 cycles of bootstrapping, which compares well with the accuracy of 62.7% achieved by poesio and almuhareb.
</prevsent>
<prevsent>however, the simile seed clearly yields the best accuracy of 84.3%, which also exceeds the accuracy of 66.4% achieved by poesio and almuhareb when using both values and attributes (such as temperature, color, etc.) for clustering, or the accuracy of 70.9% they achieve when using attributes alone.
</prevsent>
</prevsection>
<citsent citstr=" C08-1119 ">
furthermore, bootstrapping from the simile seed yields higher cluster accuracy on the 402noun data-set than veale and hao (2008) <papid> C08-1119 </papid>themselves achieve with their simile data on the same test-set (69.85%).</citsent>
<aftsection>
<nextsent>but most striking of all is the concision of the representations that are acquired using bootstrapping.
</nextsent>
<nextsent>the simile seed yields high cluster accuracy using pool of just 2,614 fine discrimina tors, while poesio and almuhareb use 51,345 features even after their feature-set has been filtered for noise.
</nextsent>
<nextsent>though starting from different initial scales, each seed converges toward fea ture-set that is roughly twenty times smaller than that used by poesio and almuhareb.
</nextsent>
<nextsent>these experiments reveal that seed knowledge of different authoritativeness, quality and size will tend to converge toward roughly the same number of finely discriminating properties and toward much the same coverage after 5 or so cycles of bootstrapping.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3094">
<title id=" E09-1073.xml">out classing wikipedia in open domain information extraction weakly supervised acquisition of attributes over conceptual hierarchies </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a set of labeled classes of instances is extracted from text and linked into an existing conceptual hierarchy.
</prevsent>
<prevsent>besides significant increase in the coverage of the class labels assigned to individual instances, the resulting resource of labeled classes is more effective than similar data derived from the manually-created wikipedia, inthe task of attribute extraction over conceptual hierarchies.
</prevsent>
</prevsection>
<citsent citstr=" P08-1004 ">
motivation: sharing basic intuitions and longterm goals with other tasks within the area of web based information extraction (banko and etzioni, 2008; <papid> P08-1004 </papid>davidov and rappoport, 2008), <papid> P08-1027 </papid>the task of acquiring class attributes relies on unstructured text available on the web, as data source forex tracting generally-useful knowledge.</citsent>
<aftsection>
<nextsent>in the caseof attribute extraction, the knowledge to be extracted consists in quantifiable properties of various classes (e.g., top speed, body style and gas mileage for the class of sports cars).
</nextsent>
<nextsent>existing work on large-scale attribute extraction focuses on producing ranked lists of attributes, for target classes of instances available in the form of flat sets of instances (e.g., ferrari modena, porsche carrera gt) sharing the same class label (e.g., sports cars).
</nextsent>
<nextsent>independently of how the input target classes are populated with instances (man ually (pasca, 2007) or automatically (pasca and van durme, 2008)), and what type of textual data source is used for extracting attributes (web documents or query logs), the extraction of attributes operates at lexical rather than semantic level.
</nextsent>
<nextsent>indeed, the class labels of the target classes may be not more than text surface strings (e.g., sportscars) or even artificially-created labels (e.g., cartoon char in lieu of cartoon characters).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3095">
<title id=" E09-1073.xml">out classing wikipedia in open domain information extraction weakly supervised acquisition of attributes over conceptual hierarchies </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a set of labeled classes of instances is extracted from text and linked into an existing conceptual hierarchy.
</prevsent>
<prevsent>besides significant increase in the coverage of the class labels assigned to individual instances, the resulting resource of labeled classes is more effective than similar data derived from the manually-created wikipedia, inthe task of attribute extraction over conceptual hierarchies.
</prevsent>
</prevsection>
<citsent citstr=" P08-1027 ">
motivation: sharing basic intuitions and longterm goals with other tasks within the area of web based information extraction (banko and etzioni, 2008; <papid> P08-1004 </papid>davidov and rappoport, 2008), <papid> P08-1027 </papid>the task of acquiring class attributes relies on unstructured text available on the web, as data source forex tracting generally-useful knowledge.</citsent>
<aftsection>
<nextsent>in the caseof attribute extraction, the knowledge to be extracted consists in quantifiable properties of various classes (e.g., top speed, body style and gas mileage for the class of sports cars).
</nextsent>
<nextsent>existing work on large-scale attribute extraction focuses on producing ranked lists of attributes, for target classes of instances available in the form of flat sets of instances (e.g., ferrari modena, porsche carrera gt) sharing the same class label (e.g., sports cars).
</nextsent>
<nextsent>independently of how the input target classes are populated with instances (man ually (pasca, 2007) or automatically (pasca and van durme, 2008)), and what type of textual data source is used for extracting attributes (web documents or query logs), the extraction of attributes operates at lexical rather than semantic level.
</nextsent>
<nextsent>indeed, the class labels of the target classes may be not more than text surface strings (e.g., sportscars) or even artificially-created labels (e.g., cartoon char in lieu of cartoon characters).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3096">
<title id=" E09-1073.xml">out classing wikipedia in open domain information extraction weakly supervised acquisition of attributes over conceptual hierarchies </title>
<section> attribute extraction over hierarchies.  </section>
<citcontext>
<prevsection>
<prevsent>specifically, evaluation results indicate that the accuracy of the extracted lists of attributes is higher by 8% at rank 10, 13% at rank 30 and 18% at rank 50, when using the automatically-extracted classes of instances rather than the comparatively more numerous and a-priori more reliable, human generated, collaboratively-vetted classes of instances available within wikipedia (remy, 2002).
</prevsent>
<prevsent>extraction of flat labeled classes: unstructured text from combination of web documents and query logs represents the source for deriving flat set of labeled classes of instances, which are necessary as input for attribute extraction experiments.
</prevsent>
</prevsection>
<citsent citstr=" C92-2082 ">
the labeled classes are acquired in three stages: 1) extraction of noisy pool of pairs of aclass label and potential class instance, by applying few is-a extraction patterns, selected from (hearst, 1992), <papid> C92-2082 </papid>to web documents: (fruits, apple), (fruits, corn), (fruits, mango), (fruits, orange), (foods, broccoli), (crops, lettuce), (flowers, rose);2) extraction of unlabeled clusters of distributionally similar phrases, by clustering vectors of contextual features collected around the occurrences of the phrases within web documents (lin and pantel, 2002): {<papid> C02-1144 </papid>lettuce, broccoli, corn, ..}, {carrot, mango, apple, orange, rose, ..};3) merging and filtering of the raw pairs and unlabeled clusters into smaller, more accurate sets of class instances associated with class labels, in an attempt to use unlabeled clusters to filter noisy raw pairs instead of merely using clusters to generalize class labels across raw pairs (pasca and van durme, 2008): fruits={apple, mango, orange, ..}.</citsent>
<aftsection>
<nextsent>to increase precision, the vocabulary of class instances is confined to the set of queries that are most frequently submitted to general-purpose web search engine.
</nextsent>
<nextsent>after merging, the resulting pairs of an instance and class label are arranged into instance sets (e.g., {ferrari modena, porsche carrera gt}), each associated with class label (e.g., sports cars).
</nextsent>
<nextsent>linking labeled classes into hierarchies: manually-constructed language resources such as wordnet provide reliable, wide-coverage upper level conceptual hierarchies, by grouping together phrases with the same meaning (e.g., {analgesic, painkiller, pain pill}) into sets of synonyms(synsets), and organizing the synsets into conceptual hierarchies (e.g., painkillers are subconcept,or hyponym, of drugs) (fellbaum, 1998).
</nextsent>
<nextsent>to determine the points of insertion of automatically extracted labeled classes into hand-built wordnet hierarchies, the class labels are looked up in wordnet using built-in morphological normalization routines.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3097">
<title id=" E09-1073.xml">out classing wikipedia in open domain information extraction weakly supervised acquisition of attributes over conceptual hierarchies </title>
<section> attribute extraction over hierarchies.  </section>
<citcontext>
<prevsection>
<prevsent>specifically, evaluation results indicate that the accuracy of the extracted lists of attributes is higher by 8% at rank 10, 13% at rank 30 and 18% at rank 50, when using the automatically-extracted classes of instances rather than the comparatively more numerous and a-priori more reliable, human generated, collaboratively-vetted classes of instances available within wikipedia (remy, 2002).
</prevsent>
<prevsent>extraction of flat labeled classes: unstructured text from combination of web documents and query logs represents the source for deriving flat set of labeled classes of instances, which are necessary as input for attribute extraction experiments.
</prevsent>
</prevsection>
<citsent citstr=" C02-1144 ">
the labeled classes are acquired in three stages: 1) extraction of noisy pool of pairs of aclass label and potential class instance, by applying few is-a extraction patterns, selected from (hearst, 1992), <papid> C92-2082 </papid>to web documents: (fruits, apple), (fruits, corn), (fruits, mango), (fruits, orange), (foods, broccoli), (crops, lettuce), (flowers, rose);2) extraction of unlabeled clusters of distributionally similar phrases, by clustering vectors of contextual features collected around the occurrences of the phrases within web documents (lin and pantel, 2002): {<papid> C02-1144 </papid>lettuce, broccoli, corn, ..}, {carrot, mango, apple, orange, rose, ..};3) merging and filtering of the raw pairs and unlabeled clusters into smaller, more accurate sets of class instances associated with class labels, in an attempt to use unlabeled clusters to filter noisy raw pairs instead of merely using clusters to generalize class labels across raw pairs (pasca and van durme, 2008): fruits={apple, mango, orange, ..}.</citsent>
<aftsection>
<nextsent>to increase precision, the vocabulary of class instances is confined to the set of queries that are most frequently submitted to general-purpose web search engine.
</nextsent>
<nextsent>after merging, the resulting pairs of an instance and class label are arranged into instance sets (e.g., {ferrari modena, porsche carrera gt}), each associated with class label (e.g., sports cars).
</nextsent>
<nextsent>linking labeled classes into hierarchies: manually-constructed language resources such as wordnet provide reliable, wide-coverage upper level conceptual hierarchies, by grouping together phrases with the same meaning (e.g., {analgesic, painkiller, pain pill}) into sets of synonyms(synsets), and organizing the synsets into conceptual hierarchies (e.g., painkillers are subconcept,or hyponym, of drugs) (fellbaum, 1998).
</nextsent>
<nextsent>to determine the points of insertion of automatically extracted labeled classes into hand-built wordnet hierarchies, the class labels are looked up in wordnet using built-in morphological normalization routines.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3098">
<title id=" E09-1073.xml">out classing wikipedia in open domain information extraction weakly supervised acquisition of attributes over conceptual hierarchies </title>
<section> attribute extraction over hierarchies.  </section>
<citcontext>
<prevsection>
<prevsent>nevertheless, choosing the first sense is attractive for three reasons.
</prevsent>
<prevsent>first, wordnet senses are often too fine-grained, making the task of choosing the correct sense difficult evenfor humans (palmer et al, 2007).
</prevsent>
</prevsection>
<citsent citstr=" W07-2016 ">
second, choosing the first sense from wordnet is sometimes better than more intelligent disambiguation techniques (pradhan et al, 2007).<papid> W07-2016 </papid></citsent>
<aftsection>
<nextsent>third, previous experimental results on linking wikipedia classes to wordnet concepts confirm that first-sense selection is more effective in practice than other techniques (suchanek et al, 2007).
</nextsent>
<nextsent>thus, class label and its associated instances are inserted under the first wordnet sense available for the class label.
</nextsent>
<nextsent>for example, silicon valley companies and its associated instances (apple, hewlett packard etc.)are inserted under the first of the 9 senses of companies in wordnet, which corresponds to companies as institutions created to conduct business.in order to trade off coverage for higher precision, the heuristic can be restricted to link class label under the first wordnet sense available, as 640before, but only when no other senses are available at the point of insertion beyond the first sense.
</nextsent>
<nextsent>with the modified heuristic, the class label internet search engines is linked under the first and only sense of search engines in wordnet, but silicon valley companies is no longer linked under the first of the 9 senses of companies.extraction of attributes for hierarchy con cepts: the labeled classes of instances linked to conceptual hierarchies constitute the input to the acquisition of attributes of hierarchy concepts, by mining collection of web search queries.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3099">
<title id=" E09-1073.xml">out classing wikipedia in open domain information extraction weakly supervised acquisition of attributes over conceptual hierarchies </title>
<section> experimental setting.  </section>
<citcontext>
<prevsection>
<prevsent>the classes, examples of which are shown in table 1, are linked under conceptual hierarchies available within wordnet 3.0, which contains total of 117,798 english noun phrases grouped in 82,115 concepts (or synsets).
</prevsent>
<prevsent>parameters for extracting attributes: for each target concept from the hierarchy, given the union of all instances associated to class labels linked to the target concept or one of its sub concepts, and given set of five seed attributes (e.g., {quality,speed, number of users, market share, reliabil ity} for search engines), the method described in (pasca, 2007) extracts ranked lists of attributes from the input query logs.
</prevsent>
</prevsection>
<citsent citstr=" P99-1004 ">
internally, the ranking of attributes uses jensen-shannon (lee, 1999)<papid> P99-1004 </papid>to compute similarity scores between internal rep 641 class label class size class instances accounting systems 40 flex cube, myob, oracle financials, peachtree accounting, sybiz antimicrobials 97 azithromycin, chloramphenicol, fusidic acid, quino lones, sulfa drugs civilizations 197 ancient greece, chaldeans, etruscans, inca, indians, roman republic elementary particles 33 axions, electrons, gravitons, leptons, muons, neutrons, posit rons farm animals 61 angora goats, burros, cattle, cows, donkeys, draft horses, mule, oxen forages 27 alsike clover, ryegrass, tall fescue, sericea lespedeza, birds foot trefoil ideologies 179 egalitarianism, laissez-faire capitalism, participatory democracy social events 436 academic conferences, afternoon teas, block parties, masquerade balls table 1: examples of instances within labeled classes extracted from unstructured text, used as input for attribute extraction experiments resent ations of seed attributes, on one hand, and each of the newly acquired attributes, on the other hand.</citsent>
<aftsection>
<nextsent>depending on the experiments, the amount of supervision is thus limited to either 5 seed attributes for each target concept, or to 5 seed attributes (population, area, president, flag and climate) provided for only one of the extracted labeled classes, namely european countries.
</nextsent>
<nextsent>experimental runs: the experiments consist of four different runs, which correspond to different choices for the source of conceptual hierarchies and class instances linked to those hierarchies, as illustrated in table 2.
</nextsent>
<nextsent>in the first run, denoted n, the class instances are those available within the latest version of wordnet (3.0) itself via hasin stance relations.
</nextsent>
<nextsent>the second run, y, corresponds toan extension of wordnet based on the manually compiled classes of instances from categories in wikipedia, as available in the 2007-w50-5 version of yago (suchanek et al, 2007).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3100">
<title id=" E09-1073.xml">out classing wikipedia in open domain information extraction weakly supervised acquisition of attributes over conceptual hierarchies </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>for instance, 643one of the target concepts, denoted country, corresponds to synset situated at the internal offset 08544813 in wordnet 3.0, which groups together the synonymous phrases country, state and land and associates them with the definition the territory occupied by nation?.
</prevsent>
<prevsent>the target concepts exhibit variation with respect to their depths within wordnet conceptual hierarchies, ranging from minimum of 5 (e.g., for food) to maximum of 11 (for flower), with mean depth of 8 over the 25 concepts.evaluation procedure: the measurement of recall requires knowledge of the complete set ofitems (in our case, attributes) to be extracted.
</prevsent>
</prevsection>
<citsent citstr=" P04-1053 ">
unfortunately, this number is often unavailable in information extraction tasks in general (hasegawa et al, 2004), <papid> P04-1053 </papid>and attribute extraction in particular.</citsent>
<aftsection>
<nextsent>indeed, the manual enumeration of all attributes of each target concept, to measure recall, is unfeasible.
</nextsent>
<nextsent>therefore, the evaluation focuses on the assessment of attribute accuracy.to remove any bias towards higher-ranked attributes during the assessment of class attributes, the ranked lists of attributes produced by each run to be evaluated are sorted alphabetically into merged list.
</nextsent>
<nextsent>each attribute of the merged list is manually assigned correctness label within its respective class.
</nextsent>
<nextsent>in accordance with previously introduced methodology, an attribute is vital if it must be present in an ideal list of attributes of the class (e.g., side effects for drug); okay if it provides useful but non-essential information; and wrong if it is incorrect (pasca, 2007).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3101">
<title id=" E09-1073.xml">out classing wikipedia in open domain information extraction weakly supervised acquisition of attributes over conceptual hierarchies </title>
<section> conclusion.  </section>
<citcontext>
<prevsection>
<prevsent>the linking of the labeled classes into existing conceptual hierarchies allows for the extraction of attributes over hierarchy concepts, without a-priori restrictions to specific domains of interest and with little supervision.
</prevsent>
<prevsent>experimental results show that the extracted attributes are more accurate when using automatically-derived labeled classes, rather than classes of instances derived from manually created resources such as wikipedia.
</prevsent>
</prevsection>
<citsent citstr=" D07-1107 ">
current work investigates the impact of the semantic distribution of the classes of instances on the overall accuracy of attributes; the potential benefits of using more compact conceptual hierarchies (snowet al, 2007) <papid> D07-1107 </papid>on attribute accuracy; and the organization of labeled classes of instances into conceptual hierarchies, as an alternative to inserting them into existing conceptual hierarchies created manually from scratch or automatically by filtering manually-generated relations among classes from wikipedia (ponzetto and strube, 2007).</citsent>
<aftsection>
<nextsent>646
</nextsent>



</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3102">
<title id=" E12-1050.xml">feature rich partofspeech tagging for morphologically complex languages application to bulgarian </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>but verb ini can write.?
</prevsent>
<prevsent>traditionally, linguists have classified english words into the following eight basic pos categories: noun, pronoun, adjective, verb, adverb, preposition, conjunction, and interjection;this list is often extended bit, e.g., with determiners, particles, participles, etc., but the number of categories considered is rarely more than 15.
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
computational linguistics works with larger inventory of pos tags, e.g., the penn treebank(marcus et al 1993) <papid> J93-2004 </papid>uses 48 tags: 36 for part of-speech, and 12 for punctuation and currency symbols.</citsent>
<aftsection>
<nextsent>this increase in the number of tags is partially due to finer granularity, e.g., there are special tags for determiners, particles, modal verbs, cardinal numbers, foreign words, existential there, etc., but also to the desire to encode morphological information as part of the tags.
</nextsent>
<nextsent>for example, there are six tags for verbs in the penn treebank: vb (verb, base form; e.g., sing), vbd (verb, past tense; e.g., sang), vbg (verb, gerund or present participle; e.g., singing), vbn(verb, past participle; e.g., sung) vbp (verb, non 3rd person singular present; e.g., sing), and vbz (verb, 3rd person singular present; e.g., sings); these tags are morpho-syntactic in nature.
</nextsent>
<nextsent>other corpora have used even larger tagsets, e.g., the brown corpus (kucera and francis, 1967) and the lancaster-oslo/bergen (lob) corpus (johansson et al 1986) use 87 and 135 tags, respectively.pos tagging poses major challenges for morphologically complex languages, whose tagsetsencode lot of additional morpho-syntactic features (for most of the basic pos categories), e.g., gender, number, person, etc. for example, the bultreebank (simov et al 2004) for bulgarian uses 680 tags, while the prague dependency tree bank (hajic?, 1998) for czech has over 1,400 tags.below we present experiments with pos tagging for bulgarian, which is an inflectional language with rich morphology.
</nextsent>
<nextsent>unlike most previous work, which has used reduced set of pos tags, we use all 680 tags in the bultreebank.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3106">
<title id=" E12-1050.xml">feature rich partofspeech tagging for morphologically complex languages application to bulgarian </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>492
</prevsent>
<prevsent>most research on part-of-speech tagging has focused on english, and has relied on the penn treebank (marcus et al 1993) <papid> J93-2004 </papid>and its tagset for training and evaluation.</prevsent>
</prevsection>
<citsent citstr=" J95-4004 ">
the task is typically address edas sequential tagging problem; one notable exception is the work of brill (1995), <papid> J95-4004 </papid>who proposed non-sequential transformation-based learning.</citsent>
<aftsection>
<nextsent>a number of different sequential learning frameworks have been tried, yielding 96-97% accuracy: lafferty et al(2001) experimented with conditional random fields (crfs) (95.7% accuracy), ratnaparkhi (1996) <papid> W96-0213 </papid>used maximum entropy sequence classifier (96.6% accuracy), brants (2000) <papid> A00-1031 </papid>employed hidden markov model(96.6% accuracy), collins (2002) <papid> W02-1001 </papid>adopted an averaged perception discriminative sequence model (97.1% accuracy).</nextsent>
<nextsent>all these models fix the order of inference from left to right.toutanova et al(2003) <papid> N03-1033 </papid>introduced cyclic dependency network (97.2% accuracy), where the search is bi-directional.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3108">
<title id=" E12-1050.xml">feature rich partofspeech tagging for morphologically complex languages application to bulgarian </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>most research on part-of-speech tagging has focused on english, and has relied on the penn treebank (marcus et al 1993) <papid> J93-2004 </papid>and its tagset for training and evaluation.</prevsent>
<prevsent>the task is typically address edas sequential tagging problem; one notable exception is the work of brill (1995), <papid> J95-4004 </papid>who proposed non-sequential transformation-based learning.</prevsent>
</prevsection>
<citsent citstr=" W96-0213 ">
a number of different sequential learning frameworks have been tried, yielding 96-97% accuracy: lafferty et al(2001) experimented with conditional random fields (crfs) (95.7% accuracy), ratnaparkhi (1996) <papid> W96-0213 </papid>used maximum entropy sequence classifier (96.6% accuracy), brants (2000) <papid> A00-1031 </papid>employed hidden markov model(96.6% accuracy), collins (2002) <papid> W02-1001 </papid>adopted an averaged perception discriminative sequence model (97.1% accuracy).</citsent>
<aftsection>
<nextsent>all these models fix the order of inference from left to right.toutanova et al(2003) <papid> N03-1033 </papid>introduced cyclic dependency network (97.2% accuracy), where the search is bi-directional.</nextsent>
<nextsent>shen et al(2007) <papid> P07-1096 </papid>have further shown that better results (97.3% accu racy) can be obtained using guided learning, aframework for bidirectional sequence classification, which integrates token classification and inference order selection into single learning task and uses perceptron-like (collins and roark, 2004) <papid> P04-1015 </papid>passive-aggressive classifier to make the easiest decisions first.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3109">
<title id=" E12-1050.xml">feature rich partofspeech tagging for morphologically complex languages application to bulgarian </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>most research on part-of-speech tagging has focused on english, and has relied on the penn treebank (marcus et al 1993) <papid> J93-2004 </papid>and its tagset for training and evaluation.</prevsent>
<prevsent>the task is typically address edas sequential tagging problem; one notable exception is the work of brill (1995), <papid> J95-4004 </papid>who proposed non-sequential transformation-based learning.</prevsent>
</prevsection>
<citsent citstr=" A00-1031 ">
a number of different sequential learning frameworks have been tried, yielding 96-97% accuracy: lafferty et al(2001) experimented with conditional random fields (crfs) (95.7% accuracy), ratnaparkhi (1996) <papid> W96-0213 </papid>used maximum entropy sequence classifier (96.6% accuracy), brants (2000) <papid> A00-1031 </papid>employed hidden markov model(96.6% accuracy), collins (2002) <papid> W02-1001 </papid>adopted an averaged perception discriminative sequence model (97.1% accuracy).</citsent>
<aftsection>
<nextsent>all these models fix the order of inference from left to right.toutanova et al(2003) <papid> N03-1033 </papid>introduced cyclic dependency network (97.2% accuracy), where the search is bi-directional.</nextsent>
<nextsent>shen et al(2007) <papid> P07-1096 </papid>have further shown that better results (97.3% accu racy) can be obtained using guided learning, aframework for bidirectional sequence classification, which integrates token classification and inference order selection into single learning task and uses perceptron-like (collins and roark, 2004) <papid> P04-1015 </papid>passive-aggressive classifier to make the easiest decisions first.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3110">
<title id=" E12-1050.xml">feature rich partofspeech tagging for morphologically complex languages application to bulgarian </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>most research on part-of-speech tagging has focused on english, and has relied on the penn treebank (marcus et al 1993) <papid> J93-2004 </papid>and its tagset for training and evaluation.</prevsent>
<prevsent>the task is typically address edas sequential tagging problem; one notable exception is the work of brill (1995), <papid> J95-4004 </papid>who proposed non-sequential transformation-based learning.</prevsent>
</prevsection>
<citsent citstr=" W02-1001 ">
a number of different sequential learning frameworks have been tried, yielding 96-97% accuracy: lafferty et al(2001) experimented with conditional random fields (crfs) (95.7% accuracy), ratnaparkhi (1996) <papid> W96-0213 </papid>used maximum entropy sequence classifier (96.6% accuracy), brants (2000) <papid> A00-1031 </papid>employed hidden markov model(96.6% accuracy), collins (2002) <papid> W02-1001 </papid>adopted an averaged perception discriminative sequence model (97.1% accuracy).</citsent>
<aftsection>
<nextsent>all these models fix the order of inference from left to right.toutanova et al(2003) <papid> N03-1033 </papid>introduced cyclic dependency network (97.2% accuracy), where the search is bi-directional.</nextsent>
<nextsent>shen et al(2007) <papid> P07-1096 </papid>have further shown that better results (97.3% accu racy) can be obtained using guided learning, aframework for bidirectional sequence classification, which integrates token classification and inference order selection into single learning task and uses perceptron-like (collins and roark, 2004) <papid> P04-1015 </papid>passive-aggressive classifier to make the easiest decisions first.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3111">
<title id=" E12-1050.xml">feature rich partofspeech tagging for morphologically complex languages application to bulgarian </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the task is typically address edas sequential tagging problem; one notable exception is the work of brill (1995), <papid> J95-4004 </papid>who proposed non-sequential transformation-based learning.</prevsent>
<prevsent>a number of different sequential learning frameworks have been tried, yielding 96-97% accuracy: lafferty et al(2001) experimented with conditional random fields (crfs) (95.7% accuracy), ratnaparkhi (1996) <papid> W96-0213 </papid>used maximum entropy sequence classifier (96.6% accuracy), brants (2000) <papid> A00-1031 </papid>employed hidden markov model(96.6% accuracy), collins (2002) <papid> W02-1001 </papid>adopted an averaged perception discriminative sequence model (97.1% accuracy).</prevsent>
</prevsection>
<citsent citstr=" N03-1033 ">
all these models fix the order of inference from left to right.toutanova et al(2003) <papid> N03-1033 </papid>introduced cyclic dependency network (97.2% accuracy), where the search is bi-directional.</citsent>
<aftsection>
<nextsent>shen et al(2007) <papid> P07-1096 </papid>have further shown that better results (97.3% accu racy) can be obtained using guided learning, aframework for bidirectional sequence classification, which integrates token classification and inference order selection into single learning task and uses perceptron-like (collins and roark, 2004) <papid> P04-1015 </papid>passive-aggressive classifier to make the easiest decisions first.</nextsent>
<nextsent>recently, tsuruoka et al(2011), <papid> W11-0328 </papid>proposed simple perceptron-based classifier applied from left to right but augmented with look ahead mechanism that searches the space of future actions, yielding 97.3% accuracy.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3112">
<title id=" E12-1050.xml">feature rich partofspeech tagging for morphologically complex languages application to bulgarian </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>a number of different sequential learning frameworks have been tried, yielding 96-97% accuracy: lafferty et al(2001) experimented with conditional random fields (crfs) (95.7% accuracy), ratnaparkhi (1996) <papid> W96-0213 </papid>used maximum entropy sequence classifier (96.6% accuracy), brants (2000) <papid> A00-1031 </papid>employed hidden markov model(96.6% accuracy), collins (2002) <papid> W02-1001 </papid>adopted an averaged perception discriminative sequence model (97.1% accuracy).</prevsent>
<prevsent>all these models fix the order of inference from left to right.toutanova et al(2003) <papid> N03-1033 </papid>introduced cyclic dependency network (97.2% accuracy), where the search is bi-directional.</prevsent>
</prevsection>
<citsent citstr=" P07-1096 ">
shen et al(2007) <papid> P07-1096 </papid>have further shown that better results (97.3% accu racy) can be obtained using guided learning, aframework for bidirectional sequence classification, which integrates token classification and inference order selection into single learning task and uses perceptron-like (collins and roark, 2004) <papid> P04-1015 </papid>passive-aggressive classifier to make the easiest decisions first.</citsent>
<aftsection>
<nextsent>recently, tsuruoka et al(2011), <papid> W11-0328 </papid>proposed simple perceptron-based classifier applied from left to right but augmented with look ahead mechanism that searches the space of future actions, yielding 97.3% accuracy.</nextsent>
<nextsent>for morphologically complex languages, the problem of pos tagging typically includes morphological disambiguation, which yields much larger number of tags.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3113">
<title id=" E12-1050.xml">feature rich partofspeech tagging for morphologically complex languages application to bulgarian </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>a number of different sequential learning frameworks have been tried, yielding 96-97% accuracy: lafferty et al(2001) experimented with conditional random fields (crfs) (95.7% accuracy), ratnaparkhi (1996) <papid> W96-0213 </papid>used maximum entropy sequence classifier (96.6% accuracy), brants (2000) <papid> A00-1031 </papid>employed hidden markov model(96.6% accuracy), collins (2002) <papid> W02-1001 </papid>adopted an averaged perception discriminative sequence model (97.1% accuracy).</prevsent>
<prevsent>all these models fix the order of inference from left to right.toutanova et al(2003) <papid> N03-1033 </papid>introduced cyclic dependency network (97.2% accuracy), where the search is bi-directional.</prevsent>
</prevsection>
<citsent citstr=" P04-1015 ">
shen et al(2007) <papid> P07-1096 </papid>have further shown that better results (97.3% accu racy) can be obtained using guided learning, aframework for bidirectional sequence classification, which integrates token classification and inference order selection into single learning task and uses perceptron-like (collins and roark, 2004) <papid> P04-1015 </papid>passive-aggressive classifier to make the easiest decisions first.</citsent>
<aftsection>
<nextsent>recently, tsuruoka et al(2011), <papid> W11-0328 </papid>proposed simple perceptron-based classifier applied from left to right but augmented with look ahead mechanism that searches the space of future actions, yielding 97.3% accuracy.</nextsent>
<nextsent>for morphologically complex languages, the problem of pos tagging typically includes morphological disambiguation, which yields much larger number of tags.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3114">
<title id=" E12-1050.xml">feature rich partofspeech tagging for morphologically complex languages application to bulgarian </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>all these models fix the order of inference from left to right.toutanova et al(2003) <papid> N03-1033 </papid>introduced cyclic dependency network (97.2% accuracy), where the search is bi-directional.</prevsent>
<prevsent>shen et al(2007) <papid> P07-1096 </papid>have further shown that better results (97.3% accu racy) can be obtained using guided learning, aframework for bidirectional sequence classification, which integrates token classification and inference order selection into single learning task and uses perceptron-like (collins and roark, 2004) <papid> P04-1015 </papid>passive-aggressive classifier to make the easiest decisions first.</prevsent>
</prevsection>
<citsent citstr=" W11-0328 ">
recently, tsuruoka et al(2011), <papid> W11-0328 </papid>proposed simple perceptron-based classifier applied from left to right but augmented with look ahead mechanism that searches the space of future actions, yielding 97.3% accuracy.</citsent>
<aftsection>
<nextsent>for morphologically complex languages, the problem of pos tagging typically includes morphological disambiguation, which yields much larger number of tags.
</nextsent>
<nextsent>for example, for arabic, habash and rambow (2005) <papid> P05-1071 </papid>used support vector machines (svm), achieving 97.6% accuracy with 139 tags from the arabic treebank (maamouri etal., 2003).</nextsent>
<nextsent>for czech, hajic?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3115">
<title id=" E12-1050.xml">feature rich partofspeech tagging for morphologically complex languages application to bulgarian </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>recently, tsuruoka et al(2011), <papid> W11-0328 </papid>proposed simple perceptron-based classifier applied from left to right but augmented with look ahead mechanism that searches the space of future actions, yielding 97.3% accuracy.</prevsent>
<prevsent>for morphologically complex languages, the problem of pos tagging typically includes morphological disambiguation, which yields much larger number of tags.</prevsent>
</prevsection>
<citsent citstr=" P05-1071 ">
for example, for arabic, habash and rambow (2005) <papid> P05-1071 </papid>used support vector machines (svm), achieving 97.6% accuracy with 139 tags from the arabic treebank (maamouri etal., 2003).</citsent>
<aftsection>
<nextsent>for czech, hajic?
</nextsent>
<nextsent>et al(2001) combined hidden markov model (hmm) with linguistic rules, which yielded 95.2% accuracy using an inventory of over 1,400 tags from the prague dependency treebank (hajic?, 1998).
</nextsent>
<nextsent>for icelandic, dredze and wallenberg (2008) <papid> P08-2009 </papid>reported 92.1% accuracy with 639 tags developed for the icelandic frequency lexicon (pind et al 1991), they used guided learning and tag decomposition: first, coarse pos class is assigned (e.g., noun, verb, adjective), then, additional fine-grained morphological features like case, number and gender are added, and finally, the proposed tags are further reconsidered using non-local features.</nextsent>
<nextsent>similarly, smith et al(2005) <papid> H05-1060 </papid>decomposed the complex tags into factors, where models for predicting part-of-speech, gender, number, case, and lemma are estimated separately, and then composed into single crf model; this yielded competitive results for arabic, korean, and czech.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3116">
<title id=" E12-1050.xml">feature rich partofspeech tagging for morphologically complex languages application to bulgarian </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>for czech, hajic?
</prevsent>
<prevsent>et al(2001) combined hidden markov model (hmm) with linguistic rules, which yielded 95.2% accuracy using an inventory of over 1,400 tags from the prague dependency treebank (hajic?, 1998).
</prevsent>
</prevsection>
<citsent citstr=" P08-2009 ">
for icelandic, dredze and wallenberg (2008) <papid> P08-2009 </papid>reported 92.1% accuracy with 639 tags developed for the icelandic frequency lexicon (pind et al 1991), they used guided learning and tag decomposition: first, coarse pos class is assigned (e.g., noun, verb, adjective), then, additional fine-grained morphological features like case, number and gender are added, and finally, the proposed tags are further reconsidered using non-local features.</citsent>
<aftsection>
<nextsent>similarly, smith et al(2005) <papid> H05-1060 </papid>decomposed the complex tags into factors, where models for predicting part-of-speech, gender, number, case, and lemma are estimated separately, and then composed into single crf model; this yielded competitive results for arabic, korean, and czech.</nextsent>
<nextsent>most previous work on bulgarian pos tagging has started with large tagsets, which were then reduced.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3117">
<title id=" E12-1050.xml">feature rich partofspeech tagging for morphologically complex languages application to bulgarian </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>et al(2001) combined hidden markov model (hmm) with linguistic rules, which yielded 95.2% accuracy using an inventory of over 1,400 tags from the prague dependency treebank (hajic?, 1998).
</prevsent>
<prevsent>for icelandic, dredze and wallenberg (2008) <papid> P08-2009 </papid>reported 92.1% accuracy with 639 tags developed for the icelandic frequency lexicon (pind et al 1991), they used guided learning and tag decomposition: first, coarse pos class is assigned (e.g., noun, verb, adjective), then, additional fine-grained morphological features like case, number and gender are added, and finally, the proposed tags are further reconsidered using non-local features.</prevsent>
</prevsection>
<citsent citstr=" H05-1060 ">
similarly, smith et al(2005) <papid> H05-1060 </papid>decomposed the complex tags into factors, where models for predicting part-of-speech, gender, number, case, and lemma are estimated separately, and then composed into single crf model; this yielded competitive results for arabic, korean, and czech.</citsent>
<aftsection>
<nextsent>most previous work on bulgarian pos tagging has started with large tagsets, which were then reduced.
</nextsent>
<nextsent>for example, dojchinova and mihov (2004) mapped their initial tagset of 946 tags to just 40, which allowed them to achieve 95.5% accuracy using the transformation-based learning of brill (1995), <papid> J95-4004 </papid>and 98.4% accuracy using manually crafted linguistic rules.</nextsent>
<nextsent>similarly, georgiev et al(2009), who used maximum entropy and the bultreebank (simov et al 2004), group edits 680 fine-grained pos tags into 95 coarse grained ones, and thus improved their accuracy from 90.34% to 94.4%.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3120">
<title id=" E12-1050.xml">feature rich partofspeech tagging for morphologically complex languages application to bulgarian </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>similarly, georgiev et al(2009), who used maximum entropy and the bultreebank (simov et al 2004), group edits 680 fine-grained pos tags into 95 coarse grained ones, and thus improved their accuracy from 90.34% to 94.4%.
</prevsent>
<prevsent>simov and osenova (2001) used recurrent neural network to predict (a) 160 morpho-syntactic tags (92.9% accuracy) and (b) 15 pos tags (95.2% accuracy).
</prevsent>
</prevsection>
<citsent citstr=" C02-1027 ">
some researchers did not reduce the tagset:savkov et al(2011) used 680 tags (94.7% ac curacy), and tanev and mitkov (2002) <papid> C02-1027 </papid>used 303tags and the bulmorph morphological analyzer (krushkov, 1997), achieving p=r=95%.</citsent>
<aftsection>
<nextsent>bulgarian is an indo-european language from the slavic language group, written with the cyrillic alphabet and spoken by about 9-12 million people.
</nextsent>
<nextsent>it is also member of the balkan sprachbund and thus differs from most other slavic languages: it has no case declensions, uses suffixed definite article (which has short and long form for singular masculine), and lacks verb infinitive forms.it further uses special evidential verb forms to express un witnessed, retold, and doubtful activities.
</nextsent>
<nextsent>bulgarian is an inflective language with very rich morphology.
</nextsent>
<nextsent>for example, bulgarian verbs have 52 synthetic word forms on average, while pronouns have altogether more than ten grammatical features (not necessarily shared by all pro nouns), including case, gender, person, number, definite ness, etc. 493this rich morphology inevitably leads to ambiguity proliferation; our analysis of bultreebank shows four major types of ambiguity: 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3133">
<title id=" E12-1050.xml">feature rich partofspeech tagging for morphologically complex languages application to bulgarian </title>
<section> method.  </section>
<citcontext>
<prevsection>
<prevsent>more bigram and trigram features and bi-.
</prevsent>
<prevsent>lexical features as in (shen et al 2007).<papid> P07-1096 </papid></prevsent>
</prevsection>
<citsent citstr=" H05-1059 ">
note that we allowed prefixes and suffixes of length up to 9, as in (toutanova et al 2003) <papid> N03-1033 </papid>and (tsuruoka and tsujii, 2005).<papid> H05-1059 </papid></citsent>
<aftsection>
<nextsent>494 we further extended the set of features with the tags proposed for the current word token by amorphological lexicon, which maps words to possible tags; it is exhaustive, i.e., the correct tag is always among the suggested ones for each token.we also used 70 linguistically-motivated, high precision rules in order to further reduce the number of possible tags suggested by the lexicon.the rules are similar to those proposed by hinrichs and trushkina (2004) for german; we implemented them as constraints in the clark system (simov et al 2003).<papid> E03-2015 </papid></nextsent>
<nextsent>here is an example of rule: if word form is ambiguous between masculine count noun (ncmt) and singular short definite masculine noun (ncmsh), the ncmt tag should be chosen if the previous token is numeral or number.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3134">
<title id=" E12-1050.xml">feature rich partofspeech tagging for morphologically complex languages application to bulgarian </title>
<section> method.  </section>
<citcontext>
<prevsection>
<prevsent>lexical features as in (shen et al 2007).<papid> P07-1096 </papid></prevsent>
<prevsent>note that we allowed prefixes and suffixes of length up to 9, as in (toutanova et al 2003) <papid> N03-1033 </papid>and (tsuruoka and tsujii, 2005).<papid> H05-1059 </papid></prevsent>
</prevsection>
<citsent citstr=" E03-2015 ">
494 we further extended the set of features with the tags proposed for the current word token by amorphological lexicon, which maps words to possible tags; it is exhaustive, i.e., the correct tag is always among the suggested ones for each token.we also used 70 linguistically-motivated, high precision rules in order to further reduce the number of possible tags suggested by the lexicon.the rules are similar to those proposed by hinrichs and trushkina (2004) for german; we implemented them as constraints in the clark system (simov et al 2003).<papid> E03-2015 </papid></citsent>
<aftsection>
<nextsent>here is an example of rule: if word form is ambiguous between masculine count noun (ncmt) and singular short definite masculine noun (ncmsh), the ncmt tag should be chosen if the previous token is numeral or number.
</nextsent>
<nextsent>the 70 rules were developed by linguists based on observations over the training dataset only.
</nextsent>
<nextsent>they target primarily the most frequent cases of ambiguity, and to lesser extent some infrequent but very problematic cases.
</nextsent>
<nextsent>some rules operate over classes of words, while other refer to particular wordforms.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3138">
<title id=" E12-1050.xml">feature rich partofspeech tagging for morphologically complex languages application to bulgarian </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>comparing line 7 to line 8 in table 4, we can see that this yields further token-level improvement: from 97.93% to 97.98%.
</prevsent>
<prevsent>table 5 compares our results to previously reported evaluation results for bulgarian.
</prevsent>
</prevsection>
<citsent citstr=" W96-0102 ">
the first four lines show the token-level accuracy for standard pos tagging tools trained and evaluated on the bultreebank:2 tree tagger (schmid, 1994), which uses decision trees, tnt (brants, 2000), <papid> A00-1031 </papid>which uses hidden markov model, svmtool (gimenez and ma`rquez, 2004), which is based on support vector machines, and acopost (schroder, 2002), implementing the memory-based model of daelemans et al(1996).<papid> W96-0102 </papid>the following lines report the token-level accuracy reported in previous work, as compared to our own experiments using guided learning.</citsent>
<aftsection>
<nextsent>we can see that we outperform by very large margin (92.53% vs. 97.98%, which represents 73% error reduction) the systems from the first four lines, which are directly comparable to our experiments: they are trained and evaluated on the bultreebank using the full inventory of 680 tags.we further achieved statistically significant improvement (p   0.0001; pearsons chi-squared test (plackett, 1983)) over the best pervious result on 680 tags: from 94.65% to 97.98%, which represents 62.24% error reduction at the token-level.2we used the pre-trained treetagger; for the rest, we report the accuracy given on the webpage of the bultreebank: www.bultreebank.org/taggers/taggers.html 497 lexicon linguistic rules (applied to filter): beam accuracy (%) # (source of) (a) the lexicon features (b) the output tags size sentence-level token-level 1 ? ?
</nextsent>
<nextsent>1 52.95 95.72 2 ? ?
</nextsent>
<nextsent>yes 1 64.50 97.20 3 features ? ?
</nextsent>
<nextsent>1 70.40 97.83 4 features yes ? 1 70.30 97.80 5 features yes, for test only ? 1 70.40 97.84 6 features ? yes 1 71.34 97.91 7 features yes yes 1 71.69 97.93 8 features yes yes 3 71.94 97.98 table 4: evaluation results on the test dataset.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3145">
<title id=" E12-1050.xml">feature rich partofspeech tagging for morphologically complex languages application to bulgarian </title>
<section> npnsi npnsd.  </section>
<citcontext>
<prevsection>
<prevsent>by combining large morphological lexicon with prior linguistic knowledge and guided learning from pos-annotated corpus, we achieved accuracy of 97.98%, which is significant improvement over the state-of-the-art for bulgarian.
</prevsent>
<prevsent>our token-levelaccuracy is also comparable to the best results reported for english.
</prevsent>
</prevsection>
<citsent citstr=" P11-1070 ">
in future work, we want to experiment with richer set of features, e.g., derived from unlabeled data (sgaard, 2011) or from the web (umansky pesin et al 2010; bansal and klein, 2011).<papid> P11-1070 </papid></citsent>
<aftsection>
<nextsent>we further plan to explore ways to decompose the complex bulgarian morpho-syntactic tags, e.g., as proposed in (simov and osenova, 2001) and (smith et al 2005).<papid> H05-1060 </papid></nextsent>
<nextsent>modeling long-distance syntactic dependencies (dredze and wallenberg, 2008) <papid> P08-2009 </papid>is another promising direction; we believe this can be implemented efficiently using posterior regularization (graca et al 2009) or expectation constraints (bellare et al 2009).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3148">
<title id=" E09-1088.xml">sequential labeling with latent variables an exact inference algorithm and its efficient approximation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>furthermore, we describe straightforward solution on approximating the ldi, and show that the approximated ldi performs as well as the exact ldi, while the speed is much faster.
</prevsent>
<prevsent>our experiments demonstrate that the proposed inference algorithm outperforms existing inference methods on variety of natural language processing tasks.
</prevsent>
</prevsection>
<citsent citstr=" P05-1010 ">
when data have distinct sub-structures, models exploiting latent variables are advantageous in learning (matsuzaki et al, 2005; <papid> P05-1010 </papid>petrov and klein, 2007; <papid> N07-1051 </papid>blunsom et al, 2008).<papid> P08-1024 </papid></citsent>
<aftsection>
<nextsent>actually, discriminative probabilistic latent variable models (dplvms) have recently become popular choices for performing variety of tasks with sub-structures, e.g., vision recognition (morency et al, 2007), syntactic parsing (petrov and klein, 2008), and syntactic chunking (sun et al, 2008).<papid> C08-1106 </papid></nextsent>
<nextsent>morency et al (2007) demonstrated that dplvm models could efficiently learn sub-structures of natural problems, and outperform several widely used conventional models, e.g., support vector machines (svms), conditional random fields (crfs) and hidden markov models (hmms).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3149">
<title id=" E09-1088.xml">sequential labeling with latent variables an exact inference algorithm and its efficient approximation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>furthermore, we describe straightforward solution on approximating the ldi, and show that the approximated ldi performs as well as the exact ldi, while the speed is much faster.
</prevsent>
<prevsent>our experiments demonstrate that the proposed inference algorithm outperforms existing inference methods on variety of natural language processing tasks.
</prevsent>
</prevsection>
<citsent citstr=" N07-1051 ">
when data have distinct sub-structures, models exploiting latent variables are advantageous in learning (matsuzaki et al, 2005; <papid> P05-1010 </papid>petrov and klein, 2007; <papid> N07-1051 </papid>blunsom et al, 2008).<papid> P08-1024 </papid></citsent>
<aftsection>
<nextsent>actually, discriminative probabilistic latent variable models (dplvms) have recently become popular choices for performing variety of tasks with sub-structures, e.g., vision recognition (morency et al, 2007), syntactic parsing (petrov and klein, 2008), and syntactic chunking (sun et al, 2008).<papid> C08-1106 </papid></nextsent>
<nextsent>morency et al (2007) demonstrated that dplvm models could efficiently learn sub-structures of natural problems, and outperform several widely used conventional models, e.g., support vector machines (svms), conditional random fields (crfs) and hidden markov models (hmms).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3150">
<title id=" E09-1088.xml">sequential labeling with latent variables an exact inference algorithm and its efficient approximation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>furthermore, we describe straightforward solution on approximating the ldi, and show that the approximated ldi performs as well as the exact ldi, while the speed is much faster.
</prevsent>
<prevsent>our experiments demonstrate that the proposed inference algorithm outperforms existing inference methods on variety of natural language processing tasks.
</prevsent>
</prevsection>
<citsent citstr=" P08-1024 ">
when data have distinct sub-structures, models exploiting latent variables are advantageous in learning (matsuzaki et al, 2005; <papid> P05-1010 </papid>petrov and klein, 2007; <papid> N07-1051 </papid>blunsom et al, 2008).<papid> P08-1024 </papid></citsent>
<aftsection>
<nextsent>actually, discriminative probabilistic latent variable models (dplvms) have recently become popular choices for performing variety of tasks with sub-structures, e.g., vision recognition (morency et al, 2007), syntactic parsing (petrov and klein, 2008), and syntactic chunking (sun et al, 2008).<papid> C08-1106 </papid></nextsent>
<nextsent>morency et al (2007) demonstrated that dplvm models could efficiently learn sub-structures of natural problems, and outperform several widely used conventional models, e.g., support vector machines (svms), conditional random fields (crfs) and hidden markov models (hmms).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3151">
<title id=" E09-1088.xml">sequential labeling with latent variables an exact inference algorithm and its efficient approximation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our experiments demonstrate that the proposed inference algorithm outperforms existing inference methods on variety of natural language processing tasks.
</prevsent>
<prevsent>when data have distinct sub-structures, models exploiting latent variables are advantageous in learning (matsuzaki et al, 2005; <papid> P05-1010 </papid>petrov and klein, 2007; <papid> N07-1051 </papid>blunsom et al, 2008).<papid> P08-1024 </papid></prevsent>
</prevsection>
<citsent citstr=" C08-1106 ">
actually, discriminative probabilistic latent variable models (dplvms) have recently become popular choices for performing variety of tasks with sub-structures, e.g., vision recognition (morency et al, 2007), syntactic parsing (petrov and klein, 2008), and syntactic chunking (sun et al, 2008).<papid> C08-1106 </papid></citsent>
<aftsection>
<nextsent>morency et al (2007) demonstrated that dplvm models could efficiently learn sub-structures of natural problems, and outperform several widely used conventional models, e.g., support vector machines (svms), conditional random fields (crfs) and hidden markov models (hmms).
</nextsent>
<nextsent>petrov and klein (2008) reported on syntactic parsing task that dplvm models can learn more compact and accurate grammars than the conventional techniques without latent variables.
</nextsent>
<nextsent>the effectiveness of dplvms was also shown on syntactic chunking task by sun et al (2008).<papid> C08-1106 </papid></nextsent>
<nextsent>dplvms outperform conventional learning models, as described in the aforementioned publications.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3157">
<title id=" E09-1088.xml">sequential labeling with latent variables an exact inference algorithm and its efficient approximation </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>775 the training stage was kept the same as morency et al (2007).
</prevsent>
<prevsent>in other words, there is no need to change the conventional parameter estimation method on dplvm models for adapting the various inference algorithms in this paper.
</prevsent>
</prevsection>
<citsent citstr=" N03-1028 ">
for more information on training dplvms, refer to morency et al (2007) and petrov and klein (2008).since the crf model is one of the most successful models in sequential labeling tasks (lafferty et al., 2001; sha and pereira, 2003), <papid> N03-1028 </papid>in this paper, wechoosed crfs as baseline model for the compar ison.</citsent>
<aftsection>
<nextsent>note that the feature sets were kept the same in dplvms and crfs.
</nextsent>
<nextsent>also, the optimizer and fine tuning strategy were kept the same.
</nextsent>
<nextsent>4.1 bionlp/nlpba-2004 shared task.
</nextsent>
<nextsent>(bio-ner) our first experiment used the data from thebionlp/nlpba-2004 shared task.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3158">
<title id=" E09-1088.xml">sequential labeling with latent variables an exact inference algorithm and its efficient approximation </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>the bio-ner task contains 5 different named entities with 11 bio encoding labels.
</prevsent>
<prevsent>the standard evaluation metrics for this task are precision (the fraction of output entities matching the reference entities), recall (the fraction of reference entities returned), and the f-measure given by = 2pr/(p+ r).
</prevsent>
</prevsection>
<citsent citstr=" P06-1059 ">
following okanohara et al (2006), <papid> P06-1059 </papid>we used word features, pos features and orthography features (prefix, postfix, uppercase/lowercase, etc.),as listed in table 1.</citsent>
<aftsection>
<nextsent>however, their globally dependent features, like preceding-entity features, were not used in our system.
</nextsent>
<nextsent>also, to speed up the training, features that appeared rarely in the training data were removed.
</nextsent>
<nextsent>for dplvm models, we tuned the number of latent variables per label from 2 to 5 on preliminary experiments, and used the word features: {wi2, wi1, wi, wi+1, wi+2, wi1wi, wiwi+1} ?{hi, hi1hi} pos features: {ti2, ti1, ti, ti+1, ti+2, ti2ti1, ti1ti, titi+1, ti+1ti+2, ti2ti1ti, ti1titi+1, titi+1ti+2} ?{hi, hi1hi} orth.
</nextsent>
<nextsent>features: {oi2, oi1, oi, oi+1, oi+2, oi2oi1, oi1oi, oioi+1, oi+1oi+2} ?{hi, hi1hi} table 1: feature templates used in the bio-nerexperiments.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3159">
<title id=" E09-1088.xml">sequential labeling with latent variables an exact inference algorithm and its efficient approximation </title>
<section> results and discussions.  </section>
<citcontext>
<prevsection>
<prevsent>(the probability of this voter, i.e., (y?)).
</prevsent>
<prevsent>finally, the label path with the highest value, combining scores and confidences, will be the optimal result.
</prevsent>
</prevsection>
<citsent citstr=" W02-1019 ">
for more details of the mbr technique, refer to goel &amp; byrne (2000) and kumar &amp; byrne (2002).<papid> W02-1019 </papid></citsent>
<aftsection>
<nextsent>an advantage of the ldi over the bhp and the bmp is that the ldi can efficiently produce the probabilities of the label sequences in lpn.
</nextsent>
<nextsent>such probabilities can be used directly for performing the mbr reranking.
</nextsent>
<nextsent>we will show that it is easyto employ the mbr reranking for the ldi, because the necessary statistics (e.g., the probabilities of the label paths, y1,y2, . . .yn) are alreadyproduced.
</nextsent>
<nextsent>in other words, by using ldi inference, set of possible label sequences has been collected with associated probabilities.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3160">
<title id=" E06-1044.xml">modelling semantic role pausibility in human sentence processing </title>
<section> test and training data.  </section>
<citcontext>
<prevsection>
<prevsent>data, so we can assume consistency of the ratings.
</prevsent>
<prevsent>however, in comparison to the mcrae dataset, the data is impoverished as it lacks ratings for plausible agents (in terms of the example in table 1, this means there are no ratings for hunter).
</prevsent>
</prevsection>
<citsent citstr=" J03-3005 ">
lastly, we use 180 items from keller and lapata (2003).<papid> J03-3005 </papid></citsent>
<aftsection>
<nextsent>in contrast with the previous two studies,the verbs and nouns for these data were not hand selected for the plausibility of their combination.
</nextsent>
<nextsent>rather, they were extracted from the bnc corpus by frequency criteria: half the verb-noun combinations are seen in the bnc with high, medium and low frequency, half are unseen combinations of the verb set with nouns from the bnc.
</nextsent>
<nextsent>thedata consists of ratings for 30 verbs and 6 arguments each, interpreted as objects.
</nextsent>
<nextsent>the human ratings were gathered using the magnitude estimation technique (bard et al , 1996).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3165">
<title id=" E06-1044.xml">modelling semantic role pausibility in human sentence processing </title>
<section> experiment 2: role labelling.  </section>
<citcontext>
<prevsection>
<prevsent>but since this task has some similarity to role labelling, we can also compare the model to standard role lab eller on both the prediction and role labelling tasks.
</prevsent>
<prevsent>the questions are: how well do we do labelling, and does standard role lab eller also predict human judgements?
</prevsent>
</prevsection>
<citsent citstr=" J02-3001 ">
beginning with work by gildea and jurafsky(2002), <papid> J02-3001 </papid>there has been large interest in semantic role labelling, as evidenced by its adoption as shared task in the senseval-iii competition (framenet data, litkowski, 2004) <papid> W04-0803 </papid>and at the conll-2004 and 2005 conference (propbank data, carreras and mrquez, 2005).</citsent>
<aftsection>
<nextsent>as our model currently focuses on noun phrase arguments only, we do not adopt these test sets but continue to use ours, defining the correct role label to be the onewith the higher probability judgement.
</nextsent>
<nextsent>we evaluate the model on the mcrae test set (recall that the other sets only contain good patients/themes and are therefore susceptible to lab eller biases).we formulate frequency baselines for our training data.
</nextsent>
<nextsent>for propbank, always assigning arg1 results in = 45.7 (43.8 on the full test set).
</nextsent>
<nextsent>for framenet, we assign the most frequent role given the verb, so the baseline is = 34.4 (26.8).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3166">
<title id=" E06-1044.xml">modelling semantic role pausibility in human sentence processing </title>
<section> experiment 2: role labelling.  </section>
<citcontext>
<prevsection>
<prevsent>but since this task has some similarity to role labelling, we can also compare the model to standard role lab eller on both the prediction and role labelling tasks.
</prevsent>
<prevsent>the questions are: how well do we do labelling, and does standard role lab eller also predict human judgements?
</prevsent>
</prevsection>
<citsent citstr=" W04-0803 ">
beginning with work by gildea and jurafsky(2002), <papid> J02-3001 </papid>there has been large interest in semantic role labelling, as evidenced by its adoption as shared task in the senseval-iii competition (framenet data, litkowski, 2004) <papid> W04-0803 </papid>and at the conll-2004 and 2005 conference (propbank data, carreras and mrquez, 2005).</citsent>
<aftsection>
<nextsent>as our model currently focuses on noun phrase arguments only, we do not adopt these test sets but continue to use ours, defining the correct role label to be the onewith the higher probability judgement.
</nextsent>
<nextsent>we evaluate the model on the mcrae test set (recall that the other sets only contain good patients/themes and are therefore susceptible to lab eller biases).we formulate frequency baselines for our training data.
</nextsent>
<nextsent>for propbank, always assigning arg1 results in = 45.7 (43.8 on the full test set).
</nextsent>
<nextsent>for framenet, we assign the most frequent role given the verb, so the baseline is = 34.4 (26.8).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3167">
<title id=" E12-1046.xml">detecting highly confident word translations from comparable corpora without any prior knowledge </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we report our results for italian-english and dutch-english language pairs that outperform the currentstate-of-the-art results by significant margin.
</prevsent>
<prevsent>in addition, we show how to use the algorithm for the construction of high-quality initial seed lexicons of translations.
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
bilingual lexicons serve as an invaluable resource of knowledge in various natural language processing tasks, such as dictionary-based cross language information retrieval (carbonell et al  1997; levow et al  2005) and statistical machine translation (smt) (och and ney, 2003).<papid> J03-1002 </papid></citsent>
<aftsection>
<nextsent>in order to construct high quality bilingual lexicons for different domains, one usually needs to possess parallel corpora or build such lexicons by hand.compiling such lexicons manually is often an expensive and time-consuming task, whereas the methods for mining the lexicons from parallel corpora are not applicable for language pairs and domains where such corpora is unavailable or missing.
</nextsent>
<nextsent>therefore the focus of researchers turned to comparable corpora, which consist of documents with partially overlapping content, usually available in abundance.
</nextsent>
<nextsent>thus, it is much easier to builda high-volume comparable corpus.
</nextsent>
<nextsent>a representative example of such comparable text collection is wikipedia, where one may observe articles discussing the similar topic, but strongly varying in style, length and vocabulary, while still sharing certain amount of main concepts (or topics).over the years, several approaches for mining translations from non-parallel corpora have emerged (rapp, 1995; <papid> P95-1050 </papid>fung and yee, 1998; <papid> P98-1069 </papid>rapp, 1999; <papid> P99-1067 </papid>diab and finch, 2000; dejean et al  2002; <papid> C02-1166 </papid>chiao and zweigenbaum, 2002; <papid> C02-2020 </papid>gaussier et al  2004; <papid> P04-1067 </papid>fung and cheung, 2004; <papid> W04-3208 </papid>morin et al  2007; <papid> P07-1084 </papid>haghighi et al  2008; <papid> P08-1088 </papid>shezaf and rappoport, 2010; <papid> P10-1011 </papid>laroche and langlais, 2010), <papid> C10-1070 </papid>all sharing the same fir thian assumption, often called the distribution ial hypothesis (harris, 1954), which states that words with similar meaning are likely to appear in similar contexts across languages.all these methods have examined different representations of word contexts and different methods for matching words across languages, but they all have in common need for seed lexicon of translations to efficiently bridge the gap between languages.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3168">
<title id=" E12-1046.xml">detecting highly confident word translations from comparable corpora without any prior knowledge </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>therefore the focus of researchers turned to comparable corpora, which consist of documents with partially overlapping content, usually available in abundance.
</prevsent>
<prevsent>thus, it is much easier to builda high-volume comparable corpus.
</prevsent>
</prevsection>
<citsent citstr=" P95-1050 ">
a representative example of such comparable text collection is wikipedia, where one may observe articles discussing the similar topic, but strongly varying in style, length and vocabulary, while still sharing certain amount of main concepts (or topics).over the years, several approaches for mining translations from non-parallel corpora have emerged (rapp, 1995; <papid> P95-1050 </papid>fung and yee, 1998; <papid> P98-1069 </papid>rapp, 1999; <papid> P99-1067 </papid>diab and finch, 2000; dejean et al  2002; <papid> C02-1166 </papid>chiao and zweigenbaum, 2002; <papid> C02-2020 </papid>gaussier et al  2004; <papid> P04-1067 </papid>fung and cheung, 2004; <papid> W04-3208 </papid>morin et al  2007; <papid> P07-1084 </papid>haghighi et al  2008; <papid> P08-1088 </papid>shezaf and rappoport, 2010; <papid> P10-1011 </papid>laroche and langlais, 2010), <papid> C10-1070 </papid>all sharing the same fir thian assumption, often called the distribution ial hypothesis (harris, 1954), which states that words with similar meaning are likely to appear in similar contexts across languages.all these methods have examined different representations of word contexts and different methods for matching words across languages, but they all have in common need for seed lexicon of translations to efficiently bridge the gap between languages.</citsent>
<aftsection>
<nextsent>that seed lexicon is usually crawled from the web or obtained from parallel corpora.recently, li et al (2011) <papid> P11-2083 </papid>have proposed an approach that improves precision of the existing methods for bilingual lexicon extraction, basedon improving the comparability of the corpus under consideration, prior to extracting actual bilingual lexicons.</nextsent>
<nextsent>other methods such as (koehn and knight, 2002) <papid> W02-0902 </papid>try to design bootstrapping algorithm based on an initial seed lexicon of translations and various lexical evidences.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3169">
<title id=" E12-1046.xml">detecting highly confident word translations from comparable corpora without any prior knowledge </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>therefore the focus of researchers turned to comparable corpora, which consist of documents with partially overlapping content, usually available in abundance.
</prevsent>
<prevsent>thus, it is much easier to builda high-volume comparable corpus.
</prevsent>
</prevsection>
<citsent citstr=" P98-1069 ">
a representative example of such comparable text collection is wikipedia, where one may observe articles discussing the similar topic, but strongly varying in style, length and vocabulary, while still sharing certain amount of main concepts (or topics).over the years, several approaches for mining translations from non-parallel corpora have emerged (rapp, 1995; <papid> P95-1050 </papid>fung and yee, 1998; <papid> P98-1069 </papid>rapp, 1999; <papid> P99-1067 </papid>diab and finch, 2000; dejean et al  2002; <papid> C02-1166 </papid>chiao and zweigenbaum, 2002; <papid> C02-2020 </papid>gaussier et al  2004; <papid> P04-1067 </papid>fung and cheung, 2004; <papid> W04-3208 </papid>morin et al  2007; <papid> P07-1084 </papid>haghighi et al  2008; <papid> P08-1088 </papid>shezaf and rappoport, 2010; <papid> P10-1011 </papid>laroche and langlais, 2010), <papid> C10-1070 </papid>all sharing the same fir thian assumption, often called the distribution ial hypothesis (harris, 1954), which states that words with similar meaning are likely to appear in similar contexts across languages.all these methods have examined different representations of word contexts and different methods for matching words across languages, but they all have in common need for seed lexicon of translations to efficiently bridge the gap between languages.</citsent>
<aftsection>
<nextsent>that seed lexicon is usually crawled from the web or obtained from parallel corpora.recently, li et al (2011) <papid> P11-2083 </papid>have proposed an approach that improves precision of the existing methods for bilingual lexicon extraction, basedon improving the comparability of the corpus under consideration, prior to extracting actual bilingual lexicons.</nextsent>
<nextsent>other methods such as (koehn and knight, 2002) <papid> W02-0902 </papid>try to design bootstrapping algorithm based on an initial seed lexicon of translations and various lexical evidences.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3170">
<title id=" E12-1046.xml">detecting highly confident word translations from comparable corpora without any prior knowledge </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>therefore the focus of researchers turned to comparable corpora, which consist of documents with partially overlapping content, usually available in abundance.
</prevsent>
<prevsent>thus, it is much easier to builda high-volume comparable corpus.
</prevsent>
</prevsection>
<citsent citstr=" P99-1067 ">
a representative example of such comparable text collection is wikipedia, where one may observe articles discussing the similar topic, but strongly varying in style, length and vocabulary, while still sharing certain amount of main concepts (or topics).over the years, several approaches for mining translations from non-parallel corpora have emerged (rapp, 1995; <papid> P95-1050 </papid>fung and yee, 1998; <papid> P98-1069 </papid>rapp, 1999; <papid> P99-1067 </papid>diab and finch, 2000; dejean et al  2002; <papid> C02-1166 </papid>chiao and zweigenbaum, 2002; <papid> C02-2020 </papid>gaussier et al  2004; <papid> P04-1067 </papid>fung and cheung, 2004; <papid> W04-3208 </papid>morin et al  2007; <papid> P07-1084 </papid>haghighi et al  2008; <papid> P08-1088 </papid>shezaf and rappoport, 2010; <papid> P10-1011 </papid>laroche and langlais, 2010), <papid> C10-1070 </papid>all sharing the same fir thian assumption, often called the distribution ial hypothesis (harris, 1954), which states that words with similar meaning are likely to appear in similar contexts across languages.all these methods have examined different representations of word contexts and different methods for matching words across languages, but they all have in common need for seed lexicon of translations to efficiently bridge the gap between languages.</citsent>
<aftsection>
<nextsent>that seed lexicon is usually crawled from the web or obtained from parallel corpora.recently, li et al (2011) <papid> P11-2083 </papid>have proposed an approach that improves precision of the existing methods for bilingual lexicon extraction, basedon improving the comparability of the corpus under consideration, prior to extracting actual bilingual lexicons.</nextsent>
<nextsent>other methods such as (koehn and knight, 2002) <papid> W02-0902 </papid>try to design bootstrapping algorithm based on an initial seed lexicon of translations and various lexical evidences.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3171">
<title id=" E12-1046.xml">detecting highly confident word translations from comparable corpora without any prior knowledge </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>therefore the focus of researchers turned to comparable corpora, which consist of documents with partially overlapping content, usually available in abundance.
</prevsent>
<prevsent>thus, it is much easier to builda high-volume comparable corpus.
</prevsent>
</prevsection>
<citsent citstr=" C02-1166 ">
a representative example of such comparable text collection is wikipedia, where one may observe articles discussing the similar topic, but strongly varying in style, length and vocabulary, while still sharing certain amount of main concepts (or topics).over the years, several approaches for mining translations from non-parallel corpora have emerged (rapp, 1995; <papid> P95-1050 </papid>fung and yee, 1998; <papid> P98-1069 </papid>rapp, 1999; <papid> P99-1067 </papid>diab and finch, 2000; dejean et al  2002; <papid> C02-1166 </papid>chiao and zweigenbaum, 2002; <papid> C02-2020 </papid>gaussier et al  2004; <papid> P04-1067 </papid>fung and cheung, 2004; <papid> W04-3208 </papid>morin et al  2007; <papid> P07-1084 </papid>haghighi et al  2008; <papid> P08-1088 </papid>shezaf and rappoport, 2010; <papid> P10-1011 </papid>laroche and langlais, 2010), <papid> C10-1070 </papid>all sharing the same fir thian assumption, often called the distribution ial hypothesis (harris, 1954), which states that words with similar meaning are likely to appear in similar contexts across languages.all these methods have examined different representations of word contexts and different methods for matching words across languages, but they all have in common need for seed lexicon of translations to efficiently bridge the gap between languages.</citsent>
<aftsection>
<nextsent>that seed lexicon is usually crawled from the web or obtained from parallel corpora.recently, li et al (2011) <papid> P11-2083 </papid>have proposed an approach that improves precision of the existing methods for bilingual lexicon extraction, basedon improving the comparability of the corpus under consideration, prior to extracting actual bilingual lexicons.</nextsent>
<nextsent>other methods such as (koehn and knight, 2002) <papid> W02-0902 </papid>try to design bootstrapping algorithm based on an initial seed lexicon of translations and various lexical evidences.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3172">
<title id=" E12-1046.xml">detecting highly confident word translations from comparable corpora without any prior knowledge </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>therefore the focus of researchers turned to comparable corpora, which consist of documents with partially overlapping content, usually available in abundance.
</prevsent>
<prevsent>thus, it is much easier to builda high-volume comparable corpus.
</prevsent>
</prevsection>
<citsent citstr=" C02-2020 ">
a representative example of such comparable text collection is wikipedia, where one may observe articles discussing the similar topic, but strongly varying in style, length and vocabulary, while still sharing certain amount of main concepts (or topics).over the years, several approaches for mining translations from non-parallel corpora have emerged (rapp, 1995; <papid> P95-1050 </papid>fung and yee, 1998; <papid> P98-1069 </papid>rapp, 1999; <papid> P99-1067 </papid>diab and finch, 2000; dejean et al  2002; <papid> C02-1166 </papid>chiao and zweigenbaum, 2002; <papid> C02-2020 </papid>gaussier et al  2004; <papid> P04-1067 </papid>fung and cheung, 2004; <papid> W04-3208 </papid>morin et al  2007; <papid> P07-1084 </papid>haghighi et al  2008; <papid> P08-1088 </papid>shezaf and rappoport, 2010; <papid> P10-1011 </papid>laroche and langlais, 2010), <papid> C10-1070 </papid>all sharing the same fir thian assumption, often called the distribution ial hypothesis (harris, 1954), which states that words with similar meaning are likely to appear in similar contexts across languages.all these methods have examined different representations of word contexts and different methods for matching words across languages, but they all have in common need for seed lexicon of translations to efficiently bridge the gap between languages.</citsent>
<aftsection>
<nextsent>that seed lexicon is usually crawled from the web or obtained from parallel corpora.recently, li et al (2011) <papid> P11-2083 </papid>have proposed an approach that improves precision of the existing methods for bilingual lexicon extraction, basedon improving the comparability of the corpus under consideration, prior to extracting actual bilingual lexicons.</nextsent>
<nextsent>other methods such as (koehn and knight, 2002) <papid> W02-0902 </papid>try to design bootstrapping algorithm based on an initial seed lexicon of translations and various lexical evidences.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3173">
<title id=" E12-1046.xml">detecting highly confident word translations from comparable corpora without any prior knowledge </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>therefore the focus of researchers turned to comparable corpora, which consist of documents with partially overlapping content, usually available in abundance.
</prevsent>
<prevsent>thus, it is much easier to builda high-volume comparable corpus.
</prevsent>
</prevsection>
<citsent citstr=" P04-1067 ">
a representative example of such comparable text collection is wikipedia, where one may observe articles discussing the similar topic, but strongly varying in style, length and vocabulary, while still sharing certain amount of main concepts (or topics).over the years, several approaches for mining translations from non-parallel corpora have emerged (rapp, 1995; <papid> P95-1050 </papid>fung and yee, 1998; <papid> P98-1069 </papid>rapp, 1999; <papid> P99-1067 </papid>diab and finch, 2000; dejean et al  2002; <papid> C02-1166 </papid>chiao and zweigenbaum, 2002; <papid> C02-2020 </papid>gaussier et al  2004; <papid> P04-1067 </papid>fung and cheung, 2004; <papid> W04-3208 </papid>morin et al  2007; <papid> P07-1084 </papid>haghighi et al  2008; <papid> P08-1088 </papid>shezaf and rappoport, 2010; <papid> P10-1011 </papid>laroche and langlais, 2010), <papid> C10-1070 </papid>all sharing the same fir thian assumption, often called the distribution ial hypothesis (harris, 1954), which states that words with similar meaning are likely to appear in similar contexts across languages.all these methods have examined different representations of word contexts and different methods for matching words across languages, but they all have in common need for seed lexicon of translations to efficiently bridge the gap between languages.</citsent>
<aftsection>
<nextsent>that seed lexicon is usually crawled from the web or obtained from parallel corpora.recently, li et al (2011) <papid> P11-2083 </papid>have proposed an approach that improves precision of the existing methods for bilingual lexicon extraction, basedon improving the comparability of the corpus under consideration, prior to extracting actual bilingual lexicons.</nextsent>
<nextsent>other methods such as (koehn and knight, 2002) <papid> W02-0902 </papid>try to design bootstrapping algorithm based on an initial seed lexicon of translations and various lexical evidences.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3174">
<title id=" E12-1046.xml">detecting highly confident word translations from comparable corpora without any prior knowledge </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>therefore the focus of researchers turned to comparable corpora, which consist of documents with partially overlapping content, usually available in abundance.
</prevsent>
<prevsent>thus, it is much easier to builda high-volume comparable corpus.
</prevsent>
</prevsection>
<citsent citstr=" W04-3208 ">
a representative example of such comparable text collection is wikipedia, where one may observe articles discussing the similar topic, but strongly varying in style, length and vocabulary, while still sharing certain amount of main concepts (or topics).over the years, several approaches for mining translations from non-parallel corpora have emerged (rapp, 1995; <papid> P95-1050 </papid>fung and yee, 1998; <papid> P98-1069 </papid>rapp, 1999; <papid> P99-1067 </papid>diab and finch, 2000; dejean et al  2002; <papid> C02-1166 </papid>chiao and zweigenbaum, 2002; <papid> C02-2020 </papid>gaussier et al  2004; <papid> P04-1067 </papid>fung and cheung, 2004; <papid> W04-3208 </papid>morin et al  2007; <papid> P07-1084 </papid>haghighi et al  2008; <papid> P08-1088 </papid>shezaf and rappoport, 2010; <papid> P10-1011 </papid>laroche and langlais, 2010), <papid> C10-1070 </papid>all sharing the same fir thian assumption, often called the distribution ial hypothesis (harris, 1954), which states that words with similar meaning are likely to appear in similar contexts across languages.all these methods have examined different representations of word contexts and different methods for matching words across languages, but they all have in common need for seed lexicon of translations to efficiently bridge the gap between languages.</citsent>
<aftsection>
<nextsent>that seed lexicon is usually crawled from the web or obtained from parallel corpora.recently, li et al (2011) <papid> P11-2083 </papid>have proposed an approach that improves precision of the existing methods for bilingual lexicon extraction, basedon improving the comparability of the corpus under consideration, prior to extracting actual bilingual lexicons.</nextsent>
<nextsent>other methods such as (koehn and knight, 2002) <papid> W02-0902 </papid>try to design bootstrapping algorithm based on an initial seed lexicon of translations and various lexical evidences.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3175">
<title id=" E12-1046.xml">detecting highly confident word translations from comparable corpora without any prior knowledge </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>therefore the focus of researchers turned to comparable corpora, which consist of documents with partially overlapping content, usually available in abundance.
</prevsent>
<prevsent>thus, it is much easier to builda high-volume comparable corpus.
</prevsent>
</prevsection>
<citsent citstr=" P07-1084 ">
a representative example of such comparable text collection is wikipedia, where one may observe articles discussing the similar topic, but strongly varying in style, length and vocabulary, while still sharing certain amount of main concepts (or topics).over the years, several approaches for mining translations from non-parallel corpora have emerged (rapp, 1995; <papid> P95-1050 </papid>fung and yee, 1998; <papid> P98-1069 </papid>rapp, 1999; <papid> P99-1067 </papid>diab and finch, 2000; dejean et al  2002; <papid> C02-1166 </papid>chiao and zweigenbaum, 2002; <papid> C02-2020 </papid>gaussier et al  2004; <papid> P04-1067 </papid>fung and cheung, 2004; <papid> W04-3208 </papid>morin et al  2007; <papid> P07-1084 </papid>haghighi et al  2008; <papid> P08-1088 </papid>shezaf and rappoport, 2010; <papid> P10-1011 </papid>laroche and langlais, 2010), <papid> C10-1070 </papid>all sharing the same fir thian assumption, often called the distribution ial hypothesis (harris, 1954), which states that words with similar meaning are likely to appear in similar contexts across languages.all these methods have examined different representations of word contexts and different methods for matching words across languages, but they all have in common need for seed lexicon of translations to efficiently bridge the gap between languages.</citsent>
<aftsection>
<nextsent>that seed lexicon is usually crawled from the web or obtained from parallel corpora.recently, li et al (2011) <papid> P11-2083 </papid>have proposed an approach that improves precision of the existing methods for bilingual lexicon extraction, basedon improving the comparability of the corpus under consideration, prior to extracting actual bilingual lexicons.</nextsent>
<nextsent>other methods such as (koehn and knight, 2002) <papid> W02-0902 </papid>try to design bootstrapping algorithm based on an initial seed lexicon of translations and various lexical evidences.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3176">
<title id=" E12-1046.xml">detecting highly confident word translations from comparable corpora without any prior knowledge </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>therefore the focus of researchers turned to comparable corpora, which consist of documents with partially overlapping content, usually available in abundance.
</prevsent>
<prevsent>thus, it is much easier to builda high-volume comparable corpus.
</prevsent>
</prevsection>
<citsent citstr=" P08-1088 ">
a representative example of such comparable text collection is wikipedia, where one may observe articles discussing the similar topic, but strongly varying in style, length and vocabulary, while still sharing certain amount of main concepts (or topics).over the years, several approaches for mining translations from non-parallel corpora have emerged (rapp, 1995; <papid> P95-1050 </papid>fung and yee, 1998; <papid> P98-1069 </papid>rapp, 1999; <papid> P99-1067 </papid>diab and finch, 2000; dejean et al  2002; <papid> C02-1166 </papid>chiao and zweigenbaum, 2002; <papid> C02-2020 </papid>gaussier et al  2004; <papid> P04-1067 </papid>fung and cheung, 2004; <papid> W04-3208 </papid>morin et al  2007; <papid> P07-1084 </papid>haghighi et al  2008; <papid> P08-1088 </papid>shezaf and rappoport, 2010; <papid> P10-1011 </papid>laroche and langlais, 2010), <papid> C10-1070 </papid>all sharing the same fir thian assumption, often called the distribution ial hypothesis (harris, 1954), which states that words with similar meaning are likely to appear in similar contexts across languages.all these methods have examined different representations of word contexts and different methods for matching words across languages, but they all have in common need for seed lexicon of translations to efficiently bridge the gap between languages.</citsent>
<aftsection>
<nextsent>that seed lexicon is usually crawled from the web or obtained from parallel corpora.recently, li et al (2011) <papid> P11-2083 </papid>have proposed an approach that improves precision of the existing methods for bilingual lexicon extraction, basedon improving the comparability of the corpus under consideration, prior to extracting actual bilingual lexicons.</nextsent>
<nextsent>other methods such as (koehn and knight, 2002) <papid> W02-0902 </papid>try to design bootstrapping algorithm based on an initial seed lexicon of translations and various lexical evidences.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3177">
<title id=" E12-1046.xml">detecting highly confident word translations from comparable corpora without any prior knowledge </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>therefore the focus of researchers turned to comparable corpora, which consist of documents with partially overlapping content, usually available in abundance.
</prevsent>
<prevsent>thus, it is much easier to builda high-volume comparable corpus.
</prevsent>
</prevsection>
<citsent citstr=" P10-1011 ">
a representative example of such comparable text collection is wikipedia, where one may observe articles discussing the similar topic, but strongly varying in style, length and vocabulary, while still sharing certain amount of main concepts (or topics).over the years, several approaches for mining translations from non-parallel corpora have emerged (rapp, 1995; <papid> P95-1050 </papid>fung and yee, 1998; <papid> P98-1069 </papid>rapp, 1999; <papid> P99-1067 </papid>diab and finch, 2000; dejean et al  2002; <papid> C02-1166 </papid>chiao and zweigenbaum, 2002; <papid> C02-2020 </papid>gaussier et al  2004; <papid> P04-1067 </papid>fung and cheung, 2004; <papid> W04-3208 </papid>morin et al  2007; <papid> P07-1084 </papid>haghighi et al  2008; <papid> P08-1088 </papid>shezaf and rappoport, 2010; <papid> P10-1011 </papid>laroche and langlais, 2010), <papid> C10-1070 </papid>all sharing the same fir thian assumption, often called the distribution ial hypothesis (harris, 1954), which states that words with similar meaning are likely to appear in similar contexts across languages.all these methods have examined different representations of word contexts and different methods for matching words across languages, but they all have in common need for seed lexicon of translations to efficiently bridge the gap between languages.</citsent>
<aftsection>
<nextsent>that seed lexicon is usually crawled from the web or obtained from parallel corpora.recently, li et al (2011) <papid> P11-2083 </papid>have proposed an approach that improves precision of the existing methods for bilingual lexicon extraction, basedon improving the comparability of the corpus under consideration, prior to extracting actual bilingual lexicons.</nextsent>
<nextsent>other methods such as (koehn and knight, 2002) <papid> W02-0902 </papid>try to design bootstrapping algorithm based on an initial seed lexicon of translations and various lexical evidences.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3178">
<title id=" E12-1046.xml">detecting highly confident word translations from comparable corpora without any prior knowledge </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>therefore the focus of researchers turned to comparable corpora, which consist of documents with partially overlapping content, usually available in abundance.
</prevsent>
<prevsent>thus, it is much easier to builda high-volume comparable corpus.
</prevsent>
</prevsection>
<citsent citstr=" C10-1070 ">
a representative example of such comparable text collection is wikipedia, where one may observe articles discussing the similar topic, but strongly varying in style, length and vocabulary, while still sharing certain amount of main concepts (or topics).over the years, several approaches for mining translations from non-parallel corpora have emerged (rapp, 1995; <papid> P95-1050 </papid>fung and yee, 1998; <papid> P98-1069 </papid>rapp, 1999; <papid> P99-1067 </papid>diab and finch, 2000; dejean et al  2002; <papid> C02-1166 </papid>chiao and zweigenbaum, 2002; <papid> C02-2020 </papid>gaussier et al  2004; <papid> P04-1067 </papid>fung and cheung, 2004; <papid> W04-3208 </papid>morin et al  2007; <papid> P07-1084 </papid>haghighi et al  2008; <papid> P08-1088 </papid>shezaf and rappoport, 2010; <papid> P10-1011 </papid>laroche and langlais, 2010), <papid> C10-1070 </papid>all sharing the same fir thian assumption, often called the distribution ial hypothesis (harris, 1954), which states that words with similar meaning are likely to appear in similar contexts across languages.all these methods have examined different representations of word contexts and different methods for matching words across languages, but they all have in common need for seed lexicon of translations to efficiently bridge the gap between languages.</citsent>
<aftsection>
<nextsent>that seed lexicon is usually crawled from the web or obtained from parallel corpora.recently, li et al (2011) <papid> P11-2083 </papid>have proposed an approach that improves precision of the existing methods for bilingual lexicon extraction, basedon improving the comparability of the corpus under consideration, prior to extracting actual bilingual lexicons.</nextsent>
<nextsent>other methods such as (koehn and knight, 2002) <papid> W02-0902 </papid>try to design bootstrapping algorithm based on an initial seed lexicon of translations and various lexical evidences.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3179">
<title id=" E12-1046.xml">detecting highly confident word translations from comparable corpora without any prior knowledge </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>thus, it is much easier to builda high-volume comparable corpus.
</prevsent>
<prevsent>a representative example of such comparable text collection is wikipedia, where one may observe articles discussing the similar topic, but strongly varying in style, length and vocabulary, while still sharing certain amount of main concepts (or topics).over the years, several approaches for mining translations from non-parallel corpora have emerged (rapp, 1995; <papid> P95-1050 </papid>fung and yee, 1998; <papid> P98-1069 </papid>rapp, 1999; <papid> P99-1067 </papid>diab and finch, 2000; dejean et al  2002; <papid> C02-1166 </papid>chiao and zweigenbaum, 2002; <papid> C02-2020 </papid>gaussier et al  2004; <papid> P04-1067 </papid>fung and cheung, 2004; <papid> W04-3208 </papid>morin et al  2007; <papid> P07-1084 </papid>haghighi et al  2008; <papid> P08-1088 </papid>shezaf and rappoport, 2010; <papid> P10-1011 </papid>laroche and langlais, 2010), <papid> C10-1070 </papid>all sharing the same fir thian assumption, often called the distribution ial hypothesis (harris, 1954), which states that words with similar meaning are likely to appear in similar contexts across languages.all these methods have examined different representations of word contexts and different methods for matching words across languages, but they all have in common need for seed lexicon of translations to efficiently bridge the gap between languages.</prevsent>
</prevsection>
<citsent citstr=" P11-2083 ">
that seed lexicon is usually crawled from the web or obtained from parallel corpora.recently, li et al (2011) <papid> P11-2083 </papid>have proposed an approach that improves precision of the existing methods for bilingual lexicon extraction, basedon improving the comparability of the corpus under consideration, prior to extracting actual bilingual lexicons.</citsent>
<aftsection>
<nextsent>other methods such as (koehn and knight, 2002) <papid> W02-0902 </papid>try to design bootstrapping algorithm based on an initial seed lexicon of translations and various lexical evidences.</nextsent>
<nextsent>however, the quality of their initial seed lexicon is disputable, 449since the construction of their lexicon is language pair biased and cannot be completely employed on distant languages.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3180">
<title id=" E12-1046.xml">detecting highly confident word translations from comparable corpora without any prior knowledge </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a representative example of such comparable text collection is wikipedia, where one may observe articles discussing the similar topic, but strongly varying in style, length and vocabulary, while still sharing certain amount of main concepts (or topics).over the years, several approaches for mining translations from non-parallel corpora have emerged (rapp, 1995; <papid> P95-1050 </papid>fung and yee, 1998; <papid> P98-1069 </papid>rapp, 1999; <papid> P99-1067 </papid>diab and finch, 2000; dejean et al  2002; <papid> C02-1166 </papid>chiao and zweigenbaum, 2002; <papid> C02-2020 </papid>gaussier et al  2004; <papid> P04-1067 </papid>fung and cheung, 2004; <papid> W04-3208 </papid>morin et al  2007; <papid> P07-1084 </papid>haghighi et al  2008; <papid> P08-1088 </papid>shezaf and rappoport, 2010; <papid> P10-1011 </papid>laroche and langlais, 2010), <papid> C10-1070 </papid>all sharing the same fir thian assumption, often called the distribution ial hypothesis (harris, 1954), which states that words with similar meaning are likely to appear in similar contexts across languages.all these methods have examined different representations of word contexts and different methods for matching words across languages, but they all have in common need for seed lexicon of translations to efficiently bridge the gap between languages.</prevsent>
<prevsent>that seed lexicon is usually crawled from the web or obtained from parallel corpora.recently, li et al (2011) <papid> P11-2083 </papid>have proposed an approach that improves precision of the existing methods for bilingual lexicon extraction, basedon improving the comparability of the corpus under consideration, prior to extracting actual bilingual lexicons.</prevsent>
</prevsection>
<citsent citstr=" W02-0902 ">
other methods such as (koehn and knight, 2002) <papid> W02-0902 </papid>try to design bootstrapping algorithm based on an initial seed lexicon of translations and various lexical evidences.</citsent>
<aftsection>
<nextsent>however, the quality of their initial seed lexicon is disputable, 449since the construction of their lexicon is language pair biased and cannot be completely employed on distant languages.
</nextsent>
<nextsent>it solely relies on unsatisfactory language-pair independent cross-language clues such as words shared across languages.
</nextsent>
<nextsent>recent work from vulic?
</nextsent>
<nextsent>et al 2011) utilized the distributional hypothesis in different direction.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3182">
<title id=" E12-1046.xml">detecting highly confident word translations from comparable corpora without any prior knowledge </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in our setting, it basically means that we keep translation pair (wsi , wtj ) if and only if, after thesymmetrization process, the top translation candidate for the source word wsi is the target word wti and vice versa.
</prevsent>
<prevsent>the one-to-one constraint aims at matching the most confident candidates during the early stages of the algorithm, and then excluding them from further search.
</prevsent>
</prevsection>
<citsent citstr=" J00-2004 ">
the utility of the constraint for parallel corpora has already been evaluated by melamed (2000).<papid> J00-2004 </papid></citsent>
<aftsection>
<nextsent>the remainder of the paper is structured as follows.
</nextsent>
<nextsent>section 2 gives brief overview ofthe methods, relying on per-topic word distributions, which serve as the tool for computing cross language similarity between words.
</nextsent>
<nextsent>in section3, we motivate the main assumptions of the algorithm and describe the full algorithm.
</nextsent>
<nextsent>section 4 justifies the underlying assumptions of the algorithm by providing comparisons with current-state-of-the-art system for italian-englishand dutch-english language pairs.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3183">
<title id=" E12-1046.xml">detecting highly confident word translations from comparable corpora without any prior knowledge </title>
<section> calculating initial cross-language.  </section>
<citcontext>
<prevsection>
<prevsent>word similarity this section gives quick overview of the cue method, the ti method, and their combination, described by vulic?
</prevsent>
<prevsent>et al 2011), which proved tobe the most efficient and accurate for identifying potential word translations once the cross language bilda topic model is trained and the associated per-topic distributions are obtained for both source and target corpora.
</prevsent>
</prevsection>
<citsent citstr=" D09-1092 ">
the bildamodel we use is natural extension of the standard lda model and, along with the definition of per-topic word distributions, has been presented in (ni et al  2009; de smet and moens, 2009; mimno et al  2009).<papid> D09-1092 </papid></citsent>
<aftsection>
<nextsent>bilda takes advantage of the document alignment by using single variable that contains the topic distribution ?.
</nextsent>
<nextsent>this variable is language-independent, because it is shared by each of the paired bilingual comparable documents.
</nextsent>
<nextsent>topics for each document are sampled from ?, from which the words are then sampled in conjugation with the vocabulary distribution ? 450 zsji wsji ? ?
</nextsent>
<nextsent>ztji wtji ? ?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3186">
<title id=" E12-1046.xml">detecting highly confident word translations from comparable corpora without any prior knowledge </title>
<section> results and discussion.  </section>
<citcontext>
<prevsection>
<prevsent>unlike, for instance, wikipedia articles, where document alignment is established via inter lin gual links, in some cases it is necessary to perform document alignment as the initial step.
</prevsent>
<prevsent>since our work focuses on wikipedia data, we will not getinto detail with algorithms for document alignment.
</prevsent>
</prevsection>
<citsent citstr=" P03-1010 ">
an ir-based method for document alignment is given in (utiyama and isahara, 2003; <papid> P03-1010 </papid>munteanu and marcu, 2005), <papid> J05-4003 </papid>and feature-based method can be found in (vu et al  2009).<papid> E09-1096 </papid></citsent>
<aftsection>
<nextsent>4.2 experimental setup.
</nextsent>
<nextsent>all our experiments relyon bilda training with comparable data.
</nextsent>
<nextsent>corpora and software for4in case of europarl, we use only the evidence of document alignment during the training and do not benefit from the parallel ness of the sentences in the corpus.
</nextsent>
<nextsent>bilda training are obtained from vulic?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3187">
<title id=" E12-1046.xml">detecting highly confident word translations from comparable corpora without any prior knowledge </title>
<section> results and discussion.  </section>
<citcontext>
<prevsection>
<prevsent>unlike, for instance, wikipedia articles, where document alignment is established via inter lin gual links, in some cases it is necessary to perform document alignment as the initial step.
</prevsent>
<prevsent>since our work focuses on wikipedia data, we will not getinto detail with algorithms for document alignment.
</prevsent>
</prevsection>
<citsent citstr=" J05-4003 ">
an ir-based method for document alignment is given in (utiyama and isahara, 2003; <papid> P03-1010 </papid>munteanu and marcu, 2005), <papid> J05-4003 </papid>and feature-based method can be found in (vu et al  2009).<papid> E09-1096 </papid></citsent>
<aftsection>
<nextsent>4.2 experimental setup.
</nextsent>
<nextsent>all our experiments relyon bilda training with comparable data.
</nextsent>
<nextsent>corpora and software for4in case of europarl, we use only the evidence of document alignment during the training and do not benefit from the parallel ness of the sentences in the corpus.
</nextsent>
<nextsent>bilda training are obtained from vulic?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3188">
<title id=" E12-1046.xml">detecting highly confident word translations from comparable corpora without any prior knowledge </title>
<section> results and discussion.  </section>
<citcontext>
<prevsection>
<prevsent>unlike, for instance, wikipedia articles, where document alignment is established via inter lin gual links, in some cases it is necessary to perform document alignment as the initial step.
</prevsent>
<prevsent>since our work focuses on wikipedia data, we will not getinto detail with algorithms for document alignment.
</prevsent>
</prevsection>
<citsent citstr=" E09-1096 ">
an ir-based method for document alignment is given in (utiyama and isahara, 2003; <papid> P03-1010 </papid>munteanu and marcu, 2005), <papid> J05-4003 </papid>and feature-based method can be found in (vu et al  2009).<papid> E09-1096 </papid></citsent>
<aftsection>
<nextsent>4.2 experimental setup.
</nextsent>
<nextsent>all our experiments relyon bilda training with comparable data.
</nextsent>
<nextsent>corpora and software for4in case of europarl, we use only the evidence of document alignment during the training and do not benefit from the parallel ness of the sentences in the corpus.
</nextsent>
<nextsent>bilda training are obtained from vulic?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3194">
<title id=" E12-1047.xml">efficient parsing with linear context free rewriting systems </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>discontinuity in constituent structures (cf.
</prevsent>
<prevsent>figure 1 &amp; 2) is important for variety of reasons.
</prevsent>
</prevsection>
<citsent citstr=" A97-1014 ">
for one, it allows tight correspondence between syntax and semantics by letting constituent structure express argument structure (skut et al  1997).<papid> A97-1014 </papid>other reasons are phenomena such as extra position and word-order freedom, which arguably require discontinuous annotations to be treated systematically in phrase-structures (mccawley, 1982;levy, 2005).</citsent>
<aftsection>
<nextsent>empirical investigations demonstrate that discontinuity is present in non-negligibleamounts: around 30% of sentences contain discontinuity in two german treebanks (maier andsgaard, 2008; maier and lichte, 2009).
</nextsent>
<nextsent>recent work on treebank parsing with discontinuous constituents (kallmeyer and maier, 2010; <papid> W10-1407 </papid>maier,2010; <papid> W10-1407 </papid>evang and kallmeyer, 2011; van cranenburgh et al  2011) shows that it is feasible to directly parse discontinuous constituency annotations, as given in the german negra (skut et al  sbarq sq vp whnp md np vb . what should do ? figure 1: tree with wh-movement from the penn treebank, in which traces have been converted to dis continuity.</nextsent>
<nextsent>taken from evang and kallmeyer (2011).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3195">
<title id=" E12-1047.xml">efficient parsing with linear context free rewriting systems </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for one, it allows tight correspondence between syntax and semantics by letting constituent structure express argument structure (skut et al  1997).<papid> A97-1014 </papid>other reasons are phenomena such as extra position and word-order freedom, which arguably require discontinuous annotations to be treated systematically in phrase-structures (mccawley, 1982;levy, 2005).</prevsent>
<prevsent>empirical investigations demonstrate that discontinuity is present in non-negligibleamounts: around 30% of sentences contain discontinuity in two german treebanks (maier andsgaard, 2008; maier and lichte, 2009).</prevsent>
</prevsection>
<citsent citstr=" W10-1407 ">
recent work on treebank parsing with discontinuous constituents (kallmeyer and maier, 2010; <papid> W10-1407 </papid>maier,2010; <papid> W10-1407 </papid>evang and kallmeyer, 2011; van cranenburgh et al  2011) shows that it is feasible to directly parse discontinuous constituency annotations, as given in the german negra (skut et al  sbarq sq vp whnp md np vb . what should do ? figure 1: tree with wh-movement from the penn treebank, in which traces have been converted to dis continuity.</citsent>
<aftsection>
<nextsent>taken from evang and kallmeyer (2011).
</nextsent>
<nextsent>1997) and tiger (brants et al  2002) corpora, or those that can be extracted from traces such as inthe penn treebank (marcus et al  1993) <papid> J93-2004 </papid>annota tion.</nextsent>
<nextsent>however, the computational complexity is such that until now, the length of sentences needed to be restricted.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3202">
<title id=" E12-1047.xml">efficient parsing with linear context free rewriting systems </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>recent work on treebank parsing with discontinuous constituents (kallmeyer and maier, 2010; <papid> W10-1407 </papid>maier,2010; <papid> W10-1407 </papid>evang and kallmeyer, 2011; van cranenburgh et al  2011) shows that it is feasible to directly parse discontinuous constituency annotations, as given in the german negra (skut et al  sbarq sq vp whnp md np vb . what should do ? figure 1: tree with wh-movement from the penn treebank, in which traces have been converted to dis continuity.</prevsent>
<prevsent>taken from evang and kallmeyer (2011).</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
1997) and tiger (brants et al  2002) corpora, or those that can be extracted from traces such as inthe penn treebank (marcus et al  1993) <papid> J93-2004 </papid>annota tion.</citsent>
<aftsection>
<nextsent>however, the computational complexity is such that until now, the length of sentences needed to be restricted.
</nextsent>
<nextsent>in the case of kallmeyer and maier (2010) <papid> W10-1407 </papid>and evang and kallmeyer (2011) the limit was 25 words.</nextsent>
<nextsent>maier (2010) <papid> W10-1407 </papid>and van cranenburgh et al (2011) manage to parse up to 30 words with heuristics and optimizations, but no further.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3210">
<title id=" E12-1047.xml">efficient parsing with linear context free rewriting systems </title>
<section> linear context-free rewriting.  </section>
<citcontext>
<prevsection>
<prevsent>translation: after that coal dust had caught fire.
</prevsent>
<prevsent>systems linear context-free rewriting systems (lcfrs; vijay-shanker et al  1987; weir, 1988) subsumea wide variety of mildly context-sensitive formalisms, such as tree-adjoining grammar (tag),combinatory categorial grammar (ccg), minimalist grammar, multiple context-free grammar (mcfg) and synchronous cfg (vijay-shanker and weir, 1994; kallmeyer, 2010).
</prevsent>
</prevsection>
<citsent citstr=" E09-1055 ">
furthermore,they can be used to parse dependency structures (kuhlmann and satta, 2009).<papid> E09-1055 </papid></citsent>
<aftsection>
<nextsent>since lcfrs subsumes various synchronous grammars, they are also important for machine translation.
</nextsent>
<nextsent>this makes it possible to use lcfrs as syntactic backbone with which various formalisms can be parsed by compiling grammars into an lcfrs, similar to the tulipa system (kallmeyer et al  2008).<papid> W08-1701 </papid></nextsent>
<nextsent>as all mildly context-sensitive formalisms, lcfrs are parsable in polynomial time, where the degree depends on the productions of the grammar.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3211">
<title id=" E12-1047.xml">efficient parsing with linear context free rewriting systems </title>
<section> linear context-free rewriting.  </section>
<citcontext>
<prevsection>
<prevsent>furthermore,they can be used to parse dependency structures (kuhlmann and satta, 2009).<papid> E09-1055 </papid></prevsent>
<prevsent>since lcfrs subsumes various synchronous grammars, they are also important for machine translation.</prevsent>
</prevsection>
<citsent citstr=" W08-1701 ">
this makes it possible to use lcfrs as syntactic backbone with which various formalisms can be parsed by compiling grammars into an lcfrs, similar to the tulipa system (kallmeyer et al  2008).<papid> W08-1701 </papid></citsent>
<aftsection>
<nextsent>as all mildly context-sensitive formalisms, lcfrs are parsable in polynomial time, where the degree depends on the productions of the grammar.
</nextsent>
<nextsent>intuitively, lcfrs can be seen as generalization of context-free grammars to rewriting other objects than just continuous strings: productions are context-free, but instead of strings they can rewrite tuples, trees or graphs.
</nextsent>
<nextsent>we focus on the use of lcfrs for parsing with discontinuous constituents.
</nextsent>
<nextsent>this follows up on recent work on parsing the discontinuous annotations in german corpora with lcfrs (maier, 2010; <papid> W10-1407 </papid>van cranenburgh et al  2011) and work on parsing the wall street journal corpus in which traces have been converted to discontinuous constituents (evang and kallmeyer, 2011).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3222">
<title id=" E12-1047.xml">efficient parsing with linear context free rewriting systems </title>
<section> binarization.  </section>
<citcontext>
<prevsection>
<prevsent>a probabilistic lcfrs can be parsed using cky like tabular parsing algorithm (cf.
</prevsent>
<prevsent>kallmeyer and maier, 2010; <papid> W10-1407 </papid>van cranenburgh et al  2011), but this requires binarized grammar.1 any lcfrs can be binarized.</prevsent>
</prevsection>
<citsent citstr=" P11-1046 ">
crescenzi et al (2011) <papid> P11-1046 </papid>state while cfgs can always be reduced to rank two (chomsky normal form), this is not the case forlcfrs with any fan-out greater than one.?</citsent>
<aftsection>
<nextsent>how ever, this assertion is made under the assumption of fixed fan-out.
</nextsent>
<nextsent>if this assumption is relaxed then it is easy to binarize either deterministically or, as will be investigated in this work, optimally with dynamic programming approach.
</nextsent>
<nextsent>binarizing an lcfrs may increase its fan-out, which results in an increase in asymptotic complexity.
</nextsent>
<nextsent>consider the following production: x(pqrs)?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3224">
<title id=" E12-1047.xml">efficient parsing with linear context free rewriting systems </title>
<section> binarization.  </section>
<citcontext>
<prevsection>
<prevsent>this binarization introduces production with fan-out of 2, which could have been avoided.
</prevsent>
<prevsent>after binarization, an lcfrs can be parsed in o(|g| ? |w|p) time, where |g| is the size of the grammar, |w| is the length of the sentence.
</prevsent>
</prevsection>
<citsent citstr=" N10-1118 ">
the degree of the polynomial is the maximum parsing complexity of rule, defined as: parsing complexity := ?+ 1 + 2 (6) where ? is the fan-out of the left-hand side and 1 and 2 are the fan-outs of the right-hand side of the rule in question (gildea, 2010).<papid> N10-1118 </papid></citsent>
<aftsection>
<nextsent>as gildea(2010) <papid> N10-1118 </papid>shows, there is no one to one correspondence between fan-out and parsing complexity: it is possible that parsing complexity can be reduced by increasing the fan-out of production.</nextsent>
<nextsent>in other words, there can be production which can be bi narized with parsing complexity that is minimal while its fan-out is sub-optimal.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3229">
<title id=" E12-1047.xml">efficient parsing with linear context free rewriting systems </title>
<section> binarization.  </section>
<citcontext>
<prevsection>
<prevsent>3.1 further binarization strategies.
</prevsent>
<prevsent>apart from optimizing for parsing complexity, for linguistic reasons it can also be useful to parse the head of constituent first, yielding so-calledhead-driven binarizations (collins, 1999).
</prevsent>
</prevsection>
<citsent citstr=" P03-1054 ">
additionally, such head-driven binarization can be markovizedi.e., the resulting production can be constrained to apply to limited amount of horizontal context as opposed to the full context in the original constituent (e.g., klein and manning,2003), <papid> P03-1054 </papid>which can have beneficial effect on accu racy.</citsent>
<aftsection>
<nextsent>in the notation of klein and manning (2003) <papid> P03-1054 </papid>there are two markov ization parameters: and v. the first parameter describes the amount of 462 xb x y e 0 1 2 3 4 5 original = 4, ? = 2 xb,c,d,e xc,d,e xd,e x y e 0 1 2 3 4 5 right branching = 5, ? = 2 xb,c,d,e xb,c,d xb,c a c d 0 1 2 3 4 5 optimal = 4, ? = 2 xb xe xd x y e 0 1 2 3 4 5 head-driven = 5, ? = 2 xd xa xb a c d 0 1 2 3 4 5 optimal head-driven = 4, ? = 2 figure 4: the four binarization strategies.</nextsent>
<nextsent>c is the head node.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3245">
<title id=" E12-1047.xml">efficient parsing with linear context free rewriting systems </title>
<section> binarization.  </section>
<citcontext>
<prevsection>
<prevsent>as data we use version 2 of the negra (skut et al 1997) <papid> A97-1014 </papid>treebank, with the common training, devel 1 10 100 1000 10000 100000 3 4 5 6 7 8 9 fr eq ue nc parsing complexity head-driven optimal head-driven figure 6: the distribution of parsing complexity among productions in markov ized, head-driven grammars read off from negra-25.</prevsent>
<prevsent>the y-axis has logarithmic scale.</prevsent>
</prevsection>
<citsent citstr=" P03-1013 ">
opment and test splits (dubey and keller, 2003).<papid> P03-1013 </papid></citsent>
<aftsection>
<nextsent>following common practice, punctuation, whichis left out of the phrase-structure in negra, is reattached to the nearest constituent.
</nextsent>
<nextsent>in the course of experiments it was discovered that the heuristic method for punctuation attachment used in previous work (e.g., maier, 2010; <papid> W10-1407 </papid>van cranenburgh et al  2011), as implemented in rparse,3 introduces additional discontinuity.</nextsent>
<nextsent>we applied slightly different heuristic: punctuation is attached to the highest constituent that contains neighbor to its right.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3258">
<title id=" E12-1047.xml">efficient parsing with linear context free rewriting systems </title>
<section> context-free grammar approximation.  </section>
<citcontext>
<prevsection>
<prevsent>however, the calculation of these estimates is not feasible for longer sentences and large grammars (van cranenburgh et al  2011).another strategy is to perform an online approximation of the sentence to be parsed, after which parsing with the lcfrs can be pruned effectively.
</prevsent>
<prevsent>this is the strategy that will be explored in the current work.
</prevsent>
</prevsection>
<citsent citstr=" N06-1022 ">
for coarse-to-fine parsing coarse-to-fine parsing (charniak et al  2006) <papid> N06-1022 </papid>is technique to speed up parsing by exploiting the information that can be gained from parsing with simpler, coarser grammarse.g., grammar witha smaller set of labels on which the original grammar can be projected.</citsent>
<aftsection>
<nextsent>constituents that do not contribute to full parse tree with coarse grammar can be ruled out for finer grammars as well, which greatly reduces the number of edges that need to be explored.
</nextsent>
<nextsent>however, by changing just the labels only the grammar constant is affected.with discontinuous treebank parsing the asymp totic complexity of the grammar also plays major role.
</nextsent>
<nextsent>therefore we suggest to parse not just with coarser grammar, but with coarser grammar formalism, following suggestion in van cranenburgh et al (2011).
</nextsent>
<nextsent>this idea is inspired by the work of barthelemy et al (2001), who apply it in non-probabilistic setting where the coarse grammar acts as guide to the non-deterministic choices of the fine grammar.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3259">
<title id=" E12-1047.xml">efficient parsing with linear context free rewriting systems </title>
<section> context-free grammar approximation.  </section>
<citcontext>
<prevsection>
<prevsent>instead of using the coarse grammar only as guide to solve non-deterministic choices, we apply it as pruning step which also discards the most sub optimal parses.
</prevsent>
<prevsent>the basic idea is to extract grammar that defines superset of the language we want to parse, but with fan out of 1.
</prevsent>
</prevsection>
<citsent citstr=" W07-1506 ">
more concretely, context-free grammar can be read off from discontinuous trees that have been transformed to context-free trees by the procedure introduced in boyd (2007).<papid> W07-1506 </papid></citsent>
<aftsection>
<nextsent>each discontinuous node is split into set of new nodes, one for each component; for example node np2 will be split into two nodes labeled np*1 and np*2 (like barthelemy et al  we mark components with an index to reduce overgeneration).
</nextsent>
<nextsent>because boyds transformation is reversible, chart items from this grammar can be converted back to discontinuous chart items, and can guide parsing of an lcfrs.this guiding takes the form of white list.
</nextsent>
<nextsent>after parsing with the coarse grammar, the resulting chart is pruned by removing all items that fail to meet certain criterion.
</nextsent>
<nextsent>in our case this is whether chart item is part of one of the k-best derivation swe use = 50 in all experiments (asin van cranenburgh et al  2011).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3263">
<title id=" E06-3004.xml">bootstrapping named entity recognition with automatically generated gazetteer lists </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>automatic information extraction and information retrieval concerning particular person, location, organization, title of movie or book, juxtaposes to the named entity recognition (ner) task.
</prevsent>
<prevsent>ner consists in detecting the most silent and informative elements in text such as names of people, company names, location, monetary currencies, dates.
</prevsent>
</prevsection>
<citsent citstr=" M98-1014 ">
early ner systems (fisher et al, 1997), (black et al, 1998) <papid> M98-1014 </papid>etc., participating in message understanding conferences (muc), used linguistic tools and gazetteer lists.</citsent>
<aftsection>
<nextsent>however these are difficult to develop and domain sensitive.
</nextsent>
<nextsent>to surmount these obstacles, application of machine learning approaches toner became are search subject.
</nextsent>
<nextsent>various state-of-the-art machine learning algorithms such as maximum entropy (borthwick, 1999), adaboost(carreras et al., 2002), <papid> W02-2004 </papid>hidden markov models (bikel et al, ), memory-based based learning (tjong kim sang, 2002b), have been used1.</nextsent>
<nextsent>(klein et al, 2003), (<papid> W03-0428 </papid>mayfield et al, 2003), (<papid> W03-0429 </papid>wu et al, 2003), (<papid> W03-0433 </papid>kozareva et al, 2005c) among others, combined several classifiers to obtain better named entity coverage rate.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3264">
<title id=" E06-3004.xml">bootstrapping named entity recognition with automatically generated gazetteer lists </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however these are difficult to develop and domain sensitive.
</prevsent>
<prevsent>to surmount these obstacles, application of machine learning approaches toner became are search subject.
</prevsent>
</prevsection>
<citsent citstr=" W02-2004 ">
various state-of-the-art machine learning algorithms such as maximum entropy (borthwick, 1999), adaboost(carreras et al., 2002), <papid> W02-2004 </papid>hidden markov models (bikel et al, ), memory-based based learning (tjong kim sang, 2002b), have been used1.</citsent>
<aftsection>
<nextsent>(klein et al, 2003), (<papid> W03-0428 </papid>mayfield et al, 2003), (<papid> W03-0429 </papid>wu et al, 2003), (<papid> W03-0433 </papid>kozareva et al, 2005c) among others, combined several classifiers to obtain better named entity coverage rate.</nextsent>
<nextsent>1for other machine learning methods, consult http://www.cnts.ua.ac.be/conll2002/ner/ http://www.cnts.ua.ac.be/conll2003/ner/nevertheless all these machine learning algorithms relyon previously hand-labeled training data.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3265">
<title id=" E06-3004.xml">bootstrapping named entity recognition with automatically generated gazetteer lists </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>to surmount these obstacles, application of machine learning approaches toner became are search subject.
</prevsent>
<prevsent>various state-of-the-art machine learning algorithms such as maximum entropy (borthwick, 1999), adaboost(carreras et al., 2002), <papid> W02-2004 </papid>hidden markov models (bikel et al, ), memory-based based learning (tjong kim sang, 2002b), have been used1.</prevsent>
</prevsection>
<citsent citstr=" W03-0428 ">
(klein et al, 2003), (<papid> W03-0428 </papid>mayfield et al, 2003), (<papid> W03-0429 </papid>wu et al, 2003), (<papid> W03-0433 </papid>kozareva et al, 2005c) among others, combined several classifiers to obtain better named entity coverage rate.</citsent>
<aftsection>
<nextsent>1for other machine learning methods, consult http://www.cnts.ua.ac.be/conll2002/ner/ http://www.cnts.ua.ac.be/conll2003/ner/nevertheless all these machine learning algorithms relyon previously hand-labeled training data.
</nextsent>
<nextsent>obtaining such data is labor-intensive, time consuming and even might not be present for languages with limited funding.
</nextsent>
<nextsent>resource limitation, directed ner research (collins and singer, 1999), (<papid> W99-0613 </papid>carreras et al, 2003), <papid> E03-1038 </papid>kozareva et al, 2005a) toward the usage of semi-supervised techniques.these techniques are needed, as we live in multilingual society and access to information from various language sources is reality.</nextsent>
<nextsent>the development of ner systems for languages other than english commenced.this paper presents the development of spanish named recognition system based on machine learning approach.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3266">
<title id=" E06-3004.xml">bootstrapping named entity recognition with automatically generated gazetteer lists </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>to surmount these obstacles, application of machine learning approaches toner became are search subject.
</prevsent>
<prevsent>various state-of-the-art machine learning algorithms such as maximum entropy (borthwick, 1999), adaboost(carreras et al., 2002), <papid> W02-2004 </papid>hidden markov models (bikel et al, ), memory-based based learning (tjong kim sang, 2002b), have been used1.</prevsent>
</prevsection>
<citsent citstr=" W03-0429 ">
(klein et al, 2003), (<papid> W03-0428 </papid>mayfield et al, 2003), (<papid> W03-0429 </papid>wu et al, 2003), (<papid> W03-0433 </papid>kozareva et al, 2005c) among others, combined several classifiers to obtain better named entity coverage rate.</citsent>
<aftsection>
<nextsent>1for other machine learning methods, consult http://www.cnts.ua.ac.be/conll2002/ner/ http://www.cnts.ua.ac.be/conll2003/ner/nevertheless all these machine learning algorithms relyon previously hand-labeled training data.
</nextsent>
<nextsent>obtaining such data is labor-intensive, time consuming and even might not be present for languages with limited funding.
</nextsent>
<nextsent>resource limitation, directed ner research (collins and singer, 1999), (<papid> W99-0613 </papid>carreras et al, 2003), <papid> E03-1038 </papid>kozareva et al, 2005a) toward the usage of semi-supervised techniques.these techniques are needed, as we live in multilingual society and access to information from various language sources is reality.</nextsent>
<nextsent>the development of ner systems for languages other than english commenced.this paper presents the development of spanish named recognition system based on machine learning approach.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3267">
<title id=" E06-3004.xml">bootstrapping named entity recognition with automatically generated gazetteer lists </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>to surmount these obstacles, application of machine learning approaches toner became are search subject.
</prevsent>
<prevsent>various state-of-the-art machine learning algorithms such as maximum entropy (borthwick, 1999), adaboost(carreras et al., 2002), <papid> W02-2004 </papid>hidden markov models (bikel et al, ), memory-based based learning (tjong kim sang, 2002b), have been used1.</prevsent>
</prevsection>
<citsent citstr=" W03-0433 ">
(klein et al, 2003), (<papid> W03-0428 </papid>mayfield et al, 2003), (<papid> W03-0429 </papid>wu et al, 2003), (<papid> W03-0433 </papid>kozareva et al, 2005c) among others, combined several classifiers to obtain better named entity coverage rate.</citsent>
<aftsection>
<nextsent>1for other machine learning methods, consult http://www.cnts.ua.ac.be/conll2002/ner/ http://www.cnts.ua.ac.be/conll2003/ner/nevertheless all these machine learning algorithms relyon previously hand-labeled training data.
</nextsent>
<nextsent>obtaining such data is labor-intensive, time consuming and even might not be present for languages with limited funding.
</nextsent>
<nextsent>resource limitation, directed ner research (collins and singer, 1999), (<papid> W99-0613 </papid>carreras et al, 2003), <papid> E03-1038 </papid>kozareva et al, 2005a) toward the usage of semi-supervised techniques.these techniques are needed, as we live in multilingual society and access to information from various language sources is reality.</nextsent>
<nextsent>the development of ner systems for languages other than english commenced.this paper presents the development of spanish named recognition system based on machine learning approach.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3268">
<title id=" E06-3004.xml">bootstrapping named entity recognition with automatically generated gazetteer lists </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>1for other machine learning methods, consult http://www.cnts.ua.ac.be/conll2002/ner/ http://www.cnts.ua.ac.be/conll2003/ner/nevertheless all these machine learning algorithms relyon previously hand-labeled training data.
</prevsent>
<prevsent>obtaining such data is labor-intensive, time consuming and even might not be present for languages with limited funding.
</prevsent>
</prevsection>
<citsent citstr=" W99-0613 ">
resource limitation, directed ner research (collins and singer, 1999), (<papid> W99-0613 </papid>carreras et al, 2003), <papid> E03-1038 </papid>kozareva et al, 2005a) toward the usage of semi-supervised techniques.these techniques are needed, as we live in multilingual society and access to information from various language sources is reality.</citsent>
<aftsection>
<nextsent>the development of ner systems for languages other than english commenced.this paper presents the development of spanish named recognition system based on machine learning approach.
</nextsent>
<nextsent>for it no morphologic or syntactic information was used.
</nextsent>
<nextsent>however, we propose and incorporate very simple method for automatic gazetteer2 construction.
</nextsent>
<nextsent>such method can be easily adapted to other languages and it islow-costly obtained as it relies on n-gram extraction from unlabeled data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3269">
<title id=" E06-3004.xml">bootstrapping named entity recognition with automatically generated gazetteer lists </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>1for other machine learning methods, consult http://www.cnts.ua.ac.be/conll2002/ner/ http://www.cnts.ua.ac.be/conll2003/ner/nevertheless all these machine learning algorithms relyon previously hand-labeled training data.
</prevsent>
<prevsent>obtaining such data is labor-intensive, time consuming and even might not be present for languages with limited funding.
</prevsent>
</prevsection>
<citsent citstr=" E03-1038 ">
resource limitation, directed ner research (collins and singer, 1999), (<papid> W99-0613 </papid>carreras et al, 2003), <papid> E03-1038 </papid>kozareva et al, 2005a) toward the usage of semi-supervised techniques.these techniques are needed, as we live in multilingual society and access to information from various language sources is reality.</citsent>
<aftsection>
<nextsent>the development of ner systems for languages other than english commenced.this paper presents the development of spanish named recognition system based on machine learning approach.
</nextsent>
<nextsent>for it no morphologic or syntactic information was used.
</nextsent>
<nextsent>however, we propose and incorporate very simple method for automatic gazetteer2 construction.
</nextsent>
<nextsent>such method can be easily adapted to other languages and it islow-costly obtained as it relies on n-gram extraction from unlabeled data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3271">
<title id=" E06-3004.xml">bootstrapping named entity recognition with automatically generated gazetteer lists </title>
<section> the ner how to.  </section>
<citcontext>
<prevsection>
<prevsent>for the supervised approach, the labels in the training data were previously known.
</prevsent>
<prevsent>for the semi-supervised approach, the labels in the training data were hidden.
</prevsent>
</prevsection>
<citsent citstr=" P02-1046 ">
we used bootstrapping (abney, 2002) <papid> P02-1046 </papid>which refers to problem setting in which one is given small set of labeled data and large set of unlabeled data, and the task is to induce classifier.</citsent>
<aftsection>
<nextsent>goals:- utilize minimal amount of supervised ex amples; 3president of the united states?
</nextsent>
<nextsent>4technical university of cataluna? 5book titles, sport events, etc.- obtain learning from many unlabeled ex amples; ? general scheme:- initial supervision seed examples for training an initial model; - corpus classification with seed model;- add most confident classifications to training data and iterate.
</nextsent>
<nextsent>in our bootstrapping, newly labeled example was added into the training data l, if the two classifiers c1 and c2 agreed on the class of that example.
</nextsent>
<nextsent>the number of iterations for our experiments is set up to 25 and when this bound is reached the bootstrapping stops.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3273">
<title id=" E06-3004.xml">bootstrapping named entity recognition with automatically generated gazetteer lists </title>
<section> the ner how to.  </section>
<citcontext>
<prevsection>
<prevsent>end for bootstrapping was previously used by (carreras et al, 2003), <papid> E03-1038 </papid>who were interested in recognizing catalan names using spanish resources.</prevsent>
<prevsent>(beckeret al, 2005) employed bootstrapping in an active learning method for tagging entities in an astronomic domain.</prevsent>
</prevsection>
<citsent citstr=" P95-1026 ">
(yarowsky, 1995) <papid> P95-1026 </papid>and (mihalcea and moldovan, 2001) utilized bootstrapping for word sense disambiguation.</citsent>
<aftsection>
<nextsent>(collins and singer, 1999) <papid> W99-0613 </papid>classified nes through co-training,(kozareva et al, 2005a) used self-training and co training to detect and classify named entities innews domain, (shen et al, 2004) <papid> P04-1075 </papid>conducted experiments with multi-criteria-based active learning for biomedical ner.</nextsent>
<nextsent>the experimental data we work with is taken from the conll-2002 competition.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3275">
<title id=" E06-3004.xml">bootstrapping named entity recognition with automatically generated gazetteer lists </title>
<section> the ner how to.  </section>
<citcontext>
<prevsection>
<prevsent>(beckeret al, 2005) employed bootstrapping in an active learning method for tagging entities in an astronomic domain.
</prevsent>
<prevsent>(yarowsky, 1995) <papid> P95-1026 </papid>and (mihalcea and moldovan, 2001) utilized bootstrapping for word sense disambiguation.</prevsent>
</prevsection>
<citsent citstr=" P04-1075 ">
(collins and singer, 1999) <papid> W99-0613 </papid>classified nes through co-training,(kozareva et al, 2005a) used self-training and co training to detect and classify named entities innews domain, (shen et al, 2004) <papid> P04-1075 </papid>conducted experiments with multi-criteria-based active learning for biomedical ner.</citsent>
<aftsection>
<nextsent>the experimental data we work with is taken from the conll-2002 competition.
</nextsent>
<nextsent>the spanish 16corpus6 comes from news domain and was previously manually annotated.
</nextsent>
<nextsent>the train dataset contains 264715 words of which 18798 are entities and the test set has 51533 words of which 3558 are entities.we decided to work with available ne annotated corpora in order to conduct an exhaustive and comparative ner study when labeled and unlabeld data is present.
</nextsent>
<nextsent>for our bootstrapping experiment, we simply ignored the presence of the labels in the training data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3276">
<title id=" E12-1005.xml">evaluating distributional models of semantics for syntactically in variant inference </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we propose two evaluation methods in relation classification and qa which reflect these goals, and apply several recent compositional distributional model sto the tasks.
</prevsent>
<prevsent>we find that the models out perform simple lemma overlap baseline slightly, demonstrating that distributional approaches can already be useful for tasks requiring deeper inference.
</prevsent>
</prevsection>
<citsent citstr=" P08-1028 ">
a number of unsupervised semantic models(mitchell and lapata, 2008, <papid> P08-1028 </papid>for example) have recently been proposed which are inspired at least in part by the distributional hypothesis (harris,1954)that words meaning can be characterized by the contexts in which it appears.</citsent>
<aftsection>
<nextsent>such models represent word meaning as one or morehigh-dimensional vectors which capture the lexical and syntactic contexts of the words occurrences in training corpus.much of the recent work in this area has, following mitchell and lapata (2008), <papid> P08-1028 </papid>focused on the notion of compositionality as the litmus test ofa truly semantic model.</nextsent>
<nextsent>compositionality is natural way to construct representations of linguistic units larger than word, and it has long history in montagovian semantics for dealing with argument structure and assembling rich semantical expressions of the kind found in predicate logic.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3280">
<title id=" E12-1005.xml">evaluating distributional models of semantics for syntactically in variant inference </title>
<section> compositionality and distributional.  </section>
<citcontext>
<prevsection>
<prevsent>our work can be seen as an extension of this notion to distributional semantic models witha more general notion of representational similarity and inference.there are many regular alternations that semantics models have tried to account for such as passive or dative alternations.
</prevsent>
<prevsent>there are also many lexical paraphrases which can take drastically different syntactic forms.
</prevsent>
</prevsection>
<citsent citstr=" D09-1001 ">
take the following example from poon and domingos (2009), <papid> D09-1001 </papid>in which thesame semantic relation can be expressed by transitive verb or an attributive prepositional phrase: (1) utah borders idaho.</citsent>
<aftsection>
<nextsent>utah is next to idaho.in distributional semantics, the original sentence similarity test proposed by kintsch (2001)served as the inspiration for the evaluation performed by mitchell and lapata (2008) <papid> P08-1028 </papid>and most later work in the area.</nextsent>
<nextsent>in transitive verbs are given 34in the context of their syntactic subject, and candidate synonyms are ranked for their appropriateness.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3285">
<title id=" E12-1005.xml">evaluating distributional models of semantics for syntactically in variant inference </title>
<section> compositionality and distributional.  </section>
<citcontext>
<prevsection>
<prevsent>another is the lexical paraphrase task of mccarthy and navigli (2009), inwhich words are given in the context of the surrounding sentence, and the task is to rank given list of proposed substitutions for that word.
</prevsent>
<prevsent>the list of substitutions as well as the correct rankings are elicited from annotators.
</prevsent>
</prevsection>
<citsent citstr=" W11-0131 ">
this task was originally conceived as an applied evaluation of wsd systems, not an evaluation of phrase representa tions.parsing accuracy has been used as preliminary evaluation of semantic models that produce syntactic structure (socher et al  2010; wu and schuler, 2011).<papid> W11-0131 </papid></citsent>
<aftsection>
<nextsent>however, syntax does not always reflect semantic content, and we are specifically interested in supporting syntactic in variance when doing semantic inference.
</nextsent>
<nextsent>also, this type of evaluation is tied to particular grammar formalism.
</nextsent>
<nextsent>the existing evaluations that are most similar in spirit to what we propose are paraphrase detection tasks that do not assume restricted syntactic context.
</nextsent>
<nextsent>washtell (2011) <papid> W11-0130 </papid>collected human judgments on the general meaning similarity of candidate phrase pairs.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3286">
<title id=" E12-1005.xml">evaluating distributional models of semantics for syntactically in variant inference </title>
<section> compositionality and distributional.  </section>
<citcontext>
<prevsection>
<prevsent>also, this type of evaluation is tied to particular grammar formalism.
</prevsent>
<prevsent>the existing evaluations that are most similar in spirit to what we propose are paraphrase detection tasks that do not assume restricted syntactic context.
</prevsent>
</prevsection>
<citsent citstr=" W11-0130 ">
washtell (2011) <papid> W11-0130 </papid>collected human judgments on the general meaning similarity of candidate phrase pairs.</citsent>
<aftsection>
<nextsent>unfortunately, no additional guidance on the definition of most similar in meaning?
</nextsent>
<nextsent>was provided, and it appears likely that subjects conflated lexical, syntactic, and semantic relatedness.
</nextsent>
<nextsent>dolan and brockett (2005) <papid> I05-5002 </papid>define paraphrase detection as identifying sentences that are in bidirectional entailment relation.</nextsent>
<nextsent>while such sentences do support exactly the same inferences, we are also interested in the inferences that canbe made from similar sentences that are not paraphrases according to this strict definition ? situation that is more often encountered in end ap plications.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3287">
<title id=" E12-1005.xml">evaluating distributional models of semantics for syntactically in variant inference </title>
<section> compositionality and distributional.  </section>
<citcontext>
<prevsection>
<prevsent>unfortunately, no additional guidance on the definition of most similar in meaning?
</prevsent>
<prevsent>was provided, and it appears likely that subjects conflated lexical, syntactic, and semantic relatedness.
</prevsent>
</prevsection>
<citsent citstr=" I05-5002 ">
dolan and brockett (2005) <papid> I05-5002 </papid>define paraphrase detection as identifying sentences that are in bidirectional entailment relation.</citsent>
<aftsection>
<nextsent>while such sentences do support exactly the same inferences, we are also interested in the inferences that canbe made from similar sentences that are not paraphrases according to this strict definition ? situation that is more often encountered in end applications.
</nextsent>
<nextsent>thus, we adopt less restricted notion of paraphrasis.
</nextsent>
<nextsent>we now describe simple, general framework for evaluating semantic models.
</nextsent>
<nextsent>our framework consists of the following components: semantic model to be evaluated, pairs of sentences that are considered to have high similarity, and pairsof sentences that are considered to have low similarity.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3289">
<title id=" E12-1005.xml">evaluating distributional models of semantics for syntactically in variant inference </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>we tested four recent distributional models and lemma overlap baseline, which we now describe.we extended several of the models to compositionally construct phrase representations usingcomponent-wise vector addition and multiplication, as we note below.
</prevsent>
<prevsent>since the focus of this paper is on evaluation methods for such models, we did not experiment with other compositionality 37operators.
</prevsent>
</prevsection>
<citsent citstr=" D09-1045 ">
we do note, however, that component wise operators have been popular in recent literature, and have been applied across unrestricted syntactic contexts (mitchell and lapata, 2009), <papid> D09-1045 </papid>so there is value in evaluating the performance of these operators in itself.</citsent>
<aftsection>
<nextsent>the models were trained on the gigaword corpus (2nd ed., ~2.3b words).
</nextsent>
<nextsent>all models use cosine similarity to measure the similarity between representations, except for the baseline model.lemma overlap this baseline simply represents sentence as the counts of each lemma present in the sentence after removing stop words.
</nextsent>
<nextsent>let sentence consist of lemma-tokensm1, . . .
</nextsent>
<nextsent>,m|x|.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3292">
<title id=" E12-1005.xml">evaluating distributional models of semantics for syntactically in variant inference </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>= va rb(r1) (17) rb(r) = ? c:f(c,r,b) ?
</prevsent>
<prevsent>f(c, r, b) ? vc, (18)where rb(r) is the vector describing the selectional preference of word in relation r, f(c, r, b) is the frequency of this dependency triple, ? is afrequency threshold to weed out uncommon dependency triples (10 in our experiments), and is vector combination operator, here component wise multiplication.
</prevsent>
</prevsection>
<citsent citstr=" P10-1097 ">
we extend the model to compute sentence representations from the contextual ized word vectors using component-wise addition and multiplication.tfp thater et al (2010)<papid> P10-1097 </papid>s model is also sensitive to selectional preferences, but to two degrees.</citsent>
<aftsection>
<nextsent>for example, the vector for catch might contain dimension labelled (obj,obj-1,throw),which indicates the strength of connection between the two verbs through all of the cooccurring direct objects which they share.
</nextsent>
<nextsent>unlikee&p;, tfps model encodes the selectional preferences in single vector using frequency counts.
</nextsent>
<nextsent>we extend the model to the sentence level with component-wise addition and multiplication, andword vectors are contextual ized by the dependency neighbours.
</nextsent>
<nextsent>we use frequency threshold of 10 and pmi threshold of 2 to prune infrequent word and dependencies.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3293">
<title id=" E12-1005.xml">evaluating distributional models of semantics for syntactically in variant inference </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>we extend the model to the sentence level with component-wise addition and multiplication, andword vectors are contextual ized by the dependency neighbours.
</prevsent>
<prevsent>we use frequency threshold of 10 and pmi threshold of 2 to prune infrequent word and dependencies.
</prevsent>
</prevsection>
<citsent citstr=" D10-1113 ">
d&l; dinu and lapata (2010) <papid> D10-1113 </papid>d&l;) assume global set of latent senses for all words, and models each word as mixture over these latent senses.</citsent>
<aftsection>
<nextsent>the vector for word ti in the context of word cj is modelled by v(ti, cj) = (z1|ti, cj), ...p (zk |ti, cj) (19)where z1...k are the latent senses.
</nextsent>
<nextsent>by making independence assumptions and decomposing probabilities, training becomes matter of estimating the probability distributions (zk|ti) and (cj |zk) from data.
</nextsent>
<nextsent>while dinu and lapata (2010) <papid> D10-1113 </papid>describe two methods to do so, based on non-negative matrix factor ization and latent dirichlet al cation, the performances are similar, so we tested only the latent dirichlet al cationmethod.</nextsent>
<nextsent>like the two previous models, we extend the model to build sentence representations 38 pfizer/rinat n. yahoo/inktomi besson/paris antoinette/vienna average overlap 0.7393 0.6007 0.7395 0.8914 0.7427 models trained on the entire gigaword m&l; add 0.6196 0.5387 0.5259 0.7275 0.6029 m&l; mult 0.9036 0.6099 0.6443 0.8467 0.7511 d&l; add 0.9214 0.8168 0.6989 0.8932 0.8326 d&l; mult 0.7732 0.6734 0.6527 0.7659 0.7163 models trained on the afp section e&p; add 0.7536 0.4933 0.2780 0.6408 0.5414 e&p; mult 0.5268 0.5328 0.5252 0.8421 0.6067 tfp add 0.4357 0.5325 0.8725 0.7183 0.6398 tfp mult 0.5554 0.5524 0.7283 0.6917 0.6320 m&l; add 0.5643 0.5504 0.4594 0.7640 0.5845 m&l; mult 0.8679 0.6324 0.4356 0.8258 0.6904 d&l; add 0.8143 0.9062 0.6373 0.8664 0.8061 d&l; mult 0.8429 0.7461 0.645 0.5948 0.7072 table 1: task 1 results in auc scores.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3298">
<title id=" E12-1005.xml">evaluating distributional models of semantics for syntactically in variant inference </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>because our main goal is to test the general performance of the models and to demonstrate the feasibility of our evaluation methods, we did not further tune the parameter settings to each of the tasks, as doing so would likely only yield minor improvements.
</prevsent>
<prevsent>4.3 task 1.
</prevsent>
</prevsection>
<citsent citstr=" P07-1073 ">
we used the dataset by bunescu and mooney (2007), <papid> P07-1073 </papid>which we selected because it contains multiple realizations of an entity pair in target semantic relation, unlike similar datasets such as the one by roth and yih (2002).<papid> C02-1151 </papid></citsent>
<aftsection>
<nextsent>controlling for the target entity pair in this manner makes the task more difficult, because the semantic model cannot make use of distributional information about the entity pair in inference.
</nextsent>
<nextsent>the dataset is separated into subsets depending on the target binary relation (company acquires company or person was born in place y) and the entity pair (e.g., yahoo and inktomi) (table 2).the dataset was constructed semi automatically using google search for the two entities in order with up to seven content words in between.
</nextsent>
<nextsent>then, the extracted sentences were hand-labelled with whether they express the target relation.
</nextsent>
<nextsent>because the order of the entities has been fixed, passive alternations do not appear 39 pure models mixed models all subset al subset overlap 0.8770 0.7291 0.8770 0.7291 models trained on the entire gigaword m&l; add 0.7467 0.6106 0.8782 0.7523 m&l; mult 0.5331 0.5690 0.8841 0.7678 d&l; add 0.6552 0.5716 0.8791 0.7539 d&l; mult 0.5488 0.5255 0.8841 0.7466 models trained on the afp section e&p; add 0.4589 0.4516 0.8748 0.7375 e&p; mult 0.5201 0.5584 0.8882 0.7719 tfp add 0.6887 0.6443 0.8940 0.7871 tfp mult 0.5210 0.5199 0.8785 0.7432 m&l; add 0.7588 0.6206 0.8710 0.7371 m&l; mult 0.5710 0.5540 0.8801 0.7540 d&l; add 0.6358 0.5402 0.8713 0.7305 d&l; mult 0.5647 0.5461 0.8856 0.7683 table 3: task 2 results, in normalized rank scores.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3299">
<title id=" E12-1005.xml">evaluating distributional models of semantics for syntactically in variant inference </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>because our main goal is to test the general performance of the models and to demonstrate the feasibility of our evaluation methods, we did not further tune the parameter settings to each of the tasks, as doing so would likely only yield minor improvements.
</prevsent>
<prevsent>4.3 task 1.
</prevsent>
</prevsection>
<citsent citstr=" C02-1151 ">
we used the dataset by bunescu and mooney (2007), <papid> P07-1073 </papid>which we selected because it contains multiple realizations of an entity pair in target semantic relation, unlike similar datasets such as the one by roth and yih (2002).<papid> C02-1151 </papid></citsent>
<aftsection>
<nextsent>controlling for the target entity pair in this manner makes the task more difficult, because the semantic model cannot make use of distributional information about the entity pair in inference.
</nextsent>
<nextsent>the dataset is separated into subsets depending on the target binary relation (company acquires company or person was born in place y) and the entity pair (e.g., yahoo and inktomi) (table 2).the dataset was constructed semi automatically using google search for the two entities in order with up to seven content words in between.
</nextsent>
<nextsent>then, the extracted sentences were hand-labelled with whether they express the target relation.
</nextsent>
<nextsent>because the order of the entities has been fixed, passive alternations do not appear 39 pure models mixed models all subset al subset overlap 0.8770 0.7291 0.8770 0.7291 models trained on the entire gigaword m&l; add 0.7467 0.6106 0.8782 0.7523 m&l; mult 0.5331 0.5690 0.8841 0.7678 d&l; add 0.6552 0.5716 0.8791 0.7539 d&l; mult 0.5488 0.5255 0.8841 0.7466 models trained on the afp section e&p; add 0.4589 0.4516 0.8748 0.7375 e&p; mult 0.5201 0.5584 0.8882 0.7719 tfp add 0.6887 0.6443 0.8940 0.7871 tfp mult 0.5210 0.5199 0.8785 0.7432 m&l; add 0.7588 0.6206 0.8710 0.7371 m&l; mult 0.5710 0.5540 0.8801 0.7540 d&l; add 0.6358 0.5402 0.8713 0.7305 d&l; mult 0.5647 0.5461 0.8856 0.7683 table 3: task 2 results, in normalized rank scores.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3301">
<title id=" E12-1005.xml">evaluating distributional models of semantics for syntactically in variant inference </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>we summarize belowa number of other word- or phrase-level distributional models.
</prevsent>
<prevsent>several approaches are specialized to deal withhomography.
</prevsent>
</prevsection>
<citsent citstr=" N10-1013 ">
the top-down multi-prototype approach determines number of senses for each word, and then clusters the occurrences of the word (reisinger and mooney, 2010) <papid> N10-1013 </papid>into these senses.</citsent>
<aftsection>
<nextsent>a prototype vector is created for each of these sense clusters.
</nextsent>
<nextsent>when new occurrence 40 of word is encountered, it is represented as acombination of the prototype vectors, with the degree of influence from each prototype determined by the similarity of the new context to the existing sense contexts.
</nextsent>
<nextsent>in contrast, the bottom-up exemplar-based approach assumes that each occurrence of word expresses different sense of theword.
</nextsent>
<nextsent>the most similar senses of the word are activated when new occurrence of it is encountered and combined, for example with knn algorithm (erk and pado?, 2010).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3303">
<title id=" E12-1005.xml">evaluating distributional models of semantics for syntactically in variant inference </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>unfortunately, this approach does not outperform various context word-based approaches in two phrase similarity tasks.
</prevsent>
<prevsent>in terms of the vector composition function, component-wise addition and multiplication arethe most popular in recent work, but there exist number of other operators such as tensorproduct and convolution product, which are reviewed by widdows (2008).
</prevsent>
</prevsection>
<citsent citstr=" P10-1093 ">
instead of vector space representations, one could also use matrix space representation with its much more expressive matrix operators (rudolph and giesbrecht,2010).<papid> P10-1093 </papid></citsent>
<aftsection>
<nextsent>so far, however, this has only been applied to specific syntactic contexts (baroni and zamparelli, 2010; <papid> D10-1115 </papid>guevara, 2010; <papid> W10-2805 </papid>grefenstette and sadrzadeh, 2011), <papid> D11-1129 </papid>or tasks (yessenalina and cardie, 2011).<papid> D11-1016 </papid></nextsent>
<nextsent>neural networks have been used to learn both phrase structure and representations.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3304">
<title id=" E12-1005.xml">evaluating distributional models of semantics for syntactically in variant inference </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in terms of the vector composition function, component-wise addition and multiplication arethe most popular in recent work, but there exist number of other operators such as tensorproduct and convolution product, which are reviewed by widdows (2008).
</prevsent>
<prevsent>instead of vector space representations, one could also use matrix space representation with its much more expressive matrix operators (rudolph and giesbrecht,2010).<papid> P10-1093 </papid></prevsent>
</prevsection>
<citsent citstr=" D10-1115 ">
so far, however, this has only been applied to specific syntactic contexts (baroni and zamparelli, 2010; <papid> D10-1115 </papid>guevara, 2010; <papid> W10-2805 </papid>grefenstette and sadrzadeh, 2011), <papid> D11-1129 </papid>or tasks (yessenalina and cardie, 2011).<papid> D11-1016 </papid></citsent>
<aftsection>
<nextsent>neural networks have been used to learn both phrase structure and representations.
</nextsent>
<nextsent>in socher etal.
</nextsent>
<nextsent>(2010), word representations learned by neural network models such as (bengio et al  2006; collobert and weston, 2008) are fed as input into recursive neural network whose nodes represent syntactic constituents.
</nextsent>
<nextsent>each node models both the probability of the input forming constituent andthe phrase representation resulting from composition.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3305">
<title id=" E12-1005.xml">evaluating distributional models of semantics for syntactically in variant inference </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in terms of the vector composition function, component-wise addition and multiplication arethe most popular in recent work, but there exist number of other operators such as tensorproduct and convolution product, which are reviewed by widdows (2008).
</prevsent>
<prevsent>instead of vector space representations, one could also use matrix space representation with its much more expressive matrix operators (rudolph and giesbrecht,2010).<papid> P10-1093 </papid></prevsent>
</prevsection>
<citsent citstr=" W10-2805 ">
so far, however, this has only been applied to specific syntactic contexts (baroni and zamparelli, 2010; <papid> D10-1115 </papid>guevara, 2010; <papid> W10-2805 </papid>grefenstette and sadrzadeh, 2011), <papid> D11-1129 </papid>or tasks (yessenalina and cardie, 2011).<papid> D11-1016 </papid></citsent>
<aftsection>
<nextsent>neural networks have been used to learn both phrase structure and representations.
</nextsent>
<nextsent>in socher etal.
</nextsent>
<nextsent>(2010), word representations learned by neural network models such as (bengio et al  2006; collobert and weston, 2008) are fed as input into recursive neural network whose nodes represent syntactic constituents.
</nextsent>
<nextsent>each node models both the probability of the input forming constituent andthe phrase representation resulting from composition.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3306">
<title id=" E12-1005.xml">evaluating distributional models of semantics for syntactically in variant inference </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in terms of the vector composition function, component-wise addition and multiplication arethe most popular in recent work, but there exist number of other operators such as tensorproduct and convolution product, which are reviewed by widdows (2008).
</prevsent>
<prevsent>instead of vector space representations, one could also use matrix space representation with its much more expressive matrix operators (rudolph and giesbrecht,2010).<papid> P10-1093 </papid></prevsent>
</prevsection>
<citsent citstr=" D11-1129 ">
so far, however, this has only been applied to specific syntactic contexts (baroni and zamparelli, 2010; <papid> D10-1115 </papid>guevara, 2010; <papid> W10-2805 </papid>grefenstette and sadrzadeh, 2011), <papid> D11-1129 </papid>or tasks (yessenalina and cardie, 2011).<papid> D11-1016 </papid></citsent>
<aftsection>
<nextsent>neural networks have been used to learn both phrase structure and representations.
</nextsent>
<nextsent>in socher etal.
</nextsent>
<nextsent>(2010), word representations learned by neural network models such as (bengio et al  2006; collobert and weston, 2008) are fed as input into recursive neural network whose nodes represent syntactic constituents.
</nextsent>
<nextsent>each node models both the probability of the input forming constituent andthe phrase representation resulting from composition.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3307">
<title id=" E12-1005.xml">evaluating distributional models of semantics for syntactically in variant inference </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in terms of the vector composition function, component-wise addition and multiplication arethe most popular in recent work, but there exist number of other operators such as tensorproduct and convolution product, which are reviewed by widdows (2008).
</prevsent>
<prevsent>instead of vector space representations, one could also use matrix space representation with its much more expressive matrix operators (rudolph and giesbrecht,2010).<papid> P10-1093 </papid></prevsent>
</prevsection>
<citsent citstr=" D11-1016 ">
so far, however, this has only been applied to specific syntactic contexts (baroni and zamparelli, 2010; <papid> D10-1115 </papid>guevara, 2010; <papid> W10-2805 </papid>grefenstette and sadrzadeh, 2011), <papid> D11-1129 </papid>or tasks (yessenalina and cardie, 2011).<papid> D11-1016 </papid></citsent>
<aftsection>
<nextsent>neural networks have been used to learn both phrase structure and representations.
</nextsent>
<nextsent>in socher etal.
</nextsent>
<nextsent>(2010), word representations learned by neural network models such as (bengio et al  2006; collobert and weston, 2008) are fed as input into recursive neural network whose nodes represent syntactic constituents.
</nextsent>
<nextsent>each node models both the probability of the input forming constituent andthe phrase representation resulting from composition.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3311">
<title id=" E87-1039.xml">acquisition of conceptual data models from natural language descriptions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the requirements for linguistic approach are that either is is constructed in the same manner as nano klaus, to employ simple input phrase structures, but embedded in .co- operative dialogue, or else it should have sufficient linguistic coverage to handle the complex sentence structures exhibited in (1) and (2).
</prevsent>
<prevsent>most importantly in the latter respect, it should have reasonable treatment of the variety of natural language quantifiers and relative clauses.
</prevsent>
</prevsection>
<citsent citstr=" J82-3002 ">
many database interfaces have such capabilities, mccord (1982), dahl (1982) and warren and pereira (1982) <papid> J82-3002 </papid>inter a//a. 245 semantics.</citsent>
<aftsection>
<nextsent>chamiak (1983) makes distinction between inferential and non-inferential semantics.
</nextsent>
<nextsent>the former is concerned with establishing the logical form corresponding to syntactic analysis of sentence, whereas the latter is concerned with co-oecurrence restrictions between phrases which may be stated in terms of lexical subcategories uch as human, mass, machine, etc. database interfaces are the most common instances of complete natural language interfaces which comprise beth syntactic and semantic components.
</nextsent>
<nextsent>as such they are potential models for the development of interfaces to new types of software systems.
</nextsent>
<nextsent>however, their approach to semantics cannot be imported wholesale.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3312">
<title id=" E87-1023.xml">a model for preference </title>
<section> what is preference?.  </section>
<citcontext>
<prevsection>
<prevsent>various approaches.
</prevsent>
<prevsent>preference strategies have often been used for dealing with the problem of ill-formed input (a particular case of robust-ness, cf below section 2.2) (ajcl 1983, charniak 1983).
</prevsent>
</prevsection>
<citsent citstr=" J83-3003 ">
following weischedel and sondheimer (1983) <papid> J83-3003 </papid>we distinguish the cases 134 where preference is part of the particular computation being performed (wilks 1973, fass and wilks 1983, <papid> J83-3004 </papid>pereira 1985) from the case where it is separate process, run after the results of the computation have been obtained (jensen et al 1983, <papid> J83-3002 </papid>weischedel and sondheimer 1983).<papid> J83-3003 </papid></citsent>
<aftsection>
<nextsent>a frequent approach to preference is scoring.
</nextsent>
<nextsent>a numeric score is calculated, independently, for each competing interpretation and is then used to rank the interpretations.
</nextsent>
<nextsent>the best interpreta-tions are then chosen.
</nextsent>
<nextsent>the score can be the number of constraints satisfied by the interpretation (wilks 1973, fass &amp; wilks 1983), <papid> J83-3004 </papid>where these constraints might be assigned relative weights by the inguist (robinson 1982, charniak 1983, bennett and slocum 1985) <papid> J85-2002 </papid>or calculated by the computer (papegaaij 1986).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3313">
<title id=" E87-1023.xml">a model for preference </title>
<section> what is preference?.  </section>
<citcontext>
<prevsection>
<prevsent>various approaches.
</prevsent>
<prevsent>preference strategies have often been used for dealing with the problem of ill-formed input (a particular case of robust-ness, cf below section 2.2) (ajcl 1983, charniak 1983).
</prevsent>
</prevsection>
<citsent citstr=" J83-3004 ">
following weischedel and sondheimer (1983) <papid> J83-3003 </papid>we distinguish the cases 134 where preference is part of the particular computation being performed (wilks 1973, fass and wilks 1983, <papid> J83-3004 </papid>pereira 1985) from the case where it is separate process, run after the results of the computation have been obtained (jensen et al 1983, <papid> J83-3002 </papid>weischedel and sondheimer 1983).<papid> J83-3003 </papid></citsent>
<aftsection>
<nextsent>a frequent approach to preference is scoring.
</nextsent>
<nextsent>a numeric score is calculated, independently, for each competing interpretation and is then used to rank the interpretations.
</nextsent>
<nextsent>the best interpreta-tions are then chosen.
</nextsent>
<nextsent>the score can be the number of constraints satisfied by the interpretation (wilks 1973, fass &amp; wilks 1983), <papid> J83-3004 </papid>where these constraints might be assigned relative weights by the inguist (robinson 1982, charniak 1983, bennett and slocum 1985) <papid> J85-2002 </papid>or calculated by the computer (papegaaij 1986).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3314">
<title id=" E87-1023.xml">a model for preference </title>
<section> what is preference?.  </section>
<citcontext>
<prevsection>
<prevsent>various approaches.
</prevsent>
<prevsent>preference strategies have often been used for dealing with the problem of ill-formed input (a particular case of robust-ness, cf below section 2.2) (ajcl 1983, charniak 1983).
</prevsent>
</prevsection>
<citsent citstr=" J83-3002 ">
following weischedel and sondheimer (1983) <papid> J83-3003 </papid>we distinguish the cases 134 where preference is part of the particular computation being performed (wilks 1973, fass and wilks 1983, <papid> J83-3004 </papid>pereira 1985) from the case where it is separate process, run after the results of the computation have been obtained (jensen et al 1983, <papid> J83-3002 </papid>weischedel and sondheimer 1983).<papid> J83-3003 </papid></citsent>
<aftsection>
<nextsent>a frequent approach to preference is scoring.
</nextsent>
<nextsent>a numeric score is calculated, independently, for each competing interpretation and is then used to rank the interpretations.
</nextsent>
<nextsent>the best interpreta-tions are then chosen.
</nextsent>
<nextsent>the score can be the number of constraints satisfied by the interpretation (wilks 1973, fass &amp; wilks 1983), <papid> J83-3004 </papid>where these constraints might be assigned relative weights by the inguist (robinson 1982, charniak 1983, bennett and slocum 1985) <papid> J85-2002 </papid>or calculated by the computer (papegaaij 1986).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3317">
<title id=" E87-1023.xml">a model for preference </title>
<section> what is preference?.  </section>
<citcontext>
<prevsection>
<prevsent>a numeric score is calculated, independently, for each competing interpretation and is then used to rank the interpretations.
</prevsent>
<prevsent>the best interpreta-tions are then chosen.
</prevsent>
</prevsection>
<citsent citstr=" J85-2002 ">
the score can be the number of constraints satisfied by the interpretation (wilks 1973, fass &amp; wilks 1983), <papid> J83-3004 </papid>where these constraints might be assigned relative weights by the inguist (robinson 1982, charniak 1983, bennett and slocum 1985) <papid> J85-2002 </papid>or calculated by the computer (papegaaij 1986).</citsent>
<aftsection>
<nextsent>such techniques have been used extensively for speech recogni-tion (paxton 1977, walker et al 1978) and in the field of expert systems (such as mycin, buchanan &amp; shortl f fe 1984), where the calculation of both score and ranking become quite complex with probabi i ies and thresholds.
</nextsent>
<nextsent>the problem with scoring is that it seems quite unnatural for l inguist to associate score (or weight or pro babil ity) to particular rule or piece of data when the knowledge being encoded is in fact qualitative.
</nextsent>
<nextsent>furthermore, combining the scores based on different types of reasoning to calculate global score for representation seems rather arbitrary procedure.
</nextsent>
<nextsent>such uniform metric, even if it can model actual inguistic knowledge, forces the grammar writer to juggle with numbers to get the behaviour he wants, thus making the preference process obscure.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3318">
<title id=" E87-1023.xml">a model for preference </title>
<section> what is preference?.  </section>
<citcontext>
<prevsection>
<prevsent>such uniform metric, even if it can model actual inguistic knowledge, forces the grammar writer to juggle with numbers to get the behaviour he wants, thus making the preference process obscure.
</prevsent>
<prevsent>a further disadvantage of this approach is that the score is often based on the way interpretations are built, rather than on the properties of the interpretations themselves.
</prevsent>
</prevsection>
<citsent citstr=" P84-1054 ">
preference is also mentioned in l inguistic controversy started by frazier and fodor (1979) with their principles of right association and minimal attachment (schubert 1984).<papid> P84-1054 </papid></citsent>
<aftsection>
<nextsent>there the problem is to disambiguate many readings (or interpreta- tions) of sentence in order to find the good (preferred) one(s).
</nextsent>
<nextsent>various contribu-tions on that issue have in common that bad interpretations are abandoned before being finished, during computation (shieber 1983, pereira 1985).
</nextsent>
<nextsent>although this method speeds up the computation, there is risk that possibl ty will be abandoned too early, before the relevant information has been found.
</nextsent>
<nextsent>this is shown by wilks et al (1985) who claim to have the ideal solution in preference seman-tics, which uses as part of its computa-tion scoring and ranking.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3319">
<title id=" E87-1023.xml">a model for preference </title>
<section> what is preference?.  </section>
<citcontext>
<prevsection>
<prevsent>2.2.
</prevsent>
<prevsent>our notion of preference.
</prevsent>
</prevsection>
<citsent citstr=" J85-2005 ">
our approach, although stemming from earlier work in the eurotra project (mcnaught et al 1983, johnson et al 1985), <papid> J85-2005 </papid>is, we believe, new and original.</citsent>
<aftsection>
<nextsent>we make the following assumptions: the relation  translation of  between texts as established by machine translation system has to be one to one (1-1)?
</nextsent>
<nextsent>ii there is apriori no formal or inguis- tic guarantee that this will be the case for the relation as whole or for the translation steps between inter-mediate levels of representation.
</nextsent>
<nextsent>(an attempt to formalize this can be found in krauwer and des tombe 1984 <papid> P84-1099 </papid>or in section 4 of johnson et al 1985).<papid> J85-2005 </papid></nextsent>
<nextsent>the problem we want to address here is the following: given the fact that one to many (l-n) translations do occur, how do we ensure that the final result is still i-1.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3320">
<title id=" E87-1023.xml">a model for preference </title>
<section> what is preference?.  </section>
<citcontext>
<prevsection>
<prevsent>we make the following assumptions: the relation  translation of  between texts as established by machine translation system has to be one to one (1-1)?
</prevsent>
<prevsent>ii there is apriori no formal or inguis- tic guarantee that this will be the case for the relation as whole or for the translation steps between inter-mediate levels of representation.
</prevsent>
</prevsection>
<citsent citstr=" P84-1099 ">
(an attempt to formalize this can be found in krauwer and des tombe 1984 <papid> P84-1099 </papid>or in section 4 of johnson et al 1985).<papid> J85-2005 </papid></citsent>
<aftsection>
<nextsent>the problem we want to address here is the following: given the fact that one to many (l-n) translations do occur, how do we ensure that the final result is still i-1.
</nextsent>
<nextsent>this problem is not restricted to machine translation: often program (for example parser or text generator) produces many interpreta-tions of the same object (usually text segment) when in the ideal case only one is wanted.
</nextsent>
<nextsent>in the following we refer to  l-n translation  for this general phenomenon.
</nextsent>
<nextsent>we see two types of solutions to this problem, each of them applicable to specific classes of cases: spurious results can be eliminated on the basis of their own individual pro-perties (e.g. well-formedness, com- pleteness); for this we wil use the term  filtering .
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3322">
<title id=" E12-1028.xml">automatic generation of short informative sentiment summaries </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>an aspect is component or attribute of product such as battery?, lens cap?, batterylife?, and picture quality?
</prevsent>
<prevsent>for cameras.
</prevsent>
</prevsection>
<citsent citstr=" P06-2063 ">
aspect oriented summarization (hu and liu, 2004;zhuang et al 2006; kim and hovy, 2006) <papid> P06-2063 </papid>collects sentiment assessments forgiven set of aspects and returns list of pros and cons about every aspect for review or, in some cases, on per-sentence basis.aspect-oriented summarization and pro/con summarization differ in number of ways from supporting sentence summarization.</citsent>
<aftsection>
<nextsent>first, aspects and pros&cons; are taken from fixed inventory.
</nextsent>
<nextsent>the inventory is typically small and doesnot cover the full spectrum of relevant information.
</nextsent>
<nextsent>second, in its most useful form, aspect oriented summarization requires classification of phrases and sentences according to the aspect they belong to; e.g., the camera is very light?
</nextsent>
<nextsent>has to be recognized as being relevant to the aspect weight?.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3323">
<title id=" E12-1028.xml">automatic generation of short informative sentiment summaries </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>berend (2011) performs form of pro/con summarization that does not relyon aspects.
</prevsent>
<prevsent>however, most of the problems of aspect-based pro/con summarization also apply to this paper: no differentiation between good and bad reasons, the need for human labels to train classifier, and inferior readability compared to well-formed sentence.
</prevsent>
</prevsection>
<citsent citstr=" N09-2010 ">
two previous approaches that have attempted to extract sentences from reviews in the context of summarization are (beineke et al 2004) and (arora et al 2009).<papid> N09-2010 </papid></citsent>
<aftsection>
<nextsent>beineke et al(2004) traina classifier on rottentomatoes.com summary sentences provided by review authors.
</nextsent>
<nextsent>these sentences sometimes contain specific reason for the overall sentiment of the review, but sometimes they are just catchy lines whose purpose is to draw moviegoers in to read the entire review; e.g., el bulli barely registers pulse stronger than books?
</nextsent>
<nextsent>(which does not give specific reason for why the movie does not register strong pulse).arora et al(2009) <papid> N09-2010 </papid>define two classes of sentences: qualified claims and bald claims.</nextsent>
<nextsent>a qualified claim gives the reader more details (e.g., this camera is small enough to fit easily in 277coat pocket?)</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3325">
<title id=" E12-1028.xml">automatic generation of short informative sentiment summaries </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>the average number of sentences of review is. 13.36 sentences, the median number of sentences is 10.
</prevsent>
<prevsent>4.3 sentiment classification.
</prevsent>
</prevsection>
<citsent citstr=" N03-5008 ">
we first build sentence sentiment classifier by training the stanford maximum entropy classifier (manning and klein, 2003) <papid> N03-5008 </papid>on the sentences in the training set.</citsent>
<aftsection>
<nextsent>sentences occurring in positive (resp.negative) reviews are labeled positive (resp.
</nextsent>
<nextsent>negative).
</nextsent>
<nextsent>we use simple bag-of-words representation (without punctuation characters and frequent stop words).
</nextsent>
<nextsent>propagating labels from documents to sentences creates noisy training set because some sentences have sentiment different from the sentiment in their documents; however, there isno alternative because we need per-sentence classification decisions, but do not have per-sentence human labels.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3326">
<title id=" E09-1043.xml">improving midrange reordering using templates of factors </title>
<section> abstract </section>
<citcontext>
<prevsection>


</prevsection>
<citsent citstr=" D07-1091 ">
we extend the factored translation model(koehn and hoang, 2007) <papid> D07-1091 </papid>to allow translations of longer phrases composed of factors such as pos and morphological tagsto act as templates for the selection andre ordering of surface phrase translation.</citsent>
<aftsection>
<nextsent>wealso reintroduce the use of alignment information within the decoder, which formsan integral part of decoding in the alignment template system (och, 2002), into phrase-based decoding.results show an increase in translation performance of up to 1.0% bleufor out-of-domain french english translation.
</nextsent>
<nextsent>we also show how this method compares and relates to lexicalized reordering.
</nextsent>
<nextsent>one of the major issues in statistical machine translation is reordering due to systematic word ordering differences between languages.
</nextsent>
<nextsent>often reordering is best explained by linguistic categories, such as part-of-speech tags.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3328">
<title id=" E09-1043.xml">improving midrange reordering using templates of factors </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>re-ordering can also be viewed as composing of number of related problems which can be explained or solved by variety of linguistic phenomena.
</prevsent>
<prevsent>firstly, differences between phrase ordering account for much of the long-range reordering.
</prevsent>
</prevsection>
<citsent citstr=" P05-1033 ">
syntax-based and hierarchical models such as (chiang, 2005) <papid> P05-1033 </papid>attempts to address thisproblem.</citsent>
<aftsection>
<nextsent>shorter range re-ordering, such as intra phrasal word re-ordering, can often be predicted from the underlying property of the words and its context, the most obvious property being pos tags.in this paper, we tackle the issue of shorter range re-ordering in phrase-based decoding by presenting an extension of the factored translation which directly models the translation of non surface factors such as pos tags.
</nextsent>
<nextsent>we shall call this extension the factored template model.
</nextsent>
<nextsent>we use the fact that factors such as pos-tags are less spars ethan surface words to obtain longer phrase translations.
</nextsent>
<nextsent>these translations are used to inform the re-ordering of surface phrases.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3329">
<title id=" E09-1043.xml">improving midrange reordering using templates of factors </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>we estimate p(t|s) by decom 1http://www.statmt.org/wmt07/shared-task.html 372 posing it into component models p(t|s) = 1 ? hm(t, s) (2)where hm(t, s) is the feature function for component and is the weight given to component m. is normalization factor which is ignored inpractice.
</prevsent>
<prevsent>components are translation model scoring functions, language model, reordering models and other features.
</prevsent>
</prevsection>
<citsent citstr=" N03-1017 ">
the problem is typically presented in log-space, which simplifies computations, but otherwise does not change the problem due to the monotonicity of the log function (hm = log hm) log p(t|s) = ? m hm(t, s) (3) phrase-based models (koehn et al, 2003) <papid> N03-1017 </papid>are limited to the mapping of small contiguous chunks of text.</citsent>
<aftsection>
<nextsent>in these models, the source sentence is segmented into number of phrases sk, which are translated one-to-one into target phrases tk. the translation feature functions htm(t, s) are computed as sum of phrase translation feature functions htm(tk, sk): htm(t, s) = ? htm(tk, sk) (4) where tk and sk are the phrases that make up the target and source sentence.
</nextsent>
<nextsent>note that typically multiple feature functions for one translation table are used (such as forward and backward probabilities and lexical backoff).
</nextsent>
<nextsent>2.2 reordering in phrase models.
</nextsent>
<nextsent>phrase-based systems implicitly perform short range reordering by translating multi-word phrases where the component words may be reordered relative to each other.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3331">
<title id=" E09-1043.xml">improving midrange reordering using templates of factors </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>while we can gain benefit from adding factor information into phrase-baseddecoding, our experience also shows the shortcomings of decomposing phrase translation.
</prevsent>
<prevsent>efforts have been made to integrate syntactic information into the decoding process to improve reordering.
</prevsent>
</prevsection>
<citsent citstr=" P05-1066 ">
collins et al (2005) <papid> P05-1066 </papid>reorder the source sentence using sequence of six manually-crafted rules,given the syntactic parse tree of the source sen tence.</citsent>
<aftsection>
<nextsent>while the transformation rules are specific to the german parser that was used, they could be adapted to other languages and parsers.
</nextsent>
<nextsent>xia and mccord (2004) automatically create rewrite rules which reorder the source sentence.
</nextsent>
<nextsent>zhang and zens (2007) take slightly different approach by using chunk level tags to reorder the source sentence, creating confusion network to represent the possible reorderings of the source sentence.
</nextsent>
<nextsent>all these approaches seek to improve reordering by making the ordering of the source sentence similar to the target sentence.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3335">
<title id=" E09-1043.xml">improving midrange reordering using templates of factors </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>the training corpus is aligned using giza++(och and ney, 2003).
</prevsent>
<prevsent>to create pos tag translation models, the surface forms on both source and target language training data are replaced with pos tags before phrases are extracted.
</prevsent>
</prevsection>
<citsent citstr=" J95-4004 ">
the taggers used were the brill tagger (brill, 1995) <papid> J95-4004 </papid>for english, the tree tagger for french (schmid, 1994), and the lopar tagger (schmidt and schulte imwalde, 2000) for german.</citsent>
<aftsection>
<nextsent>the training script supplied with the moses toolkit (koehn et al, 2007)<papid> P07-2045 </papid>was used, extended to enable alignment information of each phrase pair.</nextsent>
<nextsent>the vanilla moses mert tuning script was used throughout.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3336">
<title id=" E09-1043.xml">improving midrange reordering using templates of factors </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>to create pos tag translation models, the surface forms on both source and target language training data are replaced with pos tags before phrases are extracted.
</prevsent>
<prevsent>the taggers used were the brill tagger (brill, 1995) <papid> J95-4004 </papid>for english, the tree tagger for french (schmid, 1994), and the lopar tagger (schmidt and schulte imwalde, 2000) for german.</prevsent>
</prevsection>
<citsent citstr=" P07-2045 ">
the training script supplied with the moses toolkit (koehn et al, 2007)<papid> P07-2045 </papid>was used, extended to enable alignment information of each phrase pair.</citsent>
<aftsection>
<nextsent>the vanilla moses mert tuning script was used throughout.
</nextsent>
<nextsent>results are also presented for models trained on the larger europarl corpora3.
</nextsent>
<nextsent>5.1 germanenglish.
</nextsent>
<nextsent>we use as baseline the traditional, non-factored phrase model which obtained bleu score of 14.6% on the out-of-domain test set and 18.2% on the in-domain test set (see table 1, line 1).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3337">
<title id=" E12-2013.xml">fluid construction grammar the new kid on the block </title>
<section> wysiwyg grammar engineering.  </section>
<citcontext>
<prevsection>
<prevsent>point-of-view, it puts the burden of efficient processing on the shoulders of computational linguists, who have tofind balance between faithfulness to the handwritten theory and computational efficiency (melnik, 2005).
</prevsent>
<prevsent>for instance, there is no hpsg implementation, but rather several platforms that support the implementation of hpsg-like?
</prevsent>
</prevsection>
<citsent citstr=" C96-1049 ">
gram mars: ale (carpenter and penn, 1995), alep (schmidt et al 1996), <papid> C96-1049 </papid>cuf (drre and dorna, 64 top cxn-applied top nominal-adjectival-cxn sem-subunits footprints args sem-cat nominal-adjectival-phrase-1 (word-ballon-1 word-rouge-1) (nominal-adjectival-cxn) (red-ball-15 context-19) ((sem-function identifier))wordballon 1wordrouge 1 word-le-1 sem syn form syn-subunits syn-cat footprints nominal-adjectival-phrase-1 ((meets word-ballon-1 word-rouge-1)) (word-ballon-1 word-rouge-1) ((number singular) (gender masculine) (syn-function nominal)) (nominal-adjectival-cxn)wordrouge 1wordballon 1 word-le-1figure 2: fcg comes equipped with an interactive web interface for inspecting the linguistic inventory, construction application and search.</citsent>
<aftsection>
<nextsent>this figure shows an example construction where two units are opened up for closer inspection of their feature structures.
</nextsent>
<nextsent>1993), light (ciortuz, 2002), lkb (copestake, 2002), profit (erbach, 1995), <papid> E95-1025 </papid>tdl (krieger and schfer, 1994), tfs (emele, 1994), and others(see bolc et al 1996, for survey).</nextsent>
<nextsent>unfortunately, the optimizations and technologies developed within these platforms are often considered by theoretical linguists as engineering solutions rather than scientific contributions.fcg, on the other hand, adheres to the cognitive linguistics assumption that linguistic performance is equally important as linguistic competence, hence processing becomes central notion in the formalism.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3338">
<title id=" E12-2013.xml">fluid construction grammar the new kid on the block </title>
<section> wysiwyg grammar engineering.  </section>
<citcontext>
<prevsection>
<prevsent>gram mars: ale (carpenter and penn, 1995), alep (schmidt et al 1996), <papid> C96-1049 </papid>cuf (drre and dorna, 64 top cxn-applied top nominal-adjectival-cxn sem-subunits footprints args sem-cat nominal-adjectival-phrase-1 (word-ballon-1 word-rouge-1) (nominal-adjectival-cxn) (red-ball-15 context-19) ((sem-function identifier))wordballon 1wordrouge 1 word-le-1 sem syn form syn-subunits syn-cat footprints nominal-adjectival-phrase-1 ((meets word-ballon-1 word-rouge-1)) (word-ballon-1 word-rouge-1) ((number singular) (gender masculine) (syn-function nominal)) (nominal-adjectival-cxn)wordrouge 1wordballon 1 word-le-1figure 2: fcg comes equipped with an interactive web interface for inspecting the linguistic inventory, construction application and search.</prevsent>
<prevsent>this figure shows an example construction where two units are opened up for closer inspection of their feature structures.</prevsent>
</prevsection>
<citsent citstr=" E95-1025 ">
1993), light (ciortuz, 2002), lkb (copestake, 2002), profit (erbach, 1995), <papid> E95-1025 </papid>tdl (krieger and schfer, 1994), tfs (emele, 1994), and others(see bolc et al 1996, for survey).</citsent>
<aftsection>
<nextsent>unfortunately, the optimizations and technologies developed within these platforms are often considered by theoretical linguists as engineering solutions rather than scientific contributions.fcg, on the other hand, adheres to the cognitive linguistics assumption that linguistic performance is equally important as linguistic competence, hence processing becomes central notion in the formalism.
</nextsent>
<nextsent>fcg representations therefore offer what you see is what you get?
</nextsent>
<nextsent>approach to grammar engineering where the representations have direct impact on processing and vice versa.
</nextsent>
<nextsent>for instance, constructions division between asemantic and syntactic pole is informative with respect to how the construction is applied.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3339">
<title id=" E09-1035.xml">structural transitive and latent models for biographic fact extraction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the ensemble of these proposed models substantially outperforms standard pattern-based biographic fact extraction methods and performance is further improved by modelinginter-attribute correlations and distributions over functions of attributes, achieving an average extraction accuracy of 80% over seven types of biographic attributes.
</prevsent>
<prevsent>extracting biographic facts such as birthdate?, occupation?, nationality?, etc. is critical step for advancing the state of the art in information processing and retrieval.
</prevsent>
</prevsection>
<citsent citstr=" W03-0405 ">
an important aspect of web search is to be able to narrow down search results by distinguishing among people with the same name leading to multiple efforts focusing on web person name disambiguation in the literature (mann and yarowsky, 2003; <papid> W03-0405 </papid>artiles et al, 2007, <papid> W07-2012 </papid>cucerzan, 2007).<papid> D07-1074 </papid></citsent>
<aftsection>
<nextsent>while biographic facts are certainly useful for disambiguating person names,they also allow for automatic extraction of encylopedic knowledge that has been limited to manual efforts such as britannica, wikipedia, etc.such encyploedic knowledge can advance vertical search engines such as http://www.spock.com that are focused on people searches where one can get an enhanced search interface for searching by various biographic attributes.
</nextsent>
<nextsent>biographic facts are also useful for powerful query mechanisms such as finding what attributes are common between two people (auer and lehmann, 2007).figure 1: goal: extracting attribute-value biographic fact pairs from biographic free-text while there are large quantity of biographic texts available online, there are only few biographic fact databases available1, and most of them have been created manually, are incomplete and are available primarily in english.
</nextsent>
<nextsent>this work presents multiple novel approaches for automatically extracting biographic facts such as birthdate?, occupation?, nationality?, andreligion?, making use of diverse sources of information present in biographies.
</nextsent>
<nextsent>in particular, we have proposed and evaluated the following 6 distinct original approaches to this 1e.g.: http://www.nndb.com, http://www.biography.com, info boxes in wikipedia 300 task with large collective empirical gains: 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3341">
<title id=" E09-1035.xml">structural transitive and latent models for biographic fact extraction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the ensemble of these proposed models substantially outperforms standard pattern-based biographic fact extraction methods and performance is further improved by modelinginter-attribute correlations and distributions over functions of attributes, achieving an average extraction accuracy of 80% over seven types of biographic attributes.
</prevsent>
<prevsent>extracting biographic facts such as birthdate?, occupation?, nationality?, etc. is critical step for advancing the state of the art in information processing and retrieval.
</prevsent>
</prevsection>
<citsent citstr=" W07-2012 ">
an important aspect of web search is to be able to narrow down search results by distinguishing among people with the same name leading to multiple efforts focusing on web person name disambiguation in the literature (mann and yarowsky, 2003; <papid> W03-0405 </papid>artiles et al, 2007, <papid> W07-2012 </papid>cucerzan, 2007).<papid> D07-1074 </papid></citsent>
<aftsection>
<nextsent>while biographic facts are certainly useful for disambiguating person names,they also allow for automatic extraction of encylopedic knowledge that has been limited to manual efforts such as britannica, wikipedia, etc.such encyploedic knowledge can advance vertical search engines such as http://www.spock.com that are focused on people searches where one can get an enhanced search interface for searching by various biographic attributes.
</nextsent>
<nextsent>biographic facts are also useful for powerful query mechanisms such as finding what attributes are common between two people (auer and lehmann, 2007).figure 1: goal: extracting attribute-value biographic fact pairs from biographic free-text while there are large quantity of biographic texts available online, there are only few biographic fact databases available1, and most of them have been created manually, are incomplete and are available primarily in english.
</nextsent>
<nextsent>this work presents multiple novel approaches for automatically extracting biographic facts such as birthdate?, occupation?, nationality?, andreligion?, making use of diverse sources of information present in biographies.
</nextsent>
<nextsent>in particular, we have proposed and evaluated the following 6 distinct original approaches to this 1e.g.: http://www.nndb.com, http://www.biography.com, info boxes in wikipedia 300 task with large collective empirical gains: 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3342">
<title id=" E09-1035.xml">structural transitive and latent models for biographic fact extraction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the ensemble of these proposed models substantially outperforms standard pattern-based biographic fact extraction methods and performance is further improved by modelinginter-attribute correlations and distributions over functions of attributes, achieving an average extraction accuracy of 80% over seven types of biographic attributes.
</prevsent>
<prevsent>extracting biographic facts such as birthdate?, occupation?, nationality?, etc. is critical step for advancing the state of the art in information processing and retrieval.
</prevsent>
</prevsection>
<citsent citstr=" D07-1074 ">
an important aspect of web search is to be able to narrow down search results by distinguishing among people with the same name leading to multiple efforts focusing on web person name disambiguation in the literature (mann and yarowsky, 2003; <papid> W03-0405 </papid>artiles et al, 2007, <papid> W07-2012 </papid>cucerzan, 2007).<papid> D07-1074 </papid></citsent>
<aftsection>
<nextsent>while biographic facts are certainly useful for disambiguating person names,they also allow for automatic extraction of encylopedic knowledge that has been limited to manual efforts such as britannica, wikipedia, etc.such encyploedic knowledge can advance vertical search engines such as http://www.spock.com that are focused on people searches where one can get an enhanced search interface for searching by various biographic attributes.
</nextsent>
<nextsent>biographic facts are also useful for powerful query mechanisms such as finding what attributes are common between two people (auer and lehmann, 2007).figure 1: goal: extracting attribute-value biographic fact pairs from biographic free-text while there are large quantity of biographic texts available online, there are only few biographic fact databases available1, and most of them have been created manually, are incomplete and are available primarily in english.
</nextsent>
<nextsent>this work presents multiple novel approaches for automatically extracting biographic facts such as birthdate?, occupation?, nationality?, andreligion?, making use of diverse sources of information present in biographies.
</nextsent>
<nextsent>in particular, we have proposed and evaluated the following 6 distinct original approaches to this 1e.g.: http://www.nndb.com, http://www.biography.com, info boxes in wikipedia 300 task with large collective empirical gains: 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3343">
<title id=" E09-1035.xml">structural transitive and latent models for biographic fact extraction </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>we propose and evaluate techniques for exploiting all of the above classes of information in the next sections.
</prevsent>
<prevsent>the literature for biography extraction falls intotwo major classes.
</prevsent>
</prevsection>
<citsent citstr=" P01-1059 ">
the first one deals with identifying and extracting biographical sentences and treats the problem as summarization task (cowie et al, 2000, schiffman et al, 2001, <papid> P01-1059 </papid>zhou et al., 2004).<papid> W04-3256 </papid></citsent>
<aftsection>
<nextsent>the second and more closely related class deals with extracting specific facts such as birthplace?, occupation?, etc. for this task, the primary theme of work in the literature has been to treat the task as general semantic-class learning problem where one starts with few seeds of the semantic relationship of interest and learns contextual patterns such as ? name  was born in  birthplace ?
</nextsent>
<nextsent>or ? name  (born birth date )?
</nextsent>
<nextsent>(hearst, 1992; <papid> C92-2082 </papid>riloff, 1996; the len and riloff, 2002; <papid> W02-1028 </papid>agichtein and gravano, 2000; ravichandran and hovy, 2002; <papid> P02-1006 </papid>mann and yarowsky, 2003; <papid> W03-0405 </papid>jijkoun et al, 2004; <papid> C04-1188 </papid>mann and yarowsky, 2005; <papid> P05-1060 </papid>alfonseca et al, 2006; <papid> P06-2002 </papid>pasca etal., 2006).</nextsent>
<nextsent>there has also been some work on extracting biographic facts directly from wikipedia pages.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3344">
<title id=" E09-1035.xml">structural transitive and latent models for biographic fact extraction </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>we propose and evaluate techniques for exploiting all of the above classes of information in the next sections.
</prevsent>
<prevsent>the literature for biography extraction falls intotwo major classes.
</prevsent>
</prevsection>
<citsent citstr=" W04-3256 ">
the first one deals with identifying and extracting biographical sentences and treats the problem as summarization task (cowie et al, 2000, schiffman et al, 2001, <papid> P01-1059 </papid>zhou et al., 2004).<papid> W04-3256 </papid></citsent>
<aftsection>
<nextsent>the second and more closely related class deals with extracting specific facts such as birthplace?, occupation?, etc. for this task, the primary theme of work in the literature has been to treat the task as general semantic-class learning problem where one starts with few seeds of the semantic relationship of interest and learns contextual patterns such as ? name  was born in  birthplace ?
</nextsent>
<nextsent>or ? name  (born birth date )?
</nextsent>
<nextsent>(hearst, 1992; <papid> C92-2082 </papid>riloff, 1996; the len and riloff, 2002; <papid> W02-1028 </papid>agichtein and gravano, 2000; ravichandran and hovy, 2002; <papid> P02-1006 </papid>mann and yarowsky, 2003; <papid> W03-0405 </papid>jijkoun et al, 2004; <papid> C04-1188 </papid>mann and yarowsky, 2005; <papid> P05-1060 </papid>alfonseca et al, 2006; <papid> P06-2002 </papid>pasca etal., 2006).</nextsent>
<nextsent>there has also been some work on extracting biographic facts directly from wikipedia pages.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3345">
<title id=" E09-1035.xml">structural transitive and latent models for biographic fact extraction </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the second and more closely related class deals with extracting specific facts such as birthplace?, occupation?, etc. for this task, the primary theme of work in the literature has been to treat the task as general semantic-class learning problem where one starts with few seeds of the semantic relationship of interest and learns contextual patterns such as ? name  was born in  birthplace ?
</prevsent>
<prevsent>or ? name  (born birth date )?
</prevsent>
</prevsection>
<citsent citstr=" C92-2082 ">
(hearst, 1992; <papid> C92-2082 </papid>riloff, 1996; the len and riloff, 2002; <papid> W02-1028 </papid>agichtein and gravano, 2000; ravichandran and hovy, 2002; <papid> P02-1006 </papid>mann and yarowsky, 2003; <papid> W03-0405 </papid>jijkoun et al, 2004; <papid> C04-1188 </papid>mann and yarowsky, 2005; <papid> P05-1060 </papid>alfonseca et al, 2006; <papid> P06-2002 </papid>pasca etal., 2006).</citsent>
<aftsection>
<nextsent>there has also been some work on extracting biographic facts directly from wikipedia pages.
</nextsent>
<nextsent>culotta et al (2006) <papid> N06-1038 </papid>deal with learning contextual patterns for extracting family relationships from wikipedia.</nextsent>
<nextsent>ruiz-casado et al (2006) learn contextual patterns for biographic facts and apply them to wikipedia pages.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3346">
<title id=" E09-1035.xml">structural transitive and latent models for biographic fact extraction </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the second and more closely related class deals with extracting specific facts such as birthplace?, occupation?, etc. for this task, the primary theme of work in the literature has been to treat the task as general semantic-class learning problem where one starts with few seeds of the semantic relationship of interest and learns contextual patterns such as ? name  was born in  birthplace ?
</prevsent>
<prevsent>or ? name  (born birth date )?
</prevsent>
</prevsection>
<citsent citstr=" W02-1028 ">
(hearst, 1992; <papid> C92-2082 </papid>riloff, 1996; the len and riloff, 2002; <papid> W02-1028 </papid>agichtein and gravano, 2000; ravichandran and hovy, 2002; <papid> P02-1006 </papid>mann and yarowsky, 2003; <papid> W03-0405 </papid>jijkoun et al, 2004; <papid> C04-1188 </papid>mann and yarowsky, 2005; <papid> P05-1060 </papid>alfonseca et al, 2006; <papid> P06-2002 </papid>pasca etal., 2006).</citsent>
<aftsection>
<nextsent>there has also been some work on extracting biographic facts directly from wikipedia pages.
</nextsent>
<nextsent>culotta et al (2006) <papid> N06-1038 </papid>deal with learning contextual patterns for extracting family relationships from wikipedia.</nextsent>
<nextsent>ruiz-casado et al (2006) learn contextual patterns for biographic facts and apply them to wikipedia pages.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3347">
<title id=" E09-1035.xml">structural transitive and latent models for biographic fact extraction </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the second and more closely related class deals with extracting specific facts such as birthplace?, occupation?, etc. for this task, the primary theme of work in the literature has been to treat the task as general semantic-class learning problem where one starts with few seeds of the semantic relationship of interest and learns contextual patterns such as ? name  was born in  birthplace ?
</prevsent>
<prevsent>or ? name  (born birth date )?
</prevsent>
</prevsection>
<citsent citstr=" P02-1006 ">
(hearst, 1992; <papid> C92-2082 </papid>riloff, 1996; the len and riloff, 2002; <papid> W02-1028 </papid>agichtein and gravano, 2000; ravichandran and hovy, 2002; <papid> P02-1006 </papid>mann and yarowsky, 2003; <papid> W03-0405 </papid>jijkoun et al, 2004; <papid> C04-1188 </papid>mann and yarowsky, 2005; <papid> P05-1060 </papid>alfonseca et al, 2006; <papid> P06-2002 </papid>pasca etal., 2006).</citsent>
<aftsection>
<nextsent>there has also been some work on extracting biographic facts directly from wikipedia pages.
</nextsent>
<nextsent>culotta et al (2006) <papid> N06-1038 </papid>deal with learning contextual patterns for extracting family relationships from wikipedia.</nextsent>
<nextsent>ruiz-casado et al (2006) learn contextual patterns for biographic facts and apply them to wikipedia pages.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3351">
<title id=" E09-1035.xml">structural transitive and latent models for biographic fact extraction </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the second and more closely related class deals with extracting specific facts such as birthplace?, occupation?, etc. for this task, the primary theme of work in the literature has been to treat the task as general semantic-class learning problem where one starts with few seeds of the semantic relationship of interest and learns contextual patterns such as ? name  was born in  birthplace ?
</prevsent>
<prevsent>or ? name  (born birth date )?
</prevsent>
</prevsection>
<citsent citstr=" C04-1188 ">
(hearst, 1992; <papid> C92-2082 </papid>riloff, 1996; the len and riloff, 2002; <papid> W02-1028 </papid>agichtein and gravano, 2000; ravichandran and hovy, 2002; <papid> P02-1006 </papid>mann and yarowsky, 2003; <papid> W03-0405 </papid>jijkoun et al, 2004; <papid> C04-1188 </papid>mann and yarowsky, 2005; <papid> P05-1060 </papid>alfonseca et al, 2006; <papid> P06-2002 </papid>pasca etal., 2006).</citsent>
<aftsection>
<nextsent>there has also been some work on extracting biographic facts directly from wikipedia pages.
</nextsent>
<nextsent>culotta et al (2006) <papid> N06-1038 </papid>deal with learning contextual patterns for extracting family relationships from wikipedia.</nextsent>
<nextsent>ruiz-casado et al (2006) learn contextual patterns for biographic facts and apply them to wikipedia pages.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3352">
<title id=" E09-1035.xml">structural transitive and latent models for biographic fact extraction </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the second and more closely related class deals with extracting specific facts such as birthplace?, occupation?, etc. for this task, the primary theme of work in the literature has been to treat the task as general semantic-class learning problem where one starts with few seeds of the semantic relationship of interest and learns contextual patterns such as ? name  was born in  birthplace ?
</prevsent>
<prevsent>or ? name  (born birth date )?
</prevsent>
</prevsection>
<citsent citstr=" P05-1060 ">
(hearst, 1992; <papid> C92-2082 </papid>riloff, 1996; the len and riloff, 2002; <papid> W02-1028 </papid>agichtein and gravano, 2000; ravichandran and hovy, 2002; <papid> P02-1006 </papid>mann and yarowsky, 2003; <papid> W03-0405 </papid>jijkoun et al, 2004; <papid> C04-1188 </papid>mann and yarowsky, 2005; <papid> P05-1060 </papid>alfonseca et al, 2006; <papid> P06-2002 </papid>pasca etal., 2006).</citsent>
<aftsection>
<nextsent>there has also been some work on extracting biographic facts directly from wikipedia pages.
</nextsent>
<nextsent>culotta et al (2006) <papid> N06-1038 </papid>deal with learning contextual patterns for extracting family relationships from wikipedia.</nextsent>
<nextsent>ruiz-casado et al (2006) learn contextual patterns for biographic facts and apply them to wikipedia pages.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3355">
<title id=" E09-1035.xml">structural transitive and latent models for biographic fact extraction </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the second and more closely related class deals with extracting specific facts such as birthplace?, occupation?, etc. for this task, the primary theme of work in the literature has been to treat the task as general semantic-class learning problem where one starts with few seeds of the semantic relationship of interest and learns contextual patterns such as ? name  was born in  birthplace ?
</prevsent>
<prevsent>or ? name  (born birth date )?
</prevsent>
</prevsection>
<citsent citstr=" P06-2002 ">
(hearst, 1992; <papid> C92-2082 </papid>riloff, 1996; the len and riloff, 2002; <papid> W02-1028 </papid>agichtein and gravano, 2000; ravichandran and hovy, 2002; <papid> P02-1006 </papid>mann and yarowsky, 2003; <papid> W03-0405 </papid>jijkoun et al, 2004; <papid> C04-1188 </papid>mann and yarowsky, 2005; <papid> P05-1060 </papid>alfonseca et al, 2006; <papid> P06-2002 </papid>pasca etal., 2006).</citsent>
<aftsection>
<nextsent>there has also been some work on extracting biographic facts directly from wikipedia pages.
</nextsent>
<nextsent>culotta et al (2006) <papid> N06-1038 </papid>deal with learning contextual patterns for extracting family relationships from wikipedia.</nextsent>
<nextsent>ruiz-casado et al (2006) learn contextual patterns for biographic facts and apply them to wikipedia pages.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3357">
<title id=" E09-1035.xml">structural transitive and latent models for biographic fact extraction </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>(hearst, 1992; <papid> C92-2082 </papid>riloff, 1996; the len and riloff, 2002; <papid> W02-1028 </papid>agichtein and gravano, 2000; ravichandran and hovy, 2002; <papid> P02-1006 </papid>mann and yarowsky, 2003; <papid> W03-0405 </papid>jijkoun et al, 2004; <papid> C04-1188 </papid>mann and yarowsky, 2005; <papid> P05-1060 </papid>alfonseca et al, 2006; <papid> P06-2002 </papid>pasca etal., 2006).</prevsent>
<prevsent>there has also been some work on extracting biographic facts directly from wikipedia pages.</prevsent>
</prevsection>
<citsent citstr=" N06-1038 ">
culotta et al (2006) <papid> N06-1038 </papid>deal with learning contextual patterns for extracting family relationships from wikipedia.</citsent>
<aftsection>
<nextsent>ruiz-casado et al (2006) learn contextual patterns for biographic facts and apply them to wikipedia pages.
</nextsent>
<nextsent>while the pattern-learning approach extends wellfor few biography classes, some of the biographic facts like gender?
</nextsent>
<nextsent>and religion?
</nextsent>
<nextsent>do not have consistent contextual patterns, and only few of the explicit biographic attributes such asbirthdate?, deathdate?, birthplace?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3386">
<title id=" E12-1009.xml">the best of both worlds  a graph based completion model for transition based parsers </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>weapply the new transition-based parser on typo logically different languages such as english, chinese, czech, and german and report competitive labeled and unlabeled attachment scores.
</prevsent>
<prevsent>background.
</prevsent>
</prevsection>
<citsent citstr=" C96-1058 ">
a considerable amount of recent research has gone into data-driven dependency parsing, and interestingly throughout the continuous process of improvements, two classes of parsing algorithms have stayed at the centre of attention, the transition-based (nivre, 2003) vs. the graph-based approach (eisner, 1996; <papid> C96-1058 </papid>mcdonald et al 2005).<papid> P05-1012 </papid>1 the two approaches apply fundamentally different strategies to solve the task of finding the optimal labeled dependency tree over the words of an input sentence (where supervised machine learning is used to estimate the scoring parameters on treebank).</citsent>
<aftsection>
<nextsent>the transition-based approach is based on the conceptually (and cognitively) compelling idea 1more references will be provided in sec.
</nextsent>
<nextsent>2.that machine learning, i.e., model of linguistic experience, is used in exactly those situations when there is an attachment choice in an otherwise deterministic incremental left-to-right parsing process.
</nextsent>
<nextsent>as new word is processed, the parser has to decide on one out of small number of possible transitions (adding dependency arc pointing to the left or right and/or pushing or popping word on/from stack representation).obviously, the learning can be based on the feature information available at particular snapshot in incremental processing, i.e., only surface information for the un parsed material to the right, but full structural information for the parts of the string already processed.
</nextsent>
<nextsent>for the completely processed parts, there are no principled limitations as regards the types of structural configurations that can be checked in feature functions.the graph-based approach in contrast emphasizes the objective of exhaustive search over all possible trees spanning the input words.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3387">
<title id=" E12-1009.xml">the best of both worlds  a graph based completion model for transition based parsers </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>weapply the new transition-based parser on typo logically different languages such as english, chinese, czech, and german and report competitive labeled and unlabeled attachment scores.
</prevsent>
<prevsent>background.
</prevsent>
</prevsection>
<citsent citstr=" P05-1012 ">
a considerable amount of recent research has gone into data-driven dependency parsing, and interestingly throughout the continuous process of improvements, two classes of parsing algorithms have stayed at the centre of attention, the transition-based (nivre, 2003) vs. the graph-based approach (eisner, 1996; <papid> C96-1058 </papid>mcdonald et al 2005).<papid> P05-1012 </papid>1 the two approaches apply fundamentally different strategies to solve the task of finding the optimal labeled dependency tree over the words of an input sentence (where supervised machine learning is used to estimate the scoring parameters on treebank).</citsent>
<aftsection>
<nextsent>the transition-based approach is based on the conceptually (and cognitively) compelling idea 1more references will be provided in sec.
</nextsent>
<nextsent>2.that machine learning, i.e., model of linguistic experience, is used in exactly those situations when there is an attachment choice in an otherwise deterministic incremental left-to-right parsing process.
</nextsent>
<nextsent>as new word is processed, the parser has to decide on one out of small number of possible transitions (adding dependency arc pointing to the left or right and/or pushing or popping word on/from stack representation).obviously, the learning can be based on the feature information available at particular snapshot in incremental processing, i.e., only surface information for the un parsed material to the right, but full structural information for the parts of the string already processed.
</nextsent>
<nextsent>for the completely processed parts, there are no principled limitations as regards the types of structural configurations that can be checked in feature functions.the graph-based approach in contrast emphasizes the objective of exhaustive search over all possible trees spanning the input words.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3388">
<title id=" E12-1009.xml">the best of both worlds  a graph based completion model for transition based parsers </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the typical feature models are based on combinations of edges (so 77 called second-order factors) that closely follow the bottom-up combination of sub spans in the parsing algorithm, i.e., the feature functions depend on the presence of two specific dependency edges.
</prevsent>
<prevsent>configurations not directly supported by the bottom-up building of larger spans are more cumbersome to integrate into the model (since the combination algorithm has to be adjusted), in particular for third-order factors or higher.
</prevsent>
</prevsection>
<citsent citstr=" P08-1108 ">
empirically, i.e., when applied in supervised machine learning experiments based on existing treebanks for various languages, both strategies (and further refinements of them not mentioned here) turn out roughly equal in their capability of picking up most of the relevant patterns well;some subtle strengths and weaknesses are complementary, such that stacking of two parsers representing both strategies yields the best results(nivre and mcdonald, 2008): <papid> P08-1108 </papid>in training and application, one of the parsers is run on each sentence prior to the other, providing additional feature information for the other parser.</citsent>
<aftsection>
<nextsent>another successful technique to combine parsers is voting as carried out by sagae and lavie (2006).<papid> N06-2033 </papid></nextsent>
<nextsent>the present paper addresses the question if and how more integrated combination of the strengths of the two strategies can be achieved and implemented efficiently to warrant competitive results.the main issue and solution strategy.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3389">
<title id=" E12-1009.xml">the best of both worlds  a graph based completion model for transition based parsers </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>configurations not directly supported by the bottom-up building of larger spans are more cumbersome to integrate into the model (since the combination algorithm has to be adjusted), in particular for third-order factors or higher.
</prevsent>
<prevsent>empirically, i.e., when applied in supervised machine learning experiments based on existing treebanks for various languages, both strategies (and further refinements of them not mentioned here) turn out roughly equal in their capability of picking up most of the relevant patterns well;some subtle strengths and weaknesses are complementary, such that stacking of two parsers representing both strategies yields the best results(nivre and mcdonald, 2008): <papid> P08-1108 </papid>in training and application, one of the parsers is run on each sentence prior to the other, providing additional feature information for the other parser.</prevsent>
</prevsection>
<citsent citstr=" N06-2033 ">
another successful technique to combine parsers is voting as carried out by sagae and lavie (2006).<papid> N06-2033 </papid></citsent>
<aftsection>
<nextsent>the present paper addresses the question if and how more integrated combination of the strengths of the two strategies can be achieved and implemented efficiently to warrant competitive results.the main issue and solution strategy.
</nextsent>
<nextsent>in order to preserve the conceptual (and complexity) advantages of the transition-based strategy, the integrated algorithm we are looking for has tobe transition-based at the top level.
</nextsent>
<nextsent>the advantages of the graph-based approach ? more globally informed basis for the decision among different attachment options ? have to be include das part of the scoring procedure.
</nextsent>
<nextsent>as prerequisite, our algorithm will require memory for storing alternative analyses among which to choose.this has been previously introduced in transition based approaches in the form of beam (johans son and nugues, 2006): <papid> W06-2930 </papid>rather than representing only the best-scoring history of transitions, the best-scoring alternative histories are kept around.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3390">
<title id=" E12-1009.xml">the best of both worlds  a graph based completion model for transition based parsers </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in order to preserve the conceptual (and complexity) advantages of the transition-based strategy, the integrated algorithm we are looking for has tobe transition-based at the top level.
</prevsent>
<prevsent>the advantages of the graph-based approach ? more globally informed basis for the decision among different attachment options ? have to be include das part of the scoring procedure.
</prevsent>
</prevsection>
<citsent citstr=" W06-2930 ">
as prerequisite, our algorithm will require memory for storing alternative analyses among which to choose.this has been previously introduced in transition based approaches in the form of beam (johans son and nugues, 2006): <papid> W06-2930 </papid>rather than representing only the best-scoring history of transitions, the best-scoring alternative histories are kept around.</citsent>
<aftsection>
<nextsent>as we will indicate in the following, the mere addition of beam search does not help overcome representational key issue of transition-based parsing: in many situations, transition-based parser is forced to make an attachment decision forgiven input word at point where no or only partial information about the words own dependents (and further decendents) is available.
</nextsent>
<nextsent>figure 1 illustrates such case.
</nextsent>
<nextsent>figure 1: the left set of brackets indicates material that has been processed or is under consideration; onthe right is the input, still to be processed.
</nextsent>
<nextsent>access to information that is yet unavailable would help the parser to decide on the correct transition.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3391">
<title id=" E12-1009.xml">the best of both worlds  a graph based completion model for transition based parsers </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in section 3, we introduce our transition-based parser and in section 4 the completion model as well as the implementation of third order models.
</prevsent>
<prevsent>in section 5, we describe experiments and provide evaluation results on selected datasets.
</prevsent>
</prevsection>
<citsent citstr=" W02-2016 ">
kudo and matsumoto (2002) <papid> W02-2016 </papid>and yamada and matsumoto (2003) carried over the idea forde terministic parsing by chunks from abney (1991) to dependency parsing.</citsent>
<aftsection>
<nextsent>nivre (2003) describes in more strict sense the first incremental parser that tries to find the most appropriate dependency tree by sequence of local transitions.
</nextsent>
<nextsent>in order to optimize the results towards more globally optimal solution, johansson and nugues (2006)<papid> W06-2930 </papid>first applied beam search, which leads to substantial improv ment of the results (cf.</nextsent>
<nextsent>also (titov and henderson, 2007)).<papid> W07-2218 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3393">
<title id=" E12-1009.xml">the best of both worlds  a graph based completion model for transition based parsers </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>nivre (2003) describes in more strict sense the first incremental parser that tries to find the most appropriate dependency tree by sequence of local transitions.
</prevsent>
<prevsent>in order to optimize the results towards more globally optimal solution, johansson and nugues (2006)<papid> W06-2930 </papid>first applied beam search, which leads to substantial improv ment of the results (cf.</prevsent>
</prevsection>
<citsent citstr=" W07-2218 ">
also (titov and henderson, 2007)).<papid> W07-2218 </papid></citsent>
<aftsection>
<nextsent>zhang and clark (2008) <papid> D08-1059 </papid>augment the beam-search algorithm, adapting the early update strategy of collins and roark (2004) <papid> P04-1015 </papid>to dependency parsing.</nextsent>
<nextsent>in this approach, the parser stops and updates the model when the oracle transition sequence drops out of the beam.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3394">
<title id=" E12-1009.xml">the best of both worlds  a graph based completion model for transition based parsers </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in order to optimize the results towards more globally optimal solution, johansson and nugues (2006)<papid> W06-2930 </papid>first applied beam search, which leads to substantial improv ment of the results (cf.</prevsent>
<prevsent>also (titov and henderson, 2007)).<papid> W07-2218 </papid></prevsent>
</prevsection>
<citsent citstr=" D08-1059 ">
zhang and clark (2008) <papid> D08-1059 </papid>augment the beam-search algorithm, adapting the early update strategy of collins and roark (2004) <papid> P04-1015 </papid>to dependency parsing.</citsent>
<aftsection>
<nextsent>in this approach, the parser stops and updates the model when the oracle transition sequence drops out of the beam.
</nextsent>
<nextsent>in contrast to most other approaches, the training procedure of zhang and clark (2008) <papid> D08-1059 </papid>takes the complete transition sequence into account as it is calculating the update.</nextsent>
<nextsent>zhang and clark compare aspects of transition-based and graph-based parsing, and end up using transition-based parser with combined transition-based/second-order graph-based scoring model (zhang and clark, 2008, <papid> D08-1059 </papid>567), which is similar to the approach we describe in this paper.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3397">
<title id=" E12-1009.xml">the best of both worlds  a graph based completion model for transition based parsers </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in order to optimize the results towards more globally optimal solution, johansson and nugues (2006)<papid> W06-2930 </papid>first applied beam search, which leads to substantial improv ment of the results (cf.</prevsent>
<prevsent>also (titov and henderson, 2007)).<papid> W07-2218 </papid></prevsent>
</prevsection>
<citsent citstr=" P04-1015 ">
zhang and clark (2008) <papid> D08-1059 </papid>augment the beam-search algorithm, adapting the early update strategy of collins and roark (2004) <papid> P04-1015 </papid>to dependency parsing.</citsent>
<aftsection>
<nextsent>in this approach, the parser stops and updates the model when the oracle transition sequence drops out of the beam.
</nextsent>
<nextsent>in contrast to most other approaches, the training procedure of zhang and clark (2008) <papid> D08-1059 </papid>takes the complete transition sequence into account as it is calculating the update.</nextsent>
<nextsent>zhang and clark compare aspects of transition-based and graph-based parsing, and end up using transition-based parser with combined transition-based/second-order graph-based scoring model (zhang and clark, 2008, <papid> D08-1059 </papid>567), which is similar to the approach we describe in this paper.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3405">
<title id=" E12-1009.xml">the best of both worlds  a graph based completion model for transition based parsers </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>however, their approach does not involve beam rescoring as the partial structures built by the transition-based parser are subsequently augmented; hence, there are cases in which our approach is able to differentiate based on higher-order factors that go unnoticed by the combined model of (zhang and clark, 2008, <papid> D08-1059 </papid>567).</prevsent>
<prevsent>one step beyond the use of beam is dynamic programming approach to carry out full search in the state space, cf.</prevsent>
</prevsection>
<citsent citstr=" P10-1110 ">
(huang and sagae, 2010; <papid> P10-1110 </papid>kuhlmann et al 2011).<papid> P11-1068 </papid></citsent>
<aftsection>
<nextsent>however, in this case one has to restrict the employed features to set which fits to the elements composed by the dy 79 namic programming approach.
</nextsent>
<nextsent>this is trade-off between an exhaustive search and unrestricted (rich) feature set and the question which providesa higher accuracy is still an open research question, cf.
</nextsent>
<nextsent>(kuhlmann et al 2011).<papid> P11-1068 </papid></nextsent>
<nextsent>parsing of non-projective dependency trees is an important feature for many languages.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3406">
<title id=" E12-1009.xml">the best of both worlds  a graph based completion model for transition based parsers </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>however, their approach does not involve beam rescoring as the partial structures built by the transition-based parser are subsequently augmented; hence, there are cases in which our approach is able to differentiate based on higher-order factors that go unnoticed by the combined model of (zhang and clark, 2008, <papid> D08-1059 </papid>567).</prevsent>
<prevsent>one step beyond the use of beam is dynamic programming approach to carry out full search in the state space, cf.</prevsent>
</prevsection>
<citsent citstr=" P11-1068 ">
(huang and sagae, 2010; <papid> P10-1110 </papid>kuhlmann et al 2011).<papid> P11-1068 </papid></citsent>
<aftsection>
<nextsent>however, in this case one has to restrict the employed features to set which fits to the elements composed by the dy 79 namic programming approach.
</nextsent>
<nextsent>this is trade-off between an exhaustive search and unrestricted (rich) feature set and the question which providesa higher accuracy is still an open research question, cf.
</nextsent>
<nextsent>(kuhlmann et al 2011).<papid> P11-1068 </papid></nextsent>
<nextsent>parsing of non-projective dependency trees is an important feature for many languages.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3410">
<title id=" E12-1009.xml">the best of both worlds  a graph based completion model for transition based parsers </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>(kuhlmann et al 2011).<papid> P11-1068 </papid></prevsent>
<prevsent>parsing of non-projective dependency trees is an important feature for many languages.</prevsent>
</prevsection>
<citsent citstr=" P98-1106 ">
atfirst most algorithms were restricted to projective dependency trees and used pseudo-projective parsing (kahane et al 1998; <papid> P98-1106 </papid>nivre and nilsson,2005).<papid> P05-1013 </papid></citsent>
<aftsection>
<nextsent>later, additional transitions were introduced to handle non-projectivity (attardi, 2006; <papid> W06-2922 </papid>nivre, 2009).</nextsent>
<nextsent>the most common strategy uses the swap transition (nivre, 2009; nivre et al 2009), <papid> W09-3811 </papid>an alternative solution uses two planes and switch transition to switch between the two planes (gomez-rodrguez and nivre, 2010).since we use the scoring model of graph based parser, we briefly review releated work on graph-based parsing.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3411">
<title id=" E12-1009.xml">the best of both worlds  a graph based completion model for transition based parsers </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>(kuhlmann et al 2011).<papid> P11-1068 </papid></prevsent>
<prevsent>parsing of non-projective dependency trees is an important feature for many languages.</prevsent>
</prevsection>
<citsent citstr=" P05-1013 ">
atfirst most algorithms were restricted to projective dependency trees and used pseudo-projective parsing (kahane et al 1998; <papid> P98-1106 </papid>nivre and nilsson,2005).<papid> P05-1013 </papid></citsent>
<aftsection>
<nextsent>later, additional transitions were introduced to handle non-projectivity (attardi, 2006; <papid> W06-2922 </papid>nivre, 2009).</nextsent>
<nextsent>the most common strategy uses the swap transition (nivre, 2009; nivre et al 2009), <papid> W09-3811 </papid>an alternative solution uses two planes and switch transition to switch between the two planes (gomez-rodrguez and nivre, 2010).since we use the scoring model of graph based parser, we briefly review releated work on graph-based parsing.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3412">
<title id=" E12-1009.xml">the best of both worlds  a graph based completion model for transition based parsers </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>parsing of non-projective dependency trees is an important feature for many languages.
</prevsent>
<prevsent>atfirst most algorithms were restricted to projective dependency trees and used pseudo-projective parsing (kahane et al 1998; <papid> P98-1106 </papid>nivre and nilsson,2005).<papid> P05-1013 </papid></prevsent>
</prevsection>
<citsent citstr=" W06-2922 ">
later, additional transitions were introduced to handle non-projectivity (attardi, 2006; <papid> W06-2922 </papid>nivre, 2009).</citsent>
<aftsection>
<nextsent>the most common strategy uses the swap transition (nivre, 2009; nivre et al 2009), <papid> W09-3811 </papid>an alternative solution uses two planes and switch transition to switch between the two planes (gomez-rodrguez and nivre, 2010).since we use the scoring model of graph based parser, we briefly review releated work on graph-based parsing.</nextsent>
<nextsent>the most well knowngraph-based parser is the mst (maximum spanning tree) parser, cf.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3413">
<title id=" E12-1009.xml">the best of both worlds  a graph based completion model for transition based parsers </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>atfirst most algorithms were restricted to projective dependency trees and used pseudo-projective parsing (kahane et al 1998; <papid> P98-1106 </papid>nivre and nilsson,2005).<papid> P05-1013 </papid></prevsent>
<prevsent>later, additional transitions were introduced to handle non-projectivity (attardi, 2006; <papid> W06-2922 </papid>nivre, 2009).</prevsent>
</prevsection>
<citsent citstr=" W09-3811 ">
the most common strategy uses the swap transition (nivre, 2009; nivre et al 2009), <papid> W09-3811 </papid>an alternative solution uses two planes and switch transition to switch between the two planes (gomez-rodrguez and nivre, 2010).since we use the scoring model of graph based parser, we briefly review releated work on graph-based parsing.</citsent>
<aftsection>
<nextsent>the most well knowngraph-based parser is the mst (maximum spanning tree) parser, cf.
</nextsent>
<nextsent>(mcdonald et al 2005; <papid> P05-1012 </papid>mcdonald and pereira, 2006).<papid> E06-1011 </papid></nextsent>
<nextsent>the idea of the mst parser is to find the highest scoring tree in graph that contains all possible edges.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3417">
<title id=" E12-1009.xml">the best of both worlds  a graph based completion model for transition based parsers </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the most common strategy uses the swap transition (nivre, 2009; nivre et al 2009), <papid> W09-3811 </papid>an alternative solution uses two planes and switch transition to switch between the two planes (gomez-rodrguez and nivre, 2010).since we use the scoring model of graph based parser, we briefly review releated work on graph-based parsing.</prevsent>
<prevsent>the most well knowngraph-based parser is the mst (maximum spanning tree) parser, cf.</prevsent>
</prevsection>
<citsent citstr=" E06-1011 ">
(mcdonald et al 2005; <papid> P05-1012 </papid>mcdonald and pereira, 2006).<papid> E06-1011 </papid></citsent>
<aftsection>
<nextsent>the idea of the mst parser is to find the highest scoring tree in graph that contains all possible edges.
</nextsent>
<nextsent>eisner (1996) <papid> C96-1058 </papid>introduced dynamic programming algorithm tosolve this problem efficiently.</nextsent>
<nextsent>carreras (2007) <papid> D07-1101 </papid>introduced the left-most and right-most grandchild as factors.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3419">
<title id=" E12-1009.xml">the best of both worlds  a graph based completion model for transition based parsers </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the idea of the mst parser is to find the highest scoring tree in graph that contains all possible edges.
</prevsent>
<prevsent>eisner (1996) <papid> C96-1058 </papid>introduced dynamic programming algorithm tosolve this problem efficiently.</prevsent>
</prevsection>
<citsent citstr=" D07-1101 ">
carreras (2007) <papid> D07-1101 </papid>introduced the left-most and right-most grandchild as factors.</citsent>
<aftsection>
<nextsent>we use the factor model of carreras (2007) <papid> D07-1101 </papid>as starting point for our experiments, cf.section 4.</nextsent>
<nextsent>we extend carreras (2007) <papid> D07-1101 </papid>graph based model with factors involving three edges similar to that of koo and collins (2010).<papid> P10-1001 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3428">
<title id=" E12-1009.xml">the best of both worlds  a graph based completion model for transition based parsers </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>carreras (2007) <papid> D07-1101 </papid>introduced the left-most and right-most grandchild as factors.</prevsent>
<prevsent>we use the factor model of carreras (2007) <papid> D07-1101 </papid>as starting point for our experiments, cf.section 4.</prevsent>
</prevsection>
<citsent citstr=" P10-1001 ">
we extend carreras (2007) <papid> D07-1101 </papid>graph based model with factors involving three edges similar to that of koo and collins (2010).<papid> P10-1001 </papid></citsent>
<aftsection>
<nextsent>this section specifies the transition-based beam search parser underlying the combined approach more formally.
</nextsent>
<nextsent>sec.
</nextsent>
<nextsent>4 will discuss the graph based scoring model that we are adding.
</nextsent>
<nextsent>the input to the parser is word string x, the goal is to find the optimal set of labeled edges xil xj forming dependency tree over x?{root}.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3436">
<title id=" E12-1009.xml">the best of both worlds  a graph based completion model for transition based parsers </title>
<section> completion model.  </section>
<citcontext>
<prevsection>
<prevsent>(h,c,cm1,cm2)y ? fgra(x,h,c,cm1,cm2) (3c) ?
</prevsent>
<prevsent>(h,c,cmo,tmo)y ? fgra(x,h,c,cmo,tmo) feature set.
</prevsent>
</prevsection>
<citsent citstr=" P11-2033 ">
the feature set of the transition model is similar to that of zhang and nivre (2011).<papid> P11-2033 </papid></citsent>
<aftsection>
<nextsent>in addition, we use the cross product of morphologic features between the head and the dependent since we apply also the parser on morphologic rich languages.the feature sets of the completion model described above are mostly based on previous work (mcdonald et al 2005; <papid> P05-1012 </papid>mcdonald and pereira, 2006; <papid> E06-1011 </papid>carreras, 2007; <papid> D07-1101 </papid>koo and collins, 2010).<papid> P10-1001 </papid></nextsent>
<nextsent>the models denoted with + use all combinations of words before and after the head, dependent, sibling, grandchilrden, etc. these are respectivelythree-, and four-grams for the first order and second order.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3443">
<title id=" E12-1009.xml">the best of both worlds  a graph based completion model for transition based parsers </title>
<section> completion model.  </section>
<citcontext>
<prevsection>
<prevsent>training.
</prevsent>
<prevsent>for the training of our parser, we use variant of the perceptron algorithm that uses the passive-aggressive update function, cf.
</prevsent>
</prevsection>
<citsent citstr=" W02-1001 ">
(freund and schapire, 1998; collins, 2002; <papid> W02-1001 </papid>crammer et al., 2006).</citsent>
<aftsection>
<nextsent>the passive-aggressive perceptron uses an aggressive update strategy by modifying the weight vector by as much as needed to classify correctly the current example, cf.
</nextsent>
<nextsent>(crammer et al 2006).
</nextsent>
<nextsent>we apply random function (hash function) to retrieve the weights from the weight vector instead of table.
</nextsent>
<nextsent>bohnet (2010) <papid> C10-1011 </papid>showed that the hash kernel improves parsing speed and accuracy since the parser uses addition aly negative features.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3444">
<title id=" E12-1009.xml">the best of both worlds  a graph based completion model for transition based parsers </title>
<section> completion model.  </section>
<citcontext>
<prevsection>
<prevsent>(crammer et al 2006).
</prevsent>
<prevsent>we apply random function (hash function) to retrieve the weights from the weight vector instead of table.
</prevsent>
</prevsection>
<citsent citstr=" C10-1011 ">
bohnet (2010) <papid> C10-1011 </papid>showed that the hash kernel improves parsing speed and accuracy since the parser uses addition aly negative features.</citsent>
<aftsection>
<nextsent>ganchev and dredze (2008) <papid> W08-0804 </papid>used 82 this technique for structured prediction in nlp to reduce the needed space, cf.</nextsent>
<nextsent>(shi et al 2009).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3445">
<title id=" E12-1009.xml">the best of both worlds  a graph based completion model for transition based parsers </title>
<section> completion model.  </section>
<citcontext>
<prevsection>
<prevsent>we apply random function (hash function) to retrieve the weights from the weight vector instead of table.
</prevsent>
<prevsent>bohnet (2010) <papid> C10-1011 </papid>showed that the hash kernel improves parsing speed and accuracy since the parser uses addition aly negative features.</prevsent>
</prevsection>
<citsent citstr=" W08-0804 ">
ganchev and dredze (2008) <papid> W08-0804 </papid>used 82 this technique for structured prediction in nlp to reduce the needed space, cf.</citsent>
<aftsection>
<nextsent>(shi et al 2009).
</nextsent>
<nextsent>we use as weight vector size 800 million.
</nextsent>
<nextsent>after the training, we counted 65 millions non zero weights for english (penn2malt), 83 for czech and 87 millions for german.
</nextsent>
<nextsent>the feature vectors are the union of features originating from the transition sequence of sentence and the features of the factors over all edges of dependency tree (e.g. g2a, etc.).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3450">
<title id=" E12-1009.xml">the best of both worlds  a graph based completion model for transition based parsers </title>
<section> completion model.  </section>
<citcontext>
<prevsection>
<prevsent>the beam size is constant.
</prevsent>
<prevsent>hence, the complexity is in the worst case o(n2).the parsing time is to large degree determined by the feature extraction, the score calculation and the implementation, cf.
</prevsent>
</prevsection>
<citsent citstr=" N10-1115 ">
also (goldberg and elhadad, 2010).<papid> N10-1115 </papid></citsent>
<aftsection>
<nextsent>the transition-based parser is able to parse 30 sentences per second.
</nextsent>
<nextsent>the parser with completion model processes about 5 sentences per second with beam size of 80.
</nextsent>
<nextsent>note, we use rich feature set, completion model with third order factors, negative features, and large beam.
</nextsent>
<nextsent>3 we implemented the following optimizations: (1) we use parallel feature extraction for thebeam elements.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3452">
<title id=" E12-1009.xml">the best of both worlds  a graph based completion model for transition based parsers </title>
<section> completion model.  </section>
<citcontext>
<prevsection>
<prevsent>this is kept fix and we add only the numeric value for each of the edge labels plus value for the transition left-arc or right-arc. in this way, we create the features incrementally.
</prevsent>
<prevsent>this has some similarity to goldberg and elhadad (2010).(<papid> N10-1115 </papid>3) we apply edge filtering as it is used in graph based dependency parsing, cf.</prevsent>
</prevsection>
<citsent citstr=" W08-2123 ">
(johansson and nugues, 2008), <papid> W08-2123 </papid>i.e., we calculate the edge weights only for the labels that were found for the part-of speech combination of the head and dependent in the training data.</citsent>
<aftsection>
<nextsent>the results of different parsing systems are of ten hard to compare due to differences in phrase structure to dependency conversions, corpus version, and experimental settings.
</nextsent>
<nextsent>for better comparison, we provide results on english for two commonly used datasets, based on two different conversions of the penn treebank.
</nextsent>
<nextsent>the first uses the penn2malt conversion based on the head 36 core, 3.33 ghz intel nehalem 83 section sentences pos acc.
</nextsent>
<nextsent>training 2-21 39.832 97.08 dev 24 1.394 97.18 test 23 2.416 97.30 table 1: overview of the training, development and test data split converted to dependency graphs with head-finding rules of (yamada and matsumoto, 2003).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3466">
<title id=" E12-1009.xml">the best of both worlds  a graph based completion model for transition based parsers </title>
<section> parsing experiments and discussion.  </section>
<citcontext>
<prevsection>
<prevsent>et al 2009).
</prevsent>
<prevsent>this corpus preserves the non-projectivity of the phrase structure annotation, it has rich edge label set, and provides automatic assigned pos 4http://code.google.com/p/mate-tools/ 5we follow koo and collins (2010) <papid> P10-1001 </papid>and ignore any token whose pos tag is one of the following tokens ??</prevsent>
</prevsection>
<citsent citstr=" D10-1004 ">
parser uas las (mcdonald et al 2005) <papid> P05-1012 </papid>90.9 (mcdonald and pereira, 2006) <papid> E06-1011 </papid>91.5 (huang and sagae, 2010) <papid> P10-1110 </papid>92.1 (zhang and nivre, 2011) <papid> P11-2033 </papid>92.9 (koo and collins, 2010) <papid> P10-1001 </papid>93.04 (martins et al 2010) <papid> D10-1004 </papid>93.26 (baseline) 92.7 g2a (baseline) 92.89 t2a 92.94 91.87 t2ab 93.16 92.08 t2ab3a 93.20 92.10 t2ab3b 93.23 92.15 t2ab3c 93.17 92.10 t2ab3abc+ 93.39 92.38 g2a+ 93.1 (koo et al 2008) ? <papid> P08-1068 </papid>93.16 (carreras et al 2008) ? <papid> W08-2102 </papid>93.5 (suzuki et al 2009) ? <papid> D09-1058 </papid>93.79 table 2: english attachment scores for the penn2malt conversion of the penn treebank for the test set.</citsent>
<aftsection>
<nextsent>punctuation is excluded from the evaluation.
</nextsent>
<nextsent>the results marked with ? are not directly comparable to our work as they depend on additional sources of information (brown clusters).
</nextsent>
<nextsent>tags.
</nextsent>
<nextsent>from the same dataset, we selected the corpora for czech and german.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3467">
<title id=" E12-1009.xml">the best of both worlds  a graph based completion model for transition based parsers </title>
<section> parsing experiments and discussion.  </section>
<citcontext>
<prevsection>
<prevsent>et al 2009).
</prevsent>
<prevsent>this corpus preserves the non-projectivity of the phrase structure annotation, it has rich edge label set, and provides automatic assigned pos 4http://code.google.com/p/mate-tools/ 5we follow koo and collins (2010) <papid> P10-1001 </papid>and ignore any token whose pos tag is one of the following tokens ??</prevsent>
</prevsection>
<citsent citstr=" P08-1068 ">
parser uas las (mcdonald et al 2005) <papid> P05-1012 </papid>90.9 (mcdonald and pereira, 2006) <papid> E06-1011 </papid>91.5 (huang and sagae, 2010) <papid> P10-1110 </papid>92.1 (zhang and nivre, 2011) <papid> P11-2033 </papid>92.9 (koo and collins, 2010) <papid> P10-1001 </papid>93.04 (martins et al 2010) <papid> D10-1004 </papid>93.26 (baseline) 92.7 g2a (baseline) 92.89 t2a 92.94 91.87 t2ab 93.16 92.08 t2ab3a 93.20 92.10 t2ab3b 93.23 92.15 t2ab3c 93.17 92.10 t2ab3abc+ 93.39 92.38 g2a+ 93.1 (koo et al 2008) ? <papid> P08-1068 </papid>93.16 (carreras et al 2008) ? <papid> W08-2102 </papid>93.5 (suzuki et al 2009) ? <papid> D09-1058 </papid>93.79 table 2: english attachment scores for the penn2malt conversion of the penn treebank for the test set.</citsent>
<aftsection>
<nextsent>punctuation is excluded from the evaluation.
</nextsent>
<nextsent>the results marked with ? are not directly comparable to our work as they depend on additional sources of information (brown clusters).
</nextsent>
<nextsent>tags.
</nextsent>
<nextsent>from the same dataset, we selected the corpora for czech and german.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3468">
<title id=" E12-1009.xml">the best of both worlds  a graph based completion model for transition based parsers </title>
<section> parsing experiments and discussion.  </section>
<citcontext>
<prevsection>
<prevsent>et al 2009).
</prevsent>
<prevsent>this corpus preserves the non-projectivity of the phrase structure annotation, it has rich edge label set, and provides automatic assigned pos 4http://code.google.com/p/mate-tools/ 5we follow koo and collins (2010) <papid> P10-1001 </papid>and ignore any token whose pos tag is one of the following tokens ??</prevsent>
</prevsection>
<citsent citstr=" W08-2102 ">
parser uas las (mcdonald et al 2005) <papid> P05-1012 </papid>90.9 (mcdonald and pereira, 2006) <papid> E06-1011 </papid>91.5 (huang and sagae, 2010) <papid> P10-1110 </papid>92.1 (zhang and nivre, 2011) <papid> P11-2033 </papid>92.9 (koo and collins, 2010) <papid> P10-1001 </papid>93.04 (martins et al 2010) <papid> D10-1004 </papid>93.26 (baseline) 92.7 g2a (baseline) 92.89 t2a 92.94 91.87 t2ab 93.16 92.08 t2ab3a 93.20 92.10 t2ab3b 93.23 92.15 t2ab3c 93.17 92.10 t2ab3abc+ 93.39 92.38 g2a+ 93.1 (koo et al 2008) ? <papid> P08-1068 </papid>93.16 (carreras et al 2008) ? <papid> W08-2102 </papid>93.5 (suzuki et al 2009) ? <papid> D09-1058 </papid>93.79 table 2: english attachment scores for the penn2malt conversion of the penn treebank for the test set.</citsent>
<aftsection>
<nextsent>punctuation is excluded from the evaluation.
</nextsent>
<nextsent>the results marked with ? are not directly comparable to our work as they depend on additional sources of information (brown clusters).
</nextsent>
<nextsent>tags.
</nextsent>
<nextsent>from the same dataset, we selected the corpora for czech and german.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3469">
<title id=" E12-1009.xml">the best of both worlds  a graph based completion model for transition based parsers </title>
<section> parsing experiments and discussion.  </section>
<citcontext>
<prevsection>
<prevsent>et al 2009).
</prevsent>
<prevsent>this corpus preserves the non-projectivity of the phrase structure annotation, it has rich edge label set, and provides automatic assigned pos 4http://code.google.com/p/mate-tools/ 5we follow koo and collins (2010) <papid> P10-1001 </papid>and ignore any token whose pos tag is one of the following tokens ??</prevsent>
</prevsection>
<citsent citstr=" D09-1058 ">
parser uas las (mcdonald et al 2005) <papid> P05-1012 </papid>90.9 (mcdonald and pereira, 2006) <papid> E06-1011 </papid>91.5 (huang and sagae, 2010) <papid> P10-1110 </papid>92.1 (zhang and nivre, 2011) <papid> P11-2033 </papid>92.9 (koo and collins, 2010) <papid> P10-1001 </papid>93.04 (martins et al 2010) <papid> D10-1004 </papid>93.26 (baseline) 92.7 g2a (baseline) 92.89 t2a 92.94 91.87 t2ab 93.16 92.08 t2ab3a 93.20 92.10 t2ab3b 93.23 92.15 t2ab3c 93.17 92.10 t2ab3abc+ 93.39 92.38 g2a+ 93.1 (koo et al 2008) ? <papid> P08-1068 </papid>93.16 (carreras et al 2008) ? <papid> W08-2102 </papid>93.5 (suzuki et al 2009) ? <papid> D09-1058 </papid>93.79 table 2: english attachment scores for the penn2malt conversion of the penn treebank for the test set.</citsent>
<aftsection>
<nextsent>punctuation is excluded from the evaluation.
</nextsent>
<nextsent>the results marked with ? are not directly comparable to our work as they depend on additional sources of information (brown clusters).
</nextsent>
<nextsent>tags.
</nextsent>
<nextsent>from the same dataset, we selected the corpora for czech and german.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3470">
<title id=" E12-1009.xml">the best of both worlds  a graph based completion model for transition based parsers </title>
<section> parsing experiments and discussion.  </section>
<citcontext>
<prevsection>
<prevsent>the third order features let the transition based parser reach higher scores than the graph based parser.
</prevsent>
<prevsent>the third order features contribute for each language relatively small improvement 84 parser eng.
</prevsent>
</prevsection>
<citsent citstr=" W09-1205 ">
czech german (gesmundo et al 2009)?<papid> W09-1205 </papid></citsent>
<aftsection>
<nextsent>88.79/- 80.38 87.29 (bohnet, 2009) <papid> W09-1210 </papid>89.88/- 80.11 87.48 (baseline) 89.52/92.10 81.97/87.26 87.53/89.86 g2a (baseline) 90.14/92.36 81.13/87.65 87.79/90.12 t2a 90.20/92.55 83.01/88.12 88.22/90.36 t2ab 90.26/92.56 83.22/88.34 88.31/90.24 t2ab3a 90.20/90.51 83.21.88.30 88.14/90.23 t2ab3b 90.26/92.57 83.22/88.35 88.50/90.59 t2ab3abc 90.31/92.58 83.31/88.30 88.33/90.45 g2a+ 90.39/92.8 81.43/88.0 88.26/90.50 t2ab3ab+ 90.36/92.66 83.48/88.47 88.51/90.62 table 3: labeled attachment scores of parsers that use the datasets of the conll shared task 2009.</nextsent>
<nextsent>in line with previous work, punctuation is included.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3471">
<title id=" E12-1009.xml">the best of both worlds  a graph based completion model for transition based parsers </title>
<section> parsing experiments and discussion.  </section>
<citcontext>
<prevsection>
<prevsent>the third order features contribute for each language relatively small improvement 84 parser eng.
</prevsent>
<prevsent>czech german (gesmundo et al 2009)?<papid> W09-1205 </papid></prevsent>
</prevsection>
<citsent citstr=" W09-1210 ">
88.79/- 80.38 87.29 (bohnet, 2009) <papid> W09-1210 </papid>89.88/- 80.11 87.48 (baseline) 89.52/92.10 81.97/87.26 87.53/89.86 g2a (baseline) 90.14/92.36 81.13/87.65 87.79/90.12 t2a 90.20/92.55 83.01/88.12 88.22/90.36 t2ab 90.26/92.56 83.22/88.34 88.31/90.24 t2ab3a 90.20/90.51 83.21.88.30 88.14/90.23 t2ab3b 90.26/92.57 83.22/88.35 88.50/90.59 t2ab3abc 90.31/92.58 83.31/88.30 88.33/90.45 g2a+ 90.39/92.8 81.43/88.0 88.26/90.50 t2ab3ab+ 90.36/92.66 83.48/88.47 88.51/90.62 table 3: labeled attachment scores of parsers that use the datasets of the conll shared task 2009.</citsent>
<aftsection>
<nextsent>in line with previous work, punctuation is included.
</nextsent>
<nextsent>the parsers marked with ? used joint model for syntactic parsing and semantic role labelling.
</nextsent>
<nextsent>we provide more parsing results for the languages of conll-x shared task at http://code.google.com/p/mate-tools/.
</nextsent>
<nextsent>parser uas las (zhang and clark, 2008) <papid> D08-1059 </papid>84.3 (huang and sagae, 2010) <papid> P10-1110 </papid>85.2 (zhang and nivre, 2011) <papid> P11-2033 </papid>86.0 84.4 t2ab3abc+ 87.5 85.9table 4: chinese attachment scores for the conversion of ctb 5 with head rules of zhang and clark (2008).<papid> D08-1059 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3483">
<title id=" E12-1053.xml">learning how to conjugate the romanian verb rules for regular and partially irregular verbs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this paper we extend our work described in (dinu et al 2011) by adding more conjugational rules to the labelling system introduced there, in an attempt to capture the entire dataset of romanian verbs extracted from (barbu, 2007), and we employ machine learning techniques to pre dicta verbs correct label (which says what con jugational pattern it follows) when only the infinitive form is given.
</prevsent>
<prevsent>using only restricted group of verbs, in (dinuet al 2011) we validated the hypothesis that patterns can be identified in the conjugation of the romanian (partially irregular) verb and that these patterns can be learnt automatically so that, given the infinitive of verb, its correct conjugation for the indicative present tense can be produced.
</prevsent>
</prevsection>
<citsent citstr=" L08-1437 ">
in this paper, we extend our investigation to the whole dataset described in (barbu, 2008) <papid> L08-1437 </papid>and attempt to capture, beside the general ending patterns during conjugation, as much of the phonological alternations occuring in the stem of verbs (apophony) from the dataset as we can.traditionally, romanian has received latin inspired classification of verbs into 4 (or sometimes 5) conjugational classes based on the ending of their infiniti val form alone (costanzo, 2011).</citsent>
<aftsection>
<nextsent>however, this infinitive-based classification has proved itself inadequate due to its inability to account for the behavior of partially irregular verbs(whose stems have smaller number of allo morphs than the completely irregular) during their conjugation.
</nextsent>
<nextsent>there have been, thus, numerous attempts throughout the history of romanian linguistics to give other conjugational classifications basedon the way the verb actually conjugates.
</nextsent>
<nextsent>lombard (1955), looking at corpus of 667 verbs, combined the traditional 4 classes with the way in which the biggest two subgroups conjugate (oneusing the suffix ez?, the other esc?)
</nextsent>
<nextsent>and arrived at 6 classes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3484">
<title id=" E12-1068.xml">modeling inflection and word formation in smt </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we first present our system for dealing withthe difficult problem of inflection in german, including the inflection-dependent phenomenon ofportmanteaus.
</prevsent>
<prevsent>later, after performing an extensive analysis of this system, we will extend it 664to model compounds, highly productive phenomenon in german (see section 8).
</prevsent>
</prevsection>
<citsent citstr=" C04-1024 ">
the key linguistic knowledge sources that we use are morphological analysis and generation of german based on smor, morphological ana lyzer/generator of german (schmid et al 2004) and the bitpar parser, which is state-of-the-art parser of german (schmid, 2004).<papid> C04-1024 </papid></citsent>
<aftsection>
<nextsent>in order to ensure coherent german nps, we model linguistic features of each word in an np.
</nextsent>
<nextsent>we model case, gender, and number agreement and whether or not the word is in the scope of determiner (such as definite article), which we label in-weak-context (this linguistic feature is necessary to determine the type of inflection of adjectives and other words: strong, weak, mixed).
</nextsent>
<nextsent>this is diverse group of features.
</nextsent>
<nextsent>the number of german noun can often be determined given only the english source word.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3485">
<title id=" E12-1068.xml">modeling inflection and word formation in smt </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>verbs are represented using their inflected surface form.
</prevsent>
<prevsent>having access to inflected verb forms has positive influence on case prediction in the second 2we use an additional target factor to obtain the coarse pos for each stem, applying 7-gram pos model.
</prevsent>
</prevsection>
<citsent citstr=" D07-1091 ">
koehn and hoang (2007) <papid> D07-1091 </papid>showed that the use of pos factor only results in negligible bleu improvements, but we need access to the pos in our inflection prediction models.</citsent>
<aftsection>
<nextsent>665 input decoder output inflected merged in in appr  dat  in im die +art  def  dem contrast gegensatz +nn  masc  sg gegensatz gegensatz to zu appr  dat  zu zur the die +art  def  der animated lebhaft +adj  pos  lebhaften lebhaften debate debatte +nn  fem  sg  debatte debatte table 1: re-merging of prepositions and articles after inflection to form portmanteaus, in dem means in the.
</nextsent>
<nextsent>step through subject-verb agreement.
</nextsent>
<nextsent>articles are reduced to their stems (the stem itself makes clear the definite or indefinite distinction, but lemmatizing involves removing markings of case, gender and number features).
</nextsent>
<nextsent>other words are also represented by their stems (except for words not covered by smor, where surface forms are used instead).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3487">
<title id=" E12-1068.xml">modeling inflection and word formation in smt </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>ture).
</prevsent>
<prevsent>the sequence models already presented are limited to the n-gram feature space, and those that predict linguistic features are not strongly lexicalized.
</prevsent>
</prevsection>
<citsent citstr=" P08-1059 ">
toutanova et al(2008) <papid> P08-1059 </papid>uses an memm which allows the integration of wide variety of feature functions.</citsent>
<aftsection>
<nextsent>we also wanted to experiment with additional feature functions, and so we train 4 separate linear chain crf6 models on our data(one for each linguistic feature we want to pre dict).
</nextsent>
<nextsent>we chose crfs over memms to avoid the label bias problem (lafferty et al 2001).
</nextsent>
<nextsent>the crf feature functions, for each german word wi, are in table 3.
</nextsent>
<nextsent>the common feature functions are used in all models, while each of the 4 separate models (one for each linguistic feature) includes the context of only that linguistic feature.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3488">
<title id=" E12-1068.xml">modeling inflection and word formation in smt </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the common feature functions are used in all models, while each of the 4 separate models (one for each linguistic feature) includes the context of only that linguistic feature.
</prevsent>
<prevsent>we use l1 regularization to eliminate irrelevant feature functions, the regularization parameter is optimized on held out data.
</prevsent>
</prevsection>
<citsent citstr=" P10-1052 ">
6we use the wapiti toolkit (lavergne et al 2010) <papid> P10-1052 </papid>on 4 12-core opteron 6176 2.3 ghz with 256gb ram to train our crf models.</citsent>
<aftsection>
<nextsent>training single crf model on our data was not tractable, so we use one for each linguistic feature.
</nextsent>
<nextsent>667 common lemmawi5...wi+5 , tagwi7...wi+7 case casewi5...wi+5 gender genderwi5...wi+5 number numberwi5...wi+5 in-weak-context in-weak-contextwi5...wi+5table 3: feature functions used in crf models (fea ture functions are binary indicators of the pattern).
</nextsent>
<nextsent>5 experimental setup.
</nextsent>
<nextsent>to evaluate our end-to-end system, we perform the well-studied task of news translation, using the moses smt package.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3489">
<title id=" E12-1068.xml">modeling inflection and word formation in smt </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>then we merge them to create stemsin the same representation as before and we perform inflection and portmanteau merging exactly as previously discussed.
</prevsent>
<prevsent>8.1 details of splitting process.
</prevsent>
</prevsection>
<citsent citstr=" W10-1734 ">
we prepare the training data by splitting compounds in two steps, following the technique of fritzinger and fraser (2010).<papid> W10-1734 </papid></citsent>
<aftsection>
<nextsent>first, possible split points are extracted using smor, and second, the best split points are selected using the geometric mean of word part frequencies.
</nextsent>
<nextsent>compound word parts gloss inflations rate inflation rate inflation rate auszubrechen aus zu brechen out to break (to break out) training data is then stemmed as described in section 2.3.
</nextsent>
<nextsent>the formerly modifying words of the compound (in our example the words to the left of the rightmost word) do not have stem markup assigned, except for two cases: i) they are nouns themselves or ii) they are particles separated froma verb.
</nextsent>
<nextsent>in these cases, former modifiers are represented identically to their individual occurring counterparts, which helps generalization.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3490">
<title id=" E12-1068.xml">modeling inflection and word formation in smt </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>after translation, compound parts have to be re synthesized into compounds before inflection.
</prevsent>
<prevsent>two decisions have to be taken: i) where to 670 merge and ii) how to merge.
</prevsent>
</prevsection>
<citsent citstr=" W11-2129 ">
following the work of stymne and cancedda (2011), <papid> W11-2129 </papid>we implement linear-chain crf merging system using the following features: stemmed (separated) surface form, part-of-speech14 and frequencies from the training corpus for bigrams/merging of word andword+1, word as true prefix, word+1 as true suffix, plus frequency comparisons of these.</citsent>
<aftsection>
<nextsent>the crf is trained on the split monolingual data.
</nextsent>
<nextsent>it only proposes merging decisions, merging itself uses list extracted from the monolingual data (popovic et al 2006).
</nextsent>
<nextsent>8.3 experiments.
</nextsent>
<nextsent>we evaluated the end-to-end inflection system with the addition of compounds.15 as in the inflection experiments described in section 5, we use 5-gram surface lm and 7-gram pos lm, but for this experiment, they are trained on stemmed, split data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3494">
<title id=" E12-1068.xml">modeling inflection and word formation in smt </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>koehn and hoang (2007) <papid> D07-1091 </papid>introduced factored smt.</prevsent>
<prevsent>we use more complex context features.</prevsent>
</prevsection>
<citsent citstr=" W09-0420 ">
fraser (2009) <papid> W09-0420 </papid>tried to solve the inflection prediction problem by simply building an smt system for translating from stems to inflected forms.</citsent>
<aftsection>
<nextsent>bojar and kos (2010) improved on this by marking prepositions with the case they mark (one of the most important markups in our system).
</nextsent>
<nextsent>both efforts were ineffective on large data sets.
</nextsent>
<nextsent>williams and koehn (2011) <papid> W11-2126 </papid>used unification in an smt system to model some of the 671 agreement phenomena that we model.</nextsent>
<nextsent>our crf framework allows us to use more complex context features.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3495">
<title id=" E12-1068.xml">modeling inflection and word formation in smt </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>bojar and kos (2010) improved on this by marking prepositions with the case they mark (one of the most important markups in our system).
</prevsent>
<prevsent>both efforts were ineffective on large data sets.
</prevsent>
</prevsection>
<citsent citstr=" W11-2126 ">
williams and koehn (2011) <papid> W11-2126 </papid>used unification in an smt system to model some of the 671 agreement phenomena that we model.</citsent>
<aftsection>
<nextsent>our crf framework allows us to use more complex context features.
</nextsent>
<nextsent>we have directly addressed the question as to whether inflection should be predicted using surface forms as the target of the prediction, or whether linguistic features should be predicted, along with the use of subsequent generation step.
</nextsent>
<nextsent>the direct prediction of surface forms is limited to those forms observed in the training data, which is significant limitation.
</nextsent>
<nextsent>how ever, it is reasonable to expect that the use of features (and morphological generation) could also be problematic as this requires the use ofmorphologically-aware syntactic parsers to annotate the training data with such features, and additionally depends on the coverage of morphological analysis and generation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3496">
<title id=" E12-1068.xml">modeling inflection and word formation in smt </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>despite this, our research clearly shows that the feature-based approach is superior for english-to-german smt.this is striking result considering state-of-theart performance of german parsing is poor compared with the best performance on english parsing.
</prevsent>
<prevsent>as parsing performance improves, the performance of linguistic-feature-based approaches will increase.
</prevsent>
</prevsection>
<citsent citstr=" P08-2039 ">
virpioja et al(2007), badr et al(2008), <papid> P08-2039 </papid>luonget al(2010), <papid> D10-1015 </papid>clifton and sarkar (2011), <papid> P11-1004 </papid>and others are primarily concerned with using morpheme segmentation in smt, which is useful approach for dealing with issues of word-formation.</citsent>
<aftsection>
<nextsent>however, this does not deal directly with linguistic features marked by inflection.
</nextsent>
<nextsent>in german these linguistic features are marked very irregularly and there is widespread syncretism, making it difficult to split off morphemes specifying these features.so it is questionable as to whether morpheme segmentation techniques are sufficient to solve the inflectional problem we are addressing.much previous work looks at the impact of using source side information (i.e., feature functions on the aligned english), such as those of avramidis and koehn (2008), <papid> P08-1087 </papid>yeniterzi and oflazer (2010) <papid> P10-1047 </papid>and others.</nextsent>
<nextsent>toutanova et. al.s work showed that it is most important to model target side coherence and our stem markup also allows us to access source side information.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3497">
<title id=" E12-1068.xml">modeling inflection and word formation in smt </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>despite this, our research clearly shows that the feature-based approach is superior for english-to-german smt.this is striking result considering state-of-theart performance of german parsing is poor compared with the best performance on english parsing.
</prevsent>
<prevsent>as parsing performance improves, the performance of linguistic-feature-based approaches will increase.
</prevsent>
</prevsection>
<citsent citstr=" D10-1015 ">
virpioja et al(2007), badr et al(2008), <papid> P08-2039 </papid>luonget al(2010), <papid> D10-1015 </papid>clifton and sarkar (2011), <papid> P11-1004 </papid>and others are primarily concerned with using morpheme segmentation in smt, which is useful approach for dealing with issues of word-formation.</citsent>
<aftsection>
<nextsent>however, this does not deal directly with linguistic features marked by inflection.
</nextsent>
<nextsent>in german these linguistic features are marked very irregularly and there is widespread syncretism, making it difficult to split off morphemes specifying these features.so it is questionable as to whether morpheme segmentation techniques are sufficient to solve the inflectional problem we are addressing.much previous work looks at the impact of using source side information (i.e., feature functions on the aligned english), such as those of avramidis and koehn (2008), <papid> P08-1087 </papid>yeniterzi and oflazer (2010) <papid> P10-1047 </papid>and others.</nextsent>
<nextsent>toutanova et. al.s work showed that it is most important to model target side coherence and our stem markup also allows us to access source side information.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3498">
<title id=" E12-1068.xml">modeling inflection and word formation in smt </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>despite this, our research clearly shows that the feature-based approach is superior for english-to-german smt.this is striking result considering state-of-theart performance of german parsing is poor compared with the best performance on english parsing.
</prevsent>
<prevsent>as parsing performance improves, the performance of linguistic-feature-based approaches will increase.
</prevsent>
</prevsection>
<citsent citstr=" P11-1004 ">
virpioja et al(2007), badr et al(2008), <papid> P08-2039 </papid>luonget al(2010), <papid> D10-1015 </papid>clifton and sarkar (2011), <papid> P11-1004 </papid>and others are primarily concerned with using morpheme segmentation in smt, which is useful approach for dealing with issues of word-formation.</citsent>
<aftsection>
<nextsent>however, this does not deal directly with linguistic features marked by inflection.
</nextsent>
<nextsent>in german these linguistic features are marked very irregularly and there is widespread syncretism, making it difficult to split off morphemes specifying these features.so it is questionable as to whether morpheme segmentation techniques are sufficient to solve the inflectional problem we are addressing.much previous work looks at the impact of using source side information (i.e., feature functions on the aligned english), such as those of avramidis and koehn (2008), <papid> P08-1087 </papid>yeniterzi and oflazer (2010) <papid> P10-1047 </papid>and others.</nextsent>
<nextsent>toutanova et. al.s work showed that it is most important to model target side coherence and our stem markup also allows us to access source side information.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3499">
<title id=" E12-1068.xml">modeling inflection and word formation in smt </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>virpioja et al(2007), badr et al(2008), <papid> P08-2039 </papid>luonget al(2010), <papid> D10-1015 </papid>clifton and sarkar (2011), <papid> P11-1004 </papid>and others are primarily concerned with using morpheme segmentation in smt, which is useful approach for dealing with issues of word-formation.</prevsent>
<prevsent>however, this does not deal directly with linguistic features marked by inflection.</prevsent>
</prevsection>
<citsent citstr=" P08-1087 ">
in german these linguistic features are marked very irregularly and there is widespread syncretism, making it difficult to split off morphemes specifying these features.so it is questionable as to whether morpheme segmentation techniques are sufficient to solve the inflectional problem we are addressing.much previous work looks at the impact of using source side information (i.e., feature functions on the aligned english), such as those of avramidis and koehn (2008), <papid> P08-1087 </papid>yeniterzi and oflazer (2010) <papid> P10-1047 </papid>and others.</citsent>
<aftsection>
<nextsent>toutanova et. al.s work showed that it is most important to model target side coherence and our stem markup also allows us to access source side information.
</nextsent>
<nextsent>using additional source side information beyond the markup did not produce gain in performance.
</nextsent>
<nextsent>for compound splitting, we follow fritzinger and fraser (2010), <papid> W10-1734 </papid>using linguistic knowledge encoded in rule-based morphological analyser andthen selecting the best analysis based on the geometric mean of word part frequencies.</nextsent>
<nextsent>other approaches use less deep linguistic resources (e.g., pos-tags stymne (2008)) or are (almost) knowledge-free (e.g., koehn and knight (2003)).<papid> E03-1076 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3500">
<title id=" E12-1068.xml">modeling inflection and word formation in smt </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>virpioja et al(2007), badr et al(2008), <papid> P08-2039 </papid>luonget al(2010), <papid> D10-1015 </papid>clifton and sarkar (2011), <papid> P11-1004 </papid>and others are primarily concerned with using morpheme segmentation in smt, which is useful approach for dealing with issues of word-formation.</prevsent>
<prevsent>however, this does not deal directly with linguistic features marked by inflection.</prevsent>
</prevsection>
<citsent citstr=" P10-1047 ">
in german these linguistic features are marked very irregularly and there is widespread syncretism, making it difficult to split off morphemes specifying these features.so it is questionable as to whether morpheme segmentation techniques are sufficient to solve the inflectional problem we are addressing.much previous work looks at the impact of using source side information (i.e., feature functions on the aligned english), such as those of avramidis and koehn (2008), <papid> P08-1087 </papid>yeniterzi and oflazer (2010) <papid> P10-1047 </papid>and others.</citsent>
<aftsection>
<nextsent>toutanova et. al.s work showed that it is most important to model target side coherence and our stem markup also allows us to access source side information.
</nextsent>
<nextsent>using additional source side information beyond the markup did not produce gain in performance.
</nextsent>
<nextsent>for compound splitting, we follow fritzinger and fraser (2010), <papid> W10-1734 </papid>using linguistic knowledge encoded in rule-based morphological analyser andthen selecting the best analysis based on the geometric mean of word part frequencies.</nextsent>
<nextsent>other approaches use less deep linguistic resources (e.g., pos-tags stymne (2008)) or are (almost) knowledge-free (e.g., koehn and knight (2003)).<papid> E03-1076 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3502">
<title id=" E12-1068.xml">modeling inflection and word formation in smt </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>using additional source side information beyond the markup did not produce gain in performance.
</prevsent>
<prevsent>for compound splitting, we follow fritzinger and fraser (2010), <papid> W10-1734 </papid>using linguistic knowledge encoded in rule-based morphological analyser andthen selecting the best analysis based on the geometric mean of word part frequencies.</prevsent>
</prevsection>
<citsent citstr=" E03-1076 ">
other approaches use less deep linguistic resources (e.g., pos-tags stymne (2008)) or are (almost) knowledge-free (e.g., koehn and knight (2003)).<papid> E03-1076 </papid></citsent>
<aftsection>
<nextsent>compound merging is less well studied.
</nextsent>
<nextsent>popovic et al(2006) used simple, list-based merging approach, merging all consecutive words included in merging list.
</nextsent>
<nextsent>this approach resulted in toomany compounds.
</nextsent>
<nextsent>we follow stymne and cancedda (2011), <papid> W11-2129 </papid>for compound merging.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3504">
<title id=" E12-1002.xml">power law distributions for paraphrases extracted from bilingual corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the resulting phrase-paraphrase probabilities are built upon the conversion of the commute times into artificial cooccurrence counts with novel technique.the co-occurrence count distribution belongs to the power-law family.
</prevsent>
<prevsent>paraphrase extraction has emerged as an important problem in nlp.
</prevsent>
</prevsection>
<citsent citstr=" J10-3003 ">
currently, there exists an abundance of methods for extracting paraphrases from monolingual, comparable and bilingual corpora (madnani and dorr, 2010; <papid> J10-3003 </papid>androutsopoulos and malakasiotis, 2010); we focus on the latter and specifically on the phrase-table that is extracted from bitext during the training stage of statistical machine translation (smt).</citsent>
<aftsection>
<nextsent>bannard and callison-burch (2005) introduced the pivoting approach, which relies on 2-step transition from phrase, via its translations, to paraphrasecandidate.
</nextsent>
<nextsent>by incorporating the syntactic structure of phrases (callison-burch, 2005), the quality of the paraphrases extracted with pivoting canbe improved.
</nextsent>
<nextsent>kok and brockett (2010) (<papid> N10-1017 </papid>henceforth kb) used random walk framework to determine the similarity between phrases, which was shown to outperform pivoting with syntactic information, when multiple phrase-tables areused.</nextsent>
<nextsent>in smt, extracted paraphrases with associated pivot-based (callison-burch et al  2006; onishi et al  2010) <papid> P10-2001 </papid>and cluster-based (kuhn etal., 2010) <papid> C10-1069 </papid>probabilities have been found to im prove the quality of translation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3505">
<title id=" E12-1002.xml">power law distributions for paraphrases extracted from bilingual corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>bannard and callison-burch (2005) introduced the pivoting approach, which relies on 2-step transition from phrase, via its translations, to paraphrasecandidate.
</prevsent>
<prevsent>by incorporating the syntactic structure of phrases (callison-burch, 2005), the quality of the paraphrases extracted with pivoting canbe improved.
</prevsent>
</prevsection>
<citsent citstr=" N10-1017 ">
kok and brockett (2010) (<papid> N10-1017 </papid>henceforth kb) used random walk framework to determine the similarity between phrases, which was shown to outperform pivoting with syntactic information, when multiple phrase-tables areused.</citsent>
<aftsection>
<nextsent>in smt, extracted paraphrases with associated pivot-based (callison-burch et al  2006; onishi et al  2010) <papid> P10-2001 </papid>and cluster-based (kuhn etal., 2010) <papid> C10-1069 </papid>probabilities have been found to im prove the quality of translation.</nextsent>
<nextsent>pivoting has also been employed in the extraction of syntactic paraphrases, which are mixture of phrases and nonterminals (zhao et al  2008; <papid> P08-1089 </papid>ganitkevitch et al  2011).<papid> D11-1108 </papid>we develop method for extracting paraphrases from bitext for both the source and target languages.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3506">
<title id=" E12-1002.xml">power law distributions for paraphrases extracted from bilingual corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>by incorporating the syntactic structure of phrases (callison-burch, 2005), the quality of the paraphrases extracted with pivoting canbe improved.
</prevsent>
<prevsent>kok and brockett (2010) (<papid> N10-1017 </papid>henceforth kb) used random walk framework to determine the similarity between phrases, which was shown to outperform pivoting with syntactic information, when multiple phrase-tables areused.</prevsent>
</prevsection>
<citsent citstr=" P10-2001 ">
in smt, extracted paraphrases with associated pivot-based (callison-burch et al  2006; onishi et al  2010) <papid> P10-2001 </papid>and cluster-based (kuhn etal., 2010) <papid> C10-1069 </papid>probabilities have been found to im prove the quality of translation.</citsent>
<aftsection>
<nextsent>pivoting has also been employed in the extraction of syntactic paraphrases, which are mixture of phrases and nonterminals (zhao et al  2008; <papid> P08-1089 </papid>ganitkevitch et al  2011).<papid> D11-1108 </papid>we develop method for extracting paraphrases from bitext for both the source and target languages.</nextsent>
<nextsent>emphasis is placed on the quality of the phrase-paraphrase probabilities as well as on providing stepping stone for extracting syntactic paraphrases with equally reliable prob abilities.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3507">
<title id=" E12-1002.xml">power law distributions for paraphrases extracted from bilingual corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>by incorporating the syntactic structure of phrases (callison-burch, 2005), the quality of the paraphrases extracted with pivoting canbe improved.
</prevsent>
<prevsent>kok and brockett (2010) (<papid> N10-1017 </papid>henceforth kb) used random walk framework to determine the similarity between phrases, which was shown to outperform pivoting with syntactic information, when multiple phrase-tables areused.</prevsent>
</prevsection>
<citsent citstr=" C10-1069 ">
in smt, extracted paraphrases with associated pivot-based (callison-burch et al  2006; onishi et al  2010) <papid> P10-2001 </papid>and cluster-based (kuhn etal., 2010) <papid> C10-1069 </papid>probabilities have been found to im prove the quality of translation.</citsent>
<aftsection>
<nextsent>pivoting has also been employed in the extraction of syntactic paraphrases, which are mixture of phrases and nonterminals (zhao et al  2008; <papid> P08-1089 </papid>ganitkevitch et al  2011).<papid> D11-1108 </papid>we develop method for extracting paraphrases from bitext for both the source and target languages.</nextsent>
<nextsent>emphasis is placed on the quality of the phrase-paraphrase probabilities as well as on providing stepping stone for extracting syntactic paraphrases with equally reliable prob abilities.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3508">
<title id=" E12-1002.xml">power law distributions for paraphrases extracted from bilingual corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>kok and brockett (2010) (<papid> N10-1017 </papid>henceforth kb) used random walk framework to determine the similarity between phrases, which was shown to outperform pivoting with syntactic information, when multiple phrase-tables areused.</prevsent>
<prevsent>in smt, extracted paraphrases with associated pivot-based (callison-burch et al  2006; onishi et al  2010) <papid> P10-2001 </papid>and cluster-based (kuhn etal., 2010) <papid> C10-1069 </papid>probabilities have been found to im prove the quality of translation.</prevsent>
</prevsection>
<citsent citstr=" P08-1089 ">
pivoting has also been employed in the extraction of syntactic paraphrases, which are mixture of phrases and nonterminals (zhao et al  2008; <papid> P08-1089 </papid>ganitkevitch et al  2011).<papid> D11-1108 </papid>we develop method for extracting paraphrases from bitext for both the source and target languages.</citsent>
<aftsection>
<nextsent>emphasis is placed on the quality of the phrase-paraphrase probabilities as well as on providing stepping stone for extracting syntactic paraphrases with equally reliable probabilities.
</nextsent>
<nextsent>in line with previous work, our method depends on the connectivity of the phrase-table,but the resulting construction treats each side separately, which can potentially be benefited from additional monolingual data.
</nextsent>
<nextsent>the initial problem in harvesting paraphrases from phrase-table is the identification of the search space.
</nextsent>
<nextsent>previous work has relied on breadth first search from the query phrase with depth of 2 (pivoting) and 6 (kb).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3509">
<title id=" E12-1002.xml">power law distributions for paraphrases extracted from bilingual corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>kok and brockett (2010) (<papid> N10-1017 </papid>henceforth kb) used random walk framework to determine the similarity between phrases, which was shown to outperform pivoting with syntactic information, when multiple phrase-tables areused.</prevsent>
<prevsent>in smt, extracted paraphrases with associated pivot-based (callison-burch et al  2006; onishi et al  2010) <papid> P10-2001 </papid>and cluster-based (kuhn etal., 2010) <papid> C10-1069 </papid>probabilities have been found to im prove the quality of translation.</prevsent>
</prevsection>
<citsent citstr=" D11-1108 ">
pivoting has also been employed in the extraction of syntactic paraphrases, which are mixture of phrases and nonterminals (zhao et al  2008; <papid> P08-1089 </papid>ganitkevitch et al  2011).<papid> D11-1108 </papid>we develop method for extracting paraphrases from bitext for both the source and target languages.</citsent>
<aftsection>
<nextsent>emphasis is placed on the quality of the phrase-paraphrase probabilities as well as on providing stepping stone for extracting syntactic paraphrases with equally reliable probabilities.
</nextsent>
<nextsent>in line with previous work, our method depends on the connectivity of the phrase-table,but the resulting construction treats each side separately, which can potentially be benefited from additional monolingual data.
</nextsent>
<nextsent>the initial problem in harvesting paraphrases from phrase-table is the identification of the search space.
</nextsent>
<nextsent>previous work has relied on breadth first search from the query phrase with depth of 2 (pivoting) and 6 (kb).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3510">
<title id=" E12-1002.xml">power law distributions for paraphrases extracted from bilingual corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the latter allow (a) redistribution of the mass for phrases with no appropriate paraphrases and (b) the extraction of syntactic paraphrases.
</prevsent>
<prevsent>the proximity among vertices of graph is measured by means of random walk distance measure, the commute time (aldous and fill, 2001).
</prevsent>
</prevsection>
<citsent citstr=" W08-2006 ">
this measure is known to perform well in identifying similar words on the graph of wordnet (rao et al  2008) <papid> W08-2006 </papid>and related measure,the hitting time is known to perform well in harvesting paraphrases on graph constructed from multiple phrase-tables (kb).</citsent>
<aftsection>
<nextsent>generally in nlp, power-law distributions are typically encountered in the collection of counts during the training stage.
</nextsent>
<nextsent>the distances of section 3.1 are converted into artificial co-occurrencecounts with novel technique (section 3.2).
</nextsent>
<nextsent>although they need not be inte gers, the main challenge is the type of the underlying distributions;it should ideally emulate the resulting count distributions from the phrase extraction stage of monolingual parallel corpus (dolan et al  2004).<papid> C04-1051 </papid></nextsent>
<nextsent>these counts give rise to the desired probability distributions by means of relative frequencies.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3511">
<title id=" E12-1002.xml">power law distributions for paraphrases extracted from bilingual corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>generally in nlp, power-law distributions are typically encountered in the collection of counts during the training stage.
</prevsent>
<prevsent>the distances of section 3.1 are converted into artificial co-occurrencecounts with novel technique (section 3.2).
</prevsent>
</prevsection>
<citsent citstr=" C04-1051 ">
although they need not be inte gers, the main challenge is the type of the underlying distributions;it should ideally emulate the resulting count distributions from the phrase extraction stage of monolingual parallel corpus (dolan et al  2004).<papid> C04-1051 </papid></citsent>
<aftsection>
<nextsent>these counts give rise to the desired probability distributions by means of relative frequencies.
</nextsent>
<nextsent>2.1 extracting connected components.
</nextsent>
<nextsent>for the decomposition of the phrase-table into sub-phrase-tables it is convenient to view the phrase-table as an undirected, unweighted graph with the vertex set being the source and target phrases and the edge set being the phrase-table entries.
</nextsent>
<nextsent>for the rest of this section, we do not distinguish between source and target phrases, i.e. both types are treated equally as vertices of . when referring to the size of graph, we mean the number of vertices it contains.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3513">
<title id=" E12-1002.xml">power law distributions for paraphrases extracted from bilingual corpora </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>the baseline is given by pivoting (ban nard and callison-burch, 2005), (s?|s) = ? p(t|s)p(s?|t), (22)where p(t|s) and p(s?|t) are the phrase-based relative frequencies of the translation model.
</prevsent>
<prevsent>we select 150 phrases (an equal number for unigrams, bigrams and trigrams), for which we expect to see paraphrases, and keep the top-10 paraphrases for each phrase, ranked by the above measures.
</prevsent>
</prevsection>
<citsent citstr=" P11-2096 ">
we follow (kok and brockett, 2010;<papid> N10-1017 </papid>metzler et al  2011) <papid> P11-2096 </papid>in the evaluation of the extracted paraphrases: each phrase-paraphrase pair is manually annotated with the following options: 0) different meaning; 1) (i) same meaning, but potential replacement of the phrase with the paraphrase in sentence ruins the grammatical structure of the sentence.</citsent>
<aftsection>
<nextsent>(ii) tokens of the paraphrase are morphological inflections of the phrases tokens.
</nextsent>
<nextsent>2) samemeaning.
</nextsent>
<nextsent>although useful for smt purposes, super/substrings of?
</nextsent>
<nextsent>are annotated with 0 to achieve an objective evaluation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3514">
<title id=" E06-2024.xml">a suite of shallow processing tools for portuguese lxsuite </title>
<section> pos tagger.  </section>
<citcontext>
<prevsection>
<prevsent>a second pass with the to kenizer then looks for occurrences of tokens with composite tag and splits them: deste/v?|deste/v| deste/p+dem?|de+/p|este/dem|this approach allowed us to successfully resolve 99.4% of the ambiguous strings.
</prevsent>
<prevsent>this is amuch better value than the baseline 78.20% obtained by always considering that the ambiguous strings are contraction.5
</prevsent>
</prevsection>
<citsent citstr=" A00-1031 ">
for the pos tagging task we used brants tnt tagger (brants, 2000), <papid> A00-1031 </papid>very efficient statistical tagger based on hidden markov models.</citsent>
<aftsection>
<nextsent>for training, we used 90% of 280, 000 token corpus, accurately hand-tagged with tagset of ca.
</nextsent>
<nextsent>60 tags, with inflectional feature values left aside.
</nextsent>
<nextsent>evaluation showed an accuracy of 96.87% for this tool, obtained by averaging 10 test runs over different 10% contiguous portions of the corpus that were not used for training.
</nextsent>
<nextsent>the pos tagger we developed is currently the fastest tagger for the portuguese language, and itis in line with state-of-the-art taggers for other languages, as discussed in (branco and silva, 2004).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3515">
<title id=" E09-1034.xml">parsing mildly non projective dependency structures </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for reasons of efficiency, many practical implementations of dependency parsing are restricted to projective structures, in which the subtree rooted at each word covers contiguous substring of the sentence.
</prevsent>
<prevsent>however, while free word order languages such as czech do not satisfy this constraint, parsing without the projectivity constraint is computationally complex.
</prevsent>
</prevsection>
<citsent citstr=" H05-1066 ">
although it is possible to parse non-projective structures in quadratic time under amodel in which each dependency decision is independent of all the others (mcdonald et al, 2005),<papid> H05-1066 </papid>partially supported by mec and feder (hum2007 66607-c04) and xunta de galicia (pgidit07sin005206pr,incite08e1r104022es, incite08ena305025es, in cite08pxib302179pr, rede galega de proc.</citsent>
<aftsection>
<nextsent>da linguaxe ri, bolsas para estadas incite ? fse cofinanced).the problem is intractable in the absence of this assumption (mcdonald and satta, 2007).<papid> W07-2216 </papid></nextsent>
<nextsent>nivre and nilsson (2005) <papid> P05-1013 </papid>observe that most non-projective dependency structures appearing in practice are close?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3516">
<title id=" E09-1034.xml">parsing mildly non projective dependency structures </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, while free word order languages such as czech do not satisfy this constraint, parsing without the projectivity constraint is computationally complex.
</prevsent>
<prevsent>although it is possible to parse non-projective structures in quadratic time under amodel in which each dependency decision is independent of all the others (mcdonald et al, 2005),<papid> H05-1066 </papid>partially supported by mec and feder (hum2007 66607-c04) and xunta de galicia (pgidit07sin005206pr,incite08e1r104022es, incite08ena305025es, in cite08pxib302179pr, rede galega de proc.</prevsent>
</prevsection>
<citsent citstr=" W07-2216 ">
da linguaxe ri, bolsas para estadas incite ? fse cofinanced).the problem is intractable in the absence of this assumption (mcdonald and satta, 2007).<papid> W07-2216 </papid></citsent>
<aftsection>
<nextsent>nivre and nilsson (2005) <papid> P05-1013 </papid>observe that most non-projective dependency structures appearing in practice are close?</nextsent>
<nextsent>to being projective, since they contain only small proportion of non projective arcs.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3517">
<title id=" E09-1034.xml">parsing mildly non projective dependency structures </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>although it is possible to parse non-projective structures in quadratic time under amodel in which each dependency decision is independent of all the others (mcdonald et al, 2005),<papid> H05-1066 </papid>partially supported by mec and feder (hum2007 66607-c04) and xunta de galicia (pgidit07sin005206pr,incite08e1r104022es, incite08ena305025es, in cite08pxib302179pr, rede galega de proc.</prevsent>
<prevsent>da linguaxe ri, bolsas para estadas incite ? fse cofinanced).the problem is intractable in the absence of this assumption (mcdonald and satta, 2007).<papid> W07-2216 </papid></prevsent>
</prevsection>
<citsent citstr=" P05-1013 ">
nivre and nilsson (2005) <papid> P05-1013 </papid>observe that most non-projective dependency structures appearing in practice are close?</citsent>
<aftsection>
<nextsent>to being projective, since they contain only small proportion of non projective arcs.
</nextsent>
<nextsent>this has led to the study of classes of dependency structures that lie between projective and unrestricted non-projective structures (kuhlmann and nivre, 2006; <papid> P06-2066 </papid>havelka, 2007).<papid> P07-1077 </papid></nextsent>
<nextsent>kuhlmann (2007) investigates several such classes, based on well-nestedness and gap degree constraints (bodirsky et al, 2005), relating them to lexicalised constituency grammar formalisms.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3518">
<title id=" E09-1034.xml">parsing mildly non projective dependency structures </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>nivre and nilsson (2005) <papid> P05-1013 </papid>observe that most non-projective dependency structures appearing in practice are close?</prevsent>
<prevsent>to being projective, since they contain only small proportion of non projective arcs.</prevsent>
</prevsection>
<citsent citstr=" P06-2066 ">
this has led to the study of classes of dependency structures that lie between projective and unrestricted non-projective structures (kuhlmann and nivre, 2006; <papid> P06-2066 </papid>havelka, 2007).<papid> P07-1077 </papid></citsent>
<aftsection>
<nextsent>kuhlmann (2007) investigates several such classes, based on well-nestedness and gap degree constraints (bodirsky et al, 2005), relating them to lexicalised constituency grammar formalisms.
</nextsent>
<nextsent>specifically, he shows that: linear context-freerewriting systems (lcfrs) with fan-out (vijay shanker et al, 1987; satta, 1992) <papid> P92-1012 </papid>induce the set of dependency structures with gap degree at most ? 1; coupled context-free grammars in which the maximal rank of nonterminal is (hotz andpitsch, 1996) induce the set of well-nested dependency structures with gap degree at most ? 1; and ltags (joshi and schabes, 1997) induce the set of well-nested dependency structures with gap degree at most 1.</nextsent>
<nextsent>these results establish that there must be polynomial-time dependency parsing algorithms for well-nested structures with bounded gap degree, since such parsers exist for their corresponding lexicalised constituency-based formalisms.however, since most of the non-projective structures in treebanks are well-nested and have small gap degree (kuhlmann and nivre, 2006), <papid> P06-2066 </papid>developing efficient dependency parsing strategies for these sets of structures has considerable practical interest, since we would be able to parse directly with dependencies in data-driven manner, rather than indirectly by constructing intermediate constituency grammars and extracting dependencies from constituency parses.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3519">
<title id=" E09-1034.xml">parsing mildly non projective dependency structures </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>nivre and nilsson (2005) <papid> P05-1013 </papid>observe that most non-projective dependency structures appearing in practice are close?</prevsent>
<prevsent>to being projective, since they contain only small proportion of non projective arcs.</prevsent>
</prevsection>
<citsent citstr=" P07-1077 ">
this has led to the study of classes of dependency structures that lie between projective and unrestricted non-projective structures (kuhlmann and nivre, 2006; <papid> P06-2066 </papid>havelka, 2007).<papid> P07-1077 </papid></citsent>
<aftsection>
<nextsent>kuhlmann (2007) investigates several such classes, based on well-nestedness and gap degree constraints (bodirsky et al, 2005), relating them to lexicalised constituency grammar formalisms.
</nextsent>
<nextsent>specifically, he shows that: linear context-freerewriting systems (lcfrs) with fan-out (vijay shanker et al, 1987; satta, 1992) <papid> P92-1012 </papid>induce the set of dependency structures with gap degree at most ? 1; coupled context-free grammars in which the maximal rank of nonterminal is (hotz andpitsch, 1996) induce the set of well-nested dependency structures with gap degree at most ? 1; and ltags (joshi and schabes, 1997) induce the set of well-nested dependency structures with gap degree at most 1.</nextsent>
<nextsent>these results establish that there must be polynomial-time dependency parsing algorithms for well-nested structures with bounded gap degree, since such parsers exist for their corresponding lexicalised constituency-based formalisms.however, since most of the non-projective structures in treebanks are well-nested and have small gap degree (kuhlmann and nivre, 2006), <papid> P06-2066 </papid>developing efficient dependency parsing strategies for these sets of structures has considerable practical interest, since we would be able to parse directly with dependencies in data-driven manner, rather than indirectly by constructing intermediate constituency grammars and extracting dependencies from constituency parses.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3520">
<title id=" E09-1034.xml">parsing mildly non projective dependency structures </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this has led to the study of classes of dependency structures that lie between projective and unrestricted non-projective structures (kuhlmann and nivre, 2006; <papid> P06-2066 </papid>havelka, 2007).<papid> P07-1077 </papid></prevsent>
<prevsent>kuhlmann (2007) investigates several such classes, based on well-nestedness and gap degree constraints (bodirsky et al, 2005), relating them to lexicalised constituency grammar formalisms.</prevsent>
</prevsection>
<citsent citstr=" P92-1012 ">
specifically, he shows that: linear context-freerewriting systems (lcfrs) with fan-out (vijay shanker et al, 1987; satta, 1992) <papid> P92-1012 </papid>induce the set of dependency structures with gap degree at most ? 1; coupled context-free grammars in which the maximal rank of nonterminal is (hotz andpitsch, 1996) induce the set of well-nested dependency structures with gap degree at most ? 1; and ltags (joshi and schabes, 1997) induce the set of well-nested dependency structures with gap degree at most 1.</citsent>
<aftsection>
<nextsent>these results establish that there must be polynomial-time dependency parsing algorithms for well-nested structures with bounded gap degree, since such parsers exist for their corresponding lexicalised constituency-based formalisms.however, since most of the non-projective structures in treebanks are well-nested and have small gap degree (kuhlmann and nivre, 2006), <papid> P06-2066 </papid>developing efficient dependency parsing strategies for these sets of structures has considerable practical interest, since we would be able to parse directly with dependencies in data-driven manner, rather than indirectly by constructing intermediate constituency grammars and extracting dependencies from constituency parses.</nextsent>
<nextsent>we address this problem with the following contributions: (1) we define parsing algorithm 291for well-nested dependency structures of gap degree 1, and prove its correctness.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3526">
<title id=" E09-1034.xml">parsing mildly non projective dependency structures </title>
<section> preliminaries.  </section>
<citcontext>
<prevsection>
<prevsent>a schema is said to be complete if all correct final items are valid.
</prevsent>
<prevsent>a correct parsing schema is one which is both sound and complete.in constituency-based parsing schemata, deduction steps usually have grammar rules as side conditions.
</prevsent>
</prevsection>
<citsent citstr=" P99-1059 ">
in the case of dependency parsers it is also possible to use grammars (eisner and satta,1999), <papid> P99-1059 </papid>but many algorithms use data-driven approach instead, making individual decisions about which dependencies to create by using probabilistic models (eisner, 1996) <papid> C96-1058 </papid>or classifiers (yamadaand matsumoto, 2003).</citsent>
<aftsection>
<nextsent>to represent these algorithms as deduction systems, we use the notion of d-rules (covington, 1990).
</nextsent>
<nextsent>d-rules take the form ? b, which says that word can have aas dependent.
</nextsent>
<nextsent>deduction steps in non-grammar based parsers can be tied to the d-rules associated with the links they create.
</nextsent>
<nextsent>in this way, we obtain representation of the underlying logic of the parser while abstracting away from control structures (the particular model used to create the decisions associated with d-rules).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3527">
<title id=" E09-1034.xml">parsing mildly non projective dependency structures </title>
<section> preliminaries.  </section>
<citcontext>
<prevsection>
<prevsent>a schema is said to be complete if all correct final items are valid.
</prevsent>
<prevsent>a correct parsing schema is one which is both sound and complete.in constituency-based parsing schemata, deduction steps usually have grammar rules as side conditions.
</prevsent>
</prevsection>
<citsent citstr=" C96-1058 ">
in the case of dependency parsers it is also possible to use grammars (eisner and satta,1999), <papid> P99-1059 </papid>but many algorithms use data-driven approach instead, making individual decisions about which dependencies to create by using probabilistic models (eisner, 1996) <papid> C96-1058 </papid>or classifiers (yamadaand matsumoto, 2003).</citsent>
<aftsection>
<nextsent>to represent these algorithms as deduction systems, we use the notion of d-rules (covington, 1990).
</nextsent>
<nextsent>d-rules take the form ? b, which says that word can have aas dependent.
</nextsent>
<nextsent>deduction steps in non-grammar based parsers can be tied to the d-rules associated with the links they create.
</nextsent>
<nextsent>in this way, we obtain representation of the underlying logic of the parser while abstracting away from control structures (the particular model used to create the decisions associated with d-rules).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3528">
<title id=" E09-1034.xml">parsing mildly non projective dependency structures </title>
<section> the wg1 parser.  </section>
<citcontext>
<prevsection>
<prevsent>to prove that wg1 is correct, we need to prove its soundness and completeness.4 soundness is proven by checking that valid items always contain well-nested trees.
</prevsent>
<prevsent>completeness is proven by induction, taking initial items as the base case and showing that an item containing correct subtree for string can always be obtained from items corresponding to smaller subtrees.
</prevsent>
</prevsection>
<citsent citstr=" P07-1021 ">
in order to prove this induction step, we use the concept of order annotations (kuhlmann, 2007; kuhlmann and mohl, 2007), <papid> P07-1021 </papid>which are strings that lexicalisethe precedence relation between the nodes of dependency tree.</citsent>
<aftsection>
<nextsent>given correct subtree, we divide the proof into cases according to the order annotation of its head and we find that, for every possible form of this order annotation, we can find sequence of combine steps to infer the relevant item from smaller correct items.
</nextsent>
<nextsent>3.3 computational complexity.
</nextsent>
<nextsent>the time complexity of wg1 is o(n7), as the step combine shrinking gap centre works with 7 free string positions.
</nextsent>
<nextsent>this complexity with respect to the length of the input is as expected for this set of structures, since kuhlmann (2007) shows that they are equivalent to ltag, and the best existing parsers for this formalism also perform in o(n7) (eisner and satta, 2000).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3534">
<title id=" E12-1045.xml">cutting the long tail hybrid language models for translation style adaptation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>a possible reason for this is that lm adaptation methods were originally developed under the automatic speech recognition framework, which typically assumes the presence of one single lm.
</prevsent>
<prevsent>the progressive adoption of the log-linear modeling framework in many nlp tasks has recently introduced the useof multiple lm components (features), which permit to naturally factor out and integrate different aspects of language into one model.
</prevsent>
</prevsection>
<citsent citstr=" D07-1091 ">
in smt, the factored model (koehn and hoang, 2007), <papid> D07-1091 </papid>for instance, permits to better tailor the lm to the task syntax, by complementing word-based n-gramswith part-of-speech (pos) lm , that can be estimated even on limited amount of task-specific data.</citsent>
<aftsection>
<nextsent>besides many works addressing holistic lm domain adaptation for smt, e.g. foster and kuhn (2007), <papid> W07-0717 </papid>recently methods were also proposed to explicitly adapt the lm to the discourse topic of talk (ruiz and federico, 2011).<papid> W11-2133 </papid></nextsent>
<nextsent>our work makes another step in this direction by investigating hybrid lms that try to explicitly represent the speaking style of the talk genre.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3535">
<title id=" E12-1045.xml">cutting the long tail hybrid language models for translation style adaptation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the progressive adoption of the log-linear modeling framework in many nlp tasks has recently introduced the useof multiple lm components (features), which permit to naturally factor out and integrate different aspects of language into one model.
</prevsent>
<prevsent>in smt, the factored model (koehn and hoang, 2007), <papid> D07-1091 </papid>for instance, permits to better tailor the lm to the task syntax, by complementing word-based n-gramswith part-of-speech (pos) lm , that can be estimated even on limited amount of task-specific data.</prevsent>
</prevsection>
<citsent citstr=" W07-0717 ">
besides many works addressing holistic lm domain adaptation for smt, e.g. foster and kuhn (2007), <papid> W07-0717 </papid>recently methods were also proposed to explicitly adapt the lm to the discourse topic of talk (ruiz and federico, 2011).<papid> W11-2133 </papid></citsent>
<aftsection>
<nextsent>our work makes another step in this direction by investigating hybrid lms that try to explicitly represent the speaking style of the talk genre.
</nextsent>
<nextsent>as difference from standard class-based lms (brown et al 1992) <papid> J92-4003 </papid>or the more recent local lms (monz, 2011), <papid> D11-1080 </papid>which are used to predict sequences of classes or word class pairs, our hybrid lm is devised to predict sequences of classes interleaved by words.</nextsent>
<nextsent>while we do not claim any technical novelty inthe model itself, to our knowledge deep investigation of hybrid lms for the sake of style adaptation is definitely new.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3536">
<title id=" E12-1045.xml">cutting the long tail hybrid language models for translation style adaptation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the progressive adoption of the log-linear modeling framework in many nlp tasks has recently introduced the useof multiple lm components (features), which permit to naturally factor out and integrate different aspects of language into one model.
</prevsent>
<prevsent>in smt, the factored model (koehn and hoang, 2007), <papid> D07-1091 </papid>for instance, permits to better tailor the lm to the task syntax, by complementing word-based n-gramswith part-of-speech (pos) lm , that can be estimated even on limited amount of task-specific data.</prevsent>
</prevsection>
<citsent citstr=" W11-2133 ">
besides many works addressing holistic lm domain adaptation for smt, e.g. foster and kuhn (2007), <papid> W07-0717 </papid>recently methods were also proposed to explicitly adapt the lm to the discourse topic of talk (ruiz and federico, 2011).<papid> W11-2133 </papid></citsent>
<aftsection>
<nextsent>our work makes another step in this direction by investigating hybrid lms that try to explicitly represent the speaking style of the talk genre.
</nextsent>
<nextsent>as difference from standard class-based lms (brown et al 1992) <papid> J92-4003 </papid>or the more recent local lms (monz, 2011), <papid> D11-1080 </papid>which are used to predict sequences of classes or word class pairs, our hybrid lm is devised to predict sequences of classes interleaved by words.</nextsent>
<nextsent>while we do not claim any technical novelty inthe model itself, to our knowledge deep investigation of hybrid lms for the sake of style adaptation is definitely new.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3537">
<title id=" E12-1045.xml">cutting the long tail hybrid language models for translation style adaptation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>besides many works addressing holistic lm domain adaptation for smt, e.g. foster and kuhn (2007), <papid> W07-0717 </papid>recently methods were also proposed to explicitly adapt the lm to the discourse topic of talk (ruiz and federico, 2011).<papid> W11-2133 </papid></prevsent>
<prevsent>our work makes another step in this direction by investigating hybrid lms that try to explicitly represent the speaking style of the talk genre.</prevsent>
</prevsection>
<citsent citstr=" J92-4003 ">
as difference from standard class-based lms (brown et al 1992) <papid> J92-4003 </papid>or the more recent local lms (monz, 2011), <papid> D11-1080 </papid>which are used to predict sequences of classes or word class pairs, our hybrid lm is devised to predict sequences of classes interleaved by words.</citsent>
<aftsection>
<nextsent>while we do not claim any technical novelty inthe model itself, to our knowledge deep investigation of hybrid lms for the sake of style adaptation is definitely new.
</nextsent>
<nextsent>finally, the term hybrid lm was inspired by yazgan and saraclar (2004),which called with this name lm predicting sequences of words and sub-words units, devised tolet speech recognizer detect out-of-vocabulary words.
</nextsent>
<nextsent>hybrid lms are n-gram models trained on amixed text representation where each word is either mapped to class or left as is. this choi ceis made according to measure of word common ness and is uni vocal for each word type.
</nextsent>
<nextsent>the rationale is to discard topic-specific words,while preserving those words that best characterize the language style (note that word frequency is computed on the in-domain corpus only).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3538">
<title id=" E12-1045.xml">cutting the long tail hybrid language models for translation style adaptation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>besides many works addressing holistic lm domain adaptation for smt, e.g. foster and kuhn (2007), <papid> W07-0717 </papid>recently methods were also proposed to explicitly adapt the lm to the discourse topic of talk (ruiz and federico, 2011).<papid> W11-2133 </papid></prevsent>
<prevsent>our work makes another step in this direction by investigating hybrid lms that try to explicitly represent the speaking style of the talk genre.</prevsent>
</prevsection>
<citsent citstr=" D11-1080 ">
as difference from standard class-based lms (brown et al 1992) <papid> J92-4003 </papid>or the more recent local lms (monz, 2011), <papid> D11-1080 </papid>which are used to predict sequences of classes or word class pairs, our hybrid lm is devised to predict sequences of classes interleaved by words.</citsent>
<aftsection>
<nextsent>while we do not claim any technical novelty inthe model itself, to our knowledge deep investigation of hybrid lms for the sake of style adaptation is definitely new.
</nextsent>
<nextsent>finally, the term hybrid lm was inspired by yazgan and saraclar (2004),which called with this name lm predicting sequences of words and sub-words units, devised tolet speech recognizer detect out-of-vocabulary words.
</nextsent>
<nextsent>hybrid lms are n-gram models trained on amixed text representation where each word is either mapped to class or left as is. this choi ceis made according to measure of word common ness and is uni vocal for each word type.
</nextsent>
<nextsent>the rationale is to discard topic-specific words,while preserving those words that best characterize the language style (note that word frequency is computed on the in-domain corpus only).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3539">
<title id=" E12-1045.xml">cutting the long tail hybrid language models for translation style adaptation </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>the training and test datasets were provided by the organizers of the iwslt11 evaluation, and are summarized in table 6.
</prevsent>
<prevsent>marked in bold are the corpora used for hybridlm training.
</prevsent>
</prevsection>
<citsent citstr=" P07-2045 ">
dev and test sets have single reference translation.for both language pairs, we set up competitive phrase-based systems6 using the moses toolkit (koehn et al 2007).<papid> P07-2045 </papid></citsent>
<aftsection>
<nextsent>the decoder features statistical log-linear model including phrase translation model and phrase reordering model (tillmann, 2004; <papid> N04-4026 </papid>koehn et al 2005), two word-based language models, distortion, wordand phrase penalties.</nextsent>
<nextsent>the translation and reordering models are obtained by combining models independently trained on the available paral6the smt systems used in this paper are thoroughly described in (ruiz et al 2011).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3540">
<title id=" E12-1045.xml">cutting the long tail hybrid language models for translation style adaptation </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>marked in bold are the corpora used for hybridlm training.
</prevsent>
<prevsent>dev and test sets have single reference translation.for both language pairs, we set up competitive phrase-based systems6 using the moses toolkit (koehn et al 2007).<papid> P07-2045 </papid></prevsent>
</prevsection>
<citsent citstr=" N04-4026 ">
the decoder features statistical log-linear model including phrase translation model and phrase reordering model (tillmann, 2004; <papid> N04-4026 </papid>koehn et al 2005), two word-based language models, distortion, wordand phrase penalties.</citsent>
<aftsection>
<nextsent>the translation and reordering models are obtained by combining models independently trained on the available paral6the smt systems used in this paper are thoroughly described in (ruiz et al 2011).
</nextsent>
<nextsent>443 corpus |s| |w | ` ar-en ted 90k 1.7m 18.9 un 7.9m 220m 27.8 en ted 124k 2.4m 19.5 news 30.7m 782m 25.4 ar test dev2010 934 19k 20.0 tst2010 1664 30k 18.1 en-fr ted 105k 2.0m 19.5 un 11m 291m 26.5 news 111k 3.1m 27.6 fr ted 107k 2.2m 20.6 news 11.6m 291m 25.2 en test dev2010 934 20k 21.5 tst2010 1664 32k 19.1 table 6: iwslt11 training and test data statistics: number of sentences |s|, number of tokens |w | and average sentence length `.
</nextsent>
<nextsent>token numbers are computed on the target language, except for the test sets.lel corpora: namely ted and news for arabicenglish; ted, news and un for english french.
</nextsent>
<nextsent>to this end we applied the fill-up method(nakov, 2008; <papid> W08-0320 </papid>bisazza et al 2011) in which outof-domain phrase tables are merged with the in domain table by adding only new phrase pairs.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3541">
<title id=" E12-1045.xml">cutting the long tail hybrid language models for translation style adaptation </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>443 corpus |s| |w | ` ar-en ted 90k 1.7m 18.9 un 7.9m 220m 27.8 en ted 124k 2.4m 19.5 news 30.7m 782m 25.4 ar test dev2010 934 19k 20.0 tst2010 1664 30k 18.1 en-fr ted 105k 2.0m 19.5 un 11m 291m 26.5 news 111k 3.1m 27.6 fr ted 107k 2.2m 20.6 news 11.6m 291m 25.2 en test dev2010 934 20k 21.5 tst2010 1664 32k 19.1 table 6: iwslt11 training and test data statistics: number of sentences |s|, number of tokens |w | and average sentence length `.
</prevsent>
<prevsent>token numbers are computed on the target language, except for the test sets.lel corpora: namely ted and news for arabicenglish; ted, news and un for english french.
</prevsent>
</prevsection>
<citsent citstr=" W08-0320 ">
to this end we applied the fill-up method(nakov, 2008; <papid> W08-0320 </papid>bisazza et al 2011) in which outof-domain phrase tables are merged with the in domain table by adding only new phrase pairs.</citsent>
<aftsection>
<nextsent>out-of-domain phrases are marked with binary feature whose weight is tuned together with the smt system weights.
</nextsent>
<nextsent>for each target language, two standard 5-gram lms are trained separately on the monolingual ted and news datasets, and log-linearly combined at decoding time.
</nextsent>
<nextsent>in the arabic-englishtask, we use hierarchical reordering model (gal ley and manning, 2008; <papid> D08-1089 </papid>hardmeier et al 2011), <papid> W11-2144 </papid>while in the english-french task we use default word-based bidirectional model.</nextsent>
<nextsent>the distortion limit is set to the default value of 6.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3542">
<title id=" E12-1045.xml">cutting the long tail hybrid language models for translation style adaptation </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>out-of-domain phrases are marked with binary feature whose weight is tuned together with the smt system weights.
</prevsent>
<prevsent>for each target language, two standard 5-gram lms are trained separately on the monolingual ted and news datasets, and log-linearly combined at decoding time.
</prevsent>
</prevsection>
<citsent citstr=" D08-1089 ">
in the arabic-englishtask, we use hierarchical reordering model (gal ley and manning, 2008; <papid> D08-1089 </papid>hardmeier et al 2011), <papid> W11-2144 </papid>while in the english-french task we use default word-based bidirectional model.</citsent>
<aftsection>
<nextsent>the distortion limit is set to the default value of 6.
</nextsent>
<nextsent>note that the use of large n-gram lms and of lexicalized reordering models was shown to wipeout the improvement achievable by pos-level lm (kirch hoff and yang, 2005; <papid> W05-0821 </papid>birch et al 2007).<papid> W07-0702 </papid>concerning data preprocessing we apply standard tokenization to the english and french text, while for arabic we use an in-house tokenizer that removes diacritics and normalizes special characters and digits.</nextsent>
<nextsent>arabic text is then segmented with amira (diab et al 2004) <papid> N04-4038 </papid>according to the atb scheme7.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3543">
<title id=" E12-1045.xml">cutting the long tail hybrid language models for translation style adaptation </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>out-of-domain phrases are marked with binary feature whose weight is tuned together with the smt system weights.
</prevsent>
<prevsent>for each target language, two standard 5-gram lms are trained separately on the monolingual ted and news datasets, and log-linearly combined at decoding time.
</prevsent>
</prevsection>
<citsent citstr=" W11-2144 ">
in the arabic-englishtask, we use hierarchical reordering model (gal ley and manning, 2008; <papid> D08-1089 </papid>hardmeier et al 2011), <papid> W11-2144 </papid>while in the english-french task we use default word-based bidirectional model.</citsent>
<aftsection>
<nextsent>the distortion limit is set to the default value of 6.
</nextsent>
<nextsent>note that the use of large n-gram lms and of lexicalized reordering models was shown to wipeout the improvement achievable by pos-level lm (kirch hoff and yang, 2005; <papid> W05-0821 </papid>birch et al 2007).<papid> W07-0702 </papid>concerning data preprocessing we apply standard tokenization to the english and french text, while for arabic we use an in-house tokenizer that removes diacritics and normalizes special characters and digits.</nextsent>
<nextsent>arabic text is then segmented with amira (diab et al 2004) <papid> N04-4038 </papid>according to the atb scheme7.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3544">
<title id=" E12-1045.xml">cutting the long tail hybrid language models for translation style adaptation </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>in the arabic-englishtask, we use hierarchical reordering model (gal ley and manning, 2008; <papid> D08-1089 </papid>hardmeier et al 2011), <papid> W11-2144 </papid>while in the english-french task we use default word-based bidirectional model.</prevsent>
<prevsent>the distortion limit is set to the default value of 6.</prevsent>
</prevsection>
<citsent citstr=" W05-0821 ">
note that the use of large n-gram lms and of lexicalized reordering models was shown to wipeout the improvement achievable by pos-level lm (kirch hoff and yang, 2005; <papid> W05-0821 </papid>birch et al 2007).<papid> W07-0702 </papid>concerning data preprocessing we apply standard tokenization to the english and french text, while for arabic we use an in-house tokenizer that removes diacritics and normalizes special characters and digits.</citsent>
<aftsection>
<nextsent>arabic text is then segmented with amira (diab et al 2004) <papid> N04-4038 </papid>according to the atb scheme7.</nextsent>
<nextsent>the arabic-english system uses cased7the arabic treebank tokenization scheme isolates conjunctions w+ and f+, prepositions l+, k+, b+, future marker s+, pronominal suffixes, but not the article al+.translation models, while the english-french system uses lower cased models and standard re casing post-process.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3545">
<title id=" E12-1045.xml">cutting the long tail hybrid language models for translation style adaptation </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>in the arabic-englishtask, we use hierarchical reordering model (gal ley and manning, 2008; <papid> D08-1089 </papid>hardmeier et al 2011), <papid> W11-2144 </papid>while in the english-french task we use default word-based bidirectional model.</prevsent>
<prevsent>the distortion limit is set to the default value of 6.</prevsent>
</prevsection>
<citsent citstr=" W07-0702 ">
note that the use of large n-gram lms and of lexicalized reordering models was shown to wipeout the improvement achievable by pos-level lm (kirch hoff and yang, 2005; <papid> W05-0821 </papid>birch et al 2007).<papid> W07-0702 </papid>concerning data preprocessing we apply standard tokenization to the english and french text, while for arabic we use an in-house tokenizer that removes diacritics and normalizes special characters and digits.</citsent>
<aftsection>
<nextsent>arabic text is then segmented with amira (diab et al 2004) <papid> N04-4038 </papid>according to the atb scheme7.</nextsent>
<nextsent>the arabic-english system uses cased7the arabic treebank tokenization scheme isolates conjunctions w+ and f+, prepositions l+, k+, b+, future marker s+, pronominal suffixes, but not the article al+.translation models, while the english-french system uses lower cased models and standard re casing post-process.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3546">
<title id=" E12-1045.xml">cutting the long tail hybrid language models for translation style adaptation </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>the distortion limit is set to the default value of 6.
</prevsent>
<prevsent>note that the use of large n-gram lms and of lexicalized reordering models was shown to wipeout the improvement achievable by pos-level lm (kirch hoff and yang, 2005; <papid> W05-0821 </papid>birch et al 2007).<papid> W07-0702 </papid>concerning data preprocessing we apply standard tokenization to the english and french text, while for arabic we use an in-house tokenizer that removes diacritics and normalizes special characters and digits.</prevsent>
</prevsection>
<citsent citstr=" N04-4038 ">
arabic text is then segmented with amira (diab et al 2004) <papid> N04-4038 </papid>according to the atb scheme7.</citsent>
<aftsection>
<nextsent>the arabic-english system uses cased7the arabic treebank tokenization scheme isolates conjunctions w+ and f+, prepositions l+, k+, b+, future marker s+, pronominal suffixes, but not the article al+.translation models, while the english-french system uses lower cased models and standard re casing post-process.
</nextsent>
<nextsent>feature weights are tuned on dev2010 by means of minimum error training procedure (mert) (och, 2003).<papid> P03-1021 </papid></nextsent>
<nextsent>following suggestions by clark et al(2011) and cettolo et al(2011) on controlling optimizer instability, we run mert four times on the same configuration and use the average of the resulting weights to evaluate translation performance.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3547">
<title id=" E12-1045.xml">cutting the long tail hybrid language models for translation style adaptation </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>arabic text is then segmented with amira (diab et al 2004) <papid> N04-4038 </papid>according to the atb scheme7.</prevsent>
<prevsent>the arabic-english system uses cased7the arabic treebank tokenization scheme isolates conjunctions w+ and f+, prepositions l+, k+, b+, future marker s+, pronominal suffixes, but not the article al+.translation models, while the english-french system uses lower cased models and standard re casing post-process.</prevsent>
</prevsection>
<citsent citstr=" P03-1021 ">
feature weights are tuned on dev2010 by means of minimum error training procedure (mert) (och, 2003).<papid> P03-1021 </papid></citsent>
<aftsection>
<nextsent>following suggestions by clark et al(2011) and cettolo et al(2011) on controlling optimizer instability, we run mert four times on the same configuration and use the average of the resulting weights to evaluate translation performance.
</nextsent>
<nextsent>5.3 hybrid lm integration.
</nextsent>
<nextsent>as previously stated, hybrid lms are trained only on in-domain data and are added to the log-linear decoder as an additional target lm.
</nextsent>
<nextsent>to this end,we use the class-based lm implementation provided in moses and irstlm, which applies the word-to-class mapping to translation hypotheses before lm querying8.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3548">
<title id=" E12-1045.xml">cutting the long tail hybrid language models for translation style adaptation </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>as previously stated, hybrid lms are trained only on in-domain data and are added to the log-linear decoder as an additional target lm.
</prevsent>
<prevsent>to this end,we use the class-based lm implementation provided in moses and irstlm, which applies the word-to-class mapping to translation hypotheses before lm querying8.
</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
the order of the additional lm is set to 10 in the arabic-english evaluation and 7 in the english-french, as these appeared to be the best settings in preliminary tests.translation quality is measured by bleu (pa pineni et al 2002), <papid> P02-1040 </papid>meteor (banerjee and lavie, 2005) <papid> W05-0909 </papid>and ter (snover et al 2006)9.</citsent>
<aftsection>
<nextsent>totest whether differences among systems are statistically significant we use approximate randomization as done in (riezler and maxwell, 2005)<papid> W05-0908 </papid>10.</nextsent>
<nextsent>model variants.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3549">
<title id=" E12-1045.xml">cutting the long tail hybrid language models for translation style adaptation </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>as previously stated, hybrid lms are trained only on in-domain data and are added to the log-linear decoder as an additional target lm.
</prevsent>
<prevsent>to this end,we use the class-based lm implementation provided in moses and irstlm, which applies the word-to-class mapping to translation hypotheses before lm querying8.
</prevsent>
</prevsection>
<citsent citstr=" W05-0909 ">
the order of the additional lm is set to 10 in the arabic-english evaluation and 7 in the english-french, as these appeared to be the best settings in preliminary tests.translation quality is measured by bleu (pa pineni et al 2002), <papid> P02-1040 </papid>meteor (banerjee and lavie, 2005) <papid> W05-0909 </papid>and ter (snover et al 2006)9.</citsent>
<aftsection>
<nextsent>totest whether differences among systems are statistically significant we use approximate randomization as done in (riezler and maxwell, 2005)<papid> W05-0908 </papid>10.</nextsent>
<nextsent>model variants.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3550">
<title id=" E12-1045.xml">cutting the long tail hybrid language models for translation style adaptation </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>to this end,we use the class-based lm implementation provided in moses and irstlm, which applies the word-to-class mapping to translation hypotheses before lm querying8.
</prevsent>
<prevsent>the order of the additional lm is set to 10 in the arabic-english evaluation and 7 in the english-french, as these appeared to be the best settings in preliminary tests.translation quality is measured by bleu (pa pineni et al 2002), <papid> P02-1040 </papid>meteor (banerjee and lavie, 2005) <papid> W05-0909 </papid>and ter (snover et al 2006)9.</prevsent>
</prevsection>
<citsent citstr=" W05-0908 ">
totest whether differences among systems are statistically significant we use approximate randomization as done in (riezler and maxwell, 2005)<papid> W05-0908 </papid>10.</citsent>
<aftsection>
<nextsent>model variants.
</nextsent>
<nextsent>the effect on mt quality of various hybrid lm variants is shown in table 7.note that allpos and alllemmas refer to deterministically assigned pos tags and lemmas, respectively.
</nextsent>
<nextsent>concerning the ratio of pos-mapped tokens, the best performing values are wp =.25 in arabic-english and wp =.50 in english-french.these hybrid mappings outperform all the uniform representations (words, lemmas and pos) with statistically significant bleu and meteor improvements.the fdf experiment involves the use of document frequency for the selection of commonwords.
</nextsent>
<nextsent>its performance is very close to that of hy 8detailed instructions on how to build and use hybrid lms can be found at http://hlt.fbk.eu/people/bisazza.9we use case-sensitive bleu and ter, but case insensitive meteor to enable the use of paraphrase tables distributed with the tool (version 1.3).10translation scores and significance tests were computed with the multeval toolkit (clark et al 2011): https://github.com/jhclark/multeval.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3551">
<title id=" E12-1045.xml">cutting the long tail hybrid language models for translation style adaptation </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>comparison with baseline.
</prevsent>
<prevsent>in table 8 the best performing hybrid lm is compared against the baseline that only includes the standard lms described in section 5.2.
</prevsent>
</prevsection>
<citsent citstr=" E99-1010 ">
to complete our evaluation, we also report the effect of an in-domain lm trained on 50 word classes induced from the corpus by maximum-likelihood based clustering (och, 1999).<papid> E99-1010 </papid></citsent>
<aftsection>
<nextsent>in the two language pairs, both types of lmresult inconsistent improvements over the base line.
</nextsent>
<nextsent>however, the gains achieved by the hybrid approach are larger and all statistically significant.
</nextsent>
<nextsent>the hybrid approach is significantly better than the unsupervised one by ter in arabic english and by bleu and meteor in english french (these siginificances are not reported in (a) arabic to english, iwslttst2010 added in domain bleu?
</nextsent>
<nextsent>met ? ter ? 10g lm none (baseline) 26.0 30.4 55.6 unsup.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3552">
<title id=" E09-1086.xml">unsupervised recognition of literal and non literal use of idiomatic expressions </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>they also typically only allow very limited lexical variation (*kick the vessel, *strike the bucket).
</prevsent>
<prevsent>many approaches for identifying idioms focuson one of these two aspects.
</prevsent>
</prevsection>
<citsent citstr=" P99-1041 ">
for instance, measures that compute the association strength between the elements of an expression have been employed to determine its degree of composition ality (lin, 1999; <papid> P99-1041 </papid>fazly and stevenson, 2006) (<papid> E06-1043 </papid>see also villavicencio et al (2007) <papid> D07-1110 </papid>for an overview and comparison of different measures).</citsent>
<aftsection>
<nextsent>other approaches use latent semantic analysis (lsa)to determine the similarity between potential idiom and its components (baldwin et al, 2003).<papid> W03-1812 </papid>low similarity is supposed to indicate low compositionality.</nextsent>
<nextsent>bannard (2007) <papid> W07-1101 </papid>proposes to identify idiomatic expressions by looking at their syntactic fixed ness, i.e., how likely they are to take modifiers or be passivised, and comparing this to what would be expected based on the observed behaviour of the component words.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3553">
<title id=" E09-1086.xml">unsupervised recognition of literal and non literal use of idiomatic expressions </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>they also typically only allow very limited lexical variation (*kick the vessel, *strike the bucket).
</prevsent>
<prevsent>many approaches for identifying idioms focuson one of these two aspects.
</prevsent>
</prevsection>
<citsent citstr=" E06-1043 ">
for instance, measures that compute the association strength between the elements of an expression have been employed to determine its degree of composition ality (lin, 1999; <papid> P99-1041 </papid>fazly and stevenson, 2006) (<papid> E06-1043 </papid>see also villavicencio et al (2007) <papid> D07-1110 </papid>for an overview and comparison of different measures).</citsent>
<aftsection>
<nextsent>other approaches use latent semantic analysis (lsa)to determine the similarity between potential idiom and its components (baldwin et al, 2003).<papid> W03-1812 </papid>low similarity is supposed to indicate low compositionality.</nextsent>
<nextsent>bannard (2007) <papid> W07-1101 </papid>proposes to identify idiomatic expressions by looking at their syntactic fixed ness, i.e., how likely they are to take modifiers or be passivised, and comparing this to what would be expected based on the observed behaviour of the component words.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3554">
<title id=" E09-1086.xml">unsupervised recognition of literal and non literal use of idiomatic expressions </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>they also typically only allow very limited lexical variation (*kick the vessel, *strike the bucket).
</prevsent>
<prevsent>many approaches for identifying idioms focuson one of these two aspects.
</prevsent>
</prevsection>
<citsent citstr=" D07-1110 ">
for instance, measures that compute the association strength between the elements of an expression have been employed to determine its degree of composition ality (lin, 1999; <papid> P99-1041 </papid>fazly and stevenson, 2006) (<papid> E06-1043 </papid>see also villavicencio et al (2007) <papid> D07-1110 </papid>for an overview and comparison of different measures).</citsent>
<aftsection>
<nextsent>other approaches use latent semantic analysis (lsa)to determine the similarity between potential idiom and its components (baldwin et al, 2003).<papid> W03-1812 </papid>low similarity is supposed to indicate low compositionality.</nextsent>
<nextsent>bannard (2007) <papid> W07-1101 </papid>proposes to identify idiomatic expressions by looking at their syntactic fixed ness, i.e., how likely they are to take modifiers or be passivised, and comparing this to what would be expected based on the observed behaviour of the component words.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3555">
<title id=" E09-1086.xml">unsupervised recognition of literal and non literal use of idiomatic expressions </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>many approaches for identifying idioms focuson one of these two aspects.
</prevsent>
<prevsent>for instance, measures that compute the association strength between the elements of an expression have been employed to determine its degree of composition ality (lin, 1999; <papid> P99-1041 </papid>fazly and stevenson, 2006) (<papid> E06-1043 </papid>see also villavicencio et al (2007) <papid> D07-1110 </papid>for an overview and comparison of different measures).</prevsent>
</prevsection>
<citsent citstr=" W03-1812 ">
other approaches use latent semantic analysis (lsa)to determine the similarity between potential idiom and its components (baldwin et al, 2003).<papid> W03-1812 </papid>low similarity is supposed to indicate low compositionality.</citsent>
<aftsection>
<nextsent>bannard (2007) <papid> W07-1101 </papid>proposes to identify idiomatic expressions by looking at their syntactic fixed ness, i.e., how likely they are to take modifiers or be passivised, and comparing this to what would be expected based on the observed behaviour of the component words.</nextsent>
<nextsent>fazly and stevenson (2006) <papid> E06-1043 </papid>combine information about syntactic and lexical fixed ness (i.e., estimated degree of compositionality) into one measure.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3556">
<title id=" E09-1086.xml">unsupervised recognition of literal and non literal use of idiomatic expressions </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>for instance, measures that compute the association strength between the elements of an expression have been employed to determine its degree of composition ality (lin, 1999; <papid> P99-1041 </papid>fazly and stevenson, 2006) (<papid> E06-1043 </papid>see also villavicencio et al (2007) <papid> D07-1110 </papid>for an overview and comparison of different measures).</prevsent>
<prevsent>other approaches use latent semantic analysis (lsa)to determine the similarity between potential idiom and its components (baldwin et al, 2003).<papid> W03-1812 </papid>low similarity is supposed to indicate low compositionality.</prevsent>
</prevsection>
<citsent citstr=" W07-1101 ">
bannard (2007) <papid> W07-1101 </papid>proposes to identify idiomatic expressions by looking at their syntactic fixed ness, i.e., how likely they are to take modifiers or be passivised, and comparing this to what would be expected based on the observed behaviour of the component words.</citsent>
<aftsection>
<nextsent>fazly and stevenson (2006) <papid> E06-1043 </papid>combine information about syntactic and lexical fixed ness (i.e., estimated degree of compositionality) into one measure.</nextsent>
<nextsent>the few token-based approaches include study by katz and giesbrecht (2006), <papid> W06-1203 </papid>who devise supervised method in which they compute the meaning vectors for the literal and non-literal usages of given expression in the training data.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3558">
<title id=" E09-1086.xml">unsupervised recognition of literal and non literal use of idiomatic expressions </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>bannard (2007) <papid> W07-1101 </papid>proposes to identify idiomatic expressions by looking at their syntactic fixed ness, i.e., how likely they are to take modifiers or be passivised, and comparing this to what would be expected based on the observed behaviour of the component words.</prevsent>
<prevsent>fazly and stevenson (2006) <papid> E06-1043 </papid>combine information about syntactic and lexical fixed ness (i.e., estimated degree of compositionality) into one measure.</prevsent>
</prevsection>
<citsent citstr=" W06-1203 ">
the few token-based approaches include study by katz and giesbrecht (2006), <papid> W06-1203 </papid>who devise supervised method in which they compute the meaning vectors for the literal and non-literal usages of given expression in the training data.</citsent>
<aftsection>
<nextsent>an unseen test instance of the same expression is then labelled by performing nearest neighbour classification.
</nextsent>
<nextsent>they report an average accuracy of 72%, though their evaluation is fairly small scale, using only one expression and 67 instances.
</nextsent>
<nextsent>birke andsarkar (2006) <papid> E06-1042 </papid>model literal vs. non-literal classification as word sense disambiguation task anduse clustering algorithm which compares test instances to two automatically constructed seed sets(one with literal and one with non-literal expres sions), assigning the label of the closest set.</nextsent>
<nextsent>while the seed sets are created without immediate human intervention they do relyon manually created resources such as databases of known idioms.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3559">
<title id=" E09-1086.xml">unsupervised recognition of literal and non literal use of idiomatic expressions </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>an unseen test instance of the same expression is then labelled by performing nearest neighbour classification.
</prevsent>
<prevsent>they report an average accuracy of 72%, though their evaluation is fairly small scale, using only one expression and 67 instances.
</prevsent>
</prevsection>
<citsent citstr=" E06-1042 ">
birke andsarkar (2006) <papid> E06-1042 </papid>model literal vs. non-literal classification as word sense disambiguation task anduse clustering algorithm which compares test instances to two automatically constructed seed sets(one with literal and one with non-literal expres sions), assigning the label of the closest set.</citsent>
<aftsection>
<nextsent>while the seed sets are created without immediate human intervention they do relyon manually created resources such as databases of known idioms.
</nextsent>
<nextsent>cook et al (2007) <papid> W07-1106 </papid>and fazly et al (to appear)propose an alternative method which crucially relies on the concept of canonical form (cform).</nextsent>
<nextsent>it is assumed that for each idiom there is fixed form (or small set of those) corresponding tothe syntactic pattern(s) in which the idiom normally occurs (riehemann, 2001).1 the canonical form allows for inflectional variation of the head verb but not for other variations (such as nominal inflection, choice of determiner etc.).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3560">
<title id=" E09-1086.xml">unsupervised recognition of literal and non literal use of idiomatic expressions </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>birke andsarkar (2006) <papid> E06-1042 </papid>model literal vs. non-literal classification as word sense disambiguation task anduse clustering algorithm which compares test instances to two automatically constructed seed sets(one with literal and one with non-literal expres sions), assigning the label of the closest set.</prevsent>
<prevsent>while the seed sets are created without immediate human intervention they do relyon manually created resources such as databases of known idioms.</prevsent>
</prevsection>
<citsent citstr=" W07-1106 ">
cook et al (2007) <papid> W07-1106 </papid>and fazly et al (to appear)propose an alternative method which crucially relies on the concept of canonical form (cform).</citsent>
<aftsection>
<nextsent>it is assumed that for each idiom there is fixed form (or small set of those) corresponding tothe syntactic pattern(s) in which the idiom normally occurs (riehemann, 2001).1 the canonical form allows for inflectional variation of the head verb but not for other variations (such as nominal inflection, choice of determiner etc.).
</nextsent>
<nextsent>it has been observed that if an expression is used idiomatic ally, it typically occurs in its canonical form.
</nextsent>
<nextsent>for example, riehemann (2001, p. 34) found that for decomposable idioms 75% of the occurrences are in canonical form, rising to 97%for non-decomposable idioms.2 cook et al exploit this behaviour and propose an unsupervised method in which an expression is classified as idiomatic if it occurs in canonical form and literalotherwise.
</nextsent>
<nextsent>canonical forms are determined automatically using statistical, frequency-based measure.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3561">
<title id=" E09-1086.xml">unsupervised recognition of literal and non literal use of idiomatic expressions </title>
<section> using lexical cohesion to identify.  </section>
<citcontext>
<prevsection>
<prevsent>while cohesion-based approach to token-basedidiom classification should be intuitively successful, its practical usefulness depends crucially on the availability of suitable method for computing semantic relatedness.
</prevsent>
<prevsent>this is currently an area of active research.
</prevsent>
</prevsection>
<citsent citstr=" J06-1003 ">
there are two main approaches.methods based on manually built lexical knowledge bases, such as wordnet, model semantic relatedness by computing the shortest path between two concepts in the knowledge base and/or by looking at word overlap in the glosses (see budanitsky and hirst (2006) <papid> J06-1003 </papid>for an overview).</citsent>
<aftsection>
<nextsent>distributional approaches, on the other hand, relyon text corpora, and model relatedness by comparing the contexts in which two words occur, assuming that related words occur in similar context (e.g., hindle (1990), <papid> P90-1034 </papid>lin (1998), <papid> P98-2127 </papid>mohammad and hirst (2006)).<papid> W06-1605 </papid>more recently, there has also been research on using wikipedia and related resources for modelling semantic relatedness (ponzetto and strube, 2007; zesch et al, 2008).all approaches have advantages and disadvan tages.</nextsent>
<nextsent>wordnet-based approaches, for instance, typically have low coverage and only work for so-called classical relations?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3562">
<title id=" E09-1086.xml">unsupervised recognition of literal and non literal use of idiomatic expressions </title>
<section> using lexical cohesion to identify.  </section>
<citcontext>
<prevsection>
<prevsent>this is currently an area of active research.
</prevsent>
<prevsent>there are two main approaches.methods based on manually built lexical knowledge bases, such as wordnet, model semantic relatedness by computing the shortest path between two concepts in the knowledge base and/or by looking at word overlap in the glosses (see budanitsky and hirst (2006) <papid> J06-1003 </papid>for an overview).</prevsent>
</prevsection>
<citsent citstr=" P90-1034 ">
distributional approaches, on the other hand, relyon text corpora, and model relatedness by comparing the contexts in which two words occur, assuming that related words occur in similar context (e.g., hindle (1990), <papid> P90-1034 </papid>lin (1998), <papid> P98-2127 </papid>mohammad and hirst (2006)).<papid> W06-1605 </papid>more recently, there has also been research on using wikipedia and related resources for modelling semantic relatedness (ponzetto and strube, 2007; zesch et al, 2008).all approaches have advantages and disadvan tages.</citsent>
<aftsection>
<nextsent>wordnet-based approaches, for instance, typically have low coverage and only work for so-called classical relations?
</nextsent>
<nextsent>like hypernymy, antonymy etc. distributional approaches usually conflate different word senses and may therefore lead to un intuitive results.
</nextsent>
<nextsent>for our task, we need to model wide range of semantic relations (morris and hirst, 2004), <papid> W04-2607 </papid>for example, relations based on some kind of functional or situational association, as between fire and coal in (3) or between ice and water in example (1).</nextsent>
<nextsent>likewise we also need to model relations between non-nouns, for instance between spill and sweep up in example (2).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3563">
<title id=" E09-1086.xml">unsupervised recognition of literal and non literal use of idiomatic expressions </title>
<section> using lexical cohesion to identify.  </section>
<citcontext>
<prevsection>
<prevsent>this is currently an area of active research.
</prevsent>
<prevsent>there are two main approaches.methods based on manually built lexical knowledge bases, such as wordnet, model semantic relatedness by computing the shortest path between two concepts in the knowledge base and/or by looking at word overlap in the glosses (see budanitsky and hirst (2006) <papid> J06-1003 </papid>for an overview).</prevsent>
</prevsection>
<citsent citstr=" P98-2127 ">
distributional approaches, on the other hand, relyon text corpora, and model relatedness by comparing the contexts in which two words occur, assuming that related words occur in similar context (e.g., hindle (1990), <papid> P90-1034 </papid>lin (1998), <papid> P98-2127 </papid>mohammad and hirst (2006)).<papid> W06-1605 </papid>more recently, there has also been research on using wikipedia and related resources for modelling semantic relatedness (ponzetto and strube, 2007; zesch et al, 2008).all approaches have advantages and disadvan tages.</citsent>
<aftsection>
<nextsent>wordnet-based approaches, for instance, typically have low coverage and only work for so-called classical relations?
</nextsent>
<nextsent>like hypernymy, antonymy etc. distributional approaches usually conflate different word senses and may therefore lead to un intuitive results.
</nextsent>
<nextsent>for our task, we need to model wide range of semantic relations (morris and hirst, 2004), <papid> W04-2607 </papid>for example, relations based on some kind of functional or situational association, as between fire and coal in (3) or between ice and water in example (1).</nextsent>
<nextsent>likewise we also need to model relations between non-nouns, for instance between spill and sweep up in example (2).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3564">
<title id=" E09-1086.xml">unsupervised recognition of literal and non literal use of idiomatic expressions </title>
<section> using lexical cohesion to identify.  </section>
<citcontext>
<prevsection>
<prevsent>this is currently an area of active research.
</prevsent>
<prevsent>there are two main approaches.methods based on manually built lexical knowledge bases, such as wordnet, model semantic relatedness by computing the shortest path between two concepts in the knowledge base and/or by looking at word overlap in the glosses (see budanitsky and hirst (2006) <papid> J06-1003 </papid>for an overview).</prevsent>
</prevsection>
<citsent citstr=" W06-1605 ">
distributional approaches, on the other hand, relyon text corpora, and model relatedness by comparing the contexts in which two words occur, assuming that related words occur in similar context (e.g., hindle (1990), <papid> P90-1034 </papid>lin (1998), <papid> P98-2127 </papid>mohammad and hirst (2006)).<papid> W06-1605 </papid>more recently, there has also been research on using wikipedia and related resources for modelling semantic relatedness (ponzetto and strube, 2007; zesch et al, 2008).all approaches have advantages and disadvan tages.</citsent>
<aftsection>
<nextsent>wordnet-based approaches, for instance, typically have low coverage and only work for so-called classical relations?
</nextsent>
<nextsent>like hypernymy, antonymy etc. distributional approaches usually conflate different word senses and may therefore lead to un intuitive results.
</nextsent>
<nextsent>for our task, we need to model wide range of semantic relations (morris and hirst, 2004), <papid> W04-2607 </papid>for example, relations based on some kind of functional or situational association, as between fire and coal in (3) or between ice and water in example (1).</nextsent>
<nextsent>likewise we also need to model relations between non-nouns, for instance between spill and sweep up in example (2).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3565">
<title id=" E09-1086.xml">unsupervised recognition of literal and non literal use of idiomatic expressions </title>
<section> using lexical cohesion to identify.  </section>
<citcontext>
<prevsection>
<prevsent>wordnet-based approaches, for instance, typically have low coverage and only work for so-called classical relations?
</prevsent>
<prevsent>like hypernymy, antonymy etc. distributional approaches usually conflate different word senses and may therefore lead to un intuitive results.
</prevsent>
</prevsection>
<citsent citstr=" W04-2607 ">
for our task, we need to model wide range of semantic relations (morris and hirst, 2004), <papid> W04-2607 </papid>for example, relations based on some kind of functional or situational association, as between fire and coal in (3) or between ice and water in example (1).</citsent>
<aftsection>
<nextsent>likewise we also need to model relations between non-nouns, for instance between spill and sweep up in example (2).
</nextsent>
<nextsent>some relations also require world-knowledge, as in example (7), where the literal usage of drop theball is not only indicated by the presence of goalkeeper but also by knowing that wayne rooney and kevin campbell are both football players.(7) when rooney collided with the goalkeeper, causing him to drop the ball, kevin campbell followed in.we thus decided against wordnet-based measure of semantic relatedness, opting instead for adistributional approach, normalized google distance (ngd, see cilibrasi and vitanyi (2007)), which computes relatedness on the basis of page counts returned by search engine.
</nextsent>
<nextsent>ngd is measure of association that quantifies the strength of 756 relationship between two words.
</nextsent>
<nextsent>it is defined as follows: ngd(x, y) = max{log f(x), log f(y)} ? log f(x, y) log min{log f(x), log f(y)} (8)where and are the two words whose association strength is computed (e.g., fire and coal),f(x) is the page count returned by the search engine for the term (and likewise for f(y) and y), f(x, y) is the page count returned when querying for and y?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3566">
<title id=" E09-1086.xml">unsupervised recognition of literal and non literal use of idiomatic expressions </title>
<section> using lexical cohesion to identify.  </section>
<citcontext>
<prevsection>
<prevsent>the inflected forms were generated by the morph tools developed at the university of sussex (minnen et al, 2001).5 3.3 cohesion-based classifiers.
</prevsent>
<prevsent>we implemented two cohesion-based classifiers: the first one computes the lexical chains for the input text and classifies an expression as literal or non-literal depending on whether its component words participate in any of the chains, the second classifier builds cohesion graph and determine show this graph changes when the expression is inserted or left out.
</prevsent>
</prevsection>
<citsent citstr=" W97-0703 ">
chain-based classifier various methods for building lexical chains have been proposed in the literature (hirst and st-onge, 1998; barzilay and elhadad, 1997; <papid> W97-0703 </papid>silber and mccoy, 2002) <papid> J02-4004 </papid>but the basic idea is as follows: the content words of the text are considered in sequence and for each word it is determined whether it is similar enough to (the words in) one of the existing chains to be placed in that chain, if not it is placed in chain of its own.</citsent>
<aftsection>
<nextsent>depending on the chain building algorithm used, word is placed in chain if it is related to one other word in the chain or to all of them.
</nextsent>
<nextsent>the latter strategy is more conservative and tends to lead to shorter but more reliable chains and it is the method we adopted here.6 note that the chaining algorithm has free parameter, namely threshold which has to be surpassed to consider two words related (relatedness threshold).on the basis of the computed chains, the classifier has to decide whether the target expression isused literally or not.
</nextsent>
<nextsent>a simple strategy would classify an expression as literal whenever one or more of its component words participates in any chain.
</nextsent>
<nextsent>however, as the chains are potentially noisy, this may not be the best strategy.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3567">
<title id=" E09-1086.xml">unsupervised recognition of literal and non literal use of idiomatic expressions </title>
<section> using lexical cohesion to identify.  </section>
<citcontext>
<prevsection>
<prevsent>the inflected forms were generated by the morph tools developed at the university of sussex (minnen et al, 2001).5 3.3 cohesion-based classifiers.
</prevsent>
<prevsent>we implemented two cohesion-based classifiers: the first one computes the lexical chains for the input text and classifies an expression as literal or non-literal depending on whether its component words participate in any of the chains, the second classifier builds cohesion graph and determine show this graph changes when the expression is inserted or left out.
</prevsent>
</prevsection>
<citsent citstr=" J02-4004 ">
chain-based classifier various methods for building lexical chains have been proposed in the literature (hirst and st-onge, 1998; barzilay and elhadad, 1997; <papid> W97-0703 </papid>silber and mccoy, 2002) <papid> J02-4004 </papid>but the basic idea is as follows: the content words of the text are considered in sequence and for each word it is determined whether it is similar enough to (the words in) one of the existing chains to be placed in that chain, if not it is placed in chain of its own.</citsent>
<aftsection>
<nextsent>depending on the chain building algorithm used, word is placed in chain if it is related to one other word in the chain or to all of them.
</nextsent>
<nextsent>the latter strategy is more conservative and tends to lead to shorter but more reliable chains and it is the method we adopted here.6 note that the chaining algorithm has free parameter, namely threshold which has to be surpassed to consider two words related (relatedness threshold).on the basis of the computed chains, the classifier has to decide whether the target expression isused literally or not.
</nextsent>
<nextsent>a simple strategy would classify an expression as literal whenever one or more of its component words participates in any chain.
</nextsent>
<nextsent>however, as the chains are potentially noisy, this may not be the best strategy.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3579">
<title id=" E09-1093.xml">learning efficient parsing </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>from these results we tentatively conclude that the proposed technique is applicable across text types and domains.
</prevsent>
<prevsent>one may wonder how the technique introduced inthis paper relates to techniques in which the disambiguation model is used directly during parsing to eliminate unlikely partial parses.
</prevsent>
</prevsection>
<citsent citstr=" W05-1511 ">
an example in the context of wide coverage unification-basedparsing is the beam thresholding technique employed in the enju hpsg parser for english (tsu ruoka et al, 2004; ninomiya et al, 2005).<papid> W05-1511 </papid>in beam-search parser, unlikely partial analyses are constructed, and then - based on the probability assigned to these partial analyses - removed from further consideration.</citsent>
<aftsection>
<nextsent>one potential advantage of the use of our filters may be, that many of these partial analyses will not even be constructed in the first place, and therefore no time is spent on these alternatives at all.
</nextsent>
<nextsent>we have not performed detailed comparison, because the statistical model employed in alpino contains some features which refer to arbitrary large parts of parse.
</nextsent>
<nextsent>such non-local features are not allowed in the enju approach.
</nextsent>
<nextsent>a parsing system may also combine both types of techniques.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3580">
<title id=" E09-1093.xml">learning efficient parsing </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>the idea that corpora can be used to improve parsing efficiency was an important ingredient ofa technique that was called grammar specialization.
</prevsent>
<prevsent>an overview of grammar specialization techniques is given in (simaan, 1999).
</prevsent>
</prevsection>
<citsent citstr=" P96-1030 ">
for instance, rayner and carter (1996) <papid> P96-1030 </papid>use explanation-based learning to specialize given general grammar to specific domain.</citsent>
<aftsection>
<nextsent>they report important efficiency gains (the parser is about three times faster), coupled with mild reduction of coverage (5% loss).
</nextsent>
<nextsent>in contrast to our approach in which no manual annotation is required, rayner and carter (1996) <papid> P96-1030 </papid>report that for each sentence in the training data, the best parse was selected manually from the setof parses generated by the parser.</nextsent>
<nextsent>for the experiments described in the paper, this constituted an effort of two and half person-months.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3584">
<title id=" E09-1093.xml">learning efficient parsing </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>a further difference is related to the pruning strategies.
</prevsent>
<prevsent>our pruning strategies are extremely simple.
</prevsent>
</prevsection>
<citsent citstr=" P94-1026 ">
the cutting criteria employed in grammar specialization either require carefully manually tuning, or require more complicated statistical techniques (samuelsson, 1994); <papid> P94-1026 </papid>automatically derived cutting criteria, however, perform considerably worse.a possible improvement of our approach consists of predicting whether forgiven input sentence the filter should be used, or whether the sentence appears to be easy?</citsent>
<aftsection>
<nextsent>enough to allow for full parse.
</nextsent>
<nextsent>for instance, one may chose to use the filter only for sentences of given minimum length.
</nextsent>
<nextsent>initial experiments indicate that such asetup may improve somewhat over the results presented here.
</nextsent>
<nextsent>acknowledgments this research was carried out in part in the context of the stevin programme which is funded by the dutch and flemish governments (http://taalunieversum.org/taal/technologie/stevin/).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3585">
<title id=" E09-1098.xml">co dispersion a windowless approach to lexical association </title>
<section> the impact of window size.  </section>
<citcontext>
<prevsection>
<prevsent>the issue of how to determine appropriate window size (and shape) has often been glossed over in the literature, with such parameters being determined arbitrarily, or empirically on per application basis, and often receiving little more than cursory mention under the description of method.
</prevsent>
<prevsent>for reasons that we will discuss however, the issue has been receiving increasing attention.
</prevsent>
</prevsection>
<citsent citstr=" P97-1067 ">
some have attempted to address it intrinsically (sahlgren 2006; schulte im walde &amp; melinger, 2008; hung et al  2001); others no less earnestly in the interests of specific applications (lamjiri, 2003; edmonds, 1997; <papid> P97-1067 </papid>wang 2005; choueka &amp; lusignan, 1985) (note that this divide is sometimes subtle).</citsent>
<aftsection>
<nextsent>the 2008 workshop on distributional lexical semantics, held in conjunction with the european summer school on logic, language and learning (esslli) ? hereafter the esslli workshop - saw this issue (along with other problem?
</nextsent>
<nextsent>parameters in distributional lexical semantics) as one of its central themes, and witnessed many different takes upon it.
</nextsent>
<nextsent>interestingly, there was little consensus, with some studies appearing on the surface to starkly contradict one-another.
</nextsent>
<nextsent>it is now generally recognized that window size is, like the choice of corpus or specific association measure, parameter which can have potentially profound impact upon the performance of applications which aim to exploit co-occurrence counts.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3586">
<title id=" E09-1098.xml">co dispersion a windowless approach to lexical association </title>
<section> the impact of window size.  </section>
<citcontext>
<prevsection>
<prevsent>for example, 861 window size of two words can only ever observe bigrams, and cannot detect associations resulting from larger constructs, however ingrained in the language (e.g. if ? then?, ne ? pas?, dear ... yours?).
</prevsent>
<prevsent>this is not the full story however.
</prevsent>
</prevsection>
<citsent citstr=" C02-1007 ">
as, rapp (2002) <papid> C02-1007 </papid>observes, choosing window size involves making trade-off between various qualities.</citsent>
<aftsection>
<nextsent>so conversely for example, frequency counts within large windows, though able to detect longer-range associations, are not readily able to distinguish them from bigram style cooccurrences, and so some discriminatory power, and sensitivity to the latter, is lost.
</nextsent>
<nextsent>rapp (2002) <papid> C02-1007 </papid>calls this trade-off specificity?; equivalent observations were made by church &amp; hanks (1989) and church et al (1991), who refer to the tendency for large windows to wash out?, smear?</nextsent>
<nextsent>or defocus?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3603">
<title id=" E09-1098.xml">co dispersion a windowless approach to lexical association </title>
<section> the impact of window size.  </section>
<citcontext>
<prevsection>
<prevsent>this may involve applying insights on the statistical limitations of working from finite sample (add-?
</prevsent>
<prevsent>smoothing, good-turing smoothing), making inferences from words with similar co-occurrence patterns, or backing off?
</prevsent>
</prevsection>
<citsent citstr=" J03-3005 ">
to more general language model based on individual word frequencies, or even another corpus; for example, keller &amp; lapata (2003) <papid> J03-3005 </papid>use the web.</citsent>
<aftsection>
<nextsent>all of these approaches attempt to mitigate the data sparseness manifest in the observed cooccurrence frequencies; they do not presume to reduce data sparseness by improving the method of observation.
</nextsent>
<nextsent>indeed, the general assumption would seem to be that the only way to minimize data sparseness is to use more data.
</nextsent>
<nextsent>however, we will show that, similarly to wangs (2005) observation concerning windowed measurements in general, apparent data sparseness is as much manifestation of the observation method as it is of the data itself; there may exist much pertinent information in the corpus which yet remains unexploited.
</nextsent>
<nextsent>comprehensive multi-scalar analyses (such as applied by quasthoff, 2007; and schulte im walde &amp; melinger, 2008) can be laborious and computationally expensive, and it is not yet clear how to derive simple association scores and suchlike from the dense data they generate (typi cally separate set of statistics for each window size examined).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3604">
<title id=" E09-1098.xml">co dispersion a windowless approach to lexical association </title>
<section> proximity as association.  </section>
<citcontext>
<prevsection>
<prevsent>(2002) and washtell (2007) - both in gries (2008) - as an alternative to approaches based on corpus division, for quantifying the dispersion of words within the text.
</prevsent>
<prevsent>hardcastle (2005) and washtell (2007) apply this same concept to measuring word pair associations, the former via somewhat ad-hoc approach, the latter through an extension of clark-evans (1954) dispersion metric to the concept of co-dispersion: the tendency of unlike words to gravitate (or be similarly dispersed) in the text.
</prevsent>
</prevsection>
<citsent citstr=" C04-1147 ">
terra &amp; clarke (2004) <papid> C04-1147 </papid>use very similar approach in order to generate probabilistic language model, where previously n-gram models have been used, the allusion to proximity as fundamental indicator of lexical association does in fact per 863meate the literature.</citsent>
<aftsection>
<nextsent>halliday (1966), for example, in church et al (1991) talked not explicitly of frequencies within windows, but of identifying lexical associates via some measure of significant proximity, either scale or at least cut-off point?.
</nextsent>
<nextsent>for one (possibly practical) reason or another, the cut-off point?
</nextsent>
<nextsent>has been adopted and the intuition of proximity has since become entrained within distinctly frequency oriented model.
</nextsent>
<nextsent>by way of example, the notion of proximity has been somewhat more directly courted in some window-based studies through the use of ramped?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3607">
<title id=" E09-2013.xml">matching readers preferences and reading skills with appropriate web texts </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>on key word search for snakes?
</prevsent>
<prevsent>the same results will begiven whether the user is seven year old elementary school kid or snake expert.
</prevsent>
</prevsection>
<citsent citstr=" N07-1058 ">
prior work on assessing reading level includes(heilman et al, 2007) <papid> N07-1058 </papid>who experiment with system that employs grammatical features and vocabulary to predict readability.</citsent>
<aftsection>
<nextsent>the system is part ofthe the reap tutor, designed to help esl learners improve their vocabulary skills.
</nextsent>
<nextsent>reaps information retrieval system (collins-thompson and callan, 2004) is based on web data that have been annotated and indexed off-line.
</nextsent>
<nextsent>also, relatedly, (schwarm and ostendorf, 2005) <papid> P05-1065 </papid>use statistical language model to train svm classifiers to classify text for grade levels 2-5.</nextsent>
<nextsent>the classifiers precision ranges from 38%- 75% depending on the grade level.in this demo, we present read-x, system designed to evaluate if text retrieved from the webis appropriate for the intended reader.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3608">
<title id=" E09-2013.xml">matching readers preferences and reading skills with appropriate web texts </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the system is part ofthe the reap tutor, designed to help esl learners improve their vocabulary skills.
</prevsent>
<prevsent>reaps information retrieval system (collins-thompson and callan, 2004) is based on web data that have been annotated and indexed off-line.
</prevsent>
</prevsection>
<citsent citstr=" P05-1065 ">
also, relatedly, (schwarm and ostendorf, 2005) <papid> P05-1065 </papid>use statistical language model to train svm classifiers to classify text for grade levels 2-5.</citsent>
<aftsection>
<nextsent>the classifiers precision ranges from 38%- 75% depending on the grade level.in this demo, we present read-x, system designed to evaluate if text retrieved from the webis appropriate for the intended reader.
</nextsent>
<nextsent>our system analyzes web text and returns the thematic area and the expected reading difficulty of there trieved texts.
</nextsent>
<nextsent>1 to our knowledge, read-x is the first system that performs in real time a)keyword search, b)thematic classification and c)analysis of reading difficulty.
</nextsent>
<nextsent>search results and analyses are returned within few seconds to maximum of aminute or two depending on the speed of the connection.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3609">
<title id=" E09-2013.xml">matching readers preferences and reading skills with appropriate web texts </title>
<section> web search and text classification.  </section>
<citcontext>
<prevsection>
<prevsent>formulas whose predictions will more closely reflect reading times for text comprehension will be preferred and form the basis fora better metric in the future.
</prevsent>
<prevsent>in the current implementation, read-x reports the scores for each formula in separate column.
</prevsent>
</prevsection>
<citsent citstr=" D08-1020 ">
other readability features modeling aspects of discourse coherence (e.g.,(miltsakaki and kukich, 2004), (barzilay and lapata, 2008), (bruss et al, 2004), (pitler and nenkova, 2008)) <papid> D08-1020 </papid>can also be integrated after psy cho linguistic evaluation studies are completed and their computation of such features can be made in real time.</citsent>
<aftsection>
<nextsent>text classification for the text classification task, we a) built corpus of pre labeled thematic categories and b) compared the performance ofthree classifiers to evaluate their suitability for thematic classification task.3we collected corpus of approximately 3.4 million words.
</nextsent>
<nextsent>the corpus contains text extracted from web-pages that were previously manually classified per school subject area by educators.
</nextsent>
<nextsent>we organized it into small thematic hierarchy,with three sets of labels: a) labels for super categories, b) labels for basic categories and c) labels for subcategories.
</nextsent>
<nextsent>there are 3 super categories (literature, science, sports), 8 basic categories (arts, career and business, literature, philosophy and religion, science, social studies, sports and health, technology) and 41 subcategories (e.g., the subcategories for literature are art criticism, art history, dance, music, theater).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3610">
<title id=" E09-1044.xml">rule filtering by pattern for efficient hierarchical translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>rules are put into syntactic classes based on the number of non-terminals andthe pattern, and various filtering strategies are then applied to assess the impact on translation speed and quality.
</prevsent>
<prevsent>results are reported on the 2008 nist arabic-to english evaluation task.
</prevsent>
</prevsection>
<citsent citstr=" P05-1033 ">
hierarchical phrase-based translation (chiang,2005) <papid> P05-1033 </papid>has emerged as one of the dominant current approaches to statistical machine translation.</citsent>
<aftsection>
<nextsent>hiero translation systems incorporate many of the strengths of phrase-based translation systems,such as feature-based translation and strong target language models, while also allowing flexible translation and movement based on hierarchical rules extracted from aligned parallel text.
</nextsent>
<nextsent>the approach has been widely adopted and reported to be competitive with other large-scale data driven approaches, e.g.
</nextsent>
<nextsent>(zollmann et al, 2008).<papid> C08-1144 </papid>large-scale hierarchical smt involves automatic rule extraction from aligned parallel text, model parameter estimation, and the use of cube pruning k-best list generation in hierarchical trans lation.</nextsent>
<nextsent>the number of hierarchical rules extracted far exceeds the number of phrase translations typically found in aligned text.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3611">
<title id=" E09-1044.xml">rule filtering by pattern for efficient hierarchical translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>hiero translation systems incorporate many of the strengths of phrase-based translation systems,such as feature-based translation and strong target language models, while also allowing flexible translation and movement based on hierarchical rules extracted from aligned parallel text.
</prevsent>
<prevsent>the approach has been widely adopted and reported to be competitive with other large-scale data driven approaches, e.g.
</prevsent>
</prevsection>
<citsent citstr=" C08-1144 ">
(zollmann et al, 2008).<papid> C08-1144 </papid>large-scale hierarchical smt involves automatic rule extraction from aligned parallel text, model parameter estimation, and the use of cube pruning k-best list generation in hierarchical trans lation.</citsent>
<aftsection>
<nextsent>the number of hierarchical rules extracted far exceeds the number of phrase translations typically found in aligned text.
</nextsent>
<nextsent>while this may lead to improved translation quality, there is also the risk of lengthened translation times and increased memory usage, along with possible search errors due to the pruning procedures needed in search.we describe several techniques to reduce memory usage and search errors in hierarchical translation.
</nextsent>
<nextsent>memory usage can be reduced in cube pruning (chiang, 2007) <papid> J07-2003 </papid>through smart memoiza tion, and spreading neighborhood exploration can be used to reduce search errors.</nextsent>
<nextsent>however, search errors can still remain even when implementing simple phrase-based translation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3613">
<title id=" E09-1044.xml">rule filtering by pattern for efficient hierarchical translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the number of hierarchical rules extracted far exceeds the number of phrase translations typically found in aligned text.
</prevsent>
<prevsent>while this may lead to improved translation quality, there is also the risk of lengthened translation times and increased memory usage, along with possible search errors due to the pruning procedures needed in search.we describe several techniques to reduce memory usage and search errors in hierarchical translation.
</prevsent>
</prevsection>
<citsent citstr=" J07-2003 ">
memory usage can be reduced in cube pruning (chiang, 2007) <papid> J07-2003 </papid>through smart memoiza tion, and spreading neighborhood exploration can be used to reduce search errors.</citsent>
<aftsection>
<nextsent>however, search errors can still remain even when implementing simple phrase-based translation.
</nextsent>
<nextsent>we describe shallow?
</nextsent>
<nextsent>search through hierarchical rules which greatly speeds translation without any effect on quality.
</nextsent>
<nextsent>we then describe techniques to analyze and reduce the set of hierarchical rules.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3616">
<title id=" E09-1044.xml">rule filtering by pattern for efficient hierarchical translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the search and rule pruning techniques described in the following sections add to growing literature of refinements to the hierarchical phrase based smt systems originally described by chiang (2005), <papid> P05-1033 </papid>by chiang (2007).<papid> J07-2003 </papid></prevsent>
<prevsent>subsequent work has addressed improvements and extensions to the search procedure itself, the extraction of the hierarchical rules needed for translation, and has also reported contrastive experiments with other smt architectures.</prevsent>
</prevsection>
<citsent citstr=" P07-1019 ">
hiero search refinements huang and chiang (2007) <papid> P07-1019 </papid>offer several refinements to cube pruning to improve translation speed.</citsent>
<aftsection>
<nextsent>venugopal et al(2007) <papid> N07-1063 </papid>introduce hiero variant with relaxed constraints for hypothesis recombination during pars ing; speed and results are comparable to those of cube pruning, as described by chiang (2007).<papid> J07-2003 </papid></nextsent>
<nextsent>liand khudanpur (2008) <papid> W08-0402 </papid>report significant improvements in translation speed by taking unseen ngrams into account within cube pruning to minimize language model requests.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3622">
<title id=" E09-1044.xml">rule filtering by pattern for efficient hierarchical translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>subsequent work has addressed improvements and extensions to the search procedure itself, the extraction of the hierarchical rules needed for translation, and has also reported contrastive experiments with other smt architectures.
</prevsent>
<prevsent>hiero search refinements huang and chiang (2007) <papid> P07-1019 </papid>offer several refinements to cube pruning to improve translation speed.</prevsent>
</prevsection>
<citsent citstr=" N07-1063 ">
venugopal et al(2007) <papid> N07-1063 </papid>introduce hiero variant with relaxed constraints for hypothesis recombination during pars ing; speed and results are comparable to those of cube pruning, as described by chiang (2007).<papid> J07-2003 </papid></citsent>
<aftsection>
<nextsent>liand khudanpur (2008) <papid> W08-0402 </papid>report significant improvements in translation speed by taking unseen ngrams into account within cube pruning to minimize language model requests.</nextsent>
<nextsent>dyer et al (2008) <papid> P08-1115 </papid>380extend the translation of source sentences to translation of input lattices following chappelier et al (1999).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3624">
<title id=" E09-1044.xml">rule filtering by pattern for efficient hierarchical translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>hiero search refinements huang and chiang (2007) <papid> P07-1019 </papid>offer several refinements to cube pruning to improve translation speed.</prevsent>
<prevsent>venugopal et al(2007) <papid> N07-1063 </papid>introduce hiero variant with relaxed constraints for hypothesis recombination during pars ing; speed and results are comparable to those of cube pruning, as described by chiang (2007).<papid> J07-2003 </papid></prevsent>
</prevsection>
<citsent citstr=" W08-0402 ">
liand khudanpur (2008) <papid> W08-0402 </papid>report significant improvements in translation speed by taking unseen ngrams into account within cube pruning to minimize language model requests.</citsent>
<aftsection>
<nextsent>dyer et al (2008) <papid> P08-1115 </papid>380extend the translation of source sentences to translation of input lattices following chappelier et al (1999).</nextsent>
<nextsent>extensions to hiero blunsom et al (2008)<papid> P08-1024 </papid>discuss procedures to combine discriminative latent models with hierarchical smt.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3625">
<title id=" E09-1044.xml">rule filtering by pattern for efficient hierarchical translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>venugopal et al(2007) <papid> N07-1063 </papid>introduce hiero variant with relaxed constraints for hypothesis recombination during pars ing; speed and results are comparable to those of cube pruning, as described by chiang (2007).<papid> J07-2003 </papid></prevsent>
<prevsent>liand khudanpur (2008) <papid> W08-0402 </papid>report significant improvements in translation speed by taking unseen ngrams into account within cube pruning to minimize language model requests.</prevsent>
</prevsection>
<citsent citstr=" P08-1115 ">
dyer et al (2008) <papid> P08-1115 </papid>380extend the translation of source sentences to translation of input lattices following chappelier et al (1999).</citsent>
<aftsection>
<nextsent>extensions to hiero blunsom et al (2008)<papid> P08-1024 </papid>discuss procedures to combine discriminative latent models with hierarchical smt.</nextsent>
<nextsent>the syntax augmented machine translation system (zoll mann and venugopal, 2006) <papid> W06-3119 </papid>incorporates target language syntactic constituents in addition to the synchronous grammars used in translation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3626">
<title id=" E09-1044.xml">rule filtering by pattern for efficient hierarchical translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>liand khudanpur (2008) <papid> W08-0402 </papid>report significant improvements in translation speed by taking unseen ngrams into account within cube pruning to minimize language model requests.</prevsent>
<prevsent>dyer et al (2008) <papid> P08-1115 </papid>380extend the translation of source sentences to translation of input lattices following chappelier et al (1999).</prevsent>
</prevsection>
<citsent citstr=" P08-1024 ">
extensions to hiero blunsom et al (2008)<papid> P08-1024 </papid>discuss procedures to combine discriminative latent models with hierarchical smt.</citsent>
<aftsection>
<nextsent>the syntax augmented machine translation system (zoll mann and venugopal, 2006) <papid> W06-3119 </papid>incorporates target language syntactic constituents in addition to the synchronous grammars used in translation.</nextsent>
<nextsent>shen at al.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3627">
<title id=" E09-1044.xml">rule filtering by pattern for efficient hierarchical translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>dyer et al (2008) <papid> P08-1115 </papid>380extend the translation of source sentences to translation of input lattices following chappelier et al (1999).</prevsent>
<prevsent>extensions to hiero blunsom et al (2008)<papid> P08-1024 </papid>discuss procedures to combine discriminative latent models with hierarchical smt.</prevsent>
</prevsection>
<citsent citstr=" W06-3119 ">
the syntax augmented machine translation system (zoll mann and venugopal, 2006) <papid> W06-3119 </papid>incorporates target language syntactic constituents in addition to the synchronous grammars used in translation.</citsent>
<aftsection>
<nextsent>shen at al.
</nextsent>
<nextsent>(2008) make use of target dependency trees and target dependency language model duringdecoding.
</nextsent>
<nextsent>marton and resnik (2008) <papid> P08-1114 </papid>exploit shallow correspondences of hierarchical rules with source syntactic constituents extracted from parallel text, an approach also investigated by chiang(2005).<papid> P05-1033 </papid></nextsent>
<nextsent>zhang and gildea (2006) propose bina riz ation for synchronous grammars as means to control search complexity arising from more complex, syntactic, hierarchical rules sets.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3628">
<title id=" E09-1044.xml">rule filtering by pattern for efficient hierarchical translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>shen at al.
</prevsent>
<prevsent>(2008) make use of target dependency trees and target dependency language model duringdecoding.
</prevsent>
</prevsection>
<citsent citstr=" P08-1114 ">
marton and resnik (2008) <papid> P08-1114 </papid>exploit shallow correspondences of hierarchical rules with source syntactic constituents extracted from parallel text, an approach also investigated by chiang(2005).<papid> P05-1033 </papid></citsent>
<aftsection>
<nextsent>zhang and gildea (2006) propose bina riz ation for synchronous grammars as means to control search complexity arising from more complex, syntactic, hierarchical rules sets.
</nextsent>
<nextsent>hierarchical rule extraction zhang et al (2008) <papid> C08-1136 </papid>describe linear algorithm, modified version of shift-reduce, to extract phrase pairs organized into tree from which hierarchical rules can be directly extracted.</nextsent>
<nextsent>lopez (2007) <papid> D07-1104 </papid>extracts rules on-the-flyfrom the training bitext during decoding, searching efficiently for rule patterns using suffix arrays.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3630">
<title id=" E09-1044.xml">rule filtering by pattern for efficient hierarchical translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>marton and resnik (2008) <papid> P08-1114 </papid>exploit shallow correspondences of hierarchical rules with source syntactic constituents extracted from parallel text, an approach also investigated by chiang(2005).<papid> P05-1033 </papid></prevsent>
<prevsent>zhang and gildea (2006) propose bina riz ation for synchronous grammars as means to control search complexity arising from more complex, syntactic, hierarchical rules sets.</prevsent>
</prevsection>
<citsent citstr=" C08-1136 ">
hierarchical rule extraction zhang et al (2008) <papid> C08-1136 </papid>describe linear algorithm, modified version of shift-reduce, to extract phrase pairs organized into tree from which hierarchical rules can be directly extracted.</citsent>
<aftsection>
<nextsent>lopez (2007) <papid> D07-1104 </papid>extracts rules on-the-flyfrom the training bitext during decoding, searching efficiently for rule patterns using suffix arrays.</nextsent>
<nextsent>analysis and contrastive experiments zollman et al (2008) compare phrase-based, hierarchical and syntax-augmented decoders for translation of arabic, chinese, and urdu into english, and they find that attempts to expedite translation by simple schemes which discard rules also degrade translation performance.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3631">
<title id=" E09-1044.xml">rule filtering by pattern for efficient hierarchical translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>zhang and gildea (2006) propose bina riz ation for synchronous grammars as means to control search complexity arising from more complex, syntactic, hierarchical rules sets.
</prevsent>
<prevsent>hierarchical rule extraction zhang et al (2008) <papid> C08-1136 </papid>describe linear algorithm, modified version of shift-reduce, to extract phrase pairs organized into tree from which hierarchical rules can be directly extracted.</prevsent>
</prevsection>
<citsent citstr=" D07-1104 ">
lopez (2007) <papid> D07-1104 </papid>extracts rules on-the-flyfrom the training bitext during decoding, searching efficiently for rule patterns using suffix arrays.</citsent>
<aftsection>
<nextsent>analysis and contrastive experiments zollman et al (2008) compare phrase-based, hierarchical and syntax-augmented decoders for translation of arabic, chinese, and urdu into english, and they find that attempts to expedite translation by simple schemes which discard rules also degrade translation performance.
</nextsent>
<nextsent>lopez (2008) <papid> C08-1064 </papid>explores whether lexical reordering or the phrase dis contiguity inherent in hierarchical rules explains improvements over phrase-based systems.</nextsent>
<nextsent>hierarchical translation has also been used to great effect in combination with other translation architectures (e.g.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3632">
<title id=" E09-1044.xml">rule filtering by pattern for efficient hierarchical translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>lopez (2007) <papid> D07-1104 </papid>extracts rules on-the-flyfrom the training bitext during decoding, searching efficiently for rule patterns using suffix arrays.</prevsent>
<prevsent>analysis and contrastive experiments zollman et al (2008) compare phrase-based, hierarchical and syntax-augmented decoders for translation of arabic, chinese, and urdu into english, and they find that attempts to expedite translation by simple schemes which discard rules also degrade translation performance.</prevsent>
</prevsection>
<citsent citstr=" C08-1064 ">
lopez (2008) <papid> C08-1064 </papid>explores whether lexical reordering or the phrase dis contiguity inherent in hierarchical rules explains improvements over phrase-based systems.</citsent>
<aftsection>
<nextsent>hierarchical translation has also been used to great effect in combination with other translation architectures (e.g.
</nextsent>
<nextsent>(sim et al, 2007; rosti et al, 2007)).<papid> N07-1029 </papid></nextsent>
<nextsent>1.2 outline.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3633">
<title id=" E09-1044.xml">rule filtering by pattern for efficient hierarchical translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>lopez (2008) <papid> C08-1064 </papid>explores whether lexical reordering or the phrase dis contiguity inherent in hierarchical rules explains improvements over phrase-based systems.</prevsent>
<prevsent>hierarchical translation has also been used to great effect in combination with other translation architectures (e.g.</prevsent>
</prevsection>
<citsent citstr=" N07-1029 ">
(sim et al, 2007; rosti et al, 2007)).<papid> N07-1029 </papid></citsent>
<aftsection>
<nextsent>1.2 outline.
</nextsent>
<nextsent>the paper proceeds as follows.
</nextsent>
<nextsent>section 2 describes memo ization and spreading neighborhood exploration in cube pruning intended to reduce memory usage and search errors, respectively.
</nextsent>
<nextsent>a detailed comparison with simple phrase-basedsystem is presented.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3651">
<title id=" E09-1044.xml">rule filtering by pattern for efficient hierarchical translation </title>
<section> two refinements in cube pruning.  </section>
<citcontext>
<prevsection>
<prevsent>spreading neighborhood exploration adds candidates to the frontier queue.
</prevsent>
<prevsent>count features inspired by bender et al (2007).
</prevsent>
</prevsection>
<citsent citstr=" P03-1021 ">
met (och, 2003) <papid> P03-1021 </papid>iterative parameter estimation under ibm bleu is performed on the development set.</citsent>
<aftsection>
<nextsent>the english language used model is 4-gram estimated over the parallel text and 965 million word subset of monolingual data from the english gigaword third edition.
</nextsent>
<nextsent>in addition to themt08 set itself, we use development set mt02 05-tune formed from the odd numbered sentences of the nist mt02 through mt05 evaluation sets; the even numbered sentences form the validation set mt02-05-test.
</nextsent>
<nextsent>the mt02-05-tune set has 2,075 sentences.
</nextsent>
<nextsent>we first compare the cube pruning decoder to the ttm (kumar et al, 2006), phrase-basedsmt system implemented with weighted finite state tansducers (allauzen et al, 2007).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3652">
<title id=" E09-1044.xml">rule filtering by pattern for efficient hierarchical translation </title>
<section> two refinements in cube pruning.  </section>
<citcontext>
<prevsection>
<prevsent>the mt02-05-tune set has 2,075 sentences.
</prevsent>
<prevsent>we first compare the cube pruning decoder to the ttm (kumar et al, 2006), phrase-basedsmt system implemented with weighted finite state tansducers (allauzen et al, 2007).
</prevsent>
</prevsection>
<citsent citstr=" H05-1021 ">
the system implements either monotone phrase order translation, or an mj1 (maximum phrase jump of 1) reordering model (kumar and byrne, 2005).<papid> H05-1021 </papid></citsent>
<aftsection>
<nextsent>relative to the complex movement and translation allowed by hiero and other models, mj1 is clearly inferior (dreyer et al, 2007); <papid> W07-0414 </papid>mj1 was developed with efficiency in mind so as to run with minimum of search errors in translation and to be easily and exactly realized via wfsts.</nextsent>
<nextsent>even for the 382 large models used in an evaluation task, the ttm system is reported to run largely without pruning (blackwood et al, 2008).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3653">
<title id=" E09-1044.xml">rule filtering by pattern for efficient hierarchical translation </title>
<section> two refinements in cube pruning.  </section>
<citcontext>
<prevsection>
<prevsent>we first compare the cube pruning decoder to the ttm (kumar et al, 2006), phrase-basedsmt system implemented with weighted finite state tansducers (allauzen et al, 2007).
</prevsent>
<prevsent>the system implements either monotone phrase order translation, or an mj1 (maximum phrase jump of 1) reordering model (kumar and byrne, 2005).<papid> H05-1021 </papid></prevsent>
</prevsection>
<citsent citstr=" W07-0414 ">
relative to the complex movement and translation allowed by hiero and other models, mj1 is clearly inferior (dreyer et al, 2007); <papid> W07-0414 </papid>mj1 was developed with efficiency in mind so as to run with minimum of search errors in translation and to be easily and exactly realized via wfsts.</citsent>
<aftsection>
<nextsent>even for the 382 large models used in an evaluation task, the ttm system is reported to run largely without pruning (blackwood et al, 2008).
</nextsent>
<nextsent>the hiero decoder can easily be made to implement mj1 reordering by allowing only restricted set of reordering rules in addition to the usual glue rule, as shown in left-hand column of table 1, where is the set of terminals.
</nextsent>
<nextsent>constraining hiero in this way makes it possible to compare its performance to the exact wfst ttm implementation and to identify any search errors made by hiero.
</nextsent>
<nextsent>table 2 shows the lower cased ibm bleu scores obtained by the systems for mt02-05-tune with monotone and reordered search, and with met-optimised parameters for mj1 reordering.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3658">
<title id=" E09-1044.xml">rule filtering by pattern for efficient hierarchical translation </title>
<section> rule filtering by pattern.  </section>
<citcontext>
<prevsection>
<prevsent>the question is whether all these rules are needed for translation.
</prevsent>
<prevsent>if the rule set can be reduced without reducing translation quality, both memory efficiency and translation speed can beincreased.
</prevsent>
</prevsection>
<citsent citstr=" P08-1066 ">
previously published approaches to reducing the rule set include: enforcing minimum span of two words per non-terminal (lopez, 2008), <papid> C08-1064 </papid>which would reduce our set to 115m rules;or minimum count (mincount) threshold (zoll mann et al, 2008), <papid> C08-1144 </papid>which would reduce our set to 78m (mincount=2) or 57m (mincount=3) rules.shen et al (2008) <papid> P08-1066 </papid>describe the result of filtering rules by insisting that target-side rules are well-formed dependency trees.</citsent>
<aftsection>
<nextsent>this reduces their rule set from 140m to 26m rules.
</nextsent>
<nextsent>this filtering leads to degradation in translation performance (see table 2 of shen et al (2008)), <papid> P08-1066 </papid>which they counter by adding dependency lm in translation.</nextsent>
<nextsent>as another reference point, chiang (2007) <papid> J07-2003 </papid>reports chinese-to-english translation experiments based on 5.5m rules.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3666">
<title id=" E09-1044.xml">rule filtering by pattern for efficient hierarchical translation </title>
<section> rule filtering by pattern.  </section>
<citcontext>
<prevsection>
<prevsent>finally, in this section we report results of our shallow hierarchical system with the 2.5 mincount=1 configuration from table 7, after including the following n-best list rescoring steps.?
</prevsent>
<prevsent>large-lm rescoring.
</prevsent>
</prevsection>
<citsent citstr=" D07-1090 ">
we build sentence specific zero-cutoff stupid-backoff (brants et al., 2007) <papid> D07-1090 </papid>5-gram language models, estimated using 4.7b words of english newswire text, and apply them to rescore each 10000-best list.</citsent>
<aftsection>
<nextsent>minimum bayes risk (mbr).
</nextsent>
<nextsent>we then rescore the first 1000-best hypotheses with mbr, taking the negative sentence level bleu score as the loss function to minimise (ku mar and byrne, 2004).<papid> N04-1022 </papid>table 8 shows results for mt02-05-tune, mt0205-test, the nist subsets from the mt06 evaluation (mt06-nist-nw for newswire data and mt06 nist-ng for newsgroup) and mt08, as measured by lower cased ibm bleu and ter (snover et al, 2006).</nextsent>
<nextsent>mixed case nist bleu for this system onmt08 is 42.5.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3667">
<title id=" E09-1044.xml">rule filtering by pattern for efficient hierarchical translation </title>
<section> rule filtering by pattern.  </section>
<citcontext>
<prevsection>
<prevsent>we build sentence specific zero-cutoff stupid-backoff (brants et al., 2007) <papid> D07-1090 </papid>5-gram language models, estimated using 4.7b words of english newswire text, and apply them to rescore each 10000-best list.</prevsent>
<prevsent>minimum bayes risk (mbr).</prevsent>
</prevsection>
<citsent citstr=" N04-1022 ">
we then rescore the first 1000-best hypotheses with mbr, taking the negative sentence level bleu score as the loss function to minimise (ku mar and byrne, 2004).<papid> N04-1022 </papid>table 8 shows results for mt02-05-tune, mt0205-test, the nist subsets from the mt06 evaluation (mt06-nist-nw for newswire data and mt06 nist-ng for newsgroup) and mt08, as measured by lower cased ibm bleu and ter (snover et al, 2006).</citsent>
<aftsection>
<nextsent>mixed case nist bleu for this system onmt08 is 42.5.
</nextsent>
<nextsent>this is directly comparable to official mt08 evaluation results1.
</nextsent>
<nextsent>this paper focuses on efficient large-scale hierarchical translation while maintaining good translation quality.
</nextsent>
<nextsent>smart memo ization and spreading neighborhood exploration during cube pruning are described and shown to reduce memory consumption and hiero search errors using simple phrase based system as contrast.we then define general classification of hierarchical rules, based on their number of nonterminals, elements and their patterns, for refined extraction and filtering.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3671">
<title id=" E09-1021.xml">translation and extension of concepts across languages </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the two mainalgorithmic approaches are pattern-based discovery, and clustering of context feature vectors.
</prevsent>
<prevsent>the latter represents word contexts as vectors in some space and use similarity measures and automatic clustering in that space (deerwester et al, 1990).
</prevsent>
</prevsection>
<citsent citstr=" P98-2127 ">
pereira (1993), curran (2002) and lin (1998) <papid> P98-2127 </papid>use syntactic features in the vector definition.</citsent>
<aftsection>
<nextsent>(panteland lin, 2002) improves on the latter by clustering by committee.
</nextsent>
<nextsent>caraballo (1999) <papid> P99-1016 </papid>uses conjunction and appositive annotations in the vector rep resentation.</nextsent>
<nextsent>while great effort has focused on improving the computational complexity of these methods (gorman and curran, 2006), <papid> P06-1046 </papid>they still remain data and computation intensive.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3672">
<title id=" E09-1021.xml">translation and extension of concepts across languages </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>pereira (1993), curran (2002) and lin (1998) <papid> P98-2127 </papid>use syntactic features in the vector definition.</prevsent>
<prevsent>(panteland lin, 2002) improves on the latter by clustering by committee.</prevsent>
</prevsection>
<citsent citstr=" P99-1016 ">
caraballo (1999) <papid> P99-1016 </papid>uses conjunction and appositive annotations in the vector rep resentation.</citsent>
<aftsection>
<nextsent>while great effort has focused on improving the computational complexity of these methods (gorman and curran, 2006), <papid> P06-1046 </papid>they still remain data and computation intensive.</nextsent>
<nextsent>the current major algorithmic approach for concept acquisition is to use lexico-syntactic pat terns.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3673">
<title id=" E09-1021.xml">translation and extension of concepts across languages </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>(panteland lin, 2002) improves on the latter by clustering by committee.
</prevsent>
<prevsent>caraballo (1999) <papid> P99-1016 </papid>uses conjunction and appositive annotations in the vector rep resentation.</prevsent>
</prevsection>
<citsent citstr=" P06-1046 ">
while great effort has focused on improving the computational complexity of these methods (gorman and curran, 2006), <papid> P06-1046 </papid>they still remain data and computation intensive.</citsent>
<aftsection>
<nextsent>the current major algorithmic approach for concept acquisition is to use lexico-syntactic patterns.
</nextsent>
<nextsent>patterns have been shown to produce more accurate results than feature vectors, at lower computational cost on large corpora (pantel et al,2004).<papid> C04-1111 </papid></nextsent>
<nextsent>since (hearst, 1992), <papid> C92-2082 </papid>who used manually prepared set of initial lexical patterns in order to acquire relationships, numerous pattern-based methods have been proposed for the discovery of concepts from seeds (pantel et al, 2004; <papid> C04-1111 </papid>davidov et al, 2007; <papid> P07-1030 </papid>pasca et al, 2006).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3674">
<title id=" E09-1021.xml">translation and extension of concepts across languages </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>while great effort has focused on improving the computational complexity of these methods (gorman and curran, 2006), <papid> P06-1046 </papid>they still remain data and computation intensive.</prevsent>
<prevsent>the current major algorithmic approach for concept acquisition is to use lexico-syntactic pat terns.</prevsent>
</prevsection>
<citsent citstr=" C04-1111 ">
patterns have been shown to produce more accurate results than feature vectors, at lower computational cost on large corpora (pantel et al,2004).<papid> C04-1111 </papid></citsent>
<aftsection>
<nextsent>since (hearst, 1992), <papid> C92-2082 </papid>who used manually prepared set of initial lexical patterns in order to acquire relationships, numerous pattern-based methods have been proposed for the discovery of concepts from seeds (pantel et al, 2004; <papid> C04-1111 </papid>davidov et al, 2007; <papid> P07-1030 </papid>pasca et al, 2006).</nextsent>
<nextsent>most of these studies were done for english, while some show the applicability of their method to some other languages including russian, greek, czech and french.many papers directly target specific applications, and build lexical resources as side ef fect.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3675">
<title id=" E09-1021.xml">translation and extension of concepts across languages </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the current major algorithmic approach for concept acquisition is to use lexico-syntactic patterns.
</prevsent>
<prevsent>patterns have been shown to produce more accurate results than feature vectors, at lower computational cost on large corpora (pantel et al,2004).<papid> C04-1111 </papid></prevsent>
</prevsection>
<citsent citstr=" C92-2082 ">
since (hearst, 1992), <papid> C92-2082 </papid>who used manually prepared set of initial lexical patterns in order to acquire relationships, numerous pattern-based methods have been proposed for the discovery of concepts from seeds (pantel et al, 2004; <papid> C04-1111 </papid>davidov et al, 2007; <papid> P07-1030 </papid>pasca et al, 2006).</citsent>
<aftsection>
<nextsent>most of these studies were done for english, while some show the applicability of their method to some other languages including russian, greek, czech and french.many papers directly target specific applications, and build lexical resources as side effect.
</nextsent>
<nextsent>named entity recognition can be viewed as an instance of the concept acquisition problem where the desired categories contain words that are names of entities of particular kind, as donein (freitag, 2004) <papid> W04-3234 </papid>using co-clustering and in (et zioni et al, 2005) using predefined pattern types.many information extraction papers discover relationships between words using syntactic patterns (riloff and jones, 1999).</nextsent>
<nextsent>unlike in the majority of recent studies where the acquisition framework is designed with specific languages in mind, in our task the algorithm should be able to deal well with wide variety of target languages without any significant manualadaptations.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3677">
<title id=" E09-1021.xml">translation and extension of concepts across languages </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the current major algorithmic approach for concept acquisition is to use lexico-syntactic patterns.
</prevsent>
<prevsent>patterns have been shown to produce more accurate results than feature vectors, at lower computational cost on large corpora (pantel et al,2004).<papid> C04-1111 </papid></prevsent>
</prevsection>
<citsent citstr=" P07-1030 ">
since (hearst, 1992), <papid> C92-2082 </papid>who used manually prepared set of initial lexical patterns in order to acquire relationships, numerous pattern-based methods have been proposed for the discovery of concepts from seeds (pantel et al, 2004; <papid> C04-1111 </papid>davidov et al, 2007; <papid> P07-1030 </papid>pasca et al, 2006).</citsent>
<aftsection>
<nextsent>most of these studies were done for english, while some show the applicability of their method to some other languages including russian, greek, czech and french.many papers directly target specific applications, and build lexical resources as side effect.
</nextsent>
<nextsent>named entity recognition can be viewed as an instance of the concept acquisition problem where the desired categories contain words that are names of entities of particular kind, as donein (freitag, 2004) <papid> W04-3234 </papid>using co-clustering and in (et zioni et al, 2005) using predefined pattern types.many information extraction papers discover relationships between words using syntactic patterns (riloff and jones, 1999).</nextsent>
<nextsent>unlike in the majority of recent studies where the acquisition framework is designed with specific languages in mind, in our task the algorithm should be able to deal well with wide variety of target languages without any significant manualadaptations.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3680">
<title id=" E09-1021.xml">translation and extension of concepts across languages </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>since (hearst, 1992), <papid> C92-2082 </papid>who used manually prepared set of initial lexical patterns in order to acquire relationships, numerous pattern-based methods have been proposed for the discovery of concepts from seeds (pantel et al, 2004; <papid> C04-1111 </papid>davidov et al, 2007; <papid> P07-1030 </papid>pasca et al, 2006).</prevsent>
<prevsent>most of these studies were done for english, while some show the applicability of their method to some other languages including russian, greek, czech and french.many papers directly target specific applications, and build lexical resources as side ef fect.</prevsent>
</prevsection>
<citsent citstr=" W04-3234 ">
named entity recognition can be viewed as an instance of the concept acquisition problem where the desired categories contain words that are names of entities of particular kind, as donein (freitag, 2004) <papid> W04-3234 </papid>using co-clustering and in (et zioni et al, 2005) using predefined pattern types.many information extraction papers discover relationships between words using syntactic patterns (riloff and jones, 1999).</citsent>
<aftsection>
<nextsent>unlike in the majority of recent studies where the acquisition framework is designed with specific languages in mind, in our task the algorithm should be able to deal well with wide variety of target languages without any significant manualadaptations.
</nextsent>
<nextsent>while some of the proposed frameworks could potentially be language-independent, little research has been done to confirm it yet.
</nextsent>
<nextsent>176there are few obstacles that may hinder applying common pattern-based methods to other languages.
</nextsent>
<nextsent>many studies utilize parsing or pos tagging, which frequently depends on the availability and quality of language-specific tools.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3684">
<title id=" E09-1021.xml">translation and extension of concepts across languages </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>a few recently proposed concept acquisition methods require only handful of seed words (davidov et al, 2007; <papid> P07-1030 </papid>pasca and van durme,2008).</prevsent>
<prevsent>while these studies avoid some of the obstacles above, it still remains unconfirmed whether such methods are indeed language-independent.</prevsent>
</prevsection>
<citsent citstr=" P06-1038 ">
in the concept extension part of our algorithm weadapt our concept acquisition framework (davi dov and rappoport, 2006; <papid> P06-1038 </papid>davidov et al, 2007; <papid> P07-1030 </papid>davidov and rappoport, 2008<papid> P08-1079 </papid>a; davidov and rappoport, 2008<papid> P08-1079 </papid>b) to suit diverse languages, including ones without explicit word segmentation.</citsent>
<aftsection>
<nextsent>in our evaluation we confirm the applicability of the adapted methods to 45 languages.our study is related to cross-language information retrieval (clir/clef) frameworks.
</nextsent>
<nextsent>both deal with information extracted from set of languages.
</nextsent>
<nextsent>however, the majority of clir studies pursue different targets.
</nextsent>
<nextsent>one of the main clir goals is the retrieval of documents basedon explicit queries, when the document language is not the query language (volk and buitelaar, 2002).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3688">
<title id=" E09-1021.xml">translation and extension of concepts across languages </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>a few recently proposed concept acquisition methods require only handful of seed words (davidov et al, 2007; <papid> P07-1030 </papid>pasca and van durme,2008).</prevsent>
<prevsent>while these studies avoid some of the obstacles above, it still remains unconfirmed whether such methods are indeed language-independent.</prevsent>
</prevsection>
<citsent citstr=" P08-1079 ">
in the concept extension part of our algorithm weadapt our concept acquisition framework (davi dov and rappoport, 2006; <papid> P06-1038 </papid>davidov et al, 2007; <papid> P07-1030 </papid>davidov and rappoport, 2008<papid> P08-1079 </papid>a; davidov and rappoport, 2008<papid> P08-1079 </papid>b) to suit diverse languages, including ones without explicit word segmentation.</citsent>
<aftsection>
<nextsent>in our evaluation we confirm the applicability of the adapted methods to 45 languages.our study is related to cross-language information retrieval (clir/clef) frameworks.
</nextsent>
<nextsent>both deal with information extracted from set of languages.
</nextsent>
<nextsent>however, the majority of clir studies pursue different targets.
</nextsent>
<nextsent>one of the main clir goals is the retrieval of documents basedon explicit queries, when the document language is not the query language (volk and buitelaar, 2002).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3692">
<title id=" E09-1021.xml">translation and extension of concepts across languages </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>while our goals are different from clir, clir systems can greatly benefit from our framework, since our translated categories can be directly utilized for subsequent document retrieval.
</prevsent>
<prevsent>another field indirectly related to our research is machine translation (mt).
</prevsent>
</prevsection>
<citsent citstr=" W01-0504 ">
many mt tasks require automated creation or improvement of dictionaries (koehn and knight, 2001).<papid> W01-0504 </papid></citsent>
<aftsection>
<nextsent>however,mt mainly deals with translation and disambiguation of words at the sentence or document level,while we translate whole concepts defined independently of contexts.
</nextsent>
<nextsent>our primary target is not translation of given words, but the discovery and extension of concept in target language when the concept definition is given in some different source language.
</nextsent>
<nextsent>framework our framework has three main stages: (1) given set of words in source language as definition for some concept, we automatically translate themto the target language with multilingual dictionaries, disambiguating translations using web counts; (2) we retrieve from the web snippets where these translations co-appear; (3) we apply pattern based concept extension algorithm for discovering additional terms from the retrieved data.
</nextsent>
<nextsent>3.1 concept words and sense selection.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3694">
<title id=" E09-1021.xml">translation and extension of concepts across languages </title>
<section> cross-lingual concept translation.  </section>
<citcontext>
<prevsection>
<prevsent>prefix and post fix are limited to contain punctuation characters and/or limmwe words.
</prevsent>
<prevsent>terms of the same concept frequently co-appear in lists.
</prevsent>
</prevsection>
<citsent citstr=" C02-1114 ">
to utilize this, we introduce two additional list pattern types7: [prefix] c1[infix] (ci[infix])+ (1) [infix] (ci[infix])+ cn [postfix] (2) as in (widdows and dorow, 2002; <papid> C02-1114 </papid>davidov and rappoport, 2006), <papid> P06-1038 </papid>we define pattern graph.</citsent>
<aftsection>
<nextsent>nodes correspond to terms and patterns to edges.
</nextsent>
<nextsent>if term pair (w1, w2) appears in pattern , we add nodes nw1 , nw2 to the graph and directed edge ep (nw1 , nw2) between them.
</nextsent>
<nextsent>6as before, for languages without explicit space-basedword separation limmwe limits the number of characters instead.
</nextsent>
<nextsent>7(x)+ means one or more instances of . 178 3.3.2 symmetric patterns we consider only symmetric patterns.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3706">
<title id=" E09-1087.xml">semi supervised training for the averaged perceptron pos tagger </title>
<section> abstract </section>
<citcontext>
<prevsection>


</prevsection>
<citsent citstr=" W02-1001 ">
this paper describes pos tagging experiments with semi-supervised training as an extension to the (supervised) averaged perceptron algorithm, first introduced for this task by (collins, 2002).<papid> W02-1001 </papid></citsent>
<aftsection>
<nextsent>experiments with an iterative training on standard-sized supervised (manually annotated) dataset (106 tokens) combined with relatively modest (in the order of 108 tokens) unsupervised (plain) data in bagging-like fashion showed significant improvement of the pos classification task on typo logically different languages, yielding better than state-of-the-art results for english and czech (4.12 % and 4.86 % relative error reduction, respectively; absolute accuracies being 97.44 % and 95.89 %).
</nextsent>
<nextsent>since 2002, we have seen renewed interest in improving pos tagging results for english, and an inflow of results (initial or improved) for many other languages.
</nextsent>
<nextsent>for english, after relatively big jump achieved by (collins, 2002), <papid> W02-1001 </papid>we have seen two significant improvements: (toutanova et al, 2003) <papid> N03-1033 </papid>and (shen et al, 2007) <papid> P07-1096 </papid>pushed the results by significant amount each time.11in our final comparison, we have also included the results of (gimenez and ma`rquez, 2004), because it has surpassed (collins, 2002) <papid> W02-1001 </papid>as well and we have used this tagger in the data preparation phase.</nextsent>
<nextsent>see more details below.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3711">
<title id=" E09-1087.xml">semi supervised training for the averaged perceptron pos tagger </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>experiments with an iterative training on standard-sized supervised (manually annotated) dataset (106 tokens) combined with relatively modest (in the order of 108 tokens) unsupervised (plain) data in bagging-like fashion showed significant improvement of the pos classification task on typo logically different languages, yielding better than state-of-the-art results for english and czech (4.12 % and 4.86 % relative error reduction, respectively; absolute accuracies being 97.44 % and 95.89 %).
</prevsent>
<prevsent>since 2002, we have seen renewed interest in improving pos tagging results for english, and an inflow of results (initial or improved) for many other languages.
</prevsent>
</prevsection>
<citsent citstr=" N03-1033 ">
for english, after relatively big jump achieved by (collins, 2002), <papid> W02-1001 </papid>we have seen two significant improvements: (toutanova et al, 2003) <papid> N03-1033 </papid>and (shen et al, 2007) <papid> P07-1096 </papid>pushed the results by significant amount each time.11in our final comparison, we have also included the results of (gimenez and ma`rquez, 2004), because it has surpassed (collins, 2002) <papid> W02-1001 </papid>as well and we have used this tagger in the data preparation phase.</citsent>
<aftsection>
<nextsent>see more details below.
</nextsent>
<nextsent>most recently, (suzuki and isozaki, 2008) <papid> P08-1076 </papid>published their semi-supervised sequential labelling method, whose results on pos tagging seem to be optically better than (shen et al, 2007), <papid> P07-1096 </papid>but no significance tests were given and the tool is not available for download, i.e. for repeating the results and significance testing.</nextsent>
<nextsent>thus, we compare our results only to the tools listed above.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3716">
<title id=" E09-1087.xml">semi supervised training for the averaged perceptron pos tagger </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>experiments with an iterative training on standard-sized supervised (manually annotated) dataset (106 tokens) combined with relatively modest (in the order of 108 tokens) unsupervised (plain) data in bagging-like fashion showed significant improvement of the pos classification task on typo logically different languages, yielding better than state-of-the-art results for english and czech (4.12 % and 4.86 % relative error reduction, respectively; absolute accuracies being 97.44 % and 95.89 %).
</prevsent>
<prevsent>since 2002, we have seen renewed interest in improving pos tagging results for english, and an inflow of results (initial or improved) for many other languages.
</prevsent>
</prevsection>
<citsent citstr=" P07-1096 ">
for english, after relatively big jump achieved by (collins, 2002), <papid> W02-1001 </papid>we have seen two significant improvements: (toutanova et al, 2003) <papid> N03-1033 </papid>and (shen et al, 2007) <papid> P07-1096 </papid>pushed the results by significant amount each time.11in our final comparison, we have also included the results of (gimenez and ma`rquez, 2004), because it has surpassed (collins, 2002) <papid> W02-1001 </papid>as well and we have used this tagger in the data preparation phase.</citsent>
<aftsection>
<nextsent>see more details below.
</nextsent>
<nextsent>most recently, (suzuki and isozaki, 2008) <papid> P08-1076 </papid>published their semi-supervised sequential labelling method, whose results on pos tagging seem to be optically better than (shen et al, 2007), <papid> P07-1096 </papid>but no significance tests were given and the tool is not available for download, i.e. for repeating the results and significance testing.</nextsent>
<nextsent>thus, we compare our results only to the tools listed above.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3723">
<title id=" E09-1087.xml">semi supervised training for the averaged perceptron pos tagger </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for english, after relatively big jump achieved by (collins, 2002), <papid> W02-1001 </papid>we have seen two significant improvements: (toutanova et al, 2003) <papid> N03-1033 </papid>and (shen et al, 2007) <papid> P07-1096 </papid>pushed the results by significant amount each time.11in our final comparison, we have also included the results of (gimenez and ma`rquez, 2004), because it has surpassed (collins, 2002) <papid> W02-1001 </papid>as well and we have used this tagger in the data preparation phase.</prevsent>
<prevsent>see more details below.</prevsent>
</prevsection>
<citsent citstr=" P08-1076 ">
most recently, (suzuki and isozaki, 2008) <papid> P08-1076 </papid>published their semi-supervised sequential labelling method, whose results on pos tagging seem to be optically better than (shen et al, 2007), <papid> P07-1096 </papid>but no significance tests were given and the tool is not available for download, i.e. for repeating the results and significance testing.</citsent>
<aftsection>
<nextsent>thus, we compare our results only to the tools listed above.
</nextsent>
<nextsent>even though an improvement in pos tagging might be questionable enterprise (given that its effects on other tasks, such as parsing or other nlp problems are less than clearat least for en glish), it is still an interesting problem.
</nextsent>
<nextsent>moreover, the ideal2 situation of having single algorithm(and its implementation) for many (if not all) languages has not been reached yet.
</nextsent>
<nextsent>we have chosen collins?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3726">
<title id=" E09-1087.xml">semi supervised training for the averaged perceptron pos tagger </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>perceptron, appropriately extended, for two typo logically different languages: english and czech.
</prevsent>
<prevsent>it is clear however that the features (feature templates) that the taggers use are still language-dependent.one of the goals is also to have fast implementation for tagging large amounts of dataquickly.
</prevsent>
</prevsection>
<citsent citstr=" P98-1029 ">
we have experimented with various classifier combination methods, such as those described in (brill and wu, 1998) <papid> P98-1029 </papid>or (van halteren et al., 2001), and got improved results, as expected.</citsent>
<aftsection>
<nextsent>however, we view this only as side effect (yet, positive one)our goal was to stay on the turf of single taggers, which are both the common ground for competing on tagger accuracy today and also significantly faster at runtime.3 nevertheless, we have found that it is advantageous to use them to(pre-)tag the large amounts of plain text data dur 2we mean easy to use for further research on problems requiring pos tagging, especially multilingual ones.3and much easier to (re)implement as libraries in prototype systems, which is often difficult if not impossible with other peoples code.
</nextsent>
<nextsent>763 training data size (thousands of tokens) accuracy on evel opment ata 100 200 300 400 500 600 700 800 900 96.0 96.5 97.0 97.5 98.0figure 1: accuracy of the original averaged perceptron, supervised training on ptb/wsj (en glish) ing the training phase.
</nextsent>
<nextsent>apart from feeding the perceptron by various mixtures of manually tagged (supervised?)
</nextsent>
<nextsent>and auto-tagged (unsupervised?)4 data, we have also used various feature templates extensively; forex ample, we use lexicalization (with the added twist of lemmatization, useful especially for czech, aninflectionally rich language), manual?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3772">
<title id=" E09-1087.xml">semi supervised training for the averaged perceptron pos tagger </title>
<section> the (un)supervised training setup.  </section>
<citcontext>
<prevsection>
<prevsent>15accuracy?
</prevsent>
<prevsent>means accuracy of the semi-supervised method using this tagger for pre-tagging the unsupervised data, not the accuracy of the tagger itself.
</prevsent>
</prevsection>
<citsent citstr=" A00-1031 ">
16in fact, we have experimented with other tagger combinations and configurations as well with the tnt(brants, 2000), <papid> A00-1031 </papid>maxent (ratnaparkhi, 1996) <papid> W96-0213 </papid>and treetag ger (schmid, 1994), with or without the morce tagger in the pack; see below for the winning combination.17this patch is available on the papers website (see section 7).</citsent>
<aftsection>
<nextsent>767 three methods of unsupervised data selection, i.e. generating the unsupervised data chunks for each training iteration from the ,,pool?
</nextsent>
<nextsent>of sentences.
</nextsent>
<nextsent>these methods are: simple sequential chopping, randomized data selection with replacement and randomized selection without replacement.
</nextsent>
<nextsent>table 5 demonstrates that there is practically no difference in the results.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3773">
<title id=" E09-1087.xml">semi supervised training for the averaged perceptron pos tagger </title>
<section> the (un)supervised training setup.  </section>
<citcontext>
<prevsection>
<prevsent>15accuracy?
</prevsent>
<prevsent>means accuracy of the semi-supervised method using this tagger for pre-tagging the unsupervised data, not the accuracy of the tagger itself.
</prevsent>
</prevsection>
<citsent citstr=" W96-0213 ">
16in fact, we have experimented with other tagger combinations and configurations as well with the tnt(brants, 2000), <papid> A00-1031 </papid>maxent (ratnaparkhi, 1996) <papid> W96-0213 </papid>and treetag ger (schmid, 1994), with or without the morce tagger in the pack; see below for the winning combination.17this patch is available on the papers website (see section 7).</citsent>
<aftsection>
<nextsent>767 three methods of unsupervised data selection, i.e. generating the unsupervised data chunks for each training iteration from the ,,pool?
</nextsent>
<nextsent>of sentences.
</nextsent>
<nextsent>these methods are: simple sequential chopping, randomized data selection with replacement and randomized selection without replacement.
</nextsent>
<nextsent>table 5 demonstrates that there is practically no difference in the results.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3801">
<title id=" E12-1033.xml">generalization methods for in domain and cross domain opinion holder extraction </title>
<section> data.  </section>
<citcontext>
<prevsection>
<prevsent>we compare two state of-the-art learning methods, conditional random fields and convolution kernels, and rule-based method.
</prevsent>
<prevsent>as labeled dataset we mainly use the mpqa 2.0 corpus (wiebe et al  2005).
</prevsent>
</prevsection>
<citsent citstr=" N10-1121 ">
we adhere to the definition of opinion holders from previous work (wiegand and klakow, 2010; <papid> N10-1121 </papid>wiegand and klakow, 2011a; wiegand and klakow, 2011b), i.e. every source of private state or subjective speech event (wiebe et al  2005) is considered an opinion holder.</citsent>
<aftsection>
<nextsent>this corpus contains almost exclusively news texts.
</nextsent>
<nextsent>in order to divide it into different domains, we use the topic labels from (stoyanov et al  2004).
</nextsent>
<nextsent>by inspecting those topics, we found that many of them can grouped to cluster of news items discussing human rights issues mostly in the context of combating global terrorism.
</nextsent>
<nextsent>this means that there is little point in considering every single topic as distinct (sub)domain and, therefore, we consider this cluster as one single domain ethics.3 for our cross-domain evaluation, we want to have another topic that is fairly different from this set of documents.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3802">
<title id=" E12-1033.xml">generalization methods for in domain and cross domain opinion holder extraction </title>
<section> data.  </section>
<citcontext>
<prevsection>
<prevsent>in addition to these two (sub)domains, we chose some text type that is not even news textin order to have very distant domain.
</prevsent>
<prevsent>therefore, we had to use some text not included in thempqa corpus.
</prevsent>
</prevsection>
<citsent citstr=" P10-1059 ">
existing text collections containing product reviews (kessler et al  2010; topraket al  2010), <papid> P10-1059 </papid>which are generally popular resource for sentiment analysis, were not found suitable as they only contain few distinct opinionholders.</citsent>
<aftsection>
<nextsent>we finally used few summaries of fictional work (two shakespeare plays and one novel by jane austen4) since their language is notably different from that of news texts and they contain large number of different opinion holders(therefore opinion holder extraction is meaningful task on this text type).
</nextsent>
<nextsent>these texts make up our third domain fiction.
</nextsent>
<nextsent>we manually labeled it with opinion holder information by applying the annotation scheme of the mpqa corpus.table 1 lists the properties of the different domain corpora.
</nextsent>
<nextsent>note that ethics is the largest do main.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3803">
<title id=" E12-1033.xml">generalization methods for in domain and cross domain opinion holder extraction </title>
<section> the different types of generalization.  </section>
<citcontext>
<prevsection>
<prevsent>al., 2010).
</prevsent>
<prevsent>such generalization is, in particular,attractive as it is cheaply produced.
</prevsent>
</prevsection>
<citsent citstr=" J92-4003 ">
as state of-the-art clustering method, we consider brown clustering (brown et al  1992) <papid> J92-4003 </papid>as implemented in the srilm-toolkit (stolcke, 2002).</citsent>
<aftsection>
<nextsent>we induced 1000 clusters which is also the configuration used in (turian et al  2010).<papid> P10-1040 </papid>5 table 2 illustrates few of the clusters induced from our unlabeled dataset introduced in section (?)</nextsent>
<nextsent>2.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3804">
<title id=" E12-1033.xml">generalization methods for in domain and cross domain opinion holder extraction </title>
<section> the different types of generalization.  </section>
<citcontext>
<prevsection>
<prevsent>such generalization is, in particular,attractive as it is cheaply produced.
</prevsent>
<prevsent>as state of-the-art clustering method, we consider brown clustering (brown et al  1992) <papid> J92-4003 </papid>as implemented in the srilm-toolkit (stolcke, 2002).</prevsent>
</prevsection>
<citsent citstr=" P10-1040 ">
we induced 1000 clusters which is also the configuration used in (turian et al  2010).<papid> P10-1040 </papid>5 table 2 illustrates few of the clusters induced from our unlabeled dataset introduced in section (?)</citsent>
<aftsection>
<nextsent>2.
</nextsent>
<nextsent>some of these clusters represent location or person names (e.g. i. &amp; ii.).
</nextsent>
<nextsent>this exemplifies why clustering is effective for named-entity recognition.
</nextsent>
<nextsent>we also find clusters that intuitively seem to be meaningful for our task (e.g. iii.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3805">
<title id=" E12-1033.xml">generalization methods for in domain and cross domain opinion holder extraction </title>
<section> the different types of generalization.  </section>
<citcontext>
<prevsection>
<prevsent>(4) always supported this idea.
</prevsent>
<prevsent>holder:agent.
</prevsent>
</prevsection>
<citsent citstr=" H05-1044 ">
(5) this worries me. holder:patient (6) he disappointed me. holder:patient we follow wiegand and klakow (2011b) who found that those predicates can be best obtained by using subset of levins verb classes (levin, 1993) and the strong subjective expressions of the subjectivity lexicon (wilson et al  2005).<papid> H05-1044 </papid></citsent>
<aftsection>
<nextsent>for those predicates it is also important to consider in which argument position they usually take an opinion holder.
</nextsent>
<nextsent>bethard et al (2004) found the 5we also experimented with other sizes but they did not produce better overall performance.
</nextsent>
<nextsent>majority of holders are agents (4).
</nextsent>
<nextsent>a certain number of predicates, however, also have opinion holders inpatient position, e.g.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3806">
<title id=" E12-1033.xml">generalization methods for in domain and cross domain opinion holder extraction </title>
<section> the different types of generalization.  </section>
<citcontext>
<prevsection>
<prevsent>3.3.1 distant supervision with prototypical opinion holders lexical resources are potentially much more expressive than word clustering.
</prevsent>
<prevsent>this knowledge, however, is usually manually compiled, which makes this solution much more expensive.
</prevsent>
</prevsection>
<citsent citstr=" P09-1113 ">
wiegand and klakow (2011a) present an intermediate solution for opinion holder extraction inspired by distant supervision (mintz et al  2009).<papid> P09-1113 </papid></citsent>
<aftsection>
<nextsent>the output of that method is also lexicon of predicates but it is automatically extracted from large unlabeled corpus.
</nextsent>
<nextsent>this is achieved by collecting predicates that frequently co-occur with prototypical opinion holders, i.e. common nouns such as opponents (7) or critics (8), if they are an agent of that predicate.
</nextsent>
<nextsent>the rationale behind this isthat those nouns act very much like actual opinion holders and therefore can be seen as proxy.
</nextsent>
<nextsent>(7) opponents say these arguments miss the point.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3807">
<title id=" E12-1033.xml">generalization methods for in domain and cross domain opinion holder extraction </title>
<section> the different types of generalization.  </section>
<citcontext>
<prevsection>
<prevsent>therefore, we widen the semantic type of this position from prototypical opinion holders to persons.
</prevsent>
<prevsent>this means that we allow personal pronouns (i.e. i, you, he, she and we) to appear in this position.
</prevsent>
</prevsection>
<citsent citstr=" J04-3002 ">
we believe that this relaxation can be done in that particular case, as adjectives are much more likely to convey opinions priori than verbs (wiebe et al  2004).<papid> J04-3002 </papid></citsent>
<aftsection>
<nextsent>an intrinsic evaluation of the predicates that wethus extracted from our unlabeled corpus is difficult.
</nextsent>
<nextsent>the 250 most frequent verbs exhibiting this special property of coinciding with adjectives(this will be the list that we use in our experi ments) contains 42% entries of the amuse verbs(3.2).
</nextsent>
<nextsent>however, we also found many other potentially useful predicates on this list that are not listed as amuse verbs (table 4).
</nextsent>
<nextsent>as amuse verbs cannot be considered complete golden standard for all predicates taking opinion holders as patients, we will focus on task-based evaluation of our automatically extracted list (6).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3808">
<title id=" E12-1033.xml">generalization methods for in domain and cross domain opinion holder extraction </title>
<section> data-driven methods.  </section>
<citcontext>
<prevsection>
<prevsent>using crf, the task of opinion holder extraction is framed as tagging problem in which given sequence of observations = x1x2 . . .
</prevsent>
<prevsent>xn (words in sentence) sequence of output tagsy = y1y2 . . .
</prevsent>
</prevsection>
<citsent citstr=" H05-1045 ">
yn indicating the boundaries of opinion holders is computed by modeling the conditional probability (x|y).the features we use (table 5) are mostly inspired by choi et al (2005) <papid> H05-1045 </papid>and by the ones used for plain support vector machines (svms)in (wiegand and klakow, 2010).<papid> N10-1121 </papid></citsent>
<aftsection>
<nextsent>they are organized into groups.
</nextsent>
<nextsent>the basic group plain does not contain any generalization method.
</nextsent>
<nextsent>each other group is dedicated to one specific generalization method that we want to examine (clus, induc and lex).
</nextsent>
<nextsent>apart from considering generalization features indicating the presence of generalization types, we also consider those types in conjunction with semantic roles.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3811">
<title id=" E12-1033.xml">generalization methods for in domain and cross domain opinion holder extraction </title>
<section> data-driven methods.  </section>
<citcontext>
<prevsection>
<prevsent>responding feature from the plain feature group that also includes the lexical form of the predicate is most likely sparse feature.
</prevsent>
<prevsent>for the opinion holder me in (10), for example, it would correspond to a1 shock.
</prevsent>
</prevsection>
<citsent citstr=" P03-1054 ">
therefore, we introduce for each generalization method an additional feature replacing the sparse lexical item by generalization label, i.e. clus: a1 cluster-35265, induc: a1 induc-pred and lex: a1 lex-pred.6 for this learning method, we use crf++.7 we choose configuration that provides good performance on our source domain (i.e. ethics).8 for semantic role labeling we use swirl9, for chunk parsing cass (abney, 1991) and for constituency parsing stanford parser (klein and manning, 2003).<papid> P03-1054 </papid></citsent>
<aftsection>
<nextsent>named-entity information is provided by stanford tagger (finkel et al  2005).<papid> P05-1045 </papid></nextsent>
<nextsent>4.2 convolution kernels (ck).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3812">
<title id=" E12-1033.xml">generalization methods for in domain and cross domain opinion holder extraction </title>
<section> data-driven methods.  </section>
<citcontext>
<prevsection>
<prevsent>for the opinion holder me in (10), for example, it would correspond to a1 shock.
</prevsent>
<prevsent>therefore, we introduce for each generalization method an additional feature replacing the sparse lexical item by generalization label, i.e. clus: a1 cluster-35265, induc: a1 induc-pred and lex: a1 lex-pred.6 for this learning method, we use crf++.7 we choose configuration that provides good performance on our source domain (i.e. ethics).8 for semantic role labeling we use swirl9, for chunk parsing cass (abney, 1991) and for constituency parsing stanford parser (klein and manning, 2003).<papid> P03-1054 </papid></prevsent>
</prevsection>
<citsent citstr=" P05-1045 ">
named-entity information is provided by stanford tagger (finkel et al  2005).<papid> P05-1045 </papid></citsent>
<aftsection>
<nextsent>4.2 convolution kernels (ck).
</nextsent>
<nextsent>convolution kernels (ck) are special kernel functions.
</nextsent>
<nextsent>a kernel function : ? ? computes the similarity of two data instances xi and xj (xi ? xj ? x).
</nextsent>
<nextsent>it is mostly used in svms that estimate hyper plane to separate data instances from different classes h(~x) = ~w ? ~x + = 0 where ? rn and ? (joachims, 1999).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3814">
<title id=" E12-1033.xml">generalization methods for in domain and cross domain opinion holder extraction </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>while crf produces prediction for every word token in sentence, ck and rb only produce prediction for every noun phrase.
</prevsent>
<prevsent>for evaluation, we project the predictions from rb and ck to word token level inorder to ensure comparability.
</prevsent>
</prevsection>
<citsent citstr=" C10-1059 ">
we evaluate these quential output with precision, recall and f-scoreas defined in (johansson and moschitti, 2010; <papid> C10-1059 </papid>johansson and moschitti, 2011).<papid> P11-2018 </papid></citsent>
<aftsection>
<nextsent>6.1 rule-based classifier.
</nextsent>
<nextsent>table 6 shows the cross-domain performance ofthe different rule-based classifiers.
</nextsent>
<nextsent>rb-lex performs better than rb-induc.
</nextsent>
<nextsent>in comparison to the domains ethics and space the difference is larger on fiction.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3815">
<title id=" E12-1033.xml">generalization methods for in domain and cross domain opinion holder extraction </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>while crf produces prediction for every word token in sentence, ck and rb only produce prediction for every noun phrase.
</prevsent>
<prevsent>for evaluation, we project the predictions from rb and ck to word token level inorder to ensure comparability.
</prevsent>
</prevsection>
<citsent citstr=" P11-2018 ">
we evaluate these quential output with precision, recall and f-scoreas defined in (johansson and moschitti, 2010; <papid> C10-1059 </papid>johansson and moschitti, 2011).<papid> P11-2018 </papid></citsent>
<aftsection>
<nextsent>6.1 rule-based classifier.
</nextsent>
<nextsent>table 6 shows the cross-domain performance ofthe different rule-based classifiers.
</nextsent>
<nextsent>rb-lex performs better than rb-induc.
</nextsent>
<nextsent>in comparison to the domains ethics and space the difference is larger on fiction.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3816">
<title id=" E12-1033.xml">generalization methods for in domain and cross domain opinion holder extraction </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>therefore, rb (which exclusively uses domain-independent knowledge) outperforms both learning-based methods including the ones incorporating generalization.
</prevsent>
<prevsent>similar results have been observed for rule-based classifiers from other tasks in cross-domain sentiment analysis, such as subjectivity detection and polarityclassification.
</prevsent>
</prevsection>
<citsent citstr=" P08-1034 ">
high-level information as it is encoded in rule-based classifier generalizes better than learning-based methods (andreevskaia and bergler, 2008; <papid> P08-1034 </papid>lambov et al  2009).</citsent>
<aftsection>
<nextsent>we set up another experiment exclusively for the fiction-domain in which we combine the output of our best learning-based method, i.e. ck, with the prediction of rule-based classifier.
</nextsent>
<nextsent>the combined classifier will predict an opinion holder, if either classifier predicts one.
</nextsent>
<nextsent>the motivation for this is the following: the fiction-domain is the only domain to have significant proportion of opinion holders appearing as patients.
</nextsent>
<nextsent>we want to know how much of them can be recognized with the best out-of-domain classifier using training data with only very few instances of this type and what benefit the addition of using various rbs which have clearer notion of these constructions brings about.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3819">
<title id=" E12-1033.xml">generalization methods for in domain and cross domain opinion holder extraction </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>(12) she later gives charlotte her share of advice on running household.
</prevsent>
<prevsent>the research on opinion holder extraction has been focusing on applying different data-driven approaches.
</prevsent>
</prevsection>
<citsent citstr=" W06-1651 ">
choi et al (2005) <papid> H05-1045 </papid>and choi et al (2006) <papid> W06-1651 </papid>explore conditional random fields, wiegand and klakow (2010) <papid> N10-1121 </papid>examine different combinations of convolution kernels, while johansson and moschitti (2010) <papid> C10-1059 </papid>present re-ranking approach modeling complex relations between multiple opinions in sentence.</citsent>
<aftsection>
<nextsent>a comparison of 332 space (similar target domain) fiction (distant target domain) crf ck crf ck features prec rec f1 prec rec f1 prec rec f1 prec rec f1 plain 47.32 48.62 47.96 45.89 57.07 50.87 68.58 28.96 40.73 66.90 41.48 51.21 +clus 49.00 48.62 48.81 49.23 57.64 53.10 71.85 32.21 44.48 67.54 41.21 51.19 +induc 42.92 49.15 45.82 46.66 60.45 52.67 71.59 34.77 46.80 67.06 45.15 53.97 +lex 49.65 49.07 49.36 49.60 59.88 54.26 71.91 35.83 47.83 69.45 46.65 55.81 +clus+induc 46.61 48.78 47.67 48.65 58.20 53.00 71.32 35.88 47.74 67.46 42.17 51.90 +lex+induc 48.75 50.87 49.78 49.92 58.76 53.98 74.02 37.37 49.67 69.73 46.17 55.55 +clus+lex 49.72 50.87 50.29 53.70 59.32 56.37 73.41 37.15 49.33 70.59 43.98 54.20 all 49.87 51.03 50.44 51.68 58.76 54.99 72.00 37.44 49.26 70.61 44.83 54.84 best rb 41.72 57.80 48.47 41.72 57.80 48.47 63.26 62.96 63.11 63.26 62.96 63.11 table 9: comparison of best rb with learning-based approaches on out-of-domain classification.
</nextsent>
<nextsent>those methods has not yet been attempted.
</nextsent>
<nextsent>in this work, we compare the popular state-of-the-art learning algorithms conditional random fields and convolution kernels for the first time.
</nextsent>
<nextsent>all these data-driven methods have been evaluated on the mpqa corpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3822">
<title id=" E12-1033.xml">generalization methods for in domain and cross domain opinion holder extraction </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>this kind of knowledge should be directly learnt from the labeled training data.
</prevsent>
<prevsent>in this work, we found, however, that the distribution of argument positions of opinion holders varies throughout the different domains and, therefore, cannot be learnt from any arbitrary out-of-domain training set.
</prevsent>
</prevsection>
<citsent citstr=" W06-0301 ">
bethard et al (2004) and kim and hovy (2006) <papid> W06-0301 </papid>explore the usefulness of semantic roles provided by framenet (fillmore et al  2003).</citsent>
<aftsection>
<nextsent>bethard et al (2004) use this resource to acquire labeled training data while in (kim and hovy, 2006) <papid> W06-0301 </papid>framenet is used within rule-based classifier mapping frame-elements of frames to opinion holders.</nextsent>
<nextsent>bethard et al (2004) only evaluate on an artificial dataset (i.e. subset of sentences from framenet and propbank (kingsbury and palmer, 2002)).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3830">
<title id=" E06-1039.xml">multi document summarization of evaluative text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>there are other equally important commercial applications, such as the summarization of travel logs, andnon-commercial applications, such as the summarization of candidate reviews.
</prevsent>
<prevsent>the general problem we consider in this paper is how to effectively summarize large corpora of evaluative text about single entity (e.g., product).
</prevsent>
</prevsection>
<citsent citstr=" W04-3256 ">
in contrast, most previous work on multi document summarization has focused on factual text (e.g., news (mckeown et al, 2002), biographies (zhou et al, 2004)).<papid> W04-3256 </papid></citsent>
<aftsection>
<nextsent>for factual documents,the goal of summarizer is to select the most important facts and present them insensible ordering while avoiding repetition.
</nextsent>
<nextsent>previous work has shown that this can be effectively achieved by carefully extracting and ordering the most informative sentences from the original documents in domain-independent way.
</nextsent>
<nextsent>notice however that when the source documents are assumed to contain inconsistent information (e.g., conflicting reports of natural disaster (white et al, 2002)), different approach is needed.
</nextsent>
<nextsent>the summarizerneeds first to extract the information from the documents, then process such information to identify overlaps and inconsistencies between the different sources and finally produce summary that points out and explain those inconsistencies.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3831">
<title id=" E06-1039.xml">multi document summarization of evaluative text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in particular, the summarizer should at least know for each document: what features ofthe entity were evaluated, the polarity of the evaluations and their strengths.in this paper, we explore this hypothesis by considering two alternative approaches.
</prevsent>
<prevsent>first, we developed sentence-extraction based summarizerthat uses the information extracted from the corpus to select and rank sentences from the corpus.
</prevsent>
</prevsection>
<citsent citstr=" P03-1048 ">
we implemented this system, called mead*, by 305adapting mead (radev et al, 2003), <papid> P03-1048 </papid>an open source framework for multi-document summa riza tion.</citsent>
<aftsection>
<nextsent>second, we developed summarizer that produces summaries primarily by generating language from the information extracted from the corpus.
</nextsent>
<nextsent>we implemented this system, called the summarizer of evaluative arguments (sea), by adapting the generator of evaluative arguments (gea) (carenini and moore, expected 2006) framework for generating user tailored evaluative arguments.we have performed an empirical formative evaluation of mead* and sea in user study.
</nextsent>
<nextsent>in this evaluation, we also tested the effectiveness of human generated summaries (hgs) as topline and of summaries generated by mead without access to the extracted information as baseline.the results indicate that sea and mead* quantitatively perform equally well above mead and below hgs.
</nextsent>
<nextsent>qualitatively, we find that they perform well for different but complementary reasons.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3834">
<title id=" E12-1029.xml">boot strapped training of event extraction classifiers </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>similarly, any company can be an acquirer or an acquiree depending on the context.
</prevsent>
<prevsent>many supervised learning techniques have been used to create event extraction systems using gold standard answer key?
</prevsent>
</prevsection>
<citsent citstr=" P98-1067 ">
event templates for training (e.g., (freitag, 1998<papid> P98-1067 </papid>a; chieu and ng,2002; maslennikov and chua, 2007)).<papid> P07-1075 </papid></citsent>
<aftsection>
<nextsent>how ever, manually generating answer keys for event extraction is time-consuming and tedious.
</nextsent>
<nextsent>and more importantly, event extraction annotations are highly domain-specific, so new annotations must be obtained for each domain.the goal of our research is to use bootstrapping techniques to automatically train state-ofthe-art event extraction system without human generated answer key templates.
</nextsent>
<nextsent>the focus of our work is the tier event extraction model, whichis multi-layered architecture for event extraction (huang and riloff, 2011).<papid> P11-1114 </papid></nextsent>
<nextsent>tiers innovation over previous techniques is the use of four different classifiers that analyze document at increasing levels of granularity.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3837">
<title id=" E12-1029.xml">boot strapped training of event extraction classifiers </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>similarly, any company can be an acquirer or an acquiree depending on the context.
</prevsent>
<prevsent>many supervised learning techniques have been used to create event extraction systems using gold standard answer key?
</prevsent>
</prevsection>
<citsent citstr=" P07-1075 ">
event templates for training (e.g., (freitag, 1998<papid> P98-1067 </papid>a; chieu and ng,2002; maslennikov and chua, 2007)).<papid> P07-1075 </papid></citsent>
<aftsection>
<nextsent>how ever, manually generating answer keys for event extraction is time-consuming and tedious.
</nextsent>
<nextsent>and more importantly, event extraction annotations are highly domain-specific, so new annotations must be obtained for each domain.the goal of our research is to use bootstrapping techniques to automatically train state-ofthe-art event extraction system without human generated answer key templates.
</nextsent>
<nextsent>the focus of our work is the tier event extraction model, whichis multi-layered architecture for event extraction (huang and riloff, 2011).<papid> P11-1114 </papid></nextsent>
<nextsent>tiers innovation over previous techniques is the use of four different classifiers that analyze document at increasing levels of granularity.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3838">
<title id=" E12-1029.xml">boot strapped training of event extraction classifiers </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>how ever, manually generating answer keys for event extraction is time-consuming and tedious.
</prevsent>
<prevsent>and more importantly, event extraction annotations are highly domain-specific, so new annotations must be obtained for each domain.the goal of our research is to use bootstrapping techniques to automatically train state-ofthe-art event extraction system without human generated answer key templates.
</prevsent>
</prevsection>
<citsent citstr=" P11-1114 ">
the focus of our work is the tier event extraction model, whichis multi-layered architecture for event extraction (huang and riloff, 2011).<papid> P11-1114 </papid></citsent>
<aftsection>
<nextsent>tiers innovation over previous techniques is the use of four different classifiers that analyze document at increasing levels of granularity.
</nextsent>
<nextsent>tier progressively zooms in on event information using pipeline of classifiers that perform document-level classification, sentence classification, and noun phrase classification.
</nextsent>
<nextsent>tier outperformed previous event extraction systems on the muc-4 dataset, but relied heavily on large collection of 1,300 documents coupled with answer key templates to train its four classifiers.in this paper, we present bootstrapping solution that exploits large unannotated corpus for training by using role-identifying nouns (phillips and riloff, 2007) as seed terms.
</nextsent>
<nextsent>phillips and riloff observed that some nouns, by definition, refer to entities or objects that play specific role in an event.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3843">
<title id=" E12-1029.xml">boot strapped training of event extraction classifiers </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>event extraction techniques have largely focused on detecting event triggers?
</prevsent>
<prevsent>with their arguments for extracting role fillers.
</prevsent>
</prevsection>
<citsent citstr=" C00-2136 ">
classical methods are either pattern-based (kim and moldovan, 1993; riloff, 1993; soderland et al 1995; huffman, 1996; freitag, 1998<papid> P98-1067 </papid>b; ciravegna, 2001; califf and mooney, 2003; riloff, 1996; riloff and jones, 1999; yangarber et al 2000; <papid> C00-2136 </papid>sudo et al 2003;<papid> P03-1029 </papid>stevenson and greenwood, 2005) <papid> P05-1047 </papid>or classifier based (e.g., (freitag, 1998<papid> P98-1067 </papid>a; chieu and ng, 2002; finn and kushmerick, 2004; li et al 2005; <papid> W05-0610 </papid>yu et al., 2005)).<papid> P05-1062 </papid>recently, several approaches have been proposed to address the insufficiency of using only local context to identify role fillers.</citsent>
<aftsection>
<nextsent>some approaches look at the broader sentential context around potential role filler when making decision (e.g., (gu and cercone, 2006; <papid> P06-1061 </papid>patwardhan and riloff, 2009)).<papid> D09-1016 </papid></nextsent>
<nextsent>other systems take more global view and consider discourse properties of the document as whole to improve performance(e.g., (maslennikov and chua, 2007; <papid> P07-1075 </papid>ji and grishman, 2008; <papid> P08-1030 </papid>liao and grishman, 2010; <papid> P10-1081 </papid>huang and riloff, 2011)).<papid> P11-1114 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3845">
<title id=" E12-1029.xml">boot strapped training of event extraction classifiers </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>event extraction techniques have largely focused on detecting event triggers?
</prevsent>
<prevsent>with their arguments for extracting role fillers.
</prevsent>
</prevsection>
<citsent citstr=" P03-1029 ">
classical methods are either pattern-based (kim and moldovan, 1993; riloff, 1993; soderland et al 1995; huffman, 1996; freitag, 1998<papid> P98-1067 </papid>b; ciravegna, 2001; califf and mooney, 2003; riloff, 1996; riloff and jones, 1999; yangarber et al 2000; <papid> C00-2136 </papid>sudo et al 2003;<papid> P03-1029 </papid>stevenson and greenwood, 2005) <papid> P05-1047 </papid>or classifier based (e.g., (freitag, 1998<papid> P98-1067 </papid>a; chieu and ng, 2002; finn and kushmerick, 2004; li et al 2005; <papid> W05-0610 </papid>yu et al., 2005)).<papid> P05-1062 </papid>recently, several approaches have been proposed to address the insufficiency of using only local context to identify role fillers.</citsent>
<aftsection>
<nextsent>some approaches look at the broader sentential context around potential role filler when making decision (e.g., (gu and cercone, 2006; <papid> P06-1061 </papid>patwardhan and riloff, 2009)).<papid> D09-1016 </papid></nextsent>
<nextsent>other systems take more global view and consider discourse properties of the document as whole to improve performance(e.g., (maslennikov and chua, 2007; <papid> P07-1075 </papid>ji and grishman, 2008; <papid> P08-1030 </papid>liao and grishman, 2010; <papid> P10-1081 </papid>huang and riloff, 2011)).<papid> P11-1114 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3846">
<title id=" E12-1029.xml">boot strapped training of event extraction classifiers </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>event extraction techniques have largely focused on detecting event triggers?
</prevsent>
<prevsent>with their arguments for extracting role fillers.
</prevsent>
</prevsection>
<citsent citstr=" P05-1047 ">
classical methods are either pattern-based (kim and moldovan, 1993; riloff, 1993; soderland et al 1995; huffman, 1996; freitag, 1998<papid> P98-1067 </papid>b; ciravegna, 2001; califf and mooney, 2003; riloff, 1996; riloff and jones, 1999; yangarber et al 2000; <papid> C00-2136 </papid>sudo et al 2003;<papid> P03-1029 </papid>stevenson and greenwood, 2005) <papid> P05-1047 </papid>or classifier based (e.g., (freitag, 1998<papid> P98-1067 </papid>a; chieu and ng, 2002; finn and kushmerick, 2004; li et al 2005; <papid> W05-0610 </papid>yu et al., 2005)).<papid> P05-1062 </papid>recently, several approaches have been proposed to address the insufficiency of using only local context to identify role fillers.</citsent>
<aftsection>
<nextsent>some approaches look at the broader sentential context around potential role filler when making decision (e.g., (gu and cercone, 2006; <papid> P06-1061 </papid>patwardhan and riloff, 2009)).<papid> D09-1016 </papid></nextsent>
<nextsent>other systems take more global view and consider discourse properties of the document as whole to improve performance(e.g., (maslennikov and chua, 2007; <papid> P07-1075 </papid>ji and grishman, 2008; <papid> P08-1030 </papid>liao and grishman, 2010; <papid> P10-1081 </papid>huang and riloff, 2011)).<papid> P11-1114 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3850">
<title id=" E12-1029.xml">boot strapped training of event extraction classifiers </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>event extraction techniques have largely focused on detecting event triggers?
</prevsent>
<prevsent>with their arguments for extracting role fillers.
</prevsent>
</prevsection>
<citsent citstr=" W05-0610 ">
classical methods are either pattern-based (kim and moldovan, 1993; riloff, 1993; soderland et al 1995; huffman, 1996; freitag, 1998<papid> P98-1067 </papid>b; ciravegna, 2001; califf and mooney, 2003; riloff, 1996; riloff and jones, 1999; yangarber et al 2000; <papid> C00-2136 </papid>sudo et al 2003;<papid> P03-1029 </papid>stevenson and greenwood, 2005) <papid> P05-1047 </papid>or classifier based (e.g., (freitag, 1998<papid> P98-1067 </papid>a; chieu and ng, 2002; finn and kushmerick, 2004; li et al 2005; <papid> W05-0610 </papid>yu et al., 2005)).<papid> P05-1062 </papid>recently, several approaches have been proposed to address the insufficiency of using only local context to identify role fillers.</citsent>
<aftsection>
<nextsent>some approaches look at the broader sentential context around potential role filler when making decision (e.g., (gu and cercone, 2006; <papid> P06-1061 </papid>patwardhan and riloff, 2009)).<papid> D09-1016 </papid></nextsent>
<nextsent>other systems take more global view and consider discourse properties of the document as whole to improve performance(e.g., (maslennikov and chua, 2007; <papid> P07-1075 </papid>ji and grishman, 2008; <papid> P08-1030 </papid>liao and grishman, 2010; <papid> P10-1081 </papid>huang and riloff, 2011)).<papid> P11-1114 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3851">
<title id=" E12-1029.xml">boot strapped training of event extraction classifiers </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>event extraction techniques have largely focused on detecting event triggers?
</prevsent>
<prevsent>with their arguments for extracting role fillers.
</prevsent>
</prevsection>
<citsent citstr=" P05-1062 ">
classical methods are either pattern-based (kim and moldovan, 1993; riloff, 1993; soderland et al 1995; huffman, 1996; freitag, 1998<papid> P98-1067 </papid>b; ciravegna, 2001; califf and mooney, 2003; riloff, 1996; riloff and jones, 1999; yangarber et al 2000; <papid> C00-2136 </papid>sudo et al 2003;<papid> P03-1029 </papid>stevenson and greenwood, 2005) <papid> P05-1047 </papid>or classifier based (e.g., (freitag, 1998<papid> P98-1067 </papid>a; chieu and ng, 2002; finn and kushmerick, 2004; li et al 2005; <papid> W05-0610 </papid>yu et al., 2005)).<papid> P05-1062 </papid>recently, several approaches have been proposed to address the insufficiency of using only local context to identify role fillers.</citsent>
<aftsection>
<nextsent>some approaches look at the broader sentential context around potential role filler when making decision (e.g., (gu and cercone, 2006; <papid> P06-1061 </papid>patwardhan and riloff, 2009)).<papid> D09-1016 </papid></nextsent>
<nextsent>other systems take more global view and consider discourse properties of the document as whole to improve performance(e.g., (maslennikov and chua, 2007; <papid> P07-1075 </papid>ji and grishman, 2008; <papid> P08-1030 </papid>liao and grishman, 2010; <papid> P10-1081 </papid>huang and riloff, 2011)).<papid> P11-1114 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3852">
<title id=" E12-1029.xml">boot strapped training of event extraction classifiers </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>with their arguments for extracting role fillers.
</prevsent>
<prevsent>classical methods are either pattern-based (kim and moldovan, 1993; riloff, 1993; soderland et al 1995; huffman, 1996; freitag, 1998<papid> P98-1067 </papid>b; ciravegna, 2001; califf and mooney, 2003; riloff, 1996; riloff and jones, 1999; yangarber et al 2000; <papid> C00-2136 </papid>sudo et al 2003;<papid> P03-1029 </papid>stevenson and greenwood, 2005) <papid> P05-1047 </papid>or classifier based (e.g., (freitag, 1998<papid> P98-1067 </papid>a; chieu and ng, 2002; finn and kushmerick, 2004; li et al 2005; <papid> W05-0610 </papid>yu et al., 2005)).<papid> P05-1062 </papid>recently, several approaches have been proposed to address the insufficiency of using only local context to identify role fillers.</prevsent>
</prevsection>
<citsent citstr=" P06-1061 ">
some approaches look at the broader sentential context around potential role filler when making decision (e.g., (gu and cercone, 2006; <papid> P06-1061 </papid>patwardhan and riloff, 2009)).<papid> D09-1016 </papid></citsent>
<aftsection>
<nextsent>other systems take more global view and consider discourse properties of the document as whole to improve performance(e.g., (maslennikov and chua, 2007; <papid> P07-1075 </papid>ji and grishman, 2008; <papid> P08-1030 </papid>liao and grishman, 2010; <papid> P10-1081 </papid>huang and riloff, 2011)).<papid> P11-1114 </papid></nextsent>
<nextsent>currently, the learning-based event extraction systems that perform best all use supervised learning techniques that require large number of texts coupled with manually-generated annotations or answer key templates.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3853">
<title id=" E12-1029.xml">boot strapped training of event extraction classifiers </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>with their arguments for extracting role fillers.
</prevsent>
<prevsent>classical methods are either pattern-based (kim and moldovan, 1993; riloff, 1993; soderland et al 1995; huffman, 1996; freitag, 1998<papid> P98-1067 </papid>b; ciravegna, 2001; califf and mooney, 2003; riloff, 1996; riloff and jones, 1999; yangarber et al 2000; <papid> C00-2136 </papid>sudo et al 2003;<papid> P03-1029 </papid>stevenson and greenwood, 2005) <papid> P05-1047 </papid>or classifier based (e.g., (freitag, 1998<papid> P98-1067 </papid>a; chieu and ng, 2002; finn and kushmerick, 2004; li et al 2005; <papid> W05-0610 </papid>yu et al., 2005)).<papid> P05-1062 </papid>recently, several approaches have been proposed to address the insufficiency of using only local context to identify role fillers.</prevsent>
</prevsection>
<citsent citstr=" D09-1016 ">
some approaches look at the broader sentential context around potential role filler when making decision (e.g., (gu and cercone, 2006; <papid> P06-1061 </papid>patwardhan and riloff, 2009)).<papid> D09-1016 </papid></citsent>
<aftsection>
<nextsent>other systems take more global view and consider discourse properties of the document as whole to improve performance(e.g., (maslennikov and chua, 2007; <papid> P07-1075 </papid>ji and grishman, 2008; <papid> P08-1030 </papid>liao and grishman, 2010; <papid> P10-1081 </papid>huang and riloff, 2011)).<papid> P11-1114 </papid></nextsent>
<nextsent>currently, the learning-based event extraction systems that perform best all use supervised learning techniques that require large number of texts coupled with manually-generated annotations or answer key templates.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3855">
<title id=" E12-1029.xml">boot strapped training of event extraction classifiers </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>classical methods are either pattern-based (kim and moldovan, 1993; riloff, 1993; soderland et al 1995; huffman, 1996; freitag, 1998<papid> P98-1067 </papid>b; ciravegna, 2001; califf and mooney, 2003; riloff, 1996; riloff and jones, 1999; yangarber et al 2000; <papid> C00-2136 </papid>sudo et al 2003;<papid> P03-1029 </papid>stevenson and greenwood, 2005) <papid> P05-1047 </papid>or classifier based (e.g., (freitag, 1998<papid> P98-1067 </papid>a; chieu and ng, 2002; finn and kushmerick, 2004; li et al 2005; <papid> W05-0610 </papid>yu et al., 2005)).<papid> P05-1062 </papid>recently, several approaches have been proposed to address the insufficiency of using only local context to identify role fillers.</prevsent>
<prevsent>some approaches look at the broader sentential context around potential role filler when making decision (e.g., (gu and cercone, 2006; <papid> P06-1061 </papid>patwardhan and riloff, 2009)).<papid> D09-1016 </papid></prevsent>
</prevsection>
<citsent citstr=" P08-1030 ">
other systems take more global view and consider discourse properties of the document as whole to improve performance(e.g., (maslennikov and chua, 2007; <papid> P07-1075 </papid>ji and grishman, 2008; <papid> P08-1030 </papid>liao and grishman, 2010; <papid> P10-1081 </papid>huang and riloff, 2011)).<papid> P11-1114 </papid></citsent>
<aftsection>
<nextsent>currently, the learning-based event extraction systems that perform best all use supervised learning techniques that require large number of texts coupled with manually-generated annotations or answer key templates.
</nextsent>
<nextsent>a variety of techniques have been explored for weakly supervised training of event extraction systems, primarily in the realm of pattern or rule-based approaches (e.g., (riloff, 1996; riloff and jones, 1999; yangarber et al 2000; <papid> C00-2136 </papid>sudo et al., 2003; <papid> P03-1029 </papid>stevenson and greenwood, 2005)).<papid> P05-1047 </papid></nextsent>
<nextsent>insome of these approaches, human must manually review and clean?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3856">
<title id=" E12-1029.xml">boot strapped training of event extraction classifiers </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>classical methods are either pattern-based (kim and moldovan, 1993; riloff, 1993; soderland et al 1995; huffman, 1996; freitag, 1998<papid> P98-1067 </papid>b; ciravegna, 2001; califf and mooney, 2003; riloff, 1996; riloff and jones, 1999; yangarber et al 2000; <papid> C00-2136 </papid>sudo et al 2003;<papid> P03-1029 </papid>stevenson and greenwood, 2005) <papid> P05-1047 </papid>or classifier based (e.g., (freitag, 1998<papid> P98-1067 </papid>a; chieu and ng, 2002; finn and kushmerick, 2004; li et al 2005; <papid> W05-0610 </papid>yu et al., 2005)).<papid> P05-1062 </papid>recently, several approaches have been proposed to address the insufficiency of using only local context to identify role fillers.</prevsent>
<prevsent>some approaches look at the broader sentential context around potential role filler when making decision (e.g., (gu and cercone, 2006; <papid> P06-1061 </papid>patwardhan and riloff, 2009)).<papid> D09-1016 </papid></prevsent>
</prevsection>
<citsent citstr=" P10-1081 ">
other systems take more global view and consider discourse properties of the document as whole to improve performance(e.g., (maslennikov and chua, 2007; <papid> P07-1075 </papid>ji and grishman, 2008; <papid> P08-1030 </papid>liao and grishman, 2010; <papid> P10-1081 </papid>huang and riloff, 2011)).<papid> P11-1114 </papid></citsent>
<aftsection>
<nextsent>currently, the learning-based event extraction systems that perform best all use supervised learning techniques that require large number of texts coupled with manually-generated annotations or answer key templates.
</nextsent>
<nextsent>a variety of techniques have been explored for weakly supervised training of event extraction systems, primarily in the realm of pattern or rule-based approaches (e.g., (riloff, 1996; riloff and jones, 1999; yangarber et al 2000; <papid> C00-2136 </papid>sudo et al., 2003; <papid> P03-1029 </papid>stevenson and greenwood, 2005)).<papid> P05-1047 </papid></nextsent>
<nextsent>insome of these approaches, human must manually review and clean?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3862">
<title id=" E12-1029.xml">boot strapped training of event extraction classifiers </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>insome of these approaches, human must manually review and clean?
</prevsent>
<prevsent>the learned patterns to obtain good performance.
</prevsent>
</prevsection>
<citsent citstr=" P06-2094 ">
research has also been done to learn extraction patterns in an unsupervised way (e.g., (shinyama and sekine, 2006;<papid> P06-2094 </papid>sekine, 2006)).<papid> P06-2094 </papid></citsent>
<aftsection>
<nextsent>but these efforts target open domain information extraction.
</nextsent>
<nextsent>to extract domain specific event information, domain experts are needed to select the pattern subsets to use.there have also been weakly supervised approaches that use more than just local context.
</nextsent>
<nextsent>(patwardhan and riloff, 2007) <papid> D07-1075 </papid>uses semantic affinity measure to learn primary and secondary patterns, and the secondary patterns are applied only to event sentences.</nextsent>
<nextsent>the event sentence classifier is self-trained using seed patterns.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3865">
<title id=" E12-1029.xml">boot strapped training of event extraction classifiers </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>but these efforts target open domain information extraction.
</prevsent>
<prevsent>to extract domain specific event information, domain experts are needed to select the pattern subsets to use.there have also been weakly supervised approaches that use more than just local context.
</prevsent>
</prevsection>
<citsent citstr=" D07-1075 ">
(patwardhan and riloff, 2007) <papid> D07-1075 </papid>uses semantic affinity measure to learn primary and secondary patterns, and the secondary patterns are applied only to event sentences.</citsent>
<aftsection>
<nextsent>the event sentence classifier is self-trained using seed patterns.
</nextsent>
<nextsent>most recently, (chambers and jurafsky, 2011) <papid> P11-1098 </papid>acquire event words from an external resource, group the event words to form event scenarios, and group extraction patterns for different event roles.</nextsent>
<nextsent>how ever, these weakly supervised systems produce substantially lower performance than the best supervised systems.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3866">
<title id=" E12-1029.xml">boot strapped training of event extraction classifiers </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>(patwardhan and riloff, 2007) <papid> D07-1075 </papid>uses semantic affinity measure to learn primary and secondary patterns, and the secondary patterns are applied only to event sentences.</prevsent>
<prevsent>the event sentence classifier is self-trained using seed patterns.</prevsent>
</prevsection>
<citsent citstr=" P11-1098 ">
most recently, (chambers and jurafsky, 2011) <papid> P11-1098 </papid>acquire event words from an external resource, group the event words to form event scenarios, and group extraction patterns for different event roles.</citsent>
<aftsection>
<nextsent>how ever, these weakly supervised systems produce substantially lower performance than the best supervised systems.
</nextsent>
<nextsent>the goal of our research is to develop weakly supervised training process that can successfully train state-of-the-art event extraction system fora new domain with minimal human input.
</nextsent>
<nextsent>we decided to focus our efforts on the tier event extraction model because it recently produced better performance on the muc-4 dataset than prior learning-based event extraction systems (huang and riloff, 2011).<papid> P11-1114 </papid></nextsent>
<nextsent>in this section, we briefly givean overview of tiers architecture and its com 287 figure 1: tier overview ponents.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3869">
<title id=" E12-1029.xml">boot strapped training of event extraction classifiers </title>
<section> boot strapped training of event.  </section>
<citcontext>
<prevsection>
<prevsent>we use the semantic class labels assigned by the sundance parser (riloff and phillips, 2004) in our experiments.
</prevsent>
<prevsent>sundance looks up each noun in semantic dictionary to assign the semantic class labels.
</prevsent>
</prevsection>
<citsent citstr=" P10-1029 ">
as an alternative, general resources(e.g., wordnet (miller, 1990)) or semantic tagger (e.g., (huang and riloff, 2010)) <papid> P10-1029 </papid>could be used.</citsent>
<aftsection>
<nextsent>289 john smith was killed by . . .
</nextsent>
<nextsent>was killed by  np  role identifying patterns two armed men 1 an hour later.
</nextsent>
<nextsent>police arrested the unidentified men 3 in broad daylight this morning.
</nextsent>
<nextsent>left his house to go to work about 8:00 am.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3870">
<title id=" E12-1029.xml">boot strapped training of event extraction classifiers </title>
<section> boot strapped training of event.  </section>
<citcontext>
<prevsection>
<prevsent>we extract features from each noun phrase (np) and its surrounding context.
</prevsent>
<prevsent>the features include the np head noun and its premodifiers.
</prevsent>
</prevsection>
<citsent citstr=" P05-1045 ">
we also use the stanford ner tagger (finkel et al., 2005) <papid> P05-1045 </papid>to identify named entities within the np.</citsent>
<aftsection>
<nextsent>the context features include four words to the left of the np, four words to the right of the np,and the lexico-syntactic patterns generated by au toslog to capture expressions around the np (see (riloff, 1993) for details).
</nextsent>
<nextsent>4.2.2 event sentence classifier the event sentence classifier is responsible for identifying sentences that describe relevantevent.
</nextsent>
<nextsent>similar to the noun phrase classifier training, positive training instances are selected from the relevant documents and negative instances are drawn from the irrelevant documents.
</nextsent>
<nextsent>all sentences in the relevant documents that contain one or more labeled noun phrases (belonging to any event role) are labeled as positive training instances.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3875">
<title id=" E09-1048.xml">automatic single document key fact extraction from newswire articles </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>although these highlights could be easily generated by the respective journalists, many news organization shy away from introducing an additional manual stage into the workflow, wherepushback times of minutes are considered unacceptable in an extremely competitive news business which competes in terms of seconds rather than minutes.
</prevsent>
<prevsent>automating highlight generation can help eliminate those delays.journalistic training emphasizes that news articles should contain the most important information in the beginning, while less important information, such as background or additional details, appears further down in the article.
</prevsent>
</prevsection>
<citsent citstr=" D07-1047 ">
this is also the main reason why most summarization systems applied to news articles do not outperform simple baseline that just uses the first 100 words of an article (svore et al, 2007; <papid> D07-1047 </papid>nenkova, 2005).on the other hand, most of cnns story highlights are not taken from the beginning of the ar ticles.</citsent>
<aftsection>
<nextsent>in fact, more than 50% of the highlights stem from sentences that are not among the first100 words of the articles.
</nextsent>
<nextsent>this makes identifying story highlights much more challenging task than single-document summarization in the news domain.in order to automate story highlight identification we automatically extract syntactic, semantic, 415 figure 1: cnn.com screen shot of story excerpt with highlights.
</nextsent>
<nextsent>and purely statistical features from the document.
</nextsent>
<nextsent>the weights of the features are estimated using machine learning techniques, trained on an annotated corpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3877">
<title id=" E09-1048.xml">automatic single document key fact extraction from newswire articles </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>more recently, the work of svore et al (2007)<papid> D07-1047 </papid>is closely related to our approach as it has also exploited the cnn story highlights, although their focus was on summarization and using rougeas an evaluation and training measure.</prevsent>
<prevsent>their approach also heavily relies on additional data resources, mainly indexed wikipedia articles and microsoft live query logs, which are not readily available.</prevsent>
</prevsection>
<citsent citstr=" W07-0203 ">
linguistic features are today used mostly in summarization systems, and include the standard features sentence length, n-gram frequency, sentence position, proper noun identification, similarity to title, tf.idf, and so-called bonus?/stigmawords (neto et al, 2002; leite et al, 2007; <papid> W07-0203 </papid>pollock and zamora, 1975; goldstein et al, 1999).on the other hand, for most of these systems, simple statistical features and tf.idf still turn out to be the most important features.</citsent>
<aftsection>
<nextsent>attempts to integrate discourse models have also been made (thione et al, 2004), <papid> W04-1009 </papid>hand in hand with some of marcus (1995) earlier work.</nextsent>
<nextsent>1tf(t, d) = frequency of term in document d. idf(t,n) = inverse frequency of documents containing term in corpus , log( |n||dt| ) 416 regarding syntax, it seems to be used mainly in sentence compression or trimming.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3879">
<title id=" E09-1048.xml">automatic single document key fact extraction from newswire articles </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>their approach also heavily relies on additional data resources, mainly indexed wikipedia articles and microsoft live query logs, which are not readily available.
</prevsent>
<prevsent>linguistic features are today used mostly in summarization systems, and include the standard features sentence length, n-gram frequency, sentence position, proper noun identification, similarity to title, tf.idf, and so-called bonus?/stigmawords (neto et al, 2002; leite et al, 2007; <papid> W07-0203 </papid>pollock and zamora, 1975; goldstein et al, 1999).on the other hand, for most of these systems, simple statistical features and tf.idf still turn out to be the most important features.</prevsent>
</prevsection>
<citsent citstr=" W04-1009 ">
attempts to integrate discourse models have also been made (thione et al, 2004), <papid> W04-1009 </papid>hand in hand with some of marcus (1995) earlier work.</citsent>
<aftsection>
<nextsent>1tf(t, d) = frequency of term in document d. idf(t,n) = inverse frequency of documents containing term in corpus , log( |n||dt| ) 416 regarding syntax, it seems to be used mainly in sentence compression or trimming.
</nextsent>
<nextsent>the algorithm used by dorr et al (2003) <papid> W03-0501 </papid>removes subordinate clauses, to name one example.</nextsent>
<nextsent>while our approach does not use syntactical features as such, it is worth noting these possible enhancements.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3880">
<title id=" E09-1048.xml">automatic single document key fact extraction from newswire articles </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>attempts to integrate discourse models have also been made (thione et al, 2004), <papid> W04-1009 </papid>hand in hand with some of marcus (1995) earlier work.</prevsent>
<prevsent>1tf(t, d) = frequency of term in document d. idf(t,n) = inverse frequency of documents containing term in corpus , log( |n||dt| ) 416 regarding syntax, it seems to be used mainly in sentence compression or trimming.</prevsent>
</prevsection>
<citsent citstr=" W03-0501 ">
the algorithm used by dorr et al (2003) <papid> W03-0501 </papid>removes subordinate clauses, to name one example.</citsent>
<aftsection>
<nextsent>while our approach does not use syntactical features as such, it is worth noting these possible enhancements.
</nextsent>
<nextsent>in this section we describe which features were used and how the data was annotated to facilitate feature extraction and estimation.
</nextsent>
<nextsent>3.1 training data.
</nextsent>
<nextsent>in order to determine the features used for predicting which sentences are the sources for story highlights, we gathered statistics from 1,200 cnn newswire articles.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3884">
<title id=" E09-1048.xml">automatic single document key fact extraction from newswire articles </title>
<section> approach.  </section>
<citcontext>
<prevsection>
<prevsent>both lists also contain the wordnet synonyms of each word in the list (fellbaum, 1998).
</prevsent>
<prevsent>proper nouns.
</prevsent>
</prevsection>
<citsent citstr=" A00-2018 ">
proper nouns and other parts of speech were identified running charniaks parser (charniak, 2000) <papid> A00-2018 </papid>on the news articles.</citsent>
<aftsection>
<nextsent>3.2.3 sentence scoring the overall score of sentence is computed as the weighted linear combination of the sentence andword scores.
</nextsent>
<nextsent>the score ?(s) of sentence is defined as follows: ?(s) = wposppos(s) + n?
</nextsent>
<nextsent>k=1 wkfk + |s|?
</nextsent>
<nextsent>j=1 m?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3885">
<title id=" E09-1048.xml">automatic single document key fact extraction from newswire articles </title>
<section> approach.  </section>
<citcontext>
<prevsection>
<prevsent>how to estimate the weights is discussed next.
</prevsent>
<prevsent>3.3 parameter estimation.
</prevsent>
</prevsection>
<citsent citstr=" W02-2018 ">
there are various optimization methods that allow one to estimate the weights of features, including generalized iterative scaling and quasi-newtonmethods (malouf, 2002).<papid> W02-2018 </papid></citsent>
<aftsection>
<nextsent>we opted for generalized iterative scaling as it is commonly used for other nlp tasks and off-the-shelf implementations exist.
</nextsent>
<nextsent>here we used yasmet.3 3a maximum entropy toolkit by franz josef och, http: //www.fjoch.com/yasmet.htmlwe used development set of 240 news articles to train yasmet.
</nextsent>
<nextsent>as yasmet is supervised optimizer, we had to generate annotated data on which it was to be trained.
</nextsent>
<nextsent>for each document in the development set, we labeled each sentence as to whether story highlight was generated from it.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3886">
<title id=" E09-1048.xml">automatic single document key fact extraction from newswire articles </title>
<section> experiments and results.  </section>
<citcontext>
<prevsection>
<prevsent>19.66 (+1.87%) table 6: rouge scores for aurum-fixed, returning 4 sentences, and aurum-thresh, returning between 3 and 6 sentences.
</prevsent>
<prevsent>figure 4: position of correctly extracted sources by aurum-thresh.
</prevsent>
</prevsection>
<citsent citstr=" W04-1013 ">
rouge (lin, 2004), <papid> W04-1013 </papid>recall-oriented evaluation package for automatic summarization.</citsent>
<aftsection>
<nextsent>rouge operates essentially by comparing n-gram cooccurrences between candidate summary and number of reference summaries, and comparing that number in turn to the total number of n-grams in the reference summaries: rouge-n = ? sreferences ? ngramns match(ngramn) ? sreferences ? ngramns count(ngramn) where is the length of the n-gram, with lengths of 1 and 2 words most commonly used in current evaluations.
</nextsent>
<nextsent>rouge has become the standard tool for evaluating automatic summaries, though it is not the optimal system for this experiment.
</nextsent>
<nextsent>this is due to the fact that it is geared towards different taskas ours is not automatic summarization per seand that rouge works best judging between number of candidate and model summaries.
</nextsent>
<nextsent>the rouge scores are shown in table 6.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3890">
<title id=" E06-2013.xml">automatic annotation for all semantic layers in framenet </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in addition to the conventional annotation of frame elements and their semantic roles, we annotate additional semantic information such as support verb sand prepositions, aspect ual markers, cop ular verbs, null arguments, and slot fillers.as far as we are aware, this is the first system that finds this information automatically.
</prevsent>
<prevsent>shallow semantic parsing has been an active areaof research during the last few years.
</prevsent>
</prevsection>
<citsent citstr=" P98-1013 ">
semantic parsers, which are typically based on the framenet (baker et al, 1998) <papid> P98-1013 </papid>or propbank formalisms, have proven useful in number of nlp projects, such as information extraction and question answering.</citsent>
<aftsection>
<nextsent>the main reason for their popularity is that they can produce flat layer of semantic structure with fair degree of robustness.
</nextsent>
<nextsent>building english semantic parsers for the framenet standard has been studied widely (gildea and jurafsky, 2002; <papid> J02-3001 </papid>litkowski, 2004).<papid> W04-0803 </papid>these systems typically address the task of identifying and classifying frame elements (fes), that is semantic arguments of predicates, forgiven target word (predicate).although the fe layer is arguably the most central, the framenet annotation standard defines anumber of additional semantic layers, which contain information about support expressions (verbsand prepositions), copulas, null arguments, slot fillers, and aspect ual particles.</nextsent>
<nextsent>this information can for example be used in semantic parser to refine the meaning of predicate, to link predicates in sentence together, or possibly to improve detection and classification of fes.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3891">
<title id=" E06-2013.xml">automatic annotation for all semantic layers in framenet </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>semantic parsers, which are typically based on the framenet (baker et al, 1998) <papid> P98-1013 </papid>or propbank formalisms, have proven useful in number of nlp projects, such as information extraction and question answering.</prevsent>
<prevsent>the main reason for their popularity is that they can produce flat layer of semantic structure with fair degree of robustness.</prevsent>
</prevsection>
<citsent citstr=" J02-3001 ">
building english semantic parsers for the framenet standard has been studied widely (gildea and jurafsky, 2002; <papid> J02-3001 </papid>litkowski, 2004).<papid> W04-0803 </papid>these systems typically address the task of identifying and classifying frame elements (fes), that is semantic arguments of predicates, forgiven target word (predicate).although the fe layer is arguably the most central, the framenet annotation standard defines anumber of additional semantic layers, which contain information about support expressions (verbsand prepositions), copulas, null arguments, slot fillers, and aspect ual particles.</citsent>
<aftsection>
<nextsent>this information can for example be used in semantic parser to refine the meaning of predicate, to link predicates in sentence together, or possibly to improve detection and classification of fes.
</nextsent>
<nextsent>the task of automatic reconstruction of the additional semantic layers has not been addressed by any previoussystem.
</nextsent>
<nextsent>in this work, we describe system that automatically identifies the entities in those layers.
</nextsent>
<nextsent>framenet (baker et al, 1998; <papid> P98-1013 </papid>johnson et al, 2003) is comprehensive lexical database that lists descriptions of words in the frame-semantic paradigm (fillmore, 1976).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3893">
<title id=" E06-2013.xml">automatic annotation for all semantic layers in framenet </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>semantic parsers, which are typically based on the framenet (baker et al, 1998) <papid> P98-1013 </papid>or propbank formalisms, have proven useful in number of nlp projects, such as information extraction and question answering.</prevsent>
<prevsent>the main reason for their popularity is that they can produce flat layer of semantic structure with fair degree of robustness.</prevsent>
</prevsection>
<citsent citstr=" W04-0803 ">
building english semantic parsers for the framenet standard has been studied widely (gildea and jurafsky, 2002; <papid> J02-3001 </papid>litkowski, 2004).<papid> W04-0803 </papid>these systems typically address the task of identifying and classifying frame elements (fes), that is semantic arguments of predicates, forgiven target word (predicate).although the fe layer is arguably the most central, the framenet annotation standard defines anumber of additional semantic layers, which contain information about support expressions (verbsand prepositions), copulas, null arguments, slot fillers, and aspect ual particles.</citsent>
<aftsection>
<nextsent>this information can for example be used in semantic parser to refine the meaning of predicate, to link predicates in sentence together, or possibly to improve detection and classification of fes.
</nextsent>
<nextsent>the task of automatic reconstruction of the additional semantic layers has not been addressed by any previoussystem.
</nextsent>
<nextsent>in this work, we describe system that automatically identifies the entities in those layers.
</nextsent>
<nextsent>framenet (baker et al, 1998; <papid> P98-1013 </papid>johnson et al, 2003) is comprehensive lexical database that lists descriptions of words in the frame-semantic paradigm (fillmore, 1976).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3901">
<title id=" E12-1075.xml">syntax based word ordering incorporating a largescale language model </title>
<section> abstract </section>
<citcontext>
<prevsection>
<prevsent>a fundamental problem in text generation is word ordering.
</prevsent>
<prevsent>word ordering is computationally difficult problem, which canbe constrained to some extent for particular applications, for example by using synchronous grammars for statistical machine translation.
</prevsent>
</prevsection>
<citsent citstr=" E09-1097 ">
there have been some recent attempts at the unconstrained problem of generating sentence from multi-set of input words (wan et al  2009; <papid> E09-1097 </papid>zhang and clark, 2011).<papid> D11-1106 </papid></citsent>
<aftsection>
<nextsent>by using ccg and learning guided search, zhang and clark reported the highest scores on this task.
</nextsent>
<nextsent>one limitation of their system is the absence of an n-gram language model, which has been used by text generation systems to improve fluency.
</nextsent>
<nextsent>we take the zhang and clark system as the baseline, and incorporate an n-gram model by applying online large-margin training.
</nextsent>
<nextsent>our system significantly improved on the baseline by 3.7 bleu points.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3902">
<title id=" E12-1075.xml">syntax based word ordering incorporating a largescale language model </title>
<section> abstract </section>
<citcontext>
<prevsection>
<prevsent>a fundamental problem in text generation is word ordering.
</prevsent>
<prevsent>word ordering is computationally difficult problem, which canbe constrained to some extent for particular applications, for example by using synchronous grammars for statistical machine translation.
</prevsent>
</prevsection>
<citsent citstr=" D11-1106 ">
there have been some recent attempts at the unconstrained problem of generating sentence from multi-set of input words (wan et al  2009; <papid> E09-1097 </papid>zhang and clark, 2011).<papid> D11-1106 </papid></citsent>
<aftsection>
<nextsent>by using ccg and learning guided search, zhang and clark reported the highest scores on this task.
</nextsent>
<nextsent>one limitation of their system is the absence of an n-gram language model, which has been used by text generation systems to improve fluency.
</nextsent>
<nextsent>we take the zhang and clark system as the baseline, and incorporate an n-gram model by applying online large-margin training.
</nextsent>
<nextsent>our system significantly improved on the baseline by 3.7 bleu points.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3903">
<title id=" E12-1075.xml">syntax based word ordering incorporating a largescale language model </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>finding the best permutation for set of words according to bigram language model, for example, is np-hard, which can be proved by linear reduction from the traveling salesman problem.
</prevsent>
<prevsent>in practice, exploring the whole search space of permutations is often prevented by adding constraints.
</prevsent>
</prevsection>
<citsent citstr=" N03-1017 ">
in phrase-based machine translation (koehn et al  2003; <papid> N03-1017 </papid>koehn et al  2007), <papid> P07-2045 </papid>distortion limit is used to constrain the position of output phrases.</citsent>
<aftsection>
<nextsent>in syntax-based machine translation systems such as wu (1997) <papid> J97-3002 </papid>and chiang (2007), <papid> J07-2003 </papid>synchronous grammars limit the search space so that poly nomial time inference is feasible.</nextsent>
<nextsent>in fluency improvement (blackwood et al  2010), <papid> C10-1009 </papid>parts of translation hypotheses identified as having high local confidence are held fixed, so that word ordering elsewhere is strictly local.some recent work attempts to address the fundamental word ordering task directly, using syntactic models and heuristic search.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3904">
<title id=" E12-1075.xml">syntax based word ordering incorporating a largescale language model </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>finding the best permutation for set of words according to bigram language model, for example, is np-hard, which can be proved by linear reduction from the traveling salesman problem.
</prevsent>
<prevsent>in practice, exploring the whole search space of permutations is often prevented by adding constraints.
</prevsent>
</prevsection>
<citsent citstr=" P07-2045 ">
in phrase-based machine translation (koehn et al  2003; <papid> N03-1017 </papid>koehn et al  2007), <papid> P07-2045 </papid>distortion limit is used to constrain the position of output phrases.</citsent>
<aftsection>
<nextsent>in syntax-based machine translation systems such as wu (1997) <papid> J97-3002 </papid>and chiang (2007), <papid> J07-2003 </papid>synchronous grammars limit the search space so that poly nomial time inference is feasible.</nextsent>
<nextsent>in fluency improvement (blackwood et al  2010), <papid> C10-1009 </papid>parts of translation hypotheses identified as having high local confidence are held fixed, so that word ordering elsewhere is strictly local.some recent work attempts to address the fundamental word ordering task directly, using syntactic models and heuristic search.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3905">
<title id=" E12-1075.xml">syntax based word ordering incorporating a largescale language model </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in practice, exploring the whole search space of permutations is often prevented by adding constraints.
</prevsent>
<prevsent>in phrase-based machine translation (koehn et al  2003; <papid> N03-1017 </papid>koehn et al  2007), <papid> P07-2045 </papid>distortion limit is used to constrain the position of output phrases.</prevsent>
</prevsection>
<citsent citstr=" J97-3002 ">
in syntax-based machine translation systems such as wu (1997) <papid> J97-3002 </papid>and chiang (2007), <papid> J07-2003 </papid>synchronous grammars limit the search space so that poly nomial time inference is feasible.</citsent>
<aftsection>
<nextsent>in fluency improvement (blackwood et al  2010), <papid> C10-1009 </papid>parts of translation hypotheses identified as having high local confidence are held fixed, so that word ordering elsewhere is strictly local.some recent work attempts to address the fundamental word ordering task directly, using syntactic models and heuristic search.</nextsent>
<nextsent>wan et al  (2009) <papid> E09-1097 </papid>uses dependency grammar to solve word ordering, and zhang and clark (2011) <papid> D11-1106 </papid>uses ccg (steedman, 2000) for word ordering and word choice.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3906">
<title id=" E12-1075.xml">syntax based word ordering incorporating a largescale language model </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in practice, exploring the whole search space of permutations is often prevented by adding constraints.
</prevsent>
<prevsent>in phrase-based machine translation (koehn et al  2003; <papid> N03-1017 </papid>koehn et al  2007), <papid> P07-2045 </papid>distortion limit is used to constrain the position of output phrases.</prevsent>
</prevsection>
<citsent citstr=" J07-2003 ">
in syntax-based machine translation systems such as wu (1997) <papid> J97-3002 </papid>and chiang (2007), <papid> J07-2003 </papid>synchronous grammars limit the search space so that poly nomial time inference is feasible.</citsent>
<aftsection>
<nextsent>in fluency improvement (blackwood et al  2010), <papid> C10-1009 </papid>parts of translation hypotheses identified as having high local confidence are held fixed, so that word ordering elsewhere is strictly local.some recent work attempts to address the fundamental word ordering task directly, using syntactic models and heuristic search.</nextsent>
<nextsent>wan et al  (2009) <papid> E09-1097 </papid>uses dependency grammar to solve word ordering, and zhang and clark (2011) <papid> D11-1106 </papid>uses ccg (steedman, 2000) for word ordering and word choice.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3907">
<title id=" E12-1075.xml">syntax based word ordering incorporating a largescale language model </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in phrase-based machine translation (koehn et al  2003; <papid> N03-1017 </papid>koehn et al  2007), <papid> P07-2045 </papid>distortion limit is used to constrain the position of output phrases.</prevsent>
<prevsent>in syntax-based machine translation systems such as wu (1997) <papid> J97-3002 </papid>and chiang (2007), <papid> J07-2003 </papid>synchronous grammars limit the search space so that poly nomial time inference is feasible.</prevsent>
</prevsection>
<citsent citstr=" C10-1009 ">
in fluency improvement (blackwood et al  2010), <papid> C10-1009 </papid>parts of translation hypotheses identified as having high local confidence are held fixed, so that word ordering elsewhere is strictly local.some recent work attempts to address the fundamental word ordering task directly, using syntactic models and heuristic search.</citsent>
<aftsection>
<nextsent>wan et al  (2009) <papid> E09-1097 </papid>uses dependency grammar to solve word ordering, and zhang and clark (2011) <papid> D11-1106 </papid>uses ccg (steedman, 2000) for word ordering and word choice.</nextsent>
<nextsent>the use of syntax models makes their search problems harder than word permutation using an -gram language model only.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3912">
<title id=" E12-1075.xml">syntax based word ordering incorporating a largescale language model </title>
<section> the statistical model and decoding.  </section>
<citcontext>
<prevsection>
<prevsent>algorithm we take z&c; as our baseline system.
</prevsent>
<prevsent>given multi-set of input words, the baseline system builds ccg derivation by choosing and ordering words from the input set.
</prevsent>
</prevsection>
<citsent citstr=" J07-3004 ">
the scoring model is trained using ccgbank (hockenmaier and steedman, 2007), <papid> J07-3004 </papid>and best-first decoding is applied.</citsent>
<aftsection>
<nextsent>we apply the same decoding framework in this paper,but apply an improved training process, and incorporate an -gram language model into the syntax model.
</nextsent>
<nextsent>in this section, we describe and discuss the baseline statistical model and decoding framework, motivating our extensions.
</nextsent>
<nextsent>2.1 combinatory categorial grammar.
</nextsent>
<nextsent>ccg, and parsing with ccg, has been described elsewhere (clark and curran, 2007; <papid> J07-4004 </papid>hockenmaier and steedman, 2002); <papid> P02-1043 </papid>here we provide only short description.ccg (steedman, 2000) is lexicalized grammar formalism, which associates each word in sentence with lexical category.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3913">
<title id=" E12-1075.xml">syntax based word ordering incorporating a largescale language model </title>
<section> the statistical model and decoding.  </section>
<citcontext>
<prevsection>
<prevsent>in this section, we describe and discuss the baseline statistical model and decoding framework, motivating our extensions.
</prevsent>
<prevsent>2.1 combinatory categorial grammar.
</prevsent>
</prevsection>
<citsent citstr=" J07-4004 ">
ccg, and parsing with ccg, has been described elsewhere (clark and curran, 2007; <papid> J07-4004 </papid>hockenmaier and steedman, 2002); <papid> P02-1043 </papid>here we provide only short description.ccg (steedman, 2000) is lexicalized grammar formalism, which associates each word in sentence with lexical category.</citsent>
<aftsection>
<nextsent>there is small number of basic lexical categories, such as noun (n), noun phrase (np), and prepositional phrase(pp).
</nextsent>
<nextsent>complex lexical categories are formed recursively from basic categories and slashes, which indicate the directions of arguments.
</nextsent>
<nextsent>the ccggrammar used by our system is read off the derivations in ccgbank, following hockenmaier and steedman (2002), <papid> P02-1043 </papid>meaning that the ccg combina tory rules are encoded as rule instances, together with number of additional rules which deal with punctuation and type-changing.</nextsent>
<nextsent>given sentence,its ccg derivation can be produced by first assigning lexical category to each word, and then recursively applying ccg rules bottom-up.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3914">
<title id=" E12-1075.xml">syntax based word ordering incorporating a largescale language model </title>
<section> the statistical model and decoding.  </section>
<citcontext>
<prevsection>
<prevsent>in this section, we describe and discuss the baseline statistical model and decoding framework, motivating our extensions.
</prevsent>
<prevsent>2.1 combinatory categorial grammar.
</prevsent>
</prevsection>
<citsent citstr=" P02-1043 ">
ccg, and parsing with ccg, has been described elsewhere (clark and curran, 2007; <papid> J07-4004 </papid>hockenmaier and steedman, 2002); <papid> P02-1043 </papid>here we provide only short description.ccg (steedman, 2000) is lexicalized grammar formalism, which associates each word in sentence with lexical category.</citsent>
<aftsection>
<nextsent>there is small number of basic lexical categories, such as noun (n), noun phrase (np), and prepositional phrase(pp).
</nextsent>
<nextsent>complex lexical categories are formed recursively from basic categories and slashes, which indicate the directions of arguments.
</nextsent>
<nextsent>the ccggrammar used by our system is read off the derivations in ccgbank, following hockenmaier and steedman (2002), <papid> P02-1043 </papid>meaning that the ccg combina tory rules are encoded as rule instances, together with number of additional rules which deal with punctuation and type-changing.</nextsent>
<nextsent>given sentence,its ccg derivation can be produced by first assigning lexical category to each word, and then recursively applying ccg rules bottom-up.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3916">
<title id=" E12-1075.xml">syntax based word ordering incorporating a largescale language model </title>
<section> the statistical model and decoding.  </section>
<citcontext>
<prevsection>
<prevsent>the process repeats until goal edge is found, or timeout limit is reached.
</prevsent>
<prevsent>in the latter case, default output is produced using existing edges in the chart.pseudocode for the decoder is shown as algorithm 1.
</prevsent>
</prevsection>
<citsent citstr=" J98-2004 ">
again it is reminiscent of best-first parser (caraballo and charniak, 1998) <papid> J98-2004 </papid>in the useof an agenda and chart, but is fundamentally different due to the fact that there is no input order.</citsent>
<aftsection>
<nextsent>2.3 statistical model and feature templates.
</nextsent>
<nextsent>the baseline system uses linear model to score hypotheses.
</nextsent>
<nextsent>for an edge e, its score is defined as: f(e) = ?(e) ? ?, where ?(e) represents the feature vector of and ? is the parameter vector of the model.
</nextsent>
<nextsent>during decoding, feature vectors are computed incrementally.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3917">
<title id=" E12-1075.xml">syntax based word ordering incorporating a largescale language model </title>
<section> training.  </section>
<citcontext>
<prevsection>
<prevsent>edges being constructed before harder?
</prevsent>
<prevsent>ones (i.e. those that are less likely to be correct), and therefore improve the output accuracy.
</prevsent>
</prevsection>
<citsent citstr=" P07-1096 ">
this perspective has been observed by other works of learning-guided-search(shen et al  2007; <papid> P07-1096 </papid>shen and joshi, 2008; <papid> D08-1052 </papid>goldberg and elhadad, 2010).<papid> N10-1115 </papid></citsent>
<aftsection>
<nextsent>intuitively, the score difference between easy gold-standard and harder gold-standard edges should not be as great as the difference between gold-standard and non-gold standard edges.
</nextsent>
<nextsent>the perceptron update cannot provide such control of separation, because the amount of update is fixed to 1.
</nextsent>
<nextsent>as described earlier, we treat parameter update as finding separation between correct and incorrect edges, in which the global feature vectors ?,rather than ?, are considered.
</nextsent>
<nextsent>given positive example e+ and negative example e?, we make minimum update so that the score of e+ is higher than that of e?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3918">
<title id=" E12-1075.xml">syntax based word ordering incorporating a largescale language model </title>
<section> training.  </section>
<citcontext>
<prevsection>
<prevsent>edges being constructed before harder?
</prevsent>
<prevsent>ones (i.e. those that are less likely to be correct), and therefore improve the output accuracy.
</prevsent>
</prevsection>
<citsent citstr=" D08-1052 ">
this perspective has been observed by other works of learning-guided-search(shen et al  2007; <papid> P07-1096 </papid>shen and joshi, 2008; <papid> D08-1052 </papid>goldberg and elhadad, 2010).<papid> N10-1115 </papid></citsent>
<aftsection>
<nextsent>intuitively, the score difference between easy gold-standard and harder gold-standard edges should not be as great as the difference between gold-standard and non-gold standard edges.
</nextsent>
<nextsent>the perceptron update cannot provide such control of separation, because the amount of update is fixed to 1.
</nextsent>
<nextsent>as described earlier, we treat parameter update as finding separation between correct and incorrect edges, in which the global feature vectors ?,rather than ?, are considered.
</nextsent>
<nextsent>given positive example e+ and negative example e?, we make minimum update so that the score of e+ is higher than that of e?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3919">
<title id=" E12-1075.xml">syntax based word ordering incorporating a largescale language model </title>
<section> training.  </section>
<citcontext>
<prevsection>
<prevsent>edges being constructed before harder?
</prevsent>
<prevsent>ones (i.e. those that are less likely to be correct), and therefore improve the output accuracy.
</prevsent>
</prevsection>
<citsent citstr=" N10-1115 ">
this perspective has been observed by other works of learning-guided-search(shen et al  2007; <papid> P07-1096 </papid>shen and joshi, 2008; <papid> D08-1052 </papid>goldberg and elhadad, 2010).<papid> N10-1115 </papid></citsent>
<aftsection>
<nextsent>intuitively, the score difference between easy gold-standard and harder gold-standard edges should not be as great as the difference between gold-standard and non-gold standard edges.
</nextsent>
<nextsent>the perceptron update cannot provide such control of separation, because the amount of update is fixed to 1.
</nextsent>
<nextsent>as described earlier, we treat parameter update as finding separation between correct and incorrect edges, in which the global feature vectors ?,rather than ?, are considered.
</nextsent>
<nextsent>given positive example e+ and negative example e?, we make minimum update so that the score of e+ is higher than that of e?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3920">
<title id=" E12-1075.xml">syntax based word ordering incorporating a largescale language model </title>
<section> incorporating an n-gram language.  </section>
<citcontext>
<prevsection>
<prevsent>we show empirically that this training algorithm significantly outperforms the perceptron training of the baseline system in section 5.
</prevsent>
<prevsent>an advantage of our new training algorithm is that it enables the accommodation of separately trained -gram model into the system.
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
model since the seminal work of the ibm models (brown et al  1993), -<papid> J93-2003 </papid>gram language model shave been used as standard component in statistical machine translation systems to control output fluency.</citsent>
<aftsection>
<nextsent>for the syntax-based generation system, the incorporation of an -gram language model can potentially improve the local fluency of output sequences.
</nextsent>
<nextsent>in addition, the -gram language model can be trained separately using large amount of data, while the syntax-based model requires manual annotation for training.
</nextsent>
<nextsent>the standard method for the combination of syntax model and an -gram model is linear interpolation.
</nextsent>
<nextsent>we incorporate four gram, trigram and bigram scores into our syntax model, so that the score of an edge becomes: (e) = f(e) + g(e) = f(e) + ? ?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3922">
<title id=" E12-1075.xml">syntax based word ordering incorporating a largescale language model </title>
<section> incorporating an n-gram language.  </section>
<citcontext>
<prevsection>
<prevsent>as discussed earlier, polynomial-time decoding is typically feasible for syntax-based machine translation systems without an -gram language model, due to constraints from thegrammar.
</prevsent>
<prevsent>in these cases, incorporation of gram language models can significantly increase the complexity of dynamic-programming decoder (bar-hillel et al  1961).
</prevsent>
</prevsection>
<citsent citstr=" P11-1008 ">
efficient search has been achieved using chart pruning (chiang,2007) <papid> J07-2003 </papid>and iterative numerical approaches to constrained optimization (rush and collins, 2011).<papid> P11-1008 </papid>in contrast, the incorporation of an -gram language model into our decoder is more straightforward, and does not add to its asymptotic complexity, due to the heuristic nature of the decoder.</citsent>
<aftsection>
<nextsent>we use sections 221 of ccgbank to train our syntax model, section 00 for development and section 23 for the final test.
</nextsent>
<nextsent>derivations fromccgbank are transformed into inputs by turning their surface strings into multi-sets of words.
</nextsent>
<nextsent>following z&c;, we treat base noun phrases (i.e. nps that do not recursively contain other nps) as atomic units for the input.
</nextsent>
<nextsent>output sequences are compared with the original sentences to evaluate their quality.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3923">
<title id=" E12-1075.xml">syntax based word ordering incorporating a largescale language model </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>following z&c;, we treat base noun phrases (i.e. nps that do not recursively contain other nps) as atomic units for the input.
</prevsent>
<prevsent>output sequences are compared with the original sentences to evaluate their quality.
</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
we follow previous work and usethe bleu metric (papineni et al  2002) <papid> P02-1040 </papid>to compare outputs with references.</citsent>
<aftsection>
<nextsent>z&c; use two methods to construct leaf edges.
</nextsent>
<nextsent>the first is to assign lexical categories according to dictionary.
</nextsent>
<nextsent>there are 26.8 lexical categories for each word on average using this method, corresponding to 26.8 leaf edges.
</nextsent>
<nextsent>the other method is to use pre-processing step ? ccg super tagger (clark and curran, 2007) ? <papid> J07-4004 </papid>to prune candidate lexical categories according to the gold ccgbank sentences tokens training 39,604 929,552 development 1,913 45,422 gigaword v4 sentences tokens afp 30,363,052 684,910,697 xin 15,982,098 340,666,976 table 1: number of sentences and tokens by language model source.standard sequence, assuming that for some problems the ambiguities can be reduced (e.g. when the input is already partly correctly ordered).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3926">
<title id=" E12-1075.xml">syntax based word ordering incorporating a largescale language model </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the results are similar to table 5: our large-margin training systems outperforms the baseline by 1.5 bleu points, and adding the -gram model gave further 1.4 point improvement.
</prevsent>
<prevsent>the scores could be significantly increased by using larger timeout, as shown in our earlier development experiments.
</prevsent>
</prevsection>
<citsent citstr=" J05-3002 ">
there is recent line of research on text-to text generation, which studies the linearization of dependency structures (barzilay and mckeown, 2005; <papid> J05-3002 </papid>filippova and strube, 2007; <papid> P07-1041 </papid>filippova and strube, 2009; <papid> N09-2057 </papid>bohnet et al  2010; <papid> C10-1012 </papid>guo et al  2011).</citsent>
<aftsection>
<nextsent>unlike our system, and wan et al (2009),<papid> E09-1097 </papid>input dependencies provide additional information to these systems.</nextsent>
<nextsent>although the search space can be constrained by the assumption of projectivity, permutation of modifiers of the same headword makes exact inference for tree linearization intractable.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3927">
<title id=" E12-1075.xml">syntax based word ordering incorporating a largescale language model </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the results are similar to table 5: our large-margin training systems outperforms the baseline by 1.5 bleu points, and adding the -gram model gave further 1.4 point improvement.
</prevsent>
<prevsent>the scores could be significantly increased by using larger timeout, as shown in our earlier development experiments.
</prevsent>
</prevsection>
<citsent citstr=" P07-1041 ">
there is recent line of research on text-to text generation, which studies the linearization of dependency structures (barzilay and mckeown, 2005; <papid> J05-3002 </papid>filippova and strube, 2007; <papid> P07-1041 </papid>filippova and strube, 2009; <papid> N09-2057 </papid>bohnet et al  2010; <papid> C10-1012 </papid>guo et al  2011).</citsent>
<aftsection>
<nextsent>unlike our system, and wan et al (2009),<papid> E09-1097 </papid>input dependencies provide additional information to these systems.</nextsent>
<nextsent>although the search space can be constrained by the assumption of projectivity, permutation of modifiers of the same headword makes exact inference for tree linearization intractable.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3928">
<title id=" E12-1075.xml">syntax based word ordering incorporating a largescale language model </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the results are similar to table 5: our large-margin training systems outperforms the baseline by 1.5 bleu points, and adding the -gram model gave further 1.4 point improvement.
</prevsent>
<prevsent>the scores could be significantly increased by using larger timeout, as shown in our earlier development experiments.
</prevsent>
</prevsection>
<citsent citstr=" N09-2057 ">
there is recent line of research on text-to text generation, which studies the linearization of dependency structures (barzilay and mckeown, 2005; <papid> J05-3002 </papid>filippova and strube, 2007; <papid> P07-1041 </papid>filippova and strube, 2009; <papid> N09-2057 </papid>bohnet et al  2010; <papid> C10-1012 </papid>guo et al  2011).</citsent>
<aftsection>
<nextsent>unlike our system, and wan et al (2009),<papid> E09-1097 </papid>input dependencies provide additional information to these systems.</nextsent>
<nextsent>although the search space can be constrained by the assumption of projectivity, permutation of modifiers of the same headword makes exact inference for tree linearization intractable.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3929">
<title id=" E12-1075.xml">syntax based word ordering incorporating a largescale language model </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the results are similar to table 5: our large-margin training systems outperforms the baseline by 1.5 bleu points, and adding the -gram model gave further 1.4 point improvement.
</prevsent>
<prevsent>the scores could be significantly increased by using larger timeout, as shown in our earlier development experiments.
</prevsent>
</prevsection>
<citsent citstr=" C10-1012 ">
there is recent line of research on text-to text generation, which studies the linearization of dependency structures (barzilay and mckeown, 2005; <papid> J05-3002 </papid>filippova and strube, 2007; <papid> P07-1041 </papid>filippova and strube, 2009; <papid> N09-2057 </papid>bohnet et al  2010; <papid> C10-1012 </papid>guo et al  2011).</citsent>
<aftsection>
<nextsent>unlike our system, and wan et al (2009),<papid> E09-1097 </papid>input dependencies provide additional information to these systems.</nextsent>
<nextsent>although the search space can be constrained by the assumption of projectivity, permutation of modifiers of the same headword makes exact inference for tree linearization intractable.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3934">
<title id=" E09-1027.xml">cognitively motivated features for readability assessment </title>
<section> related work on readability metrics.  </section>
<citcontext>
<prevsection>
<prevsent>researchers in computational linguistics have investigated the use of statistical language models (unigram in particular) to capture the range of vocabulary from one grade level to another (si and callan, 2001; collins-thompson and callan, 2004).
</prevsent>
<prevsent>these metrics predicted readability better than traditional formulas when tested against corpus of web pages.
</prevsent>
</prevsection>
<citsent citstr=" N07-1058 ">
the use of syntactic features was also investigated (schwarm and ostendorf, 2005; heilman et al, 2007; <papid> N07-1058 </papid>petersen and ostendorf, 2009) in the assessment of text readability for english as second language readers.</citsent>
<aftsection>
<nextsent>while lexical features alone outperform syntactic features in classifying texts according to their reading levels, combining the lexical and syntactic features yields the best results.
</nextsent>
<nextsent>several elegant metrics that focus solely on the syntax of text have also been developed.
</nextsent>
<nextsent>the yngve (1960) measure, for instance, focuses on the depth of embedding of nodes in the parse tree; others use the ratio of terminal to nonterminal nodes in the parse tree of sentence (miller and chomsky, 1963; frazier, 1985).
</nextsent>
<nextsent>these metrics have been used to analyze the writing of potential alzheimer patients to detect mild cognitive impairments (roark, mitchell, and hollingshead, 2007), thereby indicating that cognitively motivated features of text are valuable when creating tools for specific populations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3935">
<title id=" E09-1027.xml">cognitively motivated features for readability assessment </title>
<section> related work on readability metrics.  </section>
<citcontext>
<prevsection>
<prevsent>the yngve (1960) measure, for instance, focuses on the depth of embedding of nodes in the parse tree; others use the ratio of terminal to nonterminal nodes in the parse tree of sentence (miller and chomsky, 1963; frazier, 1985).
</prevsent>
<prevsent>these metrics have been used to analyze the writing of potential alzheimer patients to detect mild cognitive impairments (roark, mitchell, and hollingshead, 2007), thereby indicating that cognitively motivated features of text are valuable when creating tools for specific populations.
</prevsent>
</prevsection>
<citsent citstr=" J08-1001 ">
barzilay and lapata (2008) <papid> J08-1001 </papid>presented early work in investigating the use of discourse to distinguish abridged from original encyclopedia articles.</citsent>
<aftsection>
<nextsent>their focus, however, is on style detection rather than readability assessment perse.
</nextsent>
<nextsent>coh-metrix is tool for automatically calculating text coherence based on features such as repetition of lexical items across sentences and latent semantic analysis (mcnamara et al, 2006).
</nextsent>
<nextsent>the tool is based on comprehension data collected from children and college students.
</nextsent>
<nextsent>our research differs from related work in that we seek to produce an automatic readability metric that is tailored to the literacy skills of adults with id. because of the specific cognitive characteristics of these users, it is an open question whether existing readability metrics and features are useful for assessing readability for adults with id. many of these earlier metrics have focused on the task of assigning texts to particular elementary school grade levels.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3936">
<title id=" E09-1027.xml">cognitively motivated features for readability assessment </title>
<section> linguistic features and readability.  </section>
<citcontext>
<prevsection>
<prevsent>5.1 basic features used in earlier work.
</prevsent>
<prevsent>we have also implemented features inspired by earlier research on readability.
</prevsent>
</prevsection>
<citsent citstr=" A00-2018 ">
petersen and ostendorf (2009) included features calculated from parsing the sentences in their corpus using the charniak parser (charniak, 2000): <papid> A00-2018 </papid>average parse tree height, average number of noun phrases per sentence, average number of verb phrases per sentence, and average number of sbars per sen tence.</citsent>
<aftsection>
<nextsent>we have implemented versions of most of these parse-tree-related features for our project.
</nextsent>
<nextsent>we also parse the sentences in our corpus using charniaks parser and calculate the following features listed in table 1: anp, an, avp, aadj, asbr, app, nnp, nn, nvp, nadj, nsbr, and npp.
</nextsent>
<nextsent>5.2 novel cognitively-motivated features.
</nextsent>
<nextsent>because of the special reading characteristics of our target users, we have designed set of cogni tively motivated features to predict readability of texts for adults with id. we have discussed how working memory limits the semantic encoding of new information by these users; so, our features indicate the number of entities in text that the reader must keep in mind while reading each sentence and throughout the entire document.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3937">
<title id=" E12-1080.xml">an unsupervised dynamic bayesian network approach to measuring speech style accommodation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our hypothesis is that stylistic shifts that occur as result of social processes are likely to display some consistency over time, and if we leverage this insight in our model,we will achievea model that better captures inherent structure within speech.
</prevsent>
<prevsent>socio linguistic research on speech style and its resulting social interpretation has frequently focused on the ways in which shifts in style areused to achieve strategic goals within interactions, for example the ways in which speakers may adapt their speaking style to suppress differences and accentuate similarities between themselves and their interlocutors in order to build solidarity (coupland, 2007; eckert &amp; rickford,2001; sanders, 1987).
</prevsent>
</prevsection>
<citsent citstr=" P11-2020 ">
we refer to this stylistic convergence as speech style accommodation.in the language technologies community, one targeted practical benefit of such modeling has been the achievement of more natural interactions with speech dialogue systems (levitan et al  2011).<papid> P11-2020 </papid></citsent>
<aftsection>
<nextsent>monitoring social processes from speech or language data has other practical benefits as well,such as enabling monitoring how beneficial an interaction is for group learning (ward &amp; litman, 2007; gweon, 2011), how equal participation is within group (dimicco et al  2004), or how conducive an environment is for fostering sense of belonging and identification with community (wang et al  2011).
</nextsent>
<nextsent>typical work on computational models of speech style accommodation have focused on specific aspects of style that may be accommodated, such as the frequency or timing of pauses or back channels (i.e., words that show attention likeun huh?
</nextsent>
<nextsent>or ok?), pitch, or speaking rate (ed lund et al  2009; levitan &amp; hirschberg, 2011).
</nextsent>
<nextsent>in this paper, we present an unsupervised dynamic bayesian model that allows us to model speech style accommodation in way that does notre quire us to specify which linguistic features weare targeting.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3940">
<title id=" E12-1080.xml">an unsupervised dynamic bayesian network approach to measuring speech style accommodation </title>
<section> theoretical framework.  </section>
<citcontext>
<prevsection>
<prevsent>one common goal of prior work on modeling speech dynamics has been for the purpose of informing the design of more natural spoken dialogue systems (levitan et al ., 2011).<papid> P11-2020 </papid></prevsent>
<prevsent>the practical goal of our work is to measure the social processes themselves, forex ample in order to estimate the extent to which group discussions show signs of productive consensus building processes (gweon, 2011).</prevsent>
</prevsection>
<citsent citstr=" D09-1035 ">
much prior work on modeling emotional speech has sought to identify features that themselves havea social interpretation, such as features that predict emotional states like uncertainty (liscombe et al  2005), or surprise (ang et al  2002), or social strategies like flirting (ranganath et al 2009).<papid> D09-1035 </papid></citsent>
<aftsection>
<nextsent>however, our goal is to monitor social processes that evolve over time and are reflected in the change in speech dynamics.
</nextsent>
<nextsent>examples include fostering trust, forming attachments, or building solidarity.
</nextsent>
<nextsent>2.1 defining speech style accommmodation.
</nextsent>
<nextsent>the concept of what we refer to as speech style accommodation has its roots in the field of the social psychology of language, where the many ways in which social processes are reflected through language, and conversely, how language influences social processes, are the objects of investigation (giles &amp; co upland, 1991).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3941">
<title id=" E12-1080.xml">an unsupervised dynamic bayesian network approach to measuring speech style accommodation </title>
<section> theoretical framework.  </section>
<citcontext>
<prevsection>
<prevsent>they say it.
</prevsent>
<prevsent>we are only interested in measuring accommodation from speech in this work.
</prevsent>
</prevsection>
<citsent citstr=" N06-2031 ">
there has been work on convergence in text such as syntactic adaptation (reitter et al 2006) <papid> N06-2031 </papid>and language similarity in online communities (huffaker et al  2006).<papid> W06-3403 </papid></citsent>
<aftsection>
<nextsent>2.2 social interpretation of speech style.
</nextsent>
<nextsent>accommodation it has long been established that while some speech style shifts are subconscious, speakers may also choose to adapt their way of speaking in order to achieve social effects within an interaction (sanders, 1987).
</nextsent>
<nextsent>one of the main motives for accommodation is to decrease social distance.
</nextsent>
<nextsent>on variety of levels, speech style accommodation has been found to affect the impression that speakers give within an interaction.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3942">
<title id=" E12-1080.xml">an unsupervised dynamic bayesian network approach to measuring speech style accommodation </title>
<section> theoretical framework.  </section>
<citcontext>
<prevsection>
<prevsent>they say it.
</prevsent>
<prevsent>we are only interested in measuring accommodation from speech in this work.
</prevsent>
</prevsection>
<citsent citstr=" W06-3403 ">
there has been work on convergence in text such as syntactic adaptation (reitter et al 2006) <papid> N06-2031 </papid>and language similarity in online communities (huffaker et al  2006).<papid> W06-3403 </papid></citsent>
<aftsection>
<nextsent>2.2 social interpretation of speech style.
</nextsent>
<nextsent>accommodation it has long been established that while some speech style shifts are subconscious, speakers may also choose to adapt their way of speaking in order to achieve social effects within an interaction (sanders, 1987).
</nextsent>
<nextsent>one of the main motives for accommodation is to decrease social distance.
</nextsent>
<nextsent>on variety of levels, speech style accommodation has been found to affect the impression that speakers give within an interaction.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3943">
<title id=" E12-1080.xml">an unsupervised dynamic bayesian network approach to measuring speech style accommodation </title>
<section> theoretical framework.  </section>
<citcontext>
<prevsection>
<prevsent>bourhis &amp; giles (1977) found that welsh speakers while answering to an english surveyor broadened their welsh accent when their ethnic identity was challenged.
</prevsent>
<prevsent>scotton (1985)found that few people hesitated to repeat lexical patterns of their partners to maintain integrity.
</prevsent>
</prevsection>
<citsent citstr=" P08-2043 ">
nenkova et al (2008) <papid> P08-2043 </papid>found that accommodation on high frequency words correlates with naturalness, task success, and coordinated turn-taking behavior.</citsent>
<aftsection>
<nextsent>2.3 computational models of speech style.
</nextsent>
<nextsent>accommodation prior research has attempted to quantify accommodation computationally by measuring similarity of speech and lexical features either over full conversations or by comparing the similarity inthe first half and the second half of the conversation.
</nextsent>
<nextsent>for example, edlund et al (2009) measure accommodation in pause and gap length using measures such as synch rony and convergence.levitan &amp; hirschberg (2011) found that accommodation is also found in special social behaviors within conversation such as backchannels.
</nextsent>
<nextsent>they show that speakers in conversation tend to use similar kinds of speech cues such as high pitch at the end of utterance to invite back channel from their partner.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3946">
<title id=" E09-2004.xml">gossip galore x2013 a self learning agent for exchanging pop trivia </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the system presented here is developed within the project responsive artificial situated cognitive agents living and learning on the internet (ras calli) supported by the european commission cognitive systems programme (ist-27596-2004).the goal of the project is to develop and implement cognitively enhanced artificial agents, using technologies in natural language processing, question answering, web-based information extraction, semantic web and interaction driven profiling with cognitive modelling (krenn, 2008).
</prevsent>
<prevsent>this paper describes conversational agent gossip galore?, an active self-learning system that can learn, update and interpret information from the web, and can make conversations with users and provide answers to their questions in the domain of celebrity gossip.
</prevsent>
</prevsection>
<citsent citstr=" P07-1074 ">
in more detail, by applying minimally supervised relation extraction system (xu et al, 2007; <papid> P07-1074 </papid>xu et al, 2008), the agent automatically collects the knowledge from relevant web sites, and also communicates with the users using question-answering engine via 3d graphic interface.</citsent>
<aftsection>
<nextsent>this paper is organized as follows.
</nextsent>
<nextsent>section 2 gives an overview of the system architecture and figure 1: gossip galore responding to tell me something about carla bruni!presents the design and functional ities of the components.
</nextsent>
<nextsent>section 3 explains the system setup and discusses implementation details, and finally section 4 draws conclusions.
</nextsent>
<nextsent>figure 1 shows use case of the system.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3950">
<title id=" E09-3006.xml">a chain starting classifier of definite nps in spanish </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the output of anaphora resolution1 are noun pronoun pairs (or pairs of discourse segment and pronoun in some cases), whereas the output of coreference resolution are chains containing variety of items: pronouns, full nps, discourse seg ments...
</prevsent>
<prevsent>thus, coreference resolution requires wider range of strategies in order to build the full chains of co referent mentions.2 1a different matter is the resolution of anaphoric full nps, i.e. those semantically dependent on previous mention.2we follow the ace terminology (nist, 2003) but instead of talking of objects in the world we talk of objects inthe discourse model: we use entity for an object or set of objects in the discourse model, and mention for reference to an entity.one of the problems specific to coreference resolution is determining, once mention is encountered by the system, whether it refers to an entity previously mentioned or it introduces new entity into the text.
</prevsent>
</prevsection>
<citsent citstr=" J01-4004 ">
many algorithms (aone and bennett, 1996; soon et al , 2001; <papid> J01-4004 </papid>yang et al , 2003)<papid> P03-1023 </papid>do not address this issue specifically, but implicitly assume all mentions to be potentially corefer ent and examine all possible combinations; onlyif the system fails to link mention with an already existing entity, it is considered to be chainstarting.3 however, such an approach is computationally expensive and prone to errors, since natural language is populated with huge number of entities that appear just once in the text.</citsent>
<aftsection>
<nextsent>even definite nps, which are traditionally believed to refer to old entities, have been demonstrated to start coreference chain over 50% of the times (fraurud, 1990; poesio and vieira, 1998).<papid> J98-2001 </papid></nextsent>
<nextsent>an alternative line of research has considered applying filter prior to coreference resolution that classifies mentions as either chain starting or coreferent.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3951">
<title id=" E09-3006.xml">a chain starting classifier of definite nps in spanish </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the output of anaphora resolution1 are noun pronoun pairs (or pairs of discourse segment and pronoun in some cases), whereas the output of coreference resolution are chains containing variety of items: pronouns, full nps, discourse seg ments...
</prevsent>
<prevsent>thus, coreference resolution requires wider range of strategies in order to build the full chains of co referent mentions.2 1a different matter is the resolution of anaphoric full nps, i.e. those semantically dependent on previous mention.2we follow the ace terminology (nist, 2003) but instead of talking of objects in the world we talk of objects inthe discourse model: we use entity for an object or set of objects in the discourse model, and mention for reference to an entity.one of the problems specific to coreference resolution is determining, once mention is encountered by the system, whether it refers to an entity previously mentioned or it introduces new entity into the text.
</prevsent>
</prevsection>
<citsent citstr=" P03-1023 ">
many algorithms (aone and bennett, 1996; soon et al , 2001; <papid> J01-4004 </papid>yang et al , 2003)<papid> P03-1023 </papid>do not address this issue specifically, but implicitly assume all mentions to be potentially corefer ent and examine all possible combinations; onlyif the system fails to link mention with an already existing entity, it is considered to be chainstarting.3 however, such an approach is computationally expensive and prone to errors, since natural language is populated with huge number of entities that appear just once in the text.</citsent>
<aftsection>
<nextsent>even definite nps, which are traditionally believed to refer to old entities, have been demonstrated to start coreference chain over 50% of the times (fraurud, 1990; poesio and vieira, 1998).<papid> J98-2001 </papid></nextsent>
<nextsent>an alternative line of research has considered applying filter prior to coreference resolution that classifies mentions as either chain starting or coreferent.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3952">
<title id=" E09-3006.xml">a chain starting classifier of definite nps in spanish </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>thus, coreference resolution requires wider range of strategies in order to build the full chains of co referent mentions.2 1a different matter is the resolution of anaphoric full nps, i.e. those semantically dependent on previous mention.2we follow the ace terminology (nist, 2003) but instead of talking of objects in the world we talk of objects inthe discourse model: we use entity for an object or set of objects in the discourse model, and mention for reference to an entity.one of the problems specific to coreference resolution is determining, once mention is encountered by the system, whether it refers to an entity previously mentioned or it introduces new entity into the text.
</prevsent>
<prevsent>many algorithms (aone and bennett, 1996; soon et al , 2001; <papid> J01-4004 </papid>yang et al , 2003)<papid> P03-1023 </papid>do not address this issue specifically, but implicitly assume all mentions to be potentially corefer ent and examine all possible combinations; onlyif the system fails to link mention with an already existing entity, it is considered to be chainstarting.3 however, such an approach is computationally expensive and prone to errors, since natural language is populated with huge number of entities that appear just once in the text.</prevsent>
</prevsection>
<citsent citstr=" J98-2001 ">
even definite nps, which are traditionally believed to refer to old entities, have been demonstrated to start coreference chain over 50% of the times (fraurud, 1990; poesio and vieira, 1998).<papid> J98-2001 </papid></citsent>
<aftsection>
<nextsent>an alternative line of research has considered applying filter prior to coreference resolution that classifies mentions as either chain starting or coreferent.
</nextsent>
<nextsent>ng and cardie (2002) <papid> C02-1139 </papid>and poesio et al  (2005) have tested the impact of such detector on the overall coreference resolution performance with encouraging results.</nextsent>
<nextsent>our chain-starting classifier is comparable ? despite some differences4 ? to the detectors suggested by ng and cardie (2002), <papid> C02-1139 </papid>uryupina (2003)<papid> P03-2012 </papid>and poesio et al  (2005) for english, but not identical to strictly anaphoric ones5 (bean and riloff, 1999; <papid> P99-1048 </papid>uryupina, 2003),<papid> P03-2012 </papid>since non-anaphoric np can corefer with previous mention.this paper presents corpus-based study of def 3by chain starting we refer to those mentions that are the first element ? and might be the only one ? in coreference chain.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3953">
<title id=" E09-3006.xml">a chain starting classifier of definite nps in spanish </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>even definite nps, which are traditionally believed to refer to old entities, have been demonstrated to start coreference chain over 50% of the times (fraurud, 1990; poesio and vieira, 1998).<papid> J98-2001 </papid></prevsent>
<prevsent>an alternative line of research has considered applying filter prior to coreference resolution that classifies mentions as either chain starting or coreferent.</prevsent>
</prevsection>
<citsent citstr=" C02-1139 ">
ng and cardie (2002) <papid> C02-1139 </papid>and poesio et al  (2005) have tested the impact of such detector on the overall coreference resolution performance with encouraging results.</citsent>
<aftsection>
<nextsent>our chain-starting classifier is comparable ? despite some differences4 ? to the detectors suggested by ng and cardie (2002), <papid> C02-1139 </papid>uryupina (2003)<papid> P03-2012 </papid>and poesio et al  (2005) for english, but not identical to strictly anaphoric ones5 (bean and riloff, 1999; <papid> P99-1048 </papid>uryupina, 2003),<papid> P03-2012 </papid>since non-anaphoric np can corefer with previous mention.this paper presents corpus-based study of def 3by chain starting we refer to those mentions that are the first element ? and might be the only one ? in coreference chain.</nextsent>
<nextsent>4ng and cardie (2002) <papid> C02-1139 </papid>and uryupina (2003)<papid> P03-2012 </papid>do not limit to definite nps but deal with all types of nps.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3957">
<title id=" E09-3006.xml">a chain starting classifier of definite nps in spanish </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>an alternative line of research has considered applying filter prior to coreference resolution that classifies mentions as either chain starting or coreferent.
</prevsent>
<prevsent>ng and cardie (2002) <papid> C02-1139 </papid>and poesio et al  (2005) have tested the impact of such detector on the overall coreference resolution performance with encouraging results.</prevsent>
</prevsection>
<citsent citstr=" P03-2012 ">
our chain-starting classifier is comparable ? despite some differences4 ? to the detectors suggested by ng and cardie (2002), <papid> C02-1139 </papid>uryupina (2003)<papid> P03-2012 </papid>and poesio et al  (2005) for english, but not identical to strictly anaphoric ones5 (bean and riloff, 1999; <papid> P99-1048 </papid>uryupina, 2003),<papid> P03-2012 </papid>since non-anaphoric np can corefer with previous mention.this paper presents corpus-based study of def 3by chain starting we refer to those mentions that are the first element ? and might be the only one ? in coreference chain.</citsent>
<aftsection>
<nextsent>4ng and cardie (2002) <papid> C02-1139 </papid>and uryupina (2003)<papid> P03-2012 </papid>do not limit to definite nps but deal with all types of nps.</nextsent>
<nextsent>5notice the confusing use of the term anaphoric in (ng and cardie, 2002) <papid> C02-1139 </papid>for describing their chain-starting filtering module.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3959">
<title id=" E09-3006.xml">a chain starting classifier of definite nps in spanish </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>an alternative line of research has considered applying filter prior to coreference resolution that classifies mentions as either chain starting or coreferent.
</prevsent>
<prevsent>ng and cardie (2002) <papid> C02-1139 </papid>and poesio et al  (2005) have tested the impact of such detector on the overall coreference resolution performance with encouraging results.</prevsent>
</prevsection>
<citsent citstr=" P99-1048 ">
our chain-starting classifier is comparable ? despite some differences4 ? to the detectors suggested by ng and cardie (2002), <papid> C02-1139 </papid>uryupina (2003)<papid> P03-2012 </papid>and poesio et al  (2005) for english, but not identical to strictly anaphoric ones5 (bean and riloff, 1999; <papid> P99-1048 </papid>uryupina, 2003),<papid> P03-2012 </papid>since non-anaphoric np can corefer with previous mention.this paper presents corpus-based study of def 3by chain starting we refer to those mentions that are the first element ? and might be the only one ? in coreference chain.</citsent>
<aftsection>
<nextsent>4ng and cardie (2002) <papid> C02-1139 </papid>and uryupina (2003)<papid> P03-2012 </papid>do not limit to definite nps but deal with all types of nps.</nextsent>
<nextsent>5notice the confusing use of the term anaphoric in (ng and cardie, 2002) <papid> C02-1139 </papid>for describing their chain-starting filtering module.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3974">
<title id=" E09-3006.xml">a chain starting classifier of definite nps in spanish </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the definite probabilities in our approach are checked with confidence intervals in order to guarantee the reliability of the results, avoiding to draw any generalization when the corpus does not contain large enough sample.
</prevsent>
<prevsent>the heuristics concerning named entities andstorage-unit variants find an equivalent in the features used in ng and cardies (2002) <papid> C02-1139 </papid>supervised classifier that represent whether the mention is proper name (determined based on capitalization, whereas our corpus includes both weak and strong named entities) and whether previous np is analias of the current mention (on the basis of rule based alias module that tries out different trans for mations).</prevsent>
</prevsection>
<citsent citstr=" J00-4003 ">
uryupina (2003)<papid> P03-2012 </papid>and vieira and poesio (2000)<papid> J00-4003 </papid>also take capital and low case letters into account.</citsent>
<aftsection>
<nextsent>all four approaches exploit syntactic structural cues of pre- and post- modification to detect complex nps, as they are considered to be unlikely to have been previously mentioned in the discourse.
</nextsent>
<nextsent>a more fine-grained distinction is made by bean and riloff (1999) <papid> P99-1048 </papid>and vieira and poesio (2000)<papid> J00-4003 </papid>to distinguish restrictive from non-restrictive post modification by ommitting those modifiers that occur between commas, which should not be classified as chain starting.</nextsent>
<nextsent>the latter also list series of special predicates?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3986">
<title id=" E09-3006.xml">a chain starting classifier of definite nps in spanish </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in this respect, our contribution lies in adapting these ideas for the way modification occurs in spanish ? where premodifiers are rare ? and in introducing distinction between pp and ap modifiers, whichwe correlate in turn with the heads of simple defi nites.we borrow the idea of classifying definites occurring in the first sentence as chain starting from bean and riloff (1999).<papid> P99-1048 </papid></prevsent>
<prevsent>the precision and recall results obtained by these classifiers ? tested on muc corpora ? are around the eighties, and around the seventies in the case of vieira and poesio (2000)<papid> J00-4003 </papid>, who use the penn treebank.</prevsent>
</prevsection>
<citsent citstr=" P04-1018 ">
luo et al  (2004) <papid> P04-1018 </papid>make use of both linking and starting probability in their bell tree algorithm for coreference resolution, but the starting probability happens to be the complementary of the linking one.</citsent>
<aftsection>
<nextsent>the chain-starting classifier webuild can be used to fine-tune the starting probability used in the construction of coreference chains in luo et al (2004) <papid> P04-1018 </papid>style.</nextsent>
<nextsent>as fully documented by lyons (1999), definite ness varies cross-linguistically.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3989">
<title id=" E09-3006.xml">a chain starting classifier of definite nps in spanish </title>
<section> neuter definites. unlike english, the span-.  </section>
<citcontext>
<prevsection>
<prevsent>a subset of ancora-co-es consisting of 60 spanish newspaper articles (23 335 tokens, 5 747 fullnps) is kept apart for the test corpus.
</prevsent>
<prevsent>ancora co-es is the coreferentially annotated ancora-escorpus, following the guidelines described in (re casens et al , 2007).
</prevsent>
</prevsection>
<citsent citstr=" W03-2120 ">
coreference relations were annotated manually with the aid of the palinka (orasan, 2003) <papid> W03-2120 </papid>and ancorapipe (bertran et al , 2008) tools.</citsent>
<aftsection>
<nextsent>interestingly enough, the test corpus contains 2 575 definite nps, out of which 1 889 are chain-starting (1401 chain-starting definite npsare actually isolated entities), namely 73% defi nites head coreference chain, which implies that successful classifier has the potential to rule out almost three quarters of all definite mentions.
</nextsent>
<nextsent>given that chain starting is the majority class and following (ng and cardie, 2002), <papid> C02-1139 </papid>we took the one class?</nextsent>
<nextsent>classification as naive baseline: all instances were classified as chain starting, giving precision of 71.95% (first row in tables 2 and 3).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3991">
<title id=" E06-2008.xml">elleipo a module that computes coordinative ellipsis for generators that dont </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we will not deal with vp ellipsis and vp anaphora because they generate pro-formsrather than eli sions and are not restricted to coordination (cf.
</prevsent>
<prevsent>the title of the paper).in current sentence generators, the coordinative ellipsis rules are often inextricably intertwined with the rules for generating non elliptical coordinate structures, so that they can not easily be ported to other grammar formalisms?
</prevsent>
</prevsection>
<citsent citstr=" C96-2103 ">
e.g., sarkar &amp; joshi (1996) <papid> C96-2103 </papid>for tree adjoining grammar; steedman (2000) for combinatory categorial grammar; bateman, matthiessen &zeng; (1999) for functional grammar.</citsent>
<aftsection>
<nextsent>generators that do include an autonomous component for coordinative ellipsis (dalianis, 1999; shaw, 2002; hielkema, 2005), use incomplete rule sets,thus risking over- or under generation, and incorrect or unnatural output.
</nextsent>
<nextsent>the module (dubbed elleipo, from greek pi?
</nextsent>
<nextsent>i leave out?)
</nextsent>
<nextsent>we present here, is less 115 formalism-dependent and, in principle, less liable to over- or under generation than its competitors.in section 2, we sketch the theoretical back ground.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3992">
<title id=" E06-2008.xml">elleipo a module that computes coordinative ellipsis for generators that dont </title>
<section> paper </section>
<citcontext>
<prevsection>
<prevsent>of coling 1996, copenhagen, pp.
</prevsent>
<prevsent>610615.
</prevsent>
</prevsection>
<citsent citstr=" P98-2199 ">
james shaw (1998).<papid> P98-2199 </papid></citsent>
<aftsection>
<nextsent>segregatory coordination and ellipsis in text generation.
</nextsent>
<nextsent>in: procs.
</nextsent>
<nextsent>of coling 1998,montreal, pp.
</nextsent>
<nextsent>12201226.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3993">
<title id=" E12-3006.xml">yet another language identifier </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>relative entropy is useful measure of the similarity between probability distributions.
</prevsent>
<prevsent>she used texts in 18 languages from the european corpus initiative cd-rom.
</prevsent>
</prevsection>
<citsent citstr=" N10-1027 ">
she achieved 100% accuracy for bigrams.in recent years, standard classification techniques such as support vector machines also be came popular and many researchers used them kruengkrai et al(2005) or baldwin and lui (2010) <papid> N10-1027 </papid>for identifying languages.</citsent>
<aftsection>
<nextsent>nowadays, language recognition is considered as an elementary nlp task3 which can be used for educational purposes.
</nextsent>
<nextsent>mcnamee (2005) used single documents for each language from project gutenberg in 10 european languages.
</nextsent>
<nextsent>he preprocessed the training documents ? the texts were lower-cased, accent marks were retained.
</nextsent>
<nextsent>then, he computed so-called profile of each language.each profile consisted of percentage of the training data attributed to each observed word.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3994">
<title id=" E12-3006.xml">yet another language identifier </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>to classify new document, the same preprocessing was done and inner product based on the words in the document and the 1000 most common words in each language was computed.
</prevsent>
<prevsent>performance varied from 80.0% for portuguese to 99.5% for german.
</prevsent>
</prevsection>
<citsent citstr=" L08-1118 ">
some researches such as hughes et al(2006) or grothe et al(2008) <papid> L08-1118 </papid>focused in their paper son the comparison of different approaches to language identification and also proposed new goalsin that field, such as as minority languages or languages written non-roman script.</citsent>
<aftsection>
<nextsent>most of the researches in the past identified mostly up to twenty languages but in recent years, language identification of minority languages became the focus of baldwin and lui (2010), <papid> N10-1027 </papid>choong et al(2011), and majlis?</nextsent>
<nextsent>(2012).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3996">
<title id=" E06-1050.xml">a probabilistic answer type model </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we would expect the answer to be city and could filter out most of the words in the following sentence: the landed aristocracy was virtually crushed by hakon v, who reigned from 1299 to 1319,and oslo became the capital of norway, replacing bergen as the principal city of the kingdom.
</prevsent>
<prevsent>the goal of answer typing is to determine whether words semantic type is appropriate asan answer for question.
</prevsent>
</prevsection>
<citsent citstr=" N01-1005 ">
many previous approaches to answer typing, e.g., (ittycheriah et al , 2001; <papid> N01-1005 </papid>li and roth, 2002; <papid> C02-1150 </papid>krishnan et al , 2005), <papid> H05-1040 </papid>employ predefined set of answer types and use supervised learning or manually constructed rulesto classify question according to expected answer type.</citsent>
<aftsection>
<nextsent>a disadvantage of this approach is that there will always be questions whose answers do not belong to any of the predefined types.consider the question: what are tourist attractions in reims??
</nextsent>
<nextsent>the answer may be many things: church, historic residence, park, famous intersection, statue, etc. common method to deal with this problem is to define catch-all class.
</nextsent>
<nextsent>this class, however, tends not to be as effective as other answer types.
</nextsent>
<nextsent>another disadvantage of predefined answer types is with regard to granularity.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3997">
<title id=" E06-1050.xml">a probabilistic answer type model </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we would expect the answer to be city and could filter out most of the words in the following sentence: the landed aristocracy was virtually crushed by hakon v, who reigned from 1299 to 1319,and oslo became the capital of norway, replacing bergen as the principal city of the kingdom.
</prevsent>
<prevsent>the goal of answer typing is to determine whether words semantic type is appropriate asan answer for question.
</prevsent>
</prevsection>
<citsent citstr=" C02-1150 ">
many previous approaches to answer typing, e.g., (ittycheriah et al , 2001; <papid> N01-1005 </papid>li and roth, 2002; <papid> C02-1150 </papid>krishnan et al , 2005), <papid> H05-1040 </papid>employ predefined set of answer types and use supervised learning or manually constructed rulesto classify question according to expected answer type.</citsent>
<aftsection>
<nextsent>a disadvantage of this approach is that there will always be questions whose answers do not belong to any of the predefined types.consider the question: what are tourist attractions in reims??
</nextsent>
<nextsent>the answer may be many things: church, historic residence, park, famous intersection, statue, etc. common method to deal with this problem is to define catch-all class.
</nextsent>
<nextsent>this class, however, tends not to be as effective as other answer types.
</nextsent>
<nextsent>another disadvantage of predefined answer types is with regard to granularity.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3998">
<title id=" E06-1050.xml">a probabilistic answer type model </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we would expect the answer to be city and could filter out most of the words in the following sentence: the landed aristocracy was virtually crushed by hakon v, who reigned from 1299 to 1319,and oslo became the capital of norway, replacing bergen as the principal city of the kingdom.
</prevsent>
<prevsent>the goal of answer typing is to determine whether words semantic type is appropriate asan answer for question.
</prevsent>
</prevsection>
<citsent citstr=" H05-1040 ">
many previous approaches to answer typing, e.g., (ittycheriah et al , 2001; <papid> N01-1005 </papid>li and roth, 2002; <papid> C02-1150 </papid>krishnan et al , 2005), <papid> H05-1040 </papid>employ predefined set of answer types and use supervised learning or manually constructed rulesto classify question according to expected answer type.</citsent>
<aftsection>
<nextsent>a disadvantage of this approach is that there will always be questions whose answers do not belong to any of the predefined types.consider the question: what are tourist attractions in reims??
</nextsent>
<nextsent>the answer may be many things: church, historic residence, park, famous intersection, statue, etc. common method to deal with this problem is to define catch-all class.
</nextsent>
<nextsent>this class, however, tends not to be as effective as other answer types.
</nextsent>
<nextsent>another disadvantage of predefined answer types is with regard to granularity.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I3999">
<title id=" E06-1050.xml">a probabilistic answer type model </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>by computing the probability of an answer candidate occurring in the question contexts directly, we avoid having multiple candidates with the same level of appropriateness as answers.there have been variety of approaches to determine the answer types, which are also known as qtargets (echihabi et al , 2003).
</prevsent>
<prevsent>most previous approaches classify the answer type of question as one of set of predefined types.
</prevsent>
</prevsection>
<citsent citstr=" W01-1203 ">
many systems construct the classification rules manually (cui et al , 2004; greenwood, 2004; hermjakob, 2001).<papid> W01-1203 </papid></citsent>
<aftsection>
<nextsent>the rules are usually triggered by the presence of certain words in the question.
</nextsent>
<nextsent>for example, if question contains author?
</nextsent>
<nextsent>then the expected answer type is person.the number of answer types as well as the number of rules can vary great deal.
</nextsent>
<nextsent>for example, (hermjakob, 2001) <papid> W01-1203 </papid>used 276 rules for 122 answer types.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4003">
<title id=" E06-1050.xml">a probabilistic answer type model </title>
<section> resources.  </section>
<citcontext>
<prevsection>
<prevsent>word clusters are way of coping with data sparseness by abstracting given word to class of relatedwords.
</prevsent>
<prevsent>clusters, as used by our probabilistic answer typing system, play role similar to that ofnamed entity types.
</prevsent>
</prevsection>
<citsent citstr=" P93-1024 ">
many methods exist for clustering, e.g., (brown et al , 1990; cutting et al , 1992; pereira et al , 1993; <papid> P93-1024 </papid>karypis et al , 1999).</citsent>
<aftsection>
<nextsent>we used the clustering by committee (cbc) 394 table 1: words and their clusters word clusters suite software, network, wireless, ... rooms, bathrooms, restrooms, ... meeting room, conference room, ... ghost rabbit, squirrel, duck, elephant, frog, ... goblins, ghosts, vampires, ghouls, ... punk, reggae, folk, pop, hip-pop, ... huge, larger, vast, significant, ... coming-of-age, true-life, ... clouds, cloud, fog, haze, mist, ...algorithm (pantel and lin, 2002) on 10 gb english text corpus to obtain 3607 clusters.
</nextsent>
<nextsent>the following is an example cluster generated by cbc: tension, anger, anxiety, tensions, frustration, resentment, uncertainty, confusion, conflict, discontent, insecurity, controversy, unease,bitterness, dispute, disagreement, nervousness, sadness, despair, animosity, hostility, outrage, discord, pessimism, anguish, ...
</nextsent>
<nextsent>in the clustering generated by cbc, word may belong to multiple clusters.
</nextsent>
<nextsent>the clusters to which word belongs often represent the senses of the word.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4004">
<title id=" E06-1050.xml">a probabilistic answer type model </title>
<section> resources.  </section>
<citcontext>
<prevsection>
<prevsent>table 1 shows two example words and their clusters.
</prevsent>
<prevsent>3.2 contexts.
</prevsent>
</prevsection>
<citsent citstr=" P89-1010 ">
the context in which word appears often imposes constraints on the semantic type of the word.this basic idea has been exploited by many proposals for distributional similarity and clustering, e.g., (church and hanks, 1989; <papid> P89-1010 </papid>lin, 1998; <papid> P98-2127 </papid>pereira et al , 1993).<papid> P93-1024 </papid></citsent>
<aftsection>
<nextsent>similar to lin and pantel (2001), we define the contexts of word to be the undirected paths in dependency trees involving that word at either the beginning or the end.
</nextsent>
<nextsent>the following diagram shows an example dependency tree: which city hosted the 1988 winter olympics?
</nextsent>
<nextsent>det subj obj nn nn detthe links in the tree represent dependency relationships.
</nextsent>
<nextsent>the direction of link is from the headto the modifier in the relationship.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4005">
<title id=" E06-1050.xml">a probabilistic answer type model </title>
<section> resources.  </section>
<citcontext>
<prevsection>
<prevsent>table 1 shows two example words and their clusters.
</prevsent>
<prevsent>3.2 contexts.
</prevsent>
</prevsection>
<citsent citstr=" P98-2127 ">
the context in which word appears often imposes constraints on the semantic type of the word.this basic idea has been exploited by many proposals for distributional similarity and clustering, e.g., (church and hanks, 1989; <papid> P89-1010 </papid>lin, 1998; <papid> P98-2127 </papid>pereira et al , 1993).<papid> P93-1024 </papid></citsent>
<aftsection>
<nextsent>similar to lin and pantel (2001), we define the contexts of word to be the undirected paths in dependency trees involving that word at either the beginning or the end.
</nextsent>
<nextsent>the following diagram shows an example dependency tree: which city hosted the 1988 winter olympics?
</nextsent>
<nextsent>det subj obj nn nn detthe links in the tree represent dependency relationships.
</nextsent>
<nextsent>the direction of link is from the headto the modifier in the relationship.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4007">
<title id=" E06-1050.xml">a probabilistic answer type model </title>
<section> resources.  </section>
<citcontext>
<prevsection>
<prevsent>however, they tend to have fewer occurrences, making them more prone to errors arising from data sparseness.
</prevsent>
<prevsent>we have restricted the path length to two (involving at most three words) and require the two ends of the path to be nouns.
</prevsent>
</prevsection>
<citsent citstr=" H01-1046 ">
we parsed the aquaint corpus (3gb) with minipar (lin, 2001) <papid> H01-1046 </papid>and collected the frequency counts of words appearing in various contexts.</citsent>
<aftsection>
<nextsent>parsing and database construction is performedoff-line as the database is identical for all questions.
</nextsent>
<nextsent>we extracted 527,768 contexts that appeared at least 25 times in the corpus.
</nextsent>
<nextsent>an example context and its fillers are shown in figure 1.
</nextsent>
<nextsent>x host olympics subj obj africa 2 grant 1 readiness 2 ap 1 he 2 rio de janeiro 1 argentina 1 homeland 3 rome 1 athens 16 ioc 1 salt lake city 2 atlanta 3 iran 2 school 1 bangkok 1 jakarta 1 s. africa 1 . . .
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4008">
<title id=" E09-1006.xml">supervised domain adaption for wsd </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in the adaptation scenario the wsd system is trained on both source and target domain and tested in the target domain (also using cross-validation over the target data).
</prevsent>
<prevsent>the source to target scenario represents weak baseline for domain adaptation, as it does not use any examples from the target domain.
</prevsent>
</prevsection>
<citsent citstr=" W00-1322 ">
the target scenario represents the hard baseline, and in fact, if the domain adaptation scenario does not yield better results, the adaptation would have failed, as it would mean that the source examples are not useful when we do have hand-labeled target examples.previous work shows that current state-of-theart wsd systems are not able to obtain better results on the adaptation scenario compared to the target scenario (escudero et al , 2000; <papid> W00-1322 </papid>agirre and martnez, 2004; chan and ng, 2007).</citsent>
<aftsection>
<nextsent>this would mean that if user of generic wsd system (i.e. based on hand-annotated examples from genericcorpus) would need to adapt it to specific do main, he would be better off throwing away the generic examples and hand-tagging domain examples directly.
</nextsent>
<nextsent>this paper will show that domain adaptation is feasible, even for difficult domain related words, in the sense that generic corpora can be reused when deploying wsd systems in specific domains.
</nextsent>
<nextsent>we will also show that, given the source corpus, our technique can save up to60% of effort when tagging domain-related occurrences.
</nextsent>
<nextsent>we performed on publicly available corpus which was designed to study the effect of domains in wsd (koeling et al , 2005).<papid> H05-1053 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4009">
<title id=" E09-1006.xml">supervised domain adaption for wsd </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this paper will show that domain adaptation is feasible, even for difficult domain related words, in the sense that generic corpora can be reused when deploying wsd systems in specific domains.
</prevsent>
<prevsent>we will also show that, given the source corpus, our technique can save up to60% of effort when tagging domain-related occurrences.
</prevsent>
</prevsection>
<citsent citstr=" H05-1053 ">
we performed on publicly available corpus which was designed to study the effect of domains in wsd (koeling et al , 2005).<papid> H05-1053 </papid></citsent>
<aftsection>
<nextsent>it comprises 41 42 nouns which are highly relevant in the sports and finances domains, with 300 examples for each.
</nextsent>
<nextsent>the use of two target domains strengthens the conclusions of this paper.our system uses singular value decomposition (svd) in order to find correlations between terms, which are helpful to overcome the scarcity of training data in wsd (gliozzo et al , 2005).<papid> P05-1050 </papid></nextsent>
<nextsent>this work explores how this ability of svd anda combination of the resulting feature spaces improves domain adaptation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4010">
<title id=" E09-1006.xml">supervised domain adaption for wsd </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we performed on publicly available corpus which was designed to study the effect of domains in wsd (koeling et al , 2005).<papid> H05-1053 </papid></prevsent>
<prevsent>it comprises 41 42 nouns which are highly relevant in the sports and finances domains, with 300 examples for each.</prevsent>
</prevsection>
<citsent citstr=" P05-1050 ">
the use of two target domains strengthens the conclusions of this paper.our system uses singular value decomposition (svd) in order to find correlations between terms, which are helpful to overcome the scarcity of training data in wsd (gliozzo et al , 2005).<papid> P05-1050 </papid></citsent>
<aftsection>
<nextsent>this work explores how this ability of svd anda combination of the resulting feature spaces improves domain adaptation.
</nextsent>
<nextsent>we present two waysto combine the reduced spaces: kernel combination with support vector machines (svm), and nearest-neighbors (k-nn) combination.the paper is structured as follows.
</nextsent>
<nextsent>section 2 reviews prior work in the area.
</nextsent>
<nextsent>section 3 presents the datasets used.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4011">
<title id=" E09-1006.xml">supervised domain adaption for wsd </title>
<section> prior work.  </section>
<citcontext>
<prevsection>
<prevsent>iii (2007) shows that simple feature augmentation method forsvm is able to effectively use both labeled target and source data to provide the best domain adaptation results in number of nlp tasks.
</prevsent>
<prevsent>his method improves or equals over previously explored more sophisticated methods (daume?
</prevsent>
</prevsection>
<citsent citstr=" W04-3237 ">
iii and marcu, 2006; chelba and acero, 2004).<papid> W04-3237 </papid></citsent>
<aftsection>
<nextsent>the feature augmentation consists in making three version of the original features: general, source specific and target-specific versions.
</nextsent>
<nextsent>that way the augmented source contains the general and source-specific version and the augmented target data general and specific versions.
</nextsent>
<nextsent>the idea behind this is that target domain data has twice the influence as the source when making predictions about test target data.
</nextsent>
<nextsent>we reimplemented this method and show that our results are better.regarding wsd, some initial works made basic analysis of domain adaptation issues.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4013">
<title id=" E09-1006.xml">supervised domain adaption for wsd </title>
<section> prior work.  </section>
<citcontext>
<prevsection>
<prevsent>they found that the source corpus didnot help when tagging the target corpus, showing that tagged corpora from each domain would suffice, and concluding that hand tagging large general corpus would not guarantee robust broad coverage wsd.
</prevsent>
<prevsent>agirre and martnez (2000) used the dso corpus in the supervised scenario to show that training on subset of the source corpora that is topically related to the target corpus does allow for some domain adaptation.
</prevsent>
</prevsection>
<citsent citstr=" P07-1007 ">
more recently, chan and ng (2007) <papid> P07-1007 </papid>performed supervised domain adaptation on manually selected subset of 21 nouns from the dso corpus.</citsent>
<aftsection>
<nextsent>they used active learning, count-merging, and predominant sense estimation in order to save target annotation effort.
</nextsent>
<nextsent>they showed that adding just 30% of the target data to the source examples the same precision as the full combination of target and source data could be achieved.
</nextsent>
<nextsent>they also showed that using the source corpus allowed to significantly improve results when only 10% 30% of the target corpus was used for training.
</nextsent>
<nextsent>unfortunately, no data was given about the target corpus results, thus failing to show that domain adaptation succeeded.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4014">
<title id=" E09-1006.xml">supervised domain adaption for wsd </title>
<section> prior work.  </section>
<citcontext>
<prevsection>
<prevsent>they also showed that using the source corpus allowed to significantly improve results when only 10% 30% of the target corpus was used for training.
</prevsent>
<prevsent>unfortunately, no data was given about the target corpus results, thus failing to show that domain adaptation succeeded.
</prevsent>
</prevsection>
<citsent citstr=" D08-1105 ">
in followup work (zhong et al ., 2008), <papid> D08-1105 </papid>the feature augmentation approach was combined with active learning and tested on the ontonotes corpus, on large domain-adaptationexperiment.</citsent>
<aftsection>
<nextsent>they reduced significantly the effort of hand-tagging, but only obtained domain adaptation for smaller fractions of the source and target corpus.
</nextsent>
<nextsent>similarly to these works we show that we can save annotation effort on the target corpus, but, in contrast, we do get domain adaptation when using the full dataset.
</nextsent>
<nextsent>in way our approach is complementary, and we could also apply active learning to further reduce the number of target examples to be tagged.
</nextsent>
<nextsent>though not addressing domain adaptation, other works on wsd also used svd and are closely related to the present paper.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4015">
<title id=" E09-1006.xml">supervised domain adaption for wsd </title>
<section> prior work.  </section>
<citcontext>
<prevsection>
<prevsent>in way our approach is complementary, and we could also apply active learning to further reduce the number of target examples to be tagged.
</prevsent>
<prevsent>though not addressing domain adaptation, other works on wsd also used svd and are closely related to the present paper.
</prevsent>
</prevsection>
<citsent citstr=" W06-2911 ">
ando (2006) <papid> W06-2911 </papid>used alternative structured optimization.</citsent>
<aftsection>
<nextsent>she first trained one linear predictor for each target word, and then performed svd on 7 carefully selected sub matrices of the feature-to-predictor matrix of weights.
</nextsent>
<nextsent>the system attained small but consistent improvements (no significance data was given) on the senseval-3 lexical sample datasets using svd and unlabeled data.
</nextsent>
<nextsent>gliozzo et al  (2005) <papid> P05-1050 </papid>used svd to reduce the space of the term-to-document matrix, and then computed the similarity between train and test 43 instances using mapping to the reduced space (similar to our sma method in section 4.2).</nextsent>
<nextsent>they combined other knowledge sources into complex kernel using svm.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4017">
<title id=" E09-1006.xml">supervised domain adaption for wsd </title>
<section> prior work.  </section>
<citcontext>
<prevsection>
<prevsent>they report improved performance on number of languages in the senseval3 lexical sample dataset.
</prevsent>
<prevsent>our present paper differs from theirs in that we propose an additional method to use svd (the omt method), and that we focus on domain adaptation.
</prevsent>
</prevsection>
<citsent citstr=" W06-1615 ">
in the semi-supervised setting, blitzer et al  (2006) <papid> W06-1615 </papid>used structural correspondence learning and unlabeled data to adapt part-of-speech tagger.</citsent>
<aftsection>
<nextsent>they carefully select so-called pivot fea tures?
</nextsent>
<nextsent>to learn linear predictors, perform svd on the weights learned by the predictor, and thus learn correspondences among features in both source and target domains.
</nextsent>
<nextsent>our technique also uses svd, but we directly apply it to all features, and thus avoid the need to define pivot features.
</nextsent>
<nextsent>in preliminary work we unsuccessfully tried to carry along the idea of pivot features to wsd.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4020">
<title id=" E09-1006.xml">supervised domain adaption for wsd </title>
<section> original and svd features.  </section>
<citcontext>
<prevsection>
<prevsent>local collocations comprise the bigrams and trigrams formed around the target word (using either lem mas, word-forms, or pos tags) , those formed with the previous/posterior lemma/word-form in the sentence, and the content words in 4-wordwindow around the target.
</prevsent>
<prevsent>syntactic dependencies use the object, subject, noun-modifier, preposition, and sibling lemmas, when available.
</prevsent>
</prevsection>
<citsent citstr=" N01-1011 ">
finally, bag-of-words features are the lemmas of the content words in the whole context, plus the salient bigrams in the context (pedersen, 2001).<papid> N01-1011 </papid></citsent>
<aftsection>
<nextsent>we refer to these features as original features.
</nextsent>
<nextsent>4.2 svd features.
</nextsent>
<nextsent>apart from the original space of features, we have used the so called svd features, obtained fromthe projection of the feature vectors into there duced space (deerwester et al , 1990).
</nextsent>
<nextsent>basically, 44 we set term-by-document or feature-by-example matrix from the corpus (see section below formore details).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4022">
<title id=" E09-1006.xml">supervised domain adaption for wsd </title>
<section> original and svd features.  </section>
<citcontext>
<prevsection>
<prevsent>the dimensionality reduction is performed once,over the whole unlabeled corpus, and it is then applied to the labeled data of each word.
</prevsent>
<prevsent>there duced space is constructed only with terms, which correspond to bag-of-words features, and thus discards the rest of the features.
</prevsent>
</prevsection>
<citsent citstr=" W07-2016 ">
given that the wsd literature shows that all features are necessary for optimal performance (pradhan et al , 2007), <papid> W07-2016 </papid>we propose the following alternative to construct the matrix.</citsent>
<aftsection>
<nextsent>one matrix per target word (svd-omt).
</nextsent>
<nextsent>foreach word: (i) construct corpus with its occurrences in the labeled and, if desired, unlabeled corpora, (ii) extract all features, (iii) build the feature by-example matrix, (iv) decompose it with svd, and (v) map all the labeled training and test data for the word.
</nextsent>
<nextsent>note that this variant performs one svd process for each target word separately, hence its name.
</nextsent>
<nextsent>when building the svd-omt matrices we can use only the training data (train) or both the train and unlabeled data (+unlab).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4023">
<title id=" E09-1077.xml">semi supervised polarity lexicon induction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the positive review is peppered with words such as enjoy able, likeable, decent, breathtakingly and the negative work done as summer internat google inc. 1source: live free or die hard, rottentomatoes.com figure 1: movie reviews with positive (left) and negative (right) sentiment.
</prevsent>
<prevsent>comment uses words like ear-shattering, humorless, unbearable.
</prevsent>
</prevsection>
<citsent citstr=" E06-1025 ">
these terms and prior knowledge oftheir polarity could be used as features in supervised classification framework to determine the sentiment of the opinionated text (e.g., (esuli and sebastiani, 2006)).<papid> E06-1025 </papid></citsent>
<aftsection>
<nextsent>thus lexicons indicating polarity of such words are indispensable resources not only in automatic sentiment analysis but also in other natural language understanding tasks like textual entailment.
</nextsent>
<nextsent>this motivation was seen in the general enquirer effort by stone et al (1966) and several others who manually construct such lexicons for the english language.2 while it is possible to manually build these resources for alan guage, the ensuing effort is onerous.
</nextsent>
<nextsent>this motivates the need for automatic language-agnosticmethods for building sentiment lexicons.
</nextsent>
<nextsent>the importance of this problem has warranted several efforts in the past, some of which will be reviewed here.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4024">
<title id=" E09-1077.xml">semi supervised polarity lexicon induction </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the literature on sentiment polarity lexicon induction can be broadly classified into two categories,those based on corpora and the ones using wordnet.
</prevsent>
<prevsent>2.1 corpora based approaches.
</prevsent>
</prevsection>
<citsent citstr=" P97-1023 ">
one of the earliest work on learning polarity of terms was by hatzivassiloglou and mckeown(1997) <papid> P97-1023 </papid>who deduce polarity by exploiting constraints on conjoined adjectives in the wall street journal corpus.</citsent>
<aftsection>
<nextsent>for example, the conjunction and?
</nextsent>
<nextsent>links adjectives of the same polarity whilebut?
</nextsent>
<nextsent>links adjectives of opposite polarity.
</nextsent>
<nextsent>however the applicability of this method for other important classes of sentiment terms like nouns and verbs is yet to be demonstrated.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4025">
<title id=" E09-1077.xml">semi supervised polarity lexicon induction </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>links adjectives of opposite polarity.
</prevsent>
<prevsent>however the applicability of this method for other important classes of sentiment terms like nouns and verbs is yet to be demonstrated.
</prevsent>
</prevsection>
<citsent citstr=" P98-2127 ">
further they assume linguistic features specific to english.wiebe (2000) uses lin (1998<papid> P98-2127 </papid>a) style distributionally similar adjectives in cluster-and-labelprocess to generate sentiment lexicon of adjec tives.in different work, riloff et al (2003) <papid> W03-0404 </papid>use manually derived pattern templates to extract subjective nouns by bootstrapping.</citsent>
<aftsection>
<nextsent>3http://www.openoffice.org another corpora based method due to turney and littman (2003) tries to measure the semantic orientation o(t) for term by o(t) = ? tis+ pmi(t, ti) ? ?
</nextsent>
<nextsent>tjs? pmi(t, tj) where s+ and s?
</nextsent>
<nextsent>are minimal sets of polar terms that contain prototypical positive and negative terms respectively, and pmi(t, ti) is the pointwise mutual information (lin, 1998<papid> P98-2127 </papid>b) between the terms and ti.</nextsent>
<nextsent>while this method is general enough to be applied to several languages our aimwas to develop methods that exploit more structured sources like wordnet to leverage benefits from the rich network structure.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4026">
<title id=" E09-1077.xml">semi supervised polarity lexicon induction </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>links adjectives of opposite polarity.
</prevsent>
<prevsent>however the applicability of this method for other important classes of sentiment terms like nouns and verbs is yet to be demonstrated.
</prevsent>
</prevsection>
<citsent citstr=" W03-0404 ">
further they assume linguistic features specific to english.wiebe (2000) uses lin (1998<papid> P98-2127 </papid>a) style distributionally similar adjectives in cluster-and-labelprocess to generate sentiment lexicon of adjec tives.in different work, riloff et al (2003) <papid> W03-0404 </papid>use manually derived pattern templates to extract subjective nouns by bootstrapping.</citsent>
<aftsection>
<nextsent>3http://www.openoffice.org another corpora based method due to turney and littman (2003) tries to measure the semantic orientation o(t) for term by o(t) = ? tis+ pmi(t, ti) ? ?
</nextsent>
<nextsent>tjs? pmi(t, tj) where s+ and s?
</nextsent>
<nextsent>are minimal sets of polar terms that contain prototypical positive and negative terms respectively, and pmi(t, ti) is the pointwise mutual information (lin, 1998<papid> P98-2127 </papid>b) between the terms and ti.</nextsent>
<nextsent>while this method is general enough to be applied to several languages our aimwas to develop methods that exploit more structured sources like wordnet to leverage benefits from the rich network structure.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4028">
<title id=" E09-1077.xml">semi supervised polarity lexicon induction </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>are minimal sets of polar terms that contain prototypical positive and negative terms respectively, and pmi(t, ti) is the pointwise mutual information (lin, 1998<papid> P98-2127 </papid>b) between the terms and ti.</prevsent>
<prevsent>while this method is general enough to be applied to several languages our aimwas to develop methods that exploit more structured sources like wordnet to leverage benefits from the rich network structure.</prevsent>
</prevsection>
<citsent citstr=" D07-1115 ">
kaji and kitsuregawa (2007) <papid> D07-1115 </papid>outline method of building sentiment lexicons for japanese using structural cues from html documents.</citsent>
<aftsection>
<nextsent>apart from being very specific to japanese, excessive dependence on html structure makes their method brittle.
</nextsent>
<nextsent>2.2 wordnet based approaches.
</nextsent>
<nextsent>these approaches use lexical relations defined in wordnet to derive sentiment lexicons.
</nextsent>
<nextsent>a simple but high-precision method proposed by kimand hovy (2006) <papid> N06-1026 </papid>is to add all synonyms of polar word with the same polarity and its antonyms with reverse polarity.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4029">
<title id=" E09-1077.xml">semi supervised polarity lexicon induction </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>2.2 wordnet based approaches.
</prevsent>
<prevsent>these approaches use lexical relations defined in wordnet to derive sentiment lexicons.
</prevsent>
</prevsection>
<citsent citstr=" N06-1026 ">
a simple but high-precision method proposed by kimand hovy (2006) <papid> N06-1026 </papid>is to add all synonyms of polar word with the same polarity and its antonyms with reverse polarity.</citsent>
<aftsection>
<nextsent>as demonstrated later, the method suffers from low recall and is unsuitable in situations when the seed polar words are too few ? not uncommon in low resource languages.
</nextsent>
<nextsent>in line with turneys work, kamps et. al.
</nextsent>
<nextsent>(2004)try to determine sentiments of adjectives in wordnet by measuring relative distance of the term from exemplars, such as good?
</nextsent>
<nextsent>and bad?.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4030">
<title id=" E09-1077.xml">semi supervised polarity lexicon induction </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>and bad?.
</prevsent>
<prevsent>the polarity orientation of term is measured as follows o(t) = d(t, good) ? d(t, bad)d(good, bad)where d(.)
</prevsent>
</prevsection>
<citsent citstr=" N04-3012 ">
is wordnet based relatedness measure (pedersen et al, 2004).<papid> N04-3012 </papid></citsent>
<aftsection>
<nextsent>again they report results for adjectives alone.
</nextsent>
<nextsent>another relevant example is the recent work by mihalcea et. al.
</nextsent>
<nextsent>(2007) on multilingual sentiment analysis using cross-lingual projections.
</nextsent>
<nextsent>this is achieved by using bridge resources like dictionaries and parallel corpora to build sentence subjectivity classifiers for the target language (roma nian).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4031">
<title id=" E09-1077.xml">semi supervised polarity lexicon induction </title>
<section> graph based semi-supervised learning.  </section>
<citcontext>
<prevsection>
<prevsent>a mincut of weighted graph g(v,e) is partitioning the vertices into v1 and v2 such that sum of the edge weights of all edges between v1 and v2 is minimal (figure 2).
</prevsent>
<prevsent>mincuts for semi-supervised learning proposed by blum and chawla (2001) tries to classify data points by partitioning the similarity graph such that it minimizes the number of similar points being labeled differently.
</prevsent>
</prevsection>
<citsent citstr=" P04-1035 ">
mincuts have been used 4as of this writing, wordnet is available for more than 40 world languages (http://www.globalwordnet.org) figure 2: semi-supervised classification using mincutsin semi-supervised learning for various tasks, including document level sentiment analysis (pang and lee, 2004).<papid> P04-1035 </papid></citsent>
<aftsection>
<nextsent>we explore the use of mincuts for the task of sentiment lexicon learning.
</nextsent>
<nextsent>3.2 randomized mincuts.
</nextsent>
<nextsent>an improvement to the basic mincut algorithm was proposed by blum et. al.
</nextsent>
<nextsent>(2004).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4033">
<title id=" E09-1023.xml">correcting dependency annotation errors </title>
<section> motivation </section>
<citcontext>
<prevsection>
<prevsent>for the task, we define feature-based model that explicitly accounts for non-relations between words, and then use ambiguities from one model to constrain second, more relaxed model.
</prevsent>
<prevsent>in this way, we are successfully able to correct many errors, in way which is potentially applicable to dependency parsing more generally.
</prevsent>
</prevsection>
<citsent citstr=" P07-1086 ">
annotation error detection has been explored forpart-of-speech (pos), syntactic constituency, semantic role, and syntactic dependency annotation (see boyd et al, 2008, and references therein).such work is extremely useful, given the harmfulness of annotation errors for training, including the learning of noise (e.g., hogan, 2007; <papid> P07-1086 </papid>habash et al, 2007), <papid> D07-1116 </papid>and for evaluation (e.g., padro and marquez, 1998).</citsent>
<aftsection>
<nextsent>but little work has been done to show the full impact of errors, or what types of cases are the most damaging, important since noise can sometimes be overcome (cf.
</nextsent>
<nextsent>osborne, 2002).
</nextsent>
<nextsent>likewise, it is not clear how to learn from consistently mis annotated data; studies often only note the presence of errors or eliminate them from evaluation (e.g., hogan, 2007), <papid> P07-1086 </papid>and previous attempt at correction was limited to pos annotation (dickinson, 2006).<papid> E06-1034 </papid></nextsent>
<nextsent>by moving from annotation error detection to error correction, we can more fully elucidate ways in which noise can be over come and ways it cannot.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4034">
<title id=" E09-1023.xml">correcting dependency annotation errors </title>
<section> motivation </section>
<citcontext>
<prevsection>
<prevsent>for the task, we define feature-based model that explicitly accounts for non-relations between words, and then use ambiguities from one model to constrain second, more relaxed model.
</prevsent>
<prevsent>in this way, we are successfully able to correct many errors, in way which is potentially applicable to dependency parsing more generally.
</prevsent>
</prevsection>
<citsent citstr=" D07-1116 ">
annotation error detection has been explored forpart-of-speech (pos), syntactic constituency, semantic role, and syntactic dependency annotation (see boyd et al, 2008, and references therein).such work is extremely useful, given the harmfulness of annotation errors for training, including the learning of noise (e.g., hogan, 2007; <papid> P07-1086 </papid>habash et al, 2007), <papid> D07-1116 </papid>and for evaluation (e.g., padro and marquez, 1998).</citsent>
<aftsection>
<nextsent>but little work has been done to show the full impact of errors, or what types of cases are the most damaging, important since noise can sometimes be overcome (cf.
</nextsent>
<nextsent>osborne, 2002).
</nextsent>
<nextsent>likewise, it is not clear how to learn from consistently mis annotated data; studies often only note the presence of errors or eliminate them from evaluation (e.g., hogan, 2007), <papid> P07-1086 </papid>and previous attempt at correction was limited to pos annotation (dickinson, 2006).<papid> E06-1034 </papid></nextsent>
<nextsent>by moving from annotation error detection to error correction, we can more fully elucidate ways in which noise can be over come and ways it cannot.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4036">
<title id=" E09-1023.xml">correcting dependency annotation errors </title>
<section> motivation </section>
<citcontext>
<prevsection>
<prevsent>but little work has been done to show the full impact of errors, or what types of cases are the most damaging, important since noise can sometimes be overcome (cf.
</prevsent>
<prevsent>osborne, 2002).
</prevsent>
</prevsection>
<citsent citstr=" E06-1034 ">
likewise, it is not clear how to learn from consistently mis annotated data; studies often only note the presence of errors or eliminate them from evaluation (e.g., hogan, 2007), <papid> P07-1086 </papid>and previous attempt at correction was limited to pos annotation (dickinson, 2006).<papid> E06-1034 </papid></citsent>
<aftsection>
<nextsent>by moving from annotation error detection to error correction, we can more fully elucidate ways in which noise can be over come and ways it cannot.
</nextsent>
<nextsent>we thus explore annotation error correction and its feasibility for dependency annotation, form of annotation that provides argument relations among words and is useful for training and testing dependency parsers (e.g., nivre, 2006; mcdonald and pereira, 2006).<papid> E06-1011 </papid></nextsent>
<nextsent>a recent innovation in dependency parsing, relevant here, is to use the predictions made by one model to refine another (nivre and mcdonald, 2008; <papid> P08-1108 </papid>torres martins et al, 2008).this general notion can be employed here, as different models of the data have different predictions about whch parts are erroneous and can highlight the contributions of different features.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4038">
<title id=" E09-1023.xml">correcting dependency annotation errors </title>
<section> motivation </section>
<citcontext>
<prevsection>
<prevsent>likewise, it is not clear how to learn from consistently mis annotated data; studies often only note the presence of errors or eliminate them from evaluation (e.g., hogan, 2007), <papid> P07-1086 </papid>and previous attempt at correction was limited to pos annotation (dickinson, 2006).<papid> E06-1034 </papid></prevsent>
<prevsent>by moving from annotation error detection to error correction, we can more fully elucidate ways in which noise can be over come and ways it cannot.</prevsent>
</prevsection>
<citsent citstr=" E06-1011 ">
we thus explore annotation error correction and its feasibility for dependency annotation, form of annotation that provides argument relations among words and is useful for training and testing dependency parsers (e.g., nivre, 2006; mcdonald and pereira, 2006).<papid> E06-1011 </papid></citsent>
<aftsection>
<nextsent>a recent innovation in dependency parsing, relevant here, is to use the predictions made by one model to refine another (nivre and mcdonald, 2008; <papid> P08-1108 </papid>torres martins et al, 2008).this general notion can be employed here, as different models of the data have different predictions about whch parts are erroneous and can highlight the contributions of different features.</nextsent>
<nextsent>using differences that complement one another, we can begin to sort accurate from inaccurate patterns, by integrating models in such way as to learn the true patterns and not the errors.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4039">
<title id=" E09-1023.xml">correcting dependency annotation errors </title>
<section> motivation </section>
<citcontext>
<prevsection>
<prevsent>by moving from annotation error detection to error correction, we can more fully elucidate ways in which noise can be over come and ways it cannot.
</prevsent>
<prevsent>we thus explore annotation error correction and its feasibility for dependency annotation, form of annotation that provides argument relations among words and is useful for training and testing dependency parsers (e.g., nivre, 2006; mcdonald and pereira, 2006).<papid> E06-1011 </papid></prevsent>
</prevsection>
<citsent citstr=" P08-1108 ">
a recent innovation in dependency parsing, relevant here, is to use the predictions made by one model to refine another (nivre and mcdonald, 2008; <papid> P08-1108 </papid>torres martins et al, 2008).this general notion can be employed here, as different models of the data have different predictions about whch parts are erroneous and can highlight the contributions of different features.</citsent>
<aftsection>
<nextsent>using differences that complement one another, we can begin to sort accurate from inaccurate patterns, by integrating models in such way as to learn the true patterns and not the errors.
</nextsent>
<nextsent>although we focuson dependency annotation, the methods are potentially applicable for different types of annotation,given that they are based on the similar data representations (see sections 2.1 and 3.2).
</nextsent>
<nextsent>in order to examine the effects of errors and to refine one model with anothers information, we need to isolate the problematic cases.
</nextsent>
<nextsent>the data representation must therefore be such that it clearly allows for the specific identification of errors between words.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4040">
<title id=" E09-1023.xml">correcting dependency annotation errors </title>
<section> motivation </section>
<citcontext>
<prevsection>
<prevsent>thus, we explore relatively simple models of the data, emphasizing small substructures (see section 3.2).
</prevsent>
<prevsent>this simple modeling is not always rich enough for full dependency parsing, but different models can reveal conflicting information and are generally useful as part ofa larger system.
</prevsent>
</prevsection>
<citsent citstr=" W06-2932 ">
graph-based models of dependency parsing (e.g., mcdonald et al, 2006), <papid> W06-2932 </papid>for example, relyon breaking parsing down into decisions about smaller substructures, and focusing onpairs of words has been used for domain adaptation (chen et al, 2008) <papid> C08-1015 </papid>and in memory-based parsing (canisius et al, 2006).<papid> W06-2924 </papid></citsent>
<aftsection>
<nextsent>exploring annotation error correction in this way can provide insights into more general uses of the annotation, just as previous work on correction for pos annotation (dickinson, 2006) <papid> E06-1034 </papid>led to way to improve pos 193 tagging (dickinson, 2007).after describing previous work on error detection and correction in section 2, we outline in section 3 how we model the data, focusing on individual relations between pairs of words.</nextsent>
<nextsent>in section 4, we illustrate the difficulties of error correction and show how simple combinations of local features perform poorly.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4041">
<title id=" E09-1023.xml">correcting dependency annotation errors </title>
<section> motivation </section>
<citcontext>
<prevsection>
<prevsent>thus, we explore relatively simple models of the data, emphasizing small substructures (see section 3.2).
</prevsent>
<prevsent>this simple modeling is not always rich enough for full dependency parsing, but different models can reveal conflicting information and are generally useful as part ofa larger system.
</prevsent>
</prevsection>
<citsent citstr=" C08-1015 ">
graph-based models of dependency parsing (e.g., mcdonald et al, 2006), <papid> W06-2932 </papid>for example, relyon breaking parsing down into decisions about smaller substructures, and focusing onpairs of words has been used for domain adaptation (chen et al, 2008) <papid> C08-1015 </papid>and in memory-based parsing (canisius et al, 2006).<papid> W06-2924 </papid></citsent>
<aftsection>
<nextsent>exploring annotation error correction in this way can provide insights into more general uses of the annotation, just as previous work on correction for pos annotation (dickinson, 2006) <papid> E06-1034 </papid>led to way to improve pos 193 tagging (dickinson, 2007).after describing previous work on error detection and correction in section 2, we outline in section 3 how we model the data, focusing on individual relations between pairs of words.</nextsent>
<nextsent>in section 4, we illustrate the difficulties of error correction and show how simple combinations of local features perform poorly.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4042">
<title id=" E09-1023.xml">correcting dependency annotation errors </title>
<section> motivation </section>
<citcontext>
<prevsection>
<prevsent>thus, we explore relatively simple models of the data, emphasizing small substructures (see section 3.2).
</prevsent>
<prevsent>this simple modeling is not always rich enough for full dependency parsing, but different models can reveal conflicting information and are generally useful as part ofa larger system.
</prevsent>
</prevsection>
<citsent citstr=" W06-2924 ">
graph-based models of dependency parsing (e.g., mcdonald et al, 2006), <papid> W06-2932 </papid>for example, relyon breaking parsing down into decisions about smaller substructures, and focusing onpairs of words has been used for domain adaptation (chen et al, 2008) <papid> C08-1015 </papid>and in memory-based parsing (canisius et al, 2006).<papid> W06-2924 </papid></citsent>
<aftsection>
<nextsent>exploring annotation error correction in this way can provide insights into more general uses of the annotation, just as previous work on correction for pos annotation (dickinson, 2006) <papid> E06-1034 </papid>led to way to improve pos 193 tagging (dickinson, 2007).after describing previous work on error detection and correction in section 2, we outline in section 3 how we model the data, focusing on individual relations between pairs of words.</nextsent>
<nextsent>in section 4, we illustrate the difficulties of error correction and show how simple combinations of local features perform poorly.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4045">
<title id=" E09-1023.xml">correcting dependency annotation errors </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>while the original proposal expanded the context as far as possible given the repeated n-gram, using only the immediately surrounding words as context is sufficient for detecting errors with high precision (boyd et al, 2008).
</prevsent>
<prevsent>this shortest?
</prevsent>
</prevsection>
<citsent citstr=" P02-1017 ">
context heuristic receives some support from research on first language acquisition (mintz, 2006) and unsupervised grammar induction (klein and manning, 2002).<papid> P02-1017 </papid>the approach can detect both bracketing and labeling errors in constituency annotation, and we already saw labeling error for next tuesday.</citsent>
<aftsection>
<nextsent>asan example of bracketing error, the variation nucleus last month occurs within the np its biggest jolt last month once with the label np and once asa non-constituent, which in the algorithm is handled through special label nil.
</nextsent>
<nextsent>the method for detecting annotation errors canbe extended to discontinuous constituency annotation (dickinson and meurers, 2005), <papid> P05-1040 </papid>making it applicable to dependency annotation, where wordsin relation can be arbitrarily far apart.</nextsent>
<nextsent>specifically, boyd et al (2008) adapt the method by treating dependency pairs as variation nuclei, and they include nil elements for pairs of words not annotated as relation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4046">
<title id=" E09-1023.xml">correcting dependency annotation errors </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>context heuristic receives some support from research on first language acquisition (mintz, 2006) and unsupervised grammar induction (klein and manning, 2002).<papid> P02-1017 </papid>the approach can detect both bracketing and labeling errors in constituency annotation, and we already saw labeling error for next tuesday.</prevsent>
<prevsent>asan example of bracketing error, the variation nucleus last month occurs within the np its biggest jolt last month once with the label np and once asa non-constituent, which in the algorithm is handled through special label nil.</prevsent>
</prevsection>
<citsent citstr=" P05-1040 ">
the method for detecting annotation errors canbe extended to discontinuous constituency annotation (dickinson and meurers, 2005), <papid> P05-1040 </papid>making it applicable to dependency annotation, where wordsin relation can be arbitrarily far apart.</citsent>
<aftsection>
<nextsent>specifically, boyd et al (2008) adapt the method by treating dependency pairs as variation nuclei, and they include nil elements for pairs of words not annotated as relation.
</nextsent>
<nextsent>the method is successful at detecting annotation errors in corpora for three different languages, with precis ions of 93% for swedish, 60% for czech, and 48% for german.1 2.2 error correction.
</nextsent>
<nextsent>correcting pos annotation errors can be done by applying pos tagger and altering the input pos tags (dickinson, 2006).<papid> E06-1034 </papid></nextsent>
<nextsent>namely, ambiguity class information (e.g., in/rb/rp) is added to each corpus position for training, creating complex ambiguity tags, such as  in/rb/rp,in .</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4051">
<title id=" E09-1023.xml">correcting dependency annotation errors </title>
<section> modeling the data.  </section>
<citcontext>
<prevsection>
<prevsent>this is slightly different goal from that of general dependency parsing methods, which often integrate variety of features in making decisions about dependency relations (cf., e.g., nivre, 2006; mcdonald and pereira, 2006).<papid> E06-1011 </papid></prevsent>
<prevsent>instead of maximizing afeature model to improve parsing, we isolate individual pieces of information (e.g., context pos tags), thereby being able to pinpoint, for example,when non-local information is needed for particular types of relations and pointing to cases where pieces of information conflict (cf.</prevsent>
</prevsection>
<citsent citstr=" D07-1013 ">
also mcdonald and nivre, 2007).<papid> D07-1013 </papid></citsent>
<aftsection>
<nextsent>to support this isolation of information, we use dependency pairs as the basic unit of analysis and assign dependency label to each word pair.
</nextsent>
<nextsent>following boyd et al (2008), we add or to the label to indicate which word is the head, the left(l) or the right (r).
</nextsent>
<nextsent>this is tantamount to handling pairs of words as single entries in lexicon?
</nextsent>
<nextsent>and provides natural way to talk of ambiguities.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4057">
<title id=" E09-1023.xml">correcting dependency annotation errors </title>
<section> modeling the data.  </section>
<citcontext>
<prevsection>
<prevsent>mbl stores all corpus instances as vectors of features, and given new instance, the task of the classifier is to find the most similar cases in memory to deduce the best class.
</prevsent>
<prevsent>given the previous discussion of the goals of correcting errors, what seems to be needed is way to find patterns which do not fully generalize because ofnoise appearing in very similar cases in the corpus.
</prevsent>
</prevsection>
<citsent citstr=" W97-1016 ">
as zavrel et al (1997, <papid> W97-1016 </papid>p. 137) state about the advantages of mbl:because language-processing tasks typically can only be described as complex interaction of regularities, sub regularities and (families of) exceptions, storing all empirical data as potentially useful in ana logical extrapolation works better than extracting the main regularities and forgetting the individual examples (daelemans, 1996).</citsent>
<aftsection>
<nextsent>by storing all corpus examples, as mbl does,both correct and incorrect data is maintained, allowing us to pinpoint the effect of errors on training.
</nextsent>
<nextsent>for our experiments, we use timbl, version6.1 (daelemans et al, 2007), with the default settings.
</nextsent>
<nextsent>we use the default overlap metric, as this maintains direct connection to majority-based correction.
</nextsent>
<nextsent>we could run timbl with different values of k, as this should lead to better feature integration.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4064">
<title id=" E12-1079.xml">behind the article recognizing dialog acts in wikipedia talk pages </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>both schemata have often been adapted for special purpose annotation tasks.
</prevsent>
<prevsent>with the rise of the social web, the amount of research analyzing user generated discourse substantially increased.
</prevsent>
</prevsection>
<citsent citstr=" W10-2923 ">
in addition to analyzing web forums (kim et al  2010<papid> W10-2923 </papid>a), chats (carpenter and fujioka, 2011) and emails (cohen et al  2004), <papid> W04-3240 </papid>wikipedia talk pages have recently moved into the center of attention of the research community.</citsent>
<aftsection>
<nextsent>viegas et al (2007) manually annotate 25 wikipedia article discussion pages with set of 11 labels in order to analyze how talk pages areused for planning the work on articles and resolving disputes among the editors.
</nextsent>
<nextsent>schneider et al  (2011) extend this schema and manually annotate 100 talk pages with 15 labels.
</nextsent>
<nextsent>they confirm the.
</nextsent>
<nextsent>findings of viegas et al that coordination requests occur most frequently in the discussions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4066">
<title id=" E12-1079.xml">behind the article recognizing dialog acts in wikipedia talk pages </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>both schemata have often been adapted for special purpose annotation tasks.
</prevsent>
<prevsent>with the rise of the social web, the amount of research analyzing user generated discourse substantially increased.
</prevsent>
</prevsection>
<citsent citstr=" W04-3240 ">
in addition to analyzing web forums (kim et al  2010<papid> W10-2923 </papid>a), chats (carpenter and fujioka, 2011) and emails (cohen et al  2004), <papid> W04-3240 </papid>wikipedia talk pages have recently moved into the center of attention of the research community.</citsent>
<aftsection>
<nextsent>viegas et al (2007) manually annotate 25 wikipedia article discussion pages with set of 11 labels in order to analyze how talk pages areused for planning the work on articles and resolving disputes among the editors.
</nextsent>
<nextsent>schneider et al  (2011) extend this schema and manually annotate 100 talk pages with 15 labels.
</nextsent>
<nextsent>they confirm the.
</nextsent>
<nextsent>findings of viegas et al that coordination requests occur most frequently in the discussions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4067">
<title id=" E12-1079.xml">behind the article recognizing dialog acts in wikipedia talk pages </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>they confirm the.
</prevsent>
<prevsent>findings of viegas et al that coordination requests occur most frequently in the discussions.
</prevsent>
</prevsection>
<citsent citstr=" W11-0707 ">
bender et al (2011) <papid> W11-0707 </papid>describe corpus of 47talk pages which have been annotated for authority claims and alignment moves.</citsent>
<aftsection>
<nextsent>with this corpus, the authors analyze how the participants in wikipedia discussions establish their credibility and how they express agreement and disagreement towards other participants or topics.
</nextsent>
<nextsent>from different perspective, stvilia et al  (2008) analyze 60 discussion pages in regard tohow information quality (iq) in wikipedia articles is assessed on the talk pages and which types of iq problems are identified by the community.
</nextsent>
<nextsent>they describe wikipedia iq assessment mod eland map it to established frameworks.
</nextsent>
<nextsent>further more, they provide list of iq problems along with related causal factors and necessary actions which has also inspired the design of our annotation schema.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4069">
<title id=" E12-1079.xml">behind the article recognizing dialog acts in wikipedia talk pages </title>
<section> corpus creation and analysis.  </section>
<citcontext>
<prevsection>
<prevsent>laniado et al (2011) tackle the thread extraction by using text indentation and inserted user signatures as clues.
</prevsent>
<prevsent>we found these 780attributes to be insufficient for reliable reconstruction of the thread structure.6 our preprocessing approach consists of three steps: data retrieval, topic segmentation and turn segmentation.
</prevsent>
</prevsection>
<citsent citstr=" L08-1139 ">
for retrieving the discussion pages, we use the java wikipedia library (jwpl) (zeschet al  2008), <papid> L08-1139 </papid>which offers efficient, database driven access to the contents of wikipedia.</citsent>
<aftsection>
<nextsent>we segment the individual talk pages into discussions topics using the mediawiki parser that comes with jwpl.
</nextsent>
<nextsent>in our corpus, the parser managed to identify all topic boundaries without any errors.
</nextsent>
<nextsent>the most complex preprocessing step is the turn segmentation.
</nextsent>
<nextsent>first, we use the revision history of the talk page to identify the author and the creation timeof each paragraph.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4070">
<title id=" E12-1079.xml">behind the article recognizing dialog acts in wikipedia talk pages </title>
<section> corpus creation and analysis.  </section>
<citcontext>
<prevsection>
<prevsent>the most complex preprocessing step is the turn segmentation.
</prevsent>
<prevsent>first, we use the revision history of the talk page to identify the author and the creation timeof each paragraph.
</prevsent>
</prevsection>
<citsent citstr=" P11-4017 ">
we use the wikipedia revision toolkit (ferschke et al  2011) <papid> P11-4017 </papid>to examine the changes between adjacent revisions of the talk page in order to identify the exact time piece oftext was added as well as the author of the con tribution.</citsent>
<aftsection>
<nextsent>we have to filter out malicious edits from the history, as they would negatively affect the segmentation process.
</nextsent>
<nextsent>we therefore disregard all edits that are reverted in later later revisions.in contrast to vandalism on article pages, this approach has proven to be sufficient to detect vandalism in the talk page history.
</nextsent>
<nextsent>within each discussion topic, we aggregate all adjacent paragraphs with the same author and the same time stamp to one turn.
</nextsent>
<nextsent>in order to account for turns that were written in multiple revisions, we regard all time stamps within window of 10 minutes7 as belonging to the same turn, unless the page was edited by another user in the meantime.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4071">
<title id=" E12-1079.xml">behind the article recognizing dialog acts in wikipedia talk pages </title>
<section> corpus creation and analysis.  </section>
<citcontext>
<prevsection>
<prevsent>4.1 inter-annotator agreement.
</prevsent>
<prevsent>to evaluate the reliability of our dataset, we perform detailed inter-rater agreement study.
</prevsent>
</prevsection>
<citsent citstr=" J96-2004 ">
for measuring the agreement of the individual labels,we report the observed agreement, kappa statistics (carletta, 1996), <papid> J96-2004 </papid>and f1-scores.</citsent>
<aftsection>
<nextsent>the latter are computed by treating one annotator as the gold standard and the other one as predictions (hripc sak and rothschild, 2005).
</nextsent>
<nextsent>the scores can be seen in table 2.the average observed agreement across all labels is po = .94.
</nextsent>
<nextsent>the individual kappa scores largely fall into the range that landis and koch (1977) regard as substantial agreement, while three labels are above the more strict .8 threshold for reliable annotations (artstein and poesio, 2008).<papid> J08-4004 </papid></nextsent>
<nextsent>furthermore, we obtain an overall pooled kappa (de vries et al  2008) of pool = .67, 8http://www.mmax2.net 9http://www.ukp.tu-darmstadt.de/data/ wikidiscourse 10http://uima.apache.org 781 annotator 1 annotator 2 inter-annotator agreement gold standard label percent percent na1a2 po ? f1 percent article criticism cm 183 13.4% 105 7.7% 193 .93 .63 .66 116 8.5% cw 106 7.8% 57 4.2% 120 .95 .52 .55 70 5.1% cu 69 5.0% 35 2.6% 83 .95 .38 .40 42 3.1% cs 164 12.0% 101 7.4% 174 .94 .66 .69 136 9.9% cl 195 14.3% 199 14.6% 244 .93 .73 .77 219 16.0% cobj 27 2.0% 23 1.7% 29 .99 .84 .84 27 2.0% co 20 1.5% 59 4.3% 71 .95 .18 .20 48 3.5% explicit per formative psr 458 33.5% 351 25.7% 503 .86 .66 .76 406 29.7% pref 43 3.1% 31 2.3% 51 .98 .61 .62 45 3.3% pfc 73 5.3% 65 4.8% 86 .98 .76 .77 77 5.6% ppc 357 26.1% 340 24.9% 371 .97 .92 .94 358 26.2% information content ip 1084 79.3% 1027 75.1% 1135 .89 .69 .93 1070 78.3% is 228 16.7% 208 15.2% 256 .95 .80 .83 220 16.1% ic 187 13.7% 109 8.0% 221 .89 .46 .51 130 9.5% interpersonal att+ 71 5.2% 140 10.2% 151 .94 .55 .58 144 10.5% attp 71 5.2% 30 2.2% 79 .96 .42 .44 33 2.4% att- 67 4.9% 74 5.4% 100 .96 .56 .58 87 6.4% table 2: label frequencies and inter-annotator agreement.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4072">
<title id=" E12-1079.xml">behind the article recognizing dialog acts in wikipedia talk pages </title>
<section> corpus creation and analysis.  </section>
<citcontext>
<prevsection>
<prevsent>the latter are computed by treating one annotator as the gold standard and the other one as predictions (hripc sak and rothschild, 2005).
</prevsent>
<prevsent>the scores can be seen in table 2.the average observed agreement across all labels is po = .94.
</prevsent>
</prevsection>
<citsent citstr=" J08-4004 ">
the individual kappa scores largely fall into the range that landis and koch (1977) regard as substantial agreement, while three labels are above the more strict .8 threshold for reliable annotations (artstein and poesio, 2008).<papid> J08-4004 </papid></citsent>
<aftsection>
<nextsent>furthermore, we obtain an overall pooled kappa (de vries et al  2008) of pool = .67, 8http://www.mmax2.net 9http://www.ukp.tu-darmstadt.de/data/ wikidiscourse 10http://uima.apache.org 781 annotator 1 annotator 2 inter-annotator agreement gold standard label percent percent na1a2 po ? f1 percent article criticism cm 183 13.4% 105 7.7% 193 .93 .63 .66 116 8.5% cw 106 7.8% 57 4.2% 120 .95 .52 .55 70 5.1% cu 69 5.0% 35 2.6% 83 .95 .38 .40 42 3.1% cs 164 12.0% 101 7.4% 174 .94 .66 .69 136 9.9% cl 195 14.3% 199 14.6% 244 .93 .73 .77 219 16.0% cobj 27 2.0% 23 1.7% 29 .99 .84 .84 27 2.0% co 20 1.5% 59 4.3% 71 .95 .18 .20 48 3.5% explicit per formative psr 458 33.5% 351 25.7% 503 .86 .66 .76 406 29.7% pref 43 3.1% 31 2.3% 51 .98 .61 .62 45 3.3% pfc 73 5.3% 65 4.8% 86 .98 .76 .77 77 5.6% ppc 357 26.1% 340 24.9% 371 .97 .92 .94 358 26.2% information content ip 1084 79.3% 1027 75.1% 1135 .89 .69 .93 1070 78.3% is 228 16.7% 208 15.2% 256 .95 .80 .83 220 16.1% ic 187 13.7% 109 8.0% 221 .89 .46 .51 130 9.5% interpersonal att+ 71 5.2% 140 10.2% 151 .94 .55 .58 144 10.5% attp 71 5.2% 30 2.2% 79 .96 .42 .44 33 2.4% att- 67 4.9% 74 5.4% 100 .96 .56 .58 87 6.4% table 2: label frequencies and inter-annotator agreement.
</nextsent>
<nextsent>na1a2 denotes the number of turns that have been labeled with the given label by at least one annotator.
</nextsent>
<nextsent>po denotes the observed agreement.
</nextsent>
<nextsent>which is defined as pool = po ? pe 1?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4084">
<title id=" E12-2021.xml">brat a web based tool for nlp assisted text annotation </title>
<section> features.  </section>
<citcontext>
<prevsection>
<prevsent>the tool presented in this work, brat, represents our attempt to realise these possibilities.
</prevsent>
<prevsent>2.1 high-quality annotation visualisation.
</prevsent>
</prevsection>
<citsent citstr=" W11-1816 ">
brat is based on our previously released open source stav text annotation visualiser (stene torp et al 2011<papid> W11-1816 </papid>b), which was designed to help users gain an understanding of complex annotations involving large number of different semantic types, dense, partially overlapping text annotations, and non-projective sets of connections between annotations.</citsent>
<aftsection>
<nextsent>both tools share vector graphics-based visualisation component, which provide scalable detail and rendering.
</nextsent>
<nextsent>brat integrates pdf and eps image format export functionality to support use in e.g. figures in publications (figure 1).
</nextsent>
<nextsent>102 figure 2: screen shot of the main brat user-interface, showing connection being made between the annotations for moving?
</nextsent>
<nextsent>and citibank?.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4085">
<title id=" E12-2021.xml">brat a web based tool for nlp assisted text annotation </title>
<section> features.  </section>
<citcontext>
<prevsection>
<prevsent>additional aspects of annotations can be marked using attributes, binary or multi-valued flags that can be added to other annotations.
</prevsent>
<prevsent>finally, annotators can attach free-form text notes to any annotation.
</prevsent>
</prevsection>
<citsent citstr=" J02-3001 ">
in addition to information extraction tasks, these annotation primitives allow brat to be configured for use in various other tasks, suchas chunking (abney, 1991), semantic role labeling (gildea and jurafsky, 2002; <papid> J02-3001 </papid>carreras and ma`rquez, 2005), and dependency annotation(nivre, 2003) (see figure 1 for examples).</citsent>
<aftsection>
<nextsent>further, both the brat client and server implement full support for the unicode standard, which allow the tool to support the annotation of text using e.g. chinese or devanagar??
</nextsent>
<nextsent>characters.
</nextsent>
<nextsent>bratis distributed with examples from over 20 corpora for variety of tasks, involving texts in seven different languages and including examples from corpora such as those introduced for the conll shared tasks on language-independent named entity recognition (tjong kim sang and de meulder, 2003) and multilingual dependency parsing (buchholz and marsi, 2006).<papid> W06-2920 </papid>brat also implements fully configurable system for checking detailed constraints on annotation semantics, for example specifying that transfer event must take exactly one of eachof giver, recipient and beneficiary arguments, each of which must have one of the types person, organization or geo-political entity, as well as money argument of type 103 figure 3: incomplete transfer event indicated to the annotator money, and may optionally take place argument of type location (ldc, 2005).</nextsent>
<nextsent>constraint checking is fully integrated into the annotation interface and feedback is immediate, with clear visual effects marking incomplete or erroneous annotations (figure 3).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4086">
<title id=" E12-2021.xml">brat a web based tool for nlp assisted text annotation </title>
<section> features.  </section>
<citcontext>
<prevsection>
<prevsent>further, both the brat client and server implement full support for the unicode standard, which allow the tool to support the annotation of text using e.g. chinese or devanagar??
</prevsent>
<prevsent>characters.
</prevsent>
</prevsection>
<citsent citstr=" W06-2920 ">
bratis distributed with examples from over 20 corpora for variety of tasks, involving texts in seven different languages and including examples from corpora such as those introduced for the conll shared tasks on language-independent named entity recognition (tjong kim sang and de meulder, 2003) and multilingual dependency parsing (buchholz and marsi, 2006).<papid> W06-2920 </papid>brat also implements fully configurable system for checking detailed constraints on annotation semantics, for example specifying that transfer event must take exactly one of eachof giver, recipient and beneficiary arguments, each of which must have one of the types person, organization or geo-political entity, as well as money argument of type 103 figure 3: incomplete transfer event indicated to the annotator money, and may optionally take place argument of type location (ldc, 2005).</citsent>
<aftsection>
<nextsent>constraint checking is fully integrated into the annotation interface and feedback is immediate, with clear visual effects marking incomplete or erroneous annotations (figure 3).
</nextsent>
<nextsent>2.4 nlp technology integration.
</nextsent>
<nextsent>brat supports two standard approaches for integrating the results of fully automatic annotation tools into an annotation workflow: bulk annotation imports can be performed by format conversion tools distributed with brat for many standard formats (such as in-line and column formatted bio), and tools that provide standard web service interfaces can be configured to be invoked from the user interface.however, human judgements cannot be replaced or based on completely automatic analysis without some risk of introducing bias andre ducing annotation quality.
</nextsent>
<nextsent>to address this issue,we have been studying ways to augment the annotation process with input from statistical and machine learning methods to support the annotation process while still involving human annotator judgement for each annotation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4091">
<title id=" E12-2021.xml">brat a web based tool for nlp assisted text annotation </title>
<section> related work and conclusions.  </section>
<citcontext>
<prevsection>
<prevsent>we presented an experiment demonstrating that integrated machine learning technology can reduce the time for type selection by over 30% and over all annotation time by 15% for multi-type entity mention annotation task.
</prevsent>
<prevsent>the design and implementation of brat was informed by experience from several annotation tasks and research efforts spanning more than decade.
</prevsent>
</prevsection>
<citsent citstr=" N06-4006 ">
a variety of previously introduced annotation tools and approaches also served toguide our design decisions, including the fast annotation mode of knowtator (ogren, 2006), <papid> N06-4006 </papid>the search capabilities of the xconc tool (kim et al 2008), and the design of web-based systems such as myminer (salgado et al 2010), and gateteamware (cunningham et al 2011).</citsent>
<aftsection>
<nextsent>using machine learning to accelerate annotation by supporting human judgements is well documented in the literature for tasks such as entity annotation(tsuruoka et al 2008) and translation (martnezgomez et al 2011), efforts which served as inspiration for our own approach.brat, along with conversion tools and extensive documentation, is freely available under the open-source mit license from its homepage at http://brat.nlplab.org acknowledgements the authors would like to thank early adopt ers ofbrat who have provided us with extensive feedback and feature suggestions.
</nextsent>
<nextsent>this work was supported by grant-in-aid for specially promoted research (mext, japan), the uk biotechnology and biological sciences research council (bb src) under project automated biological event extraction from the literature for drug discovery (reference number: bb/g013160/1), and the royal swedish academy of sciences.
</nextsent>
<nextsent>106
</nextsent>

</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4092">
<title id=" E09-1065.xml">texttotext semantic similarity for automatic short answer grading </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>inthese instances, we often have to turn to computer assisted assessment.while some forms of computer-assisted assessment do not require sophisticated text understanding (e.g., multiple choice or true/false question scan be easily graded by system if the correct solution is available), there are also student answers that consist of free text which require an analysis of the text in the answer.
</prevsent>
<prevsent>research to date has concentrated on two main subtasks of computer assisted assessment: the grading of essays, whichis done mainly by checking the style, gram mati cality, and coherence of the essay (cf.
</prevsent>
</prevsection>
<citsent citstr=" N04-1024 ">
(higgins et al, 2004)), <papid> N04-1024 </papid>and the assessment of short student answers (e.g., (leacock and chodorow, 2003; pul man and sukkarieh, 2005)), <papid> W05-0202 </papid>which is the focus of this paper.</citsent>
<aftsection>
<nextsent>an automatic short answer grading system isone which automatically assigns grade to an answer provided by student through comparison with one or more correct answers.
</nextsent>
<nextsent>it is important to note that this is different from the related task of paraphrase detection, since requirement in student answer grading is to provide grade on certain scale rather than binary yes/no decision.
</nextsent>
<nextsent>in this paper, we explore and evaluate set of unsupervised techniques for automatic short answer grading.
</nextsent>
<nextsent>unlike previous work, which has either required the availability of manually crafted patterns (sukkarieh et al, 2004; mitchell et al, 2002), or large training datasets to bootstrap such patterns (pulman and sukkarieh, 2005), <papid> W05-0202 </papid>we attempt to devise an unsupervised method that requires no human intervention.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4093">
<title id=" E09-1065.xml">texttotext semantic similarity for automatic short answer grading </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>inthese instances, we often have to turn to computer assisted assessment.while some forms of computer-assisted assessment do not require sophisticated text understanding (e.g., multiple choice or true/false question scan be easily graded by system if the correct solution is available), there are also student answers that consist of free text which require an analysis of the text in the answer.
</prevsent>
<prevsent>research to date has concentrated on two main subtasks of computer assisted assessment: the grading of essays, whichis done mainly by checking the style, gram mati cality, and coherence of the essay (cf.
</prevsent>
</prevsection>
<citsent citstr=" W05-0202 ">
(higgins et al, 2004)), <papid> N04-1024 </papid>and the assessment of short student answers (e.g., (leacock and chodorow, 2003; pul man and sukkarieh, 2005)), <papid> W05-0202 </papid>which is the focus of this paper.</citsent>
<aftsection>
<nextsent>an automatic short answer grading system isone which automatically assigns grade to an answer provided by student through comparison with one or more correct answers.
</nextsent>
<nextsent>it is important to note that this is different from the related task of paraphrase detection, since requirement in student answer grading is to provide grade on certain scale rather than binary yes/no decision.
</nextsent>
<nextsent>in this paper, we explore and evaluate set of unsupervised techniques for automatic short answer grading.
</nextsent>
<nextsent>unlike previous work, which has either required the availability of manually crafted patterns (sukkarieh et al, 2004; mitchell et al, 2002), or large training datasets to bootstrap such patterns (pulman and sukkarieh, 2005), <papid> W05-0202 </papid>we attempt to devise an unsupervised method that requires no human intervention.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4096">
<title id=" E09-1065.xml">texttotext semantic similarity for automatic short answer grading </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>one of the earliest approaches to text similarity is the vector-space model (salton et al, 1997) with term frequency / inverse document frequency (tf.idf) weighting.
</prevsent>
<prevsent>this model, along with the more sophisticated lsa semantic alternative (landauer and dumais, 1997), has been found to work well for tasks such as information retrieval and text classification.
</prevsent>
</prevsection>
<citsent citstr=" W99-0625 ">
another approach (hatzivassiloglou et al,1999) <papid> W99-0625 </papid>has been to use machine learning algorithm in which features are based on combinations of simple features (e.g., pair of nouns appear within 5 words from one another in both texts).</citsent>
<aftsection>
<nextsent>this method also attempts to account for synonymy, word ordering, text length, and word classes.
</nextsent>
<nextsent>another line of work attempts to extrapolate text similarity from the arguably simpler problem of word similarity.
</nextsent>
<nextsent>(mihalcea et al, 2006) explores the efficacy of applying wordnet-based word-to-word similarity measures (pedersen et al, 2004) to the comparison of texts and found them generally comparable to corpus-based measures such as lsa.
</nextsent>
<nextsent>an interesting study has been performed at the university of adelaide (lee et al, 2005), comparing simpler word and n-gram feature vectors to lsa and exploring the types of vector similarity metrics (e.g., binary vs. count vectors, jaccard vs. cosine vs. overlap distance measure, etc.).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4097">
<title id=" E09-1065.xml">texttotext semantic similarity for automatic short answer grading </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>an interesting study has been performed at the university of adelaide (lee et al, 2005), comparing simpler word and n-gram feature vectors to lsa and exploring the types of vector similarity metrics (e.g., binary vs. count vectors, jaccard vs. cosine vs. overlap distance measure, etc.).
</prevsent>
<prevsent>in this case, lsa was shown to perform better than the word and n-gram vectors and performed best at around 100 dimensions with binary vectors weighted according to an entropy measure, though the difference in measures was often subtle.
</prevsent>
</prevsection>
<citsent citstr=" W03-0208 ">
selsa (kanejiya et al, 2003) <papid> W03-0208 </papid>is system that attempts to add context to lsa by supplementing the feature vectors with some simple syntactical features, namely the part-of-speech of the previous word.</citsent>
<aftsection>
<nextsent>their results indicate that selsa does not perform as well as lsa in the best case, but it has wider threshold window than lsa in which the system can be used advantageously.
</nextsent>
<nextsent>finally, explicit semantic analysis (esa) (gabrilovich and markovitch, 2007) uses wikipedia as source of knowledge for text similarity.
</nextsent>
<nextsent>it creates for each text feature vector where each feature maps to wikipedia article.
</nextsent>
<nextsent>their preliminary experiments indicated that esa was able to significantly outperform lsa on some text similarity tasks.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4098">
<title id=" E09-1065.xml">texttotext semantic similarity for automatic short answer grading </title>
<section> text-to-text semantic similarity.  </section>
<citcontext>
<prevsection>
<prevsent>the leacock &amp; chodorow (leacock and chodorow, 1998) similarity is determined as: simlch = ? log length 2 (2)where length is the length of the shortest path between two concepts using node-counting, and is the maximum depth of the taxonomy.
</prevsent>
<prevsent>the lesk similarity of two concepts is defined asa function of the overlap between the corresponding definitions, as provided by dictionary.
</prevsent>
</prevsection>
<citsent citstr=" P94-1019 ">
it is based on an algorithm proposed by lesk (1986) as solution for word sense disambiguation.the wu &amp; palmer (wu and palmer, 1994) <papid> P94-1019 </papid>similarity metric measures the depth of two given concepts in the wordnet taxonomy, and the depth of the least common subsumer (lcs), and combines these figures into similarity score: simwup = 2 ? depth(lcs) depth(concept1) + depth(concept2) (3) the measure introduced by resnik (resnik, 1995) returns the information content (ic) of the lcs of two concepts: simres = ic(lcs) (4) where ic is defined as: ic(c) = ? logp (c) (5)and (c) is the probability of encountering an instance of concept in large corpus.</citsent>
<aftsection>
<nextsent>570 the measure introduced by lin (lin, 1998) builds on resniks measure of similarity, and adds normalization factor consisting of the information content of the two input concepts: simlin = 2 ? ic(lcs) ic(concept1) + ic(concept2) (6) we also consider the jiang &amp; conrath (jiang and conrath, 1997) measure of similarity: simjnc = 1 ic(concept1) + ic(concept2)?
</nextsent>
<nextsent>2 ? ic(lcs) (7) finally, we consider the hirst &amp; st. onge (hirst and st-onge, 1998) measure of similarity, which determines the similarity strength of pair of synsets by detecting lexical chains between the pair in text using the wordnet hierarchy.
</nextsent>
<nextsent>5.2 corpus-based measures.
</nextsent>
<nextsent>corpus-based measures differ from knowledge based methods in that they do not require any encoded understanding of either the vocabulary or the grammar of texts language.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4099">
<title id=" E09-1065.xml">texttotext semantic similarity for automatic short answer grading </title>
<section> conclusions.  </section>
<citcontext>
<prevsection>
<prevsent>in fact, when using lsa, our results indicate that the corpus domain may be significantly more important than corpus size once certain threshold size has been reached.finally, we introduced novel technique for integrating feedback from the student answers themselves into the grading system.
</prevsent>
<prevsent>using method similar to the pseudo-relevance feedback technique used in information retrieval, we were ableto improve the quality of our system by few percentage points.
</prevsent>
</prevsection>
<citsent citstr=" P07-1098 ">
overall, our best system consists of an lsa measure trained on domain-specific corpus built on wikipedia with feedback from student answers, which was found to bring significant absolute improvement on the 0-1 pearson scale of 0.14 over the tf*idf baseline and 0.10 over the lsa bnc model that has been used in the past.in future work, we intend to expand our analysis of both the gold-standard answer and the student answers beyond the bag-of-words paradigm by considering basic logical features in the text (i.e., and, or, not) as well as the existence of shallow grammatical features such as predicate argument structure(moschitti et al, 2007) <papid> P07-1098 </papid>as well as semantic classes for words.</citsent>
<aftsection>
<nextsent>furthermore, it maybe advantageous to expand upon the existing measures by applying machine learning techniques to create hybrid decision system that would exploit the advantages of each measure.
</nextsent>
<nextsent>the dataset introduced in this paper, along with the human-assigned grades, can be downloaded from http://lit.csci.unt.edu/index.php/downloads.
</nextsent>
<nextsent>acknowledgments this work was partially supported by national science foundation career award #0747340.the authors are grateful to samer hassan for making available his implementation of the esa algorithm.
</nextsent>

</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4100">
<title id=" E12-1054.xml">measuring contextual fitness using error contexts extracted from the wikipedia revision history </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>additionally, we show that knowledge-based approaches can be improved by using semantic relatedness measures that make use of knowledge beyond classical taxonomic relations.
</prevsent>
<prevsent>finally,we show that statistical and knowledge based methods can be combined for increased performance.
</prevsent>
</prevsection>
<citsent citstr=" N04-1038 ">
measuring the contextual fitness of term in its context is key component in different nlp applications like speech recognition (inkpen and desi lets, 2005), optical character recognition (wick et al  2007), co-reference resolution (beanand riloff, 2004), <papid> N04-1038 </papid>or malapropism detection (bolshakov and gelbukh, 2003).</citsent>
<aftsection>
<nextsent>the main idea is always to test what fits better into the current con text: the actual term or possible replacement thatis phonetic ally, structurally, or semantically similar.
</nextsent>
<nextsent>we are going to focus on malapropism detection as it allows evaluating measures of contextual fitness in more direct way than evaluating in complex application which always entails influence from other components, e.g. the quality of the optical character recognition module (walker et al  2010).<papid> D10-1024 </papid>a malapropism or real-word spelling error occurs when word is replaced with another correctly spelled word which does not suit the context, e.g. people with lots of honey usually live in big houses.?, where money?</nextsent>
<nextsent>was replaced with honey?.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4101">
<title id=" E12-1054.xml">measuring contextual fitness using error contexts extracted from the wikipedia revision history </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>measuring the contextual fitness of term in its context is key component in different nlp applications like speech recognition (inkpen and desi lets, 2005), optical character recognition (wick et al  2007), co-reference resolution (beanand riloff, 2004), <papid> N04-1038 </papid>or malapropism detection (bolshakov and gelbukh, 2003).</prevsent>
<prevsent>the main idea is always to test what fits better into the current con text: the actual term or possible replacement thatis phonetic ally, structurally, or semantically similar.</prevsent>
</prevsection>
<citsent citstr=" D10-1024 ">
we are going to focus on malapropism detection as it allows evaluating measures of contextual fitness in more direct way than evaluating in complex application which always entails influence from other components, e.g. the quality of the optical character recognition module (walker et al  2010).<papid> D10-1024 </papid>a malapropism or real-word spelling error occurs when word is replaced with another correctly spelled word which does not suit the context, e.g. people with lots of honey usually live in big houses.?, where money?</citsent>
<aftsection>
<nextsent>was replaced with honey?.
</nextsent>
<nextsent>besides typing mistakes, major source of such errors is the failed attempt of automatic spelling cor rectors to correct misspelled word (hirst and budanitsky, 2005).
</nextsent>
<nextsent>a real-word spelling error is hard to detect, as the erroneous word is not misspelled and fits syntactically into the sentence.
</nextsent>
<nextsent>thus, measures of contextual fitness are required to detect words that do not fit their contexts.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4102">
<title id=" E12-1054.xml">measuring contextual fitness using error contexts extracted from the wikipedia revision history </title>
<section> mining errors from wikipedia.  </section>
<citcontext>
<prevsection>
<prevsent>the framework contains (i) method sto extract natural errors from wikipedia, (ii) reference implementations of the knowledge-basedand the statistical methods, and (iii) the evaluation datasets described in this paper.
</prevsent>
<prevsent>measures of contextual fitness have previously been evaluated using artificially created datasets, as there are very few sources of sentences with naturally occurring errors and their corrections.
</prevsent>
</prevsection>
<citsent citstr=" N10-1056 ">
recently, the revision history of wikipedia has been introduced as valuable knowledge source for nlp (nelken and yamangil, 2008; yatskar et al ., 2010).<papid> N10-1056 </papid></citsent>
<aftsection>
<nextsent>it is also possible source of natural errors, as it is likely that wikipedia editors make 1the same artificial data as described in section 3.2.
</nextsent>
<nextsent>2http://code.google.com/p/dkpro-spelling-asl/ real-word spelling errors at some point, which are then corrected in subsequent revisions of the same article.
</nextsent>
<nextsent>the challenge lies in discriminating real-word spelling errors from all sorts of other changes, including non-word spelling errors, reformulations, or the correction of wrong facts.for that purpose, we apply set of precision oriented heuristics narrowing down the number of possible error candidates.
</nextsent>
<nextsent>such an approach is feasible, as the high number of revisions in wikipedia allows to be extremely selective.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4103">
<title id=" E12-1054.xml">measuring contextual fitness using error contexts extracted from the wikipedia revision history </title>
<section> mining errors from wikipedia.  </section>
<citcontext>
<prevsection>
<prevsent>such an approach is feasible, as the high number of revisions in wikipedia allows to be extremely selective.
</prevsent>
<prevsent>2.1 accessing the revision data.
</prevsent>
</prevsection>
<citsent citstr=" P11-4017 ">
we access the wikipedia revision data using the freely available wikipedia revision toolkit (ferschke et al  2011) <papid> P11-4017 </papid>together with the jwpl wikipedia api (zesch et al  2008<papid> L08-1139 </papid>a).3 the api outputs plain text converted from wiki-markup,but the text still contains small portion of leftover markup and other artifacts.</citsent>
<aftsection>
<nextsent>thus, we perform additional cleaning steps removing (i) tokens with more than 30 characters (often urls), (ii) sentences with less than 5 or more than 200tokens, and (iii) sentences containing high fraction of special characters like ?:?
</nextsent>
<nextsent>usually indicating wikipedia-specific artifacts like lists of language links.
</nextsent>
<nextsent>the remaining sentences are part-of speech tagged and lemmatized using treetagger(schmid, 2004).<papid> C04-1024 </papid></nextsent>
<nextsent>using these cleaned and annotated articles, we form pairs of adjacent article revisions (ri and ri+1).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4104">
<title id=" E12-1054.xml">measuring contextual fitness using error contexts extracted from the wikipedia revision history </title>
<section> mining errors from wikipedia.  </section>
<citcontext>
<prevsection>
<prevsent>such an approach is feasible, as the high number of revisions in wikipedia allows to be extremely selective.
</prevsent>
<prevsent>2.1 accessing the revision data.
</prevsent>
</prevsection>
<citsent citstr=" L08-1139 ">
we access the wikipedia revision data using the freely available wikipedia revision toolkit (ferschke et al  2011) <papid> P11-4017 </papid>together with the jwpl wikipedia api (zesch et al  2008<papid> L08-1139 </papid>a).3 the api outputs plain text converted from wiki-markup,but the text still contains small portion of leftover markup and other artifacts.</citsent>
<aftsection>
<nextsent>thus, we perform additional cleaning steps removing (i) tokens with more than 30 characters (often urls), (ii) sentences with less than 5 or more than 200tokens, and (iii) sentences containing high fraction of special characters like ?:?
</nextsent>
<nextsent>usually indicating wikipedia-specific artifacts like lists of language links.
</nextsent>
<nextsent>the remaining sentences are part-of speech tagged and lemmatized using treetagger(schmid, 2004).<papid> C04-1024 </papid></nextsent>
<nextsent>using these cleaned and annotated articles, we form pairs of adjacent article revisions (ri and ri+1).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4106">
<title id=" E12-1054.xml">measuring contextual fitness using error contexts extracted from the wikipedia revision history </title>
<section> mining errors from wikipedia.  </section>
<citcontext>
<prevsection>
<prevsent>thus, we perform additional cleaning steps removing (i) tokens with more than 30 characters (often urls), (ii) sentences with less than 5 or more than 200tokens, and (iii) sentences containing high fraction of special characters like ?:?
</prevsent>
<prevsent>usually indicating wikipedia-specific artifacts like lists of language links.
</prevsent>
</prevsection>
<citsent citstr=" C04-1024 ">
the remaining sentences are part-of speech tagged and lemmatized using treetagger(schmid, 2004).<papid> C04-1024 </papid></citsent>
<aftsection>
<nextsent>using these cleaned and annotated articles, we form pairs of adjacent article revisions (ri and ri+1).
</nextsent>
<nextsent>2.2 sentence alignment.
</nextsent>
<nextsent>fully aligning all sentences of the adjacent revisions is quite costly operation, as sentences canbe split, joined, replaced, or moved in the article.
</nextsent>
<nextsent>however, we are only looking for sentence pairs which are almost identical except for the real-word spelling error and its correction.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4107">
<title id=" E12-1054.xml">measuring contextual fitness using error contexts extracted from the wikipedia revision history </title>
<section> mining errors from wikipedia.  </section>
<citcontext>
<prevsection>
<prevsent>stop words the replaced token should not be in short list of stop words (mostly function words).
</prevsent>
<prevsent>named entity the replaced token should not be part of named entity.
</prevsent>
</prevsection>
<citsent citstr=" P05-1045 ">
for this purpose, we applied the stanford ner (finkel et al  2005).<papid> P05-1045 </papid></citsent>
<aftsection>
<nextsent>normal spelling error we apply the jazzy spelling detector4 and rule out all cases in which it is able to detect the error.
</nextsent>
<nextsent>semantic relation if the original token and the replaced token are in close lexical-semantic rela 4http://jazzy.sourceforge.net/tions, the change is likely to be semantically motivated, e.g. if house?
</nextsent>
<nextsent>was replaced with hut?.
</nextsent>
<nextsent>thus, we do not consider cases, where we detect direct semantic relation between the original andthe replaced term.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4108">
<title id=" E12-1054.xml">measuring contextual fitness using error contexts extracted from the wikipedia revision history </title>
<section> measuring contextual fitness.  </section>
<citcontext>
<prevsection>
<prevsent>we decided not to use both filters in order to better assess the influence of the underlying semantic relatedness measure on the overall performance.
</prevsent>
<prevsent>the knowledge based approach uses semantic relatedness measures to determine the cohesion between candidate and its context.
</prevsent>
</prevsection>
<citsent citstr=" J06-1003 ">
in the experiments by budanitsky and hirst (2006), <papid> J06-1003 </papid>the measure by (jiang and conrath, 1997) yields the best results.</citsent>
<aftsection>
<nextsent>however, wide range of other measures have been proposed, cf.
</nextsent>
<nextsent>(zesch and gurevych,2010).
</nextsent>
<nextsent>some measures using wider definition of semantic relatedness (gabrilovich and markovitch, 2007; zesch et al  2008<papid> L08-1139 </papid>b) instead of only using taxonomic relations in knowledge source.as semantic relatedness measures usually return numeric value, we need to determine threshold ? in order to come up with binary related/unrelated decision.</nextsent>
<nextsent>budanitsky and hirst(2006) <papid> J06-1003 </papid>used characteristic gap in the standard evaluation dataset by rubenstein and goodenough (1965) that separates unrelated from related word pairs.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4114">
<title id=" E12-1054.xml">measuring contextual fitness using error contexts extracted from the wikipedia revision history </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>thus, their dataset cannot be easily used for our purposes and is only available in french, while our framework allows creating datasets for all major languages with minimal manual effort.
</prevsent>
<prevsent>another possible source of real-word spelling errors are learner corpora (granger, 2002), e.g. the cambridge learner corpus (nicholls, 1999).
</prevsent>
</prevsection>
<citsent citstr=" W10-1004 ">
however, annotation of errors is difficult and costly (rozovskaya and roth, 2010), <papid> W10-1004 </papid>only small fraction of observed errors will be real-wordspelling errors, and learners are likely to make dif 535 ferent mistakes than proficient language users.islam and inkpen (2009) presented another statistical approach using the google web1t data (brants and franz, 2006) to create the n-gram model.</citsent>
<aftsection>
<nextsent>it slightly outperformed the approach by mays et al (1991) when evaluated on corpus of artificial errors based on the wsj corpus.
</nextsent>
<nextsent>how ever, the results are not directly comparable, as mays et al (1991) used much smaller n-gram model and our results in section 5.1 show that the size of the n-gram model has large influence on the results.
</nextsent>
<nextsent>eventually, we decided to use the mays et al (1991) approach in our study, as it is easier to adapt and augment.
</nextsent>
<nextsent>in re-evaluation of the statistical model by mays et al (1991), wilcox-ohearn et al (2008) found that it outperformed the knowledge-based method by hirst and budanitsky (2005) when evaluated on corpus of artificial errors based onthe wsj corpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4115">
<title id=" E09-1085.xml">incremental dialogue processing in a micro domain </title>
<section> incremental dialogue processing.  </section>
<citcontext>
<prevsection>
<prevsent>silence, how ever, is not good indicator: sometimes there is silence but no turn-change is intended (e.g., hesi tations), sometimes there isnt silence, but the turn changes (sacks et al, 1974).
</prevsent>
<prevsent>speakers appear to use other knowledge sources, such as prosody, syntax and semantics to detector even project the end of the utterance.
</prevsent>
</prevsection>
<citsent citstr=" W08-0101 ">
attempts have been made to incorporate such knowledge sources for turn-taking decisions in spoken dialogue systems (e.g., ferrer et al, 2002; raux &amp; eskenazi, 2008).<papid> W08-0101 </papid></citsent>
<aftsection>
<nextsent>to do so, incremental dialogue processing is clearly needed.
</nextsent>
<nextsent>incremental processing can also lead to better use of resources, since later modules can start to work on partial results and do not have to wait until earlier modules have completed processing the whole utterance.
</nextsent>
<nextsent>for example, while the speech recogniser starts to identify words, the parser can already add these to the chart.
</nextsent>
<nextsent>later modules can also assist in the processing and for example resolve ambiguities as they come up.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4116">
<title id=" E09-1085.xml">incremental dialogue processing in a micro domain </title>
<section> incremental dialogue processing.  </section>
<citcontext>
<prevsection>
<prevsent>for example, while the speech recogniser starts to identify words, the parser can already add these to the chart.
</prevsent>
<prevsent>later modules can also assist in the processing and for example resolve ambiguities as they come up.
</prevsent>
</prevsection>
<citsent citstr=" W04-0304 ">
stoness et al (2004) <papid> W04-0304 </papid>shows how reference resolution module can help an incremental parser with np suitability judgements.</citsent>
<aftsection>
<nextsent>similarly, aist et al.
</nextsent>
<nextsent>(2006) shows how vp advisor could help an incremental parser.
</nextsent>
<nextsent>on the output side, an incremental dialogue system could monitor what is actually happening to the utterance it produces.
</nextsent>
<nextsent>as discussed by raux &amp; eskenazi (2007), most dialogue managers operate asynchronously from the output components, which may lead to problems if the dialogue manager produces several actions and the user responds to one of them.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4118">
<title id=" E12-1008.xml">dependency parsing with undirected graphs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we perform experiments on several datasets from the conll-x shared task, showing that these variants outperform the original directed algorithms in most of the cases.
</prevsent>
<prevsent>dependency parsing has proven to be very useful for natural language processing tasks.
</prevsent>
</prevsection>
<citsent citstr=" W04-2407 ">
data driven dependency parsers such as those by nivre et al(2004), <papid> W04-2407 </papid>mcdonald et al(2005), <papid> H05-1066 </papid>titov and henderson (2007), <papid> W07-2218 </papid>martins et al(2009) <papid> P09-1039 </papid>or huang and sagae (2010) <papid> P10-1110 </papid>are accurate and efficient, they can be trained from annotated data without the need for grammar, and they provide simple representation of syntax that maps to predicate argument structure in straightforward way.</citsent>
<aftsection>
<nextsent>in particular, transition-based dependency parsers (nivre, 2008) <papid> J08-4003 </papid>are type of dependency parsing algorithms which use model that scores transitions between parser states.</nextsent>
<nextsent>greedy deterministic search can be used to select the transition to be taken at each state, thus achieving linear or quadratic time complexity.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4119">
<title id=" E12-1008.xml">dependency parsing with undirected graphs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we perform experiments on several datasets from the conll-x shared task, showing that these variants outperform the original directed algorithms in most of the cases.
</prevsent>
<prevsent>dependency parsing has proven to be very useful for natural language processing tasks.
</prevsent>
</prevsection>
<citsent citstr=" H05-1066 ">
data driven dependency parsers such as those by nivre et al(2004), <papid> W04-2407 </papid>mcdonald et al(2005), <papid> H05-1066 </papid>titov and henderson (2007), <papid> W07-2218 </papid>martins et al(2009) <papid> P09-1039 </papid>or huang and sagae (2010) <papid> P10-1110 </papid>are accurate and efficient, they can be trained from annotated data without the need for grammar, and they provide simple representation of syntax that maps to predicate argument structure in straightforward way.</citsent>
<aftsection>
<nextsent>in particular, transition-based dependency parsers (nivre, 2008) <papid> J08-4003 </papid>are type of dependency parsing algorithms which use model that scores transitions between parser states.</nextsent>
<nextsent>greedy deterministic search can be used to select the transition to be taken at each state, thus achieving linear or quadratic time complexity.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4120">
<title id=" E12-1008.xml">dependency parsing with undirected graphs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we perform experiments on several datasets from the conll-x shared task, showing that these variants outperform the original directed algorithms in most of the cases.
</prevsent>
<prevsent>dependency parsing has proven to be very useful for natural language processing tasks.
</prevsent>
</prevsection>
<citsent citstr=" W07-2218 ">
data driven dependency parsers such as those by nivre et al(2004), <papid> W04-2407 </papid>mcdonald et al(2005), <papid> H05-1066 </papid>titov and henderson (2007), <papid> W07-2218 </papid>martins et al(2009) <papid> P09-1039 </papid>or huang and sagae (2010) <papid> P10-1110 </papid>are accurate and efficient, they can be trained from annotated data without the need for grammar, and they provide simple representation of syntax that maps to predicate argument structure in straightforward way.</citsent>
<aftsection>
<nextsent>in particular, transition-based dependency parsers (nivre, 2008) <papid> J08-4003 </papid>are type of dependency parsing algorithms which use model that scores transitions between parser states.</nextsent>
<nextsent>greedy deterministic search can be used to select the transition to be taken at each state, thus achieving linear or quadratic time complexity.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4121">
<title id=" E12-1008.xml">dependency parsing with undirected graphs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we perform experiments on several datasets from the conll-x shared task, showing that these variants outperform the original directed algorithms in most of the cases.
</prevsent>
<prevsent>dependency parsing has proven to be very useful for natural language processing tasks.
</prevsent>
</prevsection>
<citsent citstr=" P09-1039 ">
data driven dependency parsers such as those by nivre et al(2004), <papid> W04-2407 </papid>mcdonald et al(2005), <papid> H05-1066 </papid>titov and henderson (2007), <papid> W07-2218 </papid>martins et al(2009) <papid> P09-1039 </papid>or huang and sagae (2010) <papid> P10-1110 </papid>are accurate and efficient, they can be trained from annotated data without the need for grammar, and they provide simple representation of syntax that maps to predicate argument structure in straightforward way.</citsent>
<aftsection>
<nextsent>in particular, transition-based dependency parsers (nivre, 2008) <papid> J08-4003 </papid>are type of dependency parsing algorithms which use model that scores transitions between parser states.</nextsent>
<nextsent>greedy deterministic search can be used to select the transition to be taken at each state, thus achieving linear or quadratic time complexity.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4122">
<title id=" E12-1008.xml">dependency parsing with undirected graphs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we perform experiments on several datasets from the conll-x shared task, showing that these variants outperform the original directed algorithms in most of the cases.
</prevsent>
<prevsent>dependency parsing has proven to be very useful for natural language processing tasks.
</prevsent>
</prevsection>
<citsent citstr=" P10-1110 ">
data driven dependency parsers such as those by nivre et al(2004), <papid> W04-2407 </papid>mcdonald et al(2005), <papid> H05-1066 </papid>titov and henderson (2007), <papid> W07-2218 </papid>martins et al(2009) <papid> P09-1039 </papid>or huang and sagae (2010) <papid> P10-1110 </papid>are accurate and efficient, they can be trained from annotated data without the need for grammar, and they provide simple representation of syntax that maps to predicate argument structure in straightforward way.</citsent>
<aftsection>
<nextsent>in particular, transition-based dependency parsers (nivre, 2008) <papid> J08-4003 </papid>are type of dependency parsing algorithms which use model that scores transitions between parser states.</nextsent>
<nextsent>greedy deterministic search can be used to select the transition to be taken at each state, thus achieving linear or quadratic time complexity.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4123">
<title id=" E12-1008.xml">dependency parsing with undirected graphs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>dependency parsing has proven to be very useful for natural language processing tasks.
</prevsent>
<prevsent>data driven dependency parsers such as those by nivre et al(2004), <papid> W04-2407 </papid>mcdonald et al(2005), <papid> H05-1066 </papid>titov and henderson (2007), <papid> W07-2218 </papid>martins et al(2009) <papid> P09-1039 </papid>or huang and sagae (2010) <papid> P10-1110 </papid>are accurate and efficient, they can be trained from annotated data without the need for grammar, and they provide simple representation of syntax that maps to predicate argument structure in straightforward way.</prevsent>
</prevsection>
<citsent citstr=" J08-4003 ">
in particular, transition-based dependency parsers (nivre, 2008) <papid> J08-4003 </papid>are type of dependency parsing algorithms which use model that scores transitions between parser states.</citsent>
<aftsection>
<nextsent>greedy deterministic search can be used to select the transition to be taken at each state, thus achieving linear or quadratic time complexity.
</nextsent>
<nextsent>0 1 2 3 figure 1: an example dependency structure wheretransition-based parsers enforcing the single-head constraint will incur in error propagation if they mistakenly build dependency link 1 ? 2 instead of 2 ? 1 (dependency links are represented as arrows going from head to dependent).
</nextsent>
<nextsent>it has been shown by mcdonald and nivre(2007) <papid> D07-1013 </papid>that such parsers suffer from error prop agation: an early erroneous choice can place the parser in an incorrect state that will in turn lead to more errors.</nextsent>
<nextsent>for instance, suppose that sentence whose correct analysis is the dependency graphin figure 1 is analyzed by any bottom-up or leftto-right transition-based parser that outputs dependency trees, therefore obeying the single-head constraint (only one incoming arc is allowed pernode).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4125">
<title id=" E12-1008.xml">dependency parsing with undirected graphs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>greedy deterministic search can be used to select the transition to be taken at each state, thus achieving linear or quadratic time complexity.
</prevsent>
<prevsent>0 1 2 3 figure 1: an example dependency structure wheretransition-based parsers enforcing the single-head constraint will incur in error propagation if they mistakenly build dependency link 1 ? 2 instead of 2 ? 1 (dependency links are represented as arrows going from head to dependent).
</prevsent>
</prevsection>
<citsent citstr=" D07-1013 ">
it has been shown by mcdonald and nivre(2007) <papid> D07-1013 </papid>that such parsers suffer from error prop agation: an early erroneous choice can place the parser in an incorrect state that will in turn lead to more errors.</citsent>
<aftsection>
<nextsent>for instance, suppose that sentence whose correct analysis is the dependency graphin figure 1 is analyzed by any bottom-up or leftto-right transition-based parser that outputs dependency trees, therefore obeying the single-head constraint (only one incoming arc is allowed pernode).
</nextsent>
<nextsent>if the parser chooses an erroneous transition that leads it to build dependency link from 1 to 2 instead of the correct link from 2 to 1, thi swill lead it to state where the single-head constraint makes it illegal to create the link from 3 to two attachment errors in the output tree.
</nextsent>
<nextsent>with the goal of minimizing these sources of errors, we obtain novel undirected variants of several parsers; namely, of the planar and 2 planar parsers by gomez-rodrguez and nivre (2010) and the non-projective list-based parser described by nivre (2008), <papid> J08-4003 </papid>which is based on covingtons algorithm (covington, 2001).</nextsent>
<nextsent>these variants work by collapsing the left-arc and 66 right-arc transitions in the original parsers,which create right-to-left and left-to-right dependency links, into single arc transition creating an undirected link.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4137">
<title id=" E12-1008.xml">dependency parsing with undirected graphs </title>
<section> preliminaries.  </section>
<citcontext>
<prevsection>
<prevsent>we now outline these directed parsers briefly, more detailed description can be found in the above references.
</prevsent>
<prevsent>2.3.1 planar the planar transition system by gomez rodrguez and nivre (2010) is linear-time transition-based parser for planar dependency forests, i.e., forests whose dependency arcs do not cross when drawn above the words.
</prevsent>
</prevsection>
<citsent citstr=" P06-2066 ">
the set of planar dependency structures is very mild extension of that of projective structures (kuhlmann and nivre, 2006).<papid> P06-2066 </papid></citsent>
<aftsection>
<nextsent>configurations in this system are of the form = ??, b,a? where ? and are disjoint lists of nodes from vw (for some input w), and is set of dependency links over vw.
</nextsent>
<nextsent>the list b, called the buffer, holds the input words that are still to be read.
</nextsent>
<nextsent>the list ?, called the stack, is initially empty and is used to hold words that have dependency links pending to be created.
</nextsent>
<nextsent>the syst emis shown at the top in figure 2, where the notation ? | is used for stack with top and tail ?, and we invert the notation for the buffer for clarity (i.e., | as buffer with top and tail b).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4140">
<title id=" E12-1008.xml">dependency parsing with undirected graphs </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>we believe that the better accuracy obtained with this criterion probably stems from the fact that it is biased towards changing links from the root, which tend to be more problematic for transition-based parsers, while respecting the parser output for links located deeper in the dependency structure, for which transition-based parsers tend to be more accurate (mcdonald and nivre, 2007).<papid> D07-1013 </papid>note that both variants of label-based reconstruction have the property that, if the undirected parser produces the correct edges and labels for 0 1 2 3 4 5 r l 0 1 2 3 4 5 0 1 2 3 4 5 a. b. c. figure 4: a) an undirected graph obtained by the parser with the label-based transformation, b) and c) the dependency graph obtained by each of the variants of the label-based reconstruction (note how the second variant moves an arc from the root).</prevsent>
<prevsent>given sentence, then the obtained directed tree is guaranteed to be correct (as it will simply be the tree obtained by decoding the label annotations).</prevsent>
</prevsection>
<citsent citstr=" W06-2920 ">
in this section, we evaluate the performance of the undirected planar, 2-planar and covington parsers on eight datasets from the conll-x shared task (buchholz and marsi, 2006).<papid> W06-2920 </papid></citsent>
<aftsection>
<nextsent>tables 1, 2 and 3 compare the accuracy of the undirected versions with naive and label-based reconstruction to that of the directed versions ofthe planar, 2-planar and covington parsers, respectively.
</nextsent>
<nextsent>in addition, we provide comparison to well-known state-of-the-art projective and non projective parsers: the planar parsers are compared to the arc-eager projective parser by nivre(2003), which is also restricted to planar struc tures; and the 2-planar parsers are compared withthe arc-eager parser with pseudo-projective transformation of nivre and nilsson (2005), <papid> P05-1013 </papid>capable of handling non-planar dependencies.</nextsent>
<nextsent>we use svm classifiers from the libsvmpackage (chang and lin, 2001) for all the languages except chinese, czech and german.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4142">
<title id=" E12-1008.xml">dependency parsing with undirected graphs </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>in this section, we evaluate the performance of the undirected planar, 2-planar and covington parsers on eight datasets from the conll-x shared task (buchholz and marsi, 2006).<papid> W06-2920 </papid></prevsent>
<prevsent>tables 1, 2 and 3 compare the accuracy of the undirected versions with naive and label-based reconstruction to that of the directed versions ofthe planar, 2-planar and covington parsers, re spectively.</prevsent>
</prevsection>
<citsent citstr=" P05-1013 ">
in addition, we provide comparison to well-known state-of-the-art projective and non projective parsers: the planar parsers are compared to the arc-eager projective parser by nivre(2003), which is also restricted to planar struc tures; and the 2-planar parsers are compared withthe arc-eager parser with pseudo-projective transformation of nivre and nilsson (2005), <papid> P05-1013 </papid>capable of handling non-planar dependencies.</citsent>
<aftsection>
<nextsent>we use svm classifiers from the libsvmpackage (chang and lin, 2001) for all the languages except chinese, czech and german.
</nextsent>
<nextsent>in these, we use the liblinear package (fan etal., 2008) for classification, which reduces training time for these larger datasets; and feature models adapted to this system which, in the caseof german, result in higher accuracy than published results using libsvm.
</nextsent>
<nextsent>72 the libsvm feature models for the arc-eager projective and pseudo-projective parsers are the same used by these parsers in the conll-x shared task, where the pseudo-projective version of malt parser was one of the two top performing systems (buchholz and marsi, 2006).<papid> W06-2920 </papid></nextsent>
<nextsent>for the 2 planar parser, we took the feature models fromgomez-rodrguez and nivre (2010) for the languages included in that paper.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4148">
<title id=" E09-1058.xml">user simulations for context sensitive speech recognition in spoken dialogue systems </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>hypothesis and to decide on appropriate system reactions.
</prevsent>
<prevsent>we evaluate this approach in comparison with baseline system that works in the standard way: always choosing the topmost hypothesis in the n-best list.in such systems, complex repair strategies are required when the top hypothesis is incorrect.the main novelty of this work is that we explore the use of predictions from simple statistical user simulations to re-rank n-best lists of asrhypotheses.
</prevsent>
</prevsection>
<citsent citstr=" W03-2111 ">
these user simulations are now commonly used in statistical learning approaches to dialogue management (williams and young, 2003; <papid> W03-2111 </papid>schatzmann et al, 2006; young, 2006; young et al., 2007; schatzmann et al, 2007), <papid> N07-2038 </papid>but they have not been used for context-sensitive asr before.</citsent>
<aftsection>
<nextsent>in our model, the systems belief?
</nextsent>
<nextsent>b(h) in recognition hypothesis is factored in two parts: the observation probability (o|h) (approximatedby the asr confidence score) and the user simulation probability (h|us,c) of the hypothesis: b(h) = (o|h).p (h|us,c) (1) where us is the state of the user simulation in context c. the context is simply window of di 505 alogue acts in the dialogue history, that the us is sensitive to (see section 3).
</nextsent>
<nextsent>the paper is organized as follows.
</nextsent>
<nextsent>after short relation to previous work, we describe the data (section 5) and derive baseline results (section 6).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4149">
<title id=" E09-1058.xml">user simulations for context sensitive speech recognition in spoken dialogue systems </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>hypothesis and to decide on appropriate system reactions.
</prevsent>
<prevsent>we evaluate this approach in comparison with baseline system that works in the standard way: always choosing the topmost hypothesis in the n-best list.in such systems, complex repair strategies are required when the top hypothesis is incorrect.the main novelty of this work is that we explore the use of predictions from simple statistical user simulations to re-rank n-best lists of asrhypotheses.
</prevsent>
</prevsection>
<citsent citstr=" N07-2038 ">
these user simulations are now commonly used in statistical learning approaches to dialogue management (williams and young, 2003; <papid> W03-2111 </papid>schatzmann et al, 2006; young, 2006; young et al., 2007; schatzmann et al, 2007), <papid> N07-2038 </papid>but they have not been used for context-sensitive asr before.</citsent>
<aftsection>
<nextsent>in our model, the systems belief?
</nextsent>
<nextsent>b(h) in recognition hypothesis is factored in two parts: the observation probability (o|h) (approximatedby the asr confidence score) and the user simulation probability (h|us,c) of the hypothesis: b(h) = (o|h).p (h|us,c) (1) where us is the state of the user simulation in context c. the context is simply window of di 505 alogue acts in the dialogue history, that the us is sensitive to (see section 3).
</nextsent>
<nextsent>the paper is organized as follows.
</nextsent>
<nextsent>after short relation to previous work, we describe the data (section 5) and derive baseline results (section 6).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4150">
<title id=" E09-1058.xml">user simulations for context sensitive speech recognition in spoken dialogue systems </title>
<section> relation to previous work.  </section>
<citcontext>
<prevsection>
<prevsent>in psycho linguistics, the idea that human dialogue participants simulate each other to some extent is gaining currency.
</prevsent>
<prevsent>(pickering and garrod, 2007) write:if overtly imitates a, then ascom prehension of bs utterance is facilitated by as memory for as previous utter ance.we explore aspects of this idea in computational manner.
</prevsent>
</prevsection>
<citsent citstr=" A00-2029 ">
similar work in the area of spoken dialogue systems is described below.(litman et al, 2000) <papid> A00-2029 </papid>use acoustic-prosodic information extracted from speech waveforms, together with information derived from their speech recognizer, to automatically predict mis recognized turns in corpus of train-timetable information dialogues.</citsent>
<aftsection>
<nextsent>in our experiments, we also use recognizer confidence scores and limited number of acoustic-prosodic features (e.g. amplitude in the speech signal) for hypothesis classification, but we also use user simulation predictions.(walker et al, 2000) use combination of features from the speech recognizer, natural language understanding, and dialogue manager/discourse history to classify hypotheses as correct, partially correct, or misrecognized.
</nextsent>
<nextsent>our work is related tothese experiments in that we also combine confidence scores and higher-level features for classification.
</nextsent>
<nextsent>however, both (litman et al, 2000) <papid> A00-2029 </papid>and (walker et al, 2000) consider only single-best recognition results and thus use their classifiers asfilters?</nextsent>
<nextsent>to decide whether the best recognition hypothesis for user utterance is correct or not.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4154">
<title id=" E09-1058.xml">user simulations for context sensitive speech recognition in spoken dialogue systems </title>
<section> relation to previous work.  </section>
<citcontext>
<prevsection>
<prevsent>wego step further in that we classify n-best hypotheses and then select among the alternatives.
</prevsent>
<prevsent>we also explore the use of more dialogue and task-orientedfeatures (e.g. the dialogue move type of recognition hypothesis) for classification.
</prevsent>
</prevsection>
<citsent citstr=" P04-1044 ">
(gabsdil and lemon, 2004) <papid> P04-1044 </papid>similarly perform reordering of n-best lists by combining acoustic and pragmatic features.</citsent>
<aftsection>
<nextsent>their study shows that dialogue features such as the previous system question and whether hypothesis is the correct answer to particular question contributed more to classification accuracy than the other attributes.(jonson, 2006) classifies recognition hypotheses with labels denoting acceptance, clarification, confirmation and rejection.
</nextsent>
<nextsent>these labels were learned in similar way to (gabsdil and lemon,2004) <papid> P04-1044 </papid>and correspond to varying levels of confidence, being essentially potential directives tothe dialogue manager.</nextsent>
<nextsent>apart from standard features jonson includes attributes that account for the whole n-best list, i.e. standard deviation of confidence scores.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4161">
<title id=" E09-1058.xml">user simulations for context sensitive speech recognition in spoken dialogue systems </title>
<section> data collection.  </section>
<citcontext>
<prevsection>
<prevsent>labels according to the following schema: ? opt: the hypothesis is perfectly aligned and semantically identical to the transcription ? pos: the hypothesis is not entirely aligned (wer   50) but is semantically identical to the transcription ? neg: the hypothesis is semantically identical to the transcription but does not align well (wer   50) or is semantically different to the transcription ? ign: the hypothesis was not addressed to the system (crosstalk), or the user laughed, coughed, etc. the 50% value for the wer as threshold forthe distinction between the pos?
</prevsent>
<prevsent>and neg?
</prevsent>
</prevsection>
<citsent citstr=" P03-2004 ">
category is adopted from (gabsdil, 2003), <papid> P03-2004 </papid>based on the fact that wer is affected by concept accuracy(boros et al, 1996).</citsent>
<aftsection>
<nextsent>in other words, if hypothesis is erroneous as far as its transcript is concerned 507 transcript: id like to find bar please would like to find bar please pos like to find four please neg id like to find bar please opt would like to find the or please ign table 1: example hypothesis labelling then it is highly likely that it does not convey the correct message from semantic point of view.we always label conceptually equivalent hypotheses to particular transcription as potential candidate dialogue strategy moves, and total mis recognitions as rejections.
</nextsent>
<nextsent>in table 5.1 we show examples of the four labels.
</nextsent>
<nextsent>note that in the case of silence, we give an opt?
</nextsent>
<nextsent>to the empty hypothesis.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4163">
<title id=" E12-2010.xml">a statistical spoken dialogue system using complex user goals and value directed compression </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>work in dialogue system evaluation, e.g. walker et al(2004) and lemon et al(2006), shows that real user goals are generally sets of items, rather than single item.
</prevsent>
<prevsent>people like to explore possible tradeoffs between the attributes of items.
</prevsent>
</prevsection>
<citsent citstr=" W10-4336 ">
crook and lemon (2010) <papid> W10-4336 </papid>identified this as central challenge for the field of spoken dialogue systems, proposing the use of automatic compression techniques to allow for extended accurate representations of user goals.</citsent>
<aftsection>
<nextsent>this paper presents proof of concept of these ideas in the form of complete, working spoken dialogue system.
</nextsent>
<nextsent>thepomdp dialogue manager (dm) of this demonstration system uses compressed belief space that was generated using modified version of the value directed compression (vdc) algorithm as originally proposed by poupart (2005).
</nextsent>
<nextsent>this demonstration system extends work presented by crook and lemon (2011) in that it embeds the compressed complex user goal belief space into working system and demonstrates planning (and acting) in the compressed space.
</nextsent>
<nextsent>the type of sds task that we focus on is limited domain query-dialogue, also known as slot filling?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4167">
<title id=" E12-1038.xml">inferring selectional preferences from partofspeech ngrams </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we call the target?
</prevsent>
<prevsent>because originally it referred to vocabulary word targeted for instruction, and its relative.?
</prevsent>
</prevsection>
<citsent citstr=" W97-0209 ">
notation description a relation between words a target word r,  possible relatives of g word n-gram gi and gj th and jth words of p the pos n-gram of table 1: notation used throughout this paper previous work on selectional preferences has used them primarily for natural language analytic tasks such as word sense disambiguation (resnik, 1997), <papid> W97-0209 </papid>dependency parsing (zhou et al 2011), <papid> P11-1156 </papid>and semantic role labeling (gildea and jurafsky, 2002).<papid> J02-3001 </papid></citsent>
<aftsection>
<nextsent>however, selectional preferences can also apply to natural language generation tasks such as sentence generation and question generation.
</nextsent>
<nextsent>for generation tasks, choosing the right word to express specified argument of relation requires knowing its connotations ? that is, its selectional preferences.
</nextsent>
<nextsent>therefore, it is useful to know selectional preferences for many different relations.
</nextsent>
<nextsent>such knowledge could have many uses.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4168">
<title id=" E12-1038.xml">inferring selectional preferences from partofspeech ngrams </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we call the target?
</prevsent>
<prevsent>because originally it referred to vocabulary word targeted for instruction, and its relative.?
</prevsent>
</prevsection>
<citsent citstr=" P11-1156 ">
notation description a relation between words a target word r,  possible relatives of g word n-gram gi and gj th and jth words of p the pos n-gram of table 1: notation used throughout this paper previous work on selectional preferences has used them primarily for natural language analytic tasks such as word sense disambiguation (resnik, 1997), <papid> W97-0209 </papid>dependency parsing (zhou et al 2011), <papid> P11-1156 </papid>and semantic role labeling (gildea and jurafsky, 2002).<papid> J02-3001 </papid></citsent>
<aftsection>
<nextsent>however, selectional preferences can also apply to natural language generation tasks such as sentence generation and question generation.
</nextsent>
<nextsent>for generation tasks, choosing the right word to express specified argument of relation requires knowing its connotations ? that is, its selectional preferences.
</nextsent>
<nextsent>therefore, it is useful to know selectional preferences for many different relations.
</nextsent>
<nextsent>such knowledge could have many uses.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4169">
<title id=" E12-1038.xml">inferring selectional preferences from partofspeech ngrams </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we call the target?
</prevsent>
<prevsent>because originally it referred to vocabulary word targeted for instruction, and its relative.?
</prevsent>
</prevsection>
<citsent citstr=" J02-3001 ">
notation description a relation between words a target word r,  possible relatives of g word n-gram gi and gj th and jth words of p the pos n-gram of table 1: notation used throughout this paper previous work on selectional preferences has used them primarily for natural language analytic tasks such as word sense disambiguation (resnik, 1997), <papid> W97-0209 </papid>dependency parsing (zhou et al 2011), <papid> P11-1156 </papid>and semantic role labeling (gildea and jurafsky, 2002).<papid> J02-3001 </papid></citsent>
<aftsection>
<nextsent>however, selectional preferences can also apply to natural language generation tasks such as sentence generation and question generation.
</nextsent>
<nextsent>for generation tasks, choosing the right word to express specified argument of relation requires knowing its connotations ? that is, its selectional preferences.
</nextsent>
<nextsent>therefore, it is useful to know selectional preferences for many different relations.
</nextsent>
<nextsent>such knowledge could have many uses.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4170">
<title id=" E12-1038.xml">inferring selectional preferences from partofspeech ngrams </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in machine translation, they could help generate more natural wording.
</prevsent>
<prevsent>this paper introduces method named pong (for part-of-speech n-grams) to compute selectional preferences for many different relations by combining part-of-speech information and google n-grams.
</prevsent>
</prevsection>
<citsent citstr=" J10-4007 ">
pong achieves higher precision on pseudo 377 disambiguation task than the best previous model (erk et al 2010), <papid> J10-4007 </papid>but lower coverage.</citsent>
<aftsection>
<nextsent>the paper is organized as follows.
</nextsent>
<nextsent>section 2 describes the relations for which we compute selectional preferences.
</nextsent>
<nextsent>section 3 describes pong.
</nextsent>
<nextsent>section 4 evaluates pong.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4173">
<title id=" E12-1038.xml">inferring selectional preferences from partofspeech ngrams </title>
<section> method.  </section>
<citcontext>
<prevsection>
<prevsent>therefore it cannot (and does not) compare them against fixed threshold to make decisions about selectional preferences.
</prevsent>
<prevsent>3.2 mapping pos n-grams to relations.
</prevsent>
</prevsection>
<citsent citstr=" P03-1054 ">
to estimate pr(r | p, i, j), we use the penn treebank wall street journal (wsj) corpus, which is labeled with grammatical relations using the stanford dependency parser (klein and manning, 2003).<papid> P03-1054 </papid></citsent>
<aftsection>
<nextsent>to estimate the probability pr(r | p, i, j) of relation between target at position and relative at position in pos n-gram p, we compute what fraction of the word n-grams with pos n-gram have relation between some target and relative at positions and j: pr( | , , ) freq( . .pos( ) relation( , ) ) freq( . .pos( ) relation( , )) j j p j s g g r s g g 3.3 estimating pos n-gram distributions.
</nextsent>
<nextsent>given target and relative, we need to estimate their distribution of pos n-grams and positions.
</nextsent>
<nextsent>379 figure 1: overview of pong.
</nextsent>
<nextsent>from the labeled corpus, pong extracts abstract mappings from pos n-grams to relations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4174">
<title id=" E12-1038.xml">inferring selectional preferences from partofspeech ngrams </title>
<section> method.  </section>
<citcontext>
<prevsection>
<prevsent>a labeled corpus is too sparse for this purpose, so we use the much larger unlabeled google grams corpus (franz and brants, 2006).
</prevsent>
<prevsent>the probability that an n-gram with target at position and relative at position will have the pos n-gram is: pr( | , ) freq( . .pos( ) , , )) freq( . . ) j j j t r g t p t r s g g rto compute this ratio, we first use well indexed table to efficiently retrieve all n-grams with words and at the specified positions.
</prevsent>
</prevsection>
<citsent citstr=" N03-1033 ">
we then obtain their pos n-grams from the stanford pos tagger (toutanova et al 2003), <papid> N03-1033 </papid>and count how many of them have the pos n-gram p. 3.4 reducing pos n-gram sparseness.</citsent>
<aftsection>
<nextsent>we abstract word n-grams into pos n-grams to address the sparseness of the labeled corpus, but even the pos n-grams can be sparse.
</nextsent>
<nextsent>for n=5, the rarer ones occur too sparsely (if at all) in our labeled corpus to estimate their frequency.
</nextsent>
<nextsent>to address this issue, we use coarser pos tag set than the penn treebank pos tag set.
</nextsent>
<nextsent>as table 2 shows, we merge tags for adjectives, nouns, adverbs, and verbs into four coarser tags.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4175">
<title id=" E12-1038.xml">inferring selectional preferences from partofspeech ngrams </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>we used the other half of bnc as training corpus for the baseline methods we compared pong to.
</prevsent>
<prevsent>a test set for the pseudo-disambiguation task task consists of tuples of the form (r, t, r, ).
</prevsent>
</prevsection>
<citsent citstr=" P99-1014 ">
to construct test set, we adapted the process used by rooth et al(1999) <papid> P99-1014 </papid>and erk et al(2010).<papid> J10-4007 </papid></citsent>
<aftsection>
<nextsent>first, we chose 100 (r, t) pairs for each relation at random from the test corpus.
</nextsent>
<nextsent>rooth et al(1999) <papid> P99-1014 </papid>and erk et al(2010) <papid> J10-4007 </papid>chose such pairs from training corpus to ensure that it contained the target t. in contrast, choosing pairs from an unseen test corpus includes target words whether or not they occur in the training corpus.</nextsent>
<nextsent>to obtain sample stratified by frequency, rather than skewed heavily toward high frequency pairs, erk et al(2010) <papid> J10-4007 </papid>drew (r, t) pairs from each of five frequency bands in the entire british national corpus (bnc): 50-100 occurrences; 101-200; 201-500; 500-1000; and more than 1000.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4219">
<title id=" E12-1038.xml">inferring selectional preferences from partofspeech ngrams </title>
<section> relation to prior work.  </section>
<citcontext>
<prevsection>
<prevsent>ritter (2010) computed selectional preferences by using unsupervised topic models such as linklda, which infers semantic classes of words automatically instead of requiring predefined set of classes as input.
</prevsent>
<prevsent>the contexts in which linguistic unit occurs provide information about its meaning.
</prevsent>
</prevsection>
<citsent citstr=" P07-1028 ">
erk (2007) <papid> P07-1028 </papid>and erk et al(2010) <papid> J10-4007 </papid>modeled the contexts of word as the distribution of words that co-occur with it.</citsent>
<aftsection>
<nextsent>they calculated the semantic similarity of two words as the similarity of their context distributions according to various measures.
</nextsent>
<nextsent>erk et al(2010) <papid> J10-4007 </papid>reported the state-of the-art method we used as our epp baseline.</nextsent>
<nextsent>in contrast to prior work that explored various solutions to the generalization problem, we dont so much solve this problem as circumvent it.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4239">
<title id=" E12-1012.xml">adaptation of statistical machine translation model for cross lingual information retrieval in a service context </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the standard query translation methods use different translation resources such as bilingual dictionaries, parallel corpora and/or machine translation.
</prevsent>
<prevsent>the aspect of disambiguation is important for the first two techniques.
</prevsent>
</prevsection>
<citsent citstr=" J03-3003 ">
different methods were proposed to deal with disambiguation issues, often relying on the document collection or embedding the translation step directly into the retrieval model (hiemstra and jong, 1999; berger et al 1999; kraaij et al 2003).<papid> J03-3003 </papid></citsent>
<aftsection>
<nextsent>other methods relyon external resources like query logs (gao et al 2010), wikipedia (ja didi nejad and mahmoudi, 2009) or the web (nie and chen, 2002; hu et al 2008).
</nextsent>
<nextsent>(gao et al 2006) proposes syntax-based translation models to deal with the disambiguation issues (np-based, dependency-based).
</nextsent>
<nextsent>the candidate translations proposed by these models are then reranked withthe model learned to minimize the translation er 2standard mt evaluation metric ror on the training data.to our knowledge, existing work that use mtbased techniques for query translation use an out of-the-box mt system, without adapting it for query translation in particular (jones et al 1999;wu et al 2008) (<papid> C08-1125 </papid>although some query expansion techniques might be applied to the produced translation afterwards (wu and he, 2010)).there is number of works done for domain adaptation in statistical machine transla tion.</nextsent>
<nextsent>however, we want to distinguish between genre and domain adaptation in this work.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4240">
<title id=" E12-1012.xml">adaptation of statistical machine translation model for cross lingual information retrieval in a service context </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>other methods relyon external resources like query logs (gao et al 2010), wikipedia (ja didi nejad and mahmoudi, 2009) or the web (nie and chen, 2002; hu et al 2008).
</prevsent>
<prevsent>(gao et al 2006) proposes syntax-based translation models to deal with the disambiguation issues (np-based, dependency-based).
</prevsent>
</prevsection>
<citsent citstr=" C08-1125 ">
the candidate translations proposed by these models are then reranked withthe model learned to minimize the translation er 2standard mt evaluation metric ror on the training data.to our knowledge, existing work that use mtbased techniques for query translation use an out of-the-box mt system, without adapting it for query translation in particular (jones et al 1999;wu et al 2008) (<papid> C08-1125 </papid>although some query expansion techniques might be applied to the produced translation afterwards (wu and he, 2010)).there is number of works done for domain adaptation in statistical machine transla tion.</citsent>
<aftsection>
<nextsent>however, we want to distinguish between genre and domain adaptation in this work.
</nextsent>
<nextsent>generally, genre can be seen as sub-problem of do main.
</nextsent>
<nextsent>thus, we consider genre to be the general style of the text e.g. conversation, news, blog, query (responsible mostly for the text structure) while the domain reflects more what the text is about ? eg.
</nextsent>
<nextsent>social science, healthcare, history, so domain adaptation involves lexical disambiguation and extra lexical coverage problems.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4241">
<title id=" E12-1012.xml">adaptation of statistical machine translation model for cross lingual information retrieval in a service context </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>social science, healthcare, history, so domain adaptation involves lexical disambiguation and extra lexical coverage problems.
</prevsent>
<prevsent>to our knowledge, there is not much work addressing explicitly the problem of genre adaptation for smt.
</prevsent>
</prevsection>
<citsent citstr=" W09-0432 ">
some work done on domain adaptation could be applied to genre adaptation, such as incorporating available in-domain corpora in the smt model: either monolingual (bertoldi and federico, 2009; <papid> W09-0432 </papid>wu et al 2008; <papid> C08-1125 </papid>zhao et al 2004; <papid> C04-1059 </papid>koehn and schroeder, 2007), <papid> W07-0733 </papid>or small parallel data used for tuning the smt parameters (zheng et al 2010; pecina et al 2011).</citsent>
<aftsection>
<nextsent>this work is based on the hypothesis that the general-purpose smt system needs to be adapted for query translation.
</nextsent>
<nextsent>although in (ferro and peters, 2009) it has been mentioned that using google translate (general-purpose mt) for query translation allowed to clef participants to obtain the best clir performance, there is still 10% gap between monolingual and cross-lingual ir.
</nextsent>
<nextsent>we believe that, as in (clinchant and renders, 2007), more adapted query translation, possibly further combined with query expansion techniques, can lead to improved retrieval.the problem of the smt adaptation for query genre translation has different quality aspects.on the one hand, we want our model to produce good?
</nextsent>
<nextsent>translation (well-formed and transmitting the information contained in the source query) of an input query.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4243">
<title id=" E12-1012.xml">adaptation of statistical machine translation model for cross lingual information retrieval in a service context </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>social science, healthcare, history, so domain adaptation involves lexical disambiguation and extra lexical coverage problems.
</prevsent>
<prevsent>to our knowledge, there is not much work addressing explicitly the problem of genre adaptation for smt.
</prevsent>
</prevsection>
<citsent citstr=" C04-1059 ">
some work done on domain adaptation could be applied to genre adaptation, such as incorporating available in-domain corpora in the smt model: either monolingual (bertoldi and federico, 2009; <papid> W09-0432 </papid>wu et al 2008; <papid> C08-1125 </papid>zhao et al 2004; <papid> C04-1059 </papid>koehn and schroeder, 2007), <papid> W07-0733 </papid>or small parallel data used for tuning the smt parameters (zheng et al 2010; pecina et al 2011).</citsent>
<aftsection>
<nextsent>this work is based on the hypothesis that the general-purpose smt system needs to be adapted for query translation.
</nextsent>
<nextsent>although in (ferro and peters, 2009) it has been mentioned that using google translate (general-purpose mt) for query translation allowed to clef participants to obtain the best clir performance, there is still 10% gap between monolingual and cross-lingual ir.
</nextsent>
<nextsent>we believe that, as in (clinchant and renders, 2007), more adapted query translation, possibly further combined with query expansion techniques, can lead to improved retrieval.the problem of the smt adaptation for query genre translation has different quality aspects.on the one hand, we want our model to produce good?
</nextsent>
<nextsent>translation (well-formed and transmitting the information contained in the source query) of an input query.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4244">
<title id=" E12-1012.xml">adaptation of statistical machine translation model for cross lingual information retrieval in a service context </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>social science, healthcare, history, so domain adaptation involves lexical disambiguation and extra lexical coverage problems.
</prevsent>
<prevsent>to our knowledge, there is not much work addressing explicitly the problem of genre adaptation for smt.
</prevsent>
</prevsection>
<citsent citstr=" W07-0733 ">
some work done on domain adaptation could be applied to genre adaptation, such as incorporating available in-domain corpora in the smt model: either monolingual (bertoldi and federico, 2009; <papid> W09-0432 </papid>wu et al 2008; <papid> C08-1125 </papid>zhao et al 2004; <papid> C04-1059 </papid>koehn and schroeder, 2007), <papid> W07-0733 </papid>or small parallel data used for tuning the smt parameters (zheng et al 2010; pecina et al 2011).</citsent>
<aftsection>
<nextsent>this work is based on the hypothesis that the general-purpose smt system needs to be adapted for query translation.
</nextsent>
<nextsent>although in (ferro and peters, 2009) it has been mentioned that using google translate (general-purpose mt) for query translation allowed to clef participants to obtain the best clir performance, there is still 10% gap between monolingual and cross-lingual ir.
</nextsent>
<nextsent>we believe that, as in (clinchant and renders, 2007), more adapted query translation, possibly further combined with query expansion techniques, can lead to improved retrieval.the problem of the smt adaptation for query genre translation has different quality aspects.on the one hand, we want our model to produce good?
</nextsent>
<nextsent>translation (well-formed and transmitting the information contained in the source query) of an input query.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4245">
<title id=" E12-1012.xml">adaptation of statistical machine translation model for cross lingual information retrieval in a service context </title>
<section> our approach.  </section>
<citcontext>
<prevsection>
<prevsent>the first one optimizes bleu, while the second one optimizes mean average precision (map), standard metric in information retrieval.
</prevsent>
<prevsent>well address the issue of the correlation between bleu and map in section 4.
</prevsent>
</prevsection>
<citsent citstr=" P07-2045 ">
both of the proposed approaches relyon the phrase-based smt (pbmt) model (koehn et al 2003) implemented in the open source smt toolkit moses (koehn et al 2007).<papid> P07-2045 </papid></citsent>
<aftsection>
<nextsent>3.1 tuning for genre adaptation.
</nextsent>
<nextsent>first, we propose to adapt the pbmt model by tuning the models weights on parallel set ofqueries.
</nextsent>
<nextsent>this approach addresses the first aspect of the problem, which is producing goodtranslation.
</nextsent>
<nextsent>the pbmt model combines different types of features via log-linear model.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4246">
<title id=" E12-1012.xml">adaptation of statistical machine translation model for cross lingual information retrieval in a service context </title>
<section> our approach.  </section>
<citcontext>
<prevsection>
<prevsent>this approach addresses the first aspect of the problem, which is producing goodtranslation.
</prevsent>
<prevsent>the pbmt model combines different types of features via log-linear model.
</prevsent>
</prevsection>
<citsent citstr=" P03-1021 ">
the standard features include (koehn, 2010, chapter5): language model, word penalty, distortion, different translation models, etc. the weights of these features are learned during the tuning step with the mert (och, 2003) <papid> P03-1021 </papid>algorithm.</citsent>
<aftsection>
<nextsent>roughly the mert algorithm tunes feature weights one by one and optimizes them according to the bleu score obtained.
</nextsent>
<nextsent>our hypothesis is that the impact of different features should be different depending on whether we translate full sentence, or query-genre entry.
</nextsent>
<nextsent>thus, one would expect that in the caseof query-genre the language model or the distortion features should get less importance than in the case of the full-sentence translation.
</nextsent>
<nextsent>mert tuning on genre-adapted parallel corpus should leverage this information from the data, adapting the smt model to the query-genre.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4247">
<title id=" E12-1012.xml">adaptation of statistical machine translation model for cross lingual information retrieval in a service context </title>
<section> our approach.  </section>
<citcontext>
<prevsection>
<prevsent>thus, for example, the word order which is crucial for translation quality (and istaken into account by most mt evaluation met rics) is often ignored by ir models.
</prevsent>
<prevsent>our second approach follows (nie, 2010, pp.106) argument that the translation problem is an integral part of the whole clir problem, and unified clir models integrating translation should be defined?.
</prevsent>
</prevsection>
<citsent citstr=" W08-0407 ">
we propose integrating their metric (map) into the translation model optimisation step via the reranking framework.previous attempts to apply the reranking approach to smt did not show significant improvements in terms of mt evaluation metrics (och et al 2003; nikoulina and dymetman, 2008).<papid> W08-0407 </papid></citsent>
<aftsection>
<nextsent>one of the reasons being the poor diversity of then best list of the translations.
</nextsent>
<nextsent>however, we be 111 lieve that this approach has more potential in the context of query translation.
</nextsent>
<nextsent>first of all the average query length is 5 words, which means that the nbest list of the translations is more diverse than in the case of general phrase translation (average length 25-30 words).moreover, the retrieval precision is more naturally integrated into the reranking framework than standard mt evaluation metrics such as bleu.the main reason is that the notion of average retrieval precision is well defined for single query translation, while bleu is defined on the corpus level and correlates poorly with human quality judgements for the individual translations (specia et al 2009; callison-burch et al 2009).
</nextsent>
<nextsent>finally, the reranking framework allows lotof flexibility.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4248">
<title id=" E12-1012.xml">adaptation of statistical machine translation model for cross lingual information retrieval in a service context </title>
<section> our approach.  </section>
<citcontext>
<prevsection>
<prevsent>finally, the reranking framework allows lotof flexibility.
</prevsent>
<prevsent>thus, it allows enriching the base line translation model with new complex features which might be difficult to introduce into the translation model directly.
</prevsent>
</prevsection>
<citsent citstr=" P04-1015 ">
other works applied the reranking framework to different nlp tasks such as named entities extraction (collins, 2001), parsing (collins and roark, 2004), <papid> P04-1015 </papid>and language modelling (roark et al., 2004).<papid> P04-1007 </papid></citsent>
<aftsection>
<nextsent>most of these works used the reranking framework to combine generative and discriminative methods when both approaches aim at solving the same problem: the generative model produces set of hypotheses, and the best hypothesis is chosen afterwards via the discriminative reranking model, which allows to enrich the baseline model with the new complex and heterogeneous features.
</nextsent>
<nextsent>we suggest using the reranking framework to combine two different tasks: machine translation and cross-lingual information retrieval.
</nextsent>
<nextsent>in this context the reranking framework doesnt only allow enriching the baseline translation model but also performing training using more appropriate evaluation metric.
</nextsent>
<nextsent>3.2.1 reranking training generally, the reranking framework can be resumed in the following steps : 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4249">
<title id=" E12-1012.xml">adaptation of statistical machine translation model for cross lingual information retrieval in a service context </title>
<section> our approach.  </section>
<citcontext>
<prevsection>
<prevsent>finally, the reranking framework allows lotof flexibility.
</prevsent>
<prevsent>thus, it allows enriching the base line translation model with new complex features which might be difficult to introduce into the translation model directly.
</prevsent>
</prevsection>
<citsent citstr=" P04-1007 ">
other works applied the reranking framework to different nlp tasks such as named entities extraction (collins, 2001), parsing (collins and roark, 2004), <papid> P04-1015 </papid>and language modelling (roark et al., 2004).<papid> P04-1007 </papid></citsent>
<aftsection>
<nextsent>most of these works used the reranking framework to combine generative and discriminative methods when both approaches aim at solving the same problem: the generative model produces set of hypotheses, and the best hypothesis is chosen afterwards via the discriminative reranking model, which allows to enrich the baseline model with the new complex and heterogeneous features.
</nextsent>
<nextsent>we suggest using the reranking framework to combine two different tasks: machine translation and cross-lingual information retrieval.
</nextsent>
<nextsent>in this context the reranking framework doesnt only allow enriching the baseline translation model but also performing training using more appropriate evaluation metric.
</nextsent>
<nextsent>3.2.1 reranking training generally, the reranking framework can be resumed in the following steps : 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4250">
<title id=" E12-1012.xml">adaptation of statistical machine translation model for cross lingual information retrieval in a service context </title>
<section> our approach.  </section>
<citcontext>
<prevsection>
<prevsent>the annotated training set is created out of queries {q1, ..., qk} with an nbest list of translations gen(qi) of each query qi, ? {1..k} as follows: ? list of (we take = 1000) translations (gen(qi)) is produced by the baseline mt model for each query qi, = 1..k. ? each translation ? gen(qi) is usedto perform retrieval from target document collection, and an average precision score (ap (t)) is computed for each ? gen(qi) by comparing its retrieval to the relevance annotations done during the clef campaign.
</prevsent>
<prevsent>the weights ? are learned with the objective of maximizing map for all the queries of the training set, and, therefore, are optimized for retrieval quality.
</prevsent>
</prevsection>
<citsent citstr=" D07-1080 ">
the weights optimization is done with the margin infused relaxed algorithm (mira)(crammer and singer, 2003), which was applied to smt by (watanabe et al 2007; <papid> D07-1080 </papid>chiang et al 2008).<papid> D08-1024 </papid></citsent>
<aftsection>
<nextsent>mira is an online learning algorithm where each weights update is done to keep the new weights as close as possible to theold weights (first term), and score oracle translation (the translation giving the best retrieval score : ti = argmaxtap (t)) higher than each non-oracle translation (tij) by margin at least as wide as the loss lij (second term): ? = min??
</nextsent>
<nextsent>1 2??
</nextsent>
<nextsent>2 + k i=1 maxj=1..n ( lij ? ?
</nextsent>
<nextsent>(f (ti )?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4251">
<title id=" E12-1012.xml">adaptation of statistical machine translation model for cross lingual information retrieval in a service context </title>
<section> our approach.  </section>
<citcontext>
<prevsection>
<prevsent>the annotated training set is created out of queries {q1, ..., qk} with an nbest list of translations gen(qi) of each query qi, ? {1..k} as follows: ? list of (we take = 1000) translations (gen(qi)) is produced by the baseline mt model for each query qi, = 1..k. ? each translation ? gen(qi) is usedto perform retrieval from target document collection, and an average precision score (ap (t)) is computed for each ? gen(qi) by comparing its retrieval to the relevance annotations done during the clef campaign.
</prevsent>
<prevsent>the weights ? are learned with the objective of maximizing map for all the queries of the training set, and, therefore, are optimized for retrieval quality.
</prevsent>
</prevsection>
<citsent citstr=" D08-1024 ">
the weights optimization is done with the margin infused relaxed algorithm (mira)(crammer and singer, 2003), which was applied to smt by (watanabe et al 2007; <papid> D07-1080 </papid>chiang et al 2008).<papid> D08-1024 </papid></citsent>
<aftsection>
<nextsent>mira is an online learning algorithm where each weights update is done to keep the new weights as close as possible to theold weights (first term), and score oracle translation (the translation giving the best retrieval score : ti = argmaxtap (t)) higher than each non-oracle translation (tij) by margin at least as wide as the loss lij (second term): ? = min??
</nextsent>
<nextsent>1 2??
</nextsent>
<nextsent>2 + k i=1 maxj=1..n ( lij ? ?
</nextsent>
<nextsent>(f (ti )?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4254">
<title id=" E12-1012.xml">adaptation of statistical machine translation model for cross lingual information retrieval in a service context </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>4.1.2 baseline we tested our approaches on the clef adhoc tel 2009 task (50 topics).
</prevsent>
<prevsent>this task dealt with monolingual and cross-lingual search in library catalog.
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
the monolingual retrieval is 3this alignment can be either produced by toolkit likegiza++(och and ney, 2003) <papid> J03-1002 </papid>or obtained directly by system that produced the nbest list of the translations (moses).</citsent>
<aftsection>
<nextsent>113 language pair number of queries total queries en - fr, fr - en 470 en - de, de - en 714 annotated queries en - fr, fr - en 400 en - de, de - en 350 table 1: top: total number of parallel queries gathered from all the clef tasks (size of the tuning set).
</nextsent>
<nextsent>bot tom: number of queries extracted from the tasks for which the human relevance judgements were avail ble (size of the reranking training set).
</nextsent>
<nextsent>performed with the lemur4 toolkit (ogilvie and callan, 2001).
</nextsent>
<nextsent>the preprocessing includes lemmatisation (with the xerox incremental parser xip (at-mokhtar et al 2002)) and filtering out the function words (based on xip pos tagging).table 2 shows the performance of the monolingual retrieval model for each collection.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4255">
<title id=" E09-1068.xml">using cycles and quasi cycles to disambiguate dictionary glosses </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>moreover, while computational lexicons like wordnet contain semantically explicit information such as, among others, hypernymy and meronymy relations, most thesauri, glossaries, andmachine-readable dictionaries are often just electronic transcriptions of their paper counterparts.
</prevsent>
<prevsent>as result, for each entry (e.g. word sense or thesaurus entry) they mostly provide implicit information in the form of free text.
</prevsent>
</prevsection>
<citsent citstr=" W06-1663 ">
the production of semantically richer lexical resources can help alleviate the knowledge acquisition bottleneck and potentially enable advanced natural language processing applications (cuadros and rigau, 2006).<papid> W06-1663 </papid></citsent>
<aftsection>
<nextsent>however, in order to reduce the high cost of manual annotation (ed monds, 2000), and to avoid the repetition of this effort for each knowledge resource, this task mustbe supported by wide-coverage automated techniques which do not relyon the specific resource at hand.
</nextsent>
<nextsent>in this paper, we aim to make explicit large quantities of semantic information implicitly contained in the glosses of existing wide coverage lexical knowledge resources (specifically, machine-readable dictionaries and computational lexicons).
</nextsent>
<nextsent>to this end, we present method for gloss word sense disambiguation (wsd),called the cycles and quasi-cycles (cqc) algorithm.
</nextsent>
<nextsent>the algorithm is based on novel notion of cycles in the dictionary graph (possibly withone edge reversed) which support disambiguation choice.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4256">
<title id=" E09-1068.xml">using cycles and quasi cycles to disambiguate dictionary glosses </title>
<section> all cqc :=.  </section>
<citcontext>
<prevsection>
<prevsent>wordnet.
</prevsent>
<prevsent>when using wordnet as reference resource, given sense whose gloss we aim to disambiguate, the dictionary graph includes not only edges connecting to senses of gloss words (step (iii) of the graph construction procedure, cf.
</prevsent>
</prevsection>
<citsent citstr=" W04-0804 ">
section 2.1), but also those obtained from any of the wordnet lexico-semantics relations (step (ii)).for wordnet gloss disambiguation, we employed the dataset used in the senseval-3 gloss wsd task (litkowski, 2004), <papid> W04-0804 </papid>which contains 15,179 content words from 9,257 glosses5.</citsent>
<aftsection>
<nextsent>we compared the performance of cqc, cycles, lesk, and the two baselines.
</nextsent>
<nextsent>to get full coverage and high performance, we learned threshold for each system below which they recur to the fs heuristic.
</nextsent>
<nextsent>the threshold and maximum path length were tuned on small in-house manually-annotated dataset of 100 glosses.
</nextsent>
<nextsent>the results are shown intable 2.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4257">
<title id=" E09-1068.xml">using cycles and quasi cycles to disambiguate dictionary glosses </title>
<section> all cqc :=.  </section>
<citcontext>
<prevsection>
<prevsent>algorithm prec./recall cqc 64.25 cycles 63.74 lesk 51.75 talp 68.60/68.30 fs bl 55.44 random bl 26.29 table 2: gloss wsd performance on wordnet.
</prevsent>
<prevsent>3 gloss wsd task, namely the talp system.
</prevsent>
</prevsection>
<citsent citstr=" W04-0823 ">
(castillo et al, 2004).<papid> W04-0823 </papid>cqc outperforms all other proposed approaches, obtaining 64.25% precision and recall.</citsent>
<aftsection>
<nextsent>we note that cycles also gets high performance,compared to lesk and the baselines.
</nextsent>
<nextsent>also, compared to cqc, the difference is not statistically significant.
</nextsent>
<nextsent>however, we observe that, if we do not recur to the first sense as backoff strategy, we get much lower recall for cycles (p = 65.39, = 26.70 for cqc, = 72.03, = 16.39 for cycles).
</nextsent>
<nextsent>cqc performs about 4 points below the talpsystem.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4258">
<title id=" E09-1068.xml">using cycles and quasi cycles to disambiguate dictionary glosses </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>however, in this paper we focused ona specific kind of wsd, namely the disambiguation of dictionary definitions.
</prevsent>
<prevsent>seminal works onthe topic date back to the late 1970s, with the development of models for the identification of tax onomies from lexical resources (litkowski, 1978; amsler, 1980).
</prevsent>
</prevsection>
<citsent citstr=" P85-1037 ">
subsequent works focused on the identification of genus terms (chodorow et al, 1985) <papid> P85-1037 </papid>and, more in general, on the extraction of explicit information from machine-readable dictionaries (see, e.g., (nakamura and nagao, 1988; <papid> C88-2098 </papid>ide and veronis, 1993)).</citsent>
<aftsection>
<nextsent>kozima and furugori (1993) provide an approach to the construction of ambiguous semantic networks from glosses in the longman dictionary of contemporary english (ldoce).
</nextsent>
<nextsent>in this direction, it is worth citing the work of vanderwende (1996) and richardson etal.
</nextsent>
<nextsent>(1998), who describe the construction of mind net, lexical knowledge base obtained from the automated extraction of lexico-semantic information from two machine-readable dictionaries.
</nextsent>
<nextsent>as aresult, weighted relation paths are produced to infer the semantic similarity between pairs of words.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4259">
<title id=" E09-1068.xml">using cycles and quasi cycles to disambiguate dictionary glosses </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>however, in this paper we focused ona specific kind of wsd, namely the disambiguation of dictionary definitions.
</prevsent>
<prevsent>seminal works onthe topic date back to the late 1970s, with the development of models for the identification of tax onomies from lexical resources (litkowski, 1978; amsler, 1980).
</prevsent>
</prevsection>
<citsent citstr=" C88-2098 ">
subsequent works focused on the identification of genus terms (chodorow et al, 1985) <papid> P85-1037 </papid>and, more in general, on the extraction of explicit information from machine-readable dictionaries (see, e.g., (nakamura and nagao, 1988; <papid> C88-2098 </papid>ide and veronis, 1993)).</citsent>
<aftsection>
<nextsent>kozima and furugori (1993) provide an approach to the construction of ambiguous semantic networks from glosses in the longman dictionary of contemporary english (ldoce).
</nextsent>
<nextsent>in this direction, it is worth citing the work of vanderwende (1996) and richardson etal.
</nextsent>
<nextsent>(1998), who describe the construction of mind net, lexical knowledge base obtained from the automated extraction of lexico-semantic information from two machine-readable dictionaries.
</nextsent>
<nextsent>as aresult, weighted relation paths are produced to infer the semantic similarity between pairs of words.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4260">
<title id=" E09-1068.xml">using cycles and quasi cycles to disambiguate dictionary glosses </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>(1998), who describe the construction of mind net, lexical knowledge base obtained from the automated extraction of lexico-semantic information from two machine-readable dictionaries.
</prevsent>
<prevsent>as aresult, weighted relation paths are produced to infer the semantic similarity between pairs of words.
</prevsent>
</prevsection>
<citsent citstr=" P97-1007 ">
several heuristics have been presented for the disambiguation of the genus of dictionary definition (wilks et al, 1996; rigau et al, 1997).<papid> P97-1007 </papid></citsent>
<aftsection>
<nextsent>more recently, set of heuristic techniques has been proposed to semantically annotate wordnet glosses, leading to the release of the extended wordnet (harabagiu et al, 1999; <papid> W99-0501 </papid>moldovan and novischi, 2004).</nextsent>
<nextsent>among the methods, the cross reference heuristic is the closest technique to our notion of cycles and quasi-cycles.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4261">
<title id=" E09-1068.xml">using cycles and quasi cycles to disambiguate dictionary glosses </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>as aresult, weighted relation paths are produced to infer the semantic similarity between pairs of words.
</prevsent>
<prevsent>several heuristics have been presented for the disambiguation of the genus of dictionary definition (wilks et al, 1996; rigau et al, 1997).<papid> P97-1007 </papid></prevsent>
</prevsection>
<citsent citstr=" W99-0501 ">
more recently, set of heuristic techniques has been proposed to semantically annotate wordnet glosses, leading to the release of the extended wordnet (harabagiu et al, 1999; <papid> W99-0501 </papid>moldovan and novischi, 2004).</citsent>
<aftsection>
<nextsent>among the methods, the cross reference heuristic is the closest technique to our notion of cycles and quasi-cycles.
</nextsent>
<nextsent>given pair of words and w?, this heuristic is based on the occurrence of 600 in the gloss of sense s?
</nextsent>
<nextsent>of w?
</nextsent>
<nextsent>and, vice versa, ofw?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4264">
<title id=" E09-1068.xml">using cycles and quasi cycles to disambiguate dictionary glosses </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in contrast, the approach presented in this paper performs the disambiguation of ambiguous words by exploiting only the reference dictionary itself.
</prevsent>
<prevsent>furthermore, as we showed in section 3.3, our method does not relyon wordnet, and can be applied to any lexical knowledge resource, including bilingual dictionaries.
</prevsent>
</prevsection>
<citsent citstr=" P06-1101 ">
finally, methods in the literature more focused on specific disambiguation task include statistical methods for the attachment of hyponyms under the most likely hypernym in the wordnet taxonomy (snow et al, 2006), <papid> P06-1101 </papid>structural approaches based on semantic clusters and distance metrics (pennacchiotti and pantel, 2006), <papid> P06-1100 </papid>supervised machine learning methods for the disambiguation of meronymy relations (girju et al, 2003), etc.</citsent>
<aftsection>
<nextsent>in this paper we presented novel approach to disambiguate the glosses of computational lexicons and machine-readable dictionaries, with the aim of alleviating the knowledge acquisition bottleneck.the method is based on the identification of cycles and quasi-cycles, i.e. circular edge sequences (possibly with one edge reversed) relating source to target word sense.
</nextsent>
<nextsent>the strength of the approach lies in its weakly supervised nature: (quasi-)cycles rely exclusively on the structure of the input lexical resources.
</nextsent>
<nextsent>no additional resource (such as labeled corpora or external knowledge bases) is required, assuming we do not resort to the fs baseline.
</nextsent>
<nextsent>as result, the approach can be applied to obtain semantic network from the disambiguation of virtually any lexical resource available in machine-readable format for which sense inventory is provided.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4265">
<title id=" E09-1068.xml">using cycles and quasi cycles to disambiguate dictionary glosses </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in contrast, the approach presented in this paper performs the disambiguation of ambiguous words by exploiting only the reference dictionary itself.
</prevsent>
<prevsent>furthermore, as we showed in section 3.3, our method does not relyon wordnet, and can be applied to any lexical knowledge resource, including bilingual dictionaries.
</prevsent>
</prevsection>
<citsent citstr=" P06-1100 ">
finally, methods in the literature more focused on specific disambiguation task include statistical methods for the attachment of hyponyms under the most likely hypernym in the wordnet taxonomy (snow et al, 2006), <papid> P06-1101 </papid>structural approaches based on semantic clusters and distance metrics (pennacchiotti and pantel, 2006), <papid> P06-1100 </papid>supervised machine learning methods for the disambiguation of meronymy relations (girju et al, 2003), etc.</citsent>
<aftsection>
<nextsent>in this paper we presented novel approach to disambiguate the glosses of computational lexicons and machine-readable dictionaries, with the aim of alleviating the knowledge acquisition bottleneck.the method is based on the identification of cycles and quasi-cycles, i.e. circular edge sequences (possibly with one edge reversed) relating source to target word sense.
</nextsent>
<nextsent>the strength of the approach lies in its weakly supervised nature: (quasi-)cycles rely exclusively on the structure of the input lexical resources.
</nextsent>
<nextsent>no additional resource (such as labeled corpora or external knowledge bases) is required, assuming we do not resort to the fs baseline.
</nextsent>
<nextsent>as result, the approach can be applied to obtain semantic network from the disambiguation of virtually any lexical resource available in machine-readable format for which sense inventory is provided.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4266">
<title id=" E09-1068.xml">using cycles and quasi cycles to disambiguate dictionary glosses </title>
<section> conclusions.  </section>
<citcontext>
<prevsection>
<prevsent>as result, the approach can be applied to obtain semantic network from the disambiguation of virtually any lexical resource available in machine-readable format for which sense inventory is provided.
</prevsent>
<prevsent>the utility of gloss disambiguation is even greater in bilingual dictionaries, as idiosyncrasies such as missing or redundant translations can be discovered, thus helping lexicographers improve the resources6.
</prevsent>
</prevsection>
<citsent citstr=" P98-1013 ">
an adaptation similar to that described for disambiguating the ragazzini/biagi can be employed for mapping pairs of lexical resources (e.g. framenet (baker et al, 1998) <papid> P98-1013 </papid>to wordnet), thus contributing to the beneficial knowledge integration process.</citsent>
<aftsection>
<nextsent>following this direction, we are planning to further experiment on the mapping of framenet, verbnet (kipper et al, 2000), and other lexical resources.the graphs output by the cqc algorithm for our datasets are available from http://lcl.uniroma1.it/cqc.
</nextsent>
<nextsent>weare scheduling the release of software package which includes our implementation of the cqc algorithm and allows its application to any resource for which standard interface can be written.
</nextsent>
<nextsent>finally, starting from the work of budanitsky and hirst (2006), <papid> J06-1003 </papid>we plan to experiment with thecqc algorithm when employed as semantic similarity measure, and compare it with the most successful existing approaches.</nextsent>
<nextsent>although in this paper we focused on the disambiguation of dictionary glosses, the same approach can be applied fordisambiguating collocations according to dictionary of choice, thus providing way to further enrich lexical resources with external knowledge.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4267">
<title id=" E09-1068.xml">using cycles and quasi cycles to disambiguate dictionary glosses </title>
<section> conclusions.  </section>
<citcontext>
<prevsection>
<prevsent>following this direction, we are planning to further experiment on the mapping of framenet, verbnet (kipper et al, 2000), and other lexical resources.the graphs output by the cqc algorithm for our datasets are available from http://lcl.uniroma1.it/cqc.
</prevsent>
<prevsent>weare scheduling the release of software package which includes our implementation of the cqc algorithm and allows its application to any resource for which standard interface can be written.
</prevsent>
</prevsection>
<citsent citstr=" J06-1003 ">
finally, starting from the work of budanitsky and hirst (2006), <papid> J06-1003 </papid>we plan to experiment with thecqc algorithm when employed as semantic similarity measure, and compare it with the most successful existing approaches.</citsent>
<aftsection>
<nextsent>although in this paper we focused on the disambiguation of dictionary glosses, the same approach can be applied fordisambiguating collocations according to dictionary of choice, thus providing way to further enrich lexical resources with external knowledge.
</nextsent>
<nextsent>acknowledgments the author is grateful token litkowski and the anonymous reviewers for their useful comments.
</nextsent>
<nextsent>he also wishes to thank zanichelli and macquarie for kindly making their dictionaries available for research purposes.
</nextsent>

</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4268">
<title id=" E12-1042.xml">spectral learning for non deterministic dependency parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>furthermore, we also present an inside-outside algorithm for the parsing model that runs in cubic time, hence maintaining the standard parsing costs for context-free grammars.
</prevsent>
<prevsent>dependency structures of natural language sentences exhibit significant amount of non-local phenomena.
</prevsent>
</prevsection>
<citsent citstr=" E06-1011 ">
historically, there have been twomain approaches to model non-locality: (1) increasing the order of the factors of dependency model (e.g. with sibling and grandparent relations(eisner, 2000; mcdonald and pereira, 2006; <papid> E06-1011 </papid>carreras, 2007; <papid> D07-1101 </papid>martins et al 2009; <papid> P09-1039 </papid>koo and collins,2010)), <papid> P10-1001 </papid>and (2) using hidden states to pass information across factors (matsuzaki et al 2005; <papid> P05-1010 </papid>petrov et al 2006; <papid> P06-1055 </papid>musillo and merlo, 2008).<papid> P08-2054 </papid></citsent>
<aftsection>
<nextsent>higher-order models have the advantage that they are relatively easy to train, because estimating the parameters of the model can be expressed as convex optimization.
</nextsent>
<nextsent>however, they havetwo main drawbacks.
</nextsent>
<nextsent>(1) the number of parameters grows significantly with the size of the factors, leading to potential data-sparsity problems.
</nextsent>
<nextsent>a solution to address the data-sparsity problem is to explicitly tell the model what properties of higher-order factors need to be remembered.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4269">
<title id=" E12-1042.xml">spectral learning for non deterministic dependency parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>furthermore, we also present an inside-outside algorithm for the parsing model that runs in cubic time, hence maintaining the standard parsing costs for context-free grammars.
</prevsent>
<prevsent>dependency structures of natural language sentences exhibit significant amount of non-local phenomena.
</prevsent>
</prevsection>
<citsent citstr=" D07-1101 ">
historically, there have been twomain approaches to model non-locality: (1) increasing the order of the factors of dependency model (e.g. with sibling and grandparent relations(eisner, 2000; mcdonald and pereira, 2006; <papid> E06-1011 </papid>carreras, 2007; <papid> D07-1101 </papid>martins et al 2009; <papid> P09-1039 </papid>koo and collins,2010)), <papid> P10-1001 </papid>and (2) using hidden states to pass information across factors (matsuzaki et al 2005; <papid> P05-1010 </papid>petrov et al 2006; <papid> P06-1055 </papid>musillo and merlo, 2008).<papid> P08-2054 </papid></citsent>
<aftsection>
<nextsent>higher-order models have the advantage that they are relatively easy to train, because estimating the parameters of the model can be expressed as convex optimization.
</nextsent>
<nextsent>however, they havetwo main drawbacks.
</nextsent>
<nextsent>(1) the number of parameters grows significantly with the size of the factors, leading to potential data-sparsity problems.
</nextsent>
<nextsent>a solution to address the data-sparsity problem is to explicitly tell the model what properties of higher-order factors need to be remembered.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4270">
<title id=" E12-1042.xml">spectral learning for non deterministic dependency parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>furthermore, we also present an inside-outside algorithm for the parsing model that runs in cubic time, hence maintaining the standard parsing costs for context-free grammars.
</prevsent>
<prevsent>dependency structures of natural language sentences exhibit significant amount of non-local phenomena.
</prevsent>
</prevsection>
<citsent citstr=" P09-1039 ">
historically, there have been twomain approaches to model non-locality: (1) increasing the order of the factors of dependency model (e.g. with sibling and grandparent relations(eisner, 2000; mcdonald and pereira, 2006; <papid> E06-1011 </papid>carreras, 2007; <papid> D07-1101 </papid>martins et al 2009; <papid> P09-1039 </papid>koo and collins,2010)), <papid> P10-1001 </papid>and (2) using hidden states to pass information across factors (matsuzaki et al 2005; <papid> P05-1010 </papid>petrov et al 2006; <papid> P06-1055 </papid>musillo and merlo, 2008).<papid> P08-2054 </papid></citsent>
<aftsection>
<nextsent>higher-order models have the advantage that they are relatively easy to train, because estimating the parameters of the model can be expressed as convex optimization.
</nextsent>
<nextsent>however, they havetwo main drawbacks.
</nextsent>
<nextsent>(1) the number of parameters grows significantly with the size of the factors, leading to potential data-sparsity problems.
</nextsent>
<nextsent>a solution to address the data-sparsity problem is to explicitly tell the model what properties of higher-order factors need to be remembered.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4271">
<title id=" E12-1042.xml">spectral learning for non deterministic dependency parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>furthermore, we also present an inside-outside algorithm for the parsing model that runs in cubic time, hence maintaining the standard parsing costs for context-free grammars.
</prevsent>
<prevsent>dependency structures of natural language sentences exhibit significant amount of non-local phenomena.
</prevsent>
</prevsection>
<citsent citstr=" P10-1001 ">
historically, there have been twomain approaches to model non-locality: (1) increasing the order of the factors of dependency model (e.g. with sibling and grandparent relations(eisner, 2000; mcdonald and pereira, 2006; <papid> E06-1011 </papid>carreras, 2007; <papid> D07-1101 </papid>martins et al 2009; <papid> P09-1039 </papid>koo and collins,2010)), <papid> P10-1001 </papid>and (2) using hidden states to pass information across factors (matsuzaki et al 2005; <papid> P05-1010 </papid>petrov et al 2006; <papid> P06-1055 </papid>musillo and merlo, 2008).<papid> P08-2054 </papid></citsent>
<aftsection>
<nextsent>higher-order models have the advantage that they are relatively easy to train, because estimating the parameters of the model can be expressed as convex optimization.
</nextsent>
<nextsent>however, they havetwo main drawbacks.
</nextsent>
<nextsent>(1) the number of parameters grows significantly with the size of the factors, leading to potential data-sparsity problems.
</nextsent>
<nextsent>a solution to address the data-sparsity problem is to explicitly tell the model what properties of higher-order factors need to be remembered.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4272">
<title id=" E12-1042.xml">spectral learning for non deterministic dependency parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>furthermore, we also present an inside-outside algorithm for the parsing model that runs in cubic time, hence maintaining the standard parsing costs for context-free grammars.
</prevsent>
<prevsent>dependency structures of natural language sentences exhibit significant amount of non-local phenomena.
</prevsent>
</prevsection>
<citsent citstr=" P05-1010 ">
historically, there have been twomain approaches to model non-locality: (1) increasing the order of the factors of dependency model (e.g. with sibling and grandparent relations(eisner, 2000; mcdonald and pereira, 2006; <papid> E06-1011 </papid>carreras, 2007; <papid> D07-1101 </papid>martins et al 2009; <papid> P09-1039 </papid>koo and collins,2010)), <papid> P10-1001 </papid>and (2) using hidden states to pass information across factors (matsuzaki et al 2005; <papid> P05-1010 </papid>petrov et al 2006; <papid> P06-1055 </papid>musillo and merlo, 2008).<papid> P08-2054 </papid></citsent>
<aftsection>
<nextsent>higher-order models have the advantage that they are relatively easy to train, because estimating the parameters of the model can be expressed as convex optimization.
</nextsent>
<nextsent>however, they havetwo main drawbacks.
</nextsent>
<nextsent>(1) the number of parameters grows significantly with the size of the factors, leading to potential data-sparsity problems.
</nextsent>
<nextsent>a solution to address the data-sparsity problem is to explicitly tell the model what properties of higher-order factors need to be remembered.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4274">
<title id=" E12-1042.xml">spectral learning for non deterministic dependency parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>furthermore, we also present an inside-outside algorithm for the parsing model that runs in cubic time, hence maintaining the standard parsing costs for context-free grammars.
</prevsent>
<prevsent>dependency structures of natural language sentences exhibit significant amount of non-local phenomena.
</prevsent>
</prevsection>
<citsent citstr=" P06-1055 ">
historically, there have been twomain approaches to model non-locality: (1) increasing the order of the factors of dependency model (e.g. with sibling and grandparent relations(eisner, 2000; mcdonald and pereira, 2006; <papid> E06-1011 </papid>carreras, 2007; <papid> D07-1101 </papid>martins et al 2009; <papid> P09-1039 </papid>koo and collins,2010)), <papid> P10-1001 </papid>and (2) using hidden states to pass information across factors (matsuzaki et al 2005; <papid> P05-1010 </papid>petrov et al 2006; <papid> P06-1055 </papid>musillo and merlo, 2008).<papid> P08-2054 </papid></citsent>
<aftsection>
<nextsent>higher-order models have the advantage that they are relatively easy to train, because estimating the parameters of the model can be expressed as convex optimization.
</nextsent>
<nextsent>however, they havetwo main drawbacks.
</nextsent>
<nextsent>(1) the number of parameters grows significantly with the size of the factors, leading to potential data-sparsity problems.
</nextsent>
<nextsent>a solution to address the data-sparsity problem is to explicitly tell the model what properties of higher-order factors need to be remembered.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4276">
<title id=" E12-1042.xml">spectral learning for non deterministic dependency parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>furthermore, we also present an inside-outside algorithm for the parsing model that runs in cubic time, hence maintaining the standard parsing costs for context-free grammars.
</prevsent>
<prevsent>dependency structures of natural language sentences exhibit significant amount of non-local phenomena.
</prevsent>
</prevsection>
<citsent citstr=" P08-2054 ">
historically, there have been twomain approaches to model non-locality: (1) increasing the order of the factors of dependency model (e.g. with sibling and grandparent relations(eisner, 2000; mcdonald and pereira, 2006; <papid> E06-1011 </papid>carreras, 2007; <papid> D07-1101 </papid>martins et al 2009; <papid> P09-1039 </papid>koo and collins,2010)), <papid> P10-1001 </papid>and (2) using hidden states to pass information across factors (matsuzaki et al 2005; <papid> P05-1010 </papid>petrov et al 2006; <papid> P06-1055 </papid>musillo and merlo, 2008).<papid> P08-2054 </papid></citsent>
<aftsection>
<nextsent>higher-order models have the advantage that they are relatively easy to train, because estimating the parameters of the model can be expressed as convex optimization.
</nextsent>
<nextsent>however, they havetwo main drawbacks.
</nextsent>
<nextsent>(1) the number of parameters grows significantly with the size of the factors, leading to potential data-sparsity problems.
</nextsent>
<nextsent>a solution to address the data-sparsity problem is to explicitly tell the model what properties of higher-order factors need to be remembered.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4282">
<title id=" E12-1042.xml">spectral learning for non deterministic dependency parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for example, one line of work has added hidden annotations to the non-terminals of phrase-structure grammar (matsuzaki et al 2005; <papid> P05-1010 </papid>petrov et al 2006; <papid> P06-1055 </papid>musillo and merlo,2008), <papid> P08-2054 </papid>resulting in compact grammars that obtain parsing accuracies comparable to lexicalized grammars.</prevsent>
<prevsent>a second line of work has modeled hidden sequential structure, like in our case, but using pdfa (infante-lopez and de rijke, 2004).</prevsent>
</prevsection>
<citsent citstr=" W07-2218 ">
finally, third line of work has induced hidden structure from the history of actions of parser (titov and henderson, 2007).<papid> W07-2218 </papid></citsent>
<aftsection>
<nextsent>however, the main drawback of the hidden variable approach to parsing is that, to the best of our knowledge, there has not been any convex formulation of the learning problem.
</nextsent>
<nextsent>as result,training hidden-variable model is both expensive and prone to local minima issues.
</nextsent>
<nextsent>in this paper we present learning algorithm for hidden-state split head-automata grammars(shag) (eisner and satta, 1999).<papid> P99-1059 </papid></nextsent>
<nextsent>in this for 409 malism, head-modifier sequences are generated by collection of finite-state automata.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4283">
<title id=" E12-1042.xml">spectral learning for non deterministic dependency parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, the main drawback of the hidden variable approach to parsing is that, to the best of our knowledge, there has not been any convex formulation of the learning problem.
</prevsent>
<prevsent>as result,training hidden-variable model is both expensive and prone to local minima issues.
</prevsent>
</prevsection>
<citsent citstr=" P99-1059 ">
in this paper we present learning algorithm for hidden-state split head-automata grammars(shag) (eisner and satta, 1999).<papid> P99-1059 </papid></citsent>
<aftsection>
<nextsent>in this for 409 malism, head-modifier sequences are generated by collection of finite-state automata.
</nextsent>
<nextsent>in our case, the underlying machines are probabilistic non-deterministic finite state automata (pnfa), which we parameterize using the operator model representation.
</nextsent>
<nextsent>this representation allows the use of simple spectral algorithms for estimating the model parameters from data (hsu et al 2009; bailly, 2011; balle et al 2012).
</nextsent>
<nextsent>in all previous work, the algorithms used to induce hidden structure require running repeated inference on training datae.g. expectation-maximization (demp ster et al 1977), or split-merge algorithms.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4285">
<title id=" E12-1042.xml">spectral learning for non deterministic dependency parsing </title>
<section> preliminaries.  </section>
<citcontext>
<prevsection>
<prevsent>(1)in the literature, standard arc-factored models further assume that p(x1:t |h, d) = t+1?
</prevsent>
<prevsent>t=1 p(xt|h, d, t) , (2) where xt+1 is always special stop word, and tis the state of deterministic automaton generating x1:t+1.
</prevsent>
</prevsection>
<citsent citstr=" H05-1066 ">
for example, setting 1 = first and 1 = rest corresponds to first-order models,while setting 1 = null and 1 = xt1 corresponds to sibling models (eisner, 2000; mcdonald et al 2005; <papid> H05-1066 </papid>mcdonald and pereira, 2006).<papid> E06-1011 </papid></citsent>
<aftsection>
<nextsent>1throughout the paper we assume we can distinguish the words in derivation, irrespective of whether two words at different positions correspond to the same symbol.
</nextsent>
<nextsent>410 2.2 operator models.
</nextsent>
<nextsent>an operator model with states is tuple 1, ? ?, {aa}ax ?, where aa ? rnn is an operator matrix and 1, ??
</nextsent>
<nextsent>rn are vectors.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4287">
<title id=" E12-1042.xml">spectral learning for non deterministic dependency parsing </title>
<section> parsing algorithms.  </section>
<citcontext>
<prevsection>
<prevsent>this problem, known as map inference, is known to be intractable for hidden-state structure prediction models, as it involves finding the most likely tree structure while summing out over hidden states.
</prevsent>
<prevsent>we use common approximation to map based on first computing posterior marginals of tree edges (i.e. dependencies) and then maximizing over the tree structure (see (park and darwiche, 2004) for complexity of general map inference and approximations).
</prevsent>
</prevsection>
<citsent citstr=" P96-1024 ">
for parsing, this strategy is sometimes known as mbr decoding; previous work has shown that empirically it gives good performance (goodman, 1996; <papid> P96-1024 </papid>clark and curran, 2004; <papid> P04-1014 </papid>titov and henderson, 2006; <papid> W06-1666 </papid>petrov and klein, 2007).<papid> N07-1051 </papid></citsent>
<aftsection>
<nextsent>in our case, we use the non-deterministic shag to compute posterior marginals of dependencies.
</nextsent>
<nextsent>we first explain the general strategy of mbr decoding, and then present an algorithm to compute marginals.
</nextsent>
<nextsent>413 let (si, sj) denote dependency between head word and modifier word j. the posterior or marginal probability of dependency (si, sj) given sentence s0:n is defined as i,j = p((si, sj) | s0:n ) = ? yy(s0:n ) : (si,sj)y p(y) . to compute marginals, the sum over derivations can be decomposed into product of inside and outside quantities (baker, 1979).
</nextsent>
<nextsent>below we describe an inside-outside algorithm for our grammars.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4288">
<title id=" E12-1042.xml">spectral learning for non deterministic dependency parsing </title>
<section> parsing algorithms.  </section>
<citcontext>
<prevsection>
<prevsent>this problem, known as map inference, is known to be intractable for hidden-state structure prediction models, as it involves finding the most likely tree structure while summing out over hidden states.
</prevsent>
<prevsent>we use common approximation to map based on first computing posterior marginals of tree edges (i.e. dependencies) and then maximizing over the tree structure (see (park and darwiche, 2004) for complexity of general map inference and approximations).
</prevsent>
</prevsection>
<citsent citstr=" P04-1014 ">
for parsing, this strategy is sometimes known as mbr decoding; previous work has shown that empirically it gives good performance (goodman, 1996; <papid> P96-1024 </papid>clark and curran, 2004; <papid> P04-1014 </papid>titov and henderson, 2006; <papid> W06-1666 </papid>petrov and klein, 2007).<papid> N07-1051 </papid></citsent>
<aftsection>
<nextsent>in our case, we use the non-deterministic shag to compute posterior marginals of dependencies.
</nextsent>
<nextsent>we first explain the general strategy of mbr decoding, and then present an algorithm to compute marginals.
</nextsent>
<nextsent>413 let (si, sj) denote dependency between head word and modifier word j. the posterior or marginal probability of dependency (si, sj) given sentence s0:n is defined as i,j = p((si, sj) | s0:n ) = ? yy(s0:n ) : (si,sj)y p(y) . to compute marginals, the sum over derivations can be decomposed into product of inside and outside quantities (baker, 1979).
</nextsent>
<nextsent>below we describe an inside-outside algorithm for our grammars.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4289">
<title id=" E12-1042.xml">spectral learning for non deterministic dependency parsing </title>
<section> parsing algorithms.  </section>
<citcontext>
<prevsection>
<prevsent>this problem, known as map inference, is known to be intractable for hidden-state structure prediction models, as it involves finding the most likely tree structure while summing out over hidden states.
</prevsent>
<prevsent>we use common approximation to map based on first computing posterior marginals of tree edges (i.e. dependencies) and then maximizing over the tree structure (see (park and darwiche, 2004) for complexity of general map inference and approximations).
</prevsent>
</prevsection>
<citsent citstr=" W06-1666 ">
for parsing, this strategy is sometimes known as mbr decoding; previous work has shown that empirically it gives good performance (goodman, 1996; <papid> P96-1024 </papid>clark and curran, 2004; <papid> P04-1014 </papid>titov and henderson, 2006; <papid> W06-1666 </papid>petrov and klein, 2007).<papid> N07-1051 </papid></citsent>
<aftsection>
<nextsent>in our case, we use the non-deterministic shag to compute posterior marginals of dependencies.
</nextsent>
<nextsent>we first explain the general strategy of mbr decoding, and then present an algorithm to compute marginals.
</nextsent>
<nextsent>413 let (si, sj) denote dependency between head word and modifier word j. the posterior or marginal probability of dependency (si, sj) given sentence s0:n is defined as i,j = p((si, sj) | s0:n ) = ? yy(s0:n ) : (si,sj)y p(y) . to compute marginals, the sum over derivations can be decomposed into product of inside and outside quantities (baker, 1979).
</nextsent>
<nextsent>below we describe an inside-outside algorithm for our grammars.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4290">
<title id=" E12-1042.xml">spectral learning for non deterministic dependency parsing </title>
<section> parsing algorithms.  </section>
<citcontext>
<prevsection>
<prevsent>this problem, known as map inference, is known to be intractable for hidden-state structure prediction models, as it involves finding the most likely tree structure while summing out over hidden states.
</prevsent>
<prevsent>we use common approximation to map based on first computing posterior marginals of tree edges (i.e. dependencies) and then maximizing over the tree structure (see (park and darwiche, 2004) for complexity of general map inference and approximations).
</prevsent>
</prevsection>
<citsent citstr=" N07-1051 ">
for parsing, this strategy is sometimes known as mbr decoding; previous work has shown that empirically it gives good performance (goodman, 1996; <papid> P96-1024 </papid>clark and curran, 2004; <papid> P04-1014 </papid>titov and henderson, 2006; <papid> W06-1666 </papid>petrov and klein, 2007).<papid> N07-1051 </papid></citsent>
<aftsection>
<nextsent>in our case, we use the non-deterministic shag to compute posterior marginals of dependencies.
</nextsent>
<nextsent>we first explain the general strategy of mbr decoding, and then present an algorithm to compute marginals.
</nextsent>
<nextsent>413 let (si, sj) denote dependency between head word and modifier word j. the posterior or marginal probability of dependency (si, sj) given sentence s0:n is defined as i,j = p((si, sj) | s0:n ) = ? yy(s0:n ) : (si,sj)y p(y) . to compute marginals, the sum over derivations can be decomposed into product of inside and outside quantities (baker, 1979).
</nextsent>
<nextsent>below we describe an inside-outside algorithm for our grammars.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4292">
<title id=" E09-1097.xml">improving grammaticality in statistical sentence generation introducing a dependency spanning tree algorithm with an argument satisfaction model </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>using bleu to measure performance on string regeneration task, we found an improvement, illustrating the benefit of the spanning tree approach armed with an argument satisfaction model.
</prevsent>
<prevsent>research in statistical novel sentence generation has the potential to extend the current capabilities of automatic text summarisation technology, moving from sentence extraction to abstract-like summarisation.
</prevsent>
</prevsection>
<citsent citstr=" W00-1401 ">
in this paper, we describe new algorithm that improves upon the grammaticality of statistically generated sentences, evaluated on string regeneration task, which was first propose das surrogate for grammaticality test by bangalore et al  (2000).<papid> W00-1401 </papid></citsent>
<aftsection>
<nextsent>in this task, system must regenerate the original sentence which has had its word order scrambled.as an evaluation task, string regeneration reflects the issues that challenge the sentence generation components of machine translation, paraphrase generation, and summarisation systems(soricut and marcu, 2005).<papid> P05-1009 </papid></nextsent>
<nextsent>our research in summarisation utilises the statistical generation algorithms described in this paper to generate novel summary sentences.the goal of the string regeneration task is to recover sentence once its words have been randomly ordered.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4293">
<title id=" E09-1097.xml">improving grammaticality in statistical sentence generation introducing a dependency spanning tree algorithm with an argument satisfaction model </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>research in statistical novel sentence generation has the potential to extend the current capabilities of automatic text summarisation technology, moving from sentence extraction to abstract-like summarisation.
</prevsent>
<prevsent>in this paper, we describe new algorithm that improves upon the grammaticality of statistically generated sentences, evaluated on string regeneration task, which was first propose das surrogate for grammaticality test by bangalore et al  (2000).<papid> W00-1401 </papid></prevsent>
</prevsection>
<citsent citstr=" P05-1009 ">
in this task, system must regenerate the original sentence which has had its word order scrambled.as an evaluation task, string regeneration reflects the issues that challenge the sentence generation components of machine translation, paraphrase generation, and summarisation systems(soricut and marcu, 2005).<papid> P05-1009 </papid></citsent>
<aftsection>
<nextsent>our research in summarisation utilises the statistical generation algorithms described in this paper to generate novel summary sentences.the goal of the string regeneration task is to recover sentence once its words have been randomly ordered.
</nextsent>
<nextsent>similarly, for text-to-text generation scenario, the goal is to generate sentence given an unordered list of words, typically using an n-gram language model to select the best word ordering.
</nextsent>
<nextsent>n-gram language models appear to do well at local level when examining word sequences smaller than n. however, beyond this window size, the sequence is often ungrammatical.
</nextsent>
<nextsent>this is not surprising as these methods are unable to model grammaticality at the sentence level,unless the size of is sufficiently large.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4294">
<title id=" E09-1097.xml">improving grammaticality in statistical sentence generation introducing a dependency spanning tree algorithm with an argument satisfaction model </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in practice, the lack of sufficient training data means that is often smaller than the average sentence length.even if data exists, increasing the size of corresponds to higher degree polynomial complexity search for the best word sequence.
</prevsent>
<prevsent>in response, we introduce an algorithm for searching for the best word sequence in waythat attempts to model grammaticality at the sentence level.
</prevsent>
</prevsection>
<citsent citstr=" H05-1066 ">
mirroring the use of spanning tree algorithms in parsing (mcdonald et al , 2005), <papid> H05-1066 </papid>we present an approach to statistical sentence genera tion.</citsent>
<aftsection>
<nextsent>given set of scrambled words, the approach searches for the most probable dependency tree, as defined by some corpus, such that it contains each word of the input set.
</nextsent>
<nextsent>the tree is then traversed to obtain the final word ordering.in particular, we present two spanning tree algorithms.
</nextsent>
<nextsent>we first adapt the chu-liu-edmonds(cle) algorithm (see chu and liu (1965) and edmonds (1967)), used in mcdonald et al  (2005), <papid> H05-1066 </papid>to include basic argument model, added to keep track of linear precedence between heads and modifiers.</nextsent>
<nextsent>while our adapted version of the cle algorithm finds an optimal spanning tree, this does 852 not always correspond with linguistically valid dependency tree, primarily because it does not attempt to ensure that words in the tree have plausible numbers of arguments.we propose an alternative dependency spanning tree algorithm which uses morefine-grained argument model representing argument positions.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4299">
<title id=" E09-1097.xml">improving grammaticality in statistical sentence generation introducing a dependency spanning tree algorithm with an argument satisfaction model </title>
<section> a graph representation of </section>
<citcontext>
<prevsection>
<prevsent>the graph is fully connected (except for the root vertex w0 which is only fully connected outwards) and is representation of possible dependencies.
</prevsent>
<prevsent>for an edge (u, v), we refer to as the head and as the modifier.we extend the original formulation of mcdonald et al  (2005) <papid> H05-1066 </papid>by adding notion of argument positions for word, providing points to attachmodifiers.</prevsent>
</prevsection>
<citsent citstr=" P07-1022 ">
adopting an approach similar to johnson (2007), <papid> P07-1022 </papid>we look at the direction (left or right)of the head with respect to the modifier; we consequently define set = {l, r} to represent this.</citsent>
<aftsection>
<nextsent>set represents the linear precedence of the words in the dependency relation; consequently, it partially approximates the distinction between syntactic roles like subject and object.
</nextsent>
<nextsent>each edge has pair of associated weights, one for each direction, defined by the function :ed ? r, based on probabilistic model of dependency relations.
</nextsent>
<nextsent>to calculate the edge weights,we adapt the definition of collins (1996) <papid> P96-1025 </papid>to use direction rather than relation type (represented in the original as triples of non-terminals).</nextsent>
<nextsent>given corpus, for some edge = (u, v) ? and direction ? d, we calculate the edge weight as: s((u, v), d) = log probdep(u, v, d) (1) we define the set of part-of-speech (pos) tags and function pos : ? , which maps vertices (representing words) to their pos, to calculate the probability of dependency relation, defined as: probdep(u, v, d) = cnt((u, pos(u)), (v, pos(v)), d) co-occurs((u, pos(u)), (v, pos(v))) (2)where cnt((u, pos(u)), (v, pos(v)), d) is the number of times where (v, pos(v)) and (u, pos(u)) are seen in sentence in the training data, and (v, pos(v)) modifies (u, pos(u)) in direction d. the function co-occurs((u, pos(u)), (v, pos(v))) returns the number of times that (v, pos(v)) and (u, pos(u)) are seen in sentence in the training data.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4300">
<title id=" E09-1097.xml">improving grammaticality in statistical sentence generation introducing a dependency spanning tree algorithm with an argument satisfaction model </title>
<section> a graph representation of </section>
<citcontext>
<prevsection>
<prevsent>set represents the linear precedence of the words in the dependency relation; consequently, it partially approximates the distinction between syntactic roles like subject and object.
</prevsent>
<prevsent>each edge has pair of associated weights, one for each direction, defined by the function :ed ? r, based on probabilistic model of dependency relations.
</prevsent>
</prevsection>
<citsent citstr=" P96-1025 ">
to calculate the edge weights,we adapt the definition of collins (1996) <papid> P96-1025 </papid>to use direction rather than relation type (represented in the original as triples of non-terminals).</citsent>
<aftsection>
<nextsent>given corpus, for some edge = (u, v) ? and direction ? d, we calculate the edge weight as: s((u, v), d) = log probdep(u, v, d) (1) we define the set of part-of-speech (pos) tags and function pos : ? , which maps vertices (representing words) to their pos, to calculate the probability of dependency relation, defined as: probdep(u, v, d) = cnt((u, pos(u)), (v, pos(v)), d) co-occurs((u, pos(u)), (v, pos(v))) (2)where cnt((u, pos(u)), (v, pos(v)), d) is the number of times where (v, pos(v)) and (u, pos(u)) are seen in sentence in the training data, and (v, pos(v)) modifies (u, pos(u)) in direction d. the function co-occurs((u, pos(u)), (v, pos(v))) returns the number of times that (v, pos(v)) and (u, pos(u)) are seen in sentence in the training data.
</nextsent>
<nextsent>we adopt the same smoothing strategy as collins (1996), <papid> P96-1025 </papid>which backs off to pos for unseen dependency events.</nextsent>
<nextsent>853</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4305">
<title id=" E09-1097.xml">improving grammaticality in statistical sentence generation introducing a dependency spanning tree algorithm with an argument satisfaction model </title>
<section> using an argument satisfaction model.  </section>
<citcontext>
<prevsection>
<prevsent>an edge is penalised if it is improbable that the head takes on yet another modifier, say in the example of an attachment to preposition whose argument position has already been filled.
</prevsent>
<prevsent>however, accounting for argument positions makes an edge weight dynamic and dependent on surrounding tree context.
</prevsent>
</prevsection>
<citsent citstr=" W07-2216 ">
this makes the search for an optimal tree an np-hard problem (mcdon ald and satta, 2007) <papid> W07-2216 </papid>as all possible trees must be considered to find an optimal solution.</citsent>
<aftsection>
<nextsent>consequently, we must choose heuristic search algorithm for finding the locally optimum spanning tree.
</nextsent>
<nextsent>by representing argument positions that can be filled only once, we allow modifiers to compete for argument positions and vice versa.the cle algorithm only considers this competition in one direction.
</nextsent>
<nextsent>in line 3 of algorithm 1,only heads compete for modifiers, and thus the solution will be sub-optimal.
</nextsent>
<nextsent>in wan et al  (2007), we showed that introducing model of argument positions into greedy spanning tree algorithm had little effect on performance.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4306">
<title id=" E09-1097.xml">improving grammaticality in statistical sentence generation introducing a dependency spanning tree algorithm with an argument satisfaction model </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>as surrogate measurement of grammaticality, we use the string regeneration task.
</prevsent>
<prevsent>beginning with ahuman-authored sentence with its word order random ised, the goal is to regenerate the original sentence.
</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
success is indicated by the proportion of the original sentence regenerated, as measured by any string comparison method: in our case, using the bleu metric (papineni et al , 2002).<papid> P02-1040 </papid></citsent>
<aftsection>
<nextsent>one benefit to this evaluation is that content selection, as factor, is held constant.
</nextsent>
<nextsent>specifically, the probability of word selection is uniform for all words.
</nextsent>
<nextsent>the string comparison task and its associated metrics like bleu are not perfect.3 the evaluation can be seen as being overly strict.
</nextsent>
<nextsent>it assumes that the only grammatical order is that of the original human authored sentence, referred to as the gold standard?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4307">
<title id=" E09-1097.xml">improving grammaticality in statistical sentence generation introducing a dependency spanning tree algorithm with an argument satisfaction model </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>development was done on section 00 and testing was performed on section 23.a 4-gram language model (lm) was also obtained from the ptb training data, referred to as ptb-lm.
</prevsent>
<prevsent>additionally, 4-gram language model was obtained from subsection of the bllip99 corpus (ldc number: ldc2000t43) containing three years of wsj data from 1987 to 1989 (char niak et al , 1999).
</prevsent>
</prevsection>
<citsent citstr=" P04-1030 ">
as in collins et al  (2004), <papid> P04-1030 </papid>the 1987 portion of the bllip corpus containing20 million words was also used to create language model, referred to here as bllip-lm.</citsent>
<aftsection>
<nextsent>n gram models were smoothed using katzs method, backing off to smaller values of n. for this evaluation, token isation was based on that provided by the ptb dataset.
</nextsent>
<nextsent>this dataset al deli mits base noun phrases (noun phrases without nested constituents).
</nextsent>
<nextsent>base noun phrases were treated as single tokens, and the rightmost word assumed to be the head.
</nextsent>
<nextsent>for the algorithms tested, the input set for any test case consisted ofthe single tokens identified by the ptb token isa tion.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4308">
<title id=" E09-1097.xml">improving grammaticality in statistical sentence generation introducing a dependency spanning tree algorithm with an argument satisfaction model </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>for the algorithms tested, the input set for any test case consisted ofthe single tokens identified by the ptb token isa tion.
</prevsent>
<prevsent>additionally, the heads of base noun phrases were included in this input set.
</prevsent>
</prevsection>
<citsent citstr=" P07-1044 ">
that is, we do not regenerate the base noun phrases.43alternative grammaticality measures have been developed recently (mutton et al , 2007).<papid> P07-1044 </papid></citsent>
<aftsection>
<nextsent>we are currently exploring the use of this and other metrics.4this would correspond to the use of chunking algorithm or named-entity recogniser to find noun phrases that could be re-used for sentence generation.
</nextsent>
<nextsent>857 algorithms ptb-lm bllip-lm viterbi baseline 14.9 18.0 lmo baseline 24.3 26.0 cle 26.4 26.8 ab 33.6 33.7 figure 2: string regeneration as measured in bleu points (maximum 100) 5.3 algorithms and baselines.
</nextsent>
<nextsent>we compare the baselines against the chu-liu edmonds (cle) algorithm to see if spanning tree algorithms do indeed improve upon conventional language modelling.
</nextsent>
<nextsent>we also compare the assignment-based (ab) algorithm against the baselines and cle to see if, additionally, modelling argument assignments improves the resulting tree and thus the generated word sequence.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4309">
<title id=" E09-1097.xml">improving grammaticality in statistical sentence generation introducing a dependency spanning tree algorithm with an argument satisfaction model </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>5.4 results.
</prevsent>
<prevsent>the results are presented in table 2.
</prevsent>
</prevsection>
<citsent citstr=" P05-1066 ">
significance was measured using the sign test and the sampling method outlined in (collins et al , 2005).<papid> P05-1066 </papid></citsent>
<aftsection>
<nextsent>we will examine the results in the ptb-lm column first.
</nextsent>
<nextsent>the gain of 10 bleu points by the lmo baseline over the viterbi baseline shows the performance improvement that can be gained when re inserting the base noun phrases.
</nextsent>
<nextsent>ab: the dow at this point was down about 35 points cle: was down about this point 35 points the dow at lmo: was this point about at down the down 35 points viterbi: the down 35 points at was about this point down original: at this point, the dow was down about 35 points figure 3: example generated sentences using the bllip-lm.
</nextsent>
<nextsent>the cle algorithm significantly out-performed the lmo baseline by 2 bleu points, from which we conclude that incorporating model for global syntactic structure and treating the search for dependency tree as spanning problem helps for novel sentence generation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4310">
<title id=" E09-1097.xml">improving grammaticality in statistical sentence generation introducing a dependency spanning tree algorithm with an argument satisfaction model </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>figure 3 presents sample generated strings.
</prevsent>
<prevsent>6.1 statistical surface realisers.
</prevsent>
</prevsection>
<citsent citstr=" W98-1426 ">
the work in this paper is similar to research in statistical surface realisation (for example, langk ilde and knight (1998); <papid> W98-1426 </papid>bangalore and rambow (2000); <papid> C00-1007 </papid>filippova and strube (2008)).<papid> D08-1019 </papid></citsent>
<aftsection>
<nextsent>these start with semantic representation for which specific rendering, an ordering of words, must be determined, often using language models to govern tree traversal.
</nextsent>
<nextsent>the task in this paper is different as it is text-to-text scenario and does not begin with representation of semantics.
</nextsent>
<nextsent>858the dependency model and the lmo lineari sation algorithm are based heavily on word order statistics.
</nextsent>
<nextsent>as such, the utility of this approach is limited to human languages with minimal use of inflections, such as english.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4311">
<title id=" E09-1097.xml">improving grammaticality in statistical sentence generation introducing a dependency spanning tree algorithm with an argument satisfaction model </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>figure 3 presents sample generated strings.
</prevsent>
<prevsent>6.1 statistical surface realisers.
</prevsent>
</prevsection>
<citsent citstr=" C00-1007 ">
the work in this paper is similar to research in statistical surface realisation (for example, langk ilde and knight (1998); <papid> W98-1426 </papid>bangalore and rambow (2000); <papid> C00-1007 </papid>filippova and strube (2008)).<papid> D08-1019 </papid></citsent>
<aftsection>
<nextsent>these start with semantic representation for which specific rendering, an ordering of words, must be determined, often using language models to govern tree traversal.
</nextsent>
<nextsent>the task in this paper is different as it is text-to-text scenario and does not begin with representation of semantics.
</nextsent>
<nextsent>858the dependency model and the lmo lineari sation algorithm are based heavily on word order statistics.
</nextsent>
<nextsent>as such, the utility of this approach is limited to human languages with minimal use of inflections, such as english.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4312">
<title id=" E09-1097.xml">improving grammaticality in statistical sentence generation introducing a dependency spanning tree algorithm with an argument satisfaction model </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>figure 3 presents sample generated strings.
</prevsent>
<prevsent>6.1 statistical surface realisers.
</prevsent>
</prevsection>
<citsent citstr=" D08-1019 ">
the work in this paper is similar to research in statistical surface realisation (for example, langk ilde and knight (1998); <papid> W98-1426 </papid>bangalore and rambow (2000); <papid> C00-1007 </papid>filippova and strube (2008)).<papid> D08-1019 </papid></citsent>
<aftsection>
<nextsent>these start with semantic representation for which specific rendering, an ordering of words, must be determined, often using language models to govern tree traversal.
</nextsent>
<nextsent>the task in this paper is different as it is text-to-text scenario and does not begin with representation of semantics.
</nextsent>
<nextsent>858the dependency model and the lmo lineari sation algorithm are based heavily on word order statistics.
</nextsent>
<nextsent>as such, the utility of this approach is limited to human languages with minimal use of inflections, such as english.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4313">
<title id=" E09-1097.xml">improving grammaticality in statistical sentence generation introducing a dependency spanning tree algorithm with an argument satisfaction model </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>858the dependency model and the lmo lineari sation algorithm are based heavily on word order statistics.
</prevsent>
<prevsent>as such, the utility of this approach is limited to human languages with minimal use of inflections, such as english.
</prevsent>
</prevsection>
<citsent citstr=" P07-1041 ">
approaches for other language types, for example german, have been explored (filippova and strube, 2007).<papid> P07-1041 </papid></citsent>
<aftsection>
<nextsent>6.2 text-to-text generation.
</nextsent>
<nextsent>as text-to-text approach, our work is more similar to work on information fusion (barzilay etal., 1999), <papid> P99-1071 </papid>sub-problem in multi-document sum marisation.</nextsent>
<nextsent>in this work, sentences presenting thesame information, for example multiple news articles describing the same event, are merged to form single summary by aligning repeated words and phrases across sentences.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4314">
<title id=" E09-1097.xml">improving grammaticality in statistical sentence generation introducing a dependency spanning tree algorithm with an argument satisfaction model </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>approaches for other language types, for example german, have been explored (filippova and strube, 2007).<papid> P07-1041 </papid></prevsent>
<prevsent>6.2 text-to-text generation.</prevsent>
</prevsection>
<citsent citstr=" P99-1071 ">
as text-to-text approach, our work is more similar to work on information fusion (barzilay etal., 1999), <papid> P99-1071 </papid>sub-problem in multi-document sum marisation.</citsent>
<aftsection>
<nextsent>in this work, sentences presenting thesame information, for example multiple news articles describing the same event, are merged to form single summary by aligning repeated words and phrases across sentences.
</nextsent>
<nextsent>other text-to-text approaches for generating novel sentences also aim to recycle sentence fragments where possible, as we do.
</nextsent>
<nextsent>work on phrase based statistical machine translation has been applied to paraphrase generation (bannard andcallison-burch, 2005) and multi-sentence alignment in summarisation (daume?
</nextsent>
<nextsent>iii and marcu, 2004).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4318">
<title id=" E09-1097.xml">improving grammaticality in statistical sentence generation introducing a dependency spanning tree algorithm with an argument satisfaction model </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>this is similar to work in semantic role labelling bypado?
</prevsent>
<prevsent>and lapata (2006).
</prevsent>
</prevsection>
<citsent citstr=" D07-1002 ">
the alignment of answers to question types as semantic role labelling task using similar methods was explored by shen and lapata (2007).<papid> D07-1002 </papid></citsent>
<aftsection>
<nextsent>our work is also strongly related to that ofwong and mooney (2007) <papid> N07-1022 </papid>which constructs symbolic semantic structures via an assignment process in order to provide surface realisers with input.</nextsent>
<nextsent>our approach differs in that we do not begin with fixed set of semantic labels.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4319">
<title id=" E09-1097.xml">improving grammaticality in statistical sentence generation introducing a dependency spanning tree algorithm with an argument satisfaction model </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>and lapata (2006).
</prevsent>
<prevsent>the alignment of answers to question types as semantic role labelling task using similar methods was explored by shen and lapata (2007).<papid> D07-1002 </papid></prevsent>
</prevsection>
<citsent citstr=" N07-1022 ">
our work is also strongly related to that ofwong and mooney (2007) <papid> N07-1022 </papid>which constructs symbolic semantic structures via an assignment process in order to provide surface realisers with input.</citsent>
<aftsection>
<nextsent>our approach differs in that we do not begin with fixed set of semantic labels.
</nextsent>
<nextsent>additionally, our end goal is dependency tree that encode sword precedence order, bypassing the surface realisation stage.
</nextsent>
<nextsent>in this paper, we presented new use of spanning tree algorithms for generating sentences from aninput set of words, task common to many textto-text scenarios.
</nextsent>
<nextsent>the algorithm finds the best dependency trees in order to ensure that the resulting string has grammaticality modelled at global (sentence) level.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4320">
<title id=" E06-1040.xml">comparing automatic and human evaluation of nlg systems </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, in general it is probably best for automatic evaluations to be supported by human-based evaluations, or at least by studies that demonstrate thata particular metric correlates well with human judgments in given domain.
</prevsent>
<prevsent>evaluation is becoming an increasingly important topic in natural language generation (nlg), as in other fields of computational linguistics.
</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
some nlg researchers are impressed by the success of the bleu evaluation metric (papineni et al, 2002)<papid> P02-1040 </papid>in machine translation (mt), which has transformed the mt field by allowing researchers to quickly and cheaply evaluate the impact of new ideas, algorithms, and data sets.</citsent>
<aftsection>
<nextsent>bleu andre lated metrics work by comparing the output of an mt system to set of reference (gold standard?)translations, and in principle this kind of evaluation could be done with nlg systems as well.
</nextsent>
<nextsent>indeed nlg researchers are already starting to usebleu (habash, 2004; belz, 2005) <papid> W05-1601 </papid>in their evaluations, as this is much cheaper and easier to organise than the human evaluations that have traditionally been used to evaluate nlg systems.however, the use of such corpus-based evaluation metrics is only sensible if they are known tobe correlated with the results of human-based eval uations.</nextsent>
<nextsent>while studies have shown that ratings ofmt systems by bleu and similar metrics correlate well with human judgments (papineni et al, 2002; <papid> P02-1040 </papid>doddington, 2002), we are not aware of any studies that have shown that corpus-based evaluation metrics of nlg systems are correlated with human judgments; correlation studies have been made of individual components (bangalore et al, 2000), <papid> W00-1401 </papid>but not of systems.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4321">
<title id=" E06-1040.xml">comparing automatic and human evaluation of nlg systems </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>some nlg researchers are impressed by the success of the bleu evaluation metric (papineni et al, 2002)<papid> P02-1040 </papid>in machine translation (mt), which has transformed the mt field by allowing researchers to quickly and cheaply evaluate the impact of new ideas, algorithms, and data sets.</prevsent>
<prevsent>bleu andre lated metrics work by comparing the output of an mt system to set of reference (gold standard?)translations, and in principle this kind of evaluation could be done with nlg systems as well.</prevsent>
</prevsection>
<citsent citstr=" W05-1601 ">
indeed nlg researchers are already starting to usebleu (habash, 2004; belz, 2005) <papid> W05-1601 </papid>in their evaluations, as this is much cheaper and easier to organise than the human evaluations that have traditionally been used to evaluate nlg systems.however, the use of such corpus-based evaluation metrics is only sensible if they are known tobe correlated with the results of human-based eval uations.</citsent>
<aftsection>
<nextsent>while studies have shown that ratings ofmt systems by bleu and similar metrics correlate well with human judgments (papineni et al, 2002; <papid> P02-1040 </papid>doddington, 2002), we are not aware of any studies that have shown that corpus-based evaluation metrics of nlg systems are correlated with human judgments; correlation studies have been made of individual components (bangalore et al, 2000), <papid> W00-1401 </papid>but not of systems.</nextsent>
<nextsent>in this paper we present an empirical study of how well various corpus-based metrics agree with human judgments, when evaluating several nlg systems that generate sentences which describe changes in the wind (for weather forecasts).these systems do not perform content determination (they are limited to micro planning and realisa tion), so our study does not address corpus-based evaluation of content determination.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4323">
<title id=" E06-1040.xml">comparing automatic and human evaluation of nlg systems </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>bleu andre lated metrics work by comparing the output of an mt system to set of reference (gold standard?)translations, and in principle this kind of evaluation could be done with nlg systems as well.
</prevsent>
<prevsent>indeed nlg researchers are already starting to usebleu (habash, 2004; belz, 2005) <papid> W05-1601 </papid>in their evaluations, as this is much cheaper and easier to organise than the human evaluations that have traditionally been used to evaluate nlg systems.however, the use of such corpus-based evaluation metrics is only sensible if they are known tobe correlated with the results of human-based eval uations.</prevsent>
</prevsection>
<citsent citstr=" W00-1401 ">
while studies have shown that ratings ofmt systems by bleu and similar metrics correlate well with human judgments (papineni et al, 2002; <papid> P02-1040 </papid>doddington, 2002), we are not aware of any studies that have shown that corpus-based evaluation metrics of nlg systems are correlated with human judgments; correlation studies have been made of individual components (bangalore et al, 2000), <papid> W00-1401 </papid>but not of systems.</citsent>
<aftsection>
<nextsent>in this paper we present an empirical study of how well various corpus-based metrics agree with human judgments, when evaluating several nlg systems that generate sentences which describe changes in the wind (for weather forecasts).these systems do not perform content determination (they are limited to micro planning and realisa tion), so our study does not address corpus-based evaluation of content determination.
</nextsent>
<nextsent>2.1 evaluation of nlg systems.
</nextsent>
<nextsent>nlg systems have traditionally been evaluated using human subjects (mellish and dale, 1998).
</nextsent>
<nextsent>nlg evaluations have tended to be of the intrinsic type (sparck jones and galli ers, 1996), involving subjects reading and rating texts; usually subjects 313 are shown both nlg and human-written texts, andthe nlg system is evaluated by comparing the ratings of its texts and human texts.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4324">
<title id=" E06-1040.xml">comparing automatic and human evaluation of nlg systems </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>nlg evaluations have tended to be of the intrinsic type (sparck jones and galli ers, 1996), involving subjects reading and rating texts; usually subjects 313 are shown both nlg and human-written texts, andthe nlg system is evaluated by comparing the ratings of its texts and human texts.
</prevsent>
<prevsent>in some cases, subjects are shown texts generated by several nlg systems, including baseline system which serves as another point of comparison.
</prevsent>
</prevsection>
<citsent citstr=" C96-1043 ">
this methodology was first used in nlg in the mid-1990s by coch(1996) <papid> C96-1043 </papid>and lester and porter (1997), <papid> J97-1004 </papid>and continues to be popular today.</citsent>
<aftsection>
<nextsent>other, extrinsic, types of human evaluations of nlg systems include measuring the impact of different generated texts on task performance(young, 1999), measuring how much experts post edit generated texts (sripada et al, 2005), and measuring how quickly people read generated texts (williams and reiter, 2005).
</nextsent>
<nextsent>in recent years there has been growing interest in evaluating nlg texts by comparing them to acorpus of human-written texts.
</nextsent>
<nextsent>as in other areas of nlp, the advantages of automatic corpus based evaluation are that it is potentially much cheaper and quicker than human-based evaluation,and also that it is repeatable.
</nextsent>
<nextsent>corpus-based evaluation was first used in nlg by langkilde (1998), who parsed texts from corpus, fed the output of her parser to her nlg system, and then compared the generated texts to the original corpus texts.similar evaluations have been used e.g. by bangalore et al (2000) <papid> W00-1401 </papid>and marciniak and strube (2004).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4325">
<title id=" E06-1040.xml">comparing automatic and human evaluation of nlg systems </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>nlg evaluations have tended to be of the intrinsic type (sparck jones and galli ers, 1996), involving subjects reading and rating texts; usually subjects 313 are shown both nlg and human-written texts, andthe nlg system is evaluated by comparing the ratings of its texts and human texts.
</prevsent>
<prevsent>in some cases, subjects are shown texts generated by several nlg systems, including baseline system which serves as another point of comparison.
</prevsent>
</prevsection>
<citsent citstr=" J97-1004 ">
this methodology was first used in nlg in the mid-1990s by coch(1996) <papid> C96-1043 </papid>and lester and porter (1997), <papid> J97-1004 </papid>and continues to be popular today.</citsent>
<aftsection>
<nextsent>other, extrinsic, types of human evaluations of nlg systems include measuring the impact of different generated texts on task performance(young, 1999), measuring how much experts post edit generated texts (sripada et al, 2005), and measuring how quickly people read generated texts (williams and reiter, 2005).
</nextsent>
<nextsent>in recent years there has been growing interest in evaluating nlg texts by comparing them to acorpus of human-written texts.
</nextsent>
<nextsent>as in other areas of nlp, the advantages of automatic corpus based evaluation are that it is potentially much cheaper and quicker than human-based evaluation,and also that it is repeatable.
</nextsent>
<nextsent>corpus-based evaluation was first used in nlg by langkilde (1998), who parsed texts from corpus, fed the output of her parser to her nlg system, and then compared the generated texts to the original corpus texts.similar evaluations have been used e.g. by bangalore et al (2000) <papid> W00-1401 </papid>and marciniak and strube (2004).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4329">
<title id=" E06-1040.xml">comparing automatic and human evaluation of nlg systems </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>properly calculated bleu scores have been shown to correlate reliably with human judgments (papineni et al, 2002)<papid> P02-1040 </papid>.</prevsent>
<prevsent>the nist mt evaluation metric (doddington, 2002) is an adaptation of bleu, but where bleu gives equal weight to all n-grams, nist gives more importance to less frequent (hence more infor mative) n-grams.</prevsent>
</prevsection>
<citsent citstr=" W05-0908 ">
bleus ability to detect subtle but important differences in translation quality has been questioned, some research showing nist to be more sensitive (doddington, 2002; riezler and maxwell iii, 2005).<papid> W05-0908 </papid></citsent>
<aftsection>
<nextsent>the rouge metric (lin and hovy, 2003) <papid> N03-1020 </papid>was conceived as document summarisations answer to bleu, but it does not appear to have met with thesame degree of enthusiasm.</nextsent>
<nextsent>there are several different rouge metrics.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4330">
<title id=" E06-1040.xml">comparing automatic and human evaluation of nlg systems </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>the nist mt evaluation metric (doddington, 2002) is an adaptation of bleu, but where bleu gives equal weight to all n-grams, nist gives more importance to less frequent (hence more infor mative) n-grams.
</prevsent>
<prevsent>bleus ability to detect subtle but important differences in translation quality has been questioned, some research showing nist to be more sensitive (doddington, 2002; riezler and maxwell iii, 2005).<papid> W05-0908 </papid></prevsent>
</prevsection>
<citsent citstr=" N03-1020 ">
the rouge metric (lin and hovy, 2003) <papid> N03-1020 </papid>was conceived as document summarisations answer to bleu, but it does not appear to have met with thesame degree of enthusiasm.</citsent>
<aftsection>
<nextsent>there are several different rouge metrics.
</nextsent>
<nextsent>the simplest is rouge-n,which computes the highest proportion in any reference summary of n-grams that are matched by the system-generated summary.
</nextsent>
<nextsent>a procedure is applied that averages the score across leave-oneout subsets of the set of reference texts.
</nextsent>
<nextsent>rougen is an almost straightforward n-gram recall metric between two texts, and has several counter intuitive properties, including that even text composed entirely of sentences from reference texts cannot score 1 (unless there is only one reference text).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4336">
<title id=" E06-1040.xml">comparing automatic and human evaluation of nlg systems </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>this behaviour is penalised by the automatic evaluation metrics, but the human evaluators do not seem to mind it.one of the classic rules of writing is to vary lexical and syntactic choices, in order to keep text interesting.
</prevsent>
<prevsent>however, this behaviour (variation for variations sake) will always reduce systems score under corpus-similarity metrics, even if it enhances text quality from the perspective of readers.
</prevsent>
</prevsection>
<citsent citstr=" E06-1045 ">
foster and oberlander (2006), <papid> E06-1045 </papid>in their study of facial gestures, have also noted that humans do not mind and indeed in some cases prefer variation,whereas corpus-based evaluations give higher ratings to systems which follow corpus frequency.</citsent>
<aftsection>
<nextsent>using more reference texts does counteract this tendency, but only up to point: no matter how many reference texts are used, there will still be one, or small number of, most frequent variants,and using anything else will still worsen corpus similarity scores.
</nextsent>
<nextsent>canvassing expert opinion of text quality and averaging the results is also in sense frequency based, as results reflect what the majority of experts consider good variants.
</nextsent>
<nextsent>expert opinions canvary considerably, as shown by the low correlation among experts in our study (and as seen in corpus studies, e.g. reiter et al, 2005), and evaluations by small number of experts may also be problematic, unless we have good reason to believe that expert opinions are highly correlated in the domain (which was certainly not the case in our weather forecast domain).
</nextsent>
<nextsent>ultimately, such disagreement between experts suggests that (in trinsic) judgments of the text quality ? whether by human or metric ? really should be be backed up by (extrinsic) judgments of the effectiveness ofa text in helping real users perform tasks or otherwise achieving its communicative goal.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4337">
<title id=" E09-1036.xml">semitic morphological analysis and generation using finite state transducers with feature structures </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for example, the english noun parties could be analyzed as party+plural.
</prevsent>
<prevsent>morphological generation is the reverse process.
</prevsent>
</prevsection>
<citsent citstr=" J94-3001 ">
both processes relate surface level to lexical level.the relationship between these levels has concerned many phonologists and morphologists over the years, and traditional descriptions, since the pioneering work of chomsky and halle (1968), have characterized it in terms of series of ordered content-sensitive rewrite rules, which apply in the generation, but not the analysis, direction.within computational morphology, very significant advance came with the demonstration that phonological rules could be implemented as finite state transducers (johnson, 1972; kaplan and kay, 1994) (<papid> J94-3001 </papid>fsts) and that the rule ordering could be dispensed with using fsts that relate the surface and lexical levels directly (koskenniemi,1983).</citsent>
<aftsection>
<nextsent>because of the invertibility of fsts, two level?
</nextsent>
<nextsent>phonology and morphology permitted the creation of systems of fsts that implemented both analysis (surface input, lexical output) and generation (lexical input, surface output).in addition to inversion, fsts are closed under composition.
</nextsent>
<nextsent>a second important advance in computational morphology was the recognition by karttunen et al (1992) <papid> C92-1025 </papid>that cascade of composed fsts could implement the two-level model.</nextsent>
<nextsent>this made possible quite complex finite state systems, including ordered alternation rules representing context-sensitive variation in the phonological or orthographic shape of morphemes, the morpho tactics characterizing the possible sequences of morphemes (in canonical form) forgiven word class, and one or more sublexicons.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4338">
<title id=" E09-1036.xml">semitic morphological analysis and generation using finite state transducers with feature structures </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>because of the invertibility of fsts, two level?
</prevsent>
<prevsent>phonology and morphology permitted the creation of systems of fsts that implemented both analysis (surface input, lexical output) and generation (lexical input, surface output).in addition to inversion, fsts are closed under composition.
</prevsent>
</prevsection>
<citsent citstr=" C92-1025 ">
a second important advance in computational morphology was the recognition by karttunen et al (1992) <papid> C92-1025 </papid>that cascade of composed fsts could implement the two-level model.</citsent>
<aftsection>
<nextsent>this made possible quite complex finite state systems, including ordered alternation rules representing context-sensitive variation in the phonological or orthographic shape of morphemes, the morpho tactics characterizing the possible sequences of morphemes (in canonical form) forgiven word class, and one or more sublexicons.
</nextsent>
<nextsent>for example, to handle written english nouns, we could create cascade of fsts covering the rules that insert an in words like bushes and parties and relate lexical to surface in words like buggies and parties and an fst that represents the possible sequences of morphemes in english nouns, including all of thenoun stems in the english lexicon.
</nextsent>
<nextsent>the key feature of such systems is that, even though the fstsmaking up the cascade must be composed in particular order, the result of composition is single fst relating surface and lexical levels directly, as in two-level morphology.
</nextsent>
<nextsent>1.2 fsts for non-concatenative morphology.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4339">
<title id=" E09-1036.xml">semitic morphological analysis and generation using finite state transducers with feature structures </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the stem of semitic verb consists of root, essentially sequence of consonants, and pattern, sort of template which inserts other segments between the root consonants and possibly copies certain of them (see tigrinya examples in the next section).
</prevsent>
<prevsent>researchers within the finite state framework have proposed number of ways to deal with semitic template morphology.
</prevsent>
</prevsection>
<citsent citstr=" J00-1006 ">
one approach is to make use of separate tapes for root and pattern at the lexical level (kiraz, 2000).<papid> J00-1006 </papid></citsent>
<aftsection>
<nextsent>a transition in sucha system relates single surface character to multiple lexical characters, one for each of the distinct sublexica.
</nextsent>
<nextsent>another approach is to have the transducers atthe lexical level relate an upper abstract characterization of stem to lower string that directly represents the merging of particular root and pattern.
</nextsent>
<nextsent>this lower string can then be compiled into an fst that yields surface expression (beesleyand karttunen, 2003).
</nextsent>
<nextsent>given the extra compile and-replace operation, this resulting system maps directly between abstract lexical expressions and surface strings.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4344">
<title id=" E09-1005.xml">personalizing page rank for word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in addition, we make an analysis of the performance of the algorithm, showing that it is efficient and that it could be tuned to be faster.
</prevsent>
<prevsent>word sense disambiguation (wsd) is key enabling-technology that automatically chooses the intended sense of word in context.
</prevsent>
</prevsection>
<citsent citstr=" W04-0811 ">
supervised wsd systems are the best performing in public evaluations (palmer et al, 2001; snyder and palmer, 2004; <papid> W04-0811 </papid>pradhan et al, 2007) <papid> W07-2016 </papid>but they need large amounts of hand-tagged data, which is typically very expensive to build.</citsent>
<aftsection>
<nextsent>given the relatively small amount of training data available, current state-of-the-art systems only beat the simple most frequent sense (mfs) baseline1 by small margin.
</nextsent>
<nextsent>as an alternative to supervised systems,knowledge-based wsd systems exploit the information present in lexical knowledge base (lkb) to perform wsd, without using any further corpus evidence.
</nextsent>
<nextsent>1this baseline consists of tagging all occurrences in the test data with the sense of the word that occurs more often in the training data traditional knowledge-based wsd systems as sign sense to an ambiguous word by comparing each of its senses with those of the surroundingcontext.
</nextsent>
<nextsent>typically, some semantic similarity metric is used for calculating the relatedness among senses (lesk, 1986; mccarthy et al, 2004).<papid> P04-1036 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4348">
<title id=" E09-1005.xml">personalizing page rank for word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in addition, we make an analysis of the performance of the algorithm, showing that it is efficient and that it could be tuned to be faster.
</prevsent>
<prevsent>word sense disambiguation (wsd) is key enabling-technology that automatically chooses the intended sense of word in context.
</prevsent>
</prevsection>
<citsent citstr=" W07-2016 ">
supervised wsd systems are the best performing in public evaluations (palmer et al, 2001; snyder and palmer, 2004; <papid> W04-0811 </papid>pradhan et al, 2007) <papid> W07-2016 </papid>but they need large amounts of hand-tagged data, which is typically very expensive to build.</citsent>
<aftsection>
<nextsent>given the relatively small amount of training data available, current state-of-the-art systems only beat the simple most frequent sense (mfs) baseline1 by small margin.
</nextsent>
<nextsent>as an alternative to supervised systems,knowledge-based wsd systems exploit the information present in lexical knowledge base (lkb) to perform wsd, without using any further corpus evidence.
</nextsent>
<nextsent>1this baseline consists of tagging all occurrences in the test data with the sense of the word that occurs more often in the training data traditional knowledge-based wsd systems as sign sense to an ambiguous word by comparing each of its senses with those of the surroundingcontext.
</nextsent>
<nextsent>typically, some semantic similarity metric is used for calculating the relatedness among senses (lesk, 1986; mccarthy et al, 2004).<papid> P04-1036 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4349">
<title id=" E09-1005.xml">personalizing page rank for word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>as an alternative to supervised systems,knowledge-based wsd systems exploit the information present in lexical knowledge base (lkb) to perform wsd, without using any further corpus evidence.
</prevsent>
<prevsent>1this baseline consists of tagging all occurrences in the test data with the sense of the word that occurs more often in the training data traditional knowledge-based wsd systems as sign sense to an ambiguous word by comparing each of its senses with those of the surroundingcontext.
</prevsent>
</prevsection>
<citsent citstr=" P04-1036 ">
typically, some semantic similarity metric is used for calculating the relatedness among senses (lesk, 1986; mccarthy et al, 2004).<papid> P04-1036 </papid></citsent>
<aftsection>
<nextsent>one of the major drawbacks of these approaches stems from the fact that senses are compared in pairwise fashion and thus the number of computations can grow exponentially with the number ofwords.
</nextsent>
<nextsent>although alternatives like simulated annealing (cowie et al, 1992) <papid> H92-1046 </papid>and conceptual density (agirre and rigau, 1996) <papid> C96-1005 </papid>were tried, most ofpast knowledge based wsd was done in subop timal word-by-word process, i.e., disambiguating words one at time.recently, graph-based methods for knowledge based wsd have gained much attention in thenlp community (sinha and mihalcea, 2007; navigli and lapata, 2007; mihalcea, 2005; <papid> H05-1052 </papid>agirre and soroa, 2008).</nextsent>
<nextsent>these methods use well-known graph-based techniques to find and exploit the structural properties of the graph underlying particular lkb.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4350">
<title id=" E09-1005.xml">personalizing page rank for word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>typically, some semantic similarity metric is used for calculating the relatedness among senses (lesk, 1986; mccarthy et al, 2004).<papid> P04-1036 </papid></prevsent>
<prevsent>one of the major drawbacks of these approaches stems from the fact that senses are compared in pairwise fashion and thus the number of computations can grow exponentially with the number ofwords.</prevsent>
</prevsection>
<citsent citstr=" H92-1046 ">
although alternatives like simulated annealing (cowie et al, 1992) <papid> H92-1046 </papid>and conceptual density (agirre and rigau, 1996) <papid> C96-1005 </papid>were tried, most ofpast knowledge based wsd was done in subop timal word-by-word process, i.e., disambiguating words one at time.recently, graph-based methods for knowledge based wsd have gained much attention in thenlp community (sinha and mihalcea, 2007; navigli and lapata, 2007; mihalcea, 2005; <papid> H05-1052 </papid>agirre and soroa, 2008).</citsent>
<aftsection>
<nextsent>these methods use well-known graph-based techniques to find and exploit the structural properties of the graph underlying particular lkb.
</nextsent>
<nextsent>because the graph is analyzed as awhole, these techniques have the remarkable property of being able to find globally optimal solutions, given the relations between entities.
</nextsent>
<nextsent>graph based wsd methods are particularly suited fordisambiguating word sequences, and they man age to exploit the interrelations among the senses in the given context.
</nextsent>
<nextsent>in this sense, they provide principled solution to the exponential explosion problem, with excellent performance.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4351">
<title id=" E09-1005.xml">personalizing page rank for word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>typically, some semantic similarity metric is used for calculating the relatedness among senses (lesk, 1986; mccarthy et al, 2004).<papid> P04-1036 </papid></prevsent>
<prevsent>one of the major drawbacks of these approaches stems from the fact that senses are compared in pairwise fashion and thus the number of computations can grow exponentially with the number ofwords.</prevsent>
</prevsection>
<citsent citstr=" C96-1005 ">
although alternatives like simulated annealing (cowie et al, 1992) <papid> H92-1046 </papid>and conceptual density (agirre and rigau, 1996) <papid> C96-1005 </papid>were tried, most ofpast knowledge based wsd was done in subop timal word-by-word process, i.e., disambiguating words one at time.recently, graph-based methods for knowledge based wsd have gained much attention in thenlp community (sinha and mihalcea, 2007; navigli and lapata, 2007; mihalcea, 2005; <papid> H05-1052 </papid>agirre and soroa, 2008).</citsent>
<aftsection>
<nextsent>these methods use well-known graph-based techniques to find and exploit the structural properties of the graph underlying particular lkb.
</nextsent>
<nextsent>because the graph is analyzed as awhole, these techniques have the remarkable property of being able to find globally optimal solutions, given the relations between entities.
</nextsent>
<nextsent>graph based wsd methods are particularly suited fordisambiguating word sequences, and they man age to exploit the interrelations among the senses in the given context.
</nextsent>
<nextsent>in this sense, they provide principled solution to the exponential explosion problem, with excellent performance.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4352">
<title id=" E09-1005.xml">personalizing page rank for word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>typically, some semantic similarity metric is used for calculating the relatedness among senses (lesk, 1986; mccarthy et al, 2004).<papid> P04-1036 </papid></prevsent>
<prevsent>one of the major drawbacks of these approaches stems from the fact that senses are compared in pairwise fashion and thus the number of computations can grow exponentially with the number ofwords.</prevsent>
</prevsection>
<citsent citstr=" H05-1052 ">
although alternatives like simulated annealing (cowie et al, 1992) <papid> H92-1046 </papid>and conceptual density (agirre and rigau, 1996) <papid> C96-1005 </papid>were tried, most ofpast knowledge based wsd was done in subop timal word-by-word process, i.e., disambiguating words one at time.recently, graph-based methods for knowledge based wsd have gained much attention in thenlp community (sinha and mihalcea, 2007; navigli and lapata, 2007; mihalcea, 2005; <papid> H05-1052 </papid>agirre and soroa, 2008).</citsent>
<aftsection>
<nextsent>these methods use well-known graph-based techniques to find and exploit the structural properties of the graph underlying particular lkb.
</nextsent>
<nextsent>because the graph is analyzed as awhole, these techniques have the remarkable property of being able to find globally optimal solutions, given the relations between entities.
</nextsent>
<nextsent>graph based wsd methods are particularly suited fordisambiguating word sequences, and they man age to exploit the interrelations among the senses in the given context.
</nextsent>
<nextsent>in this sense, they provide principled solution to the exponential explosion problem, with excellent performance.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4361">
<title id=" E09-1005.xml">personalizing page rank for word sense disambiguation </title>
<section> evaluation framework and results.  </section>
<citcontext>
<prevsection>
<prevsent>here the differences are in many cases significant.these results are surprising, as we would expect that the manually disambiguated gloss relations from wordnet 3.0 would lead to better results, compared to the automatically disambiguated gloss relations from the extended wordnet (linked to version 1.7).
</prevsent>
<prevsent>the lower performance of wnet30+gloss can be due to the fact that the senseval all words dataset is tagged using wordnet 1.7 synsets.
</prevsent>
</prevsection>
<citsent citstr=" P00-1064 ">
when using different lkb for wsd, mapping to wordnet 1.7 is required.although the mapping is cited as having correctness on the high 90s (daude et al, 2000), <papid> P00-1064 </papid>it could have introduced sufficient noise to counteract the benefits of the hand-disambiguated glosses.</citsent>
<aftsection>
<nextsent>table 1 also shows the most frequent sense(mfs), as well as the best supervised systems (snyder and palmer, 2004; <papid> W04-0811 </papid>palmer et al., 2001) that participated in each competition (smuaw and gambl, respectively).</nextsent>
<nextsent>the mfs isa baseline for supervised systems, but it is considered difficult competitor for unsupervised systems, which rarely come close to it.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4371">
<title id=" E12-1070.xml">framework of semantic role assignment based on extended lexical conceptual structure comparison with verbnet and framenet </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for example, most of the works on srl have used prop banks numerical role labels (arg0 to arg5).
</prevsent>
<prevsent>however, the meanings of these numbers depend on each verb in principle and propbank does not expect semantic consistency, namely onarg2 to arg5.
</prevsent>
</prevsection>
<citsent citstr=" N07-1069 ">
moreover, yi et al(2007) <papid> N07-1069 </papid>explicitly showed that arg2 to arg5 are semantically inconsistent.</citsent>
<aftsection>
<nextsent>the reason why such labels have been used in srl systems is that verb-specific roles generally have small number of instances and are not suitable for learning.
</nextsent>
<nextsent>however, it is necessary to avoid using inconsistent labels since those labels confuse machine learners and can bea cause of low accuracy in automatic processing.
</nextsent>
<nextsent>in addition, clarity of the definition of roles are particularly important for users to rationally know how to use each role in their applications.
</nextsent>
<nextsent>for this reasons, well-organized and generalized labels grounded in linguistic characteristics are needed in practice.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4372">
<title id=" E12-1070.xml">framework of semantic role assignment based on extended lexical conceptual structure comparison with verbnet and framenet </title>
<section> related works.  </section>
<citcontext>
<prevsection>
<prevsent>jackendoff also stated that an lcs represents two tiers in its structure, action tier and thematic tier, which are similar to culi cover and wilk inss two sets.
</prevsent>
<prevsent>essentially, these two approaches distinguished roles related to action and change, and successfully restricted com 687 26 6 4 cause(affect(i,j), go(j, 2 6 4 from(locate(in(i))) fromward(locate(at(k))) toward(locate(at(l))) 3 7 5 )) 3 7 7 5 figure 1: lcs of the verb throw.
</prevsent>
</prevsection>
<citsent citstr=" A97-1021 ">
bin ations of roles by taking role from each set.dorr (1997) <papid> A97-1021 </papid>created an lcs-based lexical resource as an inter lingual representation forma chine translation.</citsent>
<aftsection>
<nextsent>this framework was also usedfor text generation (habash et al 2003).
</nextsent>
<nextsent>how ever, the problem of multiple-role assignment was not completely solved on the resource.
</nextsent>
<nextsent>as comparison of different semantic structures, dorr(2001) and hajicova?
</nextsent>
<nextsent>and kucerova?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4373">
<title id=" E06-1051.xml">exploiting shallow linguistic information for relation extraction from biomedical literature </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in particular, we explore kernel-based approach based solely on shallow linguistic processing, such as tokenization, sentence splitting, part-of-speech (pos) tagging and lemmatization.
</prevsent>
<prevsent>kernel methods (shawe-taylor and cristianini, 2004) show their full potential when an explicit computation of the feature map becomes computationally infeasible, due to the high or even infinite dimension of the feature space.
</prevsent>
</prevsection>
<citsent citstr=" P04-1054 ">
for this reason, kernels have been recently used to develop innovative approaches to relation extraction based on syntactic information, in which the examples preserve their original representations (i.e. parse trees) and are compared by the kernel function (zelenko et al, 2003; culotta and sorensen, 2004; <papid> P04-1054 </papid>zhao and grishman, 2005).<papid> P05-1052 </papid></citsent>
<aftsection>
<nextsent>despite the positive results obtained exploiting syntactic information, we claim that there is still room for improvement relying exclusively on shallow linguistic information for two main reasons.
</nextsent>
<nextsent>first of all, previous comparative evaluations put more stress on the deep linguistic approaches anddid not put as much effort on developing effective methods based on shallow linguistic information.
</nextsent>
<nextsent>a second reason concerns the fact that syntactic parsing is not always robust enough to deal with real-world sentences.
</nextsent>
<nextsent>this may prevent approaches based on syntactic features from producing any result.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4374">
<title id=" E06-1051.xml">exploiting shallow linguistic information for relation extraction from biomedical literature </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in particular, we explore kernel-based approach based solely on shallow linguistic processing, such as tokenization, sentence splitting, part-of-speech (pos) tagging and lemmatization.
</prevsent>
<prevsent>kernel methods (shawe-taylor and cristianini, 2004) show their full potential when an explicit computation of the feature map becomes computationally infeasible, due to the high or even infinite dimension of the feature space.
</prevsent>
</prevsection>
<citsent citstr=" P05-1052 ">
for this reason, kernels have been recently used to develop innovative approaches to relation extraction based on syntactic information, in which the examples preserve their original representations (i.e. parse trees) and are compared by the kernel function (zelenko et al, 2003; culotta and sorensen, 2004; <papid> P04-1054 </papid>zhao and grishman, 2005).<papid> P05-1052 </papid></citsent>
<aftsection>
<nextsent>despite the positive results obtained exploiting syntactic information, we claim that there is still room for improvement relying exclusively on shallow linguistic information for two main reasons.
</nextsent>
<nextsent>first of all, previous comparative evaluations put more stress on the deep linguistic approaches anddid not put as much effort on developing effective methods based on shallow linguistic information.
</nextsent>
<nextsent>a second reason concerns the fact that syntactic parsing is not always robust enough to deal with real-world sentences.
</nextsent>
<nextsent>this may prevent approaches based on syntactic features from producing any result.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4375">
<title id=" E06-1051.xml">exploiting shallow linguistic information for relation extraction from biomedical literature </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>itis still an open issue whether the use of domain specific treebanks (such as the genia treebank1) 1http://www-tsujii.is.s.u-tokyo.ac.jp/ 401 can be successfully exploited to overcome thisproblem.
</prevsent>
<prevsent>therefore it is essential to better investigate the potential of approaches based exclusively on simple linguistic features.in our approach we use combination of kernel functions to represent two distinct information sources: the global context where entities appear and their local contexts.
</prevsent>
</prevsection>
<citsent citstr=" H05-1091 ">
the whole sentence where the entities appear (global context) is used to discover the presence of relation between two entities, similarly to what was done by bunescu and mooney (2005<papid> H05-1091 </papid>b).</citsent>
<aftsection>
<nextsent>windows of limited size around the entities (local contexts) provide useful clues to identify the roles of the entities within relation.
</nextsent>
<nextsent>the approach has some resemblance with what was proposed by roth and yih (2002).<papid> C02-1151 </papid>the main difference is that we perform the extraction task in single step via combined kernel, while they used two separate classifiers to identify entities and relations and their output is later combined with probabilistic global inference.</nextsent>
<nextsent>we evaluated our relation extraction algorithm on two biomedical datasets (i.e. the aimed corpus and the lll challenge data set; see section 4).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4378">
<title id=" E06-1051.xml">exploiting shallow linguistic information for relation extraction from biomedical literature </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the whole sentence where the entities appear (global context) is used to discover the presence of relation between two entities, similarly to what was done by bunescu and mooney (2005<papid> H05-1091 </papid>b).</prevsent>
<prevsent>windows of limited size around the entities (local contexts) provide useful clues to identify the roles of the entities within relation.</prevsent>
</prevsection>
<citsent citstr=" C02-1151 ">
the approach has some resemblance with what was proposed by roth and yih (2002).<papid> C02-1151 </papid>the main difference is that we perform the extraction task in single step via combined kernel, while they used two separate classifiers to identify entities and relations and their output is later combined with probabilistic global inference.</citsent>
<aftsection>
<nextsent>we evaluated our relation extraction algorithm on two biomedical datasets (i.e. the aimed corpus and the lll challenge data set; see section 4).
</nextsent>
<nextsent>the motivations for using these benchmarks derive from the increasing applicative interest intools able to extract relations between relevant entities in biomedical texts and, consequently, from the growing availability of annotated datasets.
</nextsent>
<nextsent>the experiments show clearly that our approach consistently improves previous results.
</nextsent>
<nextsent>surprisingly, it outperforms most of the systems based on syntactic or semantic information, even when this information is manually annotated (i.e. the lll challenge).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4379">
<title id=" E06-1051.xml">exploiting shallow linguistic information for relation extraction from biomedical literature </title>
<section> kernel methods for relation.  </section>
<citcontext>
<prevsection>
<prevsent>in our approach we use support vector machines (vapnik, 1998).
</prevsent>
<prevsent>in order to implement the approach based on shallow linguistic information we employed linear combination of kernels.
</prevsent>
</prevsection>
<citsent citstr=" P05-1050 ">
different works (gliozzo et al, 2005; <papid> P05-1050 </papid>zhao and grishman, 2005;<papid> P05-1052 </papid>culotta and sorensen, 2004) <papid> P04-1054 </papid>empirically demonstrate the effectiveness of combining kernels in this way, showing that the combined kernel always improves the performance of the individual ones.in addition, this formulation allows us to evaluate the individual contribution of each information source.</citsent>
<aftsection>
<nextsent>we designed two families of kernels: global context kernels and local context kernels, in which each single kernel is explicitly calculated as follows k(x1, x2) = ??(x1), ?(x2)?
</nextsent>
<nextsent>??(x1)???(x2)?
</nextsent>
<nextsent>, (1) where ?(?)
</nextsent>
<nextsent>is the embedding vector and ? ?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4428">
<title id=" E89-1038.xml">a new view on the process of translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a partial implementation of the grammat-ical stratum of organisation found in systemic func-tional grammar (sfg) provides the core of penman linguistic capabilities (mann and matthiessen, 1985), whereas there is strong input from sfg in these- mantic interpretation of et-d dependency struc-tures (steiner, schmidt and zelinsky-wibbelt, 1988).
</prevsent>
<prevsent>it is therefore also one of the motivations of this co-operation to investigate the potential of sfg as tool for transfer in machine translation mt, and in the wider context of systemic-functional linguistics also as theoretical environment and as formalism forex- pressing semantics.
</prevsent>
</prevsection>
<citsent citstr=" C88-1063 ">
this should be of interest to wider audience within computational linguistics, espe-cially as sfg has recently been attracting an increas-ing amount of interest in the field (see, e.g.: houghton and isard, 1987; kasper, 1988; <papid> C88-1063 </papid>patten, 1988; patten and ritchie, 1987; mellish, 1988; <papid> J88-1004 </papid>paris and bateman, 1989).</citsent>
<aftsection>
<nextsent>2.1 eurot ra -d ana lys s module.
</nextsent>
<nextsent>the german analysis module of our proposed mt sys-tem is based on the eurotra engineering framework (bech and nygaard, 1988) <papid> C88-1008 </papid>enhanced by semantic component derived from systemic theory.</nextsent>
<nextsent>1 the gen-eral eurdtra philosophy for translation is described elsewhere (arnold et al, 1986, <papid> C86-1071 </papid>1987).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4429">
<title id=" E89-1038.xml">a new view on the process of translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a partial implementation of the grammat-ical stratum of organisation found in systemic func-tional grammar (sfg) provides the core of penman linguistic capabilities (mann and matthiessen, 1985), whereas there is strong input from sfg in these- mantic interpretation of et-d dependency struc-tures (steiner, schmidt and zelinsky-wibbelt, 1988).
</prevsent>
<prevsent>it is therefore also one of the motivations of this co-operation to investigate the potential of sfg as tool for transfer in machine translation mt, and in the wider context of systemic-functional linguistics also as theoretical environment and as formalism forex- pressing semantics.
</prevsent>
</prevsection>
<citsent citstr=" J88-1004 ">
this should be of interest to wider audience within computational linguistics, espe-cially as sfg has recently been attracting an increas-ing amount of interest in the field (see, e.g.: houghton and isard, 1987; kasper, 1988; <papid> C88-1063 </papid>patten, 1988; patten and ritchie, 1987; mellish, 1988; <papid> J88-1004 </papid>paris and bateman, 1989).</citsent>
<aftsection>
<nextsent>2.1 eurot ra -d ana lys s module.
</nextsent>
<nextsent>the german analysis module of our proposed mt sys-tem is based on the eurotra engineering framework (bech and nygaard, 1988) <papid> C88-1008 </papid>enhanced by semantic component derived from systemic theory.</nextsent>
<nextsent>1 the gen-eral eurdtra philosophy for translation is described elsewhere (arnold et al, 1986, <papid> C86-1071 </papid>1987).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4430">
<title id=" E89-1038.xml">a new view on the process of translation </title>
<section> the projects involved.  </section>
<citcontext>
<prevsection>
<prevsent>this should be of interest to wider audience within computational linguistics, espe-cially as sfg has recently been attracting an increas-ing amount of interest in the field (see, e.g.: houghton and isard, 1987; kasper, 1988; <papid> C88-1063 </papid>patten, 1988; patten and ritchie, 1987; mellish, 1988; <papid> J88-1004 </papid>paris and bateman, 1989).</prevsent>
<prevsent>2.1 eurot ra -d ana lys s modu le.</prevsent>
</prevsection>
<citsent citstr=" C88-1008 ">
the german analysis module of our proposed mt sys-tem is based on the eurotra engineering framework (bech and nygaard, 1988) <papid> C88-1008 </papid>enhanced by semantic component derived from systemic theory.</citsent>
<aftsection>
<nextsent>1 the gen-eral eurdtra philosophy for translation is described elsewhere (arnold et al, 1986, <papid> C86-1071 </papid>1987).</nextsent>
<nextsent>the essentials of the eurotra-d approach are to be found in steiner, schmidt, and zelinsky-wibbelt (1988).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4431">
<title id=" E89-1038.xml">a new view on the process of translation </title>
<section> the projects involved.  </section>
<citcontext>
<prevsection>
<prevsent>2.1 eurot ra -d ana lys s module.
</prevsent>
<prevsent>the german analysis module of our proposed mt sys-tem is based on the eurotra engineering framework (bech and nygaard, 1988) <papid> C88-1008 </papid>enhanced by semantic component derived from systemic theory.</prevsent>
</prevsection>
<citsent citstr=" C86-1071 ">
1 the gen-eral eurdtra philosophy for translation is described elsewhere (arnold et al, 1986, <papid> C86-1071 </papid>1987).</citsent>
<aftsection>
<nextsent>the essentials of the eurotra-d approach are to be found in steiner, schmidt, and zelinsky-wibbelt (1988).
</nextsent>
<nextsent>the eurotra system is transfer-based multi-lingual mt-system.
</nextsent>
<nextsent>it is stratificational in the sense that analysis and syn-thesis proceed through two syntactic levels (configu- rational and functional) and one semantic level, called the interface structure (is).
</nextsent>
<nextsent>these interface represen-tations are semantically interpreted dependency struc- tures; they are described in more detail in section 3.3.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4432">
<title id=" E89-1038.xml">a new view on the process of translation </title>
<section> the projects involved.  </section>
<citcontext>
<prevsection>
<prevsent>the upper model is typically used to mediate between the organisation of knowledge found in an ap-plication domain and the kind of organisation that is most convenient for implementing the grammar in-quiries.
</prevsent>
<prevsent>we have made crucial use of the upper model in constructing our combination of the two compo-nents.
</prevsent>
</prevsection>
<citsent citstr=" H89-1022 ">
in effect, the upper model can often mediate between the results of the mt analysis, expressed in et-d interface structures, and the input that must be specified for penman, expressed in the penman sen-tence plan language (spl) (kasper, 1989).<papid> H89-1022 </papid></citsent>
<aftsection>
<nextsent>each of these information sources, the upper model, the pen-man spl, and the et-d interface structures will now be described in detail.
</nextsent>
<nextsent>german-eng i sh inter face 3.1 penman upper model.
</nextsent>
<nextsent>perhaps the crucial task for text generation is to be able to control linguistic resources as to make the generated text conform to what is to be expressed.
</nextsent>
<nextsent>in penman this is the responsibility of the grammar inquiry semantics.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4434">
<title id=" E89-1038.xml">a new view on the process of translation </title>
<section> components  f  the.  </section>
<citcontext>
<prevsection>
<prevsent>in the translation pair represented by figures 1 and 2, for example, we can see that the is s~mantic feature top=yes indicating thematic promi-nence have been transferred into the inquiry response specification :circumstantial-theme-q(s9 h1) context.
</prevsent>
<prevsent>this calls for the grammar to prepose the constituent realising $9, i.e. the since-clause, into sentence-initial thematic position, rather than letting it appear later in the sentence as it would when non-thematic.
</prevsent>
</prevsection>
<citsent citstr=" C86-1026 ">
the function of predicate-argument structures, es-pecially in connection with semantic asls is another interesting research topic (as suggested by somers (1986) <papid> C86-1026 </papid>which can be addressed in the present con-text, especially as the two components involved share their essential notions of predicate-argument struc-tures from systemic linguistics.</citsent>
<aftsection>
<nextsent>our first translations in this research environment are still sentence-based; however, in the longer term we will concentrate our research interests on issues con-cerning text structure.
</nextsent>
<nextsent>the penman group intends to enhance the penman environment to the interpersonal and textual meta functions of sfg.
</nextsent>
<nextsent>although these ex-tensions will be made primarily for text generation they should be of interest also for the design of text- based mt-analysis.
</nextsent>
<nextsent>in summary, then, we have introduced the projects involved, and the structure of the german-engllsh transfer mechanism, offering specific examples of the transfer process for some of the features present in the is analysis.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4435">
<title id=" E12-2003.xml">collaborative machine translation service for scientific texts </title>
<section> translation of scientific texts.  </section>
<citcontext>
<prevsection>
<prevsent>the generic system was mostly trained on data provided for the shared task ofsixth workshop on statistical machine transla tion6 (wmt 2011), described in table 2.
</prevsent>
<prevsent>table 3 presents results showing, in theenglishfrench direction, the impact on the statistical engine of introducing the resources extracted from hal, as well as the impact of domain adaptation techniques.
</prevsent>
</prevsection>
<citsent citstr=" P07-2045 ">
the baseline statistical engine is standard pbsmt system based on moses (koehn et al 2007) <papid> P07-2045 </papid>and the srilm tookit (stolcke, 2002).</citsent>
<aftsection>
<nextsent>is was trained and tuned only on wmt11 data (out-of-domain).
</nextsent>
<nextsent>incorporating the hal data into the language model and tuning the system on the hal development set, 6http://www.statmt.org/wmt11/translation-task.html set domain lg sent.
</nextsent>
<nextsent>words vocab.
</nextsent>
<nextsent>parallel data train cs+phys en 55.9 1.41 43.3 fr 55.9 1.63 47.9 dev cs en 1100 25.8 4.6 fr 1100 28.7 5.1 phys en 1000 26.1 5.1 fr 1000 29.1 5.6 test cs en 1100 26.1 4.6 fr 1100 29.2 5.2 phys en 1000 25.9 5.1 fr 1000 28.8 5.5 monolingual data train cs en 2.5 54 457 fr 761 19 274 phys en 2.1 50 646 fr 662 17 292 ktable 1: statistics for the parallel training, development, and test datasets extracted from thesis abstracts contained in hal, as well as monolingual data extracted from all documents in hal, in computer science (cs) and physics (phys).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4436">
<title id=" E12-1034.xml">skip ngrams and ranking functions for predicting script events </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we make several contributions, introducingskip-grams for collecting event statistics, designing improved methods for ranking event predictions, defining more reliable evaluation metric for measuring predictive ness, and providing systematic analysis of the various event prediction models.
</prevsent>
<prevsent>there has been recent interest in automatically acquiring world knowledge in the form of scripts (schank and abelson, 1977), that is, frequently recurring situations that have stereotypical sequence of events, such as visit to restaurant.
</prevsent>
</prevsection>
<citsent citstr=" P08-1090 ">
all of the techniques so far proposed for this task share common sub-task: given an event or partial chain of events, predict other events that belong to the same script (chambers and jurafsky, 2008;<papid> P08-1090 </papid>chambers and jurafsky, 2009; <papid> P09-1068 </papid>chambers and jurafsky, 2011; <papid> P11-1098 </papid>manshadi et al 2008; mcintyre and lapata, 2009; <papid> P09-1025 </papid>mcintyre and lapata, 2010; <papid> P10-1158 </papid>regneri et al 2010).<papid> P10-1100 </papid></citsent>
<aftsection>
<nextsent>such model can then serve as input to system that identifies the order of the events within that script (chambers and jurafsky, 2008;<papid> P08-1090 </papid> chambers and jurafsky, 2009) <papid> P09-1068 </papid>or that generates story using the selected events (mcintyre and lapata, 2009; <papid> P09-1025 </papid>mcintyre and lapata, 2010).<papid> P10-1158 </papid>in this article, we analyze and compare techniques for constructing models that, given partial chain of events, predict other events that belong to the script.</nextsent>
<nextsent>in particular, we consider the following questions: ? how should representative chains of events be selected from the source text?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4437">
<title id=" E12-1034.xml">skip ngrams and ranking functions for predicting script events </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we make several contributions, introducingskip-grams for collecting event statistics, designing improved methods for ranking event predictions, defining more reliable evaluation metric for measuring predictive ness, and providing systematic analysis of the various event prediction models.
</prevsent>
<prevsent>there has been recent interest in automatically acquiring world knowledge in the form of scripts (schank and abelson, 1977), that is, frequently recurring situations that have stereotypical sequence of events, such as visit to restaurant.
</prevsent>
</prevsection>
<citsent citstr=" P09-1068 ">
all of the techniques so far proposed for this task share common sub-task: given an event or partial chain of events, predict other events that belong to the same script (chambers and jurafsky, 2008;<papid> P08-1090 </papid>chambers and jurafsky, 2009; <papid> P09-1068 </papid>chambers and jurafsky, 2011; <papid> P11-1098 </papid>manshadi et al 2008; mcintyre and lapata, 2009; <papid> P09-1025 </papid>mcintyre and lapata, 2010; <papid> P10-1158 </papid>regneri et al 2010).<papid> P10-1100 </papid></citsent>
<aftsection>
<nextsent>such model can then serve as input to system that identifies the order of the events within that script (chambers and jurafsky, 2008;<papid> P08-1090 </papid> chambers and jurafsky, 2009) <papid> P09-1068 </papid>or that generates story using the selected events (mcintyre and lapata, 2009; <papid> P09-1025 </papid>mcintyre and lapata, 2010).<papid> P10-1158 </papid>in this article, we analyze and compare techniques for constructing models that, given partial chain of events, predict other events that belong to the script.</nextsent>
<nextsent>in particular, we consider the following questions: ? how should representative chains of events be selected from the source text?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4439">
<title id=" E12-1034.xml">skip ngrams and ranking functions for predicting script events </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we make several contributions, introducingskip-grams for collecting event statistics, designing improved methods for ranking event predictions, defining more reliable evaluation metric for measuring predictive ness, and providing systematic analysis of the various event prediction models.
</prevsent>
<prevsent>there has been recent interest in automatically acquiring world knowledge in the form of scripts (schank and abelson, 1977), that is, frequently recurring situations that have stereotypical sequence of events, such as visit to restaurant.
</prevsent>
</prevsection>
<citsent citstr=" P11-1098 ">
all of the techniques so far proposed for this task share common sub-task: given an event or partial chain of events, predict other events that belong to the same script (chambers and jurafsky, 2008;<papid> P08-1090 </papid>chambers and jurafsky, 2009; <papid> P09-1068 </papid>chambers and jurafsky, 2011; <papid> P11-1098 </papid>manshadi et al 2008; mcintyre and lapata, 2009; <papid> P09-1025 </papid>mcintyre and lapata, 2010; <papid> P10-1158 </papid>regneri et al 2010).<papid> P10-1100 </papid></citsent>
<aftsection>
<nextsent>such model can then serve as input to system that identifies the order of the events within that script (chambers and jurafsky, 2008;<papid> P08-1090 </papid> chambers and jurafsky, 2009) <papid> P09-1068 </papid>or that generates story using the selected events (mcintyre and lapata, 2009; <papid> P09-1025 </papid>mcintyre and lapata, 2010).<papid> P10-1158 </papid>in this article, we analyze and compare techniques for constructing models that, given partial chain of events, predict other events that belong to the script.</nextsent>
<nextsent>in particular, we consider the following questions: ? how should representative chains of events be selected from the source text?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4440">
<title id=" E12-1034.xml">skip ngrams and ranking functions for predicting script events </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we make several contributions, introducingskip-grams for collecting event statistics, designing improved methods for ranking event predictions, defining more reliable evaluation metric for measuring predictive ness, and providing systematic analysis of the various event prediction models.
</prevsent>
<prevsent>there has been recent interest in automatically acquiring world knowledge in the form of scripts (schank and abelson, 1977), that is, frequently recurring situations that have stereotypical sequence of events, such as visit to restaurant.
</prevsent>
</prevsection>
<citsent citstr=" P09-1025 ">
all of the techniques so far proposed for this task share common sub-task: given an event or partial chain of events, predict other events that belong to the same script (chambers and jurafsky, 2008;<papid> P08-1090 </papid>chambers and jurafsky, 2009; <papid> P09-1068 </papid>chambers and jurafsky, 2011; <papid> P11-1098 </papid>manshadi et al 2008; mcintyre and lapata, 2009; <papid> P09-1025 </papid>mcintyre and lapata, 2010; <papid> P10-1158 </papid>regneri et al 2010).<papid> P10-1100 </papid></citsent>
<aftsection>
<nextsent>such model can then serve as input to system that identifies the order of the events within that script (chambers and jurafsky, 2008;<papid> P08-1090 </papid> chambers and jurafsky, 2009) <papid> P09-1068 </papid>or that generates story using the selected events (mcintyre and lapata, 2009; <papid> P09-1025 </papid>mcintyre and lapata, 2010).<papid> P10-1158 </papid>in this article, we analyze and compare techniques for constructing models that, given partial chain of events, predict other events that belong to the script.</nextsent>
<nextsent>in particular, we consider the following questions: ? how should representative chains of events be selected from the source text?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4445">
<title id=" E12-1034.xml">skip ngrams and ranking functions for predicting script events </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we make several contributions, introducingskip-grams for collecting event statistics, designing improved methods for ranking event predictions, defining more reliable evaluation metric for measuring predictive ness, and providing systematic analysis of the various event prediction models.
</prevsent>
<prevsent>there has been recent interest in automatically acquiring world knowledge in the form of scripts (schank and abelson, 1977), that is, frequently recurring situations that have stereotypical sequence of events, such as visit to restaurant.
</prevsent>
</prevsection>
<citsent citstr=" P10-1158 ">
all of the techniques so far proposed for this task share common sub-task: given an event or partial chain of events, predict other events that belong to the same script (chambers and jurafsky, 2008;<papid> P08-1090 </papid>chambers and jurafsky, 2009; <papid> P09-1068 </papid>chambers and jurafsky, 2011; <papid> P11-1098 </papid>manshadi et al 2008; mcintyre and lapata, 2009; <papid> P09-1025 </papid>mcintyre and lapata, 2010; <papid> P10-1158 </papid>regneri et al 2010).<papid> P10-1100 </papid></citsent>
<aftsection>
<nextsent>such model can then serve as input to system that identifies the order of the events within that script (chambers and jurafsky, 2008;<papid> P08-1090 </papid> chambers and jurafsky, 2009) <papid> P09-1068 </papid>or that generates story using the selected events (mcintyre and lapata, 2009; <papid> P09-1025 </papid>mcintyre and lapata, 2010).<papid> P10-1158 </papid>in this article, we analyze and compare techniques for constructing models that, given partial chain of events, predict other events that belong to the script.</nextsent>
<nextsent>in particular, we consider the following questions: ? how should representative chains of events be selected from the source text?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4446">
<title id=" E12-1034.xml">skip ngrams and ranking functions for predicting script events </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we make several contributions, introducingskip-grams for collecting event statistics, designing improved methods for ranking event predictions, defining more reliable evaluation metric for measuring predictive ness, and providing systematic analysis of the various event prediction models.
</prevsent>
<prevsent>there has been recent interest in automatically acquiring world knowledge in the form of scripts (schank and abelson, 1977), that is, frequently recurring situations that have stereotypical sequence of events, such as visit to restaurant.
</prevsent>
</prevsection>
<citsent citstr=" P10-1100 ">
all of the techniques so far proposed for this task share common sub-task: given an event or partial chain of events, predict other events that belong to the same script (chambers and jurafsky, 2008;<papid> P08-1090 </papid>chambers and jurafsky, 2009; <papid> P09-1068 </papid>chambers and jurafsky, 2011; <papid> P11-1098 </papid>manshadi et al 2008; mcintyre and lapata, 2009; <papid> P09-1025 </papid>mcintyre and lapata, 2010; <papid> P10-1158 </papid>regneri et al 2010).<papid> P10-1100 </papid></citsent>
<aftsection>
<nextsent>such model can then serve as input to system that identifies the order of the events within that script (chambers and jurafsky, 2008;<papid> P08-1090 </papid> chambers and jurafsky, 2009) <papid> P09-1068 </papid>or that generates story using the selected events (mcintyre and lapata, 2009; <papid> P09-1025 </papid>mcintyre and lapata, 2010).<papid> P10-1158 </papid>in this article, we analyze and compare techniques for constructing models that, given partial chain of events, predict other events that belong to the script.</nextsent>
<nextsent>in particular, we consider the following questions: ? how should representative chains of events be selected from the source text?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4488">
<title id=" E12-1034.xml">skip ngrams and ranking functions for predicting script events </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>after removing these files, the reuters corpus was reduced to 788, 245 files.
</prevsent>
<prevsent>removing files from the fairytale corpus was not necessary ? all 437 stories were retained.
</prevsent>
</prevsection>
<citsent citstr=" P03-1054 ">
we then applied the stanford parser (klein and manning, 2003) <papid> P03-1054 </papid>to identify the dependency structure of each sentence in each article in the corpus.</citsent>
<aftsection>
<nextsent>this parser produces constitutent-based syntactic parse tree for each sentence, and then converts this tree to collapsed dependency structure via set of tree patterns.next we applied the opennlp coreference en gine5 to identify the entities in each article, and the noun phrases that were mentions of each entity.
</nextsent>
<nextsent>finally, to identify the event chains, we took each of the entities proposed by the coreference system, walked through each of the noun phrases associated with that entity, retrieved any subject 4http://www.mythfolklore.net/andrewlang/ 5http://incubator.apache.org/opennlp/ 340 or object dependencies that linked verb to that noun phrase, and created an event chain from the sequence of (verb, dependency-type) tuples in the order that they appeared in the text.
</nextsent>
<nextsent>4.3 evaluation metrics.
</nextsent>
<nextsent>we follow the approach of chambers and jurafsky (2008), <papid> P08-1090 </papid>evaluating our models for predicting script events in narrative cloze task.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4495">
<title id=" E09-2002.xml">cbseas a summarization system x2013 integration of opinion mining techniques to summarize blogs </title>
<section> related works.  </section>
<citcontext>
<prevsection>
<prevsent>in graph where nodes are sentences and edges are similarities.
</prevsent>
<prevsent>sentence selection is then performed by picking the sentences which have been visited most after random walk on the graph.the last two systems are dealing with redundancy as post-processing step.
</prevsent>
</prevsection>
<citsent citstr=" N07-1013 ">
(zhu et al, 2007), <papid> N07-1013 </papid>assuming that redundancy should be the concept on what is based multi-document summarization, offered method to deal with redundancy at the 5 same time as sentence selection.</citsent>
<aftsection>
<nextsent>for that purpose,the authors used markov absorbing chain random walk?
</nextsent>
<nextsent>on graph representing the different sentences of the corpus to summarize.mmr-md, introduced by carbon nel in (car bonell and goldstein, 1998), is measure which needs passage clustering: all passages considered as synonyms are grouped into the same clusters.
</nextsent>
<nextsent>mmr-md takes into account the similarity to query, coverage of passage (clusters that it belongs to), content in the passage, similarity to passages already selected for the summary, belonging to cluster or to document that has already contributed passage to the summary.the problem of this measure lies in the clustering method: in the literature, clustering is generally fulfilled using threshold.
</nextsent>
<nextsent>if passage has similarity to cluster centro id higher than threshold, then it is added to this cluster.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4496">
<title id=" E09-2002.xml">cbseas a summarization system x2013 integration of opinion mining techniques to summarize blogs </title>
<section> tac 2008: the opinion.  </section>
<citcontext>
<prevsection>
<prevsent>these summaries are oriented by complex user queries, such as why do people like.....??
</prevsent>
<prevsent>or why do people prefer... to...??.
</prevsent>
</prevsection>
<citsent citstr=" N06-1059 ">
the results were analyzed manually, using the pyramid method (lin et al, 2006): <papid> N06-1059 </papid>the pyramid score of summary depends on the number of simple semantic units, units considered as important by the annotators.</citsent>
<aftsection>
<nextsent>the tac evaluation for this task also included grammaticality, non redundancy, structure/coherence and overall fluency scores.
</nextsent>
<nextsent>summarization task blog summarization is very different from anew swire article or scientific paper summarization.
</nextsent>
<nextsent>linguistic quality as well as reasoning structure are variable from one blogger to another.
</nextsent>
<nextsent>we cannot use generalities on blog structure, neither on linguistic markers to improve our summarization system.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4497">
<title id=" E09-1099.xml">language id in the context of harvesting language data off the web </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>since in ter linear examples consist of ortho graphically or phonetic ally encoded language data aligned with an english translation, the corpus?
</prevsent>
<prevsent>of inter linear examples found on the web, when taken together,constitute significant multilingual, parallel corpus covering hundreds to thousands of the worlds languages.
</prevsent>
</prevsection>
<citsent citstr=" N07-1057 ">
previous work has discussed methods for harvesting inter linear text off the web (lewis, 2006), enriching it via structural projections (xia and lewis, 2007), <papid> N07-1057 </papid>and even making it available to typo logical analyses (lewis and xia, 2008) <papid> I08-2093 </papid>and search (xia and lewis, 2008).<papid> I08-1069 </papid></citsent>
<aftsection>
<nextsent>one challenge with harvesting inter linear dataoff the web is language identification of the harvested data.
</nextsent>
<nextsent>there have been extensive studies on language identification (language id) of written text, and review of previous research on this topic can be found in (hughes et al, 2006).
</nextsent>
<nextsent>in general, language id method requires collection of text for training, something on the order of thousand or more characters.
</nextsent>
<nextsent>these methods work well for languages with rich language resources; for instance, cavnar and trenkles n-gram-based algorithm achieved an accuracy as high as 99.8% when tested on news group articles across eight languages (cavnar and trenkle, 1994).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4498">
<title id=" E09-1099.xml">language id in the context of harvesting language data off the web </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>since in ter linear examples consist of ortho graphically or phonetic ally encoded language data aligned with an english translation, the corpus?
</prevsent>
<prevsent>of inter linear examples found on the web, when taken together,constitute significant multilingual, parallel corpus covering hundreds to thousands of the worlds languages.
</prevsent>
</prevsection>
<citsent citstr=" I08-2093 ">
previous work has discussed methods for harvesting inter linear text off the web (lewis, 2006), enriching it via structural projections (xia and lewis, 2007), <papid> N07-1057 </papid>and even making it available to typo logical analyses (lewis and xia, 2008) <papid> I08-2093 </papid>and search (xia and lewis, 2008).<papid> I08-1069 </papid></citsent>
<aftsection>
<nextsent>one challenge with harvesting inter linear dataoff the web is language identification of the harvested data.
</nextsent>
<nextsent>there have been extensive studies on language identification (language id) of written text, and review of previous research on this topic can be found in (hughes et al, 2006).
</nextsent>
<nextsent>in general, language id method requires collection of text for training, something on the order of thousand or more characters.
</nextsent>
<nextsent>these methods work well for languages with rich language resources; for instance, cavnar and trenkles n-gram-based algorithm achieved an accuracy as high as 99.8% when tested on news group articles across eight languages (cavnar and trenkle, 1994).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4499">
<title id=" E09-1099.xml">language id in the context of harvesting language data off the web </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>since in ter linear examples consist of ortho graphically or phonetic ally encoded language data aligned with an english translation, the corpus?
</prevsent>
<prevsent>of inter linear examples found on the web, when taken together,constitute significant multilingual, parallel corpus covering hundreds to thousands of the worlds languages.
</prevsent>
</prevsection>
<citsent citstr=" I08-1069 ">
previous work has discussed methods for harvesting inter linear text off the web (lewis, 2006), enriching it via structural projections (xia and lewis, 2007), <papid> N07-1057 </papid>and even making it available to typo logical analyses (lewis and xia, 2008) <papid> I08-2093 </papid>and search (xia and lewis, 2008).<papid> I08-1069 </papid></citsent>
<aftsection>
<nextsent>one challenge with harvesting inter linear dataoff the web is language identification of the harvested data.
</nextsent>
<nextsent>there have been extensive studies on language identification (language id) of written text, and review of previous research on this topic can be found in (hughes et al, 2006).
</nextsent>
<nextsent>in general, language id method requires collection of text for training, something on the order of thousand or more characters.
</nextsent>
<nextsent>these methods work well for languages with rich language resources; for instance, cavnar and trenkles n-gram-based algorithm achieved an accuracy as high as 99.8% when tested on news group articles across eight languages (cavnar and trenkle, 1994).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4501">
<title id=" E09-1099.xml">language id in the context of harvesting language data off the web </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>odin is valuable resource for linguists, as itcan be searched for igts that belong to particular language or language family, or those that contain particular linguistic construction (e.g., passive, wh-movement).
</prevsent>
<prevsent>in addition, there have 1http://odin.linguistlist.orgbeen some preliminary studies that show the benefits of using the resource for nlp.
</prevsent>
</prevsection>
<citsent citstr=" P06-1111 ">
for instance, our previous work shows that automatically enrichedigt data can be used to answer typo logical questions (e.g., the canonical word order of language) with high accuracy (lewis and xia, 2008), <papid> I08-2093 </papid>andthe information could serve as prototypes for prototype learning (haghighi and klein, 2006).<papid> P06-1111 </papid></citsent>
<aftsection>
<nextsent>as the size of odin increases dramatically, it is crucial to have reliable module that automatically identifies the correct language code for eachnew extracted igt to be added to odin.
</nextsent>
<nextsent>the current odin system uses two language identifiers: one is based on simple heuristics, and the otheron cavnar and trenkles algorithm (1994).
</nextsent>
<nextsent>how ever, because the task here is very different froma typical language id task (see below), both algorithms work poorly, with accuracy falling below 55%.
</nextsent>
<nextsent>the focus of this paper is on building new language identifiers with much higher accuracy.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4502">
<title id=" E09-1099.xml">language id in the context of harvesting language data off the web </title>
<section> formulating the language id task.  </section>
<citcontext>
<prevsection>
<prevsent>igt instances are like pronouns in that they often refer to language names appearing in the neighborhood.
</prevsent>
<prevsent>once the language id task is framed as coref problem, all the existing algorithms on coref can be applied to the task, as discussed below.
</prevsent>
</prevsection>
<citsent citstr=" J01-4004 ">
5.2.1 sequence labeling using traditional classifiers one common approach to the coref problem processes the mentions sequentially and determine for each mention whether it should start new entity or be linked to an existing mention (e.g., (soon et al, 2001; <papid> J01-4004 </papid>ng and cardie, 2002; <papid> P02-1014 </papid>luo, 2007)); <papid> N07-1010 </papid>that is, the approach makes series of decisions, 8there are minor differences between the language id and coreference resolution tasks.</citsent>
<aftsection>
<nextsent>for instance, each entity in the language id task must be assigned language code.
</nextsent>
<nextsent>this means that ambiguous language names will evoke multiple entities, each with different language code.
</nextsent>
<nextsent>these differences are reflected in our algorithms.one decision per (mention, entity) pair.
</nextsent>
<nextsent>applying this to the language id task, the (mention, en tity) pair would correspond to an (igt, lang code)pair, and each decision would have two possibili ties: same when the igt belongs to the language or diff when the igt does not.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4503">
<title id=" E09-1099.xml">language id in the context of harvesting language data off the web </title>
<section> formulating the language id task.  </section>
<citcontext>
<prevsection>
<prevsent>igt instances are like pronouns in that they often refer to language names appearing in the neighborhood.
</prevsent>
<prevsent>once the language id task is framed as coref problem, all the existing algorithms on coref can be applied to the task, as discussed below.
</prevsent>
</prevsection>
<citsent citstr=" P02-1014 ">
5.2.1 sequence labeling using traditional classifiers one common approach to the coref problem processes the mentions sequentially and determine for each mention whether it should start new entity or be linked to an existing mention (e.g., (soon et al, 2001; <papid> J01-4004 </papid>ng and cardie, 2002; <papid> P02-1014 </papid>luo, 2007)); <papid> N07-1010 </papid>that is, the approach makes series of decisions, 8there are minor differences between the language id and coreference resolution tasks.</citsent>
<aftsection>
<nextsent>for instance, each entity in the language id task must be assigned language code.
</nextsent>
<nextsent>this means that ambiguous language names will evoke multiple entities, each with different language code.
</nextsent>
<nextsent>these differences are reflected in our algorithms.one decision per (mention, entity) pair.
</nextsent>
<nextsent>applying this to the language id task, the (mention, en tity) pair would correspond to an (igt, lang code)pair, and each decision would have two possibili ties: same when the igt belongs to the language or diff when the igt does not.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4504">
<title id=" E09-1099.xml">language id in the context of harvesting language data off the web </title>
<section> formulating the language id task.  </section>
<citcontext>
<prevsection>
<prevsent>igt instances are like pronouns in that they often refer to language names appearing in the neighborhood.
</prevsent>
<prevsent>once the language id task is framed as coref problem, all the existing algorithms on coref can be applied to the task, as discussed below.
</prevsent>
</prevsection>
<citsent citstr=" N07-1010 ">
5.2.1 sequence labeling using traditional classifiers one common approach to the coref problem processes the mentions sequentially and determine for each mention whether it should start new entity or be linked to an existing mention (e.g., (soon et al, 2001; <papid> J01-4004 </papid>ng and cardie, 2002; <papid> P02-1014 </papid>luo, 2007)); <papid> N07-1010 </papid>that is, the approach makes series of decisions, 8there are minor differences between the language id and coreference resolution tasks.</citsent>
<aftsection>
<nextsent>for instance, each entity in the language id task must be assigned language code.
</nextsent>
<nextsent>this means that ambiguous language names will evoke multiple entities, each with different language code.
</nextsent>
<nextsent>these differences are reflected in our algorithms.one decision per (mention, entity) pair.
</nextsent>
<nextsent>applying this to the language id task, the (mention, en tity) pair would correspond to an (igt, lang code)pair, and each decision would have two possibili ties: same when the igt belongs to the language or diff when the igt does not.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4505">
<title id=" E09-1099.xml">language id in the context of harvesting language data off the web </title>
<section> formulating the language id task.  </section>
<citcontext>
<prevsection>
<prevsent>igt1-bin same nearlc prev50 lmw1 lmm1 ... igt1-lew diff nearlc prev50 ... igt1-thp diff prev50 ...
</prevsent>
<prevsent>... igt2-bin same nearlc prev50 lmw1 lmm1 iiw1 ... igt2-lew diff nearlc prev50 ... igt2-thp diff prev50 ...
</prevsent>
</prevsection>
<citsent citstr=" N07-1030 ">
table 5: feature vectors for the igts in table 1when using the coref approach with sequence labeling methods 5.2.2 joint inference using markov logic recently, joint inference has become topic of keen interests in both the machine learning andnlp communities (e.g., (bakir et al, 2007; sutton et al, 2006; poon and domingos, 2007)).there have been increasing interests in formulating coreference resolution in joint model and conducting joint inference to leverage dependen 874cies among the mentions and entities (e.g., (well ner et al, 2004; denis and baldridge, 2007; <papid> N07-1030 </papid>poon and domingos, 2008)).<papid> D08-1068 </papid></citsent>
<aftsection>
<nextsent>we have built joint model for language id in markov logic (richard son and domingos, 2006).
</nextsent>
<nextsent>markov logic is probabilistic extension offirst-order logic that makes it possible to compactly specify probability distributions over complex relational domains.
</nextsent>
<nextsent>a markov logic network (mln) is set of weighted first-orderclauses.
</nextsent>
<nextsent>together with set of constants, it defines markov network with one node per ground atom and one feature per ground clause.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4506">
<title id=" E09-1099.xml">language id in the context of harvesting language data off the web </title>
<section> formulating the language id task.  </section>
<citcontext>
<prevsection>
<prevsent>igt1-bin same nearlc prev50 lmw1 lmm1 ... igt1-lew diff nearlc prev50 ... igt1-thp diff prev50 ...
</prevsent>
<prevsent>... igt2-bin same nearlc prev50 lmw1 lmm1 iiw1 ... igt2-lew diff nearlc prev50 ... igt2-thp diff prev50 ...
</prevsent>
</prevsection>
<citsent citstr=" D08-1068 ">
table 5: feature vectors for the igts in table 1when using the coref approach with sequence labeling methods 5.2.2 joint inference using markov logic recently, joint inference has become topic of keen interests in both the machine learning andnlp communities (e.g., (bakir et al, 2007; sutton et al, 2006; poon and domingos, 2007)).there have been increasing interests in formulating coreference resolution in joint model and conducting joint inference to leverage dependen 874cies among the mentions and entities (e.g., (well ner et al, 2004; denis and baldridge, 2007; <papid> N07-1030 </papid>poon and domingos, 2008)).<papid> D08-1068 </papid></citsent>
<aftsection>
<nextsent>we have built joint model for language id in markov logic (richard son and domingos, 2006).
</nextsent>
<nextsent>markov logic is probabilistic extension offirst-order logic that makes it possible to compactly specify probability distributions over complex relational domains.
</nextsent>
<nextsent>a markov logic network (mln) is set of weighted first-orderclauses.
</nextsent>
<nextsent>together with set of constants, it defines markov network with one node per ground atom and one feature per ground clause.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4507">
<title id=" E06-1043.xml">automatically constructing a lexicon of verb phrase idiomatic combinations </title>
<section> automatic recognition of vnics.  </section>
<citcontext>
<prevsection>
<prevsent>this approach has two main chal lenges: (i) it requires prior knowledge about the idiomaticity of expressions (which is what we are developing our measure to determine); (ii) it needs information on similarity?
</prevsent>
<prevsent>among words.
</prevsent>
</prevsection>
<citsent citstr=" P99-1041 ">
inspired by lin (1999), <papid> P99-1041 </papid>we examine the strength of association between the verb and noun constituents of the target combination and its variants, as an indirect cueto their idiomaticity.</citsent>
<aftsection>
<nextsent>we use the automatically-built thesaurus of lin (1998) <papid> P98-2127 </papid>to find similar words to the noun of the target expression, in order to automatically generate variants.</nextsent>
<nextsent>only the noun constituent is varied, since replacing theverb constituent of vnic with semantically related verb is more likely to yield another vnic, as in keep/lose ones cool (nunberg et al , 1994).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4508">
<title id=" E06-1043.xml">automatically constructing a lexicon of verb phrase idiomatic combinations </title>
<section> automatic recognition of vnics.  </section>
<citcontext>
<prevsection>
<prevsent>among words.
</prevsent>
<prevsent>inspired by lin (1999), <papid> P99-1041 </papid>we examine the strength of association between the verb and noun constituents of the target combination and its variants, as an indirect cueto their idiomaticity.</prevsent>
</prevsection>
<citsent citstr=" P98-2127 ">
we use the automatically-built thesaurus of lin (1998) <papid> P98-2127 </papid>to find similar words to the noun of the target expression, in order to automatically generate variants.</citsent>
<aftsection>
<nextsent>only the noun constituent is varied, since replacing theverb constituent of vnic with semantically related verb is more likely to yield another vnic, as in keep/lose ones cool (nunberg et al , 1994).
</nextsent>
<nextsent>let
</nextsent>
<nextsent>   ffflfiffi be the set of the fi most similar nouns to the noun of the target pair  !$# &amp;% . we calculate the association.
</nextsent>
<nextsent>strength for the target pair, and for each of its variants,  !$#  %, using pointwise mutual information (pmi) (church et al , 1991): (*),+  !$# .-/0 1325476  !$# -  6  !  6  -  13254  89 :;=   !$# .-/    !$#@?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4511">
<title id=" E06-1043.xml">automatically constructing a lexicon of verb phrase idiomatic combinations </title>
<section> discussion and conclusions.  </section>
<citcontext>
<prevsection>
<prevsent>nonetheless, most research on the topic has focused on compound nouns andverb particle constructions.
</prevsent>
<prevsent>earlier work on idioms have only touched the surface of the problem,failing to propose explicit mechanisms for appropriately handling them.
</prevsent>
</prevsection>
<citsent citstr=" W04-0411 ">
here, we provide effective mechanisms for the treatment of broadly documented and cross linguistically frequent class of idioms, i.e., vnics.earlier research on the lexical encoding of idioms mainly relied on the existence of human annotations, especially for detecting which syntactic variations (e.g., passivization) an idiom can undergo (villavicencio et al , 2004).<papid> W04-0411 </papid></citsent>
<aftsection>
<nextsent>we propose techniques for the automatic acquisition and encoding of knowledge about the lexico syntactic behaviour of idiomatic combinations.
</nextsent>
<nextsent>we put forward means for automatically discovering the set of syntactic variations that are tolerated by vnicand that should be included in its lexical representation.
</nextsent>
<nextsent>moreover, we incorporate such information into statistical measures that effectively predict theidiomaticity level of given expression.
</nextsent>
<nextsent>in this regard, our work relates to previous studies on determining the compositionality (inverse of idiomatic ity) of mwes other than idioms.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4512">
<title id=" E06-1043.xml">automatically constructing a lexicon of verb phrase idiomatic combinations </title>
<section> discussion and conclusions.  </section>
<citcontext>
<prevsection>
<prevsent>moreover, we incorporate such information into statistical measures that effectively predict theidiomaticity level of given expression.
</prevsent>
<prevsent>in this regard, our work relates to previous studies on determining the compositionality (inverse of idiomatic ity) of mwes other than idioms.
</prevsent>
</prevsection>
<citsent citstr=" J93-1007 ">
most previous work on compositionality of mwes either treat them as collocations (smadja,1993), <papid> J93-1007 </papid>or examine the distributional similarity between the expression and its constituents (mccarthy et al , 2003; <papid> W03-1810 </papid>baldwin et al , 2003; <papid> W03-1812 </papid>bannard et al , 2003).<papid> W03-1809 </papid></citsent>
<aftsection>
<nextsent>lin (1999) <papid> P99-1041 </papid>and wermter and hahn (2005) <papid> H05-1106 </papid>go one step further and look into linguistic property of non-compositional compounds their lexical fixednessto identifythem.</nextsent>
<nextsent>venkatapathy and joshi (2005) <papid> H05-1113 </papid>combine aspects of the above-mentioned work, by incorporating lexical fixed ness, collocation-based, and distributional similarity measures into set of features which are used to rank verb+noun combinations according to their compositionality.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4513">
<title id=" E06-1043.xml">automatically constructing a lexicon of verb phrase idiomatic combinations </title>
<section> discussion and conclusions.  </section>
<citcontext>
<prevsection>
<prevsent>moreover, we incorporate such information into statistical measures that effectively predict theidiomaticity level of given expression.
</prevsent>
<prevsent>in this regard, our work relates to previous studies on determining the compositionality (inverse of idiomatic ity) of mwes other than idioms.
</prevsent>
</prevsection>
<citsent citstr=" W03-1810 ">
most previous work on compositionality of mwes either treat them as collocations (smadja,1993), <papid> J93-1007 </papid>or examine the distributional similarity between the expression and its constituents (mccarthy et al , 2003; <papid> W03-1810 </papid>baldwin et al , 2003; <papid> W03-1812 </papid>bannard et al , 2003).<papid> W03-1809 </papid></citsent>
<aftsection>
<nextsent>lin (1999) <papid> P99-1041 </papid>and wermter and hahn (2005) <papid> H05-1106 </papid>go one step further and look into linguistic property of non-compositional compounds their lexical fixednessto identifythem.</nextsent>
<nextsent>venkatapathy and joshi (2005) <papid> H05-1113 </papid>combine aspects of the above-mentioned work, by incorporating lexical fixed ness, collocation-based, and distributional similarity measures into set of features which are used to rank verb+noun combinations according to their compositionality.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4514">
<title id=" E06-1043.xml">automatically constructing a lexicon of verb phrase idiomatic combinations </title>
<section> discussion and conclusions.  </section>
<citcontext>
<prevsection>
<prevsent>moreover, we incorporate such information into statistical measures that effectively predict theidiomaticity level of given expression.
</prevsent>
<prevsent>in this regard, our work relates to previous studies on determining the compositionality (inverse of idiomatic ity) of mwes other than idioms.
</prevsent>
</prevsection>
<citsent citstr=" W03-1812 ">
most previous work on compositionality of mwes either treat them as collocations (smadja,1993), <papid> J93-1007 </papid>or examine the distributional similarity between the expression and its constituents (mccarthy et al , 2003; <papid> W03-1810 </papid>baldwin et al , 2003; <papid> W03-1812 </papid>bannard et al , 2003).<papid> W03-1809 </papid></citsent>
<aftsection>
<nextsent>lin (1999) <papid> P99-1041 </papid>and wermter and hahn (2005) <papid> H05-1106 </papid>go one step further and look into linguistic property of non-compositional compounds their lexical fixednessto identifythem.</nextsent>
<nextsent>venkatapathy and joshi (2005) <papid> H05-1113 </papid>combine aspects of the above-mentioned work, by incorporating lexical fixed ness, collocation-based, and distributional similarity measures into set of features which are used to rank verb+noun combinations according to their compositionality.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4515">
<title id=" E06-1043.xml">automatically constructing a lexicon of verb phrase idiomatic combinations </title>
<section> discussion and conclusions.  </section>
<citcontext>
<prevsection>
<prevsent>moreover, we incorporate such information into statistical measures that effectively predict theidiomaticity level of given expression.
</prevsent>
<prevsent>in this regard, our work relates to previous studies on determining the compositionality (inverse of idiomatic ity) of mwes other than idioms.
</prevsent>
</prevsection>
<citsent citstr=" W03-1809 ">
most previous work on compositionality of mwes either treat them as collocations (smadja,1993), <papid> J93-1007 </papid>or examine the distributional similarity between the expression and its constituents (mccarthy et al , 2003; <papid> W03-1810 </papid>baldwin et al , 2003; <papid> W03-1812 </papid>bannard et al , 2003).<papid> W03-1809 </papid></citsent>
<aftsection>
<nextsent>lin (1999) <papid> P99-1041 </papid>and wermter and hahn (2005) <papid> H05-1106 </papid>go one step further and look into linguistic property of non-compositional compounds their lexical fixednessto identifythem.</nextsent>
<nextsent>venkatapathy and joshi (2005) <papid> H05-1113 </papid>combine aspects of the above-mentioned work, by incorporating lexical fixed ness, collocation-based, and distributional similarity measures into set of features which are used to rank verb+noun combinations according to their compositionality.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4518">
<title id=" E06-1043.xml">automatically constructing a lexicon of verb phrase idiomatic combinations </title>
<section> discussion and conclusions.  </section>
<citcontext>
<prevsection>
<prevsent>in this regard, our work relates to previous studies on determining the compositionality (inverse of idiomatic ity) of mwes other than idioms.
</prevsent>
<prevsent>most previous work on compositionality of mwes either treat them as collocations (smadja,1993), <papid> J93-1007 </papid>or examine the distributional similarity between the expression and its constituents (mccarthy et al , 2003; <papid> W03-1810 </papid>baldwin et al , 2003; <papid> W03-1812 </papid>bannard et al , 2003).<papid> W03-1809 </papid></prevsent>
</prevsection>
<citsent citstr=" H05-1106 ">
lin (1999) <papid> P99-1041 </papid>and wermter and hahn (2005) <papid> H05-1106 </papid>go one step further and look into linguistic property of non-compositional compounds their lexical fixednessto identifythem.</citsent>
<aftsection>
<nextsent>venkatapathy and joshi (2005) <papid> H05-1113 </papid>combine aspects of the above-mentioned work, by incorporating lexical fixed ness, collocation-based, and distributional similarity measures into set of features which are used to rank verb+noun combinations according to their compositionality.</nextsent>
<nextsent>our work differs from such studies in that it carefully examines several linguistic properties ofvnics that distinguish them from literal (com positional) combinations.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4519">
<title id=" E06-1043.xml">automatically constructing a lexicon of verb phrase idiomatic combinations </title>
<section> discussion and conclusions.  </section>
<citcontext>
<prevsection>
<prevsent>most previous work on compositionality of mwes either treat them as collocations (smadja,1993), <papid> J93-1007 </papid>or examine the distributional similarity between the expression and its constituents (mccarthy et al , 2003; <papid> W03-1810 </papid>baldwin et al , 2003; <papid> W03-1812 </papid>bannard et al , 2003).<papid> W03-1809 </papid></prevsent>
<prevsent>lin (1999) <papid> P99-1041 </papid>and wermter and hahn (2005) <papid> H05-1106 </papid>go one step further and look into linguistic property of non-compositional compounds their lexical fixednessto identifythem.</prevsent>
</prevsection>
<citsent citstr=" H05-1113 ">
venkatapathy and joshi (2005) <papid> H05-1113 </papid>combine aspects of the above-mentioned work, by incorporating lexical fixed ness, collocation-based, and distributional similarity measures into set of features which are used to rank verb+noun combinations according to their compositionality.</citsent>
<aftsection>
<nextsent>our work differs from such studies in that it carefully examines several linguistic properties ofvnics that distinguish them from literal (com positional) combinations.
</nextsent>
<nextsent>moreover, we suggest novel techniques for translating such characteristics into measures that predict the idiomaticitylevel of verb+noun combinations.
</nextsent>
<nextsent>more specifically, we propose statistical measures that quantify the degree of lexical, syntactic, and overall fixed ness of such combinations.
</nextsent>
<nextsent>we demonstrate 343 that these measures can be successfully applied to the task of automatically distinguishing idiomatic combinations from non-idiomatic ones.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4520">
<title id=" E06-1043.xml">automatically constructing a lexicon of verb phrase idiomatic combinations </title>
<section> discussion and conclusions.  </section>
<citcontext>
<prevsection>
<prevsent>more specifically, we propose statistical measures that quantify the degree of lexical, syntactic, and overall fixed ness of such combinations.
</prevsent>
<prevsent>we demonstrate 343 that these measures can be successfully applied to the task of automatically distinguishing idiomatic combinations from non-idiomatic ones.
</prevsent>
</prevsection>
<citsent citstr=" W05-1006 ">
we also show that our syntactic and overall fixed ness measures substantially outperform widely used measure of collocation, (*),+ , even when the latter takes syntactic relations into account.others have also drawn on the notion of syntactic fixed ness for idiom detection, though specific to highly constrained type of idiom (widdowsand dorow, 2005).<papid> W05-1006 </papid></citsent>
<aftsection>
<nextsent>our syntactic fixed ness measure looks into broader set of patterns associated with large class of idiomatic expressions.
</nextsent>
<nextsent>moreover, our approach is general and can be easily extended to other idiomatic combinations.each measure we use to identify vnics captures different aspect of idiomaticity: (i)j+ reflects the statistical idiosyncrasy of vnics, while the fixed ness measures draw on their lexico syn tactic peculiarities.
</nextsent>
<nextsent>our ongoing work focuses on combining these measures to distinguish vnics from other idiosyncratic verb+noun combinations that are neither purely idiomatic nor completely literal, so that we can identify linguistically plausible classes of verb+noun combinations on this continuum (fazly and stevenson, 2005).
</nextsent>

</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4521">
<title id=" E12-1039.xml">webcage  a web harvested corpus annotated with germanet senses </title>
<section> related work and future directions.  </section>
<citcontext>
<prevsection>
<prevsent>purely manual methods were also used for the german sense-annotated corpora constructed by broscheit et al (2010) and raileanu et al (2002) as well as for other languages including the bulgarian and 393 table 3: evaluation of the algorithm of identifying the target words.
</prevsent>
<prevsent>wiktionary external wikipedia gutenberg examples web pages articles texts precision adjectives 97.70% 95.83% 99.34% 100% nouns 98.17% 98.50% 95.87% 92.19% verbs 97.38% 92.26% 100% 69.87% all word classes 97.32% 96.19% 96.26% 87.43% recall adjectives 97.70% 97.22% 98.08% 97.14% nouns 98.30% 96.03% 92.70.% 97.38% verbs 97.51% 99.60% 100% 89.20% all word classes 97.94% 97.32% 93.36% 95.42% the chinese sense-tagged corpora (koeva et al 2006; wu et al  2006).
</prevsent>
</prevsection>
<citsent citstr=" P95-1026 ">
the only previous attempts of harvesting corpus data for the purpose of constructing sense-annotated corpus are the semi-supervised method developed by yarowsky(1995), <papid> P95-1026 </papid>the knowledge-based approach of leacock et al (1998), <papid> J98-1006 </papid>later also used by agirre and lopez de lacalle (2004), and the automatic association of web directories (from the open directory project, odp) to wordnet senses by santa mara et al (2003).</citsent>
<aftsection>
<nextsent>the latter study (santamara et al  2003) is closest in spirit to the approach presented here.
</nextsent>
<nextsent>it also relies on an automatic mapping between wordnet senses and second web resource.
</nextsent>
<nextsent>while our approach is based on automatic mappings between germanet and wiktionary, their mapping algorithm maps wordnet senses to odp subdi rectories.
</nextsent>
<nextsent>since these odp sub directories contain natural language descriptions of web sites relevant to the sub directory in question, this textual material can be used for harvesting sense-specific examples.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4522">
<title id=" E12-1039.xml">webcage  a web harvested corpus annotated with germanet senses </title>
<section> related work and future directions.  </section>
<citcontext>
<prevsection>
<prevsent>purely manual methods were also used for the german sense-annotated corpora constructed by broscheit et al (2010) and raileanu et al (2002) as well as for other languages including the bulgarian and 393 table 3: evaluation of the algorithm of identifying the target words.
</prevsent>
<prevsent>wiktionary external wikipedia gutenberg examples web pages articles texts precision adjectives 97.70% 95.83% 99.34% 100% nouns 98.17% 98.50% 95.87% 92.19% verbs 97.38% 92.26% 100% 69.87% all word classes 97.32% 96.19% 96.26% 87.43% recall adjectives 97.70% 97.22% 98.08% 97.14% nouns 98.30% 96.03% 92.70.% 97.38% verbs 97.51% 99.60% 100% 89.20% all word classes 97.94% 97.32% 93.36% 95.42% the chinese sense-tagged corpora (koeva et al 2006; wu et al  2006).
</prevsent>
</prevsection>
<citsent citstr=" J98-1006 ">
the only previous attempts of harvesting corpus data for the purpose of constructing sense-annotated corpus are the semi-supervised method developed by yarowsky(1995), <papid> P95-1026 </papid>the knowledge-based approach of leacock et al (1998), <papid> J98-1006 </papid>later also used by agirre and lopez de lacalle (2004), and the automatic association of web directories (from the open directory project, odp) to wordnet senses by santa mara et al (2003).</citsent>
<aftsection>
<nextsent>the latter study (santamara et al  2003) is closest in spirit to the approach presented here.
</nextsent>
<nextsent>it also relies on an automatic mapping between wordnet senses and second web resource.
</nextsent>
<nextsent>while our approach is based on automatic mappings between germanet and wiktionary, their mapping algorithm maps wordnet senses to odp subdi rectories.
</nextsent>
<nextsent>since these odp sub directories contain natural language descriptions of web sites relevant to the sub directory in question, this textual material can be used for harvesting sense-specific examples.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4525">
<title id=" E12-1039.xml">webcage  a web harvested corpus annotated with germanet senses </title>
<section> related work and future directions.  </section>
<citcontext>
<prevsection>
<prevsent>however, we have to leave these matters for future research.
</prevsent>
<prevsent>in order to validate the language independence of our approach, we plan to apply our method tosense inventories for languages other than german. precondition for such an experiment is an existing mapping between the sense inventory in question and web-based resource such as wik tionary or wikipedia.
</prevsent>
</prevsection>
<citsent citstr=" P10-1023 ">
with babelnet, navigli and ponzetto (2010) <papid> P10-1023 </papid>have created multilingual resource that allows the testing of our approach to languages other than german.</citsent>
<aftsection>
<nextsent>as first step in this direction, we applied our approach to english using the mapping between the princeton wordnet and the english version of wiktionary provided by meyer and gurevych (2011).
</nextsent>
<nextsent>there sults of these experiments, which are reported in henrich et al (2012), confirm the general applicability of our approach.
</nextsent>
<nextsent>to conclude: this paper describes an automatic method for creating domain-independent sense annotated corpus harvested from the web.
</nextsent>
<nextsent>the data obtained by this method for german have resulted in the webcage resource which currently represents the largest sense-annotated corpus available for this language.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4526">
<title id=" E12-1051.xml">instance driven attachment of semantic annotations over conceptual hierarchies </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>over gold standard of semantic annotations and concepts that best capture their arguments, the method substantially outperforms three baselines, on average, computing concepts that are less than one step in the hierarchy away from the corresponding gold standard concepts.
</prevsent>
<prevsent>background: knowledge about the world canbe thought of as semantic assertions or annotations, at two levels of granularity: instance level (e.g., rhapsody in blue, tristan und isolde, george gershwin, richard wagner) and concept level (e.g., musical compositions?, works of art?, composers?).
</prevsent>
</prevsection>
<citsent citstr=" P10-1013 ">
instance-level annotations correspond to factual knowledge that can be found in repositories extracted automatically from text (banko et al 2007; wu and weld, 2010) <papid> P10-1013 </papid>contributions made during an internship at google.or manually created within encyclopedic resources (remy, 2002).</citsent>
<aftsection>
<nextsent>such facts could state, for instance, that rhapsody in blue was composed by george gershwin, or that tristan und isolde was composed-by richard wagner.
</nextsent>
<nextsent>in comparison, concept-level annotations more concisely and effectively capture the underlying semantics of the annotations by identifying the concepts corresponding to the arguments, e.g., musical com positions?
</nextsent>
<nextsent>are composed-by composers?.
</nextsent>
<nextsent>the frequent occurrence of instances, relative to more abstract concepts, in web documents and popular web search queries (barr et al 2008; <papid> D08-1107 </papid>li, 2010), <papid> P10-1136 </papid>is both an asset and liability from the point of view of knowledge acquisition.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4527">
<title id=" E12-1051.xml">instance driven attachment of semantic annotations over conceptual hierarchies </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in comparison, concept-level annotations more concisely and effectively capture the underlying semantics of the annotations by identifying the concepts corresponding to the arguments, e.g., musical com positions?
</prevsent>
<prevsent>are composed-by composers?.
</prevsent>
</prevsection>
<citsent citstr=" D08-1107 ">
the frequent occurrence of instances, relative to more abstract concepts, in web documents and popular web search queries (barr et al 2008; <papid> D08-1107 </papid>li, 2010), <papid> P10-1136 </papid>is both an asset and liability from the point of view of knowledge acquisition.</citsent>
<aftsection>
<nextsent>onone hand, it makes instance-level annotations relatively easy to find, either from manually created resources (remy, 2002; boll acker et al 2008), or extracted automatically from text (banko etal., 2007).
</nextsent>
<nextsent>on the other hand, it makes concept level annotations more difficult to acquire directly.
</nextsent>
<nextsent>while rhapsody in blue was composed by george gershwin [..]?
</nextsent>
<nextsent>may occur in some form within web documents, the more abstract musical compositions are composed by musicians [..]?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4528">
<title id=" E12-1051.xml">instance driven attachment of semantic annotations over conceptual hierarchies </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in comparison, concept-level annotations more concisely and effectively capture the underlying semantics of the annotations by identifying the concepts corresponding to the arguments, e.g., musical com positions?
</prevsent>
<prevsent>are composed-by composers?.
</prevsent>
</prevsection>
<citsent citstr=" P10-1136 ">
the frequent occurrence of instances, relative to more abstract concepts, in web documents and popular web search queries (barr et al 2008; <papid> D08-1107 </papid>li, 2010), <papid> P10-1136 </papid>is both an asset and liability from the point of view of knowledge acquisition.</citsent>
<aftsection>
<nextsent>onone hand, it makes instance-level annotations relatively easy to find, either from manually created resources (remy, 2002; boll acker et al 2008), or extracted automatically from text (banko etal., 2007).
</nextsent>
<nextsent>on the other hand, it makes concept level annotations more difficult to acquire directly.
</nextsent>
<nextsent>while rhapsody in blue was composed by george gershwin [..]?
</nextsent>
<nextsent>may occur in some form within web documents, the more abstract musical compositions are composed by musicians [..]?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4529">
<title id=" E12-1051.xml">instance driven attachment of semantic annotations over conceptual hierarchies </title>
<section> experimental setting.  </section>
<citcontext>
<prevsection>
<prevsent>in the process, some annotation labels are discarded, when (a) it is not clear what concept captures an argument (e.g., for the right argument of functionof-building), or (b) more than 5000 candidate concepts are available via propagation for one of the arguments, which would cause too many training or testing examples to be generated via concept pairs, and slow down the experiments.
</prevsent>
<prevsent>the retained 139 annotation labels, whose argument shave been labeled with their respective gold concepts, form the gold standard for the experiments.more precisely, an entry in the resulting gold standard consists of an annotation label, one of its arguments being considered (left or right), and gold concept that best captures that argument.the set of annotation labels from the gold standard is quite diverse and covers many domains of potential interest, e.g., has-company(industries?,companies?), written-by(films?, screen writ ers?), member-of (politicians?,political parties?), or part-of-movement(artists?, art movements?).
</prevsent>
</prevsection>
<citsent citstr=" P10-1150 ">
evaluation metric: following previous work on selectional preferences (kozareva and hovy, 2010; <papid> P10-1150 </papid>ritter et al 2010), <papid> P10-1044 </papid>each entry in the gold standard, (i.e., each argument forgiven annota tion) is evaluated separately.</citsent>
<aftsection>
<nextsent>experimental runs compute ranked list of candidate concepts foreach entry in the gold standard.
</nextsent>
<nextsent>in theory, computed candidate concept is better if it is closer semantically to the gold concept.
</nextsent>
<nextsent>in practice,the accuracy of ranked list of candidate concepts, relative to the gold concept of the annotation label, is measured by two scoring metrics that correspond to the mean reciprocal rank score(mrr) (voorhees and tice, 2000) and modification of it (drr) (pasca and alfonseca, 2009): mrr = 1 n?
</nextsent>
<nextsent>i=1 max rank 1 ranki is the number of annotations and ranki is the rank of the gold concept in the returned list for mrr.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4530">
<title id=" E12-1051.xml">instance driven attachment of semantic annotations over conceptual hierarchies </title>
<section> experimental setting.  </section>
<citcontext>
<prevsection>
<prevsent>in the process, some annotation labels are discarded, when (a) it is not clear what concept captures an argument (e.g., for the right argument of functionof-building), or (b) more than 5000 candidate concepts are available via propagation for one of the arguments, which would cause too many training or testing examples to be generated via concept pairs, and slow down the experiments.
</prevsent>
<prevsent>the retained 139 annotation labels, whose argument shave been labeled with their respective gold concepts, form the gold standard for the experiments.more precisely, an entry in the resulting gold standard consists of an annotation label, one of its arguments being considered (left or right), and gold concept that best captures that argument.the set of annotation labels from the gold standard is quite diverse and covers many domains of potential interest, e.g., has-company(industries?,companies?), written-by(films?, screen writ ers?), member-of (politicians?,political parties?), or part-of-movement(artists?, art movements?).
</prevsent>
</prevsection>
<citsent citstr=" P10-1044 ">
evaluation metric: following previous work on selectional preferences (kozareva and hovy, 2010; <papid> P10-1150 </papid>ritter et al 2010), <papid> P10-1044 </papid>each entry in the gold standard, (i.e., each argument forgiven annota tion) is evaluated separately.</citsent>
<aftsection>
<nextsent>experimental runs compute ranked list of candidate concepts foreach entry in the gold standard.
</nextsent>
<nextsent>in theory, computed candidate concept is better if it is closer semantically to the gold concept.
</nextsent>
<nextsent>in practice,the accuracy of ranked list of candidate concepts, relative to the gold concept of the annotation label, is measured by two scoring metrics that correspond to the mean reciprocal rank score(mrr) (voorhees and tice, 2000) and modification of it (drr) (pasca and alfonseca, 2009): mrr = 1 n?
</nextsent>
<nextsent>i=1 max rank 1 ranki is the number of annotations and ranki is the rank of the gold concept in the returned list for mrr.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4531">
<title id=" E12-1051.xml">instance driven attachment of semantic annotations over conceptual hierarchies </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>is not reached during propagation from instances upwards in the hierarchy.
</prevsent>
<prevsent>similar to the task of attaching semantic annotation to the concept in hierarchy that has thebest level of generality is the task of finding selectional preferences for relations.
</prevsent>
</prevsection>
<citsent citstr=" E95-1016 ">
most relevant to this paper is work that seeks to find the appropriate concept in hierarchy for an argument of specific relation (ribas, 1995; <papid> E95-1016 </papid>mccarthy, 1997; <papid> W97-0808 </papid>li and abe, 1998).</citsent>
<aftsection>
<nextsent>li and abe (1998) address this problem by attempting to identify the best tree cut in hierarchy for an argument of given verb.they use the minimum description length principle to select set of concepts from hierarchy to represent the selectional preferences.
</nextsent>
<nextsent>this work makes several limiting assumptions including that the hierarchy is tree, and every instance belong sto just one concept.
</nextsent>
<nextsent>clark and weir (2002) <papid> J02-2003 </papid>investigate the task of generalizing single relation concept pair.</nextsent>
<nextsent>a relation is propagated up hierarchy until chi-square test determines the difference between the probability of the child and parent concepts to be significant where the probabilities are relation-concept frequencies.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4532">
<title id=" E12-1051.xml">instance driven attachment of semantic annotations over conceptual hierarchies </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>is not reached during propagation from instances upwards in the hierarchy.
</prevsent>
<prevsent>similar to the task of attaching semantic annotation to the concept in hierarchy that has thebest level of generality is the task of finding selectional preferences for relations.
</prevsent>
</prevsection>
<citsent citstr=" W97-0808 ">
most relevant to this paper is work that seeks to find the appropriate concept in hierarchy for an argument of specific relation (ribas, 1995; <papid> E95-1016 </papid>mccarthy, 1997; <papid> W97-0808 </papid>li and abe, 1998).</citsent>
<aftsection>
<nextsent>li and abe (1998) address this problem by attempting to identify the best tree cut in hierarchy for an argument of given verb.they use the minimum description length principle to select set of concepts from hierarchy to represent the selectional preferences.
</nextsent>
<nextsent>this work makes several limiting assumptions including that the hierarchy is tree, and every instance belong sto just one concept.
</nextsent>
<nextsent>clark and weir (2002) <papid> J02-2003 </papid>investigate the task of generalizing single relation concept pair.</nextsent>
<nextsent>a relation is propagated up hierarchy until chi-square test determines the difference between the probability of the child and parent concepts to be significant where the probabilities are relation-concept frequencies.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4533">
<title id=" E12-1051.xml">instance driven attachment of semantic annotations over conceptual hierarchies </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>li and abe (1998) address this problem by attempting to identify the best tree cut in hierarchy for an argument of given verb.they use the minimum description length principle to select set of concepts from hierarchy to represent the selectional preferences.
</prevsent>
<prevsent>this work makes several limiting assumptions including that the hierarchy is tree, and every instance belong sto just one concept.
</prevsent>
</prevsection>
<citsent citstr=" J02-2003 ">
clark and weir (2002) <papid> J02-2003 </papid>investigate the task of generalizing single relation concept pair.</citsent>
<aftsection>
<nextsent>a relation is propagated up hierarchy until chi-square test determines the difference between the probability of the child and parent concepts to be significant where the probabilities are relation-concept frequencies.
</nextsent>
<nextsent>this method has no direct translation to the task discussed here; it is unclear how to choose the correct concept if instances generalize to different concepts.
</nextsent>
<nextsent>in other research on selectional preferences, pantel et al(2007), <papid> N07-1071 </papid>kozareva and hovy (2010)<papid> P10-1150 </papid>and ritter et al(2010) <papid> P10-1044 </papid>focus on generating admissible arguments for relations, and erk (2007) <papid> P07-1028 </papid>and bergsma et al(2008) <papid> D08-1007 </papid>investigate classifying relation-instance pair as plausible or not.important to this paper is the wikipedia category network (remy, 2002) and work on refining it.</nextsent>
<nextsent>ponzetto and navigli (2009) disambiguate wikipedia categories by using wordnet synsets and use this semantic information to construct ataxonomy.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4534">
<title id=" E12-1051.xml">instance driven attachment of semantic annotations over conceptual hierarchies </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>a relation is propagated up hierarchy until chi-square test determines the difference between the probability of the child and parent concepts to be significant where the probabilities are relation-concept frequencies.
</prevsent>
<prevsent>this method has no direct translation to the task discussed here; it is unclear how to choose the correct concept if instances generalize to different concepts.
</prevsent>
</prevsection>
<citsent citstr=" N07-1071 ">
in other research on selectional preferences, pantel et al(2007), <papid> N07-1071 </papid>kozareva and hovy (2010)<papid> P10-1150 </papid>and ritter et al(2010) <papid> P10-1044 </papid>focus on generating admissible arguments for relations, and erk (2007) <papid> P07-1028 </papid>and bergsma et al(2008) <papid> D08-1007 </papid>investigate classifying relation-instance pair as plausible or not.important to this paper is the wikipedia category network (remy, 2002) and work on refining it.</citsent>
<aftsection>
<nextsent>ponzetto and navigli (2009) disambiguate wikipedia categories by using wordnet synsets and use this semantic information to construct ataxonomy.
</nextsent>
<nextsent>the resulting taxonomy is the conceptual hierarchy used in the evaluation.
</nextsent>
<nextsent>another related area of work is the discovery of relations between concepts.
</nextsent>
<nextsent>nastase and strube(2008) use wikipedia category names and category structure to generate set of relations between concepts.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4537">
<title id=" E12-1051.xml">instance driven attachment of semantic annotations over conceptual hierarchies </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>a relation is propagated up hierarchy until chi-square test determines the difference between the probability of the child and parent concepts to be significant where the probabilities are relation-concept frequencies.
</prevsent>
<prevsent>this method has no direct translation to the task discussed here; it is unclear how to choose the correct concept if instances generalize to different concepts.
</prevsent>
</prevsection>
<citsent citstr=" P07-1028 ">
in other research on selectional preferences, pantel et al(2007), <papid> N07-1071 </papid>kozareva and hovy (2010)<papid> P10-1150 </papid>and ritter et al(2010) <papid> P10-1044 </papid>focus on generating admissible arguments for relations, and erk (2007) <papid> P07-1028 </papid>and bergsma et al(2008) <papid> D08-1007 </papid>investigate classifying relation-instance pair as plausible or not.important to this paper is the wikipedia category network (remy, 2002) and work on refining it.</citsent>
<aftsection>
<nextsent>ponzetto and navigli (2009) disambiguate wikipedia categories by using wordnet synsets and use this semantic information to construct ataxonomy.
</nextsent>
<nextsent>the resulting taxonomy is the conceptual hierarchy used in the evaluation.
</nextsent>
<nextsent>another related area of work is the discovery of relations between concepts.
</nextsent>
<nextsent>nastase and strube(2008) use wikipedia category names and category structure to generate set of relations between concepts.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4538">
<title id=" E12-1051.xml">instance driven attachment of semantic annotations over conceptual hierarchies </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>a relation is propagated up hierarchy until chi-square test determines the difference between the probability of the child and parent concepts to be significant where the probabilities are relation-concept frequencies.
</prevsent>
<prevsent>this method has no direct translation to the task discussed here; it is unclear how to choose the correct concept if instances generalize to different concepts.
</prevsent>
</prevsection>
<citsent citstr=" D08-1007 ">
in other research on selectional preferences, pantel et al(2007), <papid> N07-1071 </papid>kozareva and hovy (2010)<papid> P10-1150 </papid>and ritter et al(2010) <papid> P10-1044 </papid>focus on generating admissible arguments for relations, and erk (2007) <papid> P07-1028 </papid>and bergsma et al(2008) <papid> D08-1007 </papid>investigate classifying relation-instance pair as plausible or not.important to this paper is the wikipedia category network (remy, 2002) and work on refining it.</citsent>
<aftsection>
<nextsent>ponzetto and navigli (2009) disambiguate wikipedia categories by using wordnet synsets and use this semantic information to construct ataxonomy.
</nextsent>
<nextsent>the resulting taxonomy is the conceptual hierarchy used in the evaluation.
</nextsent>
<nextsent>another related area of work is the discovery of relations between concepts.
</nextsent>
<nextsent>nastase and strube(2008) use wikipedia category names and category structure to generate set of relations between concepts.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4539">
<title id=" E12-1051.xml">instance driven attachment of semantic annotations over conceptual hierarchies </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>another related area of work is the discovery of relations between concepts.
</prevsent>
<prevsent>nastase and strube(2008) use wikipedia category names and category structure to generate set of relations between concepts.
</prevsent>
</prevsection>
<citsent citstr=" P09-1115 ">
yan et al(2009) <papid> P09-1115 </papid>discover relations between wikipedia concepts via deep linguistic information and web frequency information.</citsent>
<aftsection>
<nextsent>mohamed et al(2011) <papid> D11-1134 </papid>generate candidate relations by co clustering text contexts for every pair of concepts in hierarchy.</nextsent>
<nextsent>in sense,this area of research is complementary to that discussed in this paper.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4540">
<title id=" E12-1051.xml">instance driven attachment of semantic annotations over conceptual hierarchies </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>nastase and strube(2008) use wikipedia category names and category structure to generate set of relations between concepts.
</prevsent>
<prevsent>yan et al(2009) <papid> P09-1115 </papid>discover relations between wikipedia concepts via deep linguistic information and web frequency information.</prevsent>
</prevsection>
<citsent citstr=" D11-1134 ">
mohamed et al(2011) <papid> D11-1134 </papid>generate candidate relations by co clustering text contexts for every pair of concepts in hierarchy.</citsent>
<aftsection>
<nextsent>in sense,this area of research is complementary to that discussed in this paper.
</nextsent>
<nextsent>these methods induce new relations, and the proposed method can be used to find appropriate levels of generalization for the arguments of any given relation.
</nextsent>
<nextsent>this paper introduces method to convert flat setsof instance-level annotations to hierarchically organized, concept-level annotations.
</nextsent>
<nextsent>the method determines the appropriate concept forgiven semantic annotation in three stages.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4541">
<title id=" E12-1051.xml">instance driven attachment of semantic annotations over conceptual hierarchies </title>
<section> conclusions.  </section>
<citcontext>
<prevsection>
<prevsent>the proposed method can take advantage of existing work on open-domain information extraction.
</prevsent>
<prevsent>the output of such work is usuallyinstance-level annotations, although often at surface level (non-disambiguated arguments) rather than semantic level (disambiguated arguments).
</prevsent>
</prevsection>
<citsent citstr=" C10-1032 ">
after argument disambiguation (e.g., (dredze et al., 2010)), <papid> C10-1032 </papid>the annotations can be used as input to determining concept-level annotations.</citsent>
<aftsection>
<nextsent>thus, the method has the potential to generalize any existing database of instance-level annotations to concept-level annotations.
</nextsent>
<nextsent>511
</nextsent>


</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4542">
<title id=" E06-2027.xml">information structure and pauses in a corpus of spoken danish </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the interest for corpora annotated with information structure has been raised recently by several authors.
</prevsent>
<prevsent>kruijff-korbayova?
</prevsent>
</prevsection>
<citsent citstr=" P05-2020 ">
and kruijff(2004) describe method where rich discourse level annotation is used to investigate information structure, while both postolache (2005) <papid> P05-2020 </papid>and diderichsen and elming (2005) <papid> P05-2019 </papid>study the application of machine learning to the problem of automatic identification of topic and focus.</citsent>
<aftsection>
<nextsent>in this study, on the contrary, information structure is annotated manually, and the annotation is usedto investigate the correlation between information structure tags and intra-clausal pauses.
</nextsent>
<nextsent>the starting point for this study was the corpus of spoken danish danpass?
</nextsent>
<nextsent>(grnnum, 2005), collection of 54 monologues produced by 18 different subjects dealing with three well-defined tasks, following the methodology established inter ken (1985).
</nextsent>
<nextsent>in the first task, the subjects describe geometrical network, in the second the process of assembling the drawing of house out of existing pieces, and in the third they solve map task.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4543">
<title id=" E06-2027.xml">information structure and pauses in a corpus of spoken danish </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the interest for corpora annotated with information structure has been raised recently by several authors.
</prevsent>
<prevsent>kruijff-korbayova?
</prevsent>
</prevsection>
<citsent citstr=" P05-2019 ">
and kruijff(2004) describe method where rich discourse level annotation is used to investigate information structure, while both postolache (2005) <papid> P05-2020 </papid>and diderichsen and elming (2005) <papid> P05-2019 </papid>study the application of machine learning to the problem of automatic identification of topic and focus.</citsent>
<aftsection>
<nextsent>in this study, on the contrary, information structure is annotated manually, and the annotation is usedto investigate the correlation between information structure tags and intra-clausal pauses.
</nextsent>
<nextsent>the starting point for this study was the corpus of spoken danish danpass?
</nextsent>
<nextsent>(grnnum, 2005), collection of 54 monologues produced by 18 different subjects dealing with three well-defined tasks, following the methodology established inter ken (1985).
</nextsent>
<nextsent>in the first task, the subjects describe geometrical network, in the second the process of assembling the drawing of house out of existing pieces, and in the third they solve map task.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4544">
<title id=" E12-2019.xml">a platform for collaborative semantic annotation </title>
<section> employing state-of-the-art nlp software for.  </section>
<citcontext>
<prevsection>
<prevsent>ensuring kerfuffle-free dissemination of.
</prevsent>
<prevsent>our semantic resource by considering only public-domain texts for annotation.
</prevsent>
</prevsection>
<citsent citstr=" W07-1505 ">
we have developed the wiki-like platform from scratch simply because existing annotation systems, such as gate (dowman et al 2005), nite (carletta et al 2003), or uima (hahn et al 2007), <papid> W07-1505 </papid>do not offer the functionality required fordeep semantic annotation combined with crowd sourcing.</citsent>
<aftsection>
<nextsent>in this description of our platform, we motivate our choice of data and explain how we manage it (section 2), we describe the complete toolchainof nlp components employed in the annotation feedback process (section 3), and the web-basedinterface itself is introduced, describing how linguists can adjust boundaries of tokens and sentences, and revise tags of named entities, parts of speech and lexical categories (section 4).
</nextsent>
<nextsent>2 data.
</nextsent>
<nextsent>the goal of the groningen meaning bank is to provide widely available corpus of texts, with deep semantic annotations.
</nextsent>
<nextsent>the gmb only comprises texts from the public domain, whose distribution isnt subject to copyright restrictions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4545">
<title id=" E12-2019.xml">a platform for collaborative semantic annotation </title>
<section> employing state-of-the-art nlp software for.  </section>
<citcontext>
<prevsection>
<prevsent>the gmb only comprises texts from the public domain, whose distribution isnt subject to copyright restrictions.
</prevsent>
<prevsent>moreover, we include texts from various genres and sources, resulting in rich, comprehensive 92 corpus appropriate for use in various disciplines within nlp.
</prevsent>
</prevsection>
<citsent citstr=" P10-2013 ">
the documents in the current version of the gmb are all in english and originate from four main sources: (i) voice of america (voa), an online newspaper published by the us federal gov ernment; (ii) the manually annotated sub-corpus(masc) from the open american national corpus (ide et al 2010); (<papid> P10-2013 </papid>iii) country descriptions from the cia world factbook (cia) (central intelligence agency, 2006), in particular the background and economy sections, and (iv) collection of aesops fables (af).</citsent>
<aftsection>
<nextsent>all these documents are in the public domain and are thus redis tributable, unlike for example the wsj data used in the penn treebank (miltsakaki et al 2004).
</nextsent>
<nextsent>each document is stored with separate file containing metadata.
</nextsent>
<nextsent>this may include the language the text is written in, the genre, date of publication, source, title, and terms of use of the document.
</nextsent>
<nextsent>this meta data is stored as simple feature-value list.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4546">
<title id=" E12-2019.xml">a platform for collaborative semantic annotation </title>
<section> the nlp tool chain.  </section>
<citcontext>
<prevsection>
<prevsent>the output of this automatic process is in the form of several layers of stand-off annotations, i.e., files with links to the original, raw documents.
</prevsent>
<prevsent>we employ chain of nlp components that carry out, respectively, tokenization and sentence boundary detection, pos tagging, lemmatization, named entity recognition, super tagging, parsing using the formalism of combinatory categorial grammar (steedman, 2001), and semantic and discourse analysis using the framework of discourse representation theory (drt) (kamp and reyle, 1993) with rhetorical relations (asher, 1993).
</prevsent>
</prevsection>
<citsent citstr=" P07-2009 ">
the lemmatizer used is morpha (minnen et al 2001), the other steps are carried out by the c&c; tools (curran et al 2007) <papid> P07-2009 </papid>and boxer (bos, 2008).</citsent>
<aftsection>
<nextsent>3.1 bits of wisdom.
</nextsent>
<nextsent>after each step in the tool chain, the intermediate result may be automatically adjusted by auxiliary components that apply annotations provided by expert users or other sources.
</nextsent>
<nextsent>these annotations are represented as bits of wisdom?
</nextsent>
<nextsent>(bows): tu ples of information regarding, for example, token and sentence boundaries, tags, word senses or discourse relations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4547">
<title id=" E12-2019.xml">a platform for collaborative semantic annotation </title>
<section> the nlp tool chain.  </section>
<citcontext>
<prevsection>
<prevsent>(bows): tu ples of information regarding, for example, token and sentence boundaries, tags, word senses or discourse relations.
</prevsent>
<prevsent>they are stored in mysql database and can originate from three different sources: (i) explicit annotation changes made by experts using the explorer web interface (see section 4); (ii) an annotation game played by non experts, similar to games with purpose?
</prevsent>
</prevsection>
<citsent citstr=" W08-2230 ">
like phrase detectives (chamberlain et al 2008) <papid> W08-2230 </papid>andjeux de mots (artignan et al 2009); and (iii) external nlp tools (e.g. for word sense disambiguation or co-reference resolution).</citsent>
<aftsection>
<nextsent>since bows come from various sources, they may contradict each other.
</nextsent>
<nextsent>in such cases, judge component resolves the conflict, currently by preferring the most recent expert bow.
</nextsent>
<nextsent>future work will involve the application of different judging techniques.
</nextsent>
<nextsent>3.2 processing cycle.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4548">
<title id=" E09-1038.xml">enhancing un lexicalized parsing performance using a wide coverage lexicon fuzzy tagset mapping and emhmmbased lexical probabilities </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>hebrew is semitic language with rich 1this is not the case with other languages, and also not true for english when adaptation scenarios are considered.
</prevsent>
<prevsent>327 morphological structure.
</prevsent>
</prevsection>
<citsent citstr=" P08-1083 ">
this rich structure yields large number of distinct word forms, resulting in high oov rate (adler et al, 2008<papid> P08-1083 </papid>a).</citsent>
<aftsection>
<nextsent>this posesa serious problem for estimating lexical probabilities from small annotated corpora, such as the hebrew treebank (simaan et al, 2001).
</nextsent>
<nextsent>hebrew has wide coverage lexicon /morphological-analyzer (henceforth, kc ana lyzer) available2, but its tagset is different than the one used by the hebrew treebank.
</nextsent>
<nextsent>these are notmere technical differences, but derive from different perspectives on the data.
</nextsent>
<nextsent>the hebrew tb tagset is syntactic in nature, while the kc tagset is lexicographic.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4554">
<title id=" E09-1038.xml">enhancing un lexicalized parsing performance using a wide coverage lexicon fuzzy tagset mapping and emhmmbased lexical probabilities </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>thus, the leaves of the syntactic parse trees do not correspond to space-delimited tokens, and the yield of the tree is not known in advance.
</prevsent>
<prevsent>we show that enhancing the parser with external lexical information is greatly beneficial, both in an artificial scenario where the token segmentation is assumed to be known (sec.
</prevsent>
</prevsection>
<citsent citstr=" P08-1043 ">
4), and in more realistic one in which parsing and segmentation are handled jointly by the parser (goldberg and tsarfaty, 2008) (<papid> P08-1043 </papid>sec.</citsent>
<aftsection>
<nextsent>5).
</nextsent>
<nextsent>external lexical information enhances un lexicalized parsing performance by as much as 6.67 f-points, an error reduction of 20% over treebank-only parser.
</nextsent>
<nextsent>our results are not only the best published results for parsing hebrew, but also on par with state-of-the-art 2http://mila.cs.technion.ac.il/hebrew/resources/lexicons/lexicalized arabic parsing results assuming gold standard fine-grained part-of-speech (maamouri et al, 2008).3
</nextsent>
<nextsent>modern hebrew has 2 major linguistic resources: the hebrew treebank (tb), and wide coverage lexicon-based morphological analyzer developed and maintained by the knowledge center for processing hebrew (kc analyzer).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4565">
<title id=" E09-1038.xml">enhancing un lexicalized parsing performance using a wide coverage lexicon fuzzy tagset mapping and emhmmbased lexical probabilities </title>
<section> a tale of two resources </section>
<citcontext>
<prevsection>
<prevsent>this goes to emphasize the productive nature of hebrew morphology, and stress that robust lexical probability estimates cannot be derived from an annotated resource as small as the treebank.
</prevsent>
<prevsent>lexical vs. syntactic pos tags the analyses produced by the kc analyzer are not compatible with the hebrew tb.
</prevsent>
</prevsection>
<citsent citstr=" W07-0808 ">
the kc tagset (adler et al, 2008<papid> P08-1083 </papid>b; netzer et al., 2007; <papid> W07-0808 </papid>adler, 2007) takes lexical approach to pos tagging (a word can assume only pos tags that would be assigned to it in dictionary?), while the tb takes syntactic one (if the word in this particular positions functions as an adverb, tag itas an adverb, even though it is listed in the dictionary only as noun?).</citsent>
<aftsection>
<nextsent>we present 2 cases that emphasize the difference: adjectives: the treebank 3our method is orthogonal to lexicalization and can be used in addition to it if one so wishes.4in these counts, all numbers are conflated to one canonical form 328treats any word in an adjectivial position as an adjective.
</nextsent>
<nextsent>this includes also demonstrative pronouns ??
</nextsent>
<nextsent>(this boy).
</nextsent>
<nextsent>however, from the kc point of view, the fact that pronoun can be used to modifya noun does not mean it should appear in dictionary as an adjective.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4572">
<title id=" E09-1038.xml">enhancing un lexicalized parsing performance using a wide coverage lexicon fuzzy tagset mapping and emhmmbased lexical probabilities </title>
<section> semi-supervised lexical probability.  </section>
<citcontext>
<prevsection>
<prevsent>, wn) = argmax ? p(ti|ti1, ti2)p(wi|ti), and taking the emission probabilities p(w|t) of that model.
</prevsent>
<prevsent>in hebrew, things are more complicated, as each emission is not space delimited token, but rather smaller unit (a morphological segment, henceforth segment).
</prevsent>
</prevsection>
<citsent citstr=" P06-1084 ">
adler and elhadad (2006)<papid> P06-1084 </papid>present lattice-based modification of the baum welch algorithm to handle this segmentation am biguity.</citsent>
<aftsection>
<nextsent>traditionally, such unsupervised em-trained hmm taggers are thought to be inaccurate, but (goldberg et al, 2008) <papid> P08-1085 </papid>showed that by feeding theem process with sufficiently good initial probabilities, accurate taggers (  91% accuracy) can be learned for both english and hebrew, based on (possibly incomplete) lexicon and large amount ofraw text.</nextsent>
<nextsent>they also present method for automatically obtaining these initial probabilities.as stated in section 2, the kc analyzer (he brew lexicon) coverage is incomplete.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4573">
<title id=" E09-1038.xml">enhancing un lexicalized parsing performance using a wide coverage lexicon fuzzy tagset mapping and emhmmbased lexical probabilities </title>
<section> semi-supervised lexical probability.  </section>
<citcontext>
<prevsection>
<prevsent>in hebrew, things are more complicated, as each emission is not space delimited token, but rather smaller unit (a morphological segment, henceforth segment).
</prevsent>
<prevsent>adler and elhadad (2006)<papid> P06-1084 </papid>present lattice-based modification of the baum welch algorithm to handle this segmentation am biguity.</prevsent>
</prevsection>
<citsent citstr=" P08-1085 ">
traditionally, such unsupervised em-trained hmm taggers are thought to be inaccurate, but (goldberg et al, 2008) <papid> P08-1085 </papid>showed that by feeding theem process with sufficiently good initial probabilities, accurate taggers (  91% accuracy) can be learned for both english and hebrew, based on (possibly incomplete) lexicon and large amount ofraw text.</citsent>
<aftsection>
<nextsent>they also present method for automatically obtaining these initial probabilities.as stated in section 2, the kc analyzer (he brew lexicon) coverage is incomplete.
</nextsent>
<nextsent>adler et al(2008<papid> P08-1083 </papid>a) use the lexicon to learn maximum entropy model for predicting possible analyses for unknown tokens based on their orthography, thus extending the lexicon to cover (even if noisily) any unknown token.</nextsent>
<nextsent>in what follows, we use kc analyzer to refer to this extended version.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4582">
<title id=" E09-1038.xml">enhancing un lexicalized parsing performance using a wide coverage lexicon fuzzy tagset mapping and emhmmbased lexical probabilities </title>
<section> parsing without segmentation oracle.  </section>
<citcontext>
<prevsection>
<prevsent>when parsing real world data, correct token segmentation is not known in advance.
</prevsent>
<prevsent>for methodological reasons, this issue has either been set aside (tsarfaty and simaan, 2007), or dealt within pipeline model in which morphological dis ambiguator is run prior to parsing to determine the correct segmentation.
</prevsent>
</prevsection>
<citsent citstr=" P06-3009 ">
however, tsarfaty (2006) <papid> P06-3009 </papid>argues that there is strong interaction between syntax and morphological segmentation, and that the two tasks should be modeled jointly, and not in pipeline model.</citsent>
<aftsection>
<nextsent>several studies followed this line, (cohen and smith, 2007) <papid> D07-1022 </papid>the most recent ofwhich is goldberg and tsarfaty (2008), <papid> P08-1043 </papid>who presented model based on unweighted lattice parsing for performing the joint task.</nextsent>
<nextsent>this model uses morphological analyzer to construct lattice over all possible morphological analyses of an input sentence.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4583">
<title id=" E09-1038.xml">enhancing un lexicalized parsing performance using a wide coverage lexicon fuzzy tagset mapping and emhmmbased lexical probabilities </title>
<section> parsing without segmentation oracle.  </section>
<citcontext>
<prevsection>
<prevsent>for methodological reasons, this issue has either been set aside (tsarfaty and simaan, 2007), or dealt within pipeline model in which morphological dis ambiguator is run prior to parsing to determine the correct segmentation.
</prevsent>
<prevsent>however, tsarfaty (2006) <papid> P06-3009 </papid>argues that there is strong interaction between syntax and morphological segmentation, and that the two tasks should be modeled jointly, and not in pipeline model.</prevsent>
</prevsection>
<citsent citstr=" D07-1022 ">
several studies followed this line, (cohen and smith, 2007) <papid> D07-1022 </papid>the most recent ofwhich is goldberg and tsarfaty (2008), <papid> P08-1043 </papid>who presented model based on unweighted lattice parsing for performing the joint task.</citsent>
<aftsection>
<nextsent>this model uses morphological analyzer to construct lattice over all possible morphological analyses of an input sentence.
</nextsent>
<nextsent>the arcs of the lattice are w, t?
</nextsent>
<nextsent>pairs, and lattice parser is used to build parse over the lattice.
</nextsent>
<nextsent>the viterbi parse over the lattice chooses lattice path,which induces segmentation over the input sentence.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4593">
<title id=" E06-1038.xml">discriminative sentence compression with soft syntactic evidence </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the ability to compress sentences grammatically with minimal information loss is an important problem in text summarization.
</prevsent>
<prevsent>most summarization systems are evaluated on the amount of relevant information retained as well as their compression rate.
</prevsent>
</prevsection>
<citsent citstr=" N03-1026 ">
thus, returning highly compressed, yet informative, sentences allows summarization systems to return larger sets of sentences and increase the overall amount of information extracted.we focus on the particular instantiation of sentence compression when the goal is to produce the compressed version solely by removing words or phrases from the original, which is the most common setting in the literature (knight and marcu, 2000; riezler et al , 2003; <papid> N03-1026 </papid>turner and charniak, 2005).<papid> P05-1036 </papid></citsent>
<aftsection>
<nextsent>in this framework, the goal is to find the shortest substring of the original sentence that conveys the most important aspects of the meaning.
</nextsent>
<nextsent>we will work in supervised learning setting and assume as input training set =(xt,yt)|t |t=1 of original sentences xt and their compress ions yt.
</nextsent>
<nextsent>we use the ziff-davis corpus, which is set of1087 pairs of sentence/compression pairs.
</nextsent>
<nextsent>furthermore, we use the same 32 testing examples from knight and marcu (2000) and the rest for training, except that we hold out 20 sentences forthe purpose of development.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4594">
<title id=" E06-1038.xml">discriminative sentence compression with soft syntactic evidence </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the ability to compress sentences grammatically with minimal information loss is an important problem in text summarization.
</prevsent>
<prevsent>most summarization systems are evaluated on the amount of relevant information retained as well as their compression rate.
</prevsent>
</prevsection>
<citsent citstr=" P05-1036 ">
thus, returning highly compressed, yet informative, sentences allows summarization systems to return larger sets of sentences and increase the overall amount of information extracted.we focus on the particular instantiation of sentence compression when the goal is to produce the compressed version solely by removing words or phrases from the original, which is the most common setting in the literature (knight and marcu, 2000; riezler et al , 2003; <papid> N03-1026 </papid>turner and charniak, 2005).<papid> P05-1036 </papid></citsent>
<aftsection>
<nextsent>in this framework, the goal is to find the shortest substring of the original sentence that conveys the most important aspects of the meaning.
</nextsent>
<nextsent>we will work in supervised learning setting and assume as input training set =(xt,yt)|t |t=1 of original sentences xt and their compress ions yt.
</nextsent>
<nextsent>we use the ziff-davis corpus, which is set of1087 pairs of sentence/compression pairs.
</nextsent>
<nextsent>furthermore, we use the same 32 testing examples from knight and marcu (2000) and the rest for training, except that we hold out 20 sentences forthe purpose of development.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4597">
<title id=" E06-1038.xml">discriminative sentence compression with soft syntactic evidence </title>
<section> discriminative sentence compression.  </section>
<citcontext>
<prevsection>
<prevsent>thus, since xn is by definition in every compressed version of (see above), then it must be the case that c[n] stores the score of the best compression.
</prevsent>
<prevsent>this table can be filled in o(n2).
</prevsent>
</prevsection>
<citsent citstr=" P05-1012 ">
this algorithm is really an extension of viterbito the case when scores factor over dynamic sub strings of the text (sarawagi and cohen, 2004; mcdonald et al , 2005<papid> P05-1012 </papid>a).</citsent>
<aftsection>
<nextsent>as such, we can use back-pointers to reconstruct the highest scoring compression as well as k-best decoding algo rithms.this decoding algorithm is dynamic with respect to compression rate.
</nextsent>
<nextsent>that is, the algorithm will return the highest scoring compression regardless of length.
</nextsent>
<nextsent>this may seem problematic since longer compress ions might contribute more to the score (since they contain more bigrams) andthus be preferred.
</nextsent>
<nextsent>however, in section 3.2 we define rich feature set, including features on words dropped from the compression that will help disfavor compress ions that drop very few words since 299 this is rarely seen in the training data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4601">
<title id=" E06-1038.xml">discriminative sentence compression with soft syntactic evidence </title>
<section> discriminative sentence compression.  </section>
<citcontext>
<prevsection>
<prevsent>for instance, dropping verbs is not that uncommon - relative clause for instance may be dropped during compression.
</prevsent>
<prevsent>however, dropping the main verb in the sentence is uncommon, since that verb and its arguments typically encode most of the information being conveyed.an obvious solution to this problem is to include features over deep syntactic analysis of the sentence.
</prevsent>
</prevsection>
<citsent citstr=" A00-2018 ">
to do this we parse every sentence twice, once with dependency parser (mcdon ald et al , 2005<papid> P05-1012 </papid>b) and once with phrase-structure parser (charniak, 2000).<papid> A00-2018 </papid></citsent>
<aftsection>
<nextsent>these parsers have been trained out-of-domain on the penn wsj treebank and as result contain noise.
</nextsent>
<nextsent>however, we are merely going to use them as an additional source of features.
</nextsent>
<nextsent>we call this soft syntactic evidence since the deep trees are not used as strict gold standard in our model but just as more evidence for 300 root0 saw2 on4 after6 mary1 ralph3 tuesday5 lunch7 vp pp pp np np np np nnp vbd nnp in nnp in nn mary1 saw2 ralph3 on4 tuesday5 after6 lunch7 figure 2: an example dependency tree from the mcdonald et al  (2005<papid> P05-1012 </papid>b) parser and phrase structure tree from the charniak (2000) <papid> A00-2018 </papid>parser.</nextsent>
<nextsent>in this example we want to add features from the trees for the case when ralph and after become adjacent in the compression, i.e., we are dropping the phrase on tuesday.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4607">
<title id=" E06-1038.xml">discriminative sentence compression with soft syntactic evidence </title>
<section> discriminative sentence compression.  </section>
<citcontext>
<prevsection>
<prevsent>thus, if the parses have too much noise, the learning algorithm can lower the weight of the parse features since they are unlikely to be useful discriminators on the training data.
</prevsent>
<prevsent>this differs from the models of knight and marcu (2000), which treat the noisy parses as gold-standard when 301 calculating probability estimates.
</prevsent>
</prevsection>
<citsent citstr=" N03-1028 ">
an important distinction we should make is the notion of supported versus unsupported features (sha and pereira, 2003).<papid> N03-1028 </papid></citsent>
<aftsection>
<nextsent>supported features are those that are on for the gold standard compress ions in the training.
</nextsent>
<nextsent>for instance, the bigram feature nn&vb;?
</nextsent>
<nextsent>will be supported since there is most likely compression that contains adjacent noun and verb.
</nextsent>
<nextsent>however, the feature jj&vb;?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4614">
<title id=" E06-1038.xml">discriminative sentence compression with soft syntactic evidence </title>
<section> discriminative sentence compression.  </section>
<citcontext>
<prevsection>
<prevsent>furthermore we found that after only 3-5 training epochs performance on the development data was maximized.
</prevsent>
<prevsent>the final weight vector is the average of all weight vectors throughout training.
</prevsent>
</prevsection>
<citsent citstr=" W02-1001 ">
averaging has been shown to reduce over fitting (collins, 2002) <papid> W02-1001 </papid>as well as reliance on the order of the examples during training.</citsent>
<aftsection>
<nextsent>we found it to be particularly important for this dataset.
</nextsent>
<nextsent>we use the same experimental methodology as knight and marcu (2000).
</nextsent>
<nextsent>we provide every compression to four judges and ask them to evaluate each one for grammaticality and importance on scale from 1 to 5.
</nextsent>
<nextsent>for each of the 32 sentences inour test set we ask the judges to evaluate three sys tems: human annotated, the decision tree model of knight and marcu (2000) and our system.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4617">
<title id=" E06-3009.xml">example based metonymy recognition for proper nouns </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>nixon in example (2) does not violate these lectional restrictions of the verb to bomb, and yet, it metonymically refers to the army under nixons command.
</prevsent>
<prevsent>(2) nixon bombed hanoi.
</prevsent>
</prevsection>
<citsent citstr=" W02-1027 ">
this example shows that metonymy recognition should not be based on rigid rules, but rather on statistical information about the semantic and grammatical context in which the target word oc curs.this statistical dependency between the reading of word and its grammatical and semantic context was investigated by markert and nissim (2002<papid> W02-1027 </papid>a) and nissim and markert (2003), <papid> P03-1008 </papid>nissim and markert (2005).</citsent>
<aftsection>
<nextsent>the key to their approach was the insight that metonymy recognition is basically sub problem of word sense disambiguation (wsd).
</nextsent>
<nextsent>possibly metonymical words are polysemous, andthey generally belong to one of number of predefined metonymical categories.
</nextsent>
<nextsent>hence, like wsd,metonymy recognition boils down to the automatic assignment of sense label to polysemous word.
</nextsent>
<nextsent>this insight thus implied that all machine learning approaches to wsd can also be applied to metonymy recognition.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4618">
<title id=" E06-3009.xml">example based metonymy recognition for proper nouns </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>nixon in example (2) does not violate these lectional restrictions of the verb to bomb, and yet, it metonymically refers to the army under nixons command.
</prevsent>
<prevsent>(2) nixon bombed hanoi.
</prevsent>
</prevsection>
<citsent citstr=" P03-1008 ">
this example shows that metonymy recognition should not be based on rigid rules, but rather on statistical information about the semantic and grammatical context in which the target word oc curs.this statistical dependency between the reading of word and its grammatical and semantic context was investigated by markert and nissim (2002<papid> W02-1027 </papid>a) and nissim and markert (2003), <papid> P03-1008 </papid>nissim and markert (2005).</citsent>
<aftsection>
<nextsent>the key to their approach was the insight that metonymy recognition is basically sub problem of word sense disambiguation (wsd).
</nextsent>
<nextsent>possibly metonymical words are polysemous, andthey generally belong to one of number of predefined metonymical categories.
</nextsent>
<nextsent>hence, like wsd,metonymy recognition boils down to the automatic assignment of sense label to polysemous word.
</nextsent>
<nextsent>this insight thus implied that all machine learning approaches to wsd can also be applied to metonymy recognition.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4632">
<title id=" E06-3009.xml">example based metonymy recognition for proper nouns </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this knowledge acquisition bottle neck is well-known problem in nlp, and many approaches have been developed to address it.
</prevsent>
<prevsent>one of these is active learning, or sample selection, astrategy that makes it possible to selectively annotate those examples that are most helpful to the classifier.
</prevsent>
</prevsection>
<citsent citstr=" N04-1012 ">
it has previously been applied to nlp tasks such as parsing (hwa, 2002; osborne and baldridge, 2004) <papid> N04-1012 </papid>and word sense disambiguation (fujii et al , 1998).<papid> J98-4002 </papid></citsent>
<aftsection>
<nextsent>in section 3, will introduce active learning into the field of metonymy recognition.
</nextsent>
<nextsent>as have argued, nissim and marker ts (2003)<papid> P03-1008 </papid>approach to metonymy recognition is quite com plex.</nextsent>
<nextsent>i therefore wanted to see if this complexity can be dispensed with, and if it can be replaced with the much more simple algorithm of memory based learning.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4633">
<title id=" E06-3009.xml">example based metonymy recognition for proper nouns </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this knowledge acquisition bottle neck is well-known problem in nlp, and many approaches have been developed to address it.
</prevsent>
<prevsent>one of these is active learning, or sample selection, astrategy that makes it possible to selectively annotate those examples that are most helpful to the classifier.
</prevsent>
</prevsection>
<citsent citstr=" J98-4002 ">
it has previously been applied to nlp tasks such as parsing (hwa, 2002; osborne and baldridge, 2004) <papid> N04-1012 </papid>and word sense disambiguation (fujii et al , 1998).<papid> J98-4002 </papid></citsent>
<aftsection>
<nextsent>in section 3, will introduce active learning into the field of metonymy recognition.
</nextsent>
<nextsent>as have argued, nissim and marker ts (2003)<papid> P03-1008 </papid>approach to metonymy recognition is quite com plex.</nextsent>
<nextsent>i therefore wanted to see if this complexity can be dispensed with, and if it can be replaced with the much more simple algorithm of memory based learning.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4647">
<title id=" E09-1019.xml">web augmentation of language models for continuous speech recognition of sms text messages </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in more recent work, the focus has turned to the collection of text rather than n-gram statistics based on page counts.
</prevsent>
<prevsent>more effort has been put into the selection of query strings.
</prevsent>
</prevsection>
<citsent citstr=" N03-2003 ">
bulyko et al (2003), <papid> N03-2003 </papid>bulyko et al (2007) first extend their baseline vocabulary with words from small in-domain training cor pus.</citsent>
<aftsection>
<nextsent>they then use n-grams with these new words in their web queries in order to retrieve text of acer tain genre.
</nextsent>
<nextsent>for instance, they succeed in obtaining conversational style phrases, such as wewere friends but we dont actually have relation ship.?
</nextsent>
<nextsent>in number of experiments, word error rate reductions of 2-3 % are obtained on english data, and 6 % on mandarin.
</nextsent>
<nextsent>the same method for web data collection is applied by cetin and stolcke (2005) in meeting and lecture transcription tasks.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4648">
<title id=" E09-1019.xml">web augmentation of language models for continuous speech recognition of sms text messages </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the error rates were 16.5, 15.9 and 15.7 for the 20 mb, 40 mb and 70 mbmodels, respectively.
</prevsent>
<prevsent>thus, kneser-ney outperformed good-turing, but the improvements were small, and statistically significant difference was measured only for the 40 mb lms.
</prevsent>
</prevsection>
<citsent citstr=" D07-1090 ">
this was expected, as it has been observed before that very simple smoothing techniques can perform well on large datasets, such as web data (brants et al, 2007).<papid> D07-1090 </papid></citsent>
<aftsection>
<nextsent>for the purpose of demonstrating the usefulness of our web data retrieval system, we concluded that there was no significant difference between gt and kn smoothing in our current setup.
</nextsent>
<nextsent>3.2 language model adaptation.
</nextsent>
<nextsent>in the second set of experiments we envisage system that adapts to the users own vocabulary.some words that the user needs may not be included in the built-in vocabulary of the device, such as names in the users contact list, names of places or words related to some specific hobby or other focus of interest.
</nextsent>
<nextsent>two adaptation techniques have been tested: (1) unigram adaptation is simple technique, in which user-specific words (for instance, names from the contact list) are added to the vocabulary.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4649">
<title id=" E12-1064.xml">towards a model of formal and informal address in english </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>darf ich sie etwas fragen?
</prevsent>
<prevsent>step 2: copy t/v class label to english sentence step 1: german pronoun provides overt t/v label v projection figure 1: t/v label induction for english sentences in parallel corpus with annotation projection in one language, but remain covert in the other.
</prevsent>
</prevsection>
<citsent citstr=" W09-0420 ">
examples include morphology (fraser, 2009) <papid> W09-0420 </papid>and tense (schiehlen, 1998).<papid> P98-2193 </papid></citsent>
<aftsection>
<nextsent>a technique that is often applied in such cases is annotation projection, the use of parallel corpora to copy information from language where it is overtly realized to one where it is not (yarowsky and ngai, 2001; <papid> N01-1026 </papid>hwa et al 2005; bentivogli and pianta, 2005).the phenomenon of formal and informal address has been considered in the contexts of translation into (hobbs and kameyama, 1990; <papid> C90-3028 </papid>kanayama, 2003) <papid> W03-1612 </papid>and generation in japanese (bateman, 1988).<papid> P88-1018 </papid></nextsent>
<nextsent>li and yarowsky (2008) <papid> D08-1108 </papid>learn pairs of formal and informal constructions in chinese with paraphrase mining strategy.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4650">
<title id=" E12-1064.xml">towards a model of formal and informal address in english </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>darf ich sie etwas fragen?
</prevsent>
<prevsent>step 2: copy t/v class label to english sentence step 1: german pronoun provides overt t/v label v projection figure 1: t/v label induction for english sentences in parallel corpus with annotation projection in one language, but remain covert in the other.
</prevsent>
</prevsection>
<citsent citstr=" P98-2193 ">
examples include morphology (fraser, 2009) <papid> W09-0420 </papid>and tense (schiehlen, 1998).<papid> P98-2193 </papid></citsent>
<aftsection>
<nextsent>a technique that is often applied in such cases is annotation projection, the use of parallel corpora to copy information from language where it is overtly realized to one where it is not (yarowsky and ngai, 2001; <papid> N01-1026 </papid>hwa et al 2005; bentivogli and pianta, 2005).the phenomenon of formal and informal address has been considered in the contexts of translation into (hobbs and kameyama, 1990; <papid> C90-3028 </papid>kanayama, 2003) <papid> W03-1612 </papid>and generation in japanese (bateman, 1988).<papid> P88-1018 </papid></nextsent>
<nextsent>li and yarowsky (2008) <papid> D08-1108 </papid>learn pairs of formal and informal constructions in chinese with paraphrase mining strategy.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4651">
<title id=" E12-1064.xml">towards a model of formal and informal address in english </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>step 2: copy t/v class label to english sentence step 1: german pronoun provides overt t/v label v projection figure 1: t/v label induction for english sentences in parallel corpus with annotation projection in one language, but remain covert in the other.
</prevsent>
<prevsent>examples include morphology (fraser, 2009) <papid> W09-0420 </papid>and tense (schiehlen, 1998).<papid> P98-2193 </papid></prevsent>
</prevsection>
<citsent citstr=" N01-1026 ">
a technique that is often applied in such cases is annotation projection, the use of parallel corpora to copy information from language where it is overtly realized to one where it is not (yarowsky and ngai, 2001; <papid> N01-1026 </papid>hwa et al 2005; bentivogli and pianta, 2005).the phenomenon of formal and informal address has been considered in the contexts of translation into (hobbs and kameyama, 1990; <papid> C90-3028 </papid>kanayama, 2003) <papid> W03-1612 </papid>and generation in japanese (bateman, 1988).<papid> P88-1018 </papid></citsent>
<aftsection>
<nextsent>li and yarowsky (2008) <papid> D08-1108 </papid>learn pairs of formal and informal constructions in chinese with paraphrase mining strategy.</nextsent>
<nextsent>other relevant recent studies consider the extraction of social networks from corpora (elson et al 2010).<papid> P10-1015 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4652">
<title id=" E12-1064.xml">towards a model of formal and informal address in english </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>step 2: copy t/v class label to english sentence step 1: german pronoun provides overt t/v label v projection figure 1: t/v label induction for english sentences in parallel corpus with annotation projection in one language, but remain covert in the other.
</prevsent>
<prevsent>examples include morphology (fraser, 2009) <papid> W09-0420 </papid>and tense (schiehlen, 1998).<papid> P98-2193 </papid></prevsent>
</prevsection>
<citsent citstr=" C90-3028 ">
a technique that is often applied in such cases is annotation projection, the use of parallel corpora to copy information from language where it is overtly realized to one where it is not (yarowsky and ngai, 2001; <papid> N01-1026 </papid>hwa et al 2005; bentivogli and pianta, 2005).the phenomenon of formal and informal address has been considered in the contexts of translation into (hobbs and kameyama, 1990; <papid> C90-3028 </papid>kanayama, 2003) <papid> W03-1612 </papid>and generation in japanese (bateman, 1988).<papid> P88-1018 </papid></citsent>
<aftsection>
<nextsent>li and yarowsky (2008) <papid> D08-1108 </papid>learn pairs of formal and informal constructions in chinese with paraphrase mining strategy.</nextsent>
<nextsent>other relevant recent studies consider the extraction of social networks from corpora (elson et al 2010).<papid> P10-1015 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4653">
<title id=" E12-1064.xml">towards a model of formal and informal address in english </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>step 2: copy t/v class label to english sentence step 1: german pronoun provides overt t/v label v projection figure 1: t/v label induction for english sentences in parallel corpus with annotation projection in one language, but remain covert in the other.
</prevsent>
<prevsent>examples include morphology (fraser, 2009) <papid> W09-0420 </papid>and tense (schiehlen, 1998).<papid> P98-2193 </papid></prevsent>
</prevsection>
<citsent citstr=" W03-1612 ">
a technique that is often applied in such cases is annotation projection, the use of parallel corpora to copy information from language where it is overtly realized to one where it is not (yarowsky and ngai, 2001; <papid> N01-1026 </papid>hwa et al 2005; bentivogli and pianta, 2005).the phenomenon of formal and informal address has been considered in the contexts of translation into (hobbs and kameyama, 1990; <papid> C90-3028 </papid>kanayama, 2003) <papid> W03-1612 </papid>and generation in japanese (bateman, 1988).<papid> P88-1018 </papid></citsent>
<aftsection>
<nextsent>li and yarowsky (2008) <papid> D08-1108 </papid>learn pairs of formal and informal constructions in chinese with paraphrase mining strategy.</nextsent>
<nextsent>other relevant recent studies consider the extraction of social networks from corpora (elson et al 2010).<papid> P10-1015 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4654">
<title id=" E12-1064.xml">towards a model of formal and informal address in english </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>step 2: copy t/v class label to english sentence step 1: german pronoun provides overt t/v label v projection figure 1: t/v label induction for english sentences in parallel corpus with annotation projection in one language, but remain covert in the other.
</prevsent>
<prevsent>examples include morphology (fraser, 2009) <papid> W09-0420 </papid>and tense (schiehlen, 1998).<papid> P98-2193 </papid></prevsent>
</prevsection>
<citsent citstr=" P88-1018 ">
a technique that is often applied in such cases is annotation projection, the use of parallel corpora to copy information from language where it is overtly realized to one where it is not (yarowsky and ngai, 2001; <papid> N01-1026 </papid>hwa et al 2005; bentivogli and pianta, 2005).the phenomenon of formal and informal address has been considered in the contexts of translation into (hobbs and kameyama, 1990; <papid> C90-3028 </papid>kanayama, 2003) <papid> W03-1612 </papid>and generation in japanese (bateman, 1988).<papid> P88-1018 </papid></citsent>
<aftsection>
<nextsent>li and yarowsky (2008) <papid> D08-1108 </papid>learn pairs of formal and informal constructions in chinese with paraphrase mining strategy.</nextsent>
<nextsent>other relevant recent studies consider the extraction of social networks from corpora (elson et al 2010).<papid> P10-1015 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4655">
<title id=" E12-1064.xml">towards a model of formal and informal address in english </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>examples include morphology (fraser, 2009) <papid> W09-0420 </papid>and tense (schiehlen, 1998).<papid> P98-2193 </papid></prevsent>
<prevsent>a technique that is often applied in such cases is annotation projection, the use of parallel corpora to copy information from language where it is overtly realized to one where it is not (yarowsky and ngai, 2001; <papid> N01-1026 </papid>hwa et al 2005; bentivogli and pianta, 2005).the phenomenon of formal and informal address has been considered in the contexts of translation into (hobbs and kameyama, 1990; <papid> C90-3028 </papid>kanayama, 2003) <papid> W03-1612 </papid>and generation in japanese (bateman, 1988).<papid> P88-1018 </papid></prevsent>
</prevsection>
<citsent citstr=" D08-1108 ">
li and yarowsky (2008) <papid> D08-1108 </papid>learn pairs of formal and informal constructions in chinese with paraphrase mining strategy.</citsent>
<aftsection>
<nextsent>other relevant recent studies consider the extraction of social networks from corpora (elson et al 2010).<papid> P10-1015 </papid></nextsent>
<nextsent>a related study is (bramsen et al 2011) <papid> P11-1078 </papid>which considers another socio linguistic distinction, classifying utterances as upspeak?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4656">
<title id=" E12-1064.xml">towards a model of formal and informal address in english </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>a technique that is often applied in such cases is annotation projection, the use of parallel corpora to copy information from language where it is overtly realized to one where it is not (yarowsky and ngai, 2001; <papid> N01-1026 </papid>hwa et al 2005; bentivogli and pianta, 2005).the phenomenon of formal and informal address has been considered in the contexts of translation into (hobbs and kameyama, 1990; <papid> C90-3028 </papid>kanayama, 2003) <papid> W03-1612 </papid>and generation in japanese (bateman, 1988).<papid> P88-1018 </papid></prevsent>
<prevsent>li and yarowsky (2008) <papid> D08-1108 </papid>learn pairs of formal and informal constructions in chinese with paraphrase mining strategy.</prevsent>
</prevsection>
<citsent citstr=" P10-1015 ">
other relevant recent studies consider the extraction of social networks from corpora (elson et al 2010).<papid> P10-1015 </papid></citsent>
<aftsection>
<nextsent>a related study is (bramsen et al 2011) <papid> P11-1078 </papid>which considers another socio linguistic distinction, classifying utterances as upspeak?</nextsent>
<nextsent>and downspeak?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4657">
<title id=" E12-1064.xml">towards a model of formal and informal address in english </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>li and yarowsky (2008) <papid> D08-1108 </papid>learn pairs of formal and informal constructions in chinese with paraphrase mining strategy.</prevsent>
<prevsent>other relevant recent studies consider the extraction of social networks from corpora (elson et al 2010).<papid> P10-1015 </papid></prevsent>
</prevsection>
<citsent citstr=" P11-1078 ">
a related study is (bramsen et al 2011) <papid> P11-1078 </papid>which considers another socio linguistic distinction, classifying utterances as upspeak?</citsent>
<aftsection>
<nextsent>and downspeak?
</nextsent>
<nextsent>based on the social relationship between speaker and addressee.
</nextsent>
<nextsent>this paper extends previous pilot study(faruqui and pad?, 2011).
</nextsent>
<nextsent>it presents more annotation, investigates larger and better motivated feature set, and discusses the findings in detail.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4659">
<title id=" E12-1064.xml">towards a model of formal and informal address in english </title>
<section> a parallel corpus of literary texts </section>
<citcontext>
<prevsection>
<prevsent>the files were then formatted to contain one sentence per line using the sentence splitter and tokenizer provided with europarl (koehn, 2005).
</prevsent>
<prevsent>blank lines were inserted to preserve paragraph boundaries.
</prevsent>
</prevsection>
<citsent citstr=" C10-2010 ">
all novels were lemmatized and pos-tagged using tree tagger (schmid, 1994).2 finally, they were sentence-aligned using gargantuan (braune and fraser, 2010), <papid> C10-2010 </papid>an aligner that supports one-to-many alignments, and word-aligned in both directions using giza++ (och and ney, 2003).<papid> J03-1002 </papid></citsent>
<aftsection>
<nextsent>3.2 t/v gold labels for english utterances as figure 1 shows, the automatic construction of t/v labels for english involves two steps.
</nextsent>
<nextsent>step 1: labeling german pronouns as t/v. german has three relevant personal pronouns for the t/v distinction: du (t), sie (v), and ihr (t/v).however, various ambiguities makes their interpretation non-straightforward.
</nextsent>
<nextsent>the pronoun ihr can both be used for plural tad dress or for somewhat archaic singular or plural address.
</nextsent>
<nextsent>in principle, these usages should be distinguished by capitalization (v pronouns are generally capitalized in german), but manyt instances in our corpora informal use are nevertheless capitalized.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4660">
<title id=" E12-1064.xml">towards a model of formal and informal address in english </title>
<section> a parallel corpus of literary texts </section>
<citcontext>
<prevsection>
<prevsent>the files were then formatted to contain one sentence per line using the sentence splitter and tokenizer provided with europarl (koehn, 2005).
</prevsent>
<prevsent>blank lines were inserted to preserve paragraph boundaries.
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
all novels were lemmatized and pos-tagged using tree tagger (schmid, 1994).2 finally, they were sentence-aligned using gargantuan (braune and fraser, 2010), <papid> C10-2010 </papid>an aligner that supports one-to-many alignments, and word-aligned in both directions using giza++ (och and ney, 2003).<papid> J03-1002 </papid></citsent>
<aftsection>
<nextsent>3.2 t/v gold labels for english utterances as figure 1 shows, the automatic construction of t/v labels for english involves two steps.
</nextsent>
<nextsent>step 1: labeling german pronouns as t/v. german has three relevant personal pronouns for the t/v distinction: du (t), sie (v), and ihr (t/v).however, various ambiguities makes their interpretation non-straightforward.
</nextsent>
<nextsent>the pronoun ihr can both be used for plural tad dress or for somewhat archaic singular or plural address.
</nextsent>
<nextsent>in principle, these usages should be distinguished by capitalization (v pronouns are generally capitalized in german), but manyt instances in our corpora informal use are nevertheless capitalized.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4661">
<title id=" E12-1064.xml">towards a model of formal and informal address in english </title>
<section> monolingual t/v modeling.  </section>
<citcontext>
<prevsection>
<prevsent>our second feature type is semantic class features.
</prevsent>
<prevsent>these can be seen as another strategy to counteract the sparseness at the level of word features.
</prevsent>
</prevsection>
<citsent citstr=" E03-1009 ">
we cluster words into 400 semantic classes on the basis of distributional and morphological similarity features which are extracted from an unlabeled english collection of gutenberg novels comprising more than 100m tokens, using the approach by clark (2003).<papid> E03-1009 </papid></citsent>
<aftsection>
<nextsent>these features measure how similar tokens are to one another in terms of their occurrences in the document and are useful in named entity recognition (finkel and manning, 2009).<papid> D09-1015 </papid></nextsent>
<nextsent>as features in the t/v classification of given sentence, we simply count for each class the number of tokens in this class present in the current sentence.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4662">
<title id=" E12-1064.xml">towards a model of formal and informal address in english </title>
<section> monolingual t/v modeling.  </section>
<citcontext>
<prevsection>
<prevsent>these can be seen as another strategy to counteract the sparseness at the level of word features.
</prevsent>
<prevsent>we cluster words into 400 semantic classes on the basis of distributional and morphological similarity features which are extracted from an unlabeled english collection of gutenberg novels comprising more than 100m tokens, using the approach by clark (2003).<papid> E03-1009 </papid></prevsent>
</prevsection>
<citsent citstr=" D09-1015 ">
these features measure how similar tokens are to one another in terms of their occurrences in the document and are useful in named entity recognition (finkel and manning, 2009).<papid> D09-1015 </papid></citsent>
<aftsection>
<nextsent>as features in the t/v classification of given sentence, we simply count for each class the number of tokens in this class present in the current sentence.
</nextsent>
<nextsent>for illustration, table 2 shows the three classes most indicative for v, ranked by the ratio of probabilities for and v, estimated on the training set.
</nextsent>
<nextsent>politeness theory features.
</nextsent>
<nextsent>the third feature type is based on the politeness theory (brownand levinson, 1987).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4663">
<title id=" E12-1064.xml">towards a model of formal and informal address in english </title>
<section> monolingual t/v modeling.  </section>
<citcontext>
<prevsection>
<prevsent>(i-ds) it will be his ghost not him!?
</prevsent>
<prevsent>(o) mr. lorry quietly chafed the hands that held his arm.11direct speech chunks belonging to the same sentence are subsequently recombined.
</prevsent>
</prevsection>
<citsent citstr=" W95-0107 ">
we define the direct speech context of size forgiven sentence as the preceding and following direct speech chunks that are labeled b-ds or i-ds while skipping any chunks labeled o. note that this definition of direct speech context still lumps 9the labels are chosen after iob notation conventions (ramshaw and marcus, 1995).<papid> W95-0107 </papid></citsent>
<aftsection>
<nextsent>10we also experimented with rule-based chunk labeling based on quotes, but found the use of quotes too inconsistent.
</nextsent>
<nextsent>11c.
</nextsent>
<nextsent>dickens: tale of two cities.
</nextsent>
<nextsent>0 2 4 6 8 10 61 62 63 64 65 66 67 context size (n) acc ura cy ( %) ? ?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4665">
<title id=" E12-1066.xml">smart paradigms and the predictability and complexity of inflectional morphology </title>
<section> smart paradigms.  </section>
<citcontext>
<prevsection>
<prevsent>section 7 concludes.
</prevsent>
<prevsent>in this paper, we will assume notion of paradigm that allows multiple arguments and arbitrary computable string operations.
</prevsent>
</prevsection>
<citsent citstr=" J94-3001 ">
as argued in (ka plan and kay, 1994) <papid> J94-3001 </papid>and amply demonstrated in (beesley and karttunen, 2003), no generality is lost if the string operators are restricted to ones computable by finite-state transducers.</citsent>
<aftsection>
<nextsent>thus the examples of paradigms that we will show (onlyinformally), can be converted to matching andre placements with regular expressions.
</nextsent>
<nextsent>for example, majority of french verbs can be defined by the following paradigm, which analyzes variable-size suffix of the infinitive form and dispatches to the bescherelle paradigms (identified by number and an example verb): mkv : string?
</nextsent>
<nextsent>string51 mkv(s) = ? conj19finir(s), if ends ir ? conj53rendre(s), if ends re ? conj14assieger(s), if ends eger ? conj11jeter(s), if ends eler or eter ? conj10ceder(s), if ends eder ? conj07placer(s), if ends cer ? conj08manger(s), if ends ger ? conj16payer(s), if ends yer ? conj06parler(s), if ends er notice that the cases must be applied in the given order; for instance, the last case applies only to those verbs ending wither that are not matched by the earlier cases.
</nextsent>
<nextsent>also notice that the above paradigm is just like the more traditional ones, in the sense that we cannot be sure if it really applies to given verb.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4666">
<title id=" E12-1066.xml">smart paradigms and the predictability and complexity of inflectional morphology </title>
<section> experimental results.  </section>
<citcontext>
<prevsection>
<prevsent>using it gives predictive power 1.13 as opposed to 1.22 with the infinitive.some modern dictionaries such as lexin4 therefore use the present indicative as the base form.
</prevsent>
<prevsent>4.3 french.
</prevsent>
</prevsection>
<citsent citstr=" W04-2104 ">
for french, we used the morphalou morphological lexicon (romary et al 2004).<papid> W04-2104 </papid></citsent>
<aftsection>
<nextsent>as stated inthe documentation5 the current version of the lexicon (version 2.0) is not complete, and in particular, many entries are missing some or all inflected forms.
</nextsent>
<nextsent>so for those experiments we only 4http://lexin.nada.kth.se/lexin/ 5http://www.cnrtl.fr/lexiques/ morphalou/lmf-morphalou.php#body_3.4.11, accessed 2011-11-04 649 table 1: lexicon size and average cost for the nouns (n) and verbs (v) in four languages, with the percentage of words correctly inferred from one and two forma (i.e. = 1 and ? 2, respectively).
</nextsent>
<nextsent>lexicon forms entries cost = 1 ? 2 eng 2 15,029 1.05 95% 100% eng 5 5,692 1.21 84% 95% swe 9 59,225 1.70 46% 92% swe 20 4,789 1.13 97% 97% fre 3 42,390 1.25 76% 99% fre 51 6,851 1.27 92% 94% fin 34 25,365 1.26 87% 97% fin 102 10,355 1.09 96% 99% included entries where all the necessary forms were presents.nouns: nouns in french have two forms (singular and plural) and an intrinsic gender (mascu line or feminine), which we also considered to bea part of the inflection table.
</nextsent>
<nextsent>most of the unpredictability comes from the impossibility to guess the gender.verbs: the paradigms generate all of the simple (as opposed to compound) tenses given intra ditional grammars such as the bescherelle.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4667">
<title id=" E12-1066.xml">smart paradigms and the predictability and complexity of inflectional morphology </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>examples of guess ers include (chanod and tapanainen, 1995) for french, (hlavacova?,2001) for czech, and (nakov et al 2003) forger man. another related domain is the unsupervised learning of morphology where machine learning is used to automatically build language morphology from corpora (goldsmith, 2006).
</prevsent>
<prevsent>the main difference is that with the smart paradigms,the paradigms and the guess heuristics are implemented manually and with high certainty; in unsupervised learning of morphology the paradigms are induced from the input forms with much lower certainty.
</prevsent>
</prevsection>
<citsent citstr=" W06-3209 ">
of particular interest are (chan, 2006) <papid> W06-3209 </papid>and (dreyer and eisner, 2011), <papid> D11-1057 </papid>dealing with the automatic extraction of paradigms from text and investigate how good these can become.</citsent>
<aftsection>
<nextsent>the main contrast is, again, that our work deals with hand written paradigms that are correct by design, and we try to see how much information we can drop before losing correctness.
</nextsent>
<nextsent>once given, set of paradigms can be used in automated lexicon extraction from raw data, as in (forsberg et al 2006) and (clement et al 2004),by method that tries to collect sufficient number of forms to determine that word belongs to certain paradigm.
</nextsent>
<nextsent>smart paradigms can then give the method to actually construct the full inflection tables from the characteristic forms.
</nextsent>
<nextsent>we have introduced the notion of smart paradigms, which implement the linguistic knowledge involved in inferring the inflection of words.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4668">
<title id=" E12-1066.xml">smart paradigms and the predictability and complexity of inflectional morphology </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>examples of guess ers include (chanod and tapanainen, 1995) for french, (hlavacova?,2001) for czech, and (nakov et al 2003) forger man. another related domain is the unsupervised learning of morphology where machine learning is used to automatically build language morphology from corpora (goldsmith, 2006).
</prevsent>
<prevsent>the main difference is that with the smart paradigms,the paradigms and the guess heuristics are implemented manually and with high certainty; in unsupervised learning of morphology the paradigms are induced from the input forms with much lower certainty.
</prevsent>
</prevsection>
<citsent citstr=" D11-1057 ">
of particular interest are (chan, 2006) <papid> W06-3209 </papid>and (dreyer and eisner, 2011), <papid> D11-1057 </papid>dealing with the automatic extraction of paradigms from text and investigate how good these can become.</citsent>
<aftsection>
<nextsent>the main contrast is, again, that our work deals with hand written paradigms that are correct by design, and we try to see how much information we can drop before losing correctness.
</nextsent>
<nextsent>once given, set of paradigms can be used in automated lexicon extraction from raw data, as in (forsberg et al 2006) and (clement et al 2004),by method that tries to collect sufficient number of forms to determine that word belongs to certain paradigm.
</nextsent>
<nextsent>smart paradigms can then give the method to actually construct the full inflection tables from the characteristic forms.
</nextsent>
<nextsent>we have introduced the notion of smart paradigms, which implement the linguistic knowledge involved in inferring the inflection of words.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4669">
<title id=" E09-1040.xml">endtoend evaluation in simultaneous translation </title>
<section> evaluation tasks.  </section>
<citcontext>
<prevsection>
<prevsent>the asr module is evaluated by computing the word error rate (wer) in case insensitive mode.slt evaluation.
</prevsent>
<prevsent>for the slt evaluation, the automatically translated text from the asr output is compared with two manual reference translations by means of automatic and human metrics.
</prevsent>
</prevsection>
<citsent citstr=" H94-1024 ">
two automatic metrics are used: bleu (papineni et al., 2001) and mwer (niessen et al, 2000).for the human evaluation, each segment is evaluated in relation to adequacy and fluency (whiteand oconnell, 1994).<papid> H94-1024 </papid></citsent>
<aftsection>
<nextsent>for the evaluation of adequacy, the target segment is compared to reference segment.
</nextsent>
<nextsent>for the evaluation of fluency, the quality of the language is evaluated.
</nextsent>
<nextsent>the two types of evaluation are done independently, but each evaluator did both evaluations (first that of fluency, then that of adequacy) for certain number of segments.
</nextsent>
<nextsent>for the evaluation of fluency, evaluators had to answer the question: is the text written in good spanish??.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4670">
<title id=" E06-2022.xml">multilingual term extraction from domain specific corpora using morphological structure </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these patterns use words as basic units and thus apply to multi-word terms.
</prevsent>
<prevsent>methods forthe acquisition of single-word terms generally depend on frequency-related information.
</prevsent>
</prevsection>
<citsent citstr=" W00-0901 ">
for instance, the frequency of occurrence of word in domain-specific corpus can be compared with its frequency of occurrence in reference corpus(rayson and garside, 2000; <papid> W00-0901 </papid>baroni and bernar dini, 2004).</citsent>
<aftsection>
<nextsent>technical words usually have high relative frequency difference between the domain specific corpus and the reference corpus.in this paper, we present pattern-based technique to extract single-word terms.
</nextsent>
<nextsent>in technical and scientific domains like medicine many terms are derivatives or neoclassical compounds (cot tez, 1984).
</nextsent>
<nextsent>there are several types of classicalword-forming units: prefixes (extra-, anti-), initial combining forms (hydro-, pharmaco-), suffixes (-ism) and final combining forms (-graphy,-logy).
</nextsent>
<nextsent>interestingly, these units are rather constant in many european languages (namer, 2005).consequently, instead of relying on subword dictionary to analyse compounds like (schulz et al, 2002), <papid> W02-0309 </papid>our method makes use of these regularities to automatically extract prefixes and initial combining forms from corpora.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4671">
<title id=" E06-2022.xml">multilingual term extraction from domain specific corpora using morphological structure </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in technical and scientific domains like medicine many terms are derivatives or neoclassical compounds (cot tez, 1984).
</prevsent>
<prevsent>there are several types of classicalword-forming units: prefixes (extra-, anti-), initial combining forms (hydro-, pharmaco-), suffixes (-ism) and final combining forms (-graphy,-logy).
</prevsent>
</prevsection>
<citsent citstr=" W02-0309 ">
interestingly, these units are rather constant in many european languages (namer, 2005).consequently, instead of relying on subword dictionary to analyse compounds like (schulz et al, 2002), <papid> W02-0309 </papid>our method makes use of these regularities to automatically extract prefixes and initial combining forms from corpora.</citsent>
<aftsection>
<nextsent>the system then identifies terms by selecting words which either begin or coalesce with these units.
</nextsent>
<nextsent>moreover, forming elements are used to group terms in morphological and hence semantic families.
</nextsent>
<nextsent>the different stages of the process are detailed in section 2.
</nextsent>
<nextsent>section 3 describes the results of experiments performed on four corpora, in english and in french.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4673">
<title id=" E06-2009.xml">an isu dialogue system exhibiting reinforcement learning of dialogue policies generic slot filling in the talk incar system </title>
<section> component-level description.  </section>
<citcontext>
<prevsection>
<prevsent>solvable).
</prevsent>
<prevsent>therl agent can be used to drive the entire dialogue policy, or can be called only in certain circumstances.
</prevsent>
</prevsection>
<citsent citstr=" C00-1073 ">
this makes it usable for whole dialogue strategies, but also, if desired, it can be targetted only on specific dialogue management decisions (e.g. implicit vs. explicit confirmation, as was done by (litman et al, 2000)).<papid> C00-1073 </papid></citsent>
<aftsection>
<nextsent>one important research issue is that of tranferringlearnt strategies between domains.
</nextsent>
<nextsent>we learnt strategy for the communicator flight booking dialogues (lemon et al, 2005; henderson et al, 2005), but this is generated by rather different scenarios than the in-car dialogues.
</nextsent>
<nextsent>however, both are slot-filling?
</nextsent>
<nextsent>orinformation-seeking applications.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4674">
<title id=" E06-1036.xml">recognizing textual parallel isms with edit distance and similarity degree </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>detection of discourse structure is crucial in manytext-based applications such as information retrieval, question-answering, text browsing, etc. thanks to discourse structure one can precisely point out an information, provide it local context, situate it globally, link it to others.the context of our research is to improve automatic discourse analysis.
</prevsent>
<prevsent>a key feature of the most popular discourse theories (rst (mann and thompson, 1987), sdrt (asher, 1993), etc.) isthe distinction between two sorts of discourse relations or rhetorical functions: the subordinating and the coordinating relations (some parts of text play subordinate role relative to other parts, while some others have equal importance).in this paper, we focus our attention on discourse feature we assume supporting coordination relations, namely the textual parallelism.
</prevsent>
</prevsection>
<citsent citstr=" H05-1104 ">
based on psycho linguistics studies (dubey et al, 2005),<papid> H05-1104 </papid>our intuition is that similarities concerning the surface, the content and the structure of textual units can be way for authors to explicit their intention to consider these units with the same rhetorical im portance.</citsent>
<aftsection>
<nextsent>parallelism can be encountered in many specific discourse structures such as continuity in information structure (kruijff-korbayova?
</nextsent>
<nextsent>and kruijff,1996), frame structures (charolles, 1997), vp ellipses (hobbs and kehler, 1997), <papid> P97-1051 </papid>headings (sum mers, 1998), enumerations (luc et al, 1999), etc.these phenomena are usually treated mostly independently within individual systems with ad-hoc resource developments.in this work, we argue that, depending on description granularity we can proceed, computing syntagmatic (succession axis of linguistic units) and paradigmatic (substitution axis) similarities between units can allow us to generically handle such discourse structural phenomena.</nextsent>
<nextsent>section 2 introduces the discourse parallelism phenomenon.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4675">
<title id=" E06-1036.xml">recognizing textual parallel isms with edit distance and similarity degree </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>based on psycho linguistics studies (dubey et al, 2005),<papid> H05-1104 </papid>our intuition is that similarities concerning the surface, the content and the structure of textual units can be way for authors to explicit their intention to consider these units with the same rhetorical im portance.</prevsent>
<prevsent>parallelism can be encountered in many specific discourse structures such as continuity in information structure (kruijff-korbayova?</prevsent>
</prevsection>
<citsent citstr=" P97-1051 ">
and kruijff,1996), frame structures (charolles, 1997), vp ellipses (hobbs and kehler, 1997), <papid> P97-1051 </papid>headings (sum mers, 1998), enumerations (luc et al, 1999), etc.these phenomena are usually treated mostly independently within individual systems with ad-hoc resource developments.in this work, we argue that, depending on description granularity we can proceed, computing syntagmatic (succession axis of linguistic units) and paradigmatic (substitution axis) similarities between units can allow us to generically handle such discourse structural phenomena.</citsent>
<aftsection>
<nextsent>section 2 introduces the discourse parallelism phenomenon.
</nextsent>
<nextsent>section 3 develops three methods we implemented to detect it: similarity degree measure, string editing distance (wagner and fischer, 1974) and tree editing distance1 (zhang and shasha, 1989).
</nextsent>
<nextsent>section 4 discusses and evaluates these method sand their relevance.
</nextsent>
<nextsent>the final section reviews related work.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4678">
<title id=" E09-3005.xml">structural correspondence learning for parse disambiguation </title>
<section> abstract </section>
<citcontext>
<prevsection>


</prevsection>
<citsent citstr=" W06-1615 ">
the paper presents an application of structural correspondence learning (scl) (blitzer et al, 2006) <papid> W06-1615 </papid>for domain adaptation of stochastic attribute-value grammar (savg).</citsent>
<aftsection>
<nextsent>so far, scl has been applied successfully in nlp for part-of-speech tagging and sentiment analysis (blitzer et al, 2006; <papid> W06-1615 </papid>blitzer et al, 2007).<papid> P07-1056 </papid></nextsent>
<nextsent>an attempt was madein the conll 2007 shared task to apply scl to non-projective dependency parsing (shimizu and nakagawa, 2007), <papid> D07-1129 </papid>however, without any clear conclusions.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4689">
<title id=" E09-3005.xml">structural correspondence learning for parse disambiguation </title>
<section> abstract </section>
<citcontext>
<prevsection>

<prevsent>the paper presents an application of structural correspondence learning (scl) (blitzer et al, 2006) <papid> W06-1615 </papid>for domain adaptation of stochastic attribute-value grammar (savg).</prevsent>
</prevsection>
<citsent citstr=" P07-1056 ">
so far, scl has been applied successfully in nlp for part-of-speech tagging and sentiment analysis (blitzer et al, 2006; <papid> W06-1615 </papid>blitzer et al, 2007).<papid> P07-1056 </papid></citsent>
<aftsection>
<nextsent>an attempt was madein the conll 2007 shared task to apply scl to non-projective dependency parsing (shimizu and nakagawa, 2007), <papid> D07-1129 </papid>however, without any clear conclusions.</nextsent>
<nextsent>we report on our exploration of applying scl to adapt syntactic disambiguation model and show promising initial results on wikipedia domains.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4690">
<title id=" E09-3005.xml">structural correspondence learning for parse disambiguation </title>
<section> abstract </section>
<citcontext>
<prevsection>
<prevsent>the paper presents an application of structural correspondence learning (scl) (blitzer et al, 2006) <papid> W06-1615 </papid>for domain adaptation of stochastic attribute-value grammar (savg).</prevsent>
<prevsent>so far, scl has been applied successfully in nlp for part-of-speech tagging and sentiment analysis (blitzer et al, 2006; <papid> W06-1615 </papid>blitzer et al, 2007).<papid> P07-1056 </papid></prevsent>
</prevsection>
<citsent citstr=" D07-1129 ">
an attempt was madein the conll 2007 shared task to apply scl to non-projective dependency parsing (shimizu and nakagawa, 2007), <papid> D07-1129 </papid>however, without any clear conclusions.</citsent>
<aftsection>
<nextsent>we report on our exploration of applying scl to adapt syntactic disambiguation model and show promising initial results on wikipedia domains.
</nextsent>
<nextsent>many current, effective natural language processing systems are based on supervised machine learning techniques.
</nextsent>
<nextsent>the parameters of such systems are estimated to best reflect the characteristics of the training data, at the cost of porta bility: system will be successful only as long as the training material resembles the input thatthe model gets.
</nextsent>
<nextsent>therefore, whenever we have access to large amount of labeled data from some source?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4692">
<title id=" E09-3005.xml">structural correspondence learning for parse disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>therefore, whenever we have access to large amount of labeled data from some source?
</prevsent>
<prevsent>(out-of-domain), but we would like model that performs well on some new target?
</prevsent>
</prevsection>
<citsent citstr=" W01-0521 ">
domain (gildea, 2001; <papid> W01-0521 </papid>daume?</citsent>
<aftsection>
<nextsent>iii, 2007), we face the problem of domain adaptation.
</nextsent>
<nextsent>the need for domain adaptation arises in many nlp tasks: part-of-speech tagging, sentiment analysis, semantic role labeling or statistical parsing, to name but few.
</nextsent>
<nextsent>for example, the performance of statistical parsing system drops in an appalling way when model trained on the wall street journal is applied to the more varied brown corpus (gildea, 2001).<papid> W01-0521 </papid></nextsent>
<nextsent>the problem itself has started to get attention only recently (roark and bacchiani, 2003; <papid> N03-1027 </papid>hara et al., 2005; <papid> I05-1018 </papid>daume?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4696">
<title id=" E09-3005.xml">structural correspondence learning for parse disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the need for domain adaptation arises in many nlp tasks: part-of-speech tagging, sentiment analysis, semantic role labeling or statistical parsing, to name but few.
</prevsent>
<prevsent>for example, the performance of statistical parsing system drops in an appalling way when model trained on the wall street journal is applied to the more varied brown corpus (gildea, 2001).<papid> W01-0521 </papid></prevsent>
</prevsection>
<citsent citstr=" N03-1027 ">
the problem itself has started to get attention only recently (roark and bacchiani, 2003; <papid> N03-1027 </papid>hara et al., 2005; <papid> I05-1018 </papid>daume?</citsent>
<aftsection>
<nextsent>iii and marcu, 2006; daume?
</nextsent>
<nextsent>iii, 2007; blitzer et al, 2006; <papid> W06-1615 </papid>mcclosky et al, 2006;<papid> N06-1020 </papid>dredze et al, 2007).<papid> D07-1112 </papid></nextsent>
<nextsent>we distinguish two main approaches to domain adaptation that have been addressed in the literature (daume?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4699">
<title id=" E09-3005.xml">structural correspondence learning for parse disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the need for domain adaptation arises in many nlp tasks: part-of-speech tagging, sentiment analysis, semantic role labeling or statistical parsing, to name but few.
</prevsent>
<prevsent>for example, the performance of statistical parsing system drops in an appalling way when model trained on the wall street journal is applied to the more varied brown corpus (gildea, 2001).<papid> W01-0521 </papid></prevsent>
</prevsection>
<citsent citstr=" I05-1018 ">
the problem itself has started to get attention only recently (roark and bacchiani, 2003; <papid> N03-1027 </papid>hara et al., 2005; <papid> I05-1018 </papid>daume?</citsent>
<aftsection>
<nextsent>iii and marcu, 2006; daume?
</nextsent>
<nextsent>iii, 2007; blitzer et al, 2006; <papid> W06-1615 </papid>mcclosky et al, 2006;<papid> N06-1020 </papid>dredze et al, 2007).<papid> D07-1112 </papid></nextsent>
<nextsent>we distinguish two main approaches to domain adaptation that have been addressed in the literature (daume?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4709">
<title id=" E09-3005.xml">structural correspondence learning for parse disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the problem itself has started to get attention only recently (roark and bacchiani, 2003; <papid> N03-1027 </papid>hara et al., 2005; <papid> I05-1018 </papid>daume?</prevsent>
<prevsent>iii and marcu, 2006; daume?</prevsent>
</prevsection>
<citsent citstr=" N06-1020 ">
iii, 2007; blitzer et al, 2006; <papid> W06-1615 </papid>mcclosky et al, 2006;<papid> N06-1020 </papid>dredze et al, 2007).<papid> D07-1112 </papid></citsent>
<aftsection>
<nextsent>we distinguish two main approaches to domain adaptation that have been addressed in the literature (daume?
</nextsent>
<nextsent>iii, 2007): supervised and semi-supervised.
</nextsent>
<nextsent>in supervised domain adaptation (gildea, 2001; <papid> W01-0521 </papid>roark and bacchiani, 2003; <papid> N03-1027 </papid>hara et al, 2005; <papid> I05-1018 </papid>daume?</nextsent>
<nextsent>iii, 2007), besides the labeled source data, we have access to comparably small, but labeled amount of target data.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4710">
<title id=" E09-3005.xml">structural correspondence learning for parse disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the problem itself has started to get attention only recently (roark and bacchiani, 2003; <papid> N03-1027 </papid>hara et al., 2005; <papid> I05-1018 </papid>daume?</prevsent>
<prevsent>iii and marcu, 2006; daume?</prevsent>
</prevsection>
<citsent citstr=" D07-1112 ">
iii, 2007; blitzer et al, 2006; <papid> W06-1615 </papid>mcclosky et al, 2006;<papid> N06-1020 </papid>dredze et al, 2007).<papid> D07-1112 </papid></citsent>
<aftsection>
<nextsent>we distinguish two main approaches to domain adaptation that have been addressed in the literature (daume?
</nextsent>
<nextsent>iii, 2007): supervised and semi-supervised.
</nextsent>
<nextsent>in supervised domain adaptation (gildea, 2001; <papid> W01-0521 </papid>roark and bacchiani, 2003; <papid> N03-1027 </papid>hara et al, 2005; <papid> I05-1018 </papid>daume?</nextsent>
<nextsent>iii, 2007), besides the labeled source data, we have access to comparably small, but labeled amount of target data.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4761">
<title id=" E09-3005.xml">structural correspondence learning for parse disambiguation </title>
<section> motivation and prior work.  </section>
<citcontext>
<prevsection>
<prevsent>the system just ended up at rank 7 out of 8 teams.
</prevsent>
<prevsent>however, based on annotation differences in the datasets (dredze et al, 2007) <papid> D07-1112 </papid>and bug in their system (shimizu and nakagawa, 2007), <papid> D07-1129 </papid>their results are inconclusive.1 thus, the effectiveness of scl is rather unexplored for parsing.so far, most previous work on domain adaptation for parsing has focused on data-driven systems (gildea, 2001; <papid> W01-0521 </papid>roark and bacchiani, 2003; <papid> N03-1027 </papid>mcclosky et al, 2006;<papid> N06-1020 </papid> shimizu and nakagawa,2007), <papid> D07-1129 </papid>i.e. systems employing (constituent or dependency based) treebank grammars (charniak, 1996).</prevsent>
</prevsection>
<citsent citstr=" P99-1069 ">
parse selection constitutes an important part of many parsing systems (johnson et al, 1999; <papid> P99-1069 </papid>hara et al, 2005; <papid> I05-1018 </papid>van noord and malouf, 2005; mcclosky et al, 2006).<papid> N06-1020 </papid></citsent>
<aftsection>
<nextsent>yet, the adaptation of parse selection models to novel domains is far less studied area.
</nextsent>
<nextsent>this may be motivated by the fact that potential gains for this task are inherently bounded by the underlying grammar.
</nextsent>
<nextsent>the few studies on adapting disambiguation models (haraet al, 2005; <papid> I05-1018 </papid>plank and van noord, 2008) have focused exclusively on the supervised scenario.</nextsent>
<nextsent>therefore, the direction we explore in this study is semi-supervised domain adaptation for parse disambiguation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4769">
<title id=" E09-3005.xml">structural correspondence learning for parse disambiguation </title>
<section> background: alpino parser.  </section>
<citcontext>
<prevsection>
<prevsent>for parse selection, alpino employs discriminative approach based on maximum entropy (maxent).
</prevsent>
<prevsent>the output of the parser is dependency structure based on the guidelines of cgn (oost dijk, 2000).
</prevsent>
</prevsection>
<citsent citstr=" J96-1002 ">
the maximum entropy model (berger et al,1996; <papid> J96-1002 </papid>ratnaparkhi, 1997; abney, 1997) <papid> J97-4005 </papid>is conditional model that assigns probability to every possible parse ? forgiven sentence s. the model consists of set of feature functions fj(?)</citsent>
<aftsection>
<nextsent>that describe properties of parses, together with their associated weights . the denominator is normalization term where (s) is the set of parses with yield s: p?(?|s; ?) = exp( j=1 jfj(?)) ? yy (s) exp( j=1 jfj(y))) (1) the parameters (weights) can be estimated efficiently by maximizing the regularized conditional likelihood of training corpus (johnson et al., 1999; <papid> P99-1069 </papid>van noord and malouf, 2005): ??</nextsent>
<nextsent>= arg max ? logl(?)</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4770">
<title id=" E09-3005.xml">structural correspondence learning for parse disambiguation </title>
<section> background: alpino parser.  </section>
<citcontext>
<prevsection>
<prevsent>for parse selection, alpino employs discriminative approach based on maximum entropy (maxent).
</prevsent>
<prevsent>the output of the parser is dependency structure based on the guidelines of cgn (oost dijk, 2000).
</prevsent>
</prevsection>
<citsent citstr=" J97-4005 ">
the maximum entropy model (berger et al,1996; <papid> J96-1002 </papid>ratnaparkhi, 1997; abney, 1997) <papid> J97-4005 </papid>is conditional model that assigns probability to every possible parse ? forgiven sentence s. the model consists of set of feature functions fj(?)</citsent>
<aftsection>
<nextsent>that describe properties of parses, together with their associated weights . the denominator is normalization term where (s) is the set of parses with yield s: p?(?|s; ?) = exp( j=1 jfj(?)) ? yy (s) exp( j=1 jfj(y))) (1) the parameters (weights) can be estimated efficiently by maximizing the regularized conditional likelihood of training corpus (johnson et al., 1999; <papid> P99-1069 </papid>van noord and malouf, 2005): ??</nextsent>
<nextsent>= arg max ? logl(?)</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4822">
<title id=" E09-3005.xml">structural correspondence learning for parse disambiguation </title>
<section> structural correspondence learning.  </section>
<citcontext>
<prevsection>
<prevsent>this sparse representation saves both time and space.
</prevsent>
<prevsent>4.2 further practical issues of scl.
</prevsent>
</prevsection>
<citsent citstr=" W06-2911 ">
in practice, there are more free parameters and model choices (ando and zhang, 2005; ando, 2006; <papid> W06-2911 </papid>blitzer et al, 2006; <papid> W06-1615 </papid>blitzer, 2008) besides the ones discussed above.</citsent>
<aftsection>
<nextsent>feature normalization and feature scaling.blitzer et al (2006) <papid> W06-1615 </papid>found it necessary to normalize and scale the new features obtained by the projection ?, in order to allow them to receive more weight from regularized discriminative learner?.</nextsent>
<nextsent>for each of the features, they centered them by subtracting out the mean and normalized them to unit variance (i.e. ? mean/sd).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4848">
<title id=" E09-3005.xml">structural correspondence learning for parse disambiguation </title>
<section> experiments and results.  </section>
<citcontext>
<prevsection>
<prevsent>5.1 experimental design.
</prevsent>
<prevsent>the base (source domain) disambiguation model is trained on the alpino treebank (van noord,2006) (newspaper text), which consists of approximately 7,000 sentences and 145,000 tokens.
</prevsent>
</prevsection>
<citsent citstr=" W02-2018 ">
for parameter estimation of the disambiguation model, in all reported experiments we use thetadm2 toolkit (toolkit for advanced discriminative training), with gaussian prior (2=1000) and the (default) limited memory variable metric estimation technique (malouf, 2002).<papid> W02-2018 </papid></citsent>
<aftsection>
<nextsent>for training the binary pivot predictors, we usethe megam3 optimization package with the socalled bernoulli implicit?
</nextsent>
<nextsent>input format.
</nextsent>
<nextsent>to compute the svd, we use svdlibc.4the output of the parser is dependency structure.
</nextsent>
<nextsent>a standard evaluation metric is to measure the amount of generated dependencies that are identical to the stored dependencies (correct labeled dependencies), expressed as f-score.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4874">
<title id=" E09-1050.xml">lightly supervised transliteration for machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in the context of an mt system, one has to first identify which terms should be transliterated rather than translated, and then produce proper transliteration for these terms.
</prevsent>
<prevsent>we address both tasks in this work.
</prevsent>
</prevsection>
<citsent citstr=" P08-1045 ">
identification of terms to-be transliterated (ttt) must not be confused with recognition of named entities (ne) (hermjakob et al, 2008).<papid> P08-1045 </papid></citsent>
<aftsection>
<nextsent>on the one hand, many nes should be translated rather than transliterated, for example:1 m$rd hm$p@im misrad hamishpatim ministry-of the-sentences ministry of justice1to facilitate readability, examples are presented with in ter linear gloss, including an ascii representation of hebrew orthography followed by broad phonemic transcription, aword-for-word gloss in english where relevant, and the corresponding free text in english.
</nextsent>
<nextsent>the following table presents the ascii encoding of hebrew used in this paper: !?
</nextsent>
<nextsent>a g h z @ k !?
</nextsent>
<nextsent>l n &amp; c r $ 433 him htikwn hayam hatichon the-sea the-central the mediterranean sea?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4875">
<title id=" E09-1050.xml">lightly supervised transliteration for machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>alwn alun will sleep?
</prevsent>
<prevsent>alwn alon alon?
</prevsent>
</prevsection>
<citsent citstr=" P97-1017 ">
(name) one usually distinguishes between two types of transliteration (knight and graehl, 1997): <papid> P97-1017 </papid>forward transliteration, where an originally hebrew term is to be transliterated to english; and backward transliteration, in which foreign term that has already been transliterated into hebrew is to be recovered.</citsent>
<aftsection>
<nextsent>forward transliteration may result in several acceptable alternatives.
</nextsent>
<nextsent>this is mainly due to phonetic gaps between the languages and lack of standards for expressing hebrew phonemes in english.
</nextsent>
<nextsent>for example, the hebrew term cdiq may be transliterated as tzadik, tsadik, tsaddiq, etc.on the other hand, backward transliteration is restrictive.
</nextsent>
<nextsent>there is usually only one acceptable wayto express the transliterated term.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4876">
<title id=" E09-1050.xml">lightly supervised transliteration for machine translation </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>both of the above methods perform only unidirectional transliteration, that is, either forward- or backward- transliteration, while our work handles both.al-onaizan and knight (2002) describe system which combines phonetic based model with spelling model for transliteration.
</prevsent>
<prevsent>the spelling based model directly maps sequences of english letters into sequences of arabic letters without the need of english pronunciation.
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
the method uses translation model based on ibm model 1 (brown et al, 1993), <papid> J93-2003 </papid>in which translation candidates of phrase are generated by combining translations and transliterations of the phrase components, and matching the result against large corpus.</citsent>
<aftsection>
<nextsent>the systems overall accuracy is about 72% for top-1 results and 84% for top-20 results.
</nextsent>
<nextsent>this method is restricted to transliterating nes, and performs best for person names.
</nextsent>
<nextsent>as noted above, the ttt problem is not identical to the ner problem.
</nextsent>
<nextsent>in addition, the method requires alist of transliteration pairs from which the transliteration model could be learned.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4877">
<title id=" E09-1050.xml">lightly supervised transliteration for machine translation </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>as noted above, the ttt problem is not identical to the ner problem.
</prevsent>
<prevsent>in addition, the method requires alist of transliteration pairs from which the transliteration model could be learned.
</prevsent>
</prevsection>
<citsent citstr=" P07-1015 ">
yoon et al (2007) <papid> P07-1015 </papid>use phonetic distinctive features and phonology-based pseudo feature sto learn both language-specific and language universal transliteration characteristics.</citsent>
<aftsection>
<nextsent>distinctive features are the characteristics that define the set of phonemic segments (consonants, vowels) in given language.
</nextsent>
<nextsent>pseudo features capture sound change patterns that involve the position in the syllable.
</nextsent>
<nextsent>distinctive features and pseudo features are extracted from source- and target-language training data to train linear classifier.
</nextsent>
<nextsent>the classifier computes compatibility scores between english source words and target-language words.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4878">
<title id=" E09-1050.xml">lightly supervised transliteration for machine translation </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>this biases the model toward transliterating person names.
</prevsent>
<prevsent>the language model presented for that method consisted of 10k entries of names which is, again, not complete.this model also uses different settings for maximum phrase length in the translation model and different n-gram order for the language model.
</prevsent>
</prevsection>
<citsent citstr=" P08-2014 ">
it achieves an accuracy of 43% when transliterating from arabic to english.goldwasser and roth (2008) <papid> P08-2014 </papid>introduce discriminative method for identifying ne transliteration pairs in english-hebrew.</citsent>
<aftsection>
<nextsent>given word pair (ws, wt), where ws is an english ne, the system determines whether wt, string in hebrew, is itstransliteration.
</nextsent>
<nextsent>the classification is based on pairwise features: sets of sub strings are extracted from each of the words, and sub strings from the two setsare then coupled to form the features.
</nextsent>
<nextsent>the accuracy of correctly identifying transliteration pairsin top-1 and top-5 was 52% and 88%, respectively.
</nextsent>
<nextsent>whereas this approach selects most suitable transliteration out of list of candidates, our approach generates list of possible transliterations ranked by their accuracy.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4884">
<title id=" E09-1050.xml">lightly supervised transliteration for machine translation </title>
<section> how to trans literate.  </section>
<citcontext>
<prevsection>
<prevsent>we use smt for transliteration; this approach views transliteration pairs as aligned sentences and characters are viewed as words.
</prevsent>
<prevsent>in the case ofphrase-based smt, phrases are sequences of characters.
</prevsent>
</prevsection>
<citsent citstr=" P07-2045 ">
we used moses (koehn et al, 2007), <papid> P07-2045 </papid>aphrase-based smt toolkit, for training the translation model (and later for decoding).</citsent>
<aftsection>
<nextsent>in order to extract phrases, bidirectional word level alignments are first created, both source to target and target to source.
</nextsent>
<nextsent>alignments are merged heuristic ally if they are consistent, in order to extract phrases.
</nextsent>
<nextsent>5.1 target language model.
</nextsent>
<nextsent>we created an english target language model from unigrams of web 1t (brants and franz, 2006).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4885">
<title id=" E12-1006.xml">cross framework evaluation for statistical parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the goal of statistical parsers is to recover formal representation of the grammatical relations that constitute the argument structure of natural language sentences.
</prevsent>
<prevsent>the argument structure encompasses grammatical relationships between elements such as subject, predicate, object, etc.,which are useful for further (e.g., semantic) processing.
</prevsent>
</prevsection>
<citsent citstr=" N10-1049 ">
the parses yielded by different parsing frameworks typically obey different formal and theoretical assumptions concerning how to represent the grammatical relationships in the data(rambow, 2010).<papid> N10-1049 </papid></citsent>
<aftsection>
<nextsent>for example, grammatical relations may be encoded on top of dependency arcs in dependency tree (melcuk, 1988), they may decorate nodes in phrase-structure tree (marcus et al  1993; <papid> J93-2004 </papid>maamouri et al  2004; simaan et al ., 2001), or they may be read off of positions ina phrase-structure tree using hard-coded conversion procedures (de marneffe et al  2006).</nextsent>
<nextsent>this diversity poses challenge to cross-experimental parser evaluation, namely: how can we evaluate the performance of these different parsers relative to one another?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4887">
<title id=" E12-1006.xml">cross framework evaluation for statistical parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the argument structure encompasses grammatical relationships between elements such as subject, predicate, object, etc.,which are useful for further (e.g., semantic) processing.
</prevsent>
<prevsent>the parses yielded by different parsing frameworks typically obey different formal and theoretical assumptions concerning how to represent the grammatical relationships in the data(rambow, 2010).<papid> N10-1049 </papid></prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
for example, grammatical relations may be encoded on top of dependency arcs in dependency tree (melcuk, 1988), they may decorate nodes in phrase-structure tree (marcus et al  1993; <papid> J93-2004 </papid>maamouri et al  2004; simaan et al ., 2001), or they may be read off of positions ina phrase-structure tree using hard-coded conversion procedures (de marneffe et al  2006).</citsent>
<aftsection>
<nextsent>this diversity poses challenge to cross-experimental parser evaluation, namely: how can we evaluate the performance of these different parsers relative to one another?
</nextsent>
<nextsent>current evaluation practices assume set of correctly annotated test data (or gold standard)for evaluation.
</nextsent>
<nextsent>typically, every parser is evaluated with respect to its own formal representation type and the underlying theory which it was trained to recover.
</nextsent>
<nextsent>therefore, numerical scores of parses across experiments are incomparable.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4888">
<title id=" E12-1006.xml">cross framework evaluation for statistical parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>and secondly, how can we alleviate the differences between formal representation types and theoretical assumptions in order to make our comparison sound ? that is, to make sure that we are not comparing apples and oranges?
</prevsent>
<prevsent>a popular way to address this has been to pick one of the frameworks and convert all parser outputs to its formal type.
</prevsent>
</prevsection>
<citsent citstr=" C10-1094 ">
when comparing constituency-based and dependency-based parsers, for instance, the output of constituency parsers has often been converted to dependency structures prior to evaluation (cer et al  2010;nivre et al  2010).<papid> C10-1094 </papid></citsent>
<aftsection>
<nextsent>this solution has various drawbacks.
</nextsent>
<nextsent>first, it demands conversion script that maps one representation type to another when some theoretical assumptions in one framework may be incompatible with the other one.
</nextsent>
<nextsent>in the constituency-to-dependency case, some constituency-based structures (e.g., coordination 44 and ellipsis) do not comply with the single head assumption of dependency treebanks.
</nextsent>
<nextsent>secondly, these scripts may be labor intensive to create, andare available mostly for english.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4889">
<title id=" E12-1006.xml">cross framework evaluation for statistical parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in the constituency-to-dependency case, some constituency-based structures (e.g., coordination 44 and ellipsis) do not comply with the single head assumption of dependency treebanks.
</prevsent>
<prevsent>secondly, these scripts may be labor intensive to create, andare available mostly for english.
</prevsent>
</prevsection>
<citsent citstr=" D11-1036 ">
so the evaluation protocol becomes language-dependent.in tsarfaty et al (2011) <papid> D11-1036 </papid>we proposed general protocol for handling annotation discrepancies when comparing parses across different dependency theories.</citsent>
<aftsection>
<nextsent>the protocol consists of three phases: converting all structures into function trees, for each sentence, generalizing the different gold standard function trees to get their common denominator, and employing an evaluation measure based on tree edit distance (ted) which discards edit operations that recover theory-specific structures.
</nextsent>
<nextsent>although the protocol is potentially applicable to wide class of syntactic representation types, formal restrictions in the procedures effectively limit its applicability only to representations that are isomorphic to dependency trees.
</nextsent>
<nextsent>the present paper breaks new ground in the ability to soundly compare the accuracy of different parsers relative to one another given that they employ different formal representation types andobey different theoretical assumptions.
</nextsent>
<nextsent>our solution generally confines with the protocol proposed in tsarfaty et al (2011) <papid> D11-1036 </papid>but is re-formalized to allow for arbitrary linearly ordered labeled trees, thus encompassing constituency-based as well asdependency-based representations.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4901">
<title id=" E12-1006.xml">cross framework evaluation for statistical parsing </title>
<section> preliminaries: relational schemes for.  </section>
<citcontext>
<prevsection>
<prevsent>all in all we show that our extended protocol, which can handle linearly ordered labeled trees with arbitrary branching, can soundly compare parsing results across frameworks in representation-independent and language-independent fashion.
</prevsent>
<prevsent>cross-framework parse evaluation traditionally, different statistical parsers have been evaluated using specially designated evaluation measures that are designed to fit their representation types.
</prevsent>
</prevsection>
<citsent citstr=" W06-2920 ">
dependency trees are evaluated using attachment scores (buchholz and marsi, 2006), <papid> W06-2920 </papid>phrase-structure trees are evaluated using parseval (black et al  1991), <papid> H91-1060 </papid>lfg-based parsers postulate an evaluation procedure based on structures (cahill et al  2008), <papid> J08-1003 </papid>and so on.</citsent>
<aftsection>
<nextsent>from downstream application point of view, there is no significance as to which formalism was used for generating the representation and which learning methods have been utilized.
</nextsent>
<nextsent>the bottom line is simply which parsing framework most accurately recovers useful representation that helps to unravel the human-perceived interpretation.relational schemes, that is, schemes that encode the set of grammatical relations that constitute the predicate-argument structures of sentences, provide an interface to semantic interpretation.
</nextsent>
<nextsent>they are more intuitively understood than, say, phrase-structure trees, and thus they are also more useful for practical applications.
</nextsent>
<nextsent>for these reasons, relational schemes have been repeatedly singled out as an appropriate level of representation for the evaluation of statistical parsers (lin, 1995; carroll et al  1998; cer et al  2010).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4902">
<title id=" E12-1006.xml">cross framework evaluation for statistical parsing </title>
<section> preliminaries: relational schemes for.  </section>
<citcontext>
<prevsection>
<prevsent>all in all we show that our extended protocol, which can handle linearly ordered labeled trees with arbitrary branching, can soundly compare parsing results across frameworks in representation-independent and language-independent fashion.
</prevsent>
<prevsent>cross-framework parse evaluation traditionally, different statistical parsers have been evaluated using specially designated evaluation measures that are designed to fit their representation types.
</prevsent>
</prevsection>
<citsent citstr=" H91-1060 ">
dependency trees are evaluated using attachment scores (buchholz and marsi, 2006), <papid> W06-2920 </papid>phrase-structure trees are evaluated using parseval (black et al  1991), <papid> H91-1060 </papid>lfg-based parsers postulate an evaluation procedure based on structures (cahill et al  2008), <papid> J08-1003 </papid>and so on.</citsent>
<aftsection>
<nextsent>from downstream application point of view, there is no significance as to which formalism was used for generating the representation and which learning methods have been utilized.
</nextsent>
<nextsent>the bottom line is simply which parsing framework most accurately recovers useful representation that helps to unravel the human-perceived interpretation.relational schemes, that is, schemes that encode the set of grammatical relations that constitute the predicate-argument structures of sentences, provide an interface to semantic interpretation.
</nextsent>
<nextsent>they are more intuitively understood than, say, phrase-structure trees, and thus they are also more useful for practical applications.
</nextsent>
<nextsent>for these reasons, relational schemes have been repeatedly singled out as an appropriate level of representation for the evaluation of statistical parsers (lin, 1995; carroll et al  1998; cer et al  2010).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4903">
<title id=" E12-1006.xml">cross framework evaluation for statistical parsing </title>
<section> preliminaries: relational schemes for.  </section>
<citcontext>
<prevsection>
<prevsent>all in all we show that our extended protocol, which can handle linearly ordered labeled trees with arbitrary branching, can soundly compare parsing results across frameworks in representation-independent and language-independent fashion.
</prevsent>
<prevsent>cross-framework parse evaluation traditionally, different statistical parsers have been evaluated using specially designated evaluation measures that are designed to fit their representation types.
</prevsent>
</prevsection>
<citsent citstr=" J08-1003 ">
dependency trees are evaluated using attachment scores (buchholz and marsi, 2006), <papid> W06-2920 </papid>phrase-structure trees are evaluated using parseval (black et al  1991), <papid> H91-1060 </papid>lfg-based parsers postulate an evaluation procedure based on structures (cahill et al  2008), <papid> J08-1003 </papid>and so on.</citsent>
<aftsection>
<nextsent>from downstream application point of view, there is no significance as to which formalism was used for generating the representation and which learning methods have been utilized.
</nextsent>
<nextsent>the bottom line is simply which parsing framework most accurately recovers useful representation that helps to unravel the human-perceived interpretation.relational schemes, that is, schemes that encode the set of grammatical relations that constitute the predicate-argument structures of sentences, provide an interface to semantic interpretation.
</nextsent>
<nextsent>they are more intuitively understood than, say, phrase-structure trees, and thus they are also more useful for practical applications.
</nextsent>
<nextsent>for these reasons, relational schemes have been repeatedly singled out as an appropriate level of representation for the evaluation of statistical parsers (lin, 1995; carroll et al  1998; cer et al  2010).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4905">
<title id=" E12-1006.xml">cross framework evaluation for statistical parsing </title>
<section> preliminaries: relational schemes for.  </section>
<citcontext>
<prevsection>
<prevsent>the penn treebank and phrase-structure annotated resources encode partial information about grammatical relations as dash-features decorating phrase structure nodes (marcus et al  1993).<papid> J93-2004 </papid></prevsent>
<prevsent>treebanks like tiger for german (brants et al  2002) and talbankenfor swedish (nivre and megyesi, 2007) explicitly map phrase structures onto grammatical relations using dedicated edge labels.</prevsent>
</prevsection>
<citsent citstr=" C08-1112 ">
the relational realizational structures of tsarfaty and simaan(2008) <papid> C08-1112 </papid>encode relational networks (sets of relations) projected and realized by syntactic categories on top of ordinary phrase-structure nodes.</citsent>
<aftsection>
<nextsent>function trees, as defined in tsarfaty et al  (2011), <papid> D11-1036 </papid>are linearly ordered labeled trees in which every node is labeled with the grammatical func 45 (a) -root- john loves marysbj objroot ? root sbj john hd loves obj mary (b) s-root np-sbj nn-hd john vp-prd v-hd loves np-obj nn-hd mary ? root sbj hd john prd hd loves obj hd mary (c) {sbj,prd,obj} sbj np {hd} hd nn john prd loves obj np {hd} hd nn mary ? root sbj hd john prd loves obj hd mary figure 1: deterministic conversion into function trees.the algorithm for extracting function tree from dependency tree as in (a) is provided in tsarfaty et al (2011).<papid> D11-1036 </papid></nextsent>
<nextsent>for phrase-structure tree as in (b) we can replace each node label with its function (dash-feature).in relational-realizational structure like (c) we can remove the projection nodes (sets) and realization nodes (phrase labels), which leaves the function nodes intact.tion of the dominated span.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4946">
<title id=" E12-1006.xml">cross framework evaluation for statistical parsing </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>the ted software that we utilize builds on the ted efficient algorithm of zhang and shasha (1989) which runs in o(|t1||t2|min(d1, n1)min(d2, n2)) time where di is the tree degree (depth) and ni is the number of terminals in the respective tree (bille, 2005).
</prevsent>
<prevsent>we validate our cross-framework evaluation procedure on two languages, english and swedish.
</prevsent>
</prevsection>
<citsent citstr=" H05-1066 ">
for english, we compare the performance of two dependency parsers, malt parser (nivre et al  2006) and mst parser (mcdonald et al  2005), <papid> H05-1066 </papid>and two constituency-based parsers, the berkeley parser (petrov et al  2006) <papid> P06-1055 </papid>and the brown parser (charniak and johnson, 2005).<papid> P05-1022 </papid></citsent>
<aftsection>
<nextsent>all experiments use penn treebank (ptb) data.
</nextsent>
<nextsent>for swedish, we compare malt parser and mst parser with two variants of the berkeley parser, one trained on phrase structure trees, and one trained on variant of the relational-realizational representation of tsarfaty and simaan (2008).<papid> C08-1112 </papid></nextsent>
<nextsent>all experiments use the tal banken swedish treebank (stb) data.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4947">
<title id=" E12-1006.xml">cross framework evaluation for statistical parsing </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>the ted software that we utilize builds on the ted efficient algorithm of zhang and shasha (1989) which runs in o(|t1||t2|min(d1, n1)min(d2, n2)) time where di is the tree degree (depth) and ni is the number of terminals in the respective tree (bille, 2005).
</prevsent>
<prevsent>we validate our cross-framework evaluation procedure on two languages, english and swedish.
</prevsent>
</prevsection>
<citsent citstr=" P06-1055 ">
for english, we compare the performance of two dependency parsers, malt parser (nivre et al  2006) and mst parser (mcdonald et al  2005), <papid> H05-1066 </papid>and two constituency-based parsers, the berkeley parser (petrov et al  2006) <papid> P06-1055 </papid>and the brown parser (charniak and johnson, 2005).<papid> P05-1022 </papid></citsent>
<aftsection>
<nextsent>all experiments use penn treebank (ptb) data.
</nextsent>
<nextsent>for swedish, we compare malt parser and mst parser with two variants of the berkeley parser, one trained on phrase structure trees, and one trained on variant of the relational-realizational representation of tsarfaty and simaan (2008).<papid> C08-1112 </papid></nextsent>
<nextsent>all experiments use the tal banken swedish treebank (stb) data.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4948">
<title id=" E12-1006.xml">cross framework evaluation for statistical parsing </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>the ted software that we utilize builds on the ted efficient algorithm of zhang and shasha (1989) which runs in o(|t1||t2|min(d1, n1)min(d2, n2)) time where di is the tree degree (depth) and ni is the number of terminals in the respective tree (bille, 2005).
</prevsent>
<prevsent>we validate our cross-framework evaluation procedure on two languages, english and swedish.
</prevsent>
</prevsection>
<citsent citstr=" P05-1022 ">
for english, we compare the performance of two dependency parsers, malt parser (nivre et al  2006) and mst parser (mcdonald et al  2005), <papid> H05-1066 </papid>and two constituency-based parsers, the berkeley parser (petrov et al  2006) <papid> P06-1055 </papid>and the brown parser (charniak and johnson, 2005).<papid> P05-1022 </papid></citsent>
<aftsection>
<nextsent>all experiments use penn treebank (ptb) data.
</nextsent>
<nextsent>for swedish, we compare malt parser and mst parser with two variants of the berkeley parser, one trained on phrase structure trees, and one trained on variant of the relational-realizational representation of tsarfaty and simaan (2008).<papid> C08-1112 </papid></nextsent>
<nextsent>all experiments use the tal banken swedish treebank (stb) data.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4959">
<title id=" E12-1006.xml">cross framework evaluation for statistical parsing </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>the conversion introduces set of annotation specific decisions which may introduce bias into the evaluation.
</prevsent>
<prevsent>in the middle column of table 1 we report the tedeval metrics measured against the generalized gold standard for all parsing frameworks.
</prevsent>
</prevsection>
<citsent citstr=" P11-1067 ">
we can now confirm thatthe constituency-based parsers significantly out perform the dependency parsers, and that this is not due to specific theoretical decisions which are seen to affect las/uas metrics (schwartz et al  2011).<papid> P11-1067 </papid></citsent>
<aftsection>
<nextsent>for the dependency parsers we now seethat malt outperforms mst on labeled dependencies slightly, but the difference is insignificant.the fact that the discrepancy in theoretical assumptions between different frameworks indeed affects the conversion-based evaluation procedure is reflected in the results we report in table 2.
</nextsent>
<nextsent>here the left most and rightmost columns report tedeval scores against the own native gold(single) and the middle column against the generalized gold (multiple).
</nextsent>
<nextsent>had the theories for sd and ptbunionsqtlsd been identical, tedeval single and tedeval multiple would have been equal in each line.
</nextsent>
<nextsent>because of theoretical discrepancies, we see small gaps in parser performance between these cases.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4967">
<title id=" E12-1006.xml">cross framework evaluation for statistical parsing </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>a pre-condition for applying our protocol is the availability of relational interpretation of trees inthe different frameworks.
</prevsent>
<prevsent>for dependency frameworks this is straightforward, as these relations are encoded on top of dependency arcs.
</prevsent>
</prevsection>
<citsent citstr=" H05-1078 ">
for constituency trees with an inherent mapping of nodes onto grammatical relations (merlo and musillo, 2005; <papid> H05-1078 </papid>gabbard et al  2006; <papid> N06-1024 </papid>tsarfaty and simaan, 2008), procedure for reading relational schemes off of the trees is trivial to implement.</citsent>
<aftsection>
<nextsent>for parsers that are trained on and parse into bare-bones phrase-structure trees this is not so.
</nextsent>
<nextsent>reading off the relational structure may be more costly and require interject ion of additional theoretical assumptions via manually written scripts.
</nextsent>
<nextsent>scripts that read off grammatical relations based on tree positions work well for configurational languages such as english (de marneffe et al 2006) but since grammatical relations are reflected differently in different languages (postaland perlmutter, 1977; bresnan, 2000), procedure to read off these relations in language independent fashion from phrase-structure trees does not, and should not, exist (rambow, 2010).<papid> N10-1049 </papid>the crucial point is that even when using external scripts for recovering relational scheme for phrase-structure trees, our protocol has clear advantage over simply scoring converted trees.manually created conversion scripts alter the theoretical assumptions inherent in the trees and thus may bias the results.</nextsent>
<nextsent>our generalization operation and three-way ted make sure that theory-specific idiosyncrasies injected through such scripts do not lead to over-penalizing or over-crediting theory-specific structural variations.certain linguistic structures cannot yet be evaluated with our protocol because of the strict assumption that the labeled spans in parse form tree.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4968">
<title id=" E12-1006.xml">cross framework evaluation for statistical parsing </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>a pre-condition for applying our protocol is the availability of relational interpretation of trees inthe different frameworks.
</prevsent>
<prevsent>for dependency frameworks this is straightforward, as these relations are encoded on top of dependency arcs.
</prevsent>
</prevsection>
<citsent citstr=" N06-1024 ">
for constituency trees with an inherent mapping of nodes onto grammatical relations (merlo and musillo, 2005; <papid> H05-1078 </papid>gabbard et al  2006; <papid> N06-1024 </papid>tsarfaty and simaan, 2008), procedure for reading relational schemes off of the trees is trivial to implement.</citsent>
<aftsection>
<nextsent>for parsers that are trained on and parse into bare-bones phrase-structure trees this is not so.
</nextsent>
<nextsent>reading off the relational structure may be more costly and require interject ion of additional theoretical assumptions via manually written scripts.
</nextsent>
<nextsent>scripts that read off grammatical relations based on tree positions work well for configurational languages such as english (de marneffe et al 2006) but since grammatical relations are reflected differently in different languages (postaland perlmutter, 1977; bresnan, 2000), procedure to read off these relations in language independent fashion from phrase-structure trees does not, and should not, exist (rambow, 2010).<papid> N10-1049 </papid>the crucial point is that even when using external scripts for recovering relational scheme for phrase-structure trees, our protocol has clear advantage over simply scoring converted trees.manually created conversion scripts alter the theoretical assumptions inherent in the trees and thus may bias the results.</nextsent>
<nextsent>our generalization operation and three-way ted make sure that theory-specific idiosyncrasies injected through such scripts do not lead to over-penalizing or over-crediting theory-specific structural variations.certain linguistic structures cannot yet be evaluated with our protocol because of the strict assumption that the labeled spans in parse form tree.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4971">
<title id=" E12-1011.xml">can click patterns across users query logs predict answers to definition questions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>and tell me about bank of america?.it is standard practice of definition question answering (qa) systems to mine kbs (e.g.,online encyclopedias and dictionaries) for reliable descriptive information on the definiendum (sacaleanu et al  2008).
</prevsent>
<prevsent>normally, these pieces of information (i.e., nuggets) explain different facets of the definiendum (e.g., ballet choreographer and born in bordeaux?), and the main idea consists in projecting the acquired nuggets into the set of answer candidates afterwards.
</prevsent>
</prevsection>
<citsent citstr=" I05-1044 ">
however, the performance of this category of method falls into sharp decline whenever few or no coverage is found across kbs (zhang et al  2005; <papid> I05-1044 </papid>han et al ., 2006).</citsent>
<aftsection>
<nextsent>put differently, this technique usually succeeds in discovering the most relevant facts about the most promiment sense of the definien dum.
</nextsent>
<nextsent>but it often misses many pertinent nuggets, especially those that can be paraphrased in several ways; and/or those regarding ancillary senses of the definiendum, which are hardly found in kbs.as means of dealing with this, current strategies try to construct general definition models inferred from collection of definitions coming from the internet or kbs (androutsopoulos and galanis, 2005; <papid> H05-1041 </papid>xu et al  2005; han et al 2006).</nextsent>
<nextsent>to great extent, models exploiting nonkb sources demand considerable annotation efforts, or when the data is obtained automatically, they benefit from empirical thresholds that ensure certain degree of similarity to an array of kb articles.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4972">
<title id=" E12-1011.xml">can click patterns across users query logs predict answers to definition questions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, the performance of this category of method falls into sharp decline whenever few or no coverage is found across kbs (zhang et al  2005; <papid> I05-1044 </papid>han et al ., 2006).</prevsent>
<prevsent>put differently, this technique usually succeeds in discovering the most relevant facts about the most promiment sense of the definien dum.</prevsent>
</prevsection>
<citsent citstr=" H05-1041 ">
but it often misses many pertinent nuggets, especially those that can be paraphrased in several ways; and/or those regarding ancillary senses of the definiendum, which are hardly found in kbs.as means of dealing with this, current strategies try to construct general definition models inferred from collection of definitions coming from the internet or kbs (androutsopoulos and galanis, 2005; <papid> H05-1041 </papid>xu et al  2005; han et al 2006).</citsent>
<aftsection>
<nextsent>to great extent, models exploiting nonkb sources demand considerable annotation efforts, or when the data is obtained automatically, they benefit from empirical thresholds that ensure certain degree of similarity to an array of kb articles.
</nextsent>
<nextsent>these thesholds attempt to trade-off the cleanness of the training material against its coverage.
</nextsent>
<nextsent>moreover, gathering negative samples is also hard as it is not easy to find wide-coverageauthoritative sources of non-descriptive information about particular definiendum.
</nextsent>
<nextsent>our approach has different innovative aspects 99compared to other research in the area of definition extraction.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4976">
<title id=" E12-1011.xml">can click patterns across users query logs predict answers to definition questions </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>but its usage relies on its cleanness and on negative set.
</prevsent>
<prevsent>many people had these intuitions before, but to the best of our knowledge, we provide the first empirical confirmation and quantification.the road-map of this paper is as follows: section 2 touches on related works; section 3 digs deeper into click patterns for definition questions,subsequently section 4 explains our corpus construction strategy; section 5 describes our experiments, and section 6 draws final conclusions.
</prevsent>
</prevsection>
<citsent citstr=" P06-1136 ">
in recent years, definition qa systems have shown trend towards the utilization of several discriminant and statistical learning techniques (androutsopoulos and galanis, 2005; <papid> H05-1041 </papid>chen et al  2006; <papid> P06-1136 </papid>han et al  2006; fahmi and bouma, 2006; <papid> W06-2609 </papid>katz et al  2007; westerhout, 2009; <papid> E09-3011 </papid>navigli and velardi, 2010).<papid> P10-1134 </papid></citsent>
<aftsection>
<nextsent>due to training, there is pressing necessity for large-scale authoritative sources of descriptive and non-descriptive nuggets.
</nextsent>
<nextsent>in the same manner, there is growing importance of strategies capable of extracting trustworthy and negative/positive samples from any type of text.conventionally, these methods interpret descriptions as positive examples, whereas contexts providing non-descriptive information as negative elements.
</nextsent>
<nextsent>four representative techniques are: ? centro id vector (xu et al  2003; cui et al ., 2004) collects an array of articles about the definiendum from battery of predetermined kbs.
</nextsent>
<nextsent>these articles are then used to learn vector of word frequencies,wherewith answer candidates are rated afterwards.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4977">
<title id=" E12-1011.xml">can click patterns across users query logs predict answers to definition questions </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>but its usage relies on its cleanness and on negative set.
</prevsent>
<prevsent>many people had these intuitions before, but to the best of our knowledge, we provide the first empirical confirmation and quantification.the road-map of this paper is as follows: section 2 touches on related works; section 3 digs deeper into click patterns for definition questions,subsequently section 4 explains our corpus construction strategy; section 5 describes our experiments, and section 6 draws final conclusions.
</prevsent>
</prevsection>
<citsent citstr=" W06-2609 ">
in recent years, definition qa systems have shown trend towards the utilization of several discriminant and statistical learning techniques (androutsopoulos and galanis, 2005; <papid> H05-1041 </papid>chen et al  2006; <papid> P06-1136 </papid>han et al  2006; fahmi and bouma, 2006; <papid> W06-2609 </papid>katz et al  2007; westerhout, 2009; <papid> E09-3011 </papid>navigli and velardi, 2010).<papid> P10-1134 </papid></citsent>
<aftsection>
<nextsent>due to training, there is pressing necessity for large-scale authoritative sources of descriptive and non-descriptive nuggets.
</nextsent>
<nextsent>in the same manner, there is growing importance of strategies capable of extracting trustworthy and negative/positive samples from any type of text.conventionally, these methods interpret descriptions as positive examples, whereas contexts providing non-descriptive information as negative elements.
</nextsent>
<nextsent>four representative techniques are: ? centro id vector (xu et al  2003; cui et al ., 2004) collects an array of articles about the definiendum from battery of predetermined kbs.
</nextsent>
<nextsent>these articles are then used to learn vector of word frequencies,wherewith answer candidates are rated afterwards.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4978">
<title id=" E12-1011.xml">can click patterns across users query logs predict answers to definition questions </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>but its usage relies on its cleanness and on negative set.
</prevsent>
<prevsent>many people had these intuitions before, but to the best of our knowledge, we provide the first empirical confirmation and quantification.the road-map of this paper is as follows: section 2 touches on related works; section 3 digs deeper into click patterns for definition questions,subsequently section 4 explains our corpus construction strategy; section 5 describes our experiments, and section 6 draws final conclusions.
</prevsent>
</prevsection>
<citsent citstr=" E09-3011 ">
in recent years, definition qa systems have shown trend towards the utilization of several discriminant and statistical learning techniques (androutsopoulos and galanis, 2005; <papid> H05-1041 </papid>chen et al  2006; <papid> P06-1136 </papid>han et al  2006; fahmi and bouma, 2006; <papid> W06-2609 </papid>katz et al  2007; westerhout, 2009; <papid> E09-3011 </papid>navigli and velardi, 2010).<papid> P10-1134 </papid></citsent>
<aftsection>
<nextsent>due to training, there is pressing necessity for large-scale authoritative sources of descriptive and non-descriptive nuggets.
</nextsent>
<nextsent>in the same manner, there is growing importance of strategies capable of extracting trustworthy and negative/positive samples from any type of text.conventionally, these methods interpret descriptions as positive examples, whereas contexts providing non-descriptive information as negative elements.
</nextsent>
<nextsent>four representative techniques are: ? centro id vector (xu et al  2003; cui et al ., 2004) collects an array of articles about the definiendum from battery of predetermined kbs.
</nextsent>
<nextsent>these articles are then used to learn vector of word frequencies,wherewith answer candidates are rated afterwards.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4979">
<title id=" E12-1011.xml">can click patterns across users query logs predict answers to definition questions </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>but its usage relies on its cleanness and on negative set.
</prevsent>
<prevsent>many people had these intuitions before, but to the best of our knowledge, we provide the first empirical confirmation and quantification.the road-map of this paper is as follows: section 2 touches on related works; section 3 digs deeper into click patterns for definition questions,subsequently section 4 explains our corpus construction strategy; section 5 describes our experiments, and section 6 draws final conclusions.
</prevsent>
</prevsection>
<citsent citstr=" P10-1134 ">
in recent years, definition qa systems have shown trend towards the utilization of several discriminant and statistical learning techniques (androutsopoulos and galanis, 2005; <papid> H05-1041 </papid>chen et al  2006; <papid> P06-1136 </papid>han et al  2006; fahmi and bouma, 2006; <papid> W06-2609 </papid>katz et al  2007; westerhout, 2009; <papid> E09-3011 </papid>navigli and velardi, 2010).<papid> P10-1134 </papid></citsent>
<aftsection>
<nextsent>due to training, there is pressing necessity for large-scale authoritative sources of descriptive and non-descriptive nuggets.
</nextsent>
<nextsent>in the same manner, there is growing importance of strategies capable of extracting trustworthy and negative/positive samples from any type of text.conventionally, these methods interpret descriptions as positive examples, whereas contexts providing non-descriptive information as negative elements.
</nextsent>
<nextsent>four representative techniques are: ? centro id vector (xu et al  2003; cui et al ., 2004) collects an array of articles about the definiendum from battery of predetermined kbs.
</nextsent>
<nextsent>these articles are then used to learn vector of word frequencies,wherewith answer candidates are rated afterwards.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4982">
<title id=" E12-1011.xml">can click patterns across users query logs predict answers to definition questions </title>
<section> click-based corpus acquisition.  </section>
<citcontext>
<prevsection>
<prevsent>here, we also randomly selected 1,000 instance sand manually checked if they were actual descriptions.
</prevsent>
<prevsent>the error of this set was 12.2%.
</prevsent>
</prevsection>
<citsent citstr=" C04-1199 ">
to put things into perspective, in contrast to other corpus acquisition approaches, the present method generated more than 1,800,000 positive and negative training samples combined, while the open-domain strategy of (miliaraki and androutsopoulos, 2004; <papid> C04-1199 </papid>androutsopoulos and galanis, 2005) <papid> H05-1041 </papid>ca.</citsent>
<aftsection>
<nextsent>20,000 examples, the close-domain technique of (xu et al  2005) about 3,000 and (fahmi and bouma, 2006) <papid> W06-2609 </papid>ca.</nextsent>
<nextsent>2,000.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4995">
<title id=" E12-1011.xml">can click patterns across users query logs predict answers to definition questions </title>
<section> conclusions.  </section>
<citcontext>
<prevsection>
<prevsent>in contrast, negative examples are obtained in conformity to redundancy patterns across snippets, which are returned by the search engine when processing several definition queries.
</prevsent>
<prevsent>the effectiveness of these patterns, and hence of the obtained corpus, was tested by means of two models different in nature, where both were capable of achieving an accuracy higher than 70%.as future work, we envision that answers detected by our strategy can aid in determining some query expansion terms, and thus to devise some relevance feedback methods that can bring about an improvement in terms of the recall of answers.along the same lines, it can cooperate on the visualization of the results by highlighting and/orextending truncated answers, that is more informative snippets, which is one of the holy grail of search operators, especially when processing informational queries.nlp tools (e.g., parsers and name entity recog nizers) can also be exploited for designing better training data filters and more discriminative features for our models that can assist in enhancing the performance, cf.
</prevsent>
</prevsection>
<citsent citstr=" P08-1082 ">
(surdeanu et al  2008; <papid> P08-1082 </papid>figueroa, 2010; surdeanu et al  2011).</citsent>
<aftsection>
<nextsent>however, this implies that these tools have to be re-trained to cope with web-snippets.
</nextsent>
<nextsent>acknowledgements this work was partially supported by r&d; project fondef d09i1185.
</nextsent>
<nextsent>we also thank our reviewers for their interesting comments, which helped us to make this work better.
</nextsent>

</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4996">
<title id=" E09-1059.xml">sentiment summarization evaluating and learning user preferences </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the growth of the internet as commerce medium, and particularly the web 2.0 phenomenon of user-generated content, have resulted in the proliferation of massive numbers of product, service and merchant reviews.
</prevsent>
<prevsent>while this means that users have plenty of information on which to base their purchasing decisions, in practice this is often too much information for user to absorb.
</prevsent>
</prevsection>
<citsent citstr=" H05-1043 ">
to alleviate this information overload, research on systems that automatically aggregate and summarize opinions have been gaining interest (hu and liu, 2004a; hu and liu, 2004b; gamon et al, 2005; popescu and etzioni, 2005; <papid> H05-1043 </papid>carenini et al, 2005; carenini et al, 2006; <papid> E06-1039 </papid>zhuang et al, 2006; blair-goldensohn et al, 2008).</citsent>
<aftsection>
<nextsent>evaluating these systems has been challenge, however, due to the number of human judgments required to draw meaningful conclusions.
</nextsent>
<nextsent>of ten systems are evaluated piecemeal, selecting pieces that can be evaluated easily and automatically (blair-goldensohn et al, 2008).
</nextsent>
<nextsent>while this technique produces meaningful evaluations of the selected components, other components remain untested, and the overall effectiveness of the entire system as whole remains unknown.
</nextsent>
<nextsent>when systems are evaluated end-to-end by human judges, the studies are often small, consisting of only handful of judges and data points (carenini etal., 2006).<papid> E06-1039 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4997">
<title id=" E09-1059.xml">sentiment summarization evaluating and learning user preferences </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the growth of the internet as commerce medium, and particularly the web 2.0 phenomenon of user-generated content, have resulted in the proliferation of massive numbers of product, service and merchant reviews.
</prevsent>
<prevsent>while this means that users have plenty of information on which to base their purchasing decisions, in practice this is often too much information for user to absorb.
</prevsent>
</prevsection>
<citsent citstr=" E06-1039 ">
to alleviate this information overload, research on systems that automatically aggregate and summarize opinions have been gaining interest (hu and liu, 2004a; hu and liu, 2004b; gamon et al, 2005; popescu and etzioni, 2005; <papid> H05-1043 </papid>carenini et al, 2005; carenini et al, 2006; <papid> E06-1039 </papid>zhuang et al, 2006; blair-goldensohn et al, 2008).</citsent>
<aftsection>
<nextsent>evaluating these systems has been challenge, however, due to the number of human judgments required to draw meaningful conclusions.
</nextsent>
<nextsent>of ten systems are evaluated piecemeal, selecting pieces that can be evaluated easily and automatically (blair-goldensohn et al, 2008).
</nextsent>
<nextsent>while this technique produces meaningful evaluations of the selected components, other components remain untested, and the overall effectiveness of the entire system as whole remains unknown.
</nextsent>
<nextsent>when systems are evaluated end-to-end by human judges, the studies are often small, consisting of only handful of judges and data points (carenini etal., 2006).<papid> E06-1039 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I4999">
<title id=" E09-1059.xml">sentiment summarization evaluating and learning user preferences </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>while this technique produces meaningful evaluations of the selected components, other components remain untested, and the overall effectiveness of the entire system as whole remains unknown.
</prevsent>
<prevsent>when systems are evaluated end-to-end by human judges, the studies are often small, consisting of only handful of judges and data points (carenini etal., 2006).<papid> E06-1039 </papid></prevsent>
</prevsection>
<citsent citstr=" N03-1020 ">
furthermore, automated summarization metrics like rouge (lin and hovy, 2003)<papid> N03-1020 </papid>are non-trivial to adapt to this domain as theyre quire human curated outputs.we present the results of large-scale, end-toend human evaluation of three sentiment summarization models applied to user reviews of consumer products.</citsent>
<aftsection>
<nextsent>the evaluation shows that thereis no significant difference in rater preference between any of the sentiment summarizers, but that raters do prefer sentiment summarizers over non sentiment baselines.
</nextsent>
<nextsent>this indicates that even simple sentiment summarizers provide users utility.
</nextsent>
<nextsent>an analysis of the rater judgments also indicates that there are identifiable situations where one sentiment summarizer is generally preferred over the others.
</nextsent>
<nextsent>we attempt to learn these preferences by training ranking svm that exploits the set of preference judgments collected during the evaluation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5001">
<title id=" E09-1059.xml">sentiment summarization evaluating and learning user preferences </title>
<section> sentiment summarization.  </section>
<citcontext>
<prevsection>
<prevsent>an example summary is given in figure 1.
</prevsent>
<prevsent>for simplicity we assume that all opinions in are about the entity being summarized.
</prevsent>
</prevsection>
<citsent citstr=" C08-1103 ">
when this assumption fails, one can parse opinions at finer-level 2http://www.nist.gov/tac/ (jindal and liu, 2006; stoyanov and cardie, 2008)<papid> C08-1103 </papid>in this study, we look at an extractive summarization setting where is built by extracting representative bits of text from the set d, subject topre-specified length constraints.</citsent>
<aftsection>
<nextsent>specifically, assume each document di is segmented into candidate text excerpts.
</nextsent>
<nextsent>for ease of discussion wewill assume all excerpts are sentences, but in practice they can be phrases or multi-sentence groups.
</nextsent>
<nextsent>viewed this way, is set of candidate sentences for our summary, = {s1, . . .
</nextsent>
<nextsent>, sn}, and summarization becomes the following optimization: argmax sd l(s) s.t.: length(s) ? (1) where is some score over possible summaries, length(s) is the length of the summary and kis the pre-specified length constraint.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5003">
<title id=" E09-1059.xml">sentiment summarization evaluating and learning user preferences </title>
<section> sentiment summarization.  </section>
<citcontext>
<prevsection>
<prevsent>first we randomly initialize the summary to length k. then we greedily insert/delete/swap sentences in and out of the summary to maximize l(s) while maintaining the bound on length.
</prevsent>
<prevsent>we run this procedure until no operation leads to higher scoring summary.
</prevsent>
</prevsection>
<citsent citstr=" H05-1045 ">
in all our experiments convergence was quick, even when employing random restarts.alternate formulations of sentiment summarization are possible, including aspect-based summarization (hu and liu, 2004a), abs tractive summarization (carenini et al, 2006) <papid> E06-1039 </papid>or related tasks such as opinion attribution (choi et al, 2005).<papid> H05-1045 </papid></citsent>
<aftsection>
<nextsent>we choose purely extractive formulation as it makes it easier to develop baselines and allows raters to compare summaries with simple, consistent presentation format.
</nextsent>
<nextsent>2.1 definitions.
</nextsent>
<nextsent>before delving into the details of the summarization models we must first define some useful functions.
</nextsent>
<nextsent>the first is the sentiment polarity function that maps lexical item t, e.g., word or short phrase, to real-valued score, lex-sent(t) ? [1, 1] 515 the lex-sent function maps items with positive polarity to higher values and items with negative polarity to lower values.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5004">
<title id=" E09-1059.xml">sentiment summarization evaluating and learning user preferences </title>
<section> sentiment summarization.  </section>
<citcontext>
<prevsection>
<prevsent>the first is the sentiment polarity function that maps lexical item t, e.g., word or short phrase, to real-valued score, lex-sent(t) ? [1, 1] 515 the lex-sent function maps items with positive polarity to higher values and items with negative polarity to lower values.
</prevsent>
<prevsent>to build this function we constructed large sentiment lexicons by seeding semantic word graph induced from wordnet with positive and negative examples and then propagating this score out across the graph with decayingconfidence.
</prevsent>
</prevsection>
<citsent citstr=" C04-1200 ">
this method is common among sentiment analysis systems (hu and liu, 2004a; kim and hovy, 2004; <papid> C04-1200 </papid>blair-goldensohn et al, 2008).</citsent>
<aftsection>
<nextsent>in particular, we use the lexicons that were created and evaluated by blair-goldensohn et al (2008).
</nextsent>
<nextsent>next we define sentiment intensity, intensity(s) = ? ts |lex-sent(t)|which simply measures the magnitude of sentiment in sentence.
</nextsent>
<nextsent>intensity can be viewed as measure of subjective ness irrespective of polarity.a central function in all our systems is sentences normalized sentiment, sent(s) = ? ts lex-sent(t) ?+ intensity(s) this function measures the (signed) ratio of lexical sentiment to intensity in sentence.
</nextsent>
<nextsent>sentences that only contain lexical items of the same polarity will have high absolute normalized sentiment, whereas sentences with mixed polarity items or no polarity items will have normalized sentiment nearzero.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5005">
<title id=" E09-1059.xml">sentiment summarization evaluating and learning user preferences </title>
<section> sentiment summarization.  </section>
<citcontext>
<prevsection>
<prevsent>we include the constant ? in the denominator so that sent gives higher absolute scores to sentences containing many strong sentiment items of the same polarity over sentences with small number of weak items of the same polarity.most sentiment summarizers assume that as input, system is given an overall rating of the entity it is attempting to summarize, ? [1, 1], where higher rating indicates more favorable opinion.
</prevsent>
<prevsent>this rating may be obtained directly from user provided information (e.g., star ratings) or automatically derived by averaging the sent function over all sentences in d. using r, we can define mismatch function between the sentiment of summary and the known sentiment of the entity, mismatch(s) = (r? 1 |s| ? sis sent(si)) 2 summaries with higher mismatch are those whose sentiment disagrees most with r. another key input many sentiment summarizers assume is list of salient entity aspects, which are specific properties of an entity that people tend to rate when expressing their opinion.
</prevsent>
</prevsection>
<citsent citstr=" P08-1031 ">
for example, aspects of digital camera could include picture quality, battery life, size, color, value, etc. finding such aspects is challenging research problem that has been addressed in number of ways (hu and liu, 2004b; gamon et al, 2005; carenini et al., 2005; zhuang et al, 2006; branavan et al, 2008; <papid> P08-1031 </papid>blair-goldensohn et al, 2008; titov and mcdonald, 2008<papid> P08-1036 </papid>b; titov and mcdonald, 2008<papid> P08-1036 </papid>a).</citsent>
<aftsection>
<nextsent>we denote the set of aspects for an entity as and each aspect as ? a. furthermore, we assume that given it is possible to determine whether some sentence ? mentions an aspect in a.for our experiments we use hybrid supervised unsupervised method for finding aspects as described and evaluated in blair-goldensohn et al (2008).having defined what an aspect is, we next define summary diversity function over aspects, diversity(s) = ? aa coverage(a) where coverage(a) ? is function that weights how well the aspect is covered in the summary and is proportional to the importance of the aspect as some aspects are more important to cover than others, e.g., picture quality?
</nextsent>
<nextsent>versusstrap?
</nextsent>
<nextsent>for digital cameras.
</nextsent>
<nextsent>the diversity function rewards summaries that cover many important aspects and plays the redundancy reducing role that is common in most extractive summarization frameworks (goldstein et al, 2000).<papid> W00-0405 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5006">
<title id=" E09-1059.xml">sentiment summarization evaluating and learning user preferences </title>
<section> sentiment summarization.  </section>
<citcontext>
<prevsection>
<prevsent>we include the constant ? in the denominator so that sent gives higher absolute scores to sentences containing many strong sentiment items of the same polarity over sentences with small number of weak items of the same polarity.most sentiment summarizers assume that as input, system is given an overall rating of the entity it is attempting to summarize, ? [1, 1], where higher rating indicates more favorable opinion.
</prevsent>
<prevsent>this rating may be obtained directly from user provided information (e.g., star ratings) or automatically derived by averaging the sent function over all sentences in d. using r, we can define mismatch function between the sentiment of summary and the known sentiment of the entity, mismatch(s) = (r? 1 |s| ? sis sent(si)) 2 summaries with higher mismatch are those whose sentiment disagrees most with r. another key input many sentiment summarizers assume is list of salient entity aspects, which are specific properties of an entity that people tend to rate when expressing their opinion.
</prevsent>
</prevsection>
<citsent citstr=" P08-1036 ">
for example, aspects of digital camera could include picture quality, battery life, size, color, value, etc. finding such aspects is challenging research problem that has been addressed in number of ways (hu and liu, 2004b; gamon et al, 2005; carenini et al., 2005; zhuang et al, 2006; branavan et al, 2008; <papid> P08-1031 </papid>blair-goldensohn et al, 2008; titov and mcdonald, 2008<papid> P08-1036 </papid>b; titov and mcdonald, 2008<papid> P08-1036 </papid>a).</citsent>
<aftsection>
<nextsent>we denote the set of aspects for an entity as and each aspect as ? a. furthermore, we assume that given it is possible to determine whether some sentence ? mentions an aspect in a.for our experiments we use hybrid supervised unsupervised method for finding aspects as described and evaluated in blair-goldensohn et al (2008).having defined what an aspect is, we next define summary diversity function over aspects, diversity(s) = ? aa coverage(a) where coverage(a) ? is function that weights how well the aspect is covered in the summary and is proportional to the importance of the aspect as some aspects are more important to cover than others, e.g., picture quality?
</nextsent>
<nextsent>versusstrap?
</nextsent>
<nextsent>for digital cameras.
</nextsent>
<nextsent>the diversity function rewards summaries that cover many important aspects and plays the redundancy reducing role that is common in most extractive summarization frameworks (goldstein et al, 2000).<papid> W00-0405 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5010">
<title id=" E09-1059.xml">sentiment summarization evaluating and learning user preferences </title>
<section> sentiment summarization.  </section>
<citcontext>
<prevsection>
<prevsent>versusstrap?
</prevsent>
<prevsent>for digital cameras.
</prevsent>
</prevsection>
<citsent citstr=" W00-0405 ">
the diversity function rewards summaries that cover many important aspects and plays the redundancy reducing role that is common in most extractive summarization frameworks (goldstein et al, 2000).<papid> W00-0405 </papid></citsent>
<aftsection>
<nextsent>2.2 systems.
</nextsent>
<nextsent>for our evaluation we developed three extractive sentiment summarization systems.
</nextsent>
<nextsent>each system models increasingly complex objectives.
</nextsent>
<nextsent>2.2.1 sentiment match (sm)the first system that we look at attempts to extract sentences so that the average sentiment of the summary is as close as possible to the entity level sentiment r, which was previously defined in section 2.1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5013">
<title id=" E09-1059.xml">sentiment summarization evaluating and learning user preferences </title>
<section> sentiment summarization.  </section>
<citcontext>
<prevsection>
<prevsent>mismatch(s) +?
</prevsent>
<prevsent>diversity(s)this score function rewards summaries for being highly subjective (intensity), reflecting the overall product rating (mismatch), and cover inga variety of product aspects (diversity).
</prevsent>
</prevsection>
<citsent citstr=" C04-1057 ">
the coefficients were set by inspection.this system has its roots in event-based summarization (filatova and hatzivassiloglou, 2004) <papid> C04-1057 </papid>for the news domain.</citsent>
<aftsection>
<nextsent>in that work an optimization problem was developed that attempted to maximize summary informative ness while covering as many (weighted) sub-events as possible.
</nextsent>
<nextsent>2.2.3 sentiment-aspect match (sam) because the smac model only utilizes an entitys overall sentiment when calculating mismatch, it is susceptible to degenerate solutions.
</nextsent>
<nextsent>consider product with aspects and b, where reviewers overwhelmingly like and dislike b, resulting in an overall sent close to zero.
</nextsent>
<nextsent>if the smac model finds very negative sentence describing anda very positive sentence describing b, it will as sign that summary high score, as the summary has high intensity, has little overall mismatch, and covers both aspects.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5017">
<title id=" E12-1072.xml">elliphant improved automatic detection of zero subjects and impersonal constructions in spanish </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>subject ellipsis is the omission of the subject ina sentence.
</prevsent>
<prevsent>we consider not only missing referential subject (zero subject) as manifestation of ellipsis, but also non-referential impersonal constructions.
</prevsent>
</prevsection>
<citsent citstr=" C02-1139 ">
various natural language processing (nlp)tasks benefit from the identification of elliptical subjects, primarily anaphora resolution (mitkov, 2002) and co-reference resolution (ngand cardie, 2002).<papid> C02-1139 </papid></citsent>
<aftsection>
<nextsent>the difficulty in detecting missing subjects and non-referential pronouns has been acknowledged since the first studies on ? this work was partially funded by la caixa?
</nextsent>
<nextsent>grant for master students.
</nextsent>
<nextsent>the computational treatment of anaphora (hobbs,1977; hirst, 1981).
</nextsent>
<nextsent>however, this task is of crucial importance when processing pro-drop languages since subject ellipsis is pervasive phenomenon in these languages (chomsky, 1981).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5018">
<title id=" E12-1072.xml">elliphant improved automatic detection of zero subjects and impersonal constructions in spanish </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, this task is of crucial importance when processing pro-drop languages since subject ellipsis is pervasive phenomenon in these languages (chomsky, 1981).
</prevsent>
<prevsent>for instance, in our spanish corpus, 29% of the subjects are elided.our method is based on classification of all expressions in subject position, including the recognition of spanish non-referential impersonal constructions which, to the best of our knowledge,has not yet been addressed.
</prevsent>
</prevsection>
<citsent citstr=" P00-1022 ">
the necessity of identifying such kind of elliptical constructions hasbeen specifically highlighted in work about spanish zero pronouns (ferrandez and peral, 2000) <papid> P00-1022 </papid>and co-reference resolution (recasens and hovy, 2009).</citsent>
<aftsection>
<nextsent>the main contributions of this study are: ? public annotated corpus in spanish to compare different strategies for detecting explicit subjects, zero subjects and impersonal constructions.
</nextsent>
<nextsent>the first ml based approach to this problem in spanish and thorough analysis regarding features, learn ability, genre and errors.?
</nextsent>
<nextsent>the best performing algorithms to automatically detect explicit subjects and impersonal constructions in spanish.the remainder of the paper is organized as follows.
</nextsent>
<nextsent>section 2 describes the classes of spanish subjects, while section 3 provides literature review.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5023">
<title id=" E12-1072.xml">elliphant improved automatic detection of zero subjects and impersonal constructions in spanish </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in the examples, explicit subjects are presented in italics.zero subjects are presented by the symbol ? and in the english translations the subjects which are elided in spanish are marked with parentheses.
</prevsent>
<prevsent>impersonal constructions are not explicitly indicated.
</prevsent>
</prevsection>
<citsent citstr=" W05-0406 ">
identification of non-referential pronouns, although crucial step in co-reference and anaphora resolution systems (mitkov, 2010),2 has been applied only to the pleonastic it in english (evans, 2001; boyd et al  2005; <papid> W05-0406 </papid>bergsma et al  2008) <papid> P08-1002 </papid>and expletive pronouns in french (danlos, 2005).<papid> I05-2013 </papid></citsent>
<aftsection>
<nextsent>machine learning methods are known to perform better than rule-based techniques for identifying non-referential expressions (boyd et al  2005).<papid> W05-0406 </papid>however, there is some debate as to which approach may be optimal in anaphora resolution systems (mitkov and hallett, 2007).both english and french texts use an explicit word, with some grammatical information (a third person pronoun), which is non-referential(mitkov, 2010).</nextsent>
<nextsent>by contrast, in spanish, non referential expressions are not realized by expletive or pleonastic pronouns but rather by certain kind of ellipsis.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5024">
<title id=" E12-1072.xml">elliphant improved automatic detection of zero subjects and impersonal constructions in spanish </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in the examples, explicit subjects are presented in italics.zero subjects are presented by the symbol ? and in the english translations the subjects which are elided in spanish are marked with parentheses.
</prevsent>
<prevsent>impersonal constructions are not explicitly indicated.
</prevsent>
</prevsection>
<citsent citstr=" P08-1002 ">
identification of non-referential pronouns, although crucial step in co-reference and anaphora resolution systems (mitkov, 2010),2 has been applied only to the pleonastic it in english (evans, 2001; boyd et al  2005; <papid> W05-0406 </papid>bergsma et al  2008) <papid> P08-1002 </papid>and expletive pronouns in french (danlos, 2005).<papid> I05-2013 </papid></citsent>
<aftsection>
<nextsent>machine learning methods are known to perform better than rule-based techniques for identifying non-referential expressions (boyd et al  2005).<papid> W05-0406 </papid>however, there is some debate as to which approach may be optimal in anaphora resolution systems (mitkov and hallett, 2007).both english and french texts use an explicit word, with some grammatical information (a third person pronoun), which is non-referential(mitkov, 2010).</nextsent>
<nextsent>by contrast, in spanish, non referential expressions are not realized by expletive or pleonastic pronouns but rather by certain kind of ellipsis.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5026">
<title id=" E12-1072.xml">elliphant improved automatic detection of zero subjects and impersonal constructions in spanish </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in the examples, explicit subjects are presented in italics.zero subjects are presented by the symbol ? and in the english translations the subjects which are elided in spanish are marked with parentheses.
</prevsent>
<prevsent>impersonal constructions are not explicitly indicated.
</prevsent>
</prevsection>
<citsent citstr=" I05-2013 ">
identification of non-referential pronouns, although crucial step in co-reference and anaphora resolution systems (mitkov, 2010),2 has been applied only to the pleonastic it in english (evans, 2001; boyd et al  2005; <papid> W05-0406 </papid>bergsma et al  2008) <papid> P08-1002 </papid>and expletive pronouns in french (danlos, 2005).<papid> I05-2013 </papid></citsent>
<aftsection>
<nextsent>machine learning methods are known to perform better than rule-based techniques for identifying non-referential expressions (boyd et al  2005).<papid> W05-0406 </papid>however, there is some debate as to which approach may be optimal in anaphora resolution systems (mitkov and hallett, 2007).both english and french texts use an explicit word, with some grammatical information (a third person pronoun), which is non-referential(mitkov, 2010).</nextsent>
<nextsent>by contrast, in spanish, non referential expressions are not realized by expletive or pleonastic pronouns but rather by certain kind of ellipsis.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5040">
<title id=" E12-1072.xml">elliphant improved automatic detection of zero subjects and impersonal constructions in spanish </title>
<section> corpus.  </section>
<citcontext>
<prevsection>
<prevsent>on the other hand, the spanish ancora corpus based on journalistic texts includes zero pronouns and impersonal constructions (recasens and mart??, 2010) while the corpus (rello and illisei, 2009b) comprises legal,instructional and encyclopedic texts but has no annotated impersonal constructions.
</prevsent>
<prevsent>the eszic corpus contains total of 6,827 verbs including 1,793 zero subjects.
</prevsent>
</prevsection>
<citsent citstr=" A97-1011 ">
except for ancora-es, with 10,791 elliptic pronouns, our corpus is larger than the ones used in previous ap proaches: about 1,830 verbs including zero and explicit subjects in (ferrandez and peral, 2000)(<papid> P00-1022 </papid>the exact number is not mentioned in the pa per) and 1,202 zero subjects in (rello and illisei, 2009b).the corpus was parsed by conn exors machin ese syntax (connexor oy, 2006), which returns lexical and morphological information as well as the dependency relations between words by employing functional dependency grammar (tapanainen and jarvinen, 1997).<papid> A97-1011 </papid>to annotate our corpus we created an annotation tool that extracts the finite clauses and the annotators assign to each example one of the defined annotation tags.</citsent>
<aftsection>
<nextsent>two volunteer graduate students of linguistics annotated the verbs after one training session.
</nextsent>
<nextsent>the annotations of third volunteer with the same profile were used to compute the inter-annotator agreement.
</nextsent>
<nextsent>during the annotation phase, we evaluated the adequacy and clarity of the annotation guidelines and established typology of the rising borderline cases, which is included in the annotation guidelines.
</nextsent>
<nextsent>table 1 shows the linguistic and formal criteria used to identify the chosen categories that served as the basis for the corpus annotation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5041">
<title id=" E12-1072.xml">elliphant improved automatic detection of zero subjects and impersonal constructions in spanish </title>
<section> corpus.  </section>
<citcontext>
<prevsection>
<prevsent>fleiss?
</prevsent>
<prevsent>kappa legal health all two annotators 0.934 0.870 0.902 three annotators 0.925 0.857 0.891 table 3: inter-annotator agreement.in table 3 we present the fleiss kappa inter annotator agreement for two and three annotators.
</prevsent>
</prevsection>
<citsent citstr=" J08-4004 ">
these results suggest that the annotation is reliable since it is common practice among researchers in computational linguistics to consider 0.8 as minimum value of acceptance (artstein and poesio, 2008).<papid> J08-4004 </papid></citsent>
<aftsection>
<nextsent>we opted for an ml approach given that our previous rule-based methodology improved only0.02 over the 0.55 f-measure of simple base line (rello and illisei, 2009b).
</nextsent>
<nextsent>besides, ml based methods for the identification of explicit non referential constructions in english appear to perform better than than rule-based ones (boyd et al  2005).<papid> W05-0406 </papid></nextsent>
<nextsent>708 linguistic information phonetic realization syntactic category verbal dia thesis semantic interpr.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5044">
<title id=" E12-1072.xml">elliphant improved automatic detection of zero subjects and impersonal constructions in spanish </title>
<section> machine learning approach.  </section>
<citcontext>
<prevsection>
<prevsent>5.1 features.
</prevsent>
<prevsent>we built the training data from the annotated corpus and defined fourteen features.
</prevsent>
</prevsection>
<citsent citstr=" D07-1057 ">
the linguistically motivated features are inspired by previous ml approaches in chinese (zhao and ng, 2007)<papid> D07-1057 </papid>and english (evans, 2001).</citsent>
<aftsection>
<nextsent>the values for the features (see table 4) were derived from information provided both by conn exors machin ese syntax parser and set of lists.
</nextsent>
<nextsent>we can describe each of the features as broadly belonging to one of ten classes, as follows: 1 parser: the presence or absence of sub-.
</nextsent>
<nextsent>ject in the clause, as identified by the parser.
</nextsent>
<nextsent>we are not aware of formal evaluation ofconnexors accuracy.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5053">
<title id=" E06-2005.xml">xmg  an expressive formalism for describing tree based grammars </title>
<section> linguistic formalism.  </section>
<citcontext>
<prevsection>
<prevsent>extension to semantics the xmg formalism further supports the integration in the grammar of semantic information.
</prevsent>
<prevsent>more generally, the language manages dimensions of descriptions so thatthe content of class can consists of several elements belonging to different dimensions.
</prevsent>
</prevsection>
<citsent citstr=" E03-1030 ">
each dimension is then processed differently according to the output that is expected (trees, set of predicates, etc).currently, xmg includes semantic representation language based on flat semantics (see (gar dent and kallmeyer, 2003)): <papid> E03-1030 </papid>description ::= `:p(e1, ..., en) | ?`:p(e1, ..., en) | ei  ej (2) where `:p(e1, ..., en) represents the predicate with parameters e1, .., en, and labeled `.</citsent>
<aftsection>
<nextsent>is the logical negation, and ei  ej is the scope between ei and ej (used to deal with quantifiers).thus, one can write classes whose content consists of tree description and/or of semantic formulas.
</nextsent>
<nextsent>the xmg formalism furthermore supports the sharing of identifiers across dimension hence allowing for straightforward encoding of the syn tax/semantics interface (see figure 1).
</nextsent>
<nextsent>grammar we now focus on the compilation process and on the constraint logic programming techniques we 104 figure 1: tree with syntax/semantics interface draw upon.as we have seen, an xmg meta grammar consists of classes that are combined.
</nextsent>
<nextsent>provided these classes can be referred to by means of names, we can view class as clause associating name with content or goal to borrow vocabulary from logic programming.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5055">
<title id=" E89-1009.xml">inference in datr </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the present paper is primarily concerned with (iii), though the examples used may hint at our stra-tegy in respect of (i) and (ii).
</prevsent>
<prevsent>inheritance networks ( semantic nets ) provide an intuitively appealing way of thinking about the representation of various kinds of knowledge.
</prevsent>
</prevsection>
<citsent citstr=" P85-1032 ">
this fact has not gone unnoticed by number of researchers working on lexical knowledge representation, e.g. de smedt (1984), flickinger et al (1985), <papid> P85-1032 </papid>calder &amp; te linden (1987), daelemans (1987<papid> E87-1012 </papid>a),  daelemans (1987<papid> E87-1012 </papid>b), gazdar (1987) and calder (1989).<papid> E89-1008 </papid></citsent>
<aftsection>
<nextsent>however, many such networks have been realized in the context of programming systems or programming languages that leave their precise meaning unclear.
</nextsent>
<nextsent>in the light of brachman (1985), ether-ington (1988) and much other ecent work, it has become apparent that the formal properties of notations intended to represent inheritance are highly problematic.
</nextsent>
<nextsent>although not discussed here, datr has formal semantics (evans &amp; gazdar 1989) for which some completeness and soundness results have been derived.
</nextsent>
<nextsent>these results, and others (on complexity, for example) will be provided in subsequent paper.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5056">
<title id=" E89-1009.xml">inference in datr </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the present paper is primarily concerned with (iii), though the examples used may hint at our stra-tegy in respect of (i) and (ii).
</prevsent>
<prevsent>inheritance networks ( semantic nets ) provide an intuitively appealing way of thinking about the representation of various kinds of knowledge.
</prevsent>
</prevsection>
<citsent citstr=" E87-1012 ">
this fact has not gone unnoticed by number of researchers working on lexical knowledge representation, e.g. de smedt (1984), flickinger et al (1985), <papid> P85-1032 </papid>calder &amp; te linden (1987), daelemans (1987<papid> E87-1012 </papid>a),  daelemans (1987<papid> E87-1012 </papid>b), gazdar (1987) and calder (1989).<papid> E89-1008 </papid></citsent>
<aftsection>
<nextsent>however, many such networks have been realized in the context of programming systems or programming languages that leave their precise meaning unclear.
</nextsent>
<nextsent>in the light of brachman (1985), ether-ington (1988) and much other ecent work, it has become apparent that the formal properties of notations intended to represent inheritance are highly problematic.
</nextsent>
<nextsent>although not discussed here, datr has formal semantics (evans &amp; gazdar 1989) for which some completeness and soundness results have been derived.
</nextsent>
<nextsent>these results, and others (on complexity, for example) will be provided in subsequent paper.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5060">
<title id=" E89-1009.xml">inference in datr </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the present paper is primarily concerned with (iii), though the examples used may hint at our stra-tegy in respect of (i) and (ii).
</prevsent>
<prevsent>inheritance networks ( semantic nets ) provide an intuitively appealing way of thinking about the representation of various kinds of knowledge.
</prevsent>
</prevsection>
<citsent citstr=" E89-1008 ">
this fact has not gone unnoticed by number of researchers working on lexical knowledge representation, e.g. de smedt (1984), flickinger et al (1985), <papid> P85-1032 </papid>calder &amp; te linden (1987), daelemans (1987<papid> E87-1012 </papid>a),  daelemans (1987<papid> E87-1012 </papid>b), gazdar (1987) and calder (1989).<papid> E89-1008 </papid></citsent>
<aftsection>
<nextsent>however, many such networks have been realized in the context of programming systems or programming languages that leave their precise meaning unclear.
</nextsent>
<nextsent>in the light of brachman (1985), ether-ington (1988) and much other ecent work, it has become apparent that the formal properties of notations intended to represent inheritance are highly problematic.
</nextsent>
<nextsent>although not discussed here, datr has formal semantics (evans &amp; gazdar 1989) for which some completeness and soundness results have been derived.
</nextsent>
<nextsent>these results, and others (on complexity, for example) will be provided in subsequent paper.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5061">
<title id=" E09-1089.xml">text summarization model based on maximum coverage problem and its variant </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>they also incorporated sentence compression based on syntactic or heuristic rules.mcdonald (2007) formulated text summarization as knapsack problem and obtained the global solution and its approximate solutions.
</prevsent>
<prevsent>its relation to our method will be discussed in section 6.1.
</prevsent>
</prevsection>
<citsent citstr=" C04-1057 ">
filatova and hatzivassiloglou (2004) <papid> C04-1057 </papid>first formulated text summarization as mckp.</citsent>
<aftsection>
<nextsent>their decoding method is greedy one and will be empirically compared with other decoding method sin this paper.
</nextsent>
<nextsent>yih et al  (2007) used slightly modified stack decoding.
</nextsent>
<nextsent>the optimization problem they solved was the mckp with the last sentence truncation.
</nextsent>
<nextsent>their stack decoding is one of the decoding methods discussed in this paper.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5068">
<title id=" E09-1089.xml">text summarization model based on maximum coverage problem and its variant </title>
<section> modeling text summarization.  </section>
<citcontext>
<prevsection>
<prevsent>the performance of the method depends on how to represent words and which words to use.
</prevsent>
<prevsent>we represent words with their stems.
</prevsent>
</prevsection>
<citsent citstr=" W04-1013 ">
we use only the words that are content words (nouns, verbs, or adjectives) and not in the stop word list used in rouge (lin, 2004).<papid> W04-1013 </papid>the weights wj of words are also an important factor of good performance.</citsent>
<aftsection>
<nextsent>we tested two weighting schemes proposed by yih et al  (2007).the first one is interpolated weights, which are interpolated values of the generative word probability in the entire document and that in the beginning part of the document (namely, the first 100 words).
</nextsent>
<nextsent>each probability is estimated with the maximum likelihood principle.
</nextsent>
<nextsent>the second one is trainedweights.
</nextsent>
<nextsent>these values are estimated by the logistic regression trained on data instances, which are labeled 1 if the word appears in summary in the training dataset, 0 otherwise.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5076">
<title id=" E09-1089.xml">text summarization model based on maximum coverage problem and its variant </title>
<section> experiments and discussion.  </section>
<citcontext>
<prevsection>
<prevsent>all the documents were segmented into sentences using script distributed by duc.
</prevsent>
<prevsent>words are stemmed by porters stemmer (porter, 1980).
</prevsent>
</prevsection>
<citsent citstr=" N03-1020 ">
rouge version 1.5.5 (lin, 2004) <papid> W04-1013 </papid>was used for evaluation.2 among others, we focuson rouge-1 in the discussion of the result, because rouge-1 has proved to have strong correlation with human annotation (lin, 2004; <papid> W04-1013 </papid>lin and hovy, 2003).<papid> N03-1020 </papid></citsent>
<aftsection>
<nextsent>wilcox on signed rank test for paired samples with significance level 0.05 was used forthe significance test of the difference in rouge 1.
</nextsent>
<nextsent>the simplex method and the branch-and-bound.
</nextsent>
<nextsent>method implemented in glpk (makhorin, 2006) were used to solve respectively linear and integer programming problems.
</nextsent>
<nextsent>the methods that are compared here are the greedy algorithm (greedy), the greedy algorithm with performance guarantee (g-greedy), the randomized algorithm (rand), the stack decoding (stack), and the branch-and-bound method (exact).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5077">
<title id=" E09-1089.xml">text summarization model based on maximum coverage problem and its variant </title>
<section> conclusion.  </section>
<citcontext>
<prevsection>
<prevsent>as discussed in section 6.2, integration with similarity based models is worth consideration.
</prevsent>
<prevsent>we will incorporate techniques for arranging sentences intoan appropriate order, while the current work concerns only selection.
</prevsent>
</prevsection>
<citsent citstr=" N07-1056 ">
deshpande et al  (2007) <papid> N07-1056 </papid>proposed selection and ordering technique, which is applicable only to the unit cost case such as selection and ordering of words for title generation.</citsent>
<aftsection>
<nextsent>we plan to refine their model so that it can be applied to general text summarization.just trickily uses non-content words to increase the evaluation measure, disregarding the actual quality of summaries.
</nextsent>
<nextsent>788
</nextsent>


</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5078">
<title id=" E12-1019.xml">when did that happen  linking events and relations to time stamps </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our best systems achievef1-scores of 0.76 on events and 0.72 on flu ents.
</prevsent>
<prevsent>it is long-standing goal of nlp to process natural language content in such way that machines can effectively reason over the entities, relations,and events discussed within that content.
</prevsent>
</prevsection>
<citsent citstr=" W10-0901 ">
the applications of such technology are numerous, including intelligence gathering, business analytics, healthcare, education, etc. indeed, the promise of machine reading is actively driving research in this area (etzioni et al 2007; barker et al 2007; clark and harrison, 2010; <papid> W10-0901 </papid>strassel et al 2010).</citsent>
<aftsection>
<nextsent>temporal information is crucial aspect of this task.
</nextsent>
<nextsent>for machine to successfully understand natural language text, it must be able to associate time points and temporal durations with relations and events it discovers in text.the first author conducted this research during an internship at ibm research.in this paper we present methods to establish links between events (e.g. bombing?
</nextsent>
<nextsent>orelection?)
</nextsent>
<nextsent>or fluents (e.g. spouseof?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5079">
<title id=" E12-1019.xml">when did that happen  linking events and relations to time stamps </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>most of the previous work on relation extraction focuses on entity-entity relations, such as in the ace (doddington et al 2004) tasks.
</prevsent>
<prevsent>temporal relations are part of this, but to lesser extent.
</prevsent>
</prevsection>
<citsent citstr=" P08-1090 ">
the primary research effort in event temporality has gone into ordering events with respect to one another (e.g., chambers and jurafsky (2008)), <papid> P08-1090 </papid>and detecting their typical durations (e.g., pan et al (2006)).<papid> P06-1050 </papid></citsent>
<aftsection>
<nextsent>recently, tempe val workshops have focused on the temporal related issues in nlp.
</nextsent>
<nextsent>some of the tempe val tasks overlap with ours in many ways.
</nextsent>
<nextsent>our task is similar to task and of tempeval-1 (verhagen et al 2007) <papid> W07-2014 </papid>in the sense that we attempt to identify temporal relation between events and time expressions or document dates.</nextsent>
<nextsent>however, we do not use restricted set of events, but focus primarily on single temporal relation tlink instead of named relations like before, after or overlap (although we show that we can incorporate these as well).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5080">
<title id=" E12-1019.xml">when did that happen  linking events and relations to time stamps </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>most of the previous work on relation extraction focuses on entity-entity relations, such as in the ace (doddington et al 2004) tasks.
</prevsent>
<prevsent>temporal relations are part of this, but to lesser extent.
</prevsent>
</prevsection>
<citsent citstr=" P06-1050 ">
the primary research effort in event temporality has gone into ordering events with respect to one another (e.g., chambers and jurafsky (2008)), <papid> P08-1090 </papid>and detecting their typical durations (e.g., pan et al (2006)).<papid> P06-1050 </papid></citsent>
<aftsection>
<nextsent>recently, tempe val workshops have focused on the temporal related issues in nlp.
</nextsent>
<nextsent>some of the tempe val tasks overlap with ours in many ways.
</nextsent>
<nextsent>our task is similar to task and of tempeval-1 (verhagen et al 2007) <papid> W07-2014 </papid>in the sense that we attempt to identify temporal relation between events and time expressions or document dates.</nextsent>
<nextsent>however, we do not use restricted set of events, but focus primarily on single temporal relation tlink instead of named relations like before, after or overlap (although we show that we can incorporate these as well).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5081">
<title id=" E12-1019.xml">when did that happen  linking events and relations to time stamps </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>recently, tempe val workshops have focused on the temporal related issues in nlp.
</prevsent>
<prevsent>some of the tempe val tasks overlap with ours in many ways.
</prevsent>
</prevsection>
<citsent citstr=" W07-2014 ">
our task is similar to task and of tempeval-1 (verhagen et al 2007) <papid> W07-2014 </papid>in the sense that we attempt to identify temporal relation between events and time expressions or document dates.</citsent>
<aftsection>
<nextsent>however, we do not use restricted set of events, but focus primarily on single temporal relation tlink instead of named relations like before, after or overlap (although we show that we can incorporate these as well).
</nextsent>
<nextsent>part of ourtask is similar to task of tempeval-2 (verhagen et al 2010), <papid> S10-1010 </papid>determining the temporal relation between an event and time expression in the same sentence.</nextsent>
<nextsent>in this paper, we do apply our system to tempeval-2 data and compare our performance to the participating systems.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5082">
<title id=" E12-1019.xml">when did that happen  linking events and relations to time stamps </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>our task is similar to task and of tempeval-1 (verhagen et al 2007) <papid> W07-2014 </papid>in the sense that we attempt to identify temporal relation between events and time expressions or document dates.</prevsent>
<prevsent>however, we do not use restricted set of events, but focus primarily on single temporal relation tlink instead of named relations like before, after or overlap (although we show that we can incorporate these as well).</prevsent>
</prevsection>
<citsent citstr=" S10-1010 ">
part of ourtask is similar to task of tempeval-2 (verhagen et al 2010), <papid> S10-1010 </papid>determining the temporal relation between an event and time expression in the same sentence.</citsent>
<aftsection>
<nextsent>in this paper, we do apply our system to tempeval-2 data and compare our performance to the participating systems.
</nextsent>
<nextsent>our work is similar to that of boguraev and ando (2005), whose research only deals with temporal links between events and time expressions (and does not consider relations at all).
</nextsent>
<nextsent>they employ sequence tagging model with manual feature engineering for the task and achieved state-of-the-art results on time bank (pustejovskyet al 2003) data.
</nextsent>
<nextsent>our task is slightly different be cause we include relations in the temporal linking, and our use of tree kernels enables us to explore wider feature space very quickly.filatova and hovy (2001) <papid> W01-1313 </papid>also explore temporal linking with events, but do not assume that events and time stamps have been provided by an external process.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5083">
<title id=" E12-1019.xml">when did that happen  linking events and relations to time stamps </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>our work is similar to that of boguraev and ando (2005), whose research only deals with temporal links between events and time expressions (and does not consider relations at all).
</prevsent>
<prevsent>they employ sequence tagging model with manual feature engineering for the task and achieved state-of-the-art results on time bank (pustejovskyet al 2003) data.
</prevsent>
</prevsection>
<citsent citstr=" W01-1313 ">
our task is slightly different be cause we include relations in the temporal linking, and our use of tree kernels enables us to explore wider feature space very quickly.filatova and hovy (2001) <papid> W01-1313 </papid>also explore temporal linking with events, but do not assume that events and time stamps have been provided by an external process.</citsent>
<aftsection>
<nextsent>they used heuristics-based approach to assign temporal expressions to events (also relying on the proximity as base case).
</nextsent>
<nextsent>they report accuracy of the assignment for the correctly classified events, the best being 82.29%.
</nextsent>
<nextsent>our best event system achieves an accuracy of 84.83%.
</nextsent>
<nextsent>these numbers are difficult to compare,however, since accuracy does not efficiently capture the performance of system on task with so many negative examples.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5084">
<title id=" E12-1019.xml">when did that happen  linking events and relations to time stamps </title>
<section> temporal linking framework.  </section>
<citcontext>
<prevsection>
<prevsent>4.2 tree kernel engineering.
</prevsent>
<prevsent>we expect that there exist certain patterns between the entities of temporal link, which manifest on several levels: some on the lexical level, others expressed by certain sequences of pos tags, ne labels, or other representations.
</prevsent>
</prevsection>
<citsent citstr=" J09-4007 ">
kernels provide principled way of expanding the number of dimensions in which we search for decision boundary, and allow us to easily model local sequences and patterns in natural way (giuliano et al., 2009).<papid> J09-4007 </papid></citsent>
<aftsection>
<nextsent>while it is possible to define spacein which we find decision boundary that separates positive and negative instances with manually engineered features, these features can hardly capture the notion of context as well as those explored by tree kernel.
</nextsent>
<nextsent>tree kernels are family of kernel functions developed to compute the similarity between tree structures by counting the number of subtrees they have in common.
</nextsent>
<nextsent>this generates high dimensional feature space that can be handled efficiently using dynamic programming techniques (shawe-taylor and christian ini, 2004).
</nextsent>
<nextsent>for our purposes we used an implementation of the sub tree and subset tree (sst) (moschitti, 2006).<papid> E06-1015 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5085">
<title id=" E12-1019.xml">when did that happen  linking events and relations to time stamps </title>
<section> temporal linking framework.  </section>
<citcontext>
<prevsection>
<prevsent>tree kernels are family of kernel functions developed to compute the similarity between tree structures by counting the number of subtrees they have in common.
</prevsent>
<prevsent>this generates high dimensional feature space that can be handled efficiently using dynamic programming techniques (shawe-taylor and christian ini, 2004).
</prevsent>
</prevsection>
<citsent citstr=" E06-1015 ">
for our purposes we used an implementation of the sub tree and subset tree (sst) (moschitti, 2006).<papid> E06-1015 </papid></citsent>
<aftsection>
<nextsent>the advantages of using tree kernels are two-fold: thanks to an existing implementation (svmlight with tree kernels, moschitti (2004)), <papid> P04-1043 </papid>itis faster and easier than traditional feature engi neering.</nextsent>
<nextsent>the tree structure also allows us to use different levels of representations (pos, lemma, etc.) and combine their contributions, while at thesame time taking into account the ordering of la bels.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5086">
<title id=" E12-1019.xml">when did that happen  linking events and relations to time stamps </title>
<section> temporal linking framework.  </section>
<citcontext>
<prevsection>
<prevsent>this generates high dimensional feature space that can be handled efficiently using dynamic programming techniques (shawe-taylor and christian ini, 2004).
</prevsent>
<prevsent>for our purposes we used an implementation of the sub tree and subset tree (sst) (moschitti, 2006).<papid> E06-1015 </papid></prevsent>
</prevsection>
<citsent citstr=" P04-1043 ">
the advantages of using tree kernels are two-fold: thanks to an existing implementation (svmlight with tree kernels, moschitti (2004)), <papid> P04-1043 </papid>itis faster and easier than traditional feature engi neering.</citsent>
<aftsection>
<nextsent>the tree structure also allows us to use different levels of representations (pos, lemma, etc.) and combine their contributions, while at thesame time taking into account the ordering of labels.
</nextsent>
<nextsent>we use pos, lemma, semantic type, and are presentation that replaces each word with concatenation of its features (capitalization, count able, abstract/concrete noun, etc.).
</nextsent>
<nextsent>we developed shallow tree representation that captures the context of the target terms, without encoding too much structure (which may preventgeneralization).
</nextsent>
<nextsent>in essence, our tree structure induces behavior somewhat similar to string kernel.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5088">
<title id=" E06-3007.xml">lexical ising word order constraints for implemented linear isation grammar </title>
<section> constrained free word order parsing.  </section>
<citcontext>
<prevsection>
<prevsent>the main difference is that we use 29the information inside the feature structure with out having media like positive/negative masks.
</prevsent>
<prevsent>3.2 implementation.
</prevsent>
</prevsection>
<citsent citstr=" E95-1025 ">
i have implemented the algorithm in prolog and coded the hpsg feature structure in the way described using profit (erbach, 1995).<papid> E95-1025 </papid></citsent>
<aftsection>
<nextsent>it is head corner, bottom-up chart parser, roughly based ongazdar and mellish (1989).
</nextsent>
<nextsent>the main modification consists of introducing bit masks and the word order checking procedure described above.i created small grammars for japanese and german and put them to the parser, to confirm that linearisation-heavy constructions such as object control construction can be successfully parsed, with the woc constraints enforced.
</nextsent>
<nextsent>what we have seen is an outline of my initial proposal and there are numerous tasks yet to be tackled.
</nextsent>
<nextsent>first of all, now that the constraints are written in individual lexical items, we are in need of appropriate typing in terms of word order constraints, in order to be able to state succinctly general constraints such as the head-final/initial constraint.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5089">
<title id=" E12-2020.xml">hadoopperceptron a toolkit for distributed perceptron training and prediction with map reduce </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the presented modules can be executed as stand-alone software or easily extended or integrated in complexsystems.
</prevsent>
<prevsent>the execution of the modules applied to specific nlp tasks can be demonstrated and tested via an interactive web interface that allows the user to inspect the status and structure of the cluster and interact with the map reduce jobs.
</prevsent>
</prevsection>
<citsent citstr=" W02-1001 ">
the perceptron training algorithm (rosenblatt, 1958; freund and schapire, 1999; collins, 2002)<papid> W02-1001 </papid>is widely applied in the natural language processing community for learning complex structured models.</citsent>
<aftsection>
<nextsent>the non-probabilistic nature of the perceptron parameters makes it possible to incorporate arbitrary features without the need to calculate partition function, which is required for its discriminative probabilistic counterparts such as crfs (lafferty et al 2001).
</nextsent>
<nextsent>additionally, the perceptron is robust to approximate inference in large search spaces.nevertheless, perceptron training is proportional to inference which is frequently non-linear in the input sequence size.
</nextsent>
<nextsent>therefore, training can be time-consuming for complex model structures.
</nextsent>
<nextsent>furthermore, for an increasing number of tasks is fundamental to leverage on huge sources of data as the world wide web.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5090">
<title id=" E12-2020.xml">hadoopperceptron a toolkit for distributed perceptron training and prediction with map reduce </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>it provides freely availableopen-source implementation of the iterative parameter mixing algorithm for training the structured perceptron on generic sequence labelingtasks.
</prevsent>
<prevsent>furthermore, the package provides two additional modules for prediction and evaluation.
</prevsent>
</prevsection>
<citsent citstr=" N10-4001 ">
the three software modules are designed within the map reduce programming model (dean and ghemawat, 2004) and implemented using the apache hadoop distributed programming framework (white, 2009; lin and dyer, 2010).<papid> N10-4001 </papid></citsent>
<aftsection>
<nextsent>the presented hadoopperceptron package reduces execution time significantly compared to its serial counterpart while maintaining comparable performance.
</nextsent>
<nextsent>97 perceptroniterparammix(t = {(xt,yt)}|t |t=1) 1.
</nextsent>
<nextsent>split into pieces = {t1, . . .
</nextsent>
<nextsent>,ts}.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5092">
<title id=" E12-2020.xml">hadoopperceptron a toolkit for distributed perceptron training and prediction with map reduce </title>
<section> distributed structured perceptron.  </section>
<citcontext>
<prevsection>
<prevsent>during training, the parameters are updated whenever the prediction that employed them is incorrect.
</prevsent>
<prevsent>unlike many batch learning algorithms that can easily be distributed through the gradient calculation, the perceptron online training is more subtle to parallelize.
</prevsent>
</prevsection>
<citsent citstr=" N10-1069 ">
however, mcdonald et al(2010)<papid> N10-1069 </papid>present simple distributed training through parameter mixing scheme.the iterative parameter mixing is given in figure 2 (mcdonald et al 2010).<papid> N10-1069 </papid></citsent>
<aftsection>
<nextsent>first the training data is divided into disjoint splits of example pairs (xt,yt) where xt is the observation sequence andyt is the associated labels.
</nextsent>
<nextsent>the algorithm proceeds to train single epoch of the perceptron algorithm for each split in parallel, and mix the local models weights w(i,n) to produce the global weight vector w. the mixed model is then passed to each split to reset the perceptron local weights, and new iteration is started.
</nextsent>
<nextsent>mcdonald et al (2010) <papid> N10-1069 </papid>provide bound analysis for the algorithm and show that it is guaranteed to converge and find sep eration hyper plane if one exists.</nextsent>
<nextsent>many algorithms need to iterate over number of records and 1) perform some calculation on each of them and then 2) aggregate the results.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5095">
<title id=" E12-2020.xml">hadoopperceptron a toolkit for distributed perceptron training and prediction with map reduce </title>
<section> hadoopperceptron implementation.  </section>
<citcontext>
<prevsection>
<prevsent>it can be easily adapted to wide range ofnlp tasks.
</prevsent>
<prevsent>incorporating new features by modifying the extensible feature extractor is straightforward.
</prevsent>
</prevsection>
<citsent citstr=" P08-1076 ">
the package includes the implementation of the basic feature set described in (suzuki and isozaki, 2008).<papid> P08-1076 </papid></citsent>
<aftsection>
<nextsent>hadoop is bundled with several web interfaces that provide concise tracking information for jobs, tasks, data nodes, etc. as shown in figure 3.
</nextsent>
<nextsent>these web interfaces can be used to demonstrate the hadoopperceptron running phases and monitor the distributed execution of the training, prediction and evaluation modules for several sequence labeling tasks including part-of-speech tagging and named entity recognition.
</nextsent>
<nextsent>we investigate hadoopperceptron training time and prediction accuracy on part-of-speech(pos) task using the penn treebank corpus (mar cus et al 1994).
</nextsent>
<nextsent>we use sections 0-18 of the wall street journal for training, and sections 22-24 for testing.we compare the regular percepton trained serially on all the training data with the distributed perceptron trained with iterative parameter mixing with variable number of splits ? {10, 20}.for each system, we report the prediction accuracy measure on the final test set to determine if any loss is observed as consequence of distributed training.for each system, figure 4 plots accuracy results computed at the end of every training epoch against consumed wall-clock time.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5097">
<title id=" E06-1041.xml">structuring knowledge for reference generation a clustering algorithm </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we describe clustering algorithm which is sufficiently general to be applied to these diverse problems, discuss its application, and evaluate its performance.
</prevsent>
<prevsent>the problem of generating referring expressions(gre) can be summed up as search for the properties in knowledge base (kb) whose combination uniquely distinguishes set of referents from their dis tractors.
</prevsent>
</prevsection>
<citsent citstr=" P90-1013 ">
the content determination strategy adopted in such algorithms is usually based on the assumption (made explicit in reiter (1990)) <papid> P90-1013 </papid>that the space of possible descriptions is partially ordered with respect to some principle(s) which determine their adequacy.</citsent>
<aftsection>
<nextsent>traditionally, these principles have been defined via an interpretation of the gricean maxims (dale, 1989; <papid> P89-1009 </papid>reiter, 1990; <papid> P90-1013 </papid>dale and reiter, 1995; van deemter,2002)1.</nextsent>
<nextsent>however, little attention has been paid to contextual or intentional influences on attribute selection(but cf.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5098">
<title id=" E06-1041.xml">structuring knowledge for reference generation a clustering algorithm </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the problem of generating referring expressions(gre) can be summed up as search for the properties in knowledge base (kb) whose combination uniquely distinguishes set of referents from their dis tractors.
</prevsent>
<prevsent>the content determination strategy adopted in such algorithms is usually based on the assumption (made explicit in reiter (1990)) <papid> P90-1013 </papid>that the space of possible descriptions is partially ordered with respect to some principle(s) which determine their adequacy.</prevsent>
</prevsection>
<citsent citstr=" P89-1009 ">
traditionally, these principles have been defined via an interpretation of the gricean maxims (dale, 1989; <papid> P89-1009 </papid>reiter, 1990; <papid> P90-1013 </papid>dale and reiter, 1995; van deemter,2002)1.</citsent>
<aftsection>
<nextsent>however, little attention has been paid to contextual or intentional influences on attribute selection(but cf.
</nextsent>
<nextsent>jordan and walker (2000); <papid> P00-1024 </papid>krahmer and the une (2002)).</nextsent>
<nextsent>furthermore, it is often assumed thatall relevant knowledge about domain objects is represented in the database in format (e.g. attribute-value pairs) that requires no further processing.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5100">
<title id=" E06-1041.xml">structuring knowledge for reference generation a clustering algorithm </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>traditionally, these principles have been defined via an interpretation of the gricean maxims (dale, 1989; <papid> P89-1009 </papid>reiter, 1990; <papid> P90-1013 </papid>dale and reiter, 1995; van deemter,2002)1.</prevsent>
<prevsent>however, little attention has been paid to contextual or intentional influences on attribute selection(but cf.</prevsent>
</prevsection>
<citsent citstr=" P00-1024 ">
jordan and walker (2000); <papid> P00-1024 </papid>krahmer and the une (2002)).</citsent>
<aftsection>
<nextsent>furthermore, it is often assumed thatall relevant knowledge about domain objects is represented in the database in format (e.g. attribute-value pairs) that requires no further processing.
</nextsent>
<nextsent>this paper is concerned with two scenarios which raise problems for such an approach to gre: 1.
</nextsent>
<nextsent>real-valued attributes, e.g. size or spatial coor-.
</nextsent>
<nextsent>dinates, which represent continuous dimensions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5101">
<title id=" E06-1041.xml">structuring knowledge for reference generation a clustering algorithm </title>
<section> perspectives and semantic similarity.  </section>
<citcontext>
<prevsection>
<prevsent>the description the student 322 and the chef for {e1, e3} is relatively odd compared to the alternative the englishman and the greek.
</prevsent>
<prevsent>in both kinds of scenarios, gre algorithm that relied on arigid preference order could not guarantee that coherent description would be generated every time it was available.the issues raised here have never been systematically addressed in the gre literature, although support for the underlying intuitions can be found in variousquarters.
</prevsent>
</prevsection>
<citsent citstr=" P89-1008 ">
kronfeld (1989) <papid> P89-1008 </papid>distinguishes between functionally and conversation ally relevant descriptions.</citsent>
<aftsection>
<nextsent>adescription is functionally relevant if it succeeds in distinguishing the intended referent(s), but conversational relevance arises in part from implicatures carried bythe use of attributes in context.
</nextsent>
<nextsent>for example, describing e1 as the student carries the (gricean) implicaturethat the entitys academic role or profession is somehow relevant to the current discourse.
</nextsent>
<nextsent>when two entities are described using contrasting properties, say the student and the italian, the listener may find it harder to work out the relevance of the contrast.
</nextsent>
<nextsent>in related vein, aloni (2002) formalises the appropriateness of an answer to question of the form wh x?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5102">
<title id=" E06-1041.xml">structuring knowledge for reference generation a clustering algorithm </title>
<section> perspectives and semantic similarity.  </section>
<citcontext>
<prevsection>
<prevsent>in related vein, aloni (2002) formalises the appropriateness of an answer to question of the form wh x?
</prevsent>
<prevsent>with reference to the conceptual covers?
</prevsent>
</prevsection>
<citsent citstr=" E89-1022 ">
or perspectives under which can be conceptual ised, not all of which are equally relevant given the hearers information state and the discourse context.with respect to plurals, eschenbach et al  (1989) <papid> E89-1022 </papid>argue that the generation of plural anaphor with split antecedent is more felicitous when the antecedents have something in common, such as their ontologicalcategory.</citsent>
<aftsection>
<nextsent>this constraint has been shown to hold psycholinguistically (kaup et al , 2002; koh and clifton, 2002; moxey et al , 2004).
</nextsent>
<nextsent>gatt and van deemter(2005a) have shown that peoples perception of the adequacy of plural descriptions of the form, the n1 and(the) n2 is significantly correlated with the semantic similarity of n1 and n2, while singular descriptions are more likely to be aggregated into plural if semantically similar attributes are available (gatt and van deemter, 2005b).
</nextsent>
<nextsent>the two kinds of problems discussed here could be resolved by pre-processing the kb in order to identify available perspectives.
</nextsent>
<nextsent>one way of doing this isto group available properties into clusters of semantically similar ones.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5103">
<title id=" E09-1074.xml">predicting strong associations on the basis of corpus data </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>to this day, two general problems remain.
</prevsent>
<prevsent>first, the literature lacksa comprehensive comparison between these general types of models.
</prevsent>
</prevsection>
<citsent citstr=" P98-2127 ">
second, we are still looking for an approach that combines several sources of information, so as to correctly predict larger variety of associations.most computational models of semantic relations aim to model semantic similarity in particular (landauer and dumais, 1997; lin, 1998; <papid> P98-2127 </papid>padoand lapata, 2007).</citsent>
<aftsection>
<nextsent>in natural language processing, these models have applications in fields like query expansion, thesaurus extraction, information retrieval, etc. similarly, in cognitive science,such models have helped explain neural activation (mitchell et al , 2008), sentence and discourse comprehension (burgess et al , 1998; foltz, 1996; landauer and dumais, 1997) and priming patterns (lowe and mcdonald, 2000), to name just fewexamples.
</nextsent>
<nextsent>however, there are number of applications and research fields that will surely benefit from models that target the more general phenomenon of association.
</nextsent>
<nextsent>for instance, automatically predicted associations may prove useful in models of information scent, which seek to explain the paths that users follow in their search for relevant information on the web (chi et al , 2001).
</nextsent>
<nextsent>after all, if the visitor of web shop clicks on music to find the prices of ipods, this behaviour is motivated by an associative relation different from similarity.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5104">
<title id=" E09-1074.xml">predicting strong associations on the basis of corpus data </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for instance, automatically predicted associations may prove useful in models of information scent, which seek to explain the paths that users follow in their search for relevant information on the web (chi et al , 2001).
</prevsent>
<prevsent>after all, if the visitor of web shop clicks on music to find the prices of ipods, this behaviour is motivated by an associative relation different from similarity.
</prevsent>
</prevsection>
<citsent citstr=" W05-0206 ">
other possible applications lie in the field of models of text coherence(landauer and dumais, 1997) and automated essay grading (kakkonen et al , 2005).<papid> W05-0206 </papid></citsent>
<aftsection>
<nextsent>in addition,all research in cognitive science that we have referred to above could benefit from computational models of association in order to study the effects of association in comparison to those of similarity.our article is structured as follows.
</nextsent>
<nextsent>in section 2, we will discuss the phenomenon of association and introduce the variety of relations thatit is motivated by.
</nextsent>
<nextsent>parallel to these relations, section 3 presents the three basic types of approaches that we use to predict strong associations.
</nextsent>
<nextsent>section 4 will first compare the results of these three approaches, for total of 43 models.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5105">
<title id=" E09-1074.xml">predicting strong associations on the basis of corpus data </title>
<section> approaches.  </section>
<citcontext>
<prevsection>
<prevsent>3.1 collocation measures.
</prevsent>
<prevsent>probably the most straightforward way to predict strong associations is to assume that cue and its strong association often co-occur in text.
</prevsent>
</prevsection>
<citsent citstr=" P89-1010 ">
as result, we can use collocation measures like point-wise mutual information (church and hanks, 1989) <papid> P89-1010 </papid>or the log-likelihood ratio (dunning, 1993) <papid> J93-1003 </papid>to predict the strong association forgiven cue.</citsent>
<aftsection>
<nextsent>point-wise mutual information (pmi) tells us if two words w1 and w2 occur together more orless often than expected on the basis of their individual frequencies and the independence assump tion: pmi(w1, w2) = log2 (w1, w2) (w1) ? (w2)the log-likelihood ratio compares the like lihoods of the independence hypothesis (i.e.,p = (w2|w1) = (w2|w1)) and the dependence hypothesis (i.e., p1 = (w2|w1) 6= (w2|w1) = p2), under the assumption that the words in text are binomially distributed: log ? = log l(p (w2|w1); p) ? l(p (w2|w1); p) l(p (w2|w1); p1) ? l(p (w2|w1); p2) 3.2 word space models.
</nextsent>
<nextsent>a respectable proportion (in our data about 18%)of the strong associations are motivated by semantic similarity to their cue.
</nextsent>
<nextsent>they can be synonyms, hyponyms, hypernyms, co-hyponyms or 649 antonyms.
</nextsent>
<nextsent>collocation measures, however, are not specifically targeted towards the discovery of semantic similarity.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5106">
<title id=" E09-1074.xml">predicting strong associations on the basis of corpus data </title>
<section> approaches.  </section>
<citcontext>
<prevsection>
<prevsent>3.1 collocation measures.
</prevsent>
<prevsent>probably the most straightforward way to predict strong associations is to assume that cue and its strong association often co-occur in text.
</prevsent>
</prevsection>
<citsent citstr=" J93-1003 ">
as result, we can use collocation measures like point-wise mutual information (church and hanks, 1989) <papid> P89-1010 </papid>or the log-likelihood ratio (dunning, 1993) <papid> J93-1003 </papid>to predict the strong association forgiven cue.</citsent>
<aftsection>
<nextsent>point-wise mutual information (pmi) tells us if two words w1 and w2 occur together more orless often than expected on the basis of their individual frequencies and the independence assump tion: pmi(w1, w2) = log2 (w1, w2) (w1) ? (w2)the log-likelihood ratio compares the like lihoods of the independence hypothesis (i.e.,p = (w2|w1) = (w2|w1)) and the dependence hypothesis (i.e., p1 = (w2|w1) 6= (w2|w1) = p2), under the assumption that the words in text are binomially distributed: log ? = log l(p (w2|w1); p) ? l(p (w2|w1); p) l(p (w2|w1); p1) ? l(p (w2|w1); p2) 3.2 word space models.
</nextsent>
<nextsent>a respectable proportion (in our data about 18%)of the strong associations are motivated by semantic similarity to their cue.
</nextsent>
<nextsent>they can be synonyms, hyponyms, hypernyms, co-hyponyms or 649 antonyms.
</nextsent>
<nextsent>collocation measures, however, are not specifically targeted towards the discovery of semantic similarity.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5108">
<title id=" E09-1074.xml">predicting strong associations on the basis of corpus data </title>
<section> ensemble-based prediction of strong.  </section>
<citcontext>
<prevsection>
<prevsent>we therefore investigated the benefits of committee-based or ensemble approach.
</prevsent>
<prevsent>associations given the varied nature of cue association relations, it could be beneficial to develop model that relies on more than one type of information.
</prevsent>
</prevsection>
<citsent citstr=" W02-1029 ">
ensemble methods have already proved their effectiveness in the related area of automatic thesaurus extraction (curran, 2002), <papid> W02-1029 </papid>where semantic similarity is the target relation.</citsent>
<aftsection>
<nextsent>curran (2002) <papid> W02-1029 </papid>explored three ways of combining multiple ordered sets of words: (1) mean, taking the mean rank of each word over the ensemble; (2) harmonic, taking the harmonic mean; (3) mixture, calculating the mean similarity score for each word.</nextsent>
<nextsent>we will study only the first two of these approaches, as the different metrics of our models cannot simply be combined in mean relatedness score.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5110">
<title id=" E09-1049.xml">ngrambased statistical machine translation versus syntax augmented machine translation comparison and system combination </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>human error analysis clarifies advantages and disadvantages of the systems under consideration.
</prevsent>
<prevsent>finally, we combine the output of both systems toyield significant improvements in translation quality.
</prevsent>
</prevsection>
<citsent citstr=" N03-1017 ">
there is an ongoing controversy regarding whether or not information about the syntax of language can benefit mt or contribute to hybrid system.classical ibm word-based models were recently augmented with phrase translation capability, as shown in koehn et al (2003), <papid> N03-1017 </papid>or inmore recent implementation, the moses mt system1 (koehn et al, 2007).</citsent>
<aftsection>
<nextsent>in parallel to the phrase based approach, the -gram-based approach appeared (mario et al, 2006).
</nextsent>
<nextsent>it stemms from 1www.statmt.org/moses/the finite-state transducers paradigm, and is extended to the log-linear modeling framework, asshown in (mario et al, 2006).
</nextsent>
<nextsent>a system following this approach deals with bilingual units, called tuples, which are composed of one or more words from the source language and zero or more words from the target one.
</nextsent>
<nextsent>the -gram-based systems allow for linguistically motivated word reordering by implementing word order monotonization.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5112">
<title id=" E09-1049.xml">ngrambased statistical machine translation versus syntax augmented machine translation comparison and system combination </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the -gram-based systems allow for linguistically motivated word reordering by implementing word order monotonization.
</prevsent>
<prevsent>prior to the smt revolution, major part of mt systems was developed using rule-based algorithms; however, starting from the 1990s,syntax-driven systems based on phrase hierarchy have gained popularity.
</prevsent>
</prevsection>
<citsent citstr=" P04-1083 ">
a representative sample of modern syntax-based systems includes models based on bilingual synchronous grammar (melamed, 2004), <papid> P04-1083 </papid>parse tree-to-string translation models (yamada and knight, 2001) <papid> P01-1067 </papid>and non isomorphic tree-to-tree mappings (eisner, 2003).<papid> P03-2041 </papid>the orthodox phrase-based model was enhanced in chiang (2005),<papid> P05-1033 </papid>where hierarchical phrase model allowing for multiple generalizations within each phrase was introduced.</citsent>
<aftsection>
<nextsent>theopen-source toolkit samt2 (zollmann and venugopal, 2006) <papid> W06-3119 </papid>is further evolution of this approach, in which syntactic categories extracted from the target side parse tree are directly assigned to the hierarchically structured phrases.</nextsent>
<nextsent>several publications discovering similarities and differences between distinct translation models have been written over the last few years.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5113">
<title id=" E09-1049.xml">ngrambased statistical machine translation versus syntax augmented machine translation comparison and system combination </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the -gram-based systems allow for linguistically motivated word reordering by implementing word order monotonization.
</prevsent>
<prevsent>prior to the smt revolution, major part of mt systems was developed using rule-based algorithms; however, starting from the 1990s,syntax-driven systems based on phrase hierarchy have gained popularity.
</prevsent>
</prevsection>
<citsent citstr=" P01-1067 ">
a representative sample of modern syntax-based systems includes models based on bilingual synchronous grammar (melamed, 2004), <papid> P04-1083 </papid>parse tree-to-string translation models (yamada and knight, 2001) <papid> P01-1067 </papid>and non isomorphic tree-to-tree mappings (eisner, 2003).<papid> P03-2041 </papid>the orthodox phrase-based model was enhanced in chiang (2005),<papid> P05-1033 </papid>where hierarchical phrase model allowing for multiple generalizations within each phrase was introduced.</citsent>
<aftsection>
<nextsent>theopen-source toolkit samt2 (zollmann and venugopal, 2006) <papid> W06-3119 </papid>is further evolution of this approach, in which syntactic categories extracted from the target side parse tree are directly assigned to the hierarchically structured phrases.</nextsent>
<nextsent>several publications discovering similarities and differences between distinct translation models have been written over the last few years.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5114">
<title id=" E09-1049.xml">ngrambased statistical machine translation versus syntax augmented machine translation comparison and system combination </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the -gram-based systems allow for linguistically motivated word reordering by implementing word order monotonization.
</prevsent>
<prevsent>prior to the smt revolution, major part of mt systems was developed using rule-based algorithms; however, starting from the 1990s,syntax-driven systems based on phrase hierarchy have gained popularity.
</prevsent>
</prevsection>
<citsent citstr=" P03-2041 ">
a representative sample of modern syntax-based systems includes models based on bilingual synchronous grammar (melamed, 2004), <papid> P04-1083 </papid>parse tree-to-string translation models (yamada and knight, 2001) <papid> P01-1067 </papid>and non isomorphic tree-to-tree mappings (eisner, 2003).<papid> P03-2041 </papid>the orthodox phrase-based model was enhanced in chiang (2005),<papid> P05-1033 </papid>where hierarchical phrase model allowing for multiple generalizations within each phrase was introduced.</citsent>
<aftsection>
<nextsent>theopen-source toolkit samt2 (zollmann and venugopal, 2006) <papid> W06-3119 </papid>is further evolution of this approach, in which syntactic categories extracted from the target side parse tree are directly assigned to the hierarchically structured phrases.</nextsent>
<nextsent>several publications discovering similarities and differences between distinct translation models have been written over the last few years.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5115">
<title id=" E09-1049.xml">ngrambased statistical machine translation versus syntax augmented machine translation comparison and system combination </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the -gram-based systems allow for linguistically motivated word reordering by implementing word order monotonization.
</prevsent>
<prevsent>prior to the smt revolution, major part of mt systems was developed using rule-based algorithms; however, starting from the 1990s,syntax-driven systems based on phrase hierarchy have gained popularity.
</prevsent>
</prevsection>
<citsent citstr=" P05-1033 ">
a representative sample of modern syntax-based systems includes models based on bilingual synchronous grammar (melamed, 2004), <papid> P04-1083 </papid>parse tree-to-string translation models (yamada and knight, 2001) <papid> P01-1067 </papid>and non isomorphic tree-to-tree mappings (eisner, 2003).<papid> P03-2041 </papid>the orthodox phrase-based model was enhanced in chiang (2005),<papid> P05-1033 </papid>where hierarchical phrase model allowing for multiple generalizations within each phrase was introduced.</citsent>
<aftsection>
<nextsent>theopen-source toolkit samt2 (zollmann and venugopal, 2006) <papid> W06-3119 </papid>is further evolution of this approach, in which syntactic categories extracted from the target side parse tree are directly assigned to the hierarchically structured phrases.</nextsent>
<nextsent>several publications discovering similarities and differences between distinct translation models have been written over the last few years.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5117">
<title id=" E09-1049.xml">ngrambased statistical machine translation versus syntax augmented machine translation comparison and system combination </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>prior to the smt revolution, major part of mt systems was developed using rule-based algorithms; however, starting from the 1990s,syntax-driven systems based on phrase hierarchy have gained popularity.
</prevsent>
<prevsent>a representative sample of modern syntax-based systems includes models based on bilingual synchronous grammar (melamed, 2004), <papid> P04-1083 </papid>parse tree-to-string translation models (yamada and knight, 2001) <papid> P01-1067 </papid>and non isomorphic tree-to-tree mappings (eisner, 2003).<papid> P03-2041 </papid>the orthodox phrase-based model was enhanced in chiang (2005),<papid> P05-1033 </papid>where hierarchical phrase model allowing for multiple generalizations within each phrase was introduced.</prevsent>
</prevsection>
<citsent citstr=" W06-3119 ">
theopen-source toolkit samt2 (zollmann and venugopal, 2006) <papid> W06-3119 </papid>is further evolution of this approach, in which syntactic categories extracted from the target side parse tree are directly assigned to the hierarchically structured phrases.</citsent>
<aftsection>
<nextsent>several publications discovering similarities and differences between distinct translation models have been written over the last few years.
</nextsent>
<nextsent>in crego et al (2005b), the -gram-based system is contrasted with state-of-the-art phrase-based framework, while in deneefe et al (2007), <papid> D07-1079 </papid>the authors seek to estimate the advantages, weakest points and possible overlap between syntax based mt and phrase-based smt.</nextsent>
<nextsent>in zollmann etal.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5119">
<title id=" E09-1049.xml">ngrambased statistical machine translation versus syntax augmented machine translation comparison and system combination </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>theopen-source toolkit samt2 (zollmann and venugopal, 2006) <papid> W06-3119 </papid>is further evolution of this approach, in which syntactic categories extracted from the target side parse tree are directly assigned to the hierarchically structured phrases.</prevsent>
<prevsent>several publications discovering similarities and differences between distinct translation models have been written over the last few years.</prevsent>
</prevsection>
<citsent citstr=" D07-1079 ">
in crego et al (2005b), the -gram-based system is contrasted with state-of-the-art phrase-based framework, while in deneefe et al (2007), <papid> D07-1079 </papid>the authors seek to estimate the advantages, weakest points and possible overlap between syntax based mt and phrase-based smt.</citsent>
<aftsection>
<nextsent>in zollmann etal.
</nextsent>
<nextsent>(2008) the comparison of phrase-based ,  chiangs style  hirearchical system and samt is pro 2www.cs.cmu.edu/zollmann/samt 424 vided.in this study, we intend to compare the differences and similarities of the statistical -gram based smt approach and the samt system.
</nextsent>
<nextsent>the comparison is performed on small arabic-to english translation task from the news domain.
</nextsent>
<nextsent>a criticism of phrase-based models is data sparseness.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5120">
<title id=" E09-1049.xml">ngrambased statistical machine translation versus syntax augmented machine translation comparison and system combination </title>
<section> samt system.  </section>
<citcontext>
<prevsection>
<prevsent>a criticism of phrase-based models is data sparseness.
</prevsent>
<prevsent>this problem is even more serious when the source, the target, or both languages are inflectional and rich in morphology.
</prevsent>
</prevsection>
<citsent citstr=" J04-4002 ">
moreover, phrase based models are unable to cope with global reordering because the distortion model is basedon movement distance, which may face computational resource limitations (och and ney, 2004).<papid> J04-4002 </papid></citsent>
<aftsection>
<nextsent>this problem was successfully addressed whenthe mt system based on generalized hierarchically structured phrases was introduced and discussed in chiang (2005).<papid> P05-1033 </papid></nextsent>
<nextsent>it operates with only two markers (a substantial phrase category and  glue marker ).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5126">
<title id=" E09-1049.xml">ngrambased statistical machine translation versus syntax augmented machine translation comparison and system combination </title>
<section> samt system.  </section>
<citcontext>
<prevsection>
<prevsent>consequently, all lexical mapping rules are covered by the phrases mapping table.
</prevsent>
<prevsent>2.2 rules annotation, generalization and.
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
pruning the samt system is based on purely lexical phrase table, which is identified as shown in koehn et al (2003), <papid> N03-1017 </papid>and word alignment, which is generated by the grow-diag-final-and method (expanding the alignment by adding directly neighboring alignment points and alignment points in the diagonal neighborhood) (och and ney, 2003).<papid> J03-1002 </papid></citsent>
<aftsection>
<nextsent>meanwhile, the target of the training corpus is parsed with charniaks parser (charniak, 2000), <papid> A00-2018 </papid>and each phrase is annotated with the constituent that spans the target side of the rules.</nextsent>
<nextsent>the set of non-terminals is extended by means of conditional and additive categories according to combinatory categorical grammar (ccg) (steedman, 1999).<papid> P99-1039 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5127">
<title id=" E09-1049.xml">ngrambased statistical machine translation versus syntax augmented machine translation comparison and system combination </title>
<section> samt system.  </section>
<citcontext>
<prevsection>
<prevsent>2.2 rules annotation, generalization and.
</prevsent>
<prevsent>pruning the samt system is based on purely lexical phrase table, which is identified as shown in koehn et al (2003), <papid> N03-1017 </papid>and word alignment, which is generated by the grow-diag-final-and method (expanding the alignment by adding directly neighboring alignment points and alignment points in the diagonal neighborhood) (och and ney, 2003).<papid> J03-1002 </papid></prevsent>
</prevsection>
<citsent citstr=" A00-2018 ">
meanwhile, the target of the training corpus is parsed with charniaks parser (charniak, 2000), <papid> A00-2018 </papid>and each phrase is annotated with the constituent that spans the target side of the rules.</citsent>
<aftsection>
<nextsent>the set of non-terminals is extended by means of conditional and additive categories according to combinatory categorical grammar (ccg) (steedman, 1999).<papid> P99-1039 </papid></nextsent>
<nextsent>under this approach, new rules can be formed.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5128">
<title id=" E09-1049.xml">ngrambased statistical machine translation versus syntax augmented machine translation comparison and system combination </title>
<section> samt system.  </section>
<citcontext>
<prevsection>
<prevsent>pruning the samt system is based on purely lexical phrase table, which is identified as shown in koehn et al (2003), <papid> N03-1017 </papid>and word alignment, which is generated by the grow-diag-final-and method (expanding the alignment by adding directly neighboring alignment points and alignment points in the diagonal neighborhood) (och and ney, 2003).<papid> J03-1002 </papid></prevsent>
<prevsent>meanwhile, the target of the training corpus is parsed with charniaks parser (charniak, 2000), <papid> A00-2018 </papid>and each phrase is annotated with the constituent that spans the target side of the rules.</prevsent>
</prevsection>
<citsent citstr=" P99-1039 ">
the set of non-terminals is extended by means of conditional and additive categories according to combinatory categorical grammar (ccg) (steedman, 1999).<papid> P99-1039 </papid></citsent>
<aftsection>
<nextsent>under this approach, new rules can be formed.
</nextsent>
<nextsent>for example, rb+vb, can represent an additive constituent consisting of two synthetically generated adjacent categories 3, i.e., an adverb and verb.
</nextsent>
<nextsent>furthermore, dt\np can indicate an incomplete noun phrase with missing determiner to the left.the rule recursive generalization procedure coincides with the one proposed in chiang (2005),<papid> P05-1033 </papid>but violates the restrictions introduced for single category grammar; for example, rules that contain adjacent generalized elements are not discarded.</nextsent>
<nextsent>thus, each rule ??</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5132">
<title id=" E09-1049.xml">ngrambased statistical machine translation versus syntax augmented machine translation comparison and system combination </title>
<section> upc n-gram smt system.  </section>
<citcontext>
<prevsection>
<prevsent>426 ei1 = argmaxei1 { p(ei1 | fj1 ) } = (1) = argmax ei1 { p(fj1 | ei1) ? p(ei1) } (2) where and represent the number of words in the target and source languages, respectively.
</prevsent>
<prevsent>modern state-of-the-art smt systems operate with the bilingual units extracted from the parallel corpus based on word-to-word alignment.
</prevsent>
</prevsection>
<citsent citstr=" P02-1038 ">
they are enhanced by the maximum entropy approach and the posterior probability is calculated as loglinear combination of set of feature functions (och and ney, 2002).<papid> P02-1038 </papid></citsent>
<aftsection>
<nextsent>using this technique, the additional models are combined to determine the translation hypothesis, as shown in (3): ei1 = argmaxei1 { m?
</nextsent>
<nextsent>m=1 mhm(ei1, fj1 ) } (3) where the feature functions hm refer to the system models and the set of refers to the weights corresponding to these models.
</nextsent>
<nextsent>3.1 n-gram-based translation system the -gram approach to smt is considered to be an alternative to the phrase-based translation,where given source word sequence is decomposed into monolingual phrases that are then translated one by one (marcu and wong, 2002).<papid> W02-1018 </papid>the -gram-based approach regards translation as stochastic process that maximizes the joint probability p(f, e), leading to decomposition based on bilingual n-grams.</nextsent>
<nextsent>the core part of the system constructed in this way is translation model (tm), which is based on bilingual units,called tuples, that are extracted from word alignment (performed with giza++ tool4) according to certain constraints.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5133">
<title id=" E09-1049.xml">ngrambased statistical machine translation versus syntax augmented machine translation comparison and system combination </title>
<section> upc n-gram smt system.  </section>
<citcontext>
<prevsection>
<prevsent>using this technique, the additional models are combined to determine the translation hypothesis, as shown in (3): ei1 = argmaxei1 { m?
</prevsent>
<prevsent>m=1 mhm(ei1, fj1 ) } (3) where the feature functions hm refer to the system models and the set of refers to the weights corresponding to these models.
</prevsent>
</prevsection>
<citsent citstr=" W02-1018 ">
3.1 n-gram-based translation system the -gram approach to smt is considered to be an alternative to the phrase-based translation,where given source word sequence is decomposed into monolingual phrases that are then translated one by one (marcu and wong, 2002).<papid> W02-1018 </papid>the -gram-based approach regards translation as stochastic process that maximizes the joint probability p(f, e), leading to decomposition based on bilingual n-grams.</citsent>
<aftsection>
<nextsent>the core part of the system constructed in this way is translation model (tm), which is based on bilingual units,called tuples, that are extracted from word alignment (performed with giza++ tool4) according to certain constraints.
</nextsent>
<nextsent>a bilingual tm actually constitutes an n-gram lm of tuples, which approximates the joint probability between the languages under consideration and can be seen here as lm, where the language is composed of tuples.
</nextsent>
<nextsent>3.2 additional features.
</nextsent>
<nextsent>the -gram translation system implements loglinear combination of five additional models: ? an n-gram target lm; 4http://code.google.com/p/giza-pp/ ? target lm of part-of-speech tags;?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5136">
<title id=" E09-1049.xml">ngrambased statistical machine translation versus syntax augmented machine translation comparison and system combination </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>4.2 arabic data preprocessing.
</prevsent>
<prevsent>arabic is vso (svo in some cases) pro drop language with rich templatic morphology, where words are made up of roots and affixesand clitics agglutinate to words.
</prevsent>
</prevsection>
<citsent citstr=" N06-2013 ">
for preprocessing, similar approach to that shown in habash and sadat (2006) <papid> N06-2013 </papid>was employed, and the mada+tokan system for disambiguation and tokenization was used.</citsent>
<aftsection>
<nextsent>for disambiguation, onlydiacritic unigram statistics were employed.
</nextsent>
<nextsent>for tokenization, the d3 scheme with -tagbies option was used.
</nextsent>
<nextsent>the scheme splits the following set ofclitics: w+, f+, b+, k+, l+, al+ and pronominal cl itics.
</nextsent>
<nextsent>the -tagbies option produces bies pos tags on all tag gable tokens.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5138">
<title id=" E09-1049.xml">ngrambased statistical machine translation versus syntax augmented machine translation comparison and system combination </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>the target side (english) of the training corpus was parsed with the charniaks parser (charniak, 2000).<papid> A00-2018 </papid></prevsent>
<prevsent>rule extraction and filtering procedures were restricted to the concatenation of the development and test sets, allowing for rules with maximal length of 12 elements in the source side and with azero minimum occurrence criterion for both non lexical and purely lexical rules.moses-style phrases extracted with phrase based system were 4.8m , while number of generalized rules representing the hierarchical model grew dramatically to 22.9m . 10.8m of them were pruned out on the filtering step.</prevsent>
</prevsection>
<citsent citstr=" N07-1063 ">
the vocabulary of the english penn treebank elementary non-terminals is 72, while number of generalized elements, including additive and truncated categories, is 35.7k.the fasttranslatechart beam-search decoder was used as an engine of mer training aiming to tune the feature weight coefficients and produce final n-best and 1-best translations by combining the intensive search with standard 4-gramlm as shown in venugopal et al (2007).<papid> N07-1063 </papid></citsent>
<aftsection>
<nextsent>the iteration limit was set to 10 with 1000-best list and the highest bleu score as optimization criteria.we did not use completely abstract rules (with out any source-side lexical utterance), since these rules significantly slow down the decoding process (noallowabstractrules option).
</nextsent>
<nextsent>table 2 shows summary of computational time and ram needed at each step of the translation.
</nextsent>
<nextsent>step time memory parsing 1.5h 80mb rules extraction 10h 3.5gb filtering&merging; 3h 4.0gb weights tuning 40h 3gb testing 2h 3gb table 2: samt: computational resources.
</nextsent>
<nextsent>evaluation scores including results of system combination (see subsection 4.6) are reported in table 3.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5139">
<title id=" E09-1049.xml">ngrambased statistical machine translation versus syntax augmented machine translation comparison and system combination </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>4.4 n-gram system experiments the core model of the -gram-based system is 4-gram lm of bilingual units containing: 184.345 1-grams7, 552.838 2-grams, 179.466 3-grams and 176.221 4-grams.along with this model, an -gram smt system implements log-linear combination of 5 gram target lm estimated on the english portion of the parallel corpus, as well as supporting 4 gram source and target models of pos tags.
</prevsent>
<prevsent>bies7this number also corresponds to the bilingual model vocabulary.
</prevsent>
</prevsection>
<citsent citstr=" A00-1031 ">
428 bleu nist mper mwer meteor samt 43.20 9.26 36.89 49.45 58.50 n-gram-based smt 46.39 10.06 32.98 48.47 62.36 system combination 48.00 10.15 33.20 47.54 62.27 moses factored system 44.73 9.62 33.92 47.23 59.84 oracle 61.90 11.41 28.84 41.52 66.19 table 3: test set evaluation results pos tags were used for the arabic portion, as shown in subsection 4.2; tnt tool was used for english pos tagging (brants, 2000).<papid> A00-1031 </papid></citsent>
<aftsection>
<nextsent>the number of non-unique initially extracted tuples is 1.1m , which were pruned according to the maximum number of translation options per tuple on the source side (30).
</nextsent>
<nextsent>tuples with nullon the source side were attached to either the previous or the next unit (mario et al, 2006).
</nextsent>
<nextsent>the feature models weights were optimized according to the same optimization criteria as in the samt experiments (the highest bleu score).
</nextsent>
<nextsent>stage-by-stage ram and time requirements are presented in table 4, while translation quality evaluation results can be found in table 3.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5140">
<title id=" E09-1049.xml">ngrambased statistical machine translation versus syntax augmented machine translation comparison and system combination </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>step time memory models estimation 0.2h 1.9gb reordering 1h ? weights tuning 15h 120mb testing 2h 120mbtable 4: tuple-based smt: computational resources.
</prevsent>
<prevsent>4.5 statistical significance.
</prevsent>
</prevsection>
<citsent citstr=" W04-3250 ">
a statistical significance test based on bootstrap re sampling method, as shown in koehn (2004), <papid> W04-3250 </papid>was performed.</citsent>
<aftsection>
<nextsent>for the 98% confidence interval and 1000 set re samples, translations generated bysamt and -gram system are significantly different according to bleu (43.201.69 for samt vs. 46.42?
</nextsent>
<nextsent>1.61 for tuple-based system).
</nextsent>
<nextsent>4.6 system combination.
</nextsent>
<nextsent>many mt systems generate very different translations of similar quality, even if the models involved into translation process are analogous.thus, the outputs of syntax-driven and purely statistical mt systems were combined at the sentence level using 1000-best lists of the most probable translations produced by the both systems.for system combination, we followed minimum bayes-risk algorithm, as introduced in kumar and byrne (2004).<papid> N04-1022 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5141">
<title id=" E09-1049.xml">ngrambased statistical machine translation versus syntax augmented machine translation comparison and system combination </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>1.61 for tuple-based system).
</prevsent>
<prevsent>4.6 system combination.
</prevsent>
</prevsection>
<citsent citstr=" N04-1022 ">
many mt systems generate very different translations of similar quality, even if the models involved into translation process are analogous.thus, the outputs of syntax-driven and purely statistical mt systems were combined at the sentence level using 1000-best lists of the most probable translations produced by the both systems.for system combination, we followed minimum bayes-risk algorithm, as introduced in kumar and byrne (2004).<papid> N04-1022 </papid></citsent>
<aftsection>
<nextsent>table 3 shows the results of the system combination experiments on the testset, which are contrasted with the oracle translation results, performed as selection of the translations with the highest bleu score from the unionof two 1000-best lists generated by samt and gram smt.
</nextsent>
<nextsent>we also analyzed the percentage contribution of each system to the system combination: 55-60% of best translations come from the tuples-basedsystem 1000-best list, both for system combination and oracle experiments on the test set.
</nextsent>
<nextsent>4.7 phrase-based reference system.
</nextsent>
<nextsent>in order to understand the obtained results compared to the state-of-the-art smt, reference phrase-based factored smt system was trained and tested on the same data using the moses toolkit.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5142">
<title id=" E09-1030.xml">reconstructing false start errors in spontaneous speech text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>1.2 related work.
</prevsent>
<prevsent>stochastic approaches for simple dis fluency detection use features such as lexical form, acoustic cues, and rule-based knowledge.
</prevsent>
</prevsection>
<citsent citstr=" P04-1005 ">
most state-of the-art methods for edit region detection such as (johnson and charniak, 2004; <papid> P04-1005 </papid>zhang and weng, 2005; <papid> W05-1519 </papid>liu et al , 2004; honal and schultz, 2005) model speech disfluencies as noisy channel model.</citsent>
<aftsection>
<nextsent>in noisy channel model we assume that an unknown but fluent string has passed througha disfluency-adding channel to produce the observed dis fluent string d, and we then aim to recover the most likely input string f?
</nextsent>
<nextsent>, defined as f?
</nextsent>
<nextsent>= argmaxfp (f |d) = argmaxfp (d|f )p (f )where (f ) represents language model defining probability distribution over fluent source strings , and (d|f ) is the channel model defining conditional probability distribution of observed sentences which may contain the types of construction errors described in the previoussubsection.
</nextsent>
<nextsent>the final output is word-level tagging of the error condition of each word in these quence, as seen in line 2 of figure 2.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5143">
<title id=" E09-1030.xml">reconstructing false start errors in spontaneous speech text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>1.2 related work.
</prevsent>
<prevsent>stochastic approaches for simple dis fluency detection use features such as lexical form, acoustic cues, and rule-based knowledge.
</prevsent>
</prevsection>
<citsent citstr=" W05-1519 ">
most state-of the-art methods for edit region detection such as (johnson and charniak, 2004; <papid> P04-1005 </papid>zhang and weng, 2005; <papid> W05-1519 </papid>liu et al , 2004; honal and schultz, 2005) model speech disfluencies as noisy channel model.</citsent>
<aftsection>
<nextsent>in noisy channel model we assume that an unknown but fluent string has passed througha disfluency-adding channel to produce the observed dis fluent string d, and we then aim to recover the most likely input string f?
</nextsent>
<nextsent>, defined as f?
</nextsent>
<nextsent>= argmaxfp (f |d) = argmaxfp (d|f )p (f )where (f ) represents language model defining probability distribution over fluent source strings , and (d|f ) is the channel model defining conditional probability distribution of observed sentences which may contain the types of construction errors described in the previoussubsection.
</nextsent>
<nextsent>the final output is word-level tagging of the error condition of each word in these quence, as seen in line 2 of figure 2.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5145">
<title id=" E09-1030.xml">reconstructing false start errors in spontaneous speech text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>1 he that uh that a relief 3 nc rc rc fl - - - -.
</prevsent>
<prevsent>figure 2: example of word class and refined word class labels, where - denotes non-error, fl denotes filler, generally denotes rep aranda, and rc and nc indicate rough copy and non-copy speaker errors, respectively.
</prevsent>
</prevsection>
<citsent citstr=" L08-1530 ">
for improvement, we used the top-performing1 jc04 noisy channel tag edit detector to produce edit detection analyses on the test segment of the spontaneous speech reconstruction (ssr) corpus(fitzgerald and jelinek, 2008).<papid> L08-1530 </papid></citsent>
<aftsection>
<nextsent>table 1 demonstrates the performance of this system for detecting filled pause fillers, discourse marker fillers, and edit words.
</nextsent>
<nextsent>the results of more granular analysis compared to hand-refined reference (as shown in line 3 of figure 2) are shown in table 2.
</nextsent>
<nextsent>the reader will recall that precision is defined as = |correct||correct|+|false| and recall = |correct| |correct|+|miss| .we denote the harmonic mean of and as score and calculate it = 21/p+1/r . as expected given the assumptions of the tag approach, jc04 identifies repetitions and most revisions in the ssr data, but less successfully labels false starts and other speaker self interruptions which do not have cross-serial correlations.
</nextsent>
<nextsent>these non-copy errors (with recall of only 43.2%), are hurting the overall edit detection recall score.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5149">
<title id=" E09-1030.xml">reconstructing false start errors in spontaneous speech text </title>
<section> approach.  </section>
<citcontext>
<prevsection>
<prevsent>the conditional probability of this model can be represented as p?(y |x) = 1 z?(x) exp( ? kfk(x,y )) (1) where z?(x) is global normalization factor and ? = (1 . . .
</prevsent>
<prevsent>k) are model parameters related to each feature function fk(x,y ).
</prevsent>
</prevsection>
<citsent citstr=" N03-1028 ">
crfs have been widely applied to tasks in natural language processing, especially those involving tagging words with labels such as part of-speech tagging and shallow parsing (sha and pereira, 2003), <papid> N03-1028 </papid>as well as sentence boundary detection (liu et al , 2005; <papid> P05-1056 </papid>liu et al , 2004).</citsent>
<aftsection>
<nextsent>these models have the advantage that they model sequential context (like hidden markov models(hmms)) but are discriminative rather than generative and have less restricted feature set.
</nextsent>
<nextsent>additionally, as compared to hmms, crfs offer conditional (versus joint) likelihood, and directly maximizes posterior label probabilities (e|o).
</nextsent>
<nextsent>we used the grmm package (sutton, 2006) to implement our crf models, each using zero mean gaussian prior to reduce over-fitting our model.
</nextsent>
<nextsent>no feature reduction is employed, except where indicated.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5150">
<title id=" E09-1030.xml">reconstructing false start errors in spontaneous speech text </title>
<section> approach.  </section>
<citcontext>
<prevsection>
<prevsent>the conditional probability of this model can be represented as p?(y |x) = 1 z?(x) exp( ? kfk(x,y )) (1) where z?(x) is global normalization factor and ? = (1 . . .
</prevsent>
<prevsent>k) are model parameters related to each feature function fk(x,y ).
</prevsent>
</prevsection>
<citsent citstr=" P05-1056 ">
crfs have been widely applied to tasks in natural language processing, especially those involving tagging words with labels such as part of-speech tagging and shallow parsing (sha and pereira, 2003), <papid> N03-1028 </papid>as well as sentence boundary detection (liu et al , 2005; <papid> P05-1056 </papid>liu et al , 2004).</citsent>
<aftsection>
<nextsent>these models have the advantage that they model sequential context (like hidden markov models(hmms)) but are discriminative rather than generative and have less restricted feature set.
</nextsent>
<nextsent>additionally, as compared to hmms, crfs offer conditional (versus joint) likelihood, and directly maximizes posterior label probabilities (e|o).
</nextsent>
<nextsent>we used the grmm package (sutton, 2006) to implement our crf models, each using zero mean gaussian prior to reduce over-fitting our model.
</nextsent>
<nextsent>no feature reduction is employed, except where indicated.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5151">
<title id=" E09-1084.xml">using non lexical features to identify effective indexing terms for biomedical illustrations </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>non-lexical features have been successful in many contexts, particularly in the areas of genre classification and text and speech summarization.
</prevsent>
<prevsent>genre classification, unlike text classification, discriminates between document style instead of topic.
</prevsent>
</prevsection>
<citsent citstr=" W01-1007 ">
dewdney et al (2001) <papid> W01-1007 </papid>show that non-lexical features, such as parts of speech and line-spacing, can be successfully used to classify genres, andferizis and bailey (2006) demonstrate that accurate classification of internet documents is possible even without the expensive part-of-speech tagging of similar methods.</citsent>
<aftsection>
<nextsent>recall that the noun ratio (f.7) was among the most effective of our features.
</nextsent>
<nextsent>finn and kushmerick (2006) describe study in which they classified documents from various domains as subjective?
</nextsent>
<nextsent>or objective.?
</nextsent>
<nextsent>they, too,found that part-of-speech statistics as well as general text statistics (e.g., average sentence length)are more effective than the traditional bag-of words representation when classifying documents from multiple domains.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5152">
<title id=" E09-1084.xml">using non lexical features to identify effective indexing terms for biomedical illustrations </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>this supports the notion that we can use non-lexical features to classify potential indexing terms in one biomedical subdo main using training data from another.
</prevsent>
<prevsent>maskey and hirschberg (2005) found that prosodic features (see ward, 2004) combined with structural features are sufficient to summarize spoken news broadcasts.
</prevsent>
</prevsection>
<citsent citstr=" P08-2052 ">
prosodic features relate tointonational variation and are associated with particularly important items, whereas structural features are associated with the organization of typical broadcast: headlines, followed by description of the stories, etc.finally, schilder and kondadadi (2008) <papid> P08-2052 </papid>describe non-lexical word-frequency features, similar to our ratio features (f.4f.7), which areused with regression svm to efficiently generate query-based multi-document summaries.</citsent>
<aftsection>
<nextsent>images convey essential information in biomedical publications.
</nextsent>
<nextsent>however, automatically extracting and selecting useful indexing terms from the article text is difficult task given the domain specific nature of biomedical images and vocabularies.
</nextsent>
<nextsent>in this work, we use the manual classification results of previous study to train binary classifier to automatically decide whether potential indexing term is useful for this purpose or not.
</nextsent>
<nextsent>we use non-lexical features generated for each term with the most effective including whether the term appears in the mesh terms assigned to the article and whether it is found in the articles title and caption.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5153">
<title id=" E09-2015.xml">grammar development in gf </title>
<section> the gf programming language.  </section>
<citcontext>
<prevsection>
<prevsent>the development of gf started in 1998 at xerox research centre europe in grenoble, within aproject entitled multilingual document author ing?
</prevsent>
<prevsent>(dymetman &amp; al. 2000).
</prevsent>
</prevsection>
<citsent citstr=" P98-2173 ">
its purpose was to make it productive to build controlled-language translators and multilingual authoring systems, previously produced by hard-coded grammar rules rather than declarative grammar formalisms (power &amp; scott 1998).<papid> P98-2173 </papid></citsent>
<aftsection>
<nextsent>later, mainly at chalmers university in gothenburg, gf developed into functional programming language inspired by mland haskell, with strict type system and operational semantics specified in (ranta 2004).
</nextsent>
<nextsent>amodule system was soon added (ranta 2007), inspired by the parametrized modules of ml and the class inheritance hierarchies of java, although with multiple inheritance in the style of c++.technically, gf falls within the class of so called curry-style categorial grammars, inspired by the distinction between tectogrammatical and phenogrammatical structure in (curry 1963).thus gf grammar has an abstract syntax defining system of types and trees (i.e. free algebra), and concrete syntax, which is homomorphic mapping from trees to strings and, more generally, to records of strings and features.
</nextsent>
<nextsent>to take simple example, the np-vp predication rule, written ::= np vp in context-free notation, becomes in gf pair of an abstract and concrete syntax rule, fun pred : np -  vp -  lin pred np vp = np ++ vpthe keyword fun stands for function declaration (declaring the function pred of type np -  vp -  s), whereas lin stands for linearization(saying that trees of form pred np vp are converted to strings where the linearization of np is followed by the linearization of vp).
</nextsent>
<nextsent>the arrow-  is the normal function type arrow of programming languages, and ++ is concatenation.patterns more complex than string concatenation can be used in linearizations of the same pred ication trees as the rule above.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5154">
<title id=" E09-2015.xml">grammar development in gf </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the unique features of gf are its type and module system, support for multilingual grammars, the large number of back-end formats, and the availability of libraries for 12 languages.
</prevsent>
<prevsent>regulus has resource 59 grammars for 7 languages, but they are smaller in scope.
</prevsent>
</prevsection>
<citsent citstr=" I05-2035 ">
in lkb, the lingo grammar matrix has been developed for several languages (bender and flickinger 2005), <papid> I05-2035 </papid>and in xle, the pargram grammar set (butt &amp; al. 2002).</citsent>
<aftsection>
<nextsent>lkb and xle tool shave been targeted to linguists working with largescale grammars, rather than for general programmers working with applications.
</nextsent>



</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5155">
<title id=" E12-1078.xml">to what extent does sentence internal realisation reflect discourse context a study on word order </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>since so many factors are involved and there is further interaction with subtle semantic and pragmatic differentiations, lexical choice, stylistic sand presumably processing factors, theoretical accounts making reliable predictions for real corpus examples have for long time proven elusive.
</prevsent>
<prevsent>as for german, only quite recently, number of corpus-based studies (filippova and strube, 2007; speyer, 2005; dipper and zinsmeister, 2009) have made some good progress towards coherence oriented account of at least the left edge of the german clause structure, the vorfeld constituent.
</prevsent>
</prevsection>
<citsent citstr=" W05-0311 ">
what makes the technological application of theoretical insights even harder is that for most relevant factors, automatic recognition cannot be performed with high accuracy (e.g., coreference accuracy in the 70s means there is good deal of noise) and for the higher-level notions such as the information-structural focus, inter annotator agreement on real corpus data tends to be much lower than for core-grammatical notions (poesio and artstein, 2005; <papid> W05-0311 </papid>ritz et al 2008).<papid> L08-1354 </papid>on the other hand, many of the relevant discourse factors are reflected indirectly in properties of the sentence-internal material.</citsent>
<aftsection>
<nextsent>most notably, knowing the shape of referring expressions narrows down many aspects of givenness and salience of its referent; pronominal realizations indicate givenness, and in german there are even two variants of the personal pronoun (er and der)for distinguishing salience.
</nextsent>
<nextsent>so, if the generation task is set in such way that the actual lexical choice, including functional categories such as determiners, is fully fixed (which is of course not always the case), one can take advantage of 767 these reflexes.
</nextsent>
<nextsent>this explains in part the fairly high baseline performance of n-gram language models in the surface realization task.
</nextsent>
<nextsent>and the effect can indeed be taken much further: the discriminative training experiments of cahill and riester (2009) <papid> P09-1092 </papid>show how effective it is to systematically take advantage of asymmetry patterns in the morphosyntactic reflexes of the discourse notion of information status (i.e., using feature set with well-chosen purely sentence-bound features).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5156">
<title id=" E12-1078.xml">to what extent does sentence internal realisation reflect discourse context a study on word order </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>since so many factors are involved and there is further interaction with subtle semantic and pragmatic differentiations, lexical choice, stylistic sand presumably processing factors, theoretical accounts making reliable predictions for real corpus examples have for long time proven elusive.
</prevsent>
<prevsent>as for german, only quite recently, number of corpus-based studies (filippova and strube, 2007; speyer, 2005; dipper and zinsmeister, 2009) have made some good progress towards coherence oriented account of at least the left edge of the german clause structure, the vorfeld constituent.
</prevsent>
</prevsection>
<citsent citstr=" L08-1354 ">
what makes the technological application of theoretical insights even harder is that for most relevant factors, automatic recognition cannot be performed with high accuracy (e.g., coreference accuracy in the 70s means there is good deal of noise) and for the higher-level notions such as the information-structural focus, inter annotator agreement on real corpus data tends to be much lower than for core-grammatical notions (poesio and artstein, 2005; <papid> W05-0311 </papid>ritz et al 2008).<papid> L08-1354 </papid>on the other hand, many of the relevant discourse factors are reflected indirectly in properties of the sentence-internal material.</citsent>
<aftsection>
<nextsent>most notably, knowing the shape of referring expressions narrows down many aspects of givenness and salience of its referent; pronominal realizations indicate givenness, and in german there are even two variants of the personal pronoun (er and der)for distinguishing salience.
</nextsent>
<nextsent>so, if the generation task is set in such way that the actual lexical choice, including functional categories such as determiners, is fully fixed (which is of course not always the case), one can take advantage of 767 these reflexes.
</nextsent>
<nextsent>this explains in part the fairly high baseline performance of n-gram language models in the surface realization task.
</nextsent>
<nextsent>and the effect can indeed be taken much further: the discriminative training experiments of cahill and riester (2009) <papid> P09-1092 </papid>show how effective it is to systematically take advantage of asymmetry patterns in the morphosyntactic reflexes of the discourse notion of information status (i.e., using feature set with well-chosen purely sentence-bound features).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5157">
<title id=" E12-1078.xml">to what extent does sentence internal realisation reflect discourse context a study on word order </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>so, if the generation task is set in such way that the actual lexical choice, including functional categories such as determiners, is fully fixed (which is of course not always the case), one can take advantage of 767 these reflexes.
</prevsent>
<prevsent>this explains in part the fairly high baseline performance of n-gram language models in the surface realization task.
</prevsent>
</prevsection>
<citsent citstr=" P09-1092 ">
and the effect can indeed be taken much further: the discriminative training experiments of cahill and riester (2009) <papid> P09-1092 </papid>show how effective it is to systematically take advantage of asymmetry patterns in the morphosyntactic reflexes of the discourse notion of information status (i.e., using feature set with well-chosen purely sentence-bound features).</citsent>
<aftsection>
<nextsent>these observations give rise to the question: inthe light of the difficulty in obtaining reliable discourse information on the one hand and the effectiveness of exploiting the reflexes of discourse in the sentence-internal material on the other ? can we nevertheless expect to gain something from adding sentence-external feature information?
</nextsent>
<nextsent>we propose two scenarios for adressing thisquestion: first, we choose an approximative access to context information and relations between discourse referents ? lexical reiteration of headwords, combined with information about their grammatical relation and topological positioning in prior sentences.
</nextsent>
<nextsent>we apply these features in rich sentence-internal surface realisation ranking model for german.
</nextsent>
<nextsent>secondly, we choose more controlled scenario: we train constituent ordering classifier based on feature model that captures properties of discourse referents in terms of manually annotated coreference relations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5160">
<title id=" E12-1078.xml">to what extent does sentence internal realisation reflect discourse context a study on word order </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>as weget the same effect in both setups ? the sentence external features do not improve over baseline that captures basic morphosyntactic properties ofthe constituents ? we conclude that sentence internal realisation is actually relatively accurate predictor of discourse context, even more accurate than information that can be obtained from coreference and lexical chain relations.
</prevsent>
<prevsent>in the generation literature, most works on exploiting sentence-external discourse information are set in summarisation or content ordering framework.
</prevsent>
</prevsection>
<citsent citstr=" N04-1015 ">
barzilay and lee (2004) <papid> N04-1015 </papid>propose an account for constraints on topic selection based on probabilistic content models.</citsent>
<aftsection>
<nextsent>barzilay and lapata(2008) <papid> J08-1001 </papid>propose an entity grid model which represents the distribution of referents in discourse for sentence ordering.</nextsent>
<nextsent>karamanis et al(2009) <papid> J09-1003 </papid>use centering-based metrics to assess coherence in an information ordering system.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5161">
<title id=" E12-1078.xml">to what extent does sentence internal realisation reflect discourse context a study on word order </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in the generation literature, most works on exploiting sentence-external discourse information are set in summarisation or content ordering framework.
</prevsent>
<prevsent>barzilay and lee (2004) <papid> N04-1015 </papid>propose an account for constraints on topic selection based on probabilistic content models.</prevsent>
</prevsection>
<citsent citstr=" J08-1001 ">
barzilay and lapata(2008) <papid> J08-1001 </papid>propose an entity grid model which represents the distribution of referents in discourse for sentence ordering.</citsent>
<aftsection>
<nextsent>karamanis et al(2009) <papid> J09-1003 </papid>use centering-based metrics to assess coherence in an information ordering system.</nextsent>
<nextsent>clarke and lapata (2010) <papid> J10-3005 </papid>have improved sentence compression system by capturing prominence of phrases or referents in terms of lexical chain information inspired by morris and hirst (1991) <papid> J91-1002 </papid>and centering (grosz et al 1995).<papid> J95-2003 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5162">
<title id=" E12-1078.xml">to what extent does sentence internal realisation reflect discourse context a study on word order </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>barzilay and lee (2004) <papid> N04-1015 </papid>propose an account for constraints on topic selection based on probabilistic content models.</prevsent>
<prevsent>barzilay and lapata(2008) <papid> J08-1001 </papid>propose an entity grid model which represents the distribution of referents in discourse for sentence ordering.</prevsent>
</prevsection>
<citsent citstr=" J09-1003 ">
karamanis et al(2009) <papid> J09-1003 </papid>use centering-based metrics to assess coherence in an information ordering system.</citsent>
<aftsection>
<nextsent>clarke and lapata (2010) <papid> J10-3005 </papid>have improved sentence compression system by capturing prominence of phrases or referents in terms of lexical chain information inspired by morris and hirst (1991) <papid> J91-1002 </papid>and centering (grosz et al 1995).<papid> J95-2003 </papid></nextsent>
<nextsent>in their system, discourse context is represented in terms of hard constraints modelling whether certain constituent can be deleted or not.in the linear isation or surface realisation domain, there is considerable body of work approximating information structure in terms of sentence-internal realisation (ringger et al 2004; <papid> C04-1097 </papid>filippova and strube, 2009; <papid> N09-2057 </papid>velldal and oepen, 2005; cahill et al 2007).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5163">
<title id=" E12-1078.xml">to what extent does sentence internal realisation reflect discourse context a study on word order </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>barzilay and lapata(2008) <papid> J08-1001 </papid>propose an entity grid model which represents the distribution of referents in discourse for sentence ordering.</prevsent>
<prevsent>karamanis et al(2009) <papid> J09-1003 </papid>use centering-based metrics to assess coherence in an information ordering system.</prevsent>
</prevsection>
<citsent citstr=" J10-3005 ">
clarke and lapata (2010) <papid> J10-3005 </papid>have improved sentence compression system by capturing prominence of phrases or referents in terms of lexical chain information inspired by morris and hirst (1991) <papid> J91-1002 </papid>and centering (grosz et al 1995).<papid> J95-2003 </papid></citsent>
<aftsection>
<nextsent>in their system, discourse context is represented in terms of hard constraints modelling whether certain constituent can be deleted or not.in the linear isation or surface realisation domain, there is considerable body of work approximating information structure in terms of sentence-internal realisation (ringger et al 2004; <papid> C04-1097 </papid>filippova and strube, 2009; <papid> N09-2057 </papid>velldal and oepen, 2005; cahill et al 2007).</nextsent>
<nextsent>cahill and riester (2009) <papid> P09-1092 </papid>improve realisation ranking for german ? which mainly deals with word order variation ? by representing precedence patterns of constituent sin terms of asymmetries in their morphosyntactic properties.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5164">
<title id=" E12-1078.xml">to what extent does sentence internal realisation reflect discourse context a study on word order </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>barzilay and lapata(2008) <papid> J08-1001 </papid>propose an entity grid model which represents the distribution of referents in discourse for sentence ordering.</prevsent>
<prevsent>karamanis et al(2009) <papid> J09-1003 </papid>use centering-based metrics to assess coherence in an information ordering system.</prevsent>
</prevsection>
<citsent citstr=" J91-1002 ">
clarke and lapata (2010) <papid> J10-3005 </papid>have improved sentence compression system by capturing prominence of phrases or referents in terms of lexical chain information inspired by morris and hirst (1991) <papid> J91-1002 </papid>and centering (grosz et al 1995).<papid> J95-2003 </papid></citsent>
<aftsection>
<nextsent>in their system, discourse context is represented in terms of hard constraints modelling whether certain constituent can be deleted or not.in the linear isation or surface realisation domain, there is considerable body of work approximating information structure in terms of sentence-internal realisation (ringger et al 2004; <papid> C04-1097 </papid>filippova and strube, 2009; <papid> N09-2057 </papid>velldal and oepen, 2005; cahill et al 2007).</nextsent>
<nextsent>cahill and riester (2009) <papid> P09-1092 </papid>improve realisation ranking for german ? which mainly deals with word order variation ? by representing precedence patterns of constituent sin terms of asymmetries in their morphosyntactic properties.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5165">
<title id=" E12-1078.xml">to what extent does sentence internal realisation reflect discourse context a study on word order </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>barzilay and lapata(2008) <papid> J08-1001 </papid>propose an entity grid model which represents the distribution of referents in discourse for sentence ordering.</prevsent>
<prevsent>karamanis et al(2009) <papid> J09-1003 </papid>use centering-based metrics to assess coherence in an information ordering system.</prevsent>
</prevsection>
<citsent citstr=" J95-2003 ">
clarke and lapata (2010) <papid> J10-3005 </papid>have improved sentence compression system by capturing prominence of phrases or referents in terms of lexical chain information inspired by morris and hirst (1991) <papid> J91-1002 </papid>and centering (grosz et al 1995).<papid> J95-2003 </papid></citsent>
<aftsection>
<nextsent>in their system, discourse context is represented in terms of hard constraints modelling whether certain constituent can be deleted or not.in the linear isation or surface realisation domain, there is considerable body of work approximating information structure in terms of sentence-internal realisation (ringger et al 2004; <papid> C04-1097 </papid>filippova and strube, 2009; <papid> N09-2057 </papid>velldal and oepen, 2005; cahill et al 2007).</nextsent>
<nextsent>cahill and riester (2009) <papid> P09-1092 </papid>improve realisation ranking for german ? which mainly deals with word order variation ? by representing precedence patterns of constituent sin terms of asymmetries in their morphosyntactic properties.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5167">
<title id=" E12-1078.xml">to what extent does sentence internal realisation reflect discourse context a study on word order </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>karamanis et al(2009) <papid> J09-1003 </papid>use centering-based metrics to assess coherence in an information ordering system.</prevsent>
<prevsent>clarke and lapata (2010) <papid> J10-3005 </papid>have improved sentence compression system by capturing prominence of phrases or referents in terms of lexical chain information inspired by morris and hirst (1991) <papid> J91-1002 </papid>and centering (grosz et al 1995).<papid> J95-2003 </papid></prevsent>
</prevsection>
<citsent citstr=" C04-1097 ">
in their system, discourse context is represented in terms of hard constraints modelling whether certain constituent can be deleted or not.in the linear isation or surface realisation domain, there is considerable body of work approximating information structure in terms of sentence-internal realisation (ringger et al 2004; <papid> C04-1097 </papid>filippova and strube, 2009; <papid> N09-2057 </papid>velldal and oepen, 2005; cahill et al 2007).</citsent>
<aftsection>
<nextsent>cahill and riester (2009) <papid> P09-1092 </papid>improve realisation ranking for german ? which mainly deals with word order variation ? by representing precedence patterns of constituent sin terms of asymmetries in their morphosyntactic properties.</nextsent>
<nextsent>as simple example, pattern exploited by cahill and riester (2009) <papid> P09-1092 </papid>is the tendency of definite elements tend to precede indef inites, which, on discourse level, reflects that given entities in sentence tend to precede new entities.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5168">
<title id=" E12-1078.xml">to what extent does sentence internal realisation reflect discourse context a study on word order </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>karamanis et al(2009) <papid> J09-1003 </papid>use centering-based metrics to assess coherence in an information ordering system.</prevsent>
<prevsent>clarke and lapata (2010) <papid> J10-3005 </papid>have improved sentence compression system by capturing prominence of phrases or referents in terms of lexical chain information inspired by morris and hirst (1991) <papid> J91-1002 </papid>and centering (grosz et al 1995).<papid> J95-2003 </papid></prevsent>
</prevsection>
<citsent citstr=" N09-2057 ">
in their system, discourse context is represented in terms of hard constraints modelling whether certain constituent can be deleted or not.in the linear isation or surface realisation domain, there is considerable body of work approximating information structure in terms of sentence-internal realisation (ringger et al 2004; <papid> C04-1097 </papid>filippova and strube, 2009; <papid> N09-2057 </papid>velldal and oepen, 2005; cahill et al 2007).</citsent>
<aftsection>
<nextsent>cahill and riester (2009) <papid> P09-1092 </papid>improve realisation ranking for german ? which mainly deals with word order variation ? by representing precedence patterns of constituent sin terms of asymmetries in their morphosyntactic properties.</nextsent>
<nextsent>as simple example, pattern exploited by cahill and riester (2009) <papid> P09-1092 </papid>is the tendency of definite elements tend to precede indef inites, which, on discourse level, reflects that given entities in sentence tend to precede new entities.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5175">
<title id=" E12-1078.xml">to what extent does sentence internal realisation reflect discourse context a study on word order </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>other work on german surface realisation has highlighted the role of the initial position in the german sentence, the so-called vorfeld (or pre field?).
</prevsent>
<prevsent>filippova and strube (2007) show that once the vorfeld (i.e. the constituent that precedes the finite verb) is correctly determined, the prediction of the order in the mittel feld (i.e. the constituents that follow the finite verb) is very easy.
</prevsent>
</prevsection>
<citsent citstr=" P10-1020 ">
cheung and penn (2010) <papid> P10-1020 </papid>extend the approach of filippova and strube (2007) and augment sentence-internal constituent ordering model withsentence-external features inspired from the entity grid model proposed by barzilay and lapata (2008).<papid> J08-1001 </papid></citsent>
<aftsection>
<nextsent>while there would be many ways to construe or represent discourse context (e.g. in terms of the global discourse or information structure), we concentrate on capturing local coherence through the distribution of discourse referents in text.
</nextsent>
<nextsent>these discourse referents basically correspond to the constituents that our surface realisation model has to put in the right order.
</nextsent>
<nextsent>as the order of referents or constituents is arguably influenced by the information structure of sentence given the previous text, our main assumption was that infor 768 (1) a. kurze zeit spater erklarte ein anrufer bei nachrichtenagenturen in pakistan , die gruppe gamaa bekenne sich.
</nextsent>
<nextsent>shortly after, caller declared at the news agencies in pakistan, that the group gamaa avowes itself.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5179">
<title id=" E12-1078.xml">to what extent does sentence internal realisation reflect discourse context a study on word order </title>
<section> motivation.  </section>
<citcontext>
<prevsection>
<prevsent>and the waythe centers are arranged in sequence of utterances to make this sequence coherent discourse.another important concept is the ranking?
</prevsent>
<prevsent>of discourse referents which basically determines the prominence of referent in certain sentence andis driven by several factors (e.g. their grammatical function).
</prevsent>
</prevsection>
<citsent citstr=" J04-3003 ">
for free word order languages like german, word order has been proposed as one of the factors that account for the ranking (poesio et al., 2004).<papid> J04-3003 </papid></citsent>
<aftsection>
<nextsent>in similar spirit, morris and hirst(1991) <papid> J91-1002 </papid>have proposed that chains of (related) lexical items in text are an important indicator of text structure.</nextsent>
<nextsent>our main hypothesis was that it is possible to exploit these intuitions from centering theory and the idea of lexical chains for word order pre diction.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5183">
<title id=" E12-1078.xml">to what extent does sentence internal realisation reflect discourse context a study on word order </title>
<section> experiment 2: constituent ordering.  </section>
<citcontext>
<prevsection>
<prevsent>the classifier is implemented with svmrank again.
</prevsent>
<prevsent>in contrast to the previous experiment where we learned to rank sentences, the classifier now learns to rank constituents.
</prevsent>
</prevsection>
<citsent citstr=" W10-1834 ">
the constituents have been extracted using the tool described in bouma (2010).<papid> W10-1834 </papid></citsent>
<aftsection>
<nextsent>the final dataset comprises 48.513 candidate sets of freely order able constituents.
</nextsent>
<nextsent>5.2 centering-inspired feature model.
</nextsent>
<nextsent>to compare the discourse context model against sentence-based model, we implemented number of sentence-internal features that are very similar to the features used in the previous experiment.since we extract them from the syntactic annotation instead of f-structures, some labels and feature names will be different, however, the design of the sentence-internal model is identical to the previous one in section 4.
</nextsent>
<nextsent>the sentence-external features differ in some aspects from section 4, since we extract coreference relations of several types (see (naumann, 2006) for the anaphoric relations annotated in the tueba-d/z).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5185">
<title id=" E12-1078.xml">to what extent does sentence internal realisation reflect discourse context a study on word order </title>
<section> conclusion.  </section>
<citcontext>
<prevsection>
<prevsent>this suggests that sentence internal realisation implicitly carries lot of im formation about discourse context.
</prevsent>
<prevsent>on average, the morphosyntactic properties of constituents in text are better approximates of their discourse status than actual coreference relations.
</prevsent>
</prevsection>
<citsent citstr=" E06-1040 ">
this result feeds into number of research questions concerning the representation of discourse and its application in generation systems.although we should certainly not expect computational model to achieve perfect accuracy in the constituent ordering task ? even humans only agree to certain extent in rating word order variants (belz and reiter, 2006; <papid> E06-1040 </papid>cahill, 2009) ? the average accuracy in the 60s for prediction of vorfeld occupance is still moderate.</citsent>
<aftsection>
<nextsent>an obvious direction would be to further investigate more complex representations of discourse that take into account the relations between utterances, such as topic shifts.
</nextsent>
<nextsent>moreover, it is not clear whether the effects we find for linear isation in this paper carryover to other levels of generation such as tactical generation where syntactic functions are not fully specified.
</nextsent>
<nextsent>in broader perspective, our results underline the need for better formalisations of discourse that can be translated into features for large-scale applications such as generation.
</nextsent>
<nextsent>acknowledgments this work was funded by the collaborative research centre (sfb 732) at the university of stuttgart.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="I5186">
<title id=" E06-2001.xml">large linguistically processed web corpora for multiple languages </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>wehave now done this for german and italian, with corpus sizes of over 1 billion words in each case.
</prevsent>
<prevsent>we provide web access to the corpora in our query tool, the sketch engine.
</prevsent>
</prevsection>
<citsent citstr=" J03-3001 ">
the web contains vast amounts of linguistic data for many languages (kilgarriff and grefenstette, 2003).<papid> J03-3001 </papid></citsent>
<aftsection>
<nextsent>one key issue for linguists and language technologists is how to access it.
</nextsent>
<nextsent>the drawbacks of using commercial search engines are presented in kilgarriff (2003).
</nextsent>
<nextsent>an alternative is to crawl theweb ourselves.1 we have done this for two languages, german and italian, and here we report on the pipeline of processes which give us reasonably well-behaved, clean?
</nextsent>
<nextsent>corpora for each language.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
</paper>