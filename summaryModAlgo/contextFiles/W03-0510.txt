we have used rouge-1 (only unigrams with lemmatization and stop word removal), which gives good results with standard summaries (lin and hovy, 2003a). 
it is also notable the study reported in (lin and hovy, 2003b) discussing the usefulness and limitations of automatic sentence extraction for text summarization, 23 single document metaù summarization algorithm summarization algo. 
unigram co-occurrence metric in a recent study (lin and hovy, 2003a), we showed that the recall-based unigram cooccurrence automatic scoring metric correlates highly with human evaluation and has high recall and precision in predicting the statistical significance of results comparing with its human counterpart. 
for evaluation, we are using the rouge evaluation toolkit1, which is a method based on ngram statistics, found to be highly correlated with human evaluations (lin and hovy, 2003a). 
it is also notable the study reported in (lin and hovy, 2003b) discussing the usefulness and limitations of automatic sentence extraction for summarization, which emphasizes the need of accurate tools for sentence extraction, as an integral part of automatic summarization systems. 
meta-evaluation of similarity metrics the question of how to know which similarity metric is best to evaluate automatic summaries/translations has been addressed by ‚Ä¢ comparing the quality of automatic items with the quality of manual references (culy and riehemann, 2003; lin and hovy, 2003b). 
let us imagine, for instance, that the best metric turns out to be a rouge (lin and hovy, 2003a) variant that only considers unigrams to compute similarity. 
these findings are additionally supported by the fact that automatic n-gram-based evaluation measures now being used to assess predominately extractive multi-document summarization systems correlate strongly with human judgments when restricted to the usage of unigrams and bigrams, but correlate weakly when longer n-grams are factored into the equation (lin & hovy, 2003). 
bleu (papineni et al, 2001) and rouge (lin and hovy, 2003a) are the standard similarity metrics used in machine translation and text summarisation. 
another recent study (lin and hovy, 2003) investigated the extent to which extractive methods may be sufficient for summarization in the single-document case. 
for evaluation, we are using the rouge evaluation toolkit, which is a method based on ngram statistics, found to be highly correlated with human evaluations (lin and hovy, 2003a). 
another notable development in the field is the uni-gram co-occurrence matching technique as proposed in (lin and hovy, 2003a). 
the issue of subjectivity gains prominence as the compression ratio increases, i.e., the shorter the summary, the larger the number ofcorrect summaries (lin and hovy, 2003b). 
we provided more in-depth discussion of this issue in other papers (lin and hovy, 2002; lin and hovy 2003b). 
