9	9	
9	19	0.0666068175770346
9	16	0.0286614995418983
9	5	0.0430215395921446
9	11	0.285759553580853
9	14	0.109059384267614
9	3	0.0577006462256531
9	10	0.0152644746143246
9	18	0.00015191485985734
9	1	0.264731953100866
9	8	0.0170062477274773
9	17	0.000283063882649032
9	7	0.0303394656719455
9	15	0.0289154555326477
9	6	2.34759494374804e-05
9	12	0.0585318656537989
9	2	0.0007044901473891
9	4	0.0284200158162479
9	13	0.0649239115086578
19	9	0.0666068175770346
19	19	
19	16	0.0481627476534128
19	5	0.0181131980351943
19	11	0.0717001790882118
19	14	0.000117528251143474
19	3	0.000131000731269064
19	10	0.000289304692885749
19	18	0.106723909524702
19	1	0.000117901520361708
19	8	0.000110217120845187
19	17	0.00116095880762229
19	7	0
19	15	0
19	6	0
19	12	0.024461060765955
19	2	0
19	4	0.0297417667037625
19	13	0.0178360277157965
16	9	0.0286614995418983
16	19	0.0481627476534128
16	16	
16	5	0.0205017976348887
16	11	7.11397630309357e-05
16	14	0.02650101296861
16	3	0.0296007873559409
16	10	0.000167965034667143
16	18	5.69622328816031e-05
16	1	0.0197809610413519
16	8	8.10881480224925e-05
16	17	2.7450244240417e-05
16	7	0.000124981392676995
16	15	6.98888933129736e-05
16	6	0
16	12	0.0684436515062418
16	2	0.0693318553837666
16	4	0.000112985648826443
16	13	0.0217201545977047
5	9	0.0430215395921446
5	19	0.0181131980351943
5	16	0.0205017976348887
5	5	
5	11	0.0628205890429624
5	14	0.299934345578781
5	3	0.0858465657602129
5	10	0.0450201675067605
5	18	7.9770263133784e-05
5	1	0.000405542443103671
5	8	0.0189604051270005
5	17	0.000287418958470877
5	7	0.0772239794689864
5	15	0.0237340005946785
5	6	1.6834628922649e-05
5	12	0.0338171646770439
5	2	0.0010321329602762
5	4	0.04738856203098
5	13	0.0158472238219865
11	9	0.285759553580853
11	19	0.0717001790882118
11	16	7.11397630309357e-05
11	5	0.0628205890429624
11	11	
11	14	6.79122841867039e-07
11	3	0.16211881131743
11	10	0.176647803471372
11	18	6.43326122377598e-05
11	1	0.305148168363678
11	8	0.163771446984364
11	17	0.0893312206472566
11	7	0.0476665084383302
11	15	0.00028366402643531
11	6	0
11	12	0.0389408315308731
11	2	0.082838815685707
11	4	0.114251030087638
11	13	0.175966439782856
14	9	0.109059384267614
14	19	0.000117528251143474
14	16	0.02650101296861
14	5	0.299934345578781
14	11	6.79122841867039e-07
14	14	
14	3	0.0529421511129069
14	10	0.142676978522688
14	18	0
14	1	0.0154971592591309
14	8	7.0205334693818e-05
14	17	0.131671808699585
14	7	2.34869543722932e-06
14	15	0.154615587790109
14	6	0.0487268028333707
14	12	0.042558358811789
14	2	3.78194239130224e-07
14	4	0.334800341873106
14	13	0.137478020933944
3	9	0.0577006462256531
3	19	0.000131000731269064
3	16	0.0296007873559409
3	5	0.0858465657602129
3	11	0.16211881131743
3	14	0.0529421511129069
3	3	
3	10	0.156440614349472
3	18	0.000368120696269578
3	1	0.01682816451346
3	8	0.288034357150827
3	17	0.0126696712547078
3	7	0.04242131466187
3	15	0.000206590708677237
3	6	6.56798487417397e-05
3	12	0.251359647737494
3	2	0.000105118379510775
3	4	0.146451712242331
3	13	0.149694444943659
10	9	0.0152644746143246
10	19	0.000289304692885749
10	16	0.000167965034667143
10	5	0.0450201675067605
10	11	0.176647803471372
10	14	0.142676978522688
10	3	0.156440614349472
10	10	
10	18	8.37715347223932e-05
10	1	0.000558606090830388
10	8	0.308453357946503
10	17	0.191677861792984
10	7	0.03107396349841
10	15	0.129068932990021
10	6	0
10	12	0.017464652858491
10	2	0.0276292017615592
10	4	0.31029774129571
10	13	0.551710365145282
18	9	0.00015191485985734
18	19	0.106723909524702
18	16	5.69622328816031e-05
18	5	7.9770263133784e-05
18	11	6.43326122377598e-05
18	14	0
18	3	0.000368120696269578
18	10	8.37715347223932e-05
18	18	
18	1	0.000102419264702743
18	8	9.5743943250147e-05
18	17	0.000105851318937687
18	7	0.482011429840622
18	15	0.103123763179183
18	6	0.000152231159864705
18	12	4.57392733677422e-05
18	2	0.000133804594206972
18	4	4.3589640695412e-05
18	13	0.000128475304645289
1	9	0.264731953100866
1	19	0.000117901520361708
1	16	0.0197809610413519
1	5	0.000405542443103671
1	11	0.305148168363678
1	14	0.0154971592591309
1	3	0.01682816451346
1	10	0.000558606090830388
1	18	0.000102419264702743
1	1	
1	8	0.000144277268996598
1	17	0.0116012570726189
1	7	0.000217650646449967
1	15	0.000406981091757558
1	6	0
1	12	0.0505872901789086
1	2	0.000515878171103308
1	4	0.000200380842194578
1	13	0.0452721692785166
8	9	0.0170062477274773
8	19	0.000110217120845187
8	16	8.10881480224925e-05
8	5	0.0189604051270005
8	11	0.163771446984364
8	14	7.0205334693818e-05
8	3	0.288034357150827
8	10	0.308453357946503
8	18	9.5743943250147e-05
8	1	0.000144277268996598
8	8	
8	17	0.225299916813863
8	7	0.0353878661061793
8	15	0.00113465275036776
8	6	0
8	12	0.0714952636539027
8	2	0.121179066937796
8	4	0.0724343106070034
8	13	0.47590638435957
17	9	0.000283063882649032
17	19	0.00116095880762229
17	16	2.7450244240417e-05
17	5	0.000287418958470877
17	11	0.0893312206472566
17	14	0.131671808699585
17	3	0.0126696712547078
17	10	0.191677861792984
17	18	0.000105851318937687
17	1	0.0116012570726189
17	8	0.225299916813863
17	17	
17	7	0.000193836163579712
17	15	0.21974315934575
17	6	2.23387463609134e-05
17	12	0.163995872223681
17	2	0.0976555774220224
17	4	0.225198991211528
17	13	0.313803721902846
7	9	0.0303394656719455
7	19	0
7	16	0.000124981392676995
7	5	0.0772239794689864
7	11	0.0476665084383302
7	14	2.34869543722932e-06
7	3	0.04242131466187
7	10	0.03107396349841
7	18	0.482011429840622
7	1	0.000217650646449967
7	8	0.0353878661061793
7	17	0.000193836163579712
7	7	
7	15	0.173808843022513
7	6	0.000128412034963729
7	12	0.0337727526558971
7	2	0.000206343300673353
7	4	0.032410973647361
7	13	0.0238867433133005
15	9	0.0289154555326477
15	19	0
15	16	6.98888933129736e-05
15	5	0.0237340005946785
15	11	0.00028366402643531
15	14	0.154615587790109
15	3	0.000206590708677237
15	10	0.129068932990021
15	18	0.103123763179183
15	1	0.000406981091757558
15	8	0.00113465275036776
15	17	0.21974315934575
15	7	0.173808843022513
15	15	
15	6	5.24628600245658e-05
15	12	0.0589105763811306
15	2	0.0965880345479521
15	4	0.265442560167217
15	13	0.189126134567342
6	9	2.34759494374804e-05
6	19	0
6	16	0
6	5	1.6834628922649e-05
6	11	0
6	14	0.0487268028333707
6	3	6.56798487417397e-05
6	10	0
6	18	0.000152231159864705
6	1	0
6	8	0
6	17	2.23387463609134e-05
6	7	0.000128412034963729
6	15	5.24628600245658e-05
6	6	
6	12	0
6	2	2.06773049789552e-05
6	4	0.0416079845406839
6	13	0
12	9	0.0585318656537989
12	19	0.024461060765955
12	16	0.0684436515062418
12	5	0.0338171646770439
12	11	0.0389408315308731
12	14	0.042558358811789
12	3	0.251359647737494
12	10	0.017464652858491
12	18	4.57392733677422e-05
12	1	0.0505872901789086
12	8	0.0714952636539027
12	17	0.163995872223681
12	7	0.0337727526558971
12	15	0.0589105763811306
12	6	0
12	12	
12	2	0.046424690468531
12	4	0.0650850331871923
12	13	0.174251408618978
2	9	0.0007044901473891
2	19	0
2	16	0.0693318553837666
2	5	0.0010321329602762
2	11	0.082838815685707
2	14	3.78194239130224e-07
2	3	0.000105118379510775
2	10	0.0276292017615592
2	18	0.000133804594206972
2	1	0.000515878171103308
2	8	0.121179066937796
2	17	0.0976555774220224
2	7	0.000206343300673353
2	15	0.0965880345479521
2	6	2.06773049789552e-05
2	12	0.046424690468531
2	2	
2	4	0.0927483272330113
2	13	0.0827575278157011
4	9	0.0284200158162479
4	19	0.0297417667037625
4	16	0.000112985648826443
4	5	0.04738856203098
4	11	0.114251030087638
4	14	0.334800341873106
4	3	0.146451712242331
4	10	0.31029774129571
4	18	4.3589640695412e-05
4	1	0.000200380842194578
4	8	0.0724343106070034
4	17	0.225198991211528
4	7	0.032410973647361
4	15	0.265442560167217
4	6	0.0416079845406839
4	12	0.0650850331871923
4	2	0.0927483272330113
4	4	
4	13	0.236707353380686
13	9	0.0649239115086578
13	19	0.0178360277157965
13	16	0.0217201545977047
13	5	0.0158472238219865
13	11	0.175966439782856
13	14	0.137478020933944
13	3	0.149694444943659
13	10	0.551710365145282
13	18	0.000128475304645289
13	1	0.0452721692785166
13	8	0.47590638435957
13	17	0.313803721902846
13	7	0.0238867433133005
13	15	0.189126134567342
13	6	0
13	12	0.174251408618978
13	2	0.0827575278157011
13	4	0.236707353380686
13	13	
9	4 evaluation to evaluate our learning approach, we trained aquarea$ on the same development set of stories and tested it on the same test set of stories as those used in all past work on the reading comprehension task . 
19	we also compared our results across various interrogatives with those previously reported in . 
16	refer to the readme le of minipar downloaded from http://www.cs.ualberta.ca/ lindek/minipar.htm 5 experimental results we selected the features used in quarc to establish the reference performance level. 
5	as such, all subsequent work uses humsent as the main scoring metric, and it is also the scoring metric that we adopted in this paper. 
11	naturally, our current work on question answering for the reading comprehension task is most related to those of . 
14	the words used in are "happen", "take place" "this", "story". 
3	in prior work the number and type of information sources used for computation is specific to and rlifferent for each question type. 
10	keywords in questions it has been observed in the work of that certain words in a when or where question tend to indicate that the dateline is an ~n~wer sentence to the question. 
18	then and reported improvements to 39.7% and 41%, respectively. 
1	in addition, a number of researchers have built systems to take reading comprehension examinations designed to evaluate childrenâ€™s reading levels . 
8	it has been observed in prior work that such sentences may be more likely to be the answer sentences to some question type . 
17	as was observed by and , the correct answer sentence often precedes/follows the sentence with the highest number of matching words. 
7	subsequently, the work of and improved the accuracy further to 39.7% and 41%, respectively. 
15	based on these technologies, riloff and thelen improved the humsent accuracy to 40% by applying a set of heuristic rules that assign handcrafted weights to matching words and ne. 
6	et al, 1999), , , and are 36.3%, 
12	3.1 feature representation our feature representation was designed to capture the information sources that prior work used in their computations or rules. 
2	quarc utilizes manually generated rules that selects a sentence deemed to contain the answer based on a combination of syntactic similarity and semantic correspondence . 
4	it is interesting to note that the words automatically determined by out procedure are also part of those words found manually in the prior work of . 
13	we designed these 4 features to capture information that will be helpful to the why questions, since it has been observed in prior work that the answer 127 sentence to a why question tends to follow the sentence in the story that has the most number of word matches with the question. 
