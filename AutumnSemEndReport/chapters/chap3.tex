\chapter{Experiments}
\section{Data-set}
To conduct our experiments, we chose the data-set which was used by the authors of C-LexRank algorithm. The data-set consists of 
\begin{itemize}
\item citations to 25 highly cited papers from 5 different domains: Text Summarization, Question Answering, Machien Translation, Textual Entailment, and Dependency Parsing.
\item Each dataset has a \textquotedblleft *.txt\textquotedblright file that has 1 citation per line, and a \textquotedblleft*.ann\textquotedblright file that has lines of the following format: \{ fact id \} \{ tab \} \{ nugget \}
\item To detect which nuggets/facts a citation contains, one should perform basic string matching.
\item The data-set is available on \href{http://www-personal.umich.edu/~vahed/resources/single.tar.gz}{this} link.
\end{itemize}
Further in our analysis, we needed the complete data-set for computations of graph based features which is available at \href{http://clair.eecs.umich.edu/aan/index.php}{The Association Of Computational Linguistics Anthology Network
}
\section{Preparing a baseline}
To evalulate the experiments, we prepare a set of baseline methods for comparison. 
\begin{itemize}
\item We first prepare a random summary of approximately 100 words and calculate the evaluation score.
\item We consider the state-of-the-art C-LexRank as a baseline for this study and prepare a summary for the same.
\end{itemize}
\section{Pre-processing}
We perform a couple of pre-processing steps to sanitize the input sentences in the citation context for efficient computation of similarities. 
\begin{itemize}
\item The reference to the name of authors and the year of the published work is removed for the computation of lexical features. This ensures that these blocks of text do not add noise to the values. An example of such a reference is :

\textit{examples of using nlp and ie in question answering include shallow parsing} \texttt{[kupiec 1993] [srihari \& li 2000]},\textit{ deep parsing}  \texttt{[li et al 2002] [litkowski 1999] [voorhees 1999]}, \textit{and ie} \texttt{[abney et al 2000] [srihari \& li 2000]}. 

The sentence after the references were removed was :

\textit{examples of using nlp and ie in question answering include shallow parsing , deep parsing , and ie}. 

\item We performed stemming on the words of the citation context and compared the evaluated results coming to a conclusion that stemming was not increasing the efficiency of the task making us remove it.

\item The next level of pre-processing was the removal of stop-words. Stop words are natural language words which have very little meaning, such as \textquotedblleft and \textquotedblright, \textquotedblleft the \textquotedblright, \textquotedblleft a \textquotedblright, \textquotedblleft and \textquotedblright, and similar words. The removal of stopwords had a positive effect on the quality of summaries as expected.

\end{itemize}

\begin{table}[]
\centering
\label{my-label}
\begin{tabular}{l|l}
\hline
\textbf{Description} & \textbf{Pyramid score} \\ \hline
Random summary       & 0.4762367241         \\ \hline
C-LexRank            & 0.64386657           \\ \hline
\end{tabular}
\caption{Preparing a baseline on 100 words summaries}

\end{table}


\section{Computing Features}
The similarity metrics between all pairs of sentences in a citation context are of critical importance to the performance of the algorithm. Considering this fact, we first started conducting experiments on exploring new features to improve the method of accurately capturing the similarity between a set of sentences belonging to the citation context. The previous work was based on computing the cosine similarity of TF-IDF vectorizations of the Bag of Words of unigrams present in the sentences. We started computing new features with an intention to better capture the similarity of topic between a set of sentences. These experiments were conducted by modifying the similarity measures for the community detection and the LexRank algorithm while keeping all other parameters intact. We calculated features from broadly three set of categories which are explained in the next sections. We ensure that the similarity scores calculated are in the range [0, 1]

\subsection{Lexical Features}
The dictionary meaning of lexical is \textit{of or relating to the words or vocabulary of a language, especially as distinguished from its grammatical and syntactical aspects.} We computed the following features with the grammatical and syntactical aspects as the intuition.

For 2 sentences in the citation context S1, S2

\subsubsection{Unigram similarity as a bag of words}

Unigram similarity involves calculation of cosine similarity using TD-IDF vectorization of unigrams after removing stopwords (stopwords are removed using NLTK corpora)

\subsubsection{Bigram similarity as a bag of words}
Bigram similarity involves calculation of cosine  similarity using TD-IDF vectorization of bigrams without removing stopwords

\subsubsection{Number of citations}
The number of citations in sentences of the citation context can be compared and can tell us about how similar two sentences are. Mathematically,  \\
Sim($ S_1, S_2 $) = $\frac{min (C(S_1), C(S_2))}{max (C(S_1), C(S_2))} $\\
Where,\\ C($S_i$) = Number of out-citations in the sentence $S_i$

\subsubsection{Unigrams of Part-of-Speech Tags}
For 2 sentences in the citation context, we tag their parts of speech using a standard POS tagger available with the NLTK package and calculate the cosine similarity after TF-IDF vectorization of the POS tags for the 2 sentences.

\subsubsection{Unigrams of Part-of-Speech Tags}
For 2 sentences in the citation context, we tag their parts of speech using a standard POS tagger available with the NLTK package and calculate the cosine similarity after TF-IDF vectorization of bigrams of POS tags for the 2 sentences.

\subsection{Graph Based Features}

\subsubsection{Bibilographic Coupling} 
Two documents are bibliographically coupled if they both cite one or more documents in common. The "coupling strength" of two given documents is higher the more citations to other documents they share. We calculate the bibilographic coupling of the 2 sentences using the following formula \\ 

sim ($S_1, S_2$) = $\frac{\lvert intersection(outCites(S1), outCites(S2) \rvert} { min ( \lvert outCites(S_1) \rvert, \lvert outCites(S_2) \rvert )}$ \\
Where, \\

inCites($S_i$) = Out-vertices of $S_i$ in the citation graph \\
intersection(A, B) = set of common elements in A \& B and, \\
$\lvert A \rvert$ = cardinality of list A

\subsubsection{Co-citation} 
Co-citation, like Bibliographic Coupling, is a semantic similarity measure for documents that makes use of citation relationships. Co-citation is defined as the frequency with which two documents are cited together by other documents.[1] If at least one other document cites two documents in common these documents are said to be co-cited. The more co-citations two documents receive, the higher their co-citation strength, and the more likely they are semantically related. We calculate the co-citation metric of 2 sentences by \\

Sim ($S_1, S_2$) = $ \frac{ \lvert intersection(inCites(S_1), inCites(S_2) \rvert}{ min ( \lvert inCites(S1) \rvert, \lvert inCites(S2) \rvert )}$ \\
Where, \\
inCites($S_i$) = In-vertices of $S_i$ in the citation graph

\subsection{Miscellaneous Features}

\subsubsection{Title similarity}
We calculate the unigram-similarity of the titles of the two documents that the 2 sentences belong to,  trying to establish the similarity measure between the 2 sentences \\
Sim ($S_1, S_2$) = Unigram-Similarity(Title(paper1), Title(paper2))

\subsubsection{Author similarty} 
We calculate the similarity in the set of authors between the two citing papers using overlapping similarity \\ 
Sim ($S_1, S_2$) = $\frac{\lvert intersection(authors(S_1), authors(S_2) \rvert}{  min ( \lvert authors(S1)\rvert, \lvert authors(S2) \rvert ) }$ \\
Where, \\
authors($S_i$) = list of authors of $S_i$

\subsubsection{Time based similarity}  
Time based similarity tries to capture the intuition that works citing the target document in a brief period belong to the same topic thus increasing the similarity between them \\
Sim ($S_1, S_2$) = $max(0, \frac{(1 - abs(year1 - year2))}{5})$\\
Where,\\
abs (x) = absolute value of x\\
year1 = publication year of S1\\
year2 = publication year of S2\\


\section{Combining Features}
After the calculation of features individually, we devised an approach to combine their values to represent a single similarity score for a set of citation context sentences. We took a variety of approaches to compute this which we describe in this section.

\subsection{Feature scaling} \label{scaling}

Before combining scores of individual features, we normalized the individual scores from various features by using a feature scaling technique to ensure that the range becomes [0, 1] 

\subsection{Mean of similarities} \label{mean}
We initiated combining the features by calculating the mean of individual similarity scores for the five features which demonstrated the highest evaluation pyramid scores.

\subsection{Maximum of similarities} \label{max}
In this experiment, the maximum value among the similarity scores by the combination of features was taken as the similarity score for the set of sentences in the citation context. 

\subsection{Minimum of similarities} \label{min}
We observed the the minimum of similarities for the combination of features was providing us the best evaluation scores on the data-set. Thus, minimum of similarity metrics from individual features was chosen as the method to combine features and calculate the similarity scores.


\section{Defining threshold for community detection}\label{thres}

To ensure that the resultant similarity weighted graph in the community detection module of the algorithm consists of relevant similarities and does not contain noise, we set up a threshold $ \alpha \in $ [0, 1] for the selection of edges in the community detection network. For a threshold $ \alpha $, we select top 100 edges in a list sorted by descending order of similarities. A default cut-off of 0.1 is taken for the similarity scores of the LexRank module of the algorithm.