<paper>
<cited id="ZC0">
<title id=" W06-2937.xml">the exploration of deterministic and efficient dependency parsing </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>section 4 presents the discussion and analysis of our parser with three selected languages.
</prevsent>
<prevsent>in section 5, we draw the future direction and conclusion.
</prevsent>
</prevsection>
<citsent citstr=" A00-2018 ">
241over the past decades, many state-of-the-art parsing algorithm were proposed, such as head-word lexicalized pcfg (collins, 1998), maximum entropy (charniak, 2000), <papid> A00-2018 </papid>maximum/minimum spanning tree (mst) (mcdonald et al, 2005), <papid> P05-1012 </papid>bottom-up deterministic parsing (yamada and matsumoto, 2003), and constant-time deterministic parsing (nivre, 2003).</citsent>
<aftsection>
<nextsent>among them, the nivres algorithm (nivre, 2003) was shown to be most efficient method, which only costs at most 2n transition actions to parse sentence (o(n3) for the bottom-up or mst approaches).
</nextsent>
<nextsent>nivres method is mainly consists of four transition actions, left/right/reduce/shift.
</nextsent>
<nextsent>we further extend these four actions by dividing the reduce?
</nextsent>
<nextsent>into reduce?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1">
<title id=" W06-2937.xml">the exploration of deterministic and efficient dependency parsing </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>section 4 presents the discussion and analysis of our parser with three selected languages.
</prevsent>
<prevsent>in section 5, we draw the future direction and conclusion.
</prevsent>
</prevsection>
<citsent citstr=" P05-1012 ">
241over the past decades, many state-of-the-art parsing algorithm were proposed, such as head-word lexicalized pcfg (collins, 1998), maximum entropy (charniak, 2000), <papid> A00-2018 </papid>maximum/minimum spanning tree (mst) (mcdonald et al, 2005), <papid> P05-1012 </papid>bottom-up deterministic parsing (yamada and matsumoto, 2003), and constant-time deterministic parsing (nivre, 2003).</citsent>
<aftsection>
<nextsent>among them, the nivres algorithm (nivre, 2003) was shown to be most efficient method, which only costs at most 2n transition actions to parse sentence (o(n3) for the bottom-up or mst approaches).
</nextsent>
<nextsent>nivres method is mainly consists of four transition actions, left/right/reduce/shift.
</nextsent>
<nextsent>we further extend these four actions by dividing the reduce?
</nextsent>
<nextsent>into reduce?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2">
<title id=" W06-2502.xml">cluster stopping rules for word sense discrimination </title>
<section> methodology.  </section>
<citcontext>
<prevsection>
<prevsent>it offers variety of lexical features (ngrams, collocations, etc.) and feature selection methods (frequency, log likelihood, etc.).
</prevsent>
<prevsent>the contexts can then be represented with those features in vector space using first or second order vectors which are then clustered.
</prevsent>
</prevsection>
<citsent citstr=" W04-2406 ">
a detailed description can be found in purandare and pedersen (2004) <papid> W04-2406 </papid>and http://www.d.umn.edu/~tpederse/senseclusters.html.</citsent>
<aftsection>
<nextsent>sense clusters links to cluto for the clustering part (http://www-users.cs.umn.edu/~karypis/ cluto/download.html).
</nextsent>
<nextsent>cluto implements in afast and efficient way the main clustering algorithms ? agglomerative, parti tional and repeated bisections.
</nextsent>
<nextsent>for the current study, we modified the nlm wsd by excluding instances sense-tagged with the none of the above?
</nextsent>
<nextsent>category.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5">
<title id=" W06-2928.xml">dependency parsing with reference to slovene spanish and swedish </title>
<section> the parser.  </section>
<citcontext>
<prevsection>
<prevsent>the development test set consisted of 250 sentences for arabic, slovene, spanish and turkish, 500 sentences for danish and portuguese, and 1,000 sentences for the other languages.
</prevsent>
<prevsent>the baseline parser predicts unlabeled directed dependencies.
</prevsent>
</prevsection>
<citsent citstr=" P05-1012 ">
as described in (corston-oliver et al, 2006), we reimplemented the parser described in (mcdonald et al, 2005) <papid> P05-1012 </papid>and validated their results for czech and english.</citsent>
<aftsection>
<nextsent>the parser finds the highest-scoring parse yamong all possible parses ? forgiven sen tence: y?
</nextsent>
<nextsent>= argmax yy s(y) (1) the score of given parse is the sum of the scores of all the dependency links (i,j) ? y: s(y) = ?
</nextsent>
<nextsent>(i,j)y d(i, j) = ?
</nextsent>
<nextsent>(i,j)y ? f(i, j) (2)where the link (i,j) indicates parent-child dependency between the token at position and the token at position j. the score d(i, j) of each dependency link (i,j) is further decomposed as the weighted sum of its features f(i, j).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC6">
<title id=" W06-2928.xml">dependency parsing with reference to slovene spanish and swedish </title>
<section> the parser.  </section>
<citcontext>
<prevsection>
<prevsent>to set w, we trained twenty averaged perceptrons on different shuffles of the training data, using the development test set to determine when the percep trons had converged.
</prevsent>
<prevsent>the averaged perceptrons were then combined to make bayes point machine (har rington et al, 2003).
</prevsent>
</prevsection>
<citsent citstr=" C96-1058 ">
at both training and run time, edges are scored independently, and eisners o(n3) decoder (eisner, 1996) <papid> C96-1058 </papid>is used to find the optimalparse.</citsent>
<aftsection>
<nextsent>this decoder produces only projective analyses, although it does allow for analyses with multiple roots.
</nextsent>
<nextsent>the features used for scoring the edges prior to applying eisners algorithm are extracted from each possible parent-child dependency.
</nextsent>
<nextsent>the features include the case-normalized original form and lemma1 of each token , the part of speech (pos) tag of each token, the pos tag of each intervening token and 1if no lemma was specified, we truncated the original form by taking the first two characters for chinese words consisting of two characters or more and the first five characters for words consisting of five characters or more in the other languages.
</nextsent>
<nextsent>of each token to the left and right of the parent and child.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC8">
<title id=" W06-2928.xml">dependency parsing with reference to slovene spanish and swedish </title>
<section> the dependency labeler.  </section>
<citcontext>
<prevsection>
<prevsent>for example, there were no features that considered subject-verb agreement nor agreement of an adjective with the number or lexical gender of the noun it modified.however, it is possible that morphological information influenced the training of edge weights if the information was implicit in the pos tags.
</prevsent>
<prevsent>4.1 classifier.
</prevsent>
</prevsection>
<citsent citstr=" J96-1002 ">
we used maximum entropy classifier (berger et al,1996) <papid> J96-1002 </papid>to assign labels to the unlabeled dependencies produced by the bayes point machine.</citsent>
<aftsection>
<nextsent>we used the same training and development test split that was used to train the dependency parser.
</nextsent>
<nextsent>we chose to use maximum entropy classifiers because they can be trained relatively quickly while still offering reason able classification accuracy and are robust in the face of large numbers of superfluous features, desirable property given the requirement that the same parser handle multiple languages.
</nextsent>
<nextsent>furthermore, maximum entropy classifiers provide good probability distributions over class labels.
</nextsent>
<nextsent>this was important to us be cause we had initially hoped to find the optimal set of dependency labels for the children of given node by modeling the probability of each set of labels conditioned on the lemma and pos of the parent.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC9">
<title id=" W06-1408.xml">generating references to parts of recursively structured objects </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>moreover, the contrast set (the set of potential dis tractors (mcdonald 1981)) is defined to entail all elements of the context set except the intended referents.
</prevsent>
<prevsent>generating referring expressions is pursued since the eighties (e.g., (appelt 1985), among several others).
</prevsent>
</prevsection>
<citsent citstr=" P90-1013 ">
subsequent years were characterized by debate about computational efficiency versus minimality of the elements appearing in the resulting referring expression (dale 1988, reiter 1990, <papid> P90-1013 </papid>and several others).</citsent>
<aftsection>
<nextsent>in the mid-nineties, this debate seemed to be settled in favor of the incremental approach (dale and reiter 1995) ? motivated by results of psychological experiments (e.g., levelt 1989), certain non-minimal expressions are tolerated in favor of adopting the fast strategy of incrementally selecting ambiguity-reducing attributes from domain-dependent preference list.
</nextsent>
<nextsent>complementary activities include the generation of vague descriptions (van deemter, 2000) and extensions to multimodal expressions (van der sluis 2005).
</nextsent>
<nextsent>recently, algorithms have also been developed to the identification of sets of objects rather than individuals (bateman 1999, <papid> P99-1017 </papid>stone 2000, <papid> W00-1416 </papid>krahmer, v. erk, and verweg 2001), and the repertoire of descriptions has been extended to boolean combinations of attributes, including neg ations (van deemter 2002).</nextsent>
<nextsent>to avoid the generation of redundant descriptions what incremental approaches typically do, gardent (2002) <papid> P02-1013 </papid>and horacek (2003) <papid> E03-1017 </papid>proposed exhaustive resp.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC10">
<title id=" W06-1408.xml">generating references to parts of recursively structured objects </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>in the mid-nineties, this debate seemed to be settled in favor of the incremental approach (dale and reiter 1995) ? motivated by results of psychological experiments (e.g., levelt 1989), certain non-minimal expressions are tolerated in favor of adopting the fast strategy of incrementally selecting ambiguity-reducing attributes from domain-dependent preference list.
</prevsent>
<prevsent>complementary activities include the generation of vague descriptions (van deemter, 2000) and extensions to multimodal expressions (van der sluis 2005).
</prevsent>
</prevsection>
<citsent citstr=" P99-1017 ">
recently, algorithms have also been developed to the identification of sets of objects rather than individuals (bateman 1999, <papid> P99-1017 </papid>stone 2000, <papid> W00-1416 </papid>krahmer, v. erk, and verweg 2001), and the repertoire of descriptions has been extended to boolean combinations of attributes, including neg ations (van deemter 2002).</citsent>
<aftsection>
<nextsent>to avoid the generation of redundant descriptions what incremental approaches typically do, gardent (2002) <papid> P02-1013 </papid>and horacek (2003) <papid> E03-1017 </papid>proposed exhaustive resp.</nextsent>
<nextsent>best-first searches.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC11">
<title id=" W06-1408.xml">generating references to parts of recursively structured objects </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>in the mid-nineties, this debate seemed to be settled in favor of the incremental approach (dale and reiter 1995) ? motivated by results of psychological experiments (e.g., levelt 1989), certain non-minimal expressions are tolerated in favor of adopting the fast strategy of incrementally selecting ambiguity-reducing attributes from domain-dependent preference list.
</prevsent>
<prevsent>complementary activities include the generation of vague descriptions (van deemter, 2000) and extensions to multimodal expressions (van der sluis 2005).
</prevsent>
</prevsection>
<citsent citstr=" W00-1416 ">
recently, algorithms have also been developed to the identification of sets of objects rather than individuals (bateman 1999, <papid> P99-1017 </papid>stone 2000, <papid> W00-1416 </papid>krahmer, v. erk, and verweg 2001), and the repertoire of descriptions has been extended to boolean combinations of attributes, including neg ations (van deemter 2002).</citsent>
<aftsection>
<nextsent>to avoid the generation of redundant descriptions what incremental approaches typically do, gardent (2002) <papid> P02-1013 </papid>and horacek (2003) <papid> E03-1017 </papid>proposed exhaustive resp.</nextsent>
<nextsent>best-first searches.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC12">
<title id=" W06-1408.xml">generating references to parts of recursively structured objects </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>complementary activities include the generation of vague descriptions (van deemter, 2000) and extensions to multimodal expressions (van der sluis 2005).
</prevsent>
<prevsent>recently, algorithms have also been developed to the identification of sets of objects rather than individuals (bateman 1999, <papid> P99-1017 </papid>stone 2000, <papid> W00-1416 </papid>krahmer, v. erk, and verweg 2001), and the repertoire of descriptions has been extended to boolean combinations of attributes, including neg ations (van deemter 2002).</prevsent>
</prevsection>
<citsent citstr=" P02-1013 ">
to avoid the generation of redundant descriptions what incremental approaches typically do, gardent (2002) <papid> P02-1013 </papid>and horacek (2003) <papid> E03-1017 </papid>proposed exhaustive resp.</citsent>
<aftsection>
<nextsent>best-first searches.
</nextsent>
<nextsent>all these procedures more or less share the design of the knowledge base which bears influence on the descriptor selection.
</nextsent>
<nextsent>objects are conceived as atomic entities, which can be described in terms of sets of attributes and relations to other objects.
</nextsent>
<nextsent>in such setting, structured object can be represented, among others, by set of relations to its components, which are themselves conceived as objects.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC13">
<title id=" W06-1408.xml">generating references to parts of recursively structured objects </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>complementary activities include the generation of vague descriptions (van deemter, 2000) and extensions to multimodal expressions (van der sluis 2005).
</prevsent>
<prevsent>recently, algorithms have also been developed to the identification of sets of objects rather than individuals (bateman 1999, <papid> P99-1017 </papid>stone 2000, <papid> W00-1416 </papid>krahmer, v. erk, and verweg 2001), and the repertoire of descriptions has been extended to boolean combinations of attributes, including neg ations (van deemter 2002).</prevsent>
</prevsection>
<citsent citstr=" E03-1017 ">
to avoid the generation of redundant descriptions what incremental approaches typically do, gardent (2002) <papid> P02-1013 </papid>and horacek (2003) <papid> E03-1017 </papid>proposed exhaustive resp.</citsent>
<aftsection>
<nextsent>best-first searches.
</nextsent>
<nextsent>all these procedures more or less share the design of the knowledge base which bears influence on the descriptor selection.
</nextsent>
<nextsent>objects are conceived as atomic entities, which can be described in terms of sets of attributes and relations to other objects.
</nextsent>
<nextsent>in such setting, structured object can be represented, among others, by set of relations to its components, which are themselves conceived as objects.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC14">
<title id=" W06-1514.xml">generating xtag parsers from algebraic specifications </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(i ? a), and n1 2 . . .
</prevsent>
<prevsent>nr is the ordered sequence of direct children of n?
</prevsent>
</prevsection>
<citsent citstr=" E99-1020 ">
the parsing schema for the tag cyk-based algorithm (alonso et al, 1999) <papid> E99-1020 </papid>is function that maps such grammar to deduction system whose domain is the set of items {[n? , i, j, p, q, adj]} verifying that n?</citsent>
<aftsection>
<nextsent>is tree node in an elementary 1where vt denotes the set of terminal symbols, vn the set of nonterminal symbols, the axiom, the set of initial trees and the set of auxiliary trees.
</nextsent>
<nextsent>2from now on, we will follow the usual conventions bywhich nonterminal symbols are represented by upper case letters (a, . . .), and terminals by lowercase letters (a, . . .).
</nextsent>
<nextsent>greek letters (?, ?...)
</nextsent>
<nextsent>will be used to represent trees, n?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC17">
<title id=" W06-1514.xml">generating xtag parsers from algebraic specifications </title>
<section> generating parsers for the xtag.  </section>
<citcontext>
<prevsection>
<prevsent>| false] ? ?
</prevsent>
<prevsent>subs(n ?) figure 1: cyk-based parser for tag.
</prevsent>
</prevsection>
<citsent citstr=" J99-3002 ">
grammar by using parsing schemata as the ones in (alonsoet al, 1999; <papid> E99-1020 </papid>nederhof, 1999) <papid> J99-3002 </papid>as input to our system, we can easily obtain efficient implementations of several tag parsing algorithms.</citsent>
<aftsection>
<nextsent>in this section, we describe how we have dealt with the particular characteristics of the xtag grammar in order to make it compatible with our generic compilation technique; and we also provide empirical results which allow us to compare the performance of several different tag parsing algorithms in the practical case of the xtag grammar. it shall be noted that similar comparisons have been made with smaller grammars, such as simplified subsets of the xtag grammar, but not with the whole xtag grammar with all its tree sand feature structures.
</nextsent>
<nextsent>therefore, our comparison provides valuable information about the behavior of various parsers on complete, large scale natural language grammar.
</nextsent>
<nextsent>this behavior is very different from the one that can be observed on small grammars, since grammar size becomes dominant factor in computational complexity when large grammars like the xtag areused to parse relatively small natural language sentences (gomez-rodrguez et al, 2006b).
</nextsent>
<nextsent>2.1 grammar conversion.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC22">
<title id=" W06-3205.xml">exploring variant definitions of pointer length in mdl </title>
<section> abstract </section>
<citcontext>
<prevsection>


</prevsection>
<citsent citstr=" J01-2001 ">
within the information-theoretical framework described by (rissanen, 1989; demarcken, 1996; goldsmith, 2001), <papid> J01-2001 </papid>pointers are used to avoid repetition of phonological material.</citsent>
<aftsection>
<nextsent>work with which we are familiar has assumed that there is only one way in which items could be pointedto.
</nextsent>
<nextsent>the purpose of this paper is to describe and compare several different methods, each of which satisfies mdls basic requirements, but which have different consequences for the treatment of linguistic phenomena.
</nextsent>
<nextsent>in particular, we assess the conditions under which these different ways of pointing yield more compact descriptions of the data, both from theoretical and an empirical perspective.
</nextsent>
<nextsent>the fundamental hypothesis underlying the minimum description length (mdl) framework (rissa nen, 1989; de marcken, 1996; goldsmith, 2001) <papid> J01-2001 </papid>is that the selection of model for explaining set of data should aim at satisfying two constraints: on the one hand, it is desirable to select model that can be described in highly compact fashion; on the other hand, the selected model should make it possible to model the data well, which is interpreted as being able to describe the data in maximally compactfashion.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC26">
<title id=" W07-0702.xml">ccg supertags in factored statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in large-scale machine translation evaluations,phrase-based models generally outperform syntax based models1.
</prevsent>
<prevsent>phrase-based models are effective because they capture the lexical dependencies between languages.
</prevsent>
</prevsection>
<citsent citstr=" N03-1019 ">
however, these models, which are equivalent to finite-state machines (kumar and byrne, 2003), <papid> N03-1019 </papid>are unable to model long range word order differences.</citsent>
<aftsection>
<nextsent>phrase-based models also lack the ability to incorporate the generalisations implicit in syntactic knowledge and they do not respect linguistic phrase boundaries.
</nextsent>
<nextsent>this makes it difficult to im prove reordering in phrase-based models.
</nextsent>
<nextsent>syntax-based models can overcome some of the problems associated with phrase-based models because they are able to capture the long range structural mappings that occur in translation.
</nextsent>
<nextsent>recently 1www.nist.gov/speech/tests/mt/mt06eval official results.html there have been few syntax-based models that show performance comparable to the phrase-basedmodels (chiang, 2005; <papid> P05-1033 </papid>marcu et al, 2006).<papid> W06-1606 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC27">
<title id=" W07-0702.xml">ccg supertags in factored statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this makes it difficult to im prove reordering in phrase-based models.
</prevsent>
<prevsent>syntax-based models can overcome some of the problems associated with phrase-based models because they are able to capture the long range structural mappings that occur in translation.
</prevsent>
</prevsection>
<citsent citstr=" P05-1033 ">
recently 1www.nist.gov/speech/tests/mt/mt06eval official results.html there have been few syntax-based models that show performance comparable to the phrase-basedmodels (chiang, 2005; <papid> P05-1033 </papid>marcu et al, 2006).<papid> W06-1606 </papid></citsent>
<aftsection>
<nextsent>how ever, reliably learning powerful rules from parallel data is very difficult and prone to problems with sparsity and noise in the data.
</nextsent>
<nextsent>these models also suffer from large search space when decoding with an integrated language model, which can lead to search errors (chiang, 2005).<papid> P05-1033 </papid>in this paper we investigate the idea of incorporating syntax into phrase-based models, thereby leveraging the strengths of both the phrase-based models and syntactic structures.</nextsent>
<nextsent>this is done using ccg supertags, which provide rich source of syntactic information.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC28">
<title id=" W07-0702.xml">ccg supertags in factored statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this makes it difficult to im prove reordering in phrase-based models.
</prevsent>
<prevsent>syntax-based models can overcome some of the problems associated with phrase-based models because they are able to capture the long range structural mappings that occur in translation.
</prevsent>
</prevsection>
<citsent citstr=" W06-1606 ">
recently 1www.nist.gov/speech/tests/mt/mt06eval official results.html there have been few syntax-based models that show performance comparable to the phrase-basedmodels (chiang, 2005; <papid> P05-1033 </papid>marcu et al, 2006).<papid> W06-1606 </papid></citsent>
<aftsection>
<nextsent>how ever, reliably learning powerful rules from parallel data is very difficult and prone to problems with sparsity and noise in the data.
</nextsent>
<nextsent>these models also suffer from large search space when decoding with an integrated language model, which can lead to search errors (chiang, 2005).<papid> P05-1033 </papid>in this paper we investigate the idea of incorporating syntax into phrase-based models, thereby leveraging the strengths of both the phrase-based models and syntactic structures.</nextsent>
<nextsent>this is done using ccg supertags, which provide rich source of syntactic information.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC30">
<title id=" W07-0702.xml">ccg supertags in factored statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>factored models allow for the easy inclusion of supertags in different ways.
</prevsent>
<prevsent>the first approach is to generate ccg supertags as factor in the target and then apply an n-gram model over them, increasing the probability of more frequently seen sequences of supertags.
</prevsent>
</prevsection>
<citsent citstr=" P07-1037 ">
this is simple way of including syntactic information in phrase-based model, and has also been suggested by hassan et al (2007).<papid> P07-1037 </papid></citsent>
<aftsection>
<nextsent>for both arabic-english (hassan et al, 2007) <papid> P07-1037 </papid>and our experiments in dutch-english, n-gram models over ccg supertags improve the quality of translation.</nextsent>
<nextsent>by preferring more likely sequences of supertags, it is conceivable that the output of the decoder is 9 more grammatical.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC32">
<title id=" W07-0702.xml">ccg supertags in factored statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the subcategorisation of verb, for instance, might help select the correct translation.
</prevsent>
<prevsent>using multiple dependencies on factors in the source, we need strategy for dealing with sparse data.
</prevsent>
</prevsection>
<citsent citstr=" P05-1003 ">
we propose using logarithmic opinion pool (smith et al, 2005) <papid> P05-1003 </papid>to combine the more specific models (which depend onboth words and supertags) with more general models (which only depends on words).</citsent>
<aftsection>
<nextsent>this paper is the first to suggest this approach for combining multiple information sources in machine translation.although the addition of supertags to phrase based translation does show some improvement, their overall impact is limited.
</nextsent>
<nextsent>sequence models over supertags clearly result in some improvement sin local reordering but syntactic information contains long distance dependencies which are simply not utilised in phrase-based models.
</nextsent>
<nextsent>inspired by work on factored language models, koehn et al (2006) extend phrase-based models to incorporate multiple levels of linguistic knowledge as factors.
</nextsent>
<nextsent>phrase-based models are limited to sequences of words as their units with no access to additional linguistic knowledge.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC33">
<title id=" W07-0702.xml">ccg supertags in factored statistical machine translation </title>
<section> factored models.  </section>
<citcontext>
<prevsection>
<prevsent>factors allow for richer translation models, for example, the gender or tense of word can be expressed.
</prevsent>
<prevsent>factors also allow the model to generalise, for example, the lemma of word could be used to generalise to unseen inflected forms.
</prevsent>
</prevsection>
<citsent citstr=" P03-1021 ">
the factored translation model combines features in log-linear fashion (och, 2003).<papid> P03-1021 </papid></citsent>
<aftsection>
<nextsent>the most likely target sentence t?
</nextsent>
<nextsent>is calculated using the decision rule in equation 1: t?
</nextsent>
<nextsent>= argmax { m?
</nextsent>
<nextsent>m=1 mhm(s fs 1 , ft 1 ) } (1) t?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC34">
<title id=" W07-0702.xml">ccg supertags in factored statistical machine translation </title>
<section> factored models.  </section>
<citcontext>
<prevsection>
<prevsent>the model is shown graphically in figure 1.
</prevsent>
<prevsent>word word ccg source target figure 1.
</prevsent>
</prevsection>
<citsent citstr=" N03-1017 ">
factored translation with source words determining target words and ccg supertags for our experiments we used the following fea tures: the translation probabilities pr(sfs1 |t ft 1 ) and pr(tft1 |s fs 1 ), the lexical weights (koehn et al, 2003) <papid> N03-1017 </papid>lex(sfs1 |t ft 1 ) and lex(t ft 1 |s fs 1 ), and phrase penalty e, which allows the model to learn preference for longer or shorter phrases.</citsent>
<aftsection>
<nextsent>added to these features 10 is the word penalty e1 which allows the model to learn preference for longer or shorter sentences, the distortion model that prefers monotone word order, and the language model probability pr(t).
</nextsent>
<nextsent>all these features are logged when combined in the log-linear model in order to retain the impact of very unlikely translations or sequences.
</nextsent>
<nextsent>one of the strengths of the factored model is it allows for n-gram distributions over factors on the target.
</nextsent>
<nextsent>we call these distributions sequence models.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC35">
<title id=" W07-0702.xml">ccg supertags in factored statistical machine translation </title>
<section> factored models.  </section>
<citcontext>
<prevsection>
<prevsent>for instance translating ccg supertags independently of words could introduce errors.
</prevsent>
<prevsent>multiple dependencies require some form of backing off to simpler models in order to cover the cases where, for instance, the word has been seen intraining, but not with that particular supertag.
</prevsent>
</prevsection>
<citsent citstr=" N03-2002 ">
different backoff paths are possible, and it would be interesting but prohibitively slow to apply strategy similar to generalised parallel backoff (bilmesand kirchhoff, 2003) <papid> N03-2002 </papid>which is used in factored language models.</citsent>
<aftsection>
<nextsent>backoff in factored language models is made more difficult because there is no obvious backoff path.
</nextsent>
<nextsent>this is compounded for factored phrase-based translation models where one has to consider backoff in terms of factors and n-gramlengths in both source and target languages.
</nextsent>
<nextsent>furthermore, the surface form of word is probably themost valuable factor and so its contribution must always be taken into account.
</nextsent>
<nextsent>we therefore did not use backoff and chose to use log-linear combination of features and models instead.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC36">
<title id=" W07-0702.xml">ccg supertags in factored statistical machine translation </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>the experiments were run using moses2, an open source factored statistical machine translation system.
</prevsent>
<prevsent>the srilm language modelling toolkit (stolcke, 2002) was used with modified kneser-ney discounting and interpolation.
</prevsent>
</prevsection>
<citsent citstr=" P04-1014 ">
the ccg super tagger (clark, 2002; clark and curran, 2004) <papid> P04-1014 </papid>was provided with the c&c; language processing tools3.</citsent>
<aftsection>
<nextsent>the super tagger was trained on the ccgbank in english (hockenmaier and steedman, 2005) and in german (hockenmaier, 2006).<papid> P06-1064 </papid></nextsent>
<nextsent>the dutch-english parallel training data comes from the europarl corpus (koehn, 2005) and excludes the proceedings from the last quarter of 2000.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC37">
<title id=" W07-0702.xml">ccg supertags in factored statistical machine translation </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>the srilm language modelling toolkit (stolcke, 2002) was used with modified kneser-ney discounting and interpolation.
</prevsent>
<prevsent>the ccg super tagger (clark, 2002; clark and curran, 2004) <papid> P04-1014 </papid>was provided with the c&c; language processing tools3.</prevsent>
</prevsection>
<citsent citstr=" P06-1064 ">
the super tagger was trained on the ccgbank in english (hockenmaier and steedman, 2005) and in german (hockenmaier, 2006).<papid> P06-1064 </papid></citsent>
<aftsection>
<nextsent>the dutch-english parallel training data comes from the europarl corpus (koehn, 2005) and excludes the proceedings from the last quarter of 2000.
</nextsent>
<nextsent>2see http://www.statmt.org/moses/ 3see http://svn.ask.it.usyd.edu.au/trac/candc/wiki this consists of 855,677 sentences with maximum of 50 words per sentence.
</nextsent>
<nextsent>500 sentences of tuning data and the 2000 sentences of test data are taken from the acl workshop on building and using parallel texts4.
</nextsent>
<nextsent>the german-english experiments use data from the naacl 2006 workshop on statistical machine translation5.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC38">
<title id=" W07-0702.xml">ccg supertags in factored statistical machine translation </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>however, this is limited way of leveraging the rich syntactic information available in the ccg categories.
</prevsent>
<prevsent>we explore the potential of letting supertags direct translation by including them as factor on the source.
</prevsent>
</prevsection>
<citsent citstr=" P05-1034 ">
this is similar to syntax-directed translation originally proposed for compiling (aho and ullman, 1969), and also used in machine translation (quirk et al., 2005; <papid> P05-1034 </papid>huang et al, 2006).<papid> W06-3601 </papid></citsent>
<aftsection>
<nextsent>information about the source words?
</nextsent>
<nextsent>syntactic function and subcategorisation can directly influence the hypotheses being searched in decoding.
</nextsent>
<nextsent>these experiments were performed on the german to english translation task, in contrast to the dutch to english results given in previous experiments.
</nextsent>
<nextsent>we use model which combines more specific dependencies on source words and source ccg supertags, with more general model which only has dependancies on the source word, see equation 4.we explore two different ways of balancing the statistical evidence from these multiple sources.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC39">
<title id=" W07-0702.xml">ccg supertags in factored statistical machine translation </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>however, this is limited way of leveraging the rich syntactic information available in the ccg categories.
</prevsent>
<prevsent>we explore the potential of letting supertags direct translation by including them as factor on the source.
</prevsent>
</prevsection>
<citsent citstr=" W06-3601 ">
this is similar to syntax-directed translation originally proposed for compiling (aho and ullman, 1969), and also used in machine translation (quirk et al., 2005; <papid> P05-1034 </papid>huang et al, 2006).<papid> W06-3601 </papid></citsent>
<aftsection>
<nextsent>information about the source words?
</nextsent>
<nextsent>syntactic function and subcategorisation can directly influence the hypotheses being searched in decoding.
</nextsent>
<nextsent>these experiments were performed on the german to english translation task, in contrast to the dutch to english results given in previous experiments.
</nextsent>
<nextsent>we use model which combines more specific dependencies on source words and source ccg supertags, with more general model which only has dependancies on the source word, see equation 4.we explore two different ways of balancing the statistical evidence from these multiple sources.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC40">
<title id=" W06-3801.xml">a graphical framework for contextual search and name disambiguation in email </title>
<section> learning.  </section>
<citcontext>
<prevsection>
<prevsent>previous researchers have described schemes for adjusting the parameters ? using gradient descent like methods (diligenti et al, 2005; nie et al, 2005).
</prevsent>
<prevsent>in this paper, we suggest an alternative approach of learning to re-order an initial ranking.
</prevsent>
</prevsection>
<citsent citstr=" J05-1003 ">
this reranking approach has been used in the past for meta search (cohen et al, 1999) and also several natural 3 language related tasks (collins and koo, 2005).<papid> J05-1003 </papid></citsent>
<aftsection>
<nextsent>the advantage of reranking over parameter tuning is that the learned classifier can take advantage of global?
</nextsent>
<nextsent>features that are not easily used in walk.
</nextsent>
<nextsent>note that node reranking, while can be used as an alternative to weight manipulation, it is better viewed as complementary approach, as the techniques can be naturally combined by first tuning the parameters ?, and then reranking the result using aclassifier which exploits non-local features.
</nextsent>
<nextsent>this hybrid approach has been used successfully in the past on tasks like parsing (collins and koo, 2005).<papid> J05-1003 </papid>we here give short overview of the reranking approach, that is described in detail elsewhere (collinsand koo, 2005).<papid> J05-1003 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC46">
<title id=" W06-3801.xml">a graphical framework for contextual search and name disambiguation in email </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>the weights for the function are learned with boosting-like method, where in each iteration the feature fk that has the most impact on the loss function is chosen, and is modified.
</prevsent>
<prevsent>closed form formulas exist for calculating the optimal additive updates and the impact per feature (schapire and singer, 1999).
</prevsent>
</prevsection>
<citsent citstr=" H05-1056 ">
we experiment with three separate corpora.the cspace corpus contains email messages collected from management course conducted at carnegie mellon university in 1997 (minkov etal., 2005).<papid> H05-1056 </papid></citsent>
<aftsection>
<nextsent>in this course, mba students, organized in teams of four to six members, ran simulated companies in different market scenarios.
</nextsent>
<nextsent>the corpus we used here includes the emails of all teams over period of four days.
</nextsent>
<nextsent>the enron corpus is collection of mail from the enron corpus that has been made available for the research community (klimt and yang, 2004).
</nextsent>
<nextsent>here, we used the saved email of two different users.2 to eliminate spam and news postings we removed email files sent from email addresses with suffix ?.com?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC48">
<title id=" W06-1656.xml">boosting unsupervised relation extraction by using ner </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this paper we show how the introduction of simple rule based ner can boost the performance of ures on variety of relations.
</prevsent>
<prevsent>we also compare the performance of ures to the performance of the state of-the-art knowitall system, and to the performance of its pattern learning component, which uses simpler and less powerful pattern language than ures.
</prevsent>
</prevsection>
<citsent citstr=" X96-1029 ">
information extraction (ie) (riloff 1993; cowie and lehnert 1996; grishman 1996; <papid> X96-1029 </papid>grishman 1997; kushmerick, weld et al 1997; freitag 1998; freitag and mccallum 1999; soderland 1999) is the task of extracting factual assertions from text.</citsent>
<aftsection>
<nextsent>most ie systems relyon knowledge engineering or on machine learning to generate extraction patterns ? the mechanism that extracts entities and relation instances from text.
</nextsent>
<nextsent>in the machine learning approach, domain expert labels instances of the target relations in set of documents.
</nextsent>
<nextsent>the system then learns extraction patterns, which can be applied to new documents automatically.
</nextsent>
<nextsent>both approaches require substantial human effort, particularly when applied to the broad range of documents, entities, and relations on the web.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC49">
<title id=" W06-1656.xml">boosting unsupervised relation extraction by using ner </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>section 5 contains conclusions and directions for future work.
</prevsent>
<prevsent>the ie systems most similar to ures are based on bootstrap learning: mutual bootstrapping (riloff and jones 1999), the dipre system (brin 1998), and the snowball system (agichtein and gravano 2000 ).
</prevsent>
</prevsection>
<citsent citstr=" P02-1006 ">
(ravichandran and hovy 2002) <papid> P02-1006 </papid>also use bootstrapping, and learn simple surface patterns for extracting binary relations from the web.</citsent>
<aftsection>
<nextsent>unlike those unsupervised ie systems, ures patterns allow gaps that can be matched by any sequences of tokens.
</nextsent>
<nextsent>this makes ures patterns much more general, and allows to recognize instances in sentences inaccessible to the simple surface patterns of systems such as (brin 1998; riloff and jones 1999; ravichandran and hovy 2002).<papid> P02-1006 </papid></nextsent>
<nextsent>the greater power of ures requires different and more complex methods for learning, scoring, and filtering of patterns.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC51">
<title id=" W06-2905.xml">what are the productive units of natural language grammar a dop approach to the automatic identification of constructions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this paper explore an alternative formal and computational approach, where multi-word constructions have no special status, but emerge in general procedure to find the best statistical grammar to describe training corpus.
</prevsent>
<prevsent>crucially, use formalism known as stochastic tree substitution grammars?
</prevsent>
</prevsection>
<citsent citstr=" E03-1005 ">
(henceforth, stsgs), which can represent single words, contiguous and noncontiguous mwes, context-free rules or complete parse trees in unified representation.my approach is closely related to work in statistical parsing known as data-oriented parsing (dop),an empirically highly successful approach with labeled recall and precision scores on the penn tree bank that are among the best currently obtained (bod, 2003).<papid> E03-1005 </papid></citsent>
<aftsection>
<nextsent>dop, first proposed in (scha, 1990), 29can be seen as an early formalization and combination of ideas from construction grammar and statistical parsing.
</nextsent>
<nextsent>its key innovations were (i) the proposal to use fragments of trees from tree bank as the symbolic backbone; (ii) the proposal to allow, in principle, trees of arbitrary size and shape as the elementary units of combination; (iii) the proposal to use the occurrence and co-occurrence frequencies as the basis for structural disambiguation in parsing.
</nextsent>
<nextsent>the model develop in this paper is true to these general dop ideals, although it differs in important respects from the many dop implementations that have been studied since its first inception (bod, 1993; <papid> E93-1006 </papid>goodman, 1996; <papid> W96-0214 </papid>bod, 1998; simaan, 2002; collins and duffy, 2002; <papid> P02-1034 </papid>bod et al, 2003, and many others).</nextsent>
<nextsent>the crucial difference is in the estimation procedure for choosing the weights of the stsg based on observed frequencies in corpus.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC52">
<title id=" W06-2905.xml">what are the productive units of natural language grammar a dop approach to the automatic identification of constructions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>dop, first proposed in (scha, 1990), 29can be seen as an early formalization and combination of ideas from construction grammar and statistical parsing.
</prevsent>
<prevsent>its key innovations were (i) the proposal to use fragments of trees from tree bank as the symbolic backbone; (ii) the proposal to allow, in principle, trees of arbitrary size and shape as the elementary units of combination; (iii) the proposal to use the occurrence and co-occurrence frequencies as the basis for structural disambiguation in parsing.
</prevsent>
</prevsection>
<citsent citstr=" E93-1006 ">
the model develop in this paper is true to these general dop ideals, although it differs in important respects from the many dop implementations that have been studied since its first inception (bod, 1993; <papid> E93-1006 </papid>goodman, 1996; <papid> W96-0214 </papid>bod, 1998; simaan, 2002; collins and duffy, 2002; <papid> P02-1034 </papid>bod et al, 2003, and many others).</citsent>
<aftsection>
<nextsent>the crucial difference is in the estimation procedure for choosing the weights of the stsg based on observed frequencies in corpus.
</nextsent>
<nextsent>existing dop models converge to stsgs that either (i) give all subtrees of the observed trees nonzero weights (bod, 1993; <papid> E93-1006 </papid>bod, 2003), <papid> E03-1005 </papid>or (ii) give only the largest possible fragments nonzero weights (simaan and buratto, 2003; zollmann and simaan, 2005).</nextsent>
<nextsent>the model in this paper, in contrast, aims at finding the smallest set of productive units that explain the occurrences and co-occurrences in corpus.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC54">
<title id=" W06-2905.xml">what are the productive units of natural language grammar a dop approach to the automatic identification of constructions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>dop, first proposed in (scha, 1990), 29can be seen as an early formalization and combination of ideas from construction grammar and statistical parsing.
</prevsent>
<prevsent>its key innovations were (i) the proposal to use fragments of trees from tree bank as the symbolic backbone; (ii) the proposal to allow, in principle, trees of arbitrary size and shape as the elementary units of combination; (iii) the proposal to use the occurrence and co-occurrence frequencies as the basis for structural disambiguation in parsing.
</prevsent>
</prevsection>
<citsent citstr=" W96-0214 ">
the model develop in this paper is true to these general dop ideals, although it differs in important respects from the many dop implementations that have been studied since its first inception (bod, 1993; <papid> E93-1006 </papid>goodman, 1996; <papid> W96-0214 </papid>bod, 1998; simaan, 2002; collins and duffy, 2002; <papid> P02-1034 </papid>bod et al, 2003, and many others).</citsent>
<aftsection>
<nextsent>the crucial difference is in the estimation procedure for choosing the weights of the stsg based on observed frequencies in corpus.
</nextsent>
<nextsent>existing dop models converge to stsgs that either (i) give all subtrees of the observed trees nonzero weights (bod, 1993; <papid> E93-1006 </papid>bod, 2003), <papid> E03-1005 </papid>or (ii) give only the largest possible fragments nonzero weights (simaan and buratto, 2003; zollmann and simaan, 2005).</nextsent>
<nextsent>the model in this paper, in contrast, aims at finding the smallest set of productive units that explain the occurrences and co-occurrences in corpus.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC55">
<title id=" W06-2905.xml">what are the productive units of natural language grammar a dop approach to the automatic identification of constructions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>dop, first proposed in (scha, 1990), 29can be seen as an early formalization and combination of ideas from construction grammar and statistical parsing.
</prevsent>
<prevsent>its key innovations were (i) the proposal to use fragments of trees from tree bank as the symbolic backbone; (ii) the proposal to allow, in principle, trees of arbitrary size and shape as the elementary units of combination; (iii) the proposal to use the occurrence and co-occurrence frequencies as the basis for structural disambiguation in parsing.
</prevsent>
</prevsection>
<citsent citstr=" P02-1034 ">
the model develop in this paper is true to these general dop ideals, although it differs in important respects from the many dop implementations that have been studied since its first inception (bod, 1993; <papid> E93-1006 </papid>goodman, 1996; <papid> W96-0214 </papid>bod, 1998; simaan, 2002; collins and duffy, 2002; <papid> P02-1034 </papid>bod et al, 2003, and many others).</citsent>
<aftsection>
<nextsent>the crucial difference is in the estimation procedure for choosing the weights of the stsg based on observed frequencies in corpus.
</nextsent>
<nextsent>existing dop models converge to stsgs that either (i) give all subtrees of the observed trees nonzero weights (bod, 1993; <papid> E93-1006 </papid>bod, 2003), <papid> E03-1005 </papid>or (ii) give only the largest possible fragments nonzero weights (simaan and buratto, 2003; zollmann and simaan, 2005).</nextsent>
<nextsent>the model in this paper, in contrast, aims at finding the smallest set of productive units that explain the occurrences and co-occurrences in corpus.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC59">
<title id=" W06-2905.xml">what are the productive units of natural language grammar a dop approach to the automatic identification of constructions </title>
<section> formalism, notation and definitions.  </section>
<citcontext>
<prevsection>
<prevsent>2.1 stochastic tree substitution grammars.
</prevsent>
<prevsent>stsgs are simple generalization of stochastic context free grammars (henceforth, scfgs), where the productive units are elementary trees of arbitrary size instead of the rewrite rules in scfgs (which can be viewed as trees of depth 1).
</prevsent>
</prevsection>
<citsent citstr=" C92-2065 ">
stsgsform restricted subclass of stochastic tree adjoining grammars (henceforth, stags) (resnik, 1992; <papid> C92-2065 </papid>schabes, 1992), <papid> C92-2066 </papid>the difference being that stsgs only allow for substitution and not for adjunction(joshi and sarkar, 2003).</citsent>
<aftsection>
<nextsent>this limits the generative capacity to that of context-free grammars, and means stsgs cannot be fully lexicalized.
</nextsent>
<nextsent>these limitations notwithstanding, the close relationship with stags is an attractive feature with extensions to the class of mildly context-sensitive languages(joshi et al, 1991) in mind.
</nextsent>
<nextsent>most importantly, how ever, stsgs are already able to model vast rangeof statistical dependencies between words and constituents, which allows them to rightly predict the occurrences of many constructions (bod, 1998).for completeness, we include the usual definitions of stsgs, the substitution operation and derivation and parse probabilities (bod, 1998), using our own notation.
</nextsent>
<nextsent>an stsg is 5-tuple vn, vt, s, t, w?, where vn is the set of non-terminalsymbols; vt is the set of terminal symbols; ? vn isthe start symbol; is set of elementary trees, such that for every ? the unique root node r(t) ? vn,the set of internal nodes i(t) ? vn and the set of leaf nodes l(t) ? vn ? vt; finally, : ? [0, 1] is aprobability (weight) distribution over the elementary trees, such that for any ? , tr(t) w(t?) = 1,where r(t) is the set of elementary trees with thesame root label as t. it will prove useful to also define the set of all possible trees ? over the defined alphabets (with the same conditions on root, internal and leaf nodes as for ), and the set of all possible complete parse trees ?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC60">
<title id=" W06-2905.xml">what are the productive units of natural language grammar a dop approach to the automatic identification of constructions </title>
<section> formalism, notation and definitions.  </section>
<citcontext>
<prevsection>
<prevsent>2.1 stochastic tree substitution grammars.
</prevsent>
<prevsent>stsgs are simple generalization of stochastic context free grammars (henceforth, scfgs), where the productive units are elementary trees of arbitrary size instead of the rewrite rules in scfgs (which can be viewed as trees of depth 1).
</prevsent>
</prevsection>
<citsent citstr=" C92-2066 ">
stsgsform restricted subclass of stochastic tree adjoining grammars (henceforth, stags) (resnik, 1992; <papid> C92-2065 </papid>schabes, 1992), <papid> C92-2066 </papid>the difference being that stsgs only allow for substitution and not for adjunction(joshi and sarkar, 2003).</citsent>
<aftsection>
<nextsent>this limits the generative capacity to that of context-free grammars, and means stsgs cannot be fully lexicalized.
</nextsent>
<nextsent>these limitations notwithstanding, the close relationship with stags is an attractive feature with extensions to the class of mildly context-sensitive languages(joshi et al, 1991) in mind.
</nextsent>
<nextsent>most importantly, how ever, stsgs are already able to model vast rangeof statistical dependencies between words and constituents, which allows them to rightly predict the occurrences of many constructions (bod, 1998).for completeness, we include the usual definitions of stsgs, the substitution operation and derivation and parse probabilities (bod, 1998), using our own notation.
</nextsent>
<nextsent>an stsg is 5-tuple vn, vt, s, t, w?, where vn is the set of non-terminalsymbols; vt is the set of terminal symbols; ? vn isthe start symbol; is set of elementary trees, such that for every ? the unique root node r(t) ? vn,the set of internal nodes i(t) ? vn and the set of leaf nodes l(t) ? vn ? vt; finally, : ? [0, 1] is aprobability (weight) distribution over the elementary trees, such that for any ? , tr(t) w(t?) = 1,where r(t) is the set of elementary trees with thesame root label as t. it will prove useful to also define the set of all possible trees ? over the defined alphabets (with the same conditions on root, internal and leaf nodes as for ), and the set of all possible complete parse trees ?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC61">
<title id=" W06-2905.xml">what are the productive units of natural language grammar a dop approach to the automatic identification of constructions </title>
<section> formalism, notation and definitions.  </section>
<citcontext>
<prevsection>
<prevsent>(7) 2.4 from usage frequency to expected.
</prevsent>
<prevsent>frequency the two remaining problems ? calculating expected frequencies from weights and estimating the weights from observed frequencies ? are surprisingly difficult and heretofore not satisfactorily solved.
</prevsent>
</prevsection>
<citsent citstr=" E06-2025 ">
in (zuidema, 2006) <papid> E06-2025 </papid>we evaluate existing estimation methods for data-oriented parsing, and show that they are ill-suited for learning tasks such as studied in this paper.</citsent>
<aftsection>
<nextsent>in the next section, we present new algorithm for estimation, which makes use of method for calculating expected frequencies that we sketch in this section.
</nextsent>
<nextsent>this method makes use of sub- and super tree relations that we explain first.
</nextsent>
<nextsent>we define two types of subtrees of given tree t, which, for lack of better terminology, we will call twigs?
</nextsent>
<nextsent>and prunes?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC62">
<title id=" W06-2905.xml">what are the productive units of natural language grammar a dop approach to the automatic identification of constructions </title>
<section> results.  </section>
<citcontext>
<prevsection>
<prevsent>we have evaluated the algorithms on number of simple example stsgs and found that the expected frequency algorithm correctly predicts observed frequencies.
</prevsent>
<prevsent>we have further found that ? unlike existing estimation methods ? the push-n-pull algorithm converges to stsgs that closely model the observed frequencies (i.e. that maximize the likelihood of the data) without putting all probability mass in the largest elementary trees (i.e. whilst retaining generalizations about the data).
</prevsent>
</prevsection>
<citsent citstr=" H90-1021 ">
here we report first quantitative results on theatis3 corpus (hemphill et al, 1990).<papid> H90-1021 </papid></citsent>
<aftsection>
<nextsent>before processing, all trees (train and test set) were converted to format that our current implementation requires (all non-terminal labels are unique, all internal nodes have two daughters, all pre terminal nodes have single lexical daughter; all unary productions andall traces were removed).
</nextsent>
<nextsent>the set of trees was randomly split in train set of 462 trees, and test set2an important topic for future research is to clarify the relation between push-n-pull and expectation maximization.
</nextsent>
<nextsent>of 116 trees.
</nextsent>
<nextsent>the push-n-pull algorithm was then run in 10 passes over the train set, with = 3,b = 0 and ? = 0.1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC64">
<title id=" W06-2905.xml">what are the productive units of natural language grammar a dop approach to the automatic identification of constructions </title>
<section> results.  </section>
<citcontext>
<prevsection>
<prevsent>shown are results were only elementary trees with scores higher than 0.3 and 0.1 respectively are used.however, more interesting is qualitative analysis of the stsg induced, which shows that, unlike dop1, push-n-pull arrives at grammar that gives high weights (and scores) to those elementary3we approximated the most probable parse as follows (fol lowing (bod, 2003)).<papid> E03-1005 </papid></prevsent>
<prevsent>we first converted the induced stsg toan isomorph scfg, by giving the internal nodes of every elementary tree unique address-labels, and reading off all cfg productions (all with weight 1.0, except for the top-production, which receives the weight of t).</prevsent>
</prevsection>
<citsent citstr=" C04-1024 ">
an existing scfg parser (schmid, 2004) <papid> C04-1024 </papid>was then used, with simple unknown word heuristic, to generate the viterbi n-best parses with = 100, and, after removing the address labels, all equal parses and their probabilities were summed, and the one with highest probability chosen.</citsent>
<aftsection>
<nextsent>33trees that best explain the over representation of certain constructions in the data.
</nextsent>
<nextsent>for instance, in runwith = 4, ? = 1.0, = 1.0, the 50 elementary trees with the highest scores, as shown in figure 1, are all exemplary of frequent formulas in the atis corpus such as show me x?, id like to x?, which of these?, what is the x?, cheapest fare?
</nextsent>
<nextsent>and flights from to y?.
</nextsent>
<nextsent>in short, the push-n-pullalgorithm ? while starting out considering all possible subtrees ? converges to grammar which makes linguistically relevant generalizations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC67">
<title id=" W06-1641.xml">sentiment retrieval using generative models </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the recent rapid expansion of access to information has significantly increased the demands on retrieval or classification of sentiment information from large amount of textual data.
</prevsent>
<prevsent>the field of sentiment classification has recently received considerable attention, where the polarities of sentiment, such as positive or negative, were identified from unstructured text (shanahan et al, 2005).
</prevsent>
</prevsection>
<citsent citstr=" W02-1011 ">
a number of studies have investigated sentiment classification at document level, e.g., (pang et al, 2002; <papid> W02-1011 </papid>dave et al, 2003), and at sentence level, e.g., (hu and liu, 2004; kim and hovy, 2004; <papid> C04-1200 </papid>nigam and hurst, 2005); however, the accuracy is still less than desirable.</citsent>
<aftsection>
<nextsent>therefore, ranking according to the likelihood of containing sentiment information is expected to serve crucial function in helping users.
</nextsent>
<nextsent>we believe that our work is the first attempt at sentiment retrieval that aims at finding sentences containing information with specific sentiment polarity on certain topic.
</nextsent>
<nextsent>intuitively, the expression of sentiment in textis dependent on the topic.
</nextsent>
<nextsent>for example, negative view for some voting event may be expressed using flaw?, while negative view for some politician may be expressed using reckless?.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC68">
<title id=" W06-1641.xml">sentiment retrieval using generative models </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the recent rapid expansion of access to information has significantly increased the demands on retrieval or classification of sentiment information from large amount of textual data.
</prevsent>
<prevsent>the field of sentiment classification has recently received considerable attention, where the polarities of sentiment, such as positive or negative, were identified from unstructured text (shanahan et al, 2005).
</prevsent>
</prevsection>
<citsent citstr=" C04-1200 ">
a number of studies have investigated sentiment classification at document level, e.g., (pang et al, 2002; <papid> W02-1011 </papid>dave et al, 2003), and at sentence level, e.g., (hu and liu, 2004; kim and hovy, 2004; <papid> C04-1200 </papid>nigam and hurst, 2005); however, the accuracy is still less than desirable.</citsent>
<aftsection>
<nextsent>therefore, ranking according to the likelihood of containing sentiment information is expected to serve crucial function in helping users.
</nextsent>
<nextsent>we believe that our work is the first attempt at sentiment retrieval that aims at finding sentences containing information with specific sentiment polarity on certain topic.
</nextsent>
<nextsent>intuitively, the expression of sentiment in textis dependent on the topic.
</nextsent>
<nextsent>for example, negative view for some voting event may be expressed using flaw?, while negative view for some politician may be expressed using reckless?.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC69">
<title id=" W06-1641.xml">sentiment retrieval using generative models </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>moreover, sentiment polarities are also dependent on topic sor domains.
</prevsent>
<prevsent>for example, the adjective unpredictable?
</prevsent>
</prevsection>
<citsent citstr=" P02-1053 ">
may have negative orientation in an automotive review, in phrase such as unpredictable steering?, but it could have positive orientation in movie review, in phrase such as unpredictableplot?, as mentioned in (turney, 2002) <papid> P02-1053 </papid>in the context of his sentiment word detection.</citsent>
<aftsection>
<nextsent>we propose sentiment retrieval models in the framework of generative language modeling, not only assuming query terms expressing certain topic, but also assuming that the polarity of sentiment interest is specified by the user in some manner, where the topic dependence of the sentiment is considered.
</nextsent>
<nextsent>to the best of our knowledge, there have been no other studies on retrieval model unifying both topic and sentiment, and further, there have been no other studies on sentiment retrieval.
</nextsent>
<nextsent>the sentiment information of ten appears as local in document, and therefore focusing on finer levels, i.e., sentence or passage levels rather than document level, is crucial.
</nextsent>
<nextsent>wethus experiment on sentiment retrieval at the sentence level in this paper.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC70">
<title id=" W06-1641.xml">sentiment retrieval using generative models </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>section 6 concludes the paper.
</prevsent>
<prevsent>some efforts for the trec novelty track were related to our work.
</prevsent>
</prevsection>
<citsent citstr=" H05-1086 ">
although some of the topics used in the novelty track in 2003 and 2004 (soboroff and harman, 2003; soboroff, 2004) were related to opinions, most of the efforts were focused on topic, such as studies using term distribution within each sentence, e.g., (allan et al, 2003; losada, 2005; murdock and croft, 2005).<papid> H05-1086 </papid></citsent>
<aftsection>
<nextsent>amongst the participants in the trec novelty track, only (kim et al, 2004) proposed method specialized to opinion-bearing sentence retrieval, by making use of lists of words with positive or negative polarities.
</nextsent>
<nextsent>they aimed to find opinions on given topic but did not distinguish or did notcare about sentiment polarities that should be represented in some sentences (hereafter, opinion re trieval).
</nextsent>
<nextsent>we focus on finding positive views or negative views according to given topic and sentiment of interest (hereafter, sentiment retrieval).
</nextsent>
<nextsent>our work is the first work on sentiment retrieval, to the best of our knowledge.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC71">
<title id=" W06-1641.xml">sentiment retrieval using generative models </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in the context of sentiment classification, some researchers have conducted studies on the topic dependence of sentiment polarities.
</prevsent>
<prevsent>(nasukawaand yi, 2003) and (yi et al, 2003) extracted positive or negative expressions on given product name using handmade lexicons.
</prevsent>
</prevsection>
<citsent citstr=" H05-1044 ">
(engstrom, 2004) studied how the topic dependence influences the accuracy of sentiment classification and attempted to reduce the influence to improve the accuracy.(wilson et al, 2005) <papid> H05-1044 </papid>investigated how context influences sentiment polarity at the phrase level in corpus, beginning with predefined list of words with polarities.</citsent>
<aftsection>
<nextsent>their focus on the phenomena of topic dependence of sentiment can be shared withour work; however, their work is not directly related to ours, because we focus on different task, sentiment retrieval, where different approaches are required.
</nextsent>
<nextsent>in this section we will provide formal underpinning for our approach to sentiment retrieval.
</nextsent>
<nextsent>the approach is based on the generative paradigm: we describe statistical process that could be viewed, hypothetically, as source of every statement of interest to our system.
</nextsent>
<nextsent>we stress that this generative process is to be treated as purely hypothetical; the process is only intended to reflect those aspects of human discourse that are pertinent to the problem of retrieving affect ively appropriate and topic relevant texts in response to query posed by our user.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC73">
<title id=" W06-1641.xml">sentiment retrieval using generative models </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>5.2.2 using automatic annotation in automatic extraction of sentiment expressions in this study, we detected sentiment-bearingwords using lists of words with established polarities.
</prevsent>
<prevsent>at this stage, topic dependence was not considered; however, at the stage of sentiment modeling, the topic dependence can be reflected, as described in sections 3 and 4.we first prepared list of words indicating sentiments.
</prevsent>
</prevsection>
<citsent citstr=" P97-1023 ">
we used hatzivassiloglou and mcke owns sentiment word list (hatzivassiloglou and mckeown, 1997), <papid> P97-1023 </papid>which consists of 657 positive and 679 negative adjectives, and the general inquirer (stone et al, 1966), which contains 1621 positive and 1989 negative words.7 by merging these lists, we obtained 1947 positive and 2348 negative words.</citsent>
<aftsection>
<nextsent>after stemming these words in the same manner as in section 5.2.1, we were left with 1667 positive and 2129 negative words, which we will use hereafter in this paper.the sentiment polarities are sometimes sensitive to the structural information, for instance, negation expression reverses the following sentiment polarity.
</nextsent>
<nextsent>to handle negation, every sentiment-bearing word was rewritten with aneg?
</nextsent>
<nextsent>suffix, such as good neg?, if an odd number of negation expressions was found within the five preceding words in the sentence.
</nextsent>
<nextsent>to detect negation expressions, we used predefined negation expression list.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC75">
<title id=" W06-3808.xml">seeing stars when there aren92t many stars graph based semi supervised learning for sentiment categorization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we then solve an optimization problem to obtain smooth rating function over the whole graph.
</prevsent>
<prevsent>when only limited labeled data is available, this method achieves significantly better predictive accuracy over other methods that ignore the unlabeled examples during training.
</prevsent>
</prevsection>
<citsent citstr=" P02-1053 ">
sentiment analysis of text documents has received considerable attention recently (shanahan et al, 2005; turney, 2002; <papid> P02-1053 </papid>dave et al, 2003; hu andliu, 2004; chaovalit and zhou, 2005).</citsent>
<aftsection>
<nextsent>unlike traditional text categorization based on topics, sentiment analysis attempts to identify the subjective sentiment expressed (or implied) in documents, such as consumer product or movie reviews.
</nextsent>
<nextsent>in particular pang and lee proposed the rating-inference problem(2005).
</nextsent>
<nextsent>rating inference is harder than binary positive / negative opinion classification.
</nextsent>
<nextsent>the goal is to infer numerical rating from reviews, for example the number of stars?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC76">
<title id=" W06-3808.xml">seeing stars when there aren92t many stars graph based semi supervised learning for sentiment categorization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>supervised learning algorithms trained on small labeled sets suffer in performance.
</prevsent>
<prevsent>can one use the unlabeled reviews to improve rating-inference?
</prevsent>
</prevsection>
<citsent citstr=" P05-1015 ">
pang and lee (2005) <papid> P05-1015 </papid>suggested that doing so should be useful.</citsent>
<aftsection>
<nextsent>we demonstrate that the answer is yes.?
</nextsent>
<nextsent>our approach is graph-based semi-supervised learning.
</nextsent>
<nextsent>semi-supervised learning is an active research area in machine learning.
</nextsent>
<nextsent>it builds better classifiers or regress ors using both labeled and unlabeled data, under appropriate assumptions (zhu, 2005; seeger, 2001).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC149">
<title id=" W06-3116.xml">mood at work ramses versus pharaoh </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>phrase-based (pb) machine translation (mt) is now popular paradigm, partly because of the relative ease with which we can automatically create an acceptable translation engine from bitext.
</prevsent>
<prevsent>as matter of fact, deriving such an engine from bitext consists in (more or less) gluing together dedicated software modules, often freely available.
</prevsent>
</prevsection>
<citsent citstr=" P00-1056 ">
word-based models, or the so-called ibm models, can be trained using the giza or giza++ tool kits (och and ney,2000).<papid> P00-1056 </papid></citsent>
<aftsection>
<nextsent>one can then train phrase-based models using the thot toolkit (ortiz-martnez et al, 2005).
</nextsent>
<nextsent>for their part, language models currently in use in smt systems can be trained using packages such as srilm (stolcke, 2002) and the cmu-slm toolkit (clarkson and rosenfeld, 1997).
</nextsent>
<nextsent>1www.statmt.org/wmt06/shared-task/ baseline.html once all the models are built, one can choo seto use pharaoh (koehn, 2004), an efficient full fledged phrase-based decoder.
</nextsent>
<nextsent>we only know of one major drawback when using pharaoh: its licensing policy.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC150">
<title id=" W06-3116.xml">mood at work ramses versus pharaoh </title>
<section> ramses versus pharaoh.  </section>
<citcontext>
<prevsection>
<prevsent>this means in practice that the language model was trained using the srilm toolkit (stolcke, 2002).
</prevsent>
<prevsent>the word alignment required to build the phrase table was produced with the giza++ package.
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
a viterbi alignment computed from an ibm model 4 (brown et al, 1993) <papid> J93-2003 </papid>was computed for each translation direction.</citsent>
<aftsection>
<nextsent>both alignments were then combined in heuristic way (koehn et al, ).
</nextsent>
<nextsent>each pair of phrases in the 2http://www.gnu.org/copyleft/gpl.html model is given 5 scores, described in the pharaoh training manual.3 to tune the coefficients of the log-linear combination that both pharaoh and ramses use when decoding, we used the organizers?
</nextsent>
<nextsent>minimum-error-rate-training.perl script.
</nextsent>
<nextsent>this tuning step was performed on the first 500 sentences of the dedicated development corpora.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC151">
<title id=" W06-3116.xml">mood at work ramses versus pharaoh </title>
<section> ramses versus pharaoh.  </section>
<citcontext>
<prevsection>
<prevsent>in contrast, on the spanish-to-english translation direction, where the two decoders offer similar performances, the difference between bleu scores never exceeds 0.23%.
</prevsent>
<prevsent>expectedly, spanish and french are much easier to translate than german.
</prevsent>
</prevsection>
<citsent citstr=" P05-1066 ">
this is because, in this study, we did not apply any pre-processing strategy that we know can improve performances, such as clause reordering or compound-word splitting (collins et al, 2005; <papid> P05-1066 </papid>langlais et al, 2005).<papid> W05-0824 </papid></citsent>
<aftsection>
<nextsent>table 2 shows that it does not seem much more difficult to translate into english than from english.
</nextsent>
<nextsent>this is surprising: translating into morphologically richer language should be more challenging.
</nextsent>
<nextsent>the opposite is true for german here: without doing any thing specific for this language, it is much easier to translate from german to english than the other wayaround.
</nextsent>
<nextsent>this may be attributed in part to the language model: for the test corpus, the perplexity of the language models provided is 105.5 for german, compared to 59.7 for english.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC152">
<title id=" W06-3116.xml">mood at work ramses versus pharaoh </title>
<section> ramses versus pharaoh.  </section>
<citcontext>
<prevsection>
<prevsent>in contrast, on the spanish-to-english translation direction, where the two decoders offer similar performances, the difference between bleu scores never exceeds 0.23%.
</prevsent>
<prevsent>expectedly, spanish and french are much easier to translate than german.
</prevsent>
</prevsection>
<citsent citstr=" W05-0824 ">
this is because, in this study, we did not apply any pre-processing strategy that we know can improve performances, such as clause reordering or compound-word splitting (collins et al, 2005; <papid> P05-1066 </papid>langlais et al, 2005).<papid> W05-0824 </papid></citsent>
<aftsection>
<nextsent>table 2 shows that it does not seem much more difficult to translate into english than from english.
</nextsent>
<nextsent>this is surprising: translating into morphologically richer language should be more challenging.
</nextsent>
<nextsent>the opposite is true for german here: without doing any thing specific for this language, it is much easier to translate from german to english than the other wayaround.
</nextsent>
<nextsent>this may be attributed in part to the language model: for the test corpus, the perplexity of the language models provided is 105.5 for german, compared to 59.7 for english.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC153">
<title id=" W07-0406.xml">machine translation as tree labeling </title>
<section> motivation </section>
<citcontext>
<prevsection>
<prevsent>the optimal tree labeling (i.e. translation) is then found through simple depth-first branch-and bound search.
</prevsent>
<prevsent>an early system founded on these ideas has been shown to be competitive with pharaoh when both are trained on small subsection of the eu roparl corpus.
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
statistical machine translation has, for while now,been dominated by the phrase-based translation paradigm (och and ney, 2003).<papid> J03-1002 </papid></citsent>
<aftsection>
<nextsent>in this paradigm, sentences are translated from source language to target language through the repeated substitution of contiguous word sequences (phrases?)
</nextsent>
<nextsent>from the source language for word sequences in the target language.
</nextsent>
<nextsent>training of the phrase translation model builds on top of standard statistical word alignment over the training corpus for identifying corresponding word blocks, assuming no further linguistic analysis of the source or target language.
</nextsent>
<nextsent>in decoding, these systems then typically relyon n-gram language models and simple statistical reordering models to shuffle the phrases into an order that is coherent in the target language.there are limits to what such an approach can ultimately achieve.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC154">
<title id=" W07-0406.xml">machine translation as tree labeling </title>
<section> motivation </section>
<citcontext>
<prevsection>
<prevsent>training of the phrase translation model builds on top of standard statistical word alignment over the training corpus for identifying corresponding word blocks, assuming no further linguistic analysis of the source or target language.
</prevsent>
<prevsent>in decoding, these systems then typically relyon n-gram language models and simple statistical reordering models to shuffle the phrases into an order that is coherent in the target language.there are limits to what such an approach can ultimately achieve.
</prevsent>
</prevsection>
<citsent citstr=" J97-3002 ">
machine translation based on adeeper analysis of the syntactic structure of sentence has long been identified as desirable objective in principle (consider (wu, 1997; <papid> J97-3002 </papid>yamada and knight, 2001)).<papid> P01-1067 </papid></citsent>
<aftsection>
<nextsent>however, attempts to retrofit syntactic information into the phrase-based paradigm have not met with enormous success (koehn et al, 2003; <papid> N03-1017 </papid>och et al, 2003)1, and purely phrase-based machine translation systems continue to outperform these syntax/phrase-based hybrids.</nextsent>
<nextsent>in this work, we try to make fresh start with syntax-based machine translation, discarding the phrase-based paradigm and designing machine translation system from the ground up, using syntax as our central guiding star.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC155">
<title id=" W07-0406.xml">machine translation as tree labeling </title>
<section> motivation </section>
<citcontext>
<prevsection>
<prevsent>training of the phrase translation model builds on top of standard statistical word alignment over the training corpus for identifying corresponding word blocks, assuming no further linguistic analysis of the source or target language.
</prevsent>
<prevsent>in decoding, these systems then typically relyon n-gram language models and simple statistical reordering models to shuffle the phrases into an order that is coherent in the target language.there are limits to what such an approach can ultimately achieve.
</prevsent>
</prevsection>
<citsent citstr=" P01-1067 ">
machine translation based on adeeper analysis of the syntactic structure of sentence has long been identified as desirable objective in principle (consider (wu, 1997; <papid> J97-3002 </papid>yamada and knight, 2001)).<papid> P01-1067 </papid></citsent>
<aftsection>
<nextsent>however, attempts to retrofit syntactic information into the phrase-based paradigm have not met with enormous success (koehn et al, 2003; <papid> N03-1017 </papid>och et al, 2003)1, and purely phrase-based machine translation systems continue to outperform these syntax/phrase-based hybrids.</nextsent>
<nextsent>in this work, we try to make fresh start with syntax-based machine translation, discarding the phrase-based paradigm and designing machine translation system from the ground up, using syntax as our central guiding star.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC156">
<title id=" W07-0406.xml">machine translation as tree labeling </title>
<section> motivation </section>
<citcontext>
<prevsection>
<prevsent>in decoding, these systems then typically relyon n-gram language models and simple statistical reordering models to shuffle the phrases into an order that is coherent in the target language.there are limits to what such an approach can ultimately achieve.
</prevsent>
<prevsent>machine translation based on adeeper analysis of the syntactic structure of sentence has long been identified as desirable objective in principle (consider (wu, 1997; <papid> J97-3002 </papid>yamada and knight, 2001)).<papid> P01-1067 </papid></prevsent>
</prevsection>
<citsent citstr=" N03-1017 ">
however, attempts to retrofit syntactic information into the phrase-based paradigm have not met with enormous success (koehn et al, 2003; <papid> N03-1017 </papid>och et al, 2003)1, and purely phrase-based machine translation systems continue to outperform these syntax/phrase-based hybrids.</citsent>
<aftsection>
<nextsent>in this work, we try to make fresh start with syntax-based machine translation, discarding the phrase-based paradigm and designing machine translation system from the ground up, using syntax as our central guiding star.
</nextsent>
<nextsent>evaluation with bleu and detailed manual error analysis of our nascent system show that this new approach might well have the potential to finally realize some of the promises of syntax.
</nextsent>
<nextsent>we want to build system that can learn to translate sentences from source language to destination language.
</nextsent>
<nextsent>as our first step, we will assume that the system will be learning from corpus consisting of triples f, e, a?, where: (i) is sentence from our source language, which is parsed (the words of the sentence and the nodes of the parse tree may or may not be annotated with auxiliary information), (ii) is gold-standard translation of sentence (the wordsof sentence may or may not be annotated with auxiliary information), and (iii) is an automatically generated word alignment (e.g. via giza++) between source sentence and destination sentence e.1(chiang, 2005) <papid> P05-1033 </papid>also reports that with his hierarchical generalization of the phrase-based approach, the addition of parser information doesnt lead to any improvements.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC157">
<title id=" W07-0406.xml">machine translation as tree labeling </title>
<section> problem formulation.  </section>
<citcontext>
<prevsection>
<prevsent>evaluation with bleu and detailed manual error analysis of our nascent system show that this new approach might well have the potential to finally realize some of the promises of syntax.
</prevsent>
<prevsent>we want to build system that can learn to translate sentences from source language to destination language.
</prevsent>
</prevsection>
<citsent citstr=" P05-1033 ">
as our first step, we will assume that the system will be learning from corpus consisting of triples f, e, a?, where: (i) is sentence from our source language, which is parsed (the words of the sentence and the nodes of the parse tree may or may not be annotated with auxiliary information), (ii) is gold-standard translation of sentence (the wordsof sentence may or may not be annotated with auxiliary information), and (iii) is an automatically generated word alignment (e.g. via giza++) between source sentence and destination sentence e.1(chiang, 2005) <papid> P05-1033 </papid>also reports that with his hierarchical generalization of the phrase-based approach, the addition of parser information doesnt lead to any improvements.</citsent>
<aftsection>
<nextsent>41 figure 1: example translation object.
</nextsent>
<nextsent>let us refer to these triples as translation objects.the learning task is: using the training data, produce scoring function that assigns score toevery translation object f, e, a?, such that this scoring function assigns high score to good translations, and low score to poor ones.
</nextsent>
<nextsent>the decoding task is: given scoring function and an arbitrary sentence from the source language, find translation object f, e, a?
</nextsent>
<nextsent>that maximizes (f, e, a?).to facilitate matters, we will map translation objects to an alternate representation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC158">
<title id=" W07-0406.xml">machine translation as tree labeling </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>our training corpus contained 50000 sentences and our test corpus contained 300 sentences.
</prevsent>
<prevsent>we also had small number of reserved sentences for development.
</prevsent>
</prevsection>
<citsent citstr=" J04-4004 ">
the english sentences were parsed using the bikel parser (bikel, 2004), <papid> J04-4004 </papid>and the sentences were aligned with giza++ (och and ney,2000).<papid> P00-1056 </papid></citsent>
<aftsection>
<nextsent>we used the weka machine learning package (witten and frank, 2005) to train the distributions (specifically, we used model trees).
</nextsent>
<nextsent>for comparison, we also trained and evaluated pharaoh (koehn, 2005) on this limited corpus, using pharaohs default parameters.
</nextsent>
<nextsent>pharaoh achieved bleu score of 11.17 on the test set, whereas our system achieved bleu score of 11.52.
</nextsent>
<nextsent>what is notable here is not the scores themselves (low due to the size of the training corpus).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC159">
<title id=" W07-0406.xml">machine translation as tree labeling </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>our training corpus contained 50000 sentences and our test corpus contained 300 sentences.
</prevsent>
<prevsent>we also had small number of reserved sentences for development.
</prevsent>
</prevsection>
<citsent citstr=" P00-1056 ">
the english sentences were parsed using the bikel parser (bikel, 2004), <papid> J04-4004 </papid>and the sentences were aligned with giza++ (och and ney,2000).<papid> P00-1056 </papid></citsent>
<aftsection>
<nextsent>we used the weka machine learning package (witten and frank, 2005) to train the distributions (specifically, we used model trees).
</nextsent>
<nextsent>for comparison, we also trained and evaluated pharaoh (koehn, 2005) on this limited corpus, using pharaohs default parameters.
</nextsent>
<nextsent>pharaoh achieved bleu score of 11.17 on the test set, whereas our system achieved bleu score of 11.52.
</nextsent>
<nextsent>what is notable here is not the scores themselves (low due to the size of the training corpus).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC160">
<title id=" W06-1905.xml">keyword translation accuracy and cross lingual question answering in chinese and japanese </title>
<section> extension for cross-lingual qa.  </section>
<citcontext>
<prevsection>
<prevsent>3.1 question analyzer.
</prevsent>
<prevsent>the question analyzer (qa) is responsible for extracting information from the input question in order to formulate representation of the eacl 2006 workshop on multilingual question answering - mlqa06 31 figure1: javelin monolingual architecture figure2: javelin architecture with cross-lingual extension information required to answer the question.
</prevsent>
</prevsection>
<citsent citstr=" W04-2606 ">
input questions are processed using the rasp parser (korhonen and briscoe, 2004), <papid> W04-2606 </papid>and the module output contains three main components: a) selected keywords; b) the answer type (e.g. numeric-expression, person-name, location); and c) the answer subtype (e.g. author, river, city).</citsent>
<aftsection>
<nextsent>the selected keywords are words or phrases which are expected to appear in documents with correct answers.
</nextsent>
<nextsent>in order to reduce noise in the document retrieval phase, we use stop-word lists to eliminate high-frequency terms; for example, the term old?
</nextsent>
<nextsent>is not included as keyword for how-old?
</nextsent>
<nextsent>questions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC161">
<title id=" W06-1905.xml">keyword translation accuracy and cross lingual question answering in chinese and japanese </title>
<section> extension for cross-lingual qa.  </section>
<citcontext>
<prevsection>
<prevsent>3.2 translation module.
</prevsent>
<prevsent>the translation module (tm) is used by the qa module to translate keywords into the language of the target corpus.
</prevsent>
</prevsection>
<citsent citstr=" W01-1413 ">
instead of combining multiple translation candidates with disjunctive query operator (isozaki et al, 2005), the tm selects the best combination of translated keywords from several sources: machine readable dictionaries (mrds), machine translation systems (mts) and web-mining-based keyword translators (wbmts) (nagata et al, 2001, <papid> W01-1413 </papid>li et al, 2003).</citsent>
<aftsection>
<nextsent>for translation from english to japanese, we used two mrds, eight mts and one wbmt.
</nextsent>
<nextsent>if none of them return translation, the word is transliterated into kana for japanese (for details on transliteration, see section 5.2).
</nextsent>
<nextsent>for translation from english to chinese, we used one mrd, three mts and one wbmt.
</nextsent>
<nextsent>after gathering all possible translations for every keyword, the tm uses noisy channel model to select the best combination of translated keywords.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC162">
<title id=" W06-1905.xml">keyword translation accuracy and cross lingual question answering in chinese and japanese </title>
<section> extension for cross-lingual qa.  </section>
<citcontext>
<prevsection>
<prevsent>the noisy channel model: in the noisy channel model, an undistorted signal passes through noisy channel and becomes distorted.
</prevsent>
<prevsent>given the distorted signal, we are to find the original, undistorted signal.
</prevsent>
</prevsection>
<citsent citstr=" J90-2002 ">
ibm applied the noisy channel model idea to translation of sentences from aligned parallel corpora, where the source language sentence is the distorted signal, and the eacl 2006 workshop on multilingual question answering - mlqa06 32 target language sentence is the original signal (brown et al, 1990).<papid> J90-2002 </papid></citsent>
<aftsection>
<nextsent>we adopt this model for disambiguating keyword translation, with the source language keyword terms as the distorted signal and the target language terms as the original signal.
</nextsent>
<nextsent>the tm job is to find the target language terms given the source language terms, by finding the probability of the target language terms given the source language terms p(t|s).
</nextsent>
<nextsent>using bayes  rule, we can break the equation down to several components: )( )|()( )|( sp tsptp stp ?= because we are comparing probabilities of different translations of the same source keyword terms, we can simplify the problem to be: )|()()|( tsptpstp ?= we can now reduce the equation to two components.
</nextsent>
<nextsent>p(t) is the language model and p(s|t) is the translation model.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC163">
<title id=" W06-1671.xml">learning field compatibilities to extract database records from unstructured text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for example, we can populate multi-field records such as contact record [person=steve jobs, job title = ceo, company = apple, city = cupertino, state = ca].
</prevsent>
<prevsent>the relational information in these types of records presents greater opportunity for text analysis.the task of associating together entities is of ten framed as binary relation extraction task:given pair of entities, label the relation between them (e.g. steve jobs located-in cuper tino).
</prevsent>
</prevsection>
<citsent citstr=" P04-3022 ">
common approaches to relation extraction include pattern matching (brin, 1998; agichtein and gravano, 2000) and classification (zelenko et al., 2003; kambhatla, 2004).<papid> P04-3022 </papid></citsent>
<aftsection>
<nextsent>however, binary relation extraction alone is not well-suited for the contact record example above, which requires associating together many fields into one record.
</nextsent>
<nextsent>we refer to this task of piecing together many fields into single record as record extraction.
</nextsent>
<nextsent>consider the task of extracting contact records from personal homepages.
</nextsent>
<nextsent>an ner system may label all mentions of cities, people, organizations, phone numbers, job titles, etc. on page, from both semi-structured an unstructured text.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC164">
<title id=" W06-1671.xml">learning field compatibilities to extract database records from unstructured text </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in this paper, we describe method to induce probabilistic compatibility function between setsof fields.
</prevsent>
<prevsent>embedding this compatibility function within graph partitioning method, we describe how to cluster highly compatible fields into records.we evaluate our approach on personal home pages that have been manually annotated with contact record information, and demonstrate 53% error reduction over baseline methods.
</prevsent>
</prevsection>
<citsent citstr=" P05-1061 ">
mcdonald et al (2005) <papid> P05-1061 </papid>present clustering techniques to extract complex relations, i.e. relations with more than two arguments.</citsent>
<aftsection>
<nextsent>record extraction can be viewed as an instance of complex relation extraction.
</nextsent>
<nextsent>we build upon this work in three ways: (1) our system learns the compatibility between sets of fields, rather than just pairs of field; (2) our system is not restricted to relations between entities in the same sentence; and (3) our problem domain has varying number of fields per record, as opposed to the fixed schema in mcdonald et al (2005).<papid> P05-1061 </papid></nextsent>
<nextsent>bansal et al (2004) present algorithms for the related task of cor relational clustering: finding an optimal clustering from matrix of pairwise compatibility scores.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC167">
<title id=" W06-1671.xml">learning field compatibilities to extract database records from unstructured text </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>second, whereas mccallum and wellner (2005) factor the coreference decisions into pairs of vertices, our compatibility decisions are made between sets of vertices.
</prevsent>
<prevsent>as we show in our experiments, factoring decisions into sets of vertices enables more powerful features that can improve performance.
</prevsent>
</prevsection>
<citsent citstr=" W06-3606 ">
thesehigher-order features have also recently been investigated in other models of coreference, both discriminative (culotta and mccallum, 2006) <papid> W06-3606 </papid>and generative (milch et al, 2005).viola and narasimhan (2005) present probabilistic grammar to parse contact informationblocks.</citsent>
<aftsection>
<nextsent>while this model is capable of learning long-distance compatibilities (such as city and state relations), features to enable this are not explored.
</nextsent>
<nextsent>additionally, their work focuses on labeling fields in documents that have been pre segmented into records.
</nextsent>
<nextsent>this record segmentation is precisely what we address in this paper.
</nextsent>
<nextsent>borkar et al (2001) and kristjannson et al(2004) also label contact address blocks, but ignore the problem of clustering fields into records.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC168">
<title id=" W06-1671.xml">learning field compatibilities to extract database records from unstructured text </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>also, culotta et al (2004) automatically extract contact records from web pages, but use heuristics to cluster fields into records.embley et al (1999) provide heuristics to detect record boundaries in highly structured web documents, such as classified ads, and embley and xu (2000) improve upon these heuristics for slightly more ambiguous domains using vector space model.
</prevsent>
<prevsent>both of these techniques apply to data for which the records are highly contiguous and have distinctive separator between records.these heuristic approaches are unlikely to be successful in the unstructured text domain we address in this paper.
</prevsent>
</prevsection>
<citsent citstr=" A00-2030 ">
most other work on relation extraction focuses only on binary relations (zelenko et al, 2003; miller et al, 2000; <papid> A00-2030 </papid>agichtein and gravano, 2000; culotta and sorensen, 2004).<papid> P04-1054 </papid></citsent>
<aftsection>
<nextsent>a serious difficulty in applying binary relation extractors to the record extraction task is that rather than enumerating over all pairs of entities, the system must enumerate over all subsets of entities, up to subsets of size k, the maximum number of fields per record.
</nextsent>
<nextsent>we address this difficulty by employing two sampling methods: one that samples uniformly, and another that samples on focused subset of the combinatorial space.
</nextsent>
<nextsent>3.1 problem definition.
</nextsent>
<nextsent>let fieldf be pair a, v?, where is an attribute (column label) and is value, e.g. fi = city, san francisco?.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC169">
<title id=" W06-1671.xml">learning field compatibilities to extract database records from unstructured text </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>also, culotta et al (2004) automatically extract contact records from web pages, but use heuristics to cluster fields into records.embley et al (1999) provide heuristics to detect record boundaries in highly structured web documents, such as classified ads, and embley and xu (2000) improve upon these heuristics for slightly more ambiguous domains using vector space model.
</prevsent>
<prevsent>both of these techniques apply to data for which the records are highly contiguous and have distinctive separator between records.these heuristic approaches are unlikely to be successful in the unstructured text domain we address in this paper.
</prevsent>
</prevsection>
<citsent citstr=" P04-1054 ">
most other work on relation extraction focuses only on binary relations (zelenko et al, 2003; miller et al, 2000; <papid> A00-2030 </papid>agichtein and gravano, 2000; culotta and sorensen, 2004).<papid> P04-1054 </papid></citsent>
<aftsection>
<nextsent>a serious difficulty in applying binary relation extractors to the record extraction task is that rather than enumerating over all pairs of entities, the system must enumerate over all subsets of entities, up to subsets of size k, the maximum number of fields per record.
</nextsent>
<nextsent>we address this difficulty by employing two sampling methods: one that samples uniformly, and another that samples on focused subset of the combinatorial space.
</nextsent>
<nextsent>3.1 problem definition.
</nextsent>
<nextsent>let fieldf be pair a, v?, where is an attribute (column label) and is value, e.g. fi = city, san francisco?.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC172">
<title id=" W06-3303.xml">term generalization and synonym resolution for biological abstracts using the gene ontology for sub cellular localization prediction </title>
<section> methods.  </section>
<citcontext>
<prevsection>
<prevsent>approach.
</prevsent>
<prevsent>3.2 retrieve abstracts.
</prevsent>
</prevsection>
<citsent citstr=" W04-1212 ">
now that set of proteins with known loca liza tions has been created, we gather each proteins abstracts and abstract titles (figure 1, process a).we do not include full text because it can be difficult to obtain automatically and because using full text does not improve f-measure (sinclair and webber, 2004).<papid> W04-1212 </papid></citsent>
<aftsection>
<nextsent>abstracts for each protein are retrieved using the pubmed ids recorded in the swiss prot database.
</nextsent>
<nextsent>pubmed (http://www.pubmed.
</nextsent>
<nextsent>gov) is database of life science articles.
</nextsent>
<nextsent>it should be noted that more than one protein in swiss-prot may point to the same abstract in pubmed.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC173">
<title id=" W06-1662.xml">sentence ordering with manifold based classification in multi document summarization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>experiments demonstrate that the method is effective.
</prevsent>
<prevsent>sentence ordering has been concern in text planning and concept-to-text generation (reiter et al., 2000).
</prevsent>
</prevsection>
<citsent citstr=" P03-1069 ">
recently, it has also drawn attention in multi-document summarization (barzilay et al, 2002; lapata, 2003; <papid> P03-1069 </papid>bollegala et al, 2005).</citsent>
<aftsection>
<nextsent>since summary sentences generally come from different sources in multi-document summarization, an optimal ordering is crucial to make summaries coherent and readable.
</nextsent>
<nextsent>in general, the strategies for sentence ordering in multi-document summarization fall in two categories.
</nextsent>
<nextsent>one is chronological ordering (barzilay et al, 2002; bollegala et al, 2005), which is based on time-related features of the documents.
</nextsent>
<nextsent>however, such temporal features may be not available in all cases.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC175">
<title id=" W06-1506.xml">pied piping in relative clauses syntax and compositional semantics based on synchronous tree adjoining grammar </title>
<section> stag-based compositional semantics.  </section>
<citcontext>
<prevsection>
<prevsent>particularly, (a movie) is valid elementary tree, as noun can form an extended projection with dp, in line with the dp hypothesis.
</prevsent>
<prevsent>the proper name tree in (john) is paired with tree represent inga term in the semantics, and the attributive adjective tree in (good) is paired with an auxiliary treein the semantics that represents one-place predicate to be adjoined to another one-place predicate.
</prevsent>
</prevsection>
<citsent citstr=" C90-3045 ">
as for the syntax-semantics pairing of elementary trees for quantified dps, follow shieber and schabes (1990), <papid> C90-3045 </papid>and use tree local multi-componenttag (as defined in weir (1988)) in the seman tics.</citsent>
<aftsection>
<nextsent>thus, the dp in (a movie) is paired with multi-component set {(a movie), (a movie)}in the semantics: (a movie) provides an argument variable, and (a movie) provides the existential quantifier with the restriction and scope.the transitive tree in (saw) is paired with semantic tree representing formula that consists of two-place predicate and two term nodes.
</nextsent>
<nextsent>the links, shown with boxed numbers, guarantee that whatever substitutes into dpi, the corresponding semantic tree will substitute into the term node marked with 1 , and whatever substitutes into dpis paired up with multi-component set in the semantics where one of the components will substitute into the term node marked with 2 and the other will adjoin onto the node marked with 2 . the syntactic and semantic derivation trees are given in figure 2, and the derived trees are given in figure 3.
</nextsent>
<nextsent>i leave out the tree addresses in the semantic derivation tree, as these are determined by the links between the syntactic and semantic elementary trees.1 ?(3) (saw) (a movie) dp (good) np (john) dpi (3) (saw) {(a movie), (a movie)} (good) (john) ? figure 2: derivation trees for john saw good movie.
</nextsent>
<nextsent>the semantic derived trees can be reduced by applying ?-conversion, as the nodes dominate typed ?-expressions and terms.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC176">
<title id=" W06-1506.xml">pied piping in relative clauses syntax and compositional semantics based on synchronous tree adjoining grammar </title>
<section> stag-based compositional semantics.  </section>
<citcontext>
<prevsection>
<prevsent>when reducing semantic derived trees, in addition to ?-conversion, propose to use predicate modification, as defined in heim and kratzer (1998) in (4).
</prevsent>
<prevsent>(4) predicate modification if ? has the form ? ?
</prevsent>
</prevsection>
<citsent citstr=" J94-1004 ">
,1in sentences with more than one quantified dps, assume multiple adjoining (as defined in schabes and shieber (1994)) <papid> J94-1004 </papid>of quantifier trees at the same node, leaving the order unspecified.</citsent>
<aftsection>
<nextsent>this provides an underspecified representation and accounts for scope ambiguity.
</nextsent>
<nextsent>42 ?(3) tp dpi john t?
</nextsent>
<nextsent>t vp dp ti v?
</nextsent>
<nextsent>v saw dp a np adjp adj good np movie (3) x r x.good(x) x.movie(x) x r xy.saw(y, x) x john?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC177">
<title id=" W06-2604.xml">a multi classifier based document categorization system profiting from the singular value decomposition dimensionality reduction technique </title>
<section> proposed approach.  </section>
<citcontext>
<prevsection>
<prevsent>documents in the training and testing sets are represented in areduced dimensional vector space.
</prevsent>
<prevsent>different training databases are generated from the original train 2actually, this result is obtained for 118 categories which correspond to the 115 mentioned before and three more categories which have testing documents but no training document assigned.
</prevsent>
</prevsection>
<citsent citstr=" W05-0608 ">
results reported by r(90) top-10 (gao et al, 2003) 88.42 93.07 (kim et al, 2005) 87.11 92.21 (gliozzo and strapparava, 2005) <papid> W05-0608 </papid>92.80 table 2: f1 results reported for the reuters-21578 modapte split.</citsent>
<aftsection>
<nextsent>ing dataset in order to construct the multiclassifier.
</nextsent>
<nextsent>we use the k-nn classification algorithm, which according to each training database makes prediction for testing documents.
</nextsent>
<nextsent>finally, bayesian voting scheme is used in order to definitively as sign category labels to testing documents.in the rest of this section we make brief review of the svd dimensionality reduction technique, the k-nn algorithm and the combination of classifiers used.
</nextsent>
<nextsent>3.1 the svd dimensionality reduction.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC178">
<title id=" W06-2603.xml">decomposition kernels for natural language processing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>kernel methods (see e.g. shawe-taylor and cristianini, 2004) are well suited to handle learning tasks in structured domains as the statistical side of learning algorithm can be naturally decoupled from any representational details that are handled by the kernel function.
</prevsent>
<prevsent>as matter of facts, kernel-basedstatistical learning has gained substantial importance in the nlp field.
</prevsent>
</prevsection>
<citsent citstr=" P02-1034 ">
applications are numerous and diverse and include for example refinement of statistical parsers (collins and duffy, 2002), <papid> P02-1034 </papid>tagging named entities (cumby and roth, 2003; tsochantaridis et al, 2004), syntactic chunking(daume?</citsent>
<aftsection>
<nextsent>iii and marcu, 2005), extraction of relations between entities (zelenko et al, 2003; culotta and sorensen, 2004), <papid> P04-1054 </papid>semantic role labeling (moschitti, 2004).<papid> P04-1043 </papid></nextsent>
<nextsent>the literature is rich with examples of kernels on discrete data structures such as sequences (lodhi et al, 2002; leslie et al., 2002; cortes et al, 2004), trees (collins and duffy, 2002; <papid> P02-1034 </papid>kashima and koyanagi, 2002), and annotated graphs (gartner, 2003; smola and kon dor, 2003; kashima et al, 2003; horvath et al,2004).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC179">
<title id=" W06-2603.xml">decomposition kernels for natural language processing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>as matter of facts, kernel-basedstatistical learning has gained substantial importance in the nlp field.
</prevsent>
<prevsent>applications are numerous and diverse and include for example refinement of statistical parsers (collins and duffy, 2002), <papid> P02-1034 </papid>tagging named entities (cumby and roth, 2003; tsochantaridis et al, 2004), syntactic chunking(daume?</prevsent>
</prevsection>
<citsent citstr=" P04-1054 ">
iii and marcu, 2005), extraction of relations between entities (zelenko et al, 2003; culotta and sorensen, 2004), <papid> P04-1054 </papid>semantic role labeling (moschitti, 2004).<papid> P04-1043 </papid></citsent>
<aftsection>
<nextsent>the literature is rich with examples of kernels on discrete data structures such as sequences (lodhi et al, 2002; leslie et al., 2002; cortes et al, 2004), trees (collins and duffy, 2002; <papid> P02-1034 </papid>kashima and koyanagi, 2002), and annotated graphs (gartner, 2003; smola and kon dor, 2003; kashima et al, 2003; horvath et al,2004).</nextsent>
<nextsent>kernels of this kind can be almost invariably described as special cases of convolution and other decomposition kernels (haussler, 1999).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC180">
<title id=" W06-2603.xml">decomposition kernels for natural language processing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>as matter of facts, kernel-basedstatistical learning has gained substantial importance in the nlp field.
</prevsent>
<prevsent>applications are numerous and diverse and include for example refinement of statistical parsers (collins and duffy, 2002), <papid> P02-1034 </papid>tagging named entities (cumby and roth, 2003; tsochantaridis et al, 2004), syntactic chunking(daume?</prevsent>
</prevsection>
<citsent citstr=" P04-1043 ">
iii and marcu, 2005), extraction of relations between entities (zelenko et al, 2003; culotta and sorensen, 2004), <papid> P04-1054 </papid>semantic role labeling (moschitti, 2004).<papid> P04-1043 </papid></citsent>
<aftsection>
<nextsent>the literature is rich with examples of kernels on discrete data structures such as sequences (lodhi et al, 2002; leslie et al., 2002; cortes et al, 2004), trees (collins and duffy, 2002; <papid> P02-1034 </papid>kashima and koyanagi, 2002), and annotated graphs (gartner, 2003; smola and kon dor, 2003; kashima et al, 2003; horvath et al,2004).</nextsent>
<nextsent>kernels of this kind can be almost invariably described as special cases of convolution and other decomposition kernels (haussler, 1999).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC182">
<title id=" W06-2603.xml">decomposition kernels for natural language processing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>how ever, excessively large feature spaces may result from the combinatorial growth of the number of distinct subparts with their size.
</prevsent>
<prevsent>when too many dimensions in the feature space are irrelevant, the gram matrix will be nearly diagonal (scholkopf et al, 2002), adversely affecting generalization inspite of using large margin classifiers (ben-david et al, 2002).
</prevsent>
</prevsection>
<citsent citstr=" P04-1016 ">
possible cures include extensive useof prior knowledge to guide the choice of relevant parts (cumby and roth, 2003; frasconi et al, 2004), the use of feature selection (suzuki et al, 2004), <papid> P04-1016 </papid>and soft matches (saunders et al, 2002).</citsent>
<aftsection>
<nextsent>in (menchetti et al, 2005) we have shown that better generalization can indeed be achieved by avoiding hard comparisons between large parts.
</nextsent>
<nextsent>in 17 weighted decomposition kernel (wdk) only small parts are matched, whereas the importance of the match is determined by comparing the sufficient statistics of elementary probabilistic models fitted on larger contextual substructures.
</nextsent>
<nextsent>here we introduce position-dependent version of wdk that can solve sequence labeling problems without searching the output space, as required by other recently proposed kernel-based solutions (tsochan taridis et al, 2004; daume?
</nextsent>
<nextsent>iii and marcu, 2005).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC183">
<title id=" W06-2603.xml">decomposition kernels for natural language processing </title>
<section> kernels for word semantic ambiguity.  </section>
<citcontext>
<prevsection>
<prevsent>the different lexical categories (verbs, nouns, adjectives and adverbs) are contained indistinct hierarchies and each one is rooted by many synsets.
</prevsent>
<prevsent>several metrics have been devised to computea similarity score between two words using wordnet.
</prevsent>
</prevsection>
<citsent citstr=" W05-0601 ">
in the following we resort to multi set version of the proximity measure used in (siolas anddalche buc, 2000), though more refined alternatives are also possible (for example using the conceptual density as in (basili et al, 2005)).<papid> W05-0601 </papid></citsent>
<aftsection>
<nextsent>given the acyclic nature of the semantic hierarchies, each synset can be represented by group of paths that follows the hypernym relations and finish in one ofthe top level concepts.
</nextsent>
<nextsent>two paths can then be compared by counting how many steps from the roots they have in common.
</nextsent>
<nextsent>this number must then be normalized dividing by the square root of the product between the path lengths.
</nextsent>
<nextsent>in this way one can accounts for the un balancing that arise from different parts of the hierarchies being differently detailed.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC184">
<title id=" W06-2603.xml">decomposition kernels for natural language processing </title>
<section> kernels for word semantic ambiguity.  </section>
<citcontext>
<prevsection>
<prevsent>(9) where ? are the synsets associated to the word ?.
</prevsent>
<prevsent>5.2 pp attachment experimental results.
</prevsent>
</prevsection>
<citsent citstr=" H94-1048 ">
the experiments have been performed using thewall-street journal dataset described in (ratna parkhi et al, 1994).<papid> H94-1048 </papid></citsent>
<aftsection>
<nextsent>this dataset contains 20, 800 training examples and 3, 097 testing examples.
</nextsent>
<nextsent>each phrase in the dataset is reduced to verb xv, its object noun xn1 and prepositional phrase formed by preposition xp and noun xn2 . the target is either or whether the phrase is attached to the verb or the noun.
</nextsent>
<nextsent>data have been preprocessed by assigning to all the words their corresponding synsets.
</nextsent>
<nextsent>additional meanings derived from specific synsets have been attached to the words as described in (stetina and nagao, 1997).<papid> W97-0109 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC185">
<title id=" W06-2603.xml">decomposition kernels for natural language processing </title>
<section> kernels for word semantic ambiguity.  </section>
<citcontext>
<prevsection>
<prevsent>each phrase in the dataset is reduced to verb xv, its object noun xn1 and prepositional phrase formed by preposition xp and noun xn2 . the target is either or whether the phrase is attached to the verb or the noun.
</prevsent>
<prevsent>data have been preprocessed by assigning to all the words their corresponding synsets.
</prevsent>
</prevsection>
<citsent citstr=" W97-0109 ">
additional meanings derived from specific synsets have been attached to the words as described in (stetina and nagao, 1997).<papid> W97-0109 </papid></citsent>
<aftsection>
<nextsent>the kernel between two phrases and x?
</nextsent>
<nextsent>is then computed by combining the kernels between single words using either the sum or the product.
</nextsent>
<nextsent>21 method acc pre rec 84.6% ? 0.65% 90.8% 82.2% 84.8% ? 0.65% 92.2% 81.0% sw 85.4% ? 0.64% 90.9% 83.6% swl 85.3% ? 0.64% 91.1% 83.2% pw 85.9% ? 0.63% 92.2% 83.1% pwl 86.2% ? 0.62% 92.1% 83.7% table 3: summary of the experimental results onthe pp attachment problem for various kernel parameters.
</nextsent>
<nextsent>results of the experiments are reported in tab.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC186">
<title id=" W06-3102.xml">initial explorations in english to turkish statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we find that this approach coupled with simple grouping of most frequent morphemes and function words on both sides improve the bleu score from the baseline of 0.0752 to 0.0913 with the small training data.
</prevsent>
<prevsent>we close with discussion on why one should not expect distortion parameters to model word-local morpheme ordering and that anew approach to handling complex mor pho tactics is needed.
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
the availability of large amounts of so-called parallel texts has motivated the application of statistical techniques to the problem of machine translation starting with the seminal work at ibm in the early90s (brown et al , 1992; brown et al , 1993).<papid> J93-2003 </papid></citsent>
<aftsection>
<nextsent>statistical machine translation views the translation process as noisy-channel signal recovery process in which one tries to recover the input signal?
</nextsent>
<nextsent>e, from the observed output signal f.1 early statistical machine translation systems used purely word-based approach without taking into account any of the morphological or syntactic properties of the languages (brown et al , 1993).<papid> J93-2003 </papid></nextsent>
<nextsent>limitations of basic word-based models prompted researchers to exploit morphological and/or syntac tic/phrasal structure (niessen and ney, (2004), <papid> J04-2003 </papid>lee,(2004), <papid> N04-4015 </papid>yamada and knight (2001), <papid> P01-1067 </papid>marcu and wong (2002), <papid> W02-1018 </papid>och and ney (2004),<papid> J04-4002 </papid>koehn et al  (2003), <papid> N03-1017 </papid>among others.)in the context of the agglutinative languages similar to turkish (in at least morphological aspects) , there has been some recent work on translating from and to finnish with the significant amount of datain the europarl corpus.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC190">
<title id=" W06-3102.xml">initial explorations in english to turkish statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>statistical machine translation views the translation process as noisy-channel signal recovery process in which one tries to recover the input signal?
</prevsent>
<prevsent>e, from the observed output signal f.1 early statistical machine translation systems used purely word-based approach without taking into account any of the morphological or syntactic properties of the languages (brown et al , 1993).<papid> J93-2003 </papid></prevsent>
</prevsection>
<citsent citstr=" J04-2003 ">
limitations of basic word-based models prompted researchers to exploit morphological and/or syntac tic/phrasal structure (niessen and ney, (2004), <papid> J04-2003 </papid>lee,(2004), <papid> N04-4015 </papid>yamada and knight (2001), <papid> P01-1067 </papid>marcu and wong (2002), <papid> W02-1018 </papid>och and ney (2004),<papid> J04-4002 </papid>koehn et al  (2003), <papid> N03-1017 </papid>among others.)in the context of the agglutinative languages similar to turkish (in at least morphological aspects) , there has been some recent work on translating from and to finnish with the significant amount of datain the europarl corpus.</citsent>
<aftsection>
<nextsent>although the bleu (pap ineni et al , 2002) <papid> P02-1040 </papid>score from finnish to english is 21.8, the score in the reverse direction is report edas 13.0 which is one of the lowest scores in 11 european languages scores (koehn, 2005).</nextsent>
<nextsent>also, reported from and to translation scores for finnish are the lowest on average, even with the large number of 1denoting english and french as used in the original ibm project which translated from french to english using the parallel text of the hansa rds, the canadian parliament proceedings.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC191">
<title id=" W06-3102.xml">initial explorations in english to turkish statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>statistical machine translation views the translation process as noisy-channel signal recovery process in which one tries to recover the input signal?
</prevsent>
<prevsent>e, from the observed output signal f.1 early statistical machine translation systems used purely word-based approach without taking into account any of the morphological or syntactic properties of the languages (brown et al , 1993).<papid> J93-2003 </papid></prevsent>
</prevsection>
<citsent citstr=" N04-4015 ">
limitations of basic word-based models prompted researchers to exploit morphological and/or syntac tic/phrasal structure (niessen and ney, (2004), <papid> J04-2003 </papid>lee,(2004), <papid> N04-4015 </papid>yamada and knight (2001), <papid> P01-1067 </papid>marcu and wong (2002), <papid> W02-1018 </papid>och and ney (2004),<papid> J04-4002 </papid>koehn et al  (2003), <papid> N03-1017 </papid>among others.)in the context of the agglutinative languages similar to turkish (in at least morphological aspects) , there has been some recent work on translating from and to finnish with the significant amount of datain the europarl corpus.</citsent>
<aftsection>
<nextsent>although the bleu (pap ineni et al , 2002) <papid> P02-1040 </papid>score from finnish to english is 21.8, the score in the reverse direction is report edas 13.0 which is one of the lowest scores in 11 european languages scores (koehn, 2005).</nextsent>
<nextsent>also, reported from and to translation scores for finnish are the lowest on average, even with the large number of 1denoting english and french as used in the original ibm project which translated from french to english using the parallel text of the hansa rds, the canadian parliament proceedings.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC192">
<title id=" W06-3102.xml">initial explorations in english to turkish statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>statistical machine translation views the translation process as noisy-channel signal recovery process in which one tries to recover the input signal?
</prevsent>
<prevsent>e, from the observed output signal f.1 early statistical machine translation systems used purely word-based approach without taking into account any of the morphological or syntactic properties of the languages (brown et al , 1993).<papid> J93-2003 </papid></prevsent>
</prevsection>
<citsent citstr=" P01-1067 ">
limitations of basic word-based models prompted researchers to exploit morphological and/or syntac tic/phrasal structure (niessen and ney, (2004), <papid> J04-2003 </papid>lee,(2004), <papid> N04-4015 </papid>yamada and knight (2001), <papid> P01-1067 </papid>marcu and wong (2002), <papid> W02-1018 </papid>och and ney (2004),<papid> J04-4002 </papid>koehn et al  (2003), <papid> N03-1017 </papid>among others.)in the context of the agglutinative languages similar to turkish (in at least morphological aspects) , there has been some recent work on translating from and to finnish with the significant amount of datain the europarl corpus.</citsent>
<aftsection>
<nextsent>although the bleu (pap ineni et al , 2002) <papid> P02-1040 </papid>score from finnish to english is 21.8, the score in the reverse direction is report edas 13.0 which is one of the lowest scores in 11 european languages scores (koehn, 2005).</nextsent>
<nextsent>also, reported from and to translation scores for finnish are the lowest on average, even with the large number of 1denoting english and french as used in the original ibm project which translated from french to english using the parallel text of the hansa rds, the canadian parliament proceedings.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC193">
<title id=" W06-3102.xml">initial explorations in english to turkish statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>statistical machine translation views the translation process as noisy-channel signal recovery process in which one tries to recover the input signal?
</prevsent>
<prevsent>e, from the observed output signal f.1 early statistical machine translation systems used purely word-based approach without taking into account any of the morphological or syntactic properties of the languages (brown et al , 1993).<papid> J93-2003 </papid></prevsent>
</prevsection>
<citsent citstr=" W02-1018 ">
limitations of basic word-based models prompted researchers to exploit morphological and/or syntac tic/phrasal structure (niessen and ney, (2004), <papid> J04-2003 </papid>lee,(2004), <papid> N04-4015 </papid>yamada and knight (2001), <papid> P01-1067 </papid>marcu and wong (2002), <papid> W02-1018 </papid>och and ney (2004),<papid> J04-4002 </papid>koehn et al  (2003), <papid> N03-1017 </papid>among others.)in the context of the agglutinative languages similar to turkish (in at least morphological aspects) , there has been some recent work on translating from and to finnish with the significant amount of datain the europarl corpus.</citsent>
<aftsection>
<nextsent>although the bleu (pap ineni et al , 2002) <papid> P02-1040 </papid>score from finnish to english is 21.8, the score in the reverse direction is report edas 13.0 which is one of the lowest scores in 11 european languages scores (koehn, 2005).</nextsent>
<nextsent>also, reported from and to translation scores for finnish are the lowest on average, even with the large number of 1denoting english and french as used in the original ibm project which translated from french to english using the parallel text of the hansa rds, the canadian parliament proceedings.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC194">
<title id=" W06-3102.xml">initial explorations in english to turkish statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>statistical machine translation views the translation process as noisy-channel signal recovery process in which one tries to recover the input signal?
</prevsent>
<prevsent>e, from the observed output signal f.1 early statistical machine translation systems used purely word-based approach without taking into account any of the morphological or syntactic properties of the languages (brown et al , 1993).<papid> J93-2003 </papid></prevsent>
</prevsection>
<citsent citstr=" J04-4002 ">
limitations of basic word-based models prompted researchers to exploit morphological and/or syntac tic/phrasal structure (niessen and ney, (2004), <papid> J04-2003 </papid>lee,(2004), <papid> N04-4015 </papid>yamada and knight (2001), <papid> P01-1067 </papid>marcu and wong (2002), <papid> W02-1018 </papid>och and ney (2004),<papid> J04-4002 </papid>koehn et al  (2003), <papid> N03-1017 </papid>among others.)in the context of the agglutinative languages similar to turkish (in at least morphological aspects) , there has been some recent work on translating from and to finnish with the significant amount of datain the europarl corpus.</citsent>
<aftsection>
<nextsent>although the bleu (pap ineni et al , 2002) <papid> P02-1040 </papid>score from finnish to english is 21.8, the score in the reverse direction is report edas 13.0 which is one of the lowest scores in 11 european languages scores (koehn, 2005).</nextsent>
<nextsent>also, reported from and to translation scores for finnish are the lowest on average, even with the large number of 1denoting english and french as used in the original ibm project which translated from french to english using the parallel text of the hansa rds, the canadian parliament proceedings.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC195">
<title id=" W06-3102.xml">initial explorations in english to turkish statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>statistical machine translation views the translation process as noisy-channel signal recovery process in which one tries to recover the input signal?
</prevsent>
<prevsent>e, from the observed output signal f.1 early statistical machine translation systems used purely word-based approach without taking into account any of the morphological or syntactic properties of the languages (brown et al , 1993).<papid> J93-2003 </papid></prevsent>
</prevsection>
<citsent citstr=" N03-1017 ">
limitations of basic word-based models prompted researchers to exploit morphological and/or syntac tic/phrasal structure (niessen and ney, (2004), <papid> J04-2003 </papid>lee,(2004), <papid> N04-4015 </papid>yamada and knight (2001), <papid> P01-1067 </papid>marcu and wong (2002), <papid> W02-1018 </papid>och and ney (2004),<papid> J04-4002 </papid>koehn et al  (2003), <papid> N03-1017 </papid>among others.)in the context of the agglutinative languages similar to turkish (in at least morphological aspects) , there has been some recent work on translating from and to finnish with the significant amount of datain the europarl corpus.</citsent>
<aftsection>
<nextsent>although the bleu (pap ineni et al , 2002) <papid> P02-1040 </papid>score from finnish to english is 21.8, the score in the reverse direction is report edas 13.0 which is one of the lowest scores in 11 european languages scores (koehn, 2005).</nextsent>
<nextsent>also, reported from and to translation scores for finnish are the lowest on average, even with the large number of 1denoting english and french as used in the original ibm project which translated from french to english using the parallel text of the hansa rds, the canadian parliament proceedings.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC196">
<title id=" W06-3102.xml">initial explorations in english to turkish statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>e, from the observed output signal f.1 early statistical machine translation systems used purely word-based approach without taking into account any of the morphological or syntactic properties of the languages (brown et al , 1993).<papid> J93-2003 </papid></prevsent>
<prevsent>limitations of basic word-based models prompted researchers to exploit morphological and/or syntac tic/phrasal structure (niessen and ney, (2004), <papid> J04-2003 </papid>lee,(2004), <papid> N04-4015 </papid>yamada and knight (2001), <papid> P01-1067 </papid>marcu and wong (2002), <papid> W02-1018 </papid>och and ney (2004),<papid> J04-4002 </papid>koehn et al  (2003), <papid> N03-1017 </papid>among others.)in the context of the agglutinative languages similar to turkish (in at least morphological aspects) , there has been some recent work on translating from and to finnish with the significant amount of datain the europarl corpus.</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
although the bleu (pap ineni et al , 2002) <papid> P02-1040 </papid>score from finnish to english is 21.8, the score in the reverse direction is report edas 13.0 which is one of the lowest scores in 11 european languages scores (koehn, 2005).</citsent>
<aftsection>
<nextsent>also, reported from and to translation scores for finnish are the lowest on average, even with the large number of 1denoting english and french as used in the original ibm project which translated from french to english using the parallel text of the hansa rds, the canadian parliament proceedings.
</nextsent>
<nextsent>7 sentences available.
</nextsent>
<nextsent>these may hint at the fact that standard alignment models may be poorly equipped to deal with translation from poor morphology language like english to an complex morphology language like finnish or turkish.this paper presents results from some very preliminary explorations into developing an english-to turkish statistical machine translation system and discusses the various problems encountered.
</nextsent>
<nextsent>starting with baseline word model trained from about 20k aligned sentences, we explore various ways of exploiting morphological structure to improve upon the baseline system.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC198">
<title id=" W06-3102.xml">initial explorations in english to turkish statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(4) morpheme grouping: observing that certain sequence of morphemes in turkish texts are translations of some continuous sequence of functional words and tags in english texts, and that some morphemes should be aligned differently depending on the other morphemes in their context, we attempted morpheme grouping.
</prevsent>
<prevsent>for example the morpheme sequence +dhr +ma marks infinitive form of causative verb which in turkish inflects like noun; or the lexical morpheme sequence +yacak +dhr usually maps to it/he/she will?.
</prevsent>
</prevsection>
<citsent citstr=" P05-1033 ">
to find such groups of morphemes and functional words, we applied asequence of morpheme groupings by extracting frequently occuring n-grams of morphemes as follows (much like the grouping used by chiang (2005): <papid> P05-1033 </papid>in aseries of iterations, we obtained high-frequency bigrams from the morphemic representation of parallel texts, of either morphemes, or of previously such identified morpheme groups and neighboring morphemes until up to four morphemes or one root 3 morpheme could be combined.</citsent>
<aftsection>
<nextsent>during this proces swe ignored those combinations that contain punctuation or morpheme preceding root word.
</nextsent>
<nextsent>a similar grouping was done on the english side grouping function words and morphemes before and after root words.the aim of this process was two-fold: it let frequent morphemes to behave as single token and help pharaoh with identification of some of the phrases.
</nextsent>
<nextsent>also since the number of tokens on both sides were reduced, this enabled giza++ to produce somewhat better alignments.the morpheme level translations that were obtained from training with this parallel texts were then converted into surface forms by concatenating the morphemes in the sequence produced.
</nextsent>
<nextsent>this resulted in bleu score of 0.0644.(5) morpheme grouping with selective morpheme concatenation: this was the same as (4) with the morphemes selectively combined as in (3).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC199">
<title id=" W06-1620.xml">multilingual deep lexical acquisition for hpsgs via super tagging </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>precision grammars are defined as implemented grammars of natural language which capture fine grained linguistic distinctions, and are generative in the sense of distinguishing between grammatical and ungrammatical inputs (or at least have some in-built notion of linguistic markedness?).
</prevsent>
<prevsent>additional characteristics of precision grammars are that they are frequently bidirectional, and output rich semantic abstraction for each spanning parse of the input string.
</prevsent>
</prevsection>
<citsent citstr=" C04-1180 ">
examples include delph-in grammars such as the english resource grammar (flickinger, 2002; uszkoreit, 2002), the various pargram grammars (butt et al, 1999), and the edinburgh ccg parser (bos et al, 2004).<papid> C04-1180 </papid></citsent>
<aftsection>
<nextsent>due to their linguistic complexity, precision grammars are generally hand-constructed and thus restricted in size and coverage.
</nextsent>
<nextsent>attempts to(semi-)automate the process of expanding the coverage of precision grammars have focused on ei ther: (a) constructional coverage, e.g. in the form of error mining for constructional expansion (vannoord, 2004; zhang and kordoni, 2006), or relaxation of lexico-grammatical constraints to support partial and/or robust parsing (riezler et al, 2002); <papid> P02-1035 </papid>or (b) lexical coverage, e.g. in bootstrapping from pre-existing grammar and lexicon to learn new lexical items (baldwin, 2005<papid> W05-1008 </papid>a).</nextsent>
<nextsent>our particular interest in this paper is in the latter of these two,that is the development of methods for automatically expanding the lexical coverage of an existing precision grammar, or more broadly deep lexical acquisition (dla hereafter).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC200">
<title id=" W06-1620.xml">multilingual deep lexical acquisition for hpsgs via super tagging </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>examples include delph-in grammars such as the english resource grammar (flickinger, 2002; uszkoreit, 2002), the various pargram grammars (butt et al, 1999), and the edinburgh ccg parser (bos et al, 2004).<papid> C04-1180 </papid></prevsent>
<prevsent>due to their linguistic complexity, precision grammars are generally hand-constructed and thus restricted in size and coverage.</prevsent>
</prevsection>
<citsent citstr=" P02-1035 ">
attempts to(semi-)automate the process of expanding the coverage of precision grammars have focused on ei ther: (a) constructional coverage, e.g. in the form of error mining for constructional expansion (vannoord, 2004; zhang and kordoni, 2006), or relaxation of lexico-grammatical constraints to support partial and/or robust parsing (riezler et al, 2002); <papid> P02-1035 </papid>or (b) lexical coverage, e.g. in bootstrapping from pre-existing grammar and lexicon to learn new lexical items (baldwin, 2005<papid> W05-1008 </papid>a).</citsent>
<aftsection>
<nextsent>our particular interest in this paper is in the latter of these two,that is the development of methods for automatically expanding the lexical coverage of an existing precision grammar, or more broadly deep lexical acquisition (dla hereafter).
</nextsent>
<nextsent>in this, we follow baldwin (2005<papid> W05-1008 </papid>a) in assuming semi-mature precision grammar with fixed inventory of lexical types, based on which we learn new lexical items.for the purposes of this paper, we focus specifically on super tagging as the mechanism for hy pothesising new lexical items.</nextsent>
<nextsent>super tagging can be defined as the process of applying sequential tagger to the task of predicting the lexical type(s) associated with each word in an input string, relative to given grammar.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC201">
<title id=" W06-1620.xml">multilingual deep lexical acquisition for hpsgs via super tagging </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>examples include delph-in grammars such as the english resource grammar (flickinger, 2002; uszkoreit, 2002), the various pargram grammars (butt et al, 1999), and the edinburgh ccg parser (bos et al, 2004).<papid> C04-1180 </papid></prevsent>
<prevsent>due to their linguistic complexity, precision grammars are generally hand-constructed and thus restricted in size and coverage.</prevsent>
</prevsection>
<citsent citstr=" W05-1008 ">
attempts to(semi-)automate the process of expanding the coverage of precision grammars have focused on ei ther: (a) constructional coverage, e.g. in the form of error mining for constructional expansion (vannoord, 2004; zhang and kordoni, 2006), or relaxation of lexico-grammatical constraints to support partial and/or robust parsing (riezler et al, 2002); <papid> P02-1035 </papid>or (b) lexical coverage, e.g. in bootstrapping from pre-existing grammar and lexicon to learn new lexical items (baldwin, 2005<papid> W05-1008 </papid>a).</citsent>
<aftsection>
<nextsent>our particular interest in this paper is in the latter of these two,that is the development of methods for automatically expanding the lexical coverage of an existing precision grammar, or more broadly deep lexical acquisition (dla hereafter).
</nextsent>
<nextsent>in this, we follow baldwin (2005<papid> W05-1008 </papid>a) in assuming semi-mature precision grammar with fixed inventory of lexical types, based on which we learn new lexical items.for the purposes of this paper, we focus specifically on super tagging as the mechanism for hy pothesising new lexical items.</nextsent>
<nextsent>super tagging can be defined as the process of applying sequential tagger to the task of predicting the lexical type(s) associated with each word in an input string, relative to given grammar.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC209">
<title id=" W06-1620.xml">multilingual deep lexical acquisition for hpsgs via super tagging </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this, we follow baldwin (2005<papid> W05-1008 </papid>a) in assuming semi-mature precision grammar with fixed inventory of lexical types, based on which we learn new lexical items.for the purposes of this paper, we focus specifically on super tagging as the mechanism for hy pothesising new lexical items.</prevsent>
<prevsent>super tagging can be defined as the process of applying sequential tagger to the task of predicting the lexical type(s) associated with each word in an input string, relative to given grammar.</prevsent>
</prevsection>
<citsent citstr=" J99-2004 ">
it was first introduced as means of reducing parser ambiguity by bangalore and joshi (1999) <papid> J99-2004 </papid>in the context of the ltag formalism, and has since been applied in similar context within the ccg formalism (clark and curran, 2004).<papid> C04-1041 </papid></citsent>
<aftsection>
<nextsent>in both of these cases, super tagging provides the means to perform beam search over the plausible lexical items fora given string context, and ideally reduces parsing complexity without sacrificing parser accuracy.
</nextsent>
<nextsent>an alternate application of super tagging is in dla, in postulating novel lexical items with which to populate the lexicon of given grammar to boost parser coverage.
</nextsent>
<nextsent>this can take place 164 either: (a) off-line for the purposes of rounding out the coverage of static lexicon, in which case we are generally interested in globally maximising precision over given corpus and hence predicting the single most plausible lexical type for each word token (off-line dla: baldwin (2005<papid> W05-1008 </papid>b)); or(b) on the fly forgiven input string to temporarily expand lexical coverage and achieve spanning parse, in which case we are interested in maximising recall by producing (possibly weighted) list of lexical item hypotheses to run past the grammar (on-line dla: zhang and kordoni (2005)).</nextsent>
<nextsent>our immediate interest in this paper is in the first ofthese tasks, although we would ideally like to develop an off-line method which is trivially portable to the second task of on-line dla.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC210">
<title id=" W06-1620.xml">multilingual deep lexical acquisition for hpsgs via super tagging </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this, we follow baldwin (2005<papid> W05-1008 </papid>a) in assuming semi-mature precision grammar with fixed inventory of lexical types, based on which we learn new lexical items.for the purposes of this paper, we focus specifically on super tagging as the mechanism for hy pothesising new lexical items.</prevsent>
<prevsent>super tagging can be defined as the process of applying sequential tagger to the task of predicting the lexical type(s) associated with each word in an input string, relative to given grammar.</prevsent>
</prevsection>
<citsent citstr=" C04-1041 ">
it was first introduced as means of reducing parser ambiguity by bangalore and joshi (1999) <papid> J99-2004 </papid>in the context of the ltag formalism, and has since been applied in similar context within the ccg formalism (clark and curran, 2004).<papid> C04-1041 </papid></citsent>
<aftsection>
<nextsent>in both of these cases, super tagging provides the means to perform beam search over the plausible lexical items fora given string context, and ideally reduces parsing complexity without sacrificing parser accuracy.
</nextsent>
<nextsent>an alternate application of super tagging is in dla, in postulating novel lexical items with which to populate the lexicon of given grammar to boost parser coverage.
</nextsent>
<nextsent>this can take place 164 either: (a) off-line for the purposes of rounding out the coverage of static lexicon, in which case we are generally interested in globally maximising precision over given corpus and hence predicting the single most plausible lexical type for each word token (off-line dla: baldwin (2005<papid> W05-1008 </papid>b)); or(b) on the fly forgiven input string to temporarily expand lexical coverage and achieve spanning parse, in which case we are interested in maximising recall by producing (possibly weighted) list of lexical item hypotheses to run past the grammar (on-line dla: zhang and kordoni (2005)).</nextsent>
<nextsent>our immediate interest in this paper is in the first ofthese tasks, although we would ideally like to develop an off-line method which is trivially portable to the second task of on-line dla.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC219">
<title id=" W06-1620.xml">multilingual deep lexical acquisition for hpsgs via super tagging </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this can take place 164 either: (a) off-line for the purposes of rounding out the coverage of static lexicon, in which case we are generally interested in globally maximising precision over given corpus and hence predicting the single most plausible lexical type for each word token (off-line dla: baldwin (2005<papid> W05-1008 </papid>b)); or(b) on the fly forgiven input string to temporarily expand lexical coverage and achieve spanning parse, in which case we are interested in maximising recall by producing (possibly weighted) list of lexical item hypotheses to run past the grammar (on-line dla: zhang and kordoni (2005)).</prevsent>
<prevsent>our immediate interest in this paper is in the first ofthese tasks, although we would ideally like to develop an off-line method which is trivially portable to the second task of on-line dla.</prevsent>
</prevsection>
<citsent citstr=" W02-1502 ">
in this research, we focus particularly on the grammar matrix-based delph-in family of grammars (bender et al, 2002), <papid> W02-1502 </papid>which includes grammars of english, japanese, norwegian, modern greek, portuguese and korean.</citsent>
<aftsection>
<nextsent>the grammar matrix is framework for streamlining and standardising hpsg-based multilingual grammardevelopment.
</nextsent>
<nextsent>one property of grammar matrix based grammars is that they are strongly lexical ist and adhere to highly constrained lexicon grammar interface via unique (terminal) lexical type for each lexical item.
</nextsent>
<nextsent>as such, lexical item creation in any of the grammar matrix-basedgrammars, irrespective of language, consists predominantly of predicting the appropriate lexical type for each lexical item, relative to the lexical hierarchy for the corresponding grammar.
</nextsent>
<nextsent>in this same spirit of standardisation and multilinguality, the aim of this research is to develop maximally language-independent super tagging methods which can be applied to any grammar matrix based grammar with the minimum of effort.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC237">
<title id=" W06-1620.xml">multilingual deep lexical acquisition for hpsgs via super tagging </title>
<section> past research.  </section>
<citcontext>
<prevsection>
<prevsent>that is,the precision grammar itself drives the incremental learning process within parsing context.
</prevsent>
<prevsent>165 an alternate approach is to compile out set ofword templates for each lexical type (with the important qualification that they do not relyon preprocessing of any form), and check for corpus occurrences of an unknown word in such contexts.that is, the morphological, syntactic and/or semantic predictions implicit in each lexical type aremade explicit in the form of templates which represent distinguishing lexical contexts of that lexical type.
</prevsent>
</prevsection>
<citsent citstr=" N04-1016 ">
this approach has been shown to be particularly effective over web data, where the sheer size of the data precludes the possibility of linguistic preprocessing but at the same time amelioratesthe effects of data sparseness inherent in any lexicalised dla approach (lapata and keller, 2004).<papid> N04-1016 </papid></citsent>
<aftsection>
<nextsent>other work on dla (e.g. korhonen (2002), joanis and stevenson (2003), baldwin (2005<papid> W05-1008 </papid>a)) has tended to take an in vitro dla approach, in extrapolating away from dlr to corpus or web data, and analysing occurrences of words through the conduit of an external resource (e.g. secondary parser or pos tagger).</nextsent>
<nextsent>in vitro dla canalso take the form of resource translation, in mapping one dlr onto another to arrive at the lexical information in the desired format.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC245">
<title id=" W06-1620.xml">multilingual deep lexical acquisition for hpsgs via super tagging </title>
<section> task and resources.  </section>
<citcontext>
<prevsection>
<prevsent>other work on dla (e.g. korhonen (2002), joanis and stevenson (2003), baldwin (2005<papid> W05-1008 </papid>a)) has tended to take an in vitro dla approach, in extrapolating away from dlr to corpus or web data, and analysing occurrences of words through the conduit of an external resource (e.g. secondary parser or pos tagger).</prevsent>
<prevsent>in vitro dla canalso take the form of resource translation, in mapping one dlr onto another to arrive at the lexical information in the desired format.</prevsent>
</prevsection>
<citsent citstr=" W02-1210 ">
in this section, we outline the resources targeted in this research, namely the english resource grammar (erg: flickinger (2002), copestake and flickinger (2000)) and the jacy grammar of japanese (siegel and bender, 2002).<papid> W02-1210 </papid></citsent>
<aftsection>
<nextsent>note that our choice of the erg and jacy as test beds for experimentation in this paper is somewhat arbitrary, and that we could equally run experiments over any grammar matrix-based grammar for which there is treebank data.
</nextsent>
<nextsent>both the erg and jacy are implementedopen-source broad-coverage precision head driven phrase structure grammars (hpsgs: pollard and sag (1994)).
</nextsent>
<nextsent>a lexical item in each of the grammars consists of unique identifier, lexical type (a leaf type of type hierarchy), an orthography, and semantic relation.
</nextsent>
<nextsent>for example, in the english grammar, the lexical item for the noun dog is simply: dog_n1 := n_-_c_le &amp; [ stem    dog   , synsem [ lkeys.keyrel.pred  _dog_n_1_rel  ] ].
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC246">
<title id=" W06-1620.xml">multilingual deep lexical acquisition for hpsgs via super tagging </title>
<section> supper tagging.  </section>
<citcontext>
<prevsection>
<prevsent>  8j    ffig  * .we use zero-mean gaussian prior, with the probability density function sr/   ut 56 vgwyx[z\ ]^ \/_ . this yields log-pseudo-likelihood objective function of:`    2 4.b c6de.fhgji
</prevsent>
<prevsent>fi 2  i     (4) in order to train the model, we maximize (4).while the log-pseudo-likelihood cannot be maximised for the parameters, * , in closed form, it is convex function, and thus we resort to numerical optimisation to find the globally optimal parameters.
</prevsent>
</prevsection>
<citsent citstr=" W02-2018 ">
we use l-bfgs, an iterative quasi-newton optimisation method, which performs well for training log-linear models (malouf, 2002; <papid> W02-2018 </papid>sha and pereira, 2003).<papid> N03-1028 </papid></citsent>
<aftsection>
<nextsent>each l-bfgs iteration requires the objective value and its gradient with respect to the model parameters.
</nextsent>
<nextsent>as we cannot observe label values for the test data we must use 01
</nextsent>
<nextsent>fi when decoding.
</nextsent>
<nextsent>the viterbi algorithm is used to find the maximum posterior probability alignment for test sentences,  k ba df?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC247">
<title id=" W06-1620.xml">multilingual deep lexical acquisition for hpsgs via super tagging </title>
<section> supper tagging.  </section>
<citcontext>
<prevsection>
<prevsent>  8j    ffig  * .we use zero-mean gaussian prior, with the probability density function sr/   ut 56 vgwyx[z\ ]^ \/_ . this yields log-pseudo-likelihood objective function of:`    2 4.b c6de.fhgji
</prevsent>
<prevsent>fi 2  i     (4) in order to train the model, we maximize (4).while the log-pseudo-likelihood cannot be maximised for the parameters, * , in closed form, it is convex function, and thus we resort to numerical optimisation to find the globally optimal parameters.
</prevsent>
</prevsection>
<citsent citstr=" N03-1028 ">
we use l-bfgs, an iterative quasi-newton optimisation method, which performs well for training log-linear models (malouf, 2002; <papid> W02-2018 </papid>sha and pereira, 2003).<papid> N03-1028 </papid></citsent>
<aftsection>
<nextsent>each l-bfgs iteration requires the objective value and its gradient with respect to the model parameters.
</nextsent>
<nextsent>as we cannot observe label values for the test data we must use 01
</nextsent>
<nextsent>fi when decoding.
</nextsent>
<nextsent>the viterbi algorithm is used to find the maximum posterior probability alignment for test sentences,  k ba df?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC255">
<title id=" W06-1620.xml">multilingual deep lexical acquisition for hpsgs via super tagging </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>  ffifl   in the instance that  was not observed in the training data, we back off to the majority lexical type in the training data.
</prevsent>
<prevsent>5.2 benchmark: fntbl.
</prevsent>
</prevsection>
<citsent citstr=" N01-1006 ">
in order to benchmark our results with the crf models, we reimplemented the super tagger model proposed by baldwin (2005<papid> W05-1008 </papid>b) which simply takes fntbl 1.1 (ngai and florian, 2001) <papid> N01-1006 </papid>off the shelf and trains it over our particular training set.</citsent>
<aftsection>
<nextsent>fntbl is transformation-based learner that is distributed with pre-optimised pos tagging modules for english and other european languages that can be redeployed over the task of supertagging.following baldwin (2005<papid> W05-1008 </papid>b), the only modifications we make to the default english pos tagging methodology are: (1) to set the default lexical types for singular common and proper nouns to - le and - pn le, respectively; and (2) reduce the threshold score for lexical and context transformation rules to 1.</nextsent>
<nextsent>it is important to realise that, unlike our proposed method, the english pos tagger implementation in fntbl has been fine tuned to the english pos task, and includes rich set of lexical templates specific to english.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC263">
<title id=" W06-3803.xml">graph based text representation for novelty detection </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>more sophisticated metrics can be defined on the basis of graph representations.
</prevsent>
<prevsent>graph representations of text can be constructed without performing linguistic analysis, by using term distances in sentences and pointwise mutual information between terms to form edges between term-vertices.
</prevsent>
</prevsection>
<citsent citstr=" P04-3020 ">
a term-distance based representation has been used successfully for variety of tasks in mihalcea (2004) <papid> P04-3020 </papid>and mihalcea and tarau (2004).<papid> W04-3252 </papid></citsent>
<aftsection>
<nextsent>there were 13 participants and 54 submitted runs for the 2004 trec novelty track task 2.
</nextsent>
<nextsent>each participant submitted up to five runs with different system configurations.
</nextsent>
<nextsent>metrics and approaches varied widely, from purely string based approaches to systems that used sophisticated linguistic components for synonymy resolution, coreference resolution and named entity recognition.
</nextsent>
<nextsent>many systems employed thresholding approach to the task, defining novelty metric and then determining sentence to be novel if the threshold is exceeded (e.g. blott et al 2004, zhang et al 2004, abdul-jaleel et al 2004, eichmann et al 2004, erkan 2004).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC265">
<title id=" W06-3803.xml">graph based text representation for novelty detection </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>more sophisticated metrics can be defined on the basis of graph representations.
</prevsent>
<prevsent>graph representations of text can be constructed without performing linguistic analysis, by using term distances in sentences and pointwise mutual information between terms to form edges between term-vertices.
</prevsent>
</prevsection>
<citsent citstr=" W04-3252 ">
a term-distance based representation has been used successfully for variety of tasks in mihalcea (2004) <papid> P04-3020 </papid>and mihalcea and tarau (2004).<papid> W04-3252 </papid></citsent>
<aftsection>
<nextsent>there were 13 participants and 54 submitted runs for the 2004 trec novelty track task 2.
</nextsent>
<nextsent>each participant submitted up to five runs with different system configurations.
</nextsent>
<nextsent>metrics and approaches varied widely, from purely string based approaches to systems that used sophisticated linguistic components for synonymy resolution, coreference resolution and named entity recognition.
</nextsent>
<nextsent>many systems employed thresholding approach to the task, defining novelty metric and then determining sentence to be novel if the threshold is exceeded (e.g. blott et al 2004, zhang et al 2004, abdul-jaleel et al 2004, eichmann et al 2004, erkan 2004).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC270">
<title id=" W06-3803.xml">graph based text representation for novelty detection </title>
<section> to miyama et al(2004) (meiji university):.  </section>
<citcontext>
<prevsection>
<prevsent>as the set of seen sentences grows and cooccurrence between words becomes more prevalent, pmi becomes more influential on edge weights, strengthening edges between words that have high pmi.
</prevsent>
<prevsent>( , ) 2 ( , )log ( ) ( )i p jpmi i j= equation 2: pointwise mutual information (pmi) between two terms and j. 2 this view is supported by examining dependency structures derived from the penn tree bank and mapping the probability of dependency to the distance between words.
</prevsent>
</prevsection>
<citsent citstr=" W05-1504 ">
see also eisner and smith (2005) <papid> W05-1504 </papid>who explore this generalization for dependency parsing.</citsent>
<aftsection>
<nextsent>3 we also computed results from graph where the edge weight is determined only by term distance, without pmi.
</nextsent>
<nextsent>these results were consistently worse than the ones reported here.
</nextsent>
<nextsent>4 we are grateful to an anonymous reviewer for pointing this out.
</nextsent>
<nextsent>formally, the weight wt for each edge in the graph is defined as in equation 3, where di,j is the distance between words wi and wj.and pmi(i,j) is the pointwise mutual information between words wi and wj, given the sentences seen so far.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC275">
<title id=" W06-1322.xml">a computational model of multimodal grounding for human robot interaction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>under this circumstance the dialog system must support mixed initiative dialog style to receive user commands onthe one side and to report on the perceived environmental changes on the other side.
</prevsent>
<prevsent>otherwise the robot had to break up the task execution and there is no way for the user to find out the reason.the second challenge for hri dialog management is the embodiment of robot which changes the way of interaction.
</prevsent>
</prevsection>
<citsent citstr=" P03-1070 ">
empirical studies show that the visual access to the interlocutors body affects the conversation in the way that non-verbal behaviors are used as communicative signals (nakano et al., 2003).<papid> P03-1070 </papid></citsent>
<aftsection>
<nextsent>for example, to refer to cup that is visible to both dialog partners, the speaker tends to say this cup?
</nextsent>
<nextsent>while pointing to it.
</nextsent>
<nextsent>the same strategy is considerably ineffective during phone call.
</nextsent>
<nextsent>this example shows, an hri dialog system must account for multi-modal communication.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC276">
<title id=" W06-2910.xml">can human verb associations help identify salient features for semantic verb classification </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the different frameworks depend on different instantiations of semantic similarity, e.g. levin relies on verb similarity referring to syntax-semantic alternation behaviour, wordnet uses synonymy, and framenet relies on situation-based agreement as defined in fillmoresframe semantics (fillmore, 1982).
</prevsent>
<prevsent>as an alternative to the resource-intensive manual classifications,automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.
</prevsent>
</prevsection>
<citsent citstr=" J01-3003 ">
(merlo and stevenson, 2001; <papid> J01-3003 </papid>joanisand stevenson, 2003; <papid> E03-1040 </papid>korhonen et al, 2003; <papid> P03-1009 </papid>stevenson and joanis, 2003; <papid> W03-0410 </papid>schulte im walde, 2003; ferrer, 2004).<papid> P04-2007 </papid></citsent>
<aftsection>
<nextsent>depending on the types of verb classes to be induced, the automatic approaches vary their choice of verbs and classification/clustering algorithm.
</nextsent>
<nextsent>however, another central parameter for the automatic induction of semantic verb classes is the selection of verb features.since the target classification determines the similarity and dissimilarity of the verbs, the verb feature selection should model the similarity of interest.
</nextsent>
<nextsent>for example, merlo and stevenson (2001) <papid> J01-3003 </papid>classify 60 english verbs which alternate between an in transitive and transitive usage, and assign them tothree verb classes, according to the semantic role assignment in the frames; their verb features are chosen such that they model the syntactic frame alternation proportions and also heuristics for semantic role assignment.</nextsent>
<nextsent>in larger-scale classifications such as (korhonen et al, 2003; <papid> P03-1009 </papid>stevenson and joanis, 2003; <papid> W03-0410 </papid>schulte im walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC277">
<title id=" W06-2910.xml">can human verb associations help identify salient features for semantic verb classification </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the different frameworks depend on different instantiations of semantic similarity, e.g. levin relies on verb similarity referring to syntax-semantic alternation behaviour, wordnet uses synonymy, and framenet relies on situation-based agreement as defined in fillmoresframe semantics (fillmore, 1982).
</prevsent>
<prevsent>as an alternative to the resource-intensive manual classifications,automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.
</prevsent>
</prevsection>
<citsent citstr=" E03-1040 ">
(merlo and stevenson, 2001; <papid> J01-3003 </papid>joanisand stevenson, 2003; <papid> E03-1040 </papid>korhonen et al, 2003; <papid> P03-1009 </papid>stevenson and joanis, 2003; <papid> W03-0410 </papid>schulte im walde, 2003; ferrer, 2004).<papid> P04-2007 </papid></citsent>
<aftsection>
<nextsent>depending on the types of verb classes to be induced, the automatic approaches vary their choice of verbs and classification/clustering algorithm.
</nextsent>
<nextsent>however, another central parameter for the automatic induction of semantic verb classes is the selection of verb features.since the target classification determines the similarity and dissimilarity of the verbs, the verb feature selection should model the similarity of interest.
</nextsent>
<nextsent>for example, merlo and stevenson (2001) <papid> J01-3003 </papid>classify 60 english verbs which alternate between an in transitive and transitive usage, and assign them tothree verb classes, according to the semantic role assignment in the frames; their verb features are chosen such that they model the syntactic frame alternation proportions and also heuristics for semantic role assignment.</nextsent>
<nextsent>in larger-scale classifications such as (korhonen et al, 2003; <papid> P03-1009 </papid>stevenson and joanis, 2003; <papid> W03-0410 </papid>schulte im walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC278">
<title id=" W06-2910.xml">can human verb associations help identify salient features for semantic verb classification </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the different frameworks depend on different instantiations of semantic similarity, e.g. levin relies on verb similarity referring to syntax-semantic alternation behaviour, wordnet uses synonymy, and framenet relies on situation-based agreement as defined in fillmoresframe semantics (fillmore, 1982).
</prevsent>
<prevsent>as an alternative to the resource-intensive manual classifications,automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.
</prevsent>
</prevsection>
<citsent citstr=" P03-1009 ">
(merlo and stevenson, 2001; <papid> J01-3003 </papid>joanisand stevenson, 2003; <papid> E03-1040 </papid>korhonen et al, 2003; <papid> P03-1009 </papid>stevenson and joanis, 2003; <papid> W03-0410 </papid>schulte im walde, 2003; ferrer, 2004).<papid> P04-2007 </papid></citsent>
<aftsection>
<nextsent>depending on the types of verb classes to be induced, the automatic approaches vary their choice of verbs and classification/clustering algorithm.
</nextsent>
<nextsent>however, another central parameter for the automatic induction of semantic verb classes is the selection of verb features.since the target classification determines the similarity and dissimilarity of the verbs, the verb feature selection should model the similarity of interest.
</nextsent>
<nextsent>for example, merlo and stevenson (2001) <papid> J01-3003 </papid>classify 60 english verbs which alternate between an in transitive and transitive usage, and assign them tothree verb classes, according to the semantic role assignment in the frames; their verb features are chosen such that they model the syntactic frame alternation proportions and also heuristics for semantic role assignment.</nextsent>
<nextsent>in larger-scale classifications such as (korhonen et al, 2003; <papid> P03-1009 </papid>stevenson and joanis, 2003; <papid> W03-0410 </papid>schulte im walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC280">
<title id=" W06-2910.xml">can human verb associations help identify salient features for semantic verb classification </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the different frameworks depend on different instantiations of semantic similarity, e.g. levin relies on verb similarity referring to syntax-semantic alternation behaviour, wordnet uses synonymy, and framenet relies on situation-based agreement as defined in fillmoresframe semantics (fillmore, 1982).
</prevsent>
<prevsent>as an alternative to the resource-intensive manual classifications,automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.
</prevsent>
</prevsection>
<citsent citstr=" W03-0410 ">
(merlo and stevenson, 2001; <papid> J01-3003 </papid>joanisand stevenson, 2003; <papid> E03-1040 </papid>korhonen et al, 2003; <papid> P03-1009 </papid>stevenson and joanis, 2003; <papid> W03-0410 </papid>schulte im walde, 2003; ferrer, 2004).<papid> P04-2007 </papid></citsent>
<aftsection>
<nextsent>depending on the types of verb classes to be induced, the automatic approaches vary their choice of verbs and classification/clustering algorithm.
</nextsent>
<nextsent>however, another central parameter for the automatic induction of semantic verb classes is the selection of verb features.since the target classification determines the similarity and dissimilarity of the verbs, the verb feature selection should model the similarity of interest.
</nextsent>
<nextsent>for example, merlo and stevenson (2001) <papid> J01-3003 </papid>classify 60 english verbs which alternate between an in transitive and transitive usage, and assign them tothree verb classes, according to the semantic role assignment in the frames; their verb features are chosen such that they model the syntactic frame alternation proportions and also heuristics for semantic role assignment.</nextsent>
<nextsent>in larger-scale classifications such as (korhonen et al, 2003; <papid> P03-1009 </papid>stevenson and joanis, 2003; <papid> W03-0410 </papid>schulte im walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC283">
<title id=" W06-2910.xml">can human verb associations help identify salient features for semantic verb classification </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the different frameworks depend on different instantiations of semantic similarity, e.g. levin relies on verb similarity referring to syntax-semantic alternation behaviour, wordnet uses synonymy, and framenet relies on situation-based agreement as defined in fillmoresframe semantics (fillmore, 1982).
</prevsent>
<prevsent>as an alternative to the resource-intensive manual classifications,automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.
</prevsent>
</prevsection>
<citsent citstr=" P04-2007 ">
(merlo and stevenson, 2001; <papid> J01-3003 </papid>joanisand stevenson, 2003; <papid> E03-1040 </papid>korhonen et al, 2003; <papid> P03-1009 </papid>stevenson and joanis, 2003; <papid> W03-0410 </papid>schulte im walde, 2003; ferrer, 2004).<papid> P04-2007 </papid></citsent>
<aftsection>
<nextsent>depending on the types of verb classes to be induced, the automatic approaches vary their choice of verbs and classification/clustering algorithm.
</nextsent>
<nextsent>however, another central parameter for the automatic induction of semantic verb classes is the selection of verb features.since the target classification determines the similarity and dissimilarity of the verbs, the verb feature selection should model the similarity of interest.
</nextsent>
<nextsent>for example, merlo and stevenson (2001) <papid> J01-3003 </papid>classify 60 english verbs which alternate between an in transitive and transitive usage, and assign them tothree verb classes, according to the semantic role assignment in the frames; their verb features are chosen such that they model the syntactic frame alternation proportions and also heuristics for semantic role assignment.</nextsent>
<nextsent>in larger-scale classifications such as (korhonen et al, 2003; <papid> P03-1009 </papid>stevenson and joanis, 2003; <papid> W03-0410 </papid>schulte im walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC290">
<title id=" W06-2910.xml">can human verb associations help identify salient features for semantic verb classification </title>
<section> association-based verb classes.  </section>
<citcontext>
<prevsection>
<prevsent>schulte im walde (2003).
</prevsent>
<prevsent>our claim is that the hierarchical verb classe sand their underlying features (i.e. the verb associations) represent useful basis for theory independent semantic classification of the german verbs.
</prevsent>
</prevsection>
<citsent citstr=" P03-1068 ">
to support this claim, we validated theassoc-classes against standard approaches to semantic verb classes, i.e. germanet as the german wordnet (kunze, 2000), and the german counterpart of framenet in the salsa project (erk et al, 2003).<papid> P03-1068 </papid></citsent>
<aftsection>
<nextsent>details of the validation can be found in (schulte im walde, 2006); the main issues are as follows.
</nextsent>
<nextsent>we did not directly compare the assoc-classes against the germanet/framenet classes, since not all of our 330 experiments verbs were covered by the two resources.
</nextsent>
<nextsent>instead, we replicated the above cluster experiment for reduced number of verbs: we extracted those classes from the resources which contain association verbs; light verbs, non association verbs, other classes as well as singletons were disregarded.
</nextsent>
<nextsent>this left us with 33 classes from germanet, and 38 classes from framenet.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC291">
<title id=" W06-2910.xml">can human verb associations help identify salient features for semantic verb classification </title>
<section> association-based verb classes.  </section>
<citcontext>
<prevsection>
<prevsent>as for the complete set of experiments verbs, we performed hierarchical clustering on the respective subsets of the experiment verbs, with their associations as verb features.
</prevsent>
<prevsent>the actual validation procedure then used the reducedclassifications: the resulting analyses were evaluated against the resource classes on each level in the hierarchies, i.e. from 56/91 classes to 1 class.
</prevsent>
</prevsection>
<citsent citstr=" P93-1023 ">
as evaluation measure, we used pair-wise measure which calculates precision, recall and harmonic fscore as follows: each verb pair in the cluster analysis was compared to the verb pairs in the gold standard classes, and evaluated as true or false positive (hatzivassiloglou and mckeown, 1993).<papid> P93-1023 </papid></citsent>
<aftsection>
<nextsent>the association-based clusters show overlap with the lexical resource classes of an f-score of 62.69% (for 32 verb classes) when comparing to germanet, and 34.68% (for 10 verb classes) when comparing to framenet.
</nextsent>
<nextsent>the corresponding upper bounds are 82.35% for germanet and 60.31% for framenet.1the comparison therefore demonstrates consider able overlap between association-based classes and existing semantic classes.
</nextsent>
<nextsent>the different results forthe two resources are due to their semantic back ground (i.e. capturing synonymy vs. situation-based agreement), the numbers of verbs, and the degrees of ambiguity (an average of 1.6 senses per verb in framenet, as compared to 1.3 senses in germanet).
</nextsent>
<nextsent>the purpose of the validation against semantic resources was to demonstrate that clustering asbased on the verb associations and standard clustering setting compares well with existing semantic classes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC292">
<title id=" W06-2910.xml">can human verb associations help identify salient features for semantic verb classification </title>
<section> exploring semantic class features.  </section>
<citcontext>
<prevsection>
<prevsent>71 on which associations/features.
</prevsent>
<prevsent>we relyon theassoc-classes in the 100-class analysis of the hierarchical clustering2 and features which exist for at least two verbs in common class (and therefore hint to minimum of verb similarity), and compare the associations underlying the assoc-classes with standard corpus-based feature types: we check on how many of the associations we find among the corpus-based features, such as adverbs, direct object nouns, etc. there are various possibilities to determine corpus-based features that potentially cover the associations; we decided in favour of feature types which have been suggested in related work: a) grammar-based relations: previous work on distributional similarity has focused either on specific word-word relation (such as pereira et al.
</prevsent>
</prevsection>
<citsent citstr=" P99-1014 ">
(1993) and rooth et al (1999) <papid> P99-1014 </papid>referring to direct object noun for describing verbs), or used any syntactic relationship detected by chunker or parser (such as lin (1998)<papid> P98-2127 </papid>and mccarthy et al (2003)).<papid> W03-1810 </papid></citsent>
<aftsection>
<nextsent>we used statistical grammar (schulte im walde, 2003)to filter all verb-noun pairs where the nouns represent nominal heads in nps or pps in syntactic relation to the verb (subject, object, adverbial function,etc.), and to filter all verb-adverb pairs where the ad verbs modify the verbs.
</nextsent>
<nextsent>b) co-occurrence window: in previous work (schulte im walde and melinger, 2005), we showed that only 28% of all noun associates were identified by the above statistical grammar as subcate gorised nouns, but 69% were captured by 20-wordco-occurrence window in 200-million word newspaper corpus.
</nextsent>
<nextsent>this finding suggests to use cooccurrence window as alternative source for verb features, as compared to specific syntactic relations.
</nextsent>
<nextsent>we therefore determined the co-occurring words for all experiment verbs in 20-word window (i.e. 20words preceding and following the verb), irrespective of the part-of-speech of the co-occurring words.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC293">
<title id=" W06-2910.xml">can human verb associations help identify salient features for semantic verb classification </title>
<section> exploring semantic class features.  </section>
<citcontext>
<prevsection>
<prevsent>71 on which associations/features.
</prevsent>
<prevsent>we relyon theassoc-classes in the 100-class analysis of the hierarchical clustering2 and features which exist for at least two verbs in common class (and therefore hint to minimum of verb similarity), and compare the associations underlying the assoc-classes with standard corpus-based feature types: we check on how many of the associations we find among the corpus-based features, such as adverbs, direct object nouns, etc. there are various possibilities to determine corpus-based features that potentially cover the associations; we decided in favour of feature types which have been suggested in related work: a) grammar-based relations: previous work on distributional similarity has focused either on specific word-word relation (such as pereira et al.
</prevsent>
</prevsection>
<citsent citstr=" P98-2127 ">
(1993) and rooth et al (1999) <papid> P99-1014 </papid>referring to direct object noun for describing verbs), or used any syntactic relationship detected by chunker or parser (such as lin (1998)<papid> P98-2127 </papid>and mccarthy et al (2003)).<papid> W03-1810 </papid></citsent>
<aftsection>
<nextsent>we used statistical grammar (schulte im walde, 2003)to filter all verb-noun pairs where the nouns represent nominal heads in nps or pps in syntactic relation to the verb (subject, object, adverbial function,etc.), and to filter all verb-adverb pairs where the ad verbs modify the verbs.
</nextsent>
<nextsent>b) co-occurrence window: in previous work (schulte im walde and melinger, 2005), we showed that only 28% of all noun associates were identified by the above statistical grammar as subcate gorised nouns, but 69% were captured by 20-wordco-occurrence window in 200-million word newspaper corpus.
</nextsent>
<nextsent>this finding suggests to use cooccurrence window as alternative source for verb features, as compared to specific syntactic relations.
</nextsent>
<nextsent>we therefore determined the co-occurring words for all experiment verbs in 20-word window (i.e. 20words preceding and following the verb), irrespective of the part-of-speech of the co-occurring words.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC295">
<title id=" W06-2910.xml">can human verb associations help identify salient features for semantic verb classification </title>
<section> exploring semantic class features.  </section>
<citcontext>
<prevsection>
<prevsent>71 on which associations/features.
</prevsent>
<prevsent>we relyon theassoc-classes in the 100-class analysis of the hierarchical clustering2 and features which exist for at least two verbs in common class (and therefore hint to minimum of verb similarity), and compare the associations underlying the assoc-classes with standard corpus-based feature types: we check on how many of the associations we find among the corpus-based features, such as adverbs, direct object nouns, etc. there are various possibilities to determine corpus-based features that potentially cover the associations; we decided in favour of feature types which have been suggested in related work: a) grammar-based relations: previous work on distributional similarity has focused either on specific word-word relation (such as pereira et al.
</prevsent>
</prevsection>
<citsent citstr=" W03-1810 ">
(1993) and rooth et al (1999) <papid> P99-1014 </papid>referring to direct object noun for describing verbs), or used any syntactic relationship detected by chunker or parser (such as lin (1998)<papid> P98-2127 </papid>and mccarthy et al (2003)).<papid> W03-1810 </papid></citsent>
<aftsection>
<nextsent>we used statistical grammar (schulte im walde, 2003)to filter all verb-noun pairs where the nouns represent nominal heads in nps or pps in syntactic relation to the verb (subject, object, adverbial function,etc.), and to filter all verb-adverb pairs where the ad verbs modify the verbs.
</nextsent>
<nextsent>b) co-occurrence window: in previous work (schulte im walde and melinger, 2005), we showed that only 28% of all noun associates were identified by the above statistical grammar as subcate gorised nouns, but 69% were captured by 20-wordco-occurrence window in 200-million word newspaper corpus.
</nextsent>
<nextsent>this finding suggests to use cooccurrence window as alternative source for verb features, as compared to specific syntactic relations.
</nextsent>
<nextsent>we therefore determined the co-occurring words for all experiment verbs in 20-word window (i.e. 20words preceding and following the verb), irrespective of the part-of-speech of the co-occurring words.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC296">
<title id=" W06-2910.xml">can human verb associations help identify salient features for semantic verb classification </title>
<section> exploring semantic class features.  </section>
<citcontext>
<prevsection>
<prevsent>applying the frequency cut-offs reduces the overlap of association types and co-occurring words to 58%.
</prevsent>
<prevsent>specifying the window results for the part-of-speech types illustrates that the nouns play the most important role in describing verb meaning (39% of the verb association types in the assoc-classes were found among the nouns in the corpus windows, 16% among the verbs, 9% among the adjectives, and 2% among the adverbs).3 the proportions of the nouns with specific grammar relationship to the verbs show that we find more associations among direct objects than intran sitive/transitive subjects.
</prevsent>
</prevsection>
<citsent citstr=" P93-1024 ">
this insight confirms the assumption in previous work where only direct object nouns were used as salient features in distributional verb similarity, such as pereira et al (1993).<papid> P93-1024 </papid>however, the proportions are all below 10%.</citsent>
<aftsection>
<nextsent>considering all nps and/or pps, we find that the proportions increase for the nps, and that the nps play more important role than the pps.
</nextsent>
<nextsent>this insight confirms work on distributional similarity where not only direct object nouns, but all functional nouns 3caveat: these numbers correlate with the part-of-speech types of all associate responses: 62% of the responses were nouns, 25% verbs, 11% adjectives, and 2% adverbs.
</nextsent>
<nextsent>72 features grammar relations na na np pp np&pp; adv cov.
</nextsent>
<nextsent>(%) 3.82 4.32 6.93 12.23 5.36 14.08 3.63 features co-occurrence: window-20 all cut adj adv v cov.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC303">
<title id=" W07-0707.xml">word error rates decomposition over pos classes and applications for error analysis </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>human evaluation is expensive and time consuming.
</prevsent>
<prevsent>therefore variety of automatic evaluation measures have been studied over the last years.
</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
the most widely used are word error rate (wer), position independent word error rate (per), the bleu score (papineni et al, 2002)<papid> P02-1040 </papid>and the nist score (doddington, 2002).</citsent>
<aftsection>
<nextsent>these measures have shown to be valuable tools for comparing different systems as well as for evaluating improvements within one system.
</nextsent>
<nextsent>however, these measures do not give any details about the nature of translation errors.
</nextsent>
<nextsent>therefore some more detailed analysis of the generated output is needed in order to identify the main problems and to focus the research efforts.
</nextsent>
<nextsent>aframework for human error analysis has been proposed in (vilar et al, 2006), but as every human evaluation, this is also time consuming task.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC305">
<title id=" W07-0707.xml">word error rates decomposition over pos classes and applications for error analysis </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>automatic evaluation measures for machine translation output are receiving more and more attention in the last years.
</prevsent>
<prevsent>the bleu metric (papineni et al, 2002)<papid> P02-1040 </papid> and the closely related nist metric (doddington, 2002) along with wer and per 48 have been widely used by many machine translation researchers.</prevsent>
</prevsection>
<citsent citstr=" P04-1079 ">
an extended version of bleu which uses n-grams weighted according to their frequency estimated from monolingual corpus is proposed in (babych and hartley, 2004).<papid> P04-1079 </papid></citsent>
<aftsection>
<nextsent>(leusch et al, 2005)<papid> W05-0903 </papid>investigate preprocessing and normalisation methods for improving the evaluation using the standard measures wer, per, bleu and nist.</nextsent>
<nextsent>the same set of measures is examined in (matusov et al, 2005)in combination with automatic sentence segmentation in order to enable evaluation of translation out put without sentence boundaries (e.g. translation of speech recognition output).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC306">
<title id=" W07-0707.xml">word error rates decomposition over pos classes and applications for error analysis </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the bleu metric (papineni et al, 2002)<papid> P02-1040 </papid> and the closely related nist metric (doddington, 2002) along with wer and per 48 have been widely used by many machine translation researchers.</prevsent>
<prevsent>an extended version of bleu which uses n-grams weighted according to their frequency estimated from monolingual corpus is proposed in (babych and hartley, 2004).<papid> P04-1079 </papid></prevsent>
</prevsection>
<citsent citstr=" W05-0903 ">
(leusch et al, 2005)<papid> W05-0903 </papid>investigate preprocessing and normalisation methods for improving the evaluation using the standard measures wer, per, bleu and nist.</citsent>
<aftsection>
<nextsent>the same set of measures is examined in (matusov et al, 2005)in combination with automatic sentence segmentation in order to enable evaluation of translation out put without sentence boundaries (e.g. translation of speech recognition output).
</nextsent>
<nextsent>a new automatic metric meteor (banerjee and lavie, 2005) <papid> W05-0909 </papid>uses stems and synonyms of the words.</nextsent>
<nextsent>this measure counts the number of exact word matches between the out put and the reference.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC307">
<title id=" W07-0707.xml">word error rates decomposition over pos classes and applications for error analysis </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>(leusch et al, 2005)<papid> W05-0903 </papid>investigate preprocessing and normalisation methods for improving the evaluation using the standard measures wer, per, bleu and nist.</prevsent>
<prevsent>the same set of measures is examined in (matusov et al, 2005)in combination with automatic sentence segmentation in order to enable evaluation of translation out put without sentence boundaries (e.g. translation of speech recognition output).</prevsent>
</prevsection>
<citsent citstr=" W05-0909 ">
a new automatic metric meteor (banerjee and lavie, 2005) <papid> W05-0909 </papid>uses stems and synonyms of the words.</citsent>
<aftsection>
<nextsent>this measure counts the number of exact word matches between the out put and the reference.
</nextsent>
<nextsent>in second step, unmatched words are converted into stems or synonyms and then matched.
</nextsent>
<nextsent>the ter metric (snover et al, 2006) measures the amount of editing that human would have to perform to change the system output so thatit exactly matches the reference.
</nextsent>
<nextsent>the cder measure (leusch et al, 2006) <papid> E06-1031 </papid>is based on edit distance, such as the well-known wer, but allows reordering of blocks.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC308">
<title id=" W07-0707.xml">word error rates decomposition over pos classes and applications for error analysis </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in second step, unmatched words are converted into stems or synonyms and then matched.
</prevsent>
<prevsent>the ter metric (snover et al, 2006) measures the amount of editing that human would have to perform to change the system output so thatit exactly matches the reference.
</prevsent>
</prevsection>
<citsent citstr=" E06-1031 ">
the cder measure (leusch et al, 2006) <papid> E06-1031 </papid>is based on edit distance, such as the well-known wer, but allows reordering of blocks.</citsent>
<aftsection>
<nextsent>nevertheless, none of these measures or extensions takes into account linguistic knowledge about actual translation errors, for example what is the contribution of verbs in the overall error rate, how many full forms are wrong whereas their base forms are correct, etc. framework for human error analysis has been proposed in (vilar et al, 2006) and detailed analysis of the obtained results has been carried out.
</nextsent>
<nextsent>however, human error analysis, like any human evaluation, is time consuming task.whereas the use of linguistic knowledge for improving the performance of statistical machine translation system is investigated in many publications for various language pairs (like for example (nieen and ney, 2000), (goldwater and mcclosky, 2005)), <papid> H05-1085 </papid>its use for the analysis of translation errors is still rather unexplored area.</nextsent>
<nextsent>some automatic methods for error analysis using base forms and pos tags are proposed in (popovic?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC309">
<title id=" W07-0707.xml">word error rates decomposition over pos classes and applications for error analysis </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the cder measure (leusch et al, 2006) <papid> E06-1031 </papid>is based on edit distance, such as the well-known wer, but allows reordering of blocks.</prevsent>
<prevsent>nevertheless, none of these measures or extensions takes into account linguistic knowledge about actual translation errors, for example what is the contribution of verbs in the overall error rate, how many full forms are wrong whereas their base forms are correct, etc. framework for human error analysis has been proposed in (vilar et al, 2006) and detailed analysis of the obtained results has been carried out.</prevsent>
</prevsection>
<citsent citstr=" H05-1085 ">
however, human error analysis, like any human evaluation, is time consuming task.whereas the use of linguistic knowledge for improving the performance of statistical machine translation system is investigated in many publications for various language pairs (like for example (nieen and ney, 2000), (goldwater and mcclosky, 2005)), <papid> H05-1085 </papid>its use for the analysis of translation errors is still rather unexplored area.</citsent>
<aftsection>
<nextsent>some automatic methods for error analysis using base forms and pos tags are proposed in (popovic?
</nextsent>
<nextsent>et al, 2006; popovic?
</nextsent>
<nextsent>and ney, 2006).
</nextsent>
<nextsent>these measures are basedon differences between wer and per which are calculated separately for each pos class using subsets extracted from the original texts.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC311">
<title id=" W06-3101.xml">morphosyntactic information for automatic error analysis of statistical machine translation output </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>automatic evaluation is preferred because human evaluation is time consuming and expensive task.
</prevsent>
<prevsent>a variety of automatic evaluation measures have been proposed and studied over the last years, someof them are shown to be very useful tool for comparing different systems as well as for evaluating improvements within one system.
</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
the most widely used are word error rate (wer), position independent word error rate (per), the bleu score (pap ineni et al, 2002) <papid> P02-1040 </papid>and the nist score (doddington, 2002).</citsent>
<aftsection>
<nextsent>however, none of these measures give any details about the nature of translation errors.
</nextsent>
<nextsent>a relationship between these error measures and the actual errors in the translation outputs is not easy to find.
</nextsent>
<nextsent>therefore some analysis of the translation errors is necessary in order to define the main problems andto focus the research efforts.
</nextsent>
<nextsent>a framework for human error analysis and error classification has been proposed in (vilar et al, 2006), but like human evaluation, this is also time consuming task.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC313">
<title id=" W06-3101.xml">morphosyntactic information for automatic error analysis of statistical machine translation output </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>a framework for human error analysis and error classification has been proposed in (vilar et al, 2006), but like human evaluation, this is also time consuming task.
</prevsent>
<prevsent>the goal of this work is to present framework for automatic error analysis of machine translation output based on morpho-syntactic information.
</prevsent>
</prevsection>
<citsent citstr=" P04-1079 ">
there is number of publications dealing with various automatic evaluation measures for machine translation output, some of them proposing new measures, some proposing improvements and extensions of the existing ones (doddington, 2002; papineni et al, 2002; <papid> P02-1040 </papid>babych and hartley, 2004; <papid> P04-1079 </papid>matusov et al, 2005).</citsent>
<aftsection>
<nextsent>semi-automatic evaluation measures have been also investigated, for example in (nieen et al, 2000).
</nextsent>
<nextsent>an automatic metric which uses base forms and synonyms of the words in order to correlate better to human judgements has been 1 proposed in (banerjee and lavie, 2005).<papid> W05-0909 </papid></nextsent>
<nextsent>however, error analysis is still rather unexplored area.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC314">
<title id=" W06-3101.xml">morphosyntactic information for automatic error analysis of statistical machine translation output </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>there is number of publications dealing with various automatic evaluation measures for machine translation output, some of them proposing new measures, some proposing improvements and extensions of the existing ones (doddington, 2002; papineni et al, 2002; <papid> P02-1040 </papid>babych and hartley, 2004; <papid> P04-1079 </papid>matusov et al, 2005).</prevsent>
<prevsent>semi-automatic evaluation measures have been also investigated, for example in (nieen et al, 2000).</prevsent>
</prevsection>
<citsent citstr=" W05-0909 ">
an automatic metric which uses base forms and synonyms of the words in order to correlate better to human judgements has been 1 proposed in (banerjee and lavie, 2005).<papid> W05-0909 </papid></citsent>
<aftsection>
<nextsent>however, error analysis is still rather unexplored area.
</nextsent>
<nextsent>aframework for human error analysis and error classification has been proposed in (vilar et al, 2006) and detailed analysis of the obtained results hasbeen carried out.
</nextsent>
<nextsent>automatic methods for error analysis to our knowledge have not been studied yet.many publications propose the use of morphosyntactic information for improving the performance of statistical machine translation system.
</nextsent>
<nextsent>various methods for treating morphological and syntactical differences between german and english are investigated in (nieen and ney, 2000; nieenand ney, 2001a; nieen and ney, 2001b).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC315">
<title id=" W06-3101.xml">morphosyntactic information for automatic error analysis of statistical machine translation output </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>automatic methods for error analysis to our knowledge have not been studied yet.many publications propose the use of morphosyntactic information for improving the performance of statistical machine translation system.
</prevsent>
<prevsent>various methods for treating morphological and syntactical differences between german and english are investigated in (nieen and ney, 2000; nieenand ney, 2001a; nieen and ney, 2001b).
</prevsent>
</prevsection>
<citsent citstr=" N04-4015 ">
morphological analysis has been used for improvingarabic-english translation (lee, 2004), <papid> N04-4015 </papid>for serbian english translation (popovic?</citsent>
<aftsection>
<nextsent>et al, 2005) as well asfor czech-english translation (goldwater and mcclosky, 2005).<papid> H05-1085 </papid></nextsent>
<nextsent>inflectional morphology of spanish verbs is dealt with in (popovic?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC316">
<title id=" W06-3101.xml">morphosyntactic information for automatic error analysis of statistical machine translation output </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>various methods for treating morphological and syntactical differences between german and english are investigated in (nieen and ney, 2000; nieenand ney, 2001a; nieen and ney, 2001b).
</prevsent>
<prevsent>morphological analysis has been used for improvingarabic-english translation (lee, 2004), <papid> N04-4015 </papid>for serbian english translation (popovic?</prevsent>
</prevsection>
<citsent citstr=" H05-1085 ">
et al, 2005) as well asfor czech-english translation (goldwater and mcclosky, 2005).<papid> H05-1085 </papid></citsent>
<aftsection>
<nextsent>inflectional morphology of spanish verbs is dealt with in (popovic?
</nextsent>
<nextsent>and ney, 2004; de gispert et al, 2005).
</nextsent>
<nextsent>to the best of our knowledge, the use of morpho-syntactic information for error analysis of translation output has not been investigated so far.
</nextsent>
<nextsent>automatic evaluation we propose the use of morpho-syntactic information in combination with the automatic evaluation measures wer and per in order to get more details about the translation errors.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC317">
<title id=" W06-1521.xml">parsing tag with abstract categorial grammar </title>
<section> abstract </section>
<citcontext>
<prevsection>


</prevsection>
<citsent citstr=" P88-1032 ">
this paper presents informally an earley algorithm for tag which behaves as the algorithm given by (schabes and joshi, 1988).<papid> P88-1032 </papid></citsent>
<aftsection>
<nextsent>this algorithm is specialization to tag of more general algorithm dedicated to second order acgs.
</nextsent>
<nextsent>as second order acgs allows to encode linear context free rewriting systems (lcfrs) (degroote and pogodalla, 2004), the presentation of this algorithm gives rough presentation of the formal tools which can be used to design efficient algorithms for lcfrs.
</nextsent>
<nextsent>furthermore, as these tools allow to parse linear ?-terms, they can be used as basis for deve lopping algorithms for generation.
</nextsent>
<nextsent>the algorithm we present is specialization to tags of more general one dedicated to second order abstract categorial grammars (acgs) (de groote, 2001).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC319">
<title id=" W06-2505.xml">multilingual versus monolingual wsd </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the applications for which wsd has been thought to be helpful include information retrieval, information extraction, and machine translation (mt) (ide and vernis, 1998).
</prevsent>
<prevsent>the usefulness of wsd for mt, particularly, has been recently subject of debate, with conflicting results.
</prevsent>
</prevsection>
<citsent citstr=" H05-1097 ">
vickrey et al (2005), <papid> H05-1097 </papid>e.g., show that the inclusion of wsd module significantly improves the performance of their statistical mt system.</citsent>
<aftsection>
<nextsent>conversely, carpuat and wu (2005) <papid> P05-1048 </papid>found that wsd does not yield significantly better translation quality than statistical mt system alone.</nextsent>
<nextsent>in this latter work, however, the wsd module was not specifically designed for mt: it is based on the use of monolingual methods to identify the source language senses, which are then mapped into the target language transla tions.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC320">
<title id=" W06-2505.xml">multilingual versus monolingual wsd </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the usefulness of wsd for mt, particularly, has been recently subject of debate, with conflicting results.
</prevsent>
<prevsent>vickrey et al (2005), <papid> H05-1097 </papid>e.g., show that the inclusion of wsd module significantly improves the performance of their statistical mt system.</prevsent>
</prevsection>
<citsent citstr=" P05-1048 ">
conversely, carpuat and wu (2005) <papid> P05-1048 </papid>found that wsd does not yield significantly better translation quality than statistical mt system alone.</citsent>
<aftsection>
<nextsent>in this latter work, however, the wsd module was not specifically designed for mt: it is based on the use of monolingual methods to identify the source language senses, which are then mapped into the target language translations.
</nextsent>
<nextsent>in fact, although it has been agreed that wsd is more useful when it is meant for specific application (wilks and stevenson, 1998; kilgarriff, 1997; resnik and yarowsky, 1997), little has been done on the development of wsd modules specifically for particular applications.
</nextsent>
<nextsent>wsd models in general are application independent, and focus on monolingual contexts, particularly english.
</nextsent>
<nextsent>approaches to wsd as an application independent task usually apply standardised sense repositories, such as wordnet (miller, 1990).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC322">
<title id=" W06-2505.xml">multilingual versus monolingual wsd </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>they argued that differences in both these aspects do not depend only on the sense of the verb.
</prevsent>
<prevsent>out of the 14 senses analyzed, six had 2-5 different translations each.
</prevsent>
</prevsection>
<citsent citstr=" C04-1053 ">
bentivogli et al (2004) <papid> C04-1053 </papid>proposed an approach to create an italian sense tagged corpus (multisemcor) based on the transference of the annotations from the english sense tagged corpus semcor (miller et al, 1994), <papid> H94-1046 </papid>by means of word alignment methods.</citsent>
<aftsection>
<nextsent>a gold standard corpus was created by manually transferring senses in sem cor to the italian words in translated version of that corpus.
</nextsent>
<nextsent>from total of 1,054 english words, 155 annotations were considered nontransferable to their corresponding italian words, mainly due to the lack of synonymy at the lexical level.
</nextsent>
<nextsent>mihltz (2005) manually mapped senses from the english in sense tagged corpus to hungarian translations, in order to carry out wsd between these languages.
</nextsent>
<nextsent>out of 43 ambiguous nouns, 38 had all or most of their english senses mapped into the same hungarian translation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC323">
<title id=" W06-2505.xml">multilingual versus monolingual wsd </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>they argued that differences in both these aspects do not depend only on the sense of the verb.
</prevsent>
<prevsent>out of the 14 senses analyzed, six had 2-5 different translations each.
</prevsent>
</prevsection>
<citsent citstr=" H94-1046 ">
bentivogli et al (2004) <papid> C04-1053 </papid>proposed an approach to create an italian sense tagged corpus (multisemcor) based on the transference of the annotations from the english sense tagged corpus semcor (miller et al, 1994), <papid> H94-1046 </papid>by means of word alignment methods.</citsent>
<aftsection>
<nextsent>a gold standard corpus was created by manually transferring senses in sem cor to the italian words in translated version of that corpus.
</nextsent>
<nextsent>from total of 1,054 english words, 155 annotations were considered nontransferable to their corresponding italian words, mainly due to the lack of synonymy at the lexical level.
</nextsent>
<nextsent>mihltz (2005) manually mapped senses from the english in sense tagged corpus to hungarian translations, in order to carry out wsd between these languages.
</nextsent>
<nextsent>out of 43 ambiguous nouns, 38 had all or most of their english senses mapped into the same hungarian translation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC324">
<title id=" W06-2505.xml">multilingual versus monolingual wsd </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in general, the assumptions behind these approaches are the following: (1) if source language word is translated differently into second language, it might be ambiguous and the different translations can indicate the senses in the source language.
</prevsent>
<prevsent>34 (2) if two distinct source language words are translated as the same word into second language, it often indicates that the two are being used with similar senses.
</prevsent>
</prevsection>
<citsent citstr=" W99-0508 ">
ide (1999), <papid> W99-0508 </papid>for example, analyzes translations of english words into four different languages, in order to check if the different senses of an english word are lexicalized by different words in all the other languages.</citsent>
<aftsection>
<nextsent>a parallel aligned corpus is used and the translated senses are mapped into wordnet senses.
</nextsent>
<nextsent>she uses this information to determine set of monolingual sense distinctions that is potentially useful for nlp applications.
</nextsent>
<nextsent>in subsequent work (ide et al, 2002), <papid> W02-0808 </papid>seven languages and clustering techniques are employed to create sense groups based on the translations.</nextsent>
<nextsent>diab and resnik (2002) <papid> P02-1033 </papid>use multilingual information to create an english sense tagged corpus to train monolingual wsd approach.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC325">
<title id=" W06-2505.xml">multilingual versus monolingual wsd </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>a parallel aligned corpus is used and the translated senses are mapped into wordnet senses.
</prevsent>
<prevsent>she uses this information to determine set of monolingual sense distinctions that is potentially useful for nlp applications.
</prevsent>
</prevsection>
<citsent citstr=" W02-0808 ">
in subsequent work (ide et al, 2002), <papid> W02-0808 </papid>seven languages and clustering techniques are employed to create sense groups based on the translations.</citsent>
<aftsection>
<nextsent>diab and resnik (2002) <papid> P02-1033 </papid>use multilingual information to create an english sense tagged corpus to train monolingual wsd approach.</nextsent>
<nextsent>an english sense inventory and parallel corpus automatically produced by an mt system are employed.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC326">
<title id=" W06-2505.xml">multilingual versus monolingual wsd </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>she uses this information to determine set of monolingual sense distinctions that is potentially useful for nlp applications.
</prevsent>
<prevsent>in subsequent work (ide et al, 2002), <papid> W02-0808 </papid>seven languages and clustering techniques are employed to create sense groups based on the translations.</prevsent>
</prevsection>
<citsent citstr=" P02-1033 ">
diab and resnik (2002) <papid> P02-1033 </papid>use multilingual information to create an english sense tagged corpus to train monolingual wsd approach.</citsent>
<aftsection>
<nextsent>an english sense inventory and parallel corpus automatically produced by an mt system are employed.
</nextsent>
<nextsent>sentence and word alignment systems are used to assign the word correspondences between the two languages.
</nextsent>
<nextsent>after grouping all the words that correspond to translations of single word in the target language, all their possible senses are considered as candidates.
</nextsent>
<nextsent>the sense that maximizes the semantic similarity of the word with the others in the group is chosen.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC327">
<title id=" W06-2505.xml">multilingual versus monolingual wsd </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>after grouping all the words that correspond to translations of single word in the target language, all their possible senses are considered as candidates.
</prevsent>
<prevsent>the sense that maximizes the semantic similarity of the word with the others in the group is chosen.
</prevsent>
</prevsection>
<citsent citstr=" P03-1058 ">
similarly, ng et al (2003) <papid> P03-1058 </papid>employ english chinese parallel word aligned corpora to identify repository of senses for english.</citsent>
<aftsection>
<nextsent>the english word senses are manually defined, based on the wordnet senses, and then revised in the light of the chinese translations.
</nextsent>
<nextsent>for example, if two occurrences of word with two different senses in wordnet are translated into the same chinese word, they will be considered to have the same english sense.
</nextsent>
<nextsent>in general, these approaches relyon the two previously mentioned assumptions about the interaction between translations and word senses.
</nextsent>
<nextsent>although these assumptions can be useful when using cross-language information as an approximation to monolingual disambiguation, they are not very helpful in the opposite direction, i.e., using monolingual information for cross language disambiguation, as we will show in section 4.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC328">
<title id=" W06-2505.xml">multilingual versus monolingual wsd </title>
<section> results and discussion.  </section>
<citcontext>
<prevsection>
<prevsent>therefore, this also shows that, for these verbs, sense inventories that are specific to the translation between the pair of languages under consideration would be more appropriate to achieve effective wsd.
</prevsent>
<prevsent>5.1 agreement between translators.
</prevsent>
</prevsection>
<citsent citstr=" J96-2004 ">
in an attempt to quantify the agreement between the two groups of translators, we computed the kappa coefficient for annotation tasks, as defined by carletta (1996).<papid> J96-2004 </papid></citsent>
<aftsection>
<nextsent>kappa was calculated separately for our two areas of inquiry, i.e., cases (1) and (2) discussed in section 5.
</nextsent>
<nextsent>in the experiment referring to case (1), groups were considered to agree about sense of verb if they both judged that the translation of such 38verb # sen tences# senses # translations % (a) % (b) (b) average % (c&d;) (c&d;) average % (e) ask 83 8 3 100 0 0 87.5 3.5 12.5 come 202 68 42 62 38 3.1 73.2 6.3 26.8 get 226 90 61 70 30 2.6 61.1 3.4 38.9 give 241 57 12 48.7 51.3 3.3 84.2 6.3 15.8 live 55 10 7 83.3 16.7 3.0 70 2.7 30 look 82 26 18 63.2 36.8 2.4 84.6 2.7 15.4 make 225 53 42 51.4 48.6 2.9 77.4 4.1 22.6 tell 73 10 10 37.5 62.5 2.8 60 4.0 40 table 4.
</nextsent>
<nextsent>results of the procedure contrasting senses and translations verb was or was not shared by other senses.
</nextsent>
<nextsent>for example, both groups agreed that the word fazer?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC329">
<title id=" W06-2911.xml">applying alternating structure optimization to word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>sense and the river bank?
</prevsent>
<prevsent>sense.
</prevsent>
</prevsection>
<citsent citstr=" W02-1004 ">
previous studies e.g., (lee and ng, 2002; florian and yarowsky, 2002), <papid> W02-1004 </papid>have applied supervised learning techniques to wsd with success.</citsent>
<aftsection>
<nextsent>a practical issue that arises in supervised wsd is the paucity of labeled examples (sense-annotateddata) available for training.
</nextsent>
<nextsent>for example, the training set of the senseval-21 english lexical sample 1http://www.cs.unt.edu/~rada/senseval/.
</nextsent>
<nextsent>wsd systems have task has only 10 labeled training examples per sense on average, which is in contrast to nearly 6k training examples per name class (on average) used for the conll-2003 named entity chunking shared task2.
</nextsent>
<nextsent>one problem is that there are so many words and somany senses that it is hard to make available sufficient number of labeled training examples for each of large number of target words.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC330">
<title id=" W06-2911.xml">applying alternating structure optimization to word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>on the other hand, this indicates that the total number of available labeled examples (irrespective of target words) can be relatively large.
</prevsent>
<prevsent>a natural question to ask is whether we can effectively use all the labeled examples (irrespective of target words) for learning on each individual wsd problem.
</prevsent>
</prevsection>
<citsent citstr=" P05-1001 ">
based on these observations, we study new application of alternating structure optimization (aso) (ando and zhang, 2005<papid> P05-1001 </papid>a; ando and zhang,2005<papid> P05-1001 </papid>b) to wsd.</citsent>
<aftsection>
<nextsent>aso is recently proposed machine learning method for learning predictive structure (i.e., information useful for predictions) shared by multiple prediction problems via joint empirical risk minimization.
</nextsent>
<nextsent>it has been shown that on several tasks, performance can be significantly improved by semi-supervised application of aso, which obtains useful information from unlabeled data by learning automatically created predictionproblems.
</nextsent>
<nextsent>in addition to such semi-supervised learning, this paper explores aso multi-task learning,which learns number of wsd problems simultaneously to exploit the inherent predictive structure shared by these wsd problems.
</nextsent>
<nextsent>thus, in effect, each individual problem (e.g., disambiguation of art?)
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC334">
<title id=" W06-2911.xml">applying alternating structure optimization to word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the notion of benefiting from training data for other word senses is not new by itself.
</prevsent>
<prevsent>for instance, been evaluated in the series of senseval workshops.
</prevsent>
</prevsection>
<citsent citstr=" P05-1005 ">
2http://www.cnts.ua.ac.be/conll2003/ner/ 77 on the wsd task with respect to wordnet synsets, kohomban and lee (2005) <papid> P05-1005 </papid>trained classifiers for thetop-level synsets of the wordnet semantic hierarchy, consolidating labeled examples associated withthe wordnet sub-trees.</citsent>
<aftsection>
<nextsent>to disambiguate test instances, these coarse-grained classifiers are first applied, and then fine-grained senses are determined using heuristic mapping.
</nextsent>
<nextsent>by contrast, our approach does not require pre-defined relations among senses such as the wordnet hierarchy.
</nextsent>
<nextsent>rather, welet the machine learning algorithm aso automatically and implicitly find relations with respect to the disambiguation problems (i.e., finding shared predictive structure).
</nextsent>
<nextsent>interestingly, in our experiments,seemingly unrelated or only loosely related word sense pairs help to improve performance.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC343">
<title id=" W06-2911.xml">applying alternating structure optimization to word sense disambiguation </title>
<section> alternating structure optimization.  </section>
<citcontext>
<prevsection>
<prevsent>each instance contains an occurrence of one of the target words and its surrounding words, typically few sentences.
</prevsent>
<prevsent>the task is to assign sense to each test instance.
</prevsent>
</prevsection>
<citsent citstr=" W02-1006 ">
features we adopt the feature design used by lee and ng (2002), <papid> W02-1006 </papid>which consists of the following four types: (1) local context: n-grams of nearby words (position sensitive); (2) global context: allthe words (excluding stopwords) in the given context (position-insensitive; bag of words); (3) pos:parts-of-speech n-grams of nearby words; (4) syn 79 tactic relations: syntactic information obtained from parser output.</citsent>
<aftsection>
<nextsent>to generate syntactic relation features, we use the slot grammar-based full parser esg (mccord, 1990).
</nextsent>
<nextsent>we use as features syntactic relation types (e.g., subject-of, object-of, and nounmodifier), participants of syntactic relations, and bi grams of syntactic relations / participants.
</nextsent>
<nextsent>details of the other three types are shown in figure 2.
</nextsent>
<nextsent>implementation our implementation follows ando and zhang (2005<papid> P05-1001 </papid>a).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC353">
<title id=" W06-2911.xml">applying alternating structure optimization to word sense disambiguation </title>
<section> alternating structure optimization.  </section>
<citcontext>
<prevsection>
<prevsent>(in section 4, we will present evaluation results on aso multi-task learning (optimum config.)
</prevsent>
<prevsent>68.1 classifier combination [fy02] 66.5 polynomial kpca [wsc04] 65.8 svm [ln02] 65.4 our single-task baseline 65.3 senseval-2 (2001) best participant 64.2figure 7: performance comparison with previous best systems on senseval-2 english lexical sample test set.
</prevsent>
</prevsection>
<citsent citstr=" P04-1081 ">
fy02 (flo rian and yarowsky, 2002), <papid> W02-1004 </papid>wsc04 (wu et al, 2004), <papid> P04-1081 </papid>ln02 (lee and ng, 2002) <papid> W02-1006 </papid>the unseen senseval-3 test sets.)</citsent>
<aftsection>
<nextsent>nevertheless, it is worth noting that our potential performance (68.1%) exceeds those of the previous best systems.
</nextsent>
<nextsent>our single-task baseline performance is almost the same as ln02 (lee and ng, 2002), which uses svm.
</nextsent>
<nextsent>this is consistent with the fact that we adopted ln02s feature design.
</nextsent>
<nextsent>fy02 (florian and yarowsky, 2002) <papid> W02-1004 </papid>combines classifiers by linear average stacking.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC366">
<title id=" W06-2911.xml">applying alternating structure optimization to word sense disambiguation </title>
<section> alternating structure optimization.  </section>
<citcontext>
<prevsection>
<prevsent>an alternative to aso for this purpose is to use directly as features the output values of classifiers trained for disambiguating the other words, which we call output-based method?
</prevsent>
<prevsent>(cf.
</prevsent>
</prevsection>
<citsent citstr=" W03-0425 ">
florian et al (2003)<papid> W03-0425 </papid></citsent>
<aftsection>
<nextsent>we explore several variations similarly to section 3.1 and report the ceiling performance.
</nextsent>
<nextsent>4.4 evaluation results.
</nextsent>
<nextsent>figure 10 shows f-measure results on the four senseval-3 datasets using the official training / testsplits.
</nextsent>
<nextsent>both aso multi-task learning and semi supervised learning improve performance over the #words #train avg #sense avg #train per word per sense english 73 8611 10.7 10.0 senseval-3 datasets english 57 7860 6.5 21.3 catalan 27 4469 3.1 53.2 italian 45 5145 6.2 18.4 spanish 46 8430 3.3 55.5 figure 9: data statistics of senseval-2 english lexical sample dataset (first row) and senseval-3 datasets.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC369">
<title id=" W06-2911.xml">applying alternating structure optimization to word sense disambiguation </title>
<section> alternating structure optimization.  </section>
<citcontext>
<prevsection>
<prevsent>83 methods english catalan italian spanish multi-task learning 73.8 (+0.8) 89.5 (+1.5) 63.2 (+4.9) 89.0 (+1.0) aso semi-supervised learning 73.5 (+0.5) 88.6 (+0.6) 62.4 (+4.1) 88.9 (+0.9) multi-task+semi-supervised 74.1 (+1.1) 89.9 (+1.9) 64.0 (+5.7) 89.5 (+1.5) baselines output-based 73.0 (0.0) 88.3 (+0.3) 58.0 (-0.3) 88.2 (+0.2) single-task supervised learning 73.0 88.0 58.3 88.0 previous svm with lsa kernel [ggs05] 73.3 89.0 61.3 88.2 systems senseval-3 (2004) best systems 72.9 [g04] 85.2 [sgg04] 53.1 [sgg04] 84.2 [sgg04] inter-annotator agreement 67.3 93.1 89.0 85.3 figure 10: performance results on the senseval-3 lexical sample test sets.
</prevsent>
<prevsent>numbers in the parentheses are performance gains compared with the single-task supervised baseline (italicized).
</prevsent>
</prevsection>
<citsent citstr=" W04-0831 ">
[g04] grozea (2004); [<papid> W04-0831 </papid>sgg04] strapparava et al (2004).<papid> W04-0856 </papid></citsent>
<aftsection>
<nextsent>ggs05 combined various kernels, which includes the lsa kernel that exploits unlabeled data with global context features.
</nextsent>
<nextsent>our implementation of thelsa kernel with our classifier (and our other fea tures) also produced performance similar to that of ggs05.
</nextsent>
<nextsent>while the lsa kernel is closely related to special case of the semi-supervised application of aso (see the discussion of pca in ando and zhang (2005<papid> P05-1001 </papid>a)), our approach here is more general in that we exploit not only unlabeled data and global context features but also the labeled examples of other target words and other types of features.</nextsent>
<nextsent>g04achieved high performance on english using regularized least squares with compensation for skewed class distributions.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC370">
<title id=" W06-2911.xml">applying alternating structure optimization to word sense disambiguation </title>
<section> alternating structure optimization.  </section>
<citcontext>
<prevsection>
<prevsent>83 methods english catalan italian spanish multi-task learning 73.8 (+0.8) 89.5 (+1.5) 63.2 (+4.9) 89.0 (+1.0) aso semi-supervised learning 73.5 (+0.5) 88.6 (+0.6) 62.4 (+4.1) 88.9 (+0.9) multi-task+semi-supervised 74.1 (+1.1) 89.9 (+1.9) 64.0 (+5.7) 89.5 (+1.5) baselines output-based 73.0 (0.0) 88.3 (+0.3) 58.0 (-0.3) 88.2 (+0.2) single-task supervised learning 73.0 88.0 58.3 88.0 previous svm with lsa kernel [ggs05] 73.3 89.0 61.3 88.2 systems senseval-3 (2004) best systems 72.9 [g04] 85.2 [sgg04] 53.1 [sgg04] 84.2 [sgg04] inter-annotator agreement 67.3 93.1 89.0 85.3 figure 10: performance results on the senseval-3 lexical sample test sets.
</prevsent>
<prevsent>numbers in the parentheses are performance gains compared with the single-task supervised baseline (italicized).
</prevsent>
</prevsection>
<citsent citstr=" W04-0856 ">
[g04] grozea (2004); [<papid> W04-0831 </papid>sgg04] strapparava et al (2004).<papid> W04-0856 </papid></citsent>
<aftsection>
<nextsent>ggs05 combined various kernels, which includes the lsa kernel that exploits unlabeled data with global context features.
</nextsent>
<nextsent>our implementation of thelsa kernel with our classifier (and our other fea tures) also produced performance similar to that of ggs05.
</nextsent>
<nextsent>while the lsa kernel is closely related to special case of the semi-supervised application of aso (see the discussion of pca in ando and zhang (2005<papid> P05-1001 </papid>a)), our approach here is more general in that we exploit not only unlabeled data and global context features but also the labeled examples of other target words and other types of features.</nextsent>
<nextsent>g04achieved high performance on english using regularized least squares with compensation for skewed class distributions.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC376">
<title id=" W06-2001.xml">multilingual extension of a temporal expression normalizer using annotated corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>multilingual/cross-language capabilities among the hottest research topics, such as the european cross-language evaluation forum2 (clef), successful evaluation campaign which aims at fostering research in different areas of multilingual information retrieval.
</prevsent>
<prevsent>at the same time, in the temporal expressions recognition and normalization field, systems featuring multilingual capabilities have been proposed.
</prevsent>
</prevsection>
<citsent citstr=" W01-1306 ">
among others, (moia, 2001; <papid> W01-1306 </papid>wilson et al, 2001; <papid> W01-1312 </papid>negri and marseglia, 2004) emphasized the potentialities of such applications for different information retrieval related tasks.as many other nlp areas, research in automated temporal reasoning has recently seen the emergence of machine learning approaches trying to overcome the difficulties of extending language model to other languages (carpenter, 2004;ittycheriah et al, 2003).<papid> N03-2014 </papid></citsent>
<aftsection>
<nextsent>in this direction, the outcomes of the first time expression recognition and normalization workshop (tern 20043) provide clear indication of the state of the field.
</nextsent>
<nextsent>inspite of the good results obtained in the recognition task, normalization by means of machine learning techniques still shows relatively poor results with respect to rule-based approaches, and still remains an unresolved problem.the difficulty of porting systems to new languages (or domains) affects both rule-based and machine learning approaches.
</nextsent>
<nextsent>with rule-based approaches (schilder and habel, 2001; <papid> W01-1309 </papid>filatova and hovy, 2001), <papid> W01-1313 </papid>the main problems are related to the fact that the porting process requires rewriting from scratch, or adapting to each new language,large numbers of rules, which is costly and time 2http://www.clef-campaign.org/ 3http://timex2.mitre.org/tern.html 1 consuming work.</nextsent>
<nextsent>machine learning approaches (setzer and gaizauskas, 2002; katz and arosio, 2001), <papid> W01-1315 </papid>on the other hand, can be extended with little human intervention through the use of language corpora.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC377">
<title id=" W06-2001.xml">multilingual extension of a temporal expression normalizer using annotated corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>multilingual/cross-language capabilities among the hottest research topics, such as the european cross-language evaluation forum2 (clef), successful evaluation campaign which aims at fostering research in different areas of multilingual information retrieval.
</prevsent>
<prevsent>at the same time, in the temporal expressions recognition and normalization field, systems featuring multilingual capabilities have been proposed.
</prevsent>
</prevsection>
<citsent citstr=" W01-1312 ">
among others, (moia, 2001; <papid> W01-1306 </papid>wilson et al, 2001; <papid> W01-1312 </papid>negri and marseglia, 2004) emphasized the potentialities of such applications for different information retrieval related tasks.as many other nlp areas, research in automated temporal reasoning has recently seen the emergence of machine learning approaches trying to overcome the difficulties of extending language model to other languages (carpenter, 2004;ittycheriah et al, 2003).<papid> N03-2014 </papid></citsent>
<aftsection>
<nextsent>in this direction, the outcomes of the first time expression recognition and normalization workshop (tern 20043) provide clear indication of the state of the field.
</nextsent>
<nextsent>inspite of the good results obtained in the recognition task, normalization by means of machine learning techniques still shows relatively poor results with respect to rule-based approaches, and still remains an unresolved problem.the difficulty of porting systems to new languages (or domains) affects both rule-based and machine learning approaches.
</nextsent>
<nextsent>with rule-based approaches (schilder and habel, 2001; <papid> W01-1309 </papid>filatova and hovy, 2001), <papid> W01-1313 </papid>the main problems are related to the fact that the porting process requires rewriting from scratch, or adapting to each new language,large numbers of rules, which is costly and time 2http://www.clef-campaign.org/ 3http://timex2.mitre.org/tern.html 1 consuming work.</nextsent>
<nextsent>machine learning approaches (setzer and gaizauskas, 2002; katz and arosio, 2001), <papid> W01-1315 </papid>on the other hand, can be extended with little human intervention through the use of language corpora.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC378">
<title id=" W06-2001.xml">multilingual extension of a temporal expression normalizer using annotated corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>multilingual/cross-language capabilities among the hottest research topics, such as the european cross-language evaluation forum2 (clef), successful evaluation campaign which aims at fostering research in different areas of multilingual information retrieval.
</prevsent>
<prevsent>at the same time, in the temporal expressions recognition and normalization field, systems featuring multilingual capabilities have been proposed.
</prevsent>
</prevsection>
<citsent citstr=" N03-2014 ">
among others, (moia, 2001; <papid> W01-1306 </papid>wilson et al, 2001; <papid> W01-1312 </papid>negri and marseglia, 2004) emphasized the potentialities of such applications for different information retrieval related tasks.as many other nlp areas, research in automated temporal reasoning has recently seen the emergence of machine learning approaches trying to overcome the difficulties of extending language model to other languages (carpenter, 2004;ittycheriah et al, 2003).<papid> N03-2014 </papid></citsent>
<aftsection>
<nextsent>in this direction, the outcomes of the first time expression recognition and normalization workshop (tern 20043) provide clear indication of the state of the field.
</nextsent>
<nextsent>inspite of the good results obtained in the recognition task, normalization by means of machine learning techniques still shows relatively poor results with respect to rule-based approaches, and still remains an unresolved problem.the difficulty of porting systems to new languages (or domains) affects both rule-based and machine learning approaches.
</nextsent>
<nextsent>with rule-based approaches (schilder and habel, 2001; <papid> W01-1309 </papid>filatova and hovy, 2001), <papid> W01-1313 </papid>the main problems are related to the fact that the porting process requires rewriting from scratch, or adapting to each new language,large numbers of rules, which is costly and time 2http://www.clef-campaign.org/ 3http://timex2.mitre.org/tern.html 1 consuming work.</nextsent>
<nextsent>machine learning approaches (setzer and gaizauskas, 2002; katz and arosio, 2001), <papid> W01-1315 </papid>on the other hand, can be extended with little human intervention through the use of language corpora.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC379">
<title id=" W06-2001.xml">multilingual extension of a temporal expression normalizer using annotated corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this direction, the outcomes of the first time expression recognition and normalization workshop (tern 20043) provide clear indication of the state of the field.
</prevsent>
<prevsent>inspite of the good results obtained in the recognition task, normalization by means of machine learning techniques still shows relatively poor results with respect to rule-based approaches, and still remains an unresolved problem.the difficulty of porting systems to new languages (or domains) affects both rule-based and machine learning approaches.
</prevsent>
</prevsection>
<citsent citstr=" W01-1309 ">
with rule-based approaches (schilder and habel, 2001; <papid> W01-1309 </papid>filatova and hovy, 2001), <papid> W01-1313 </papid>the main problems are related to the fact that the porting process requires rewriting from scratch, or adapting to each new language,large numbers of rules, which is costly and time 2http://www.clef-campaign.org/ 3http://timex2.mitre.org/tern.html 1 consuming work.</citsent>
<aftsection>
<nextsent>machine learning approaches (setzer and gaizauskas, 2002; katz and arosio, 2001), <papid> W01-1315 </papid>on the other hand, can be extended with little human intervention through the use of language corpora.</nextsent>
<nextsent>however, the large annotated corpora that are necessary to obtain high performance are not always available.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC380">
<title id=" W06-2001.xml">multilingual extension of a temporal expression normalizer using annotated corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this direction, the outcomes of the first time expression recognition and normalization workshop (tern 20043) provide clear indication of the state of the field.
</prevsent>
<prevsent>inspite of the good results obtained in the recognition task, normalization by means of machine learning techniques still shows relatively poor results with respect to rule-based approaches, and still remains an unresolved problem.the difficulty of porting systems to new languages (or domains) affects both rule-based and machine learning approaches.
</prevsent>
</prevsection>
<citsent citstr=" W01-1313 ">
with rule-based approaches (schilder and habel, 2001; <papid> W01-1309 </papid>filatova and hovy, 2001), <papid> W01-1313 </papid>the main problems are related to the fact that the porting process requires rewriting from scratch, or adapting to each new language,large numbers of rules, which is costly and time 2http://www.clef-campaign.org/ 3http://timex2.mitre.org/tern.html 1 consuming work.</citsent>
<aftsection>
<nextsent>machine learning approaches (setzer and gaizauskas, 2002; katz and arosio, 2001), <papid> W01-1315 </papid>on the other hand, can be extended with little human intervention through the use of language corpora.</nextsent>
<nextsent>however, the large annotated corpora that are necessary to obtain high performance are not always available.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC381">
<title id=" W06-2001.xml">multilingual extension of a temporal expression normalizer using annotated corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>inspite of the good results obtained in the recognition task, normalization by means of machine learning techniques still shows relatively poor results with respect to rule-based approaches, and still remains an unresolved problem.the difficulty of porting systems to new languages (or domains) affects both rule-based and machine learning approaches.
</prevsent>
<prevsent>with rule-based approaches (schilder and habel, 2001; <papid> W01-1309 </papid>filatova and hovy, 2001), <papid> W01-1313 </papid>the main problems are related to the fact that the porting process requires rewriting from scratch, or adapting to each new language,large numbers of rules, which is costly and time 2http://www.clef-campaign.org/ 3http://timex2.mitre.org/tern.html 1 consuming work.</prevsent>
</prevsection>
<citsent citstr=" W01-1315 ">
machine learning approaches (setzer and gaizauskas, 2002; katz and arosio, 2001), <papid> W01-1315 </papid>on the other hand, can be extended with little human intervention through the use of language corpora.</citsent>
<aftsection>
<nextsent>however, the large annotated corpora that are necessary to obtain high performance are not always available.
</nextsent>
<nextsent>in this paper we describe new procedure to build temporal models for new languages, starting from previously defined ones.
</nextsent>
<nextsent>while still adhering to the rule-based paradigm, its main contribution is the proposal of simple, but effective, methodology to automate the porting ofa system from one language to another.
</nextsent>
<nextsent>in this procedure, we take advantage of the architecture of an existing system developed for spanish (terseo, see (saquete et al, 2005)), where the recognition model is language-dependent but the normalizing procedure is completely independent.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC382">
<title id=" W06-3105.xml">why generative phrase models underperform surface heuristics </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>alternate segment ations rather than alternate alignments compete, resulting in increased determinization of the phrase table, decreased generalization, and decreased final bleu score.
</prevsent>
<prevsent>we also show that interpolation of the two methods can result in modest increase in bleu score.
</prevsent>
</prevsection>
<citsent citstr=" N03-1017 ">
at the core of phrase-based statistical machine translation system is phrase table containing pairs of source and target language phrases, each weighted by conditional translation probability.koehn et al (2003<papid> N03-1017 </papid>a) showed that translation quality is very sensitive to how this table is extracted from the training data.</citsent>
<aftsection>
<nextsent>one particularly surprising result is that simple heuristic extraction algorithm based on surface statistics of word-aligned training set outperformed the phrase-based generative model proposed by marcu and wong (2002).<papid> W02-1018 </papid>this result is surprising in light of the reverse situation for word-based statistical translation.</nextsent>
<nextsent>specifically, in the task of word alignment, heuristic approaches such as the dice coefficient consistently underperform their re-estimated counterparts, such as the ibm word alignment models (brown et al,1993).<papid> J93-2003 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC384">
<title id=" W06-3105.xml">why generative phrase models underperform surface heuristics </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we also show that interpolation of the two methods can result in modest increase in bleu score.
</prevsent>
<prevsent>at the core of phrase-based statistical machine translation system is phrase table containing pairs of source and target language phrases, each weighted by conditional translation probability.koehn et al (2003<papid> N03-1017 </papid>a) showed that translation quality is very sensitive to how this table is extracted from the training data.</prevsent>
</prevsection>
<citsent citstr=" W02-1018 ">
one particularly surprising result is that simple heuristic extraction algorithm based on surface statistics of word-aligned training set outperformed the phrase-based generative model proposed by marcu and wong (2002).<papid> W02-1018 </papid>this result is surprising in light of the reverse situation for word-based statistical translation.</citsent>
<aftsection>
<nextsent>specifically, in the task of word alignment, heuristic approaches such as the dice coefficient consistently underperform their re-estimated counterparts, such as the ibm word alignment models (brown et al,1993).<papid> J93-2003 </papid></nextsent>
<nextsent>this well-known result is unsurprising: reestimation introduces an element of competition into the learning process.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC385">
<title id=" W06-3105.xml">why generative phrase models underperform surface heuristics </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>at the core of phrase-based statistical machine translation system is phrase table containing pairs of source and target language phrases, each weighted by conditional translation probability.koehn et al (2003<papid> N03-1017 </papid>a) showed that translation quality is very sensitive to how this table is extracted from the training data.</prevsent>
<prevsent>one particularly surprising result is that simple heuristic extraction algorithm based on surface statistics of word-aligned training set outperformed the phrase-based generative model proposed by marcu and wong (2002).<papid> W02-1018 </papid>this result is surprising in light of the reverse situation for word-based statistical translation.</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
specifically, in the task of word alignment, heuristic approaches such as the dice coefficient consistently underperform their re-estimated counterparts, such as the ibm word alignment models (brown et al,1993).<papid> J93-2003 </papid></citsent>
<aftsection>
<nextsent>this well-known result is unsurprising: reestimation introduces an element of competition into the learning process.
</nextsent>
<nextsent>the key virtue of competition in word alignment is that, to first approximation, only one source word should generate each target word.
</nextsent>
<nextsent>if good alignment for word token is found, other plausible alignments are explained away and should be discounted as incorrect for that token.as we show in this paper, this effect does not prevail for phrase-level alignments.
</nextsent>
<nextsent>the central difference is that phrase-based models, such as the ones presented in section 2 or marcu and wong (2002), <papid> W02-1018 </papid>contain an element of segmentation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC394">
<title id=" W06-3815.xml">context comparison as a minimum cost flow problem </title>
<section> the network flow method.  </section>
<citcontext>
<prevsection>
<prevsent>one simple method is to transfer each element in the word vector(i.e., the frequency of each word) to the corresponding concepts in the ontology, resulting in vector of concept frequencies.
</prevsent>
<prevsent>in this paper, we have chosen uniform distribution of word frequency counts among concepts, instead of weighted distribution towards the relevant concepts for particular text.
</prevsent>
</prevsection>
<citsent citstr=" P98-2124 ">
since we wish to evaluate the strength of our method alone without any additional nlp effort, we bypass the issue of approximating the true distribution ofthe concepts via word sense disambiguation or class based approximation methods, such as those by li and abe (1998) <papid> P98-2124 </papid>and clark and weir (2002).<papid> J02-2003 </papid></citsent>
<aftsection>
<nextsent>to calculate the distance between two profiles, we need to cast one profile as the supply ( ? ) and the other as the demand ( ? ).
</nextsent>
<nextsent>note that our distance is symmetric, so the choice of the supply and the demand is arbitrary.
</nextsent>
<nextsent>next, we must determine the value of fffiflffi at each concept node  ; this is justthe difference between the (normalized) supply frequency n??ffi??
</nextsent>
<nextsent>and demand frequency ????ffid : } . [|\???
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC395">
<title id=" W06-3815.xml">context comparison as a minimum cost flow problem </title>
<section> the network flow method.  </section>
<citcontext>
<prevsection>
<prevsent>one simple method is to transfer each element in the word vector(i.e., the frequency of each word) to the corresponding concepts in the ontology, resulting in vector of concept frequencies.
</prevsent>
<prevsent>in this paper, we have chosen uniform distribution of word frequency counts among concepts, instead of weighted distribution towards the relevant concepts for particular text.
</prevsent>
</prevsection>
<citsent citstr=" J02-2003 ">
since we wish to evaluate the strength of our method alone without any additional nlp effort, we bypass the issue of approximating the true distribution ofthe concepts via word sense disambiguation or class based approximation methods, such as those by li and abe (1998) <papid> P98-2124 </papid>and clark and weir (2002).<papid> J02-2003 </papid></citsent>
<aftsection>
<nextsent>to calculate the distance between two profiles, we need to cast one profile as the supply ( ? ) and the other as the demand ( ? ).
</nextsent>
<nextsent>note that our distance is symmetric, so the choice of the supply and the demand is arbitrary.
</nextsent>
<nextsent>next, we must determine the value of fffiflffi at each concept node  ; this is justthe difference between the (normalized) supply frequency n??ffi??
</nextsent>
<nextsent>and demand frequency ????ffid : } . [|\???
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC398">
<title id=" W06-3707.xml">medslt a limited domain unidirectional grammar based medical speech translator </title>
<section> the development environment.  </section>
<citcontext>
<prevsection>
<prevsent>as probst and levin suggest, one obvious way to attack the problem is to implement (formal or informal) elicitation strategy, which biases the informant towards translations which are consistent with the existing ones.
</prevsent>
<prevsent>this is the approach we have adopted in medslt.
</prevsent>
</prevsection>
<citsent citstr=" E03-2010 ">
the regulus platform, on which medsltis based, supports rapid construction of complex grammar-based language models; it uses an example-based method driven by small corpora of disambiguated parsed examples (rayner et al, 2003; <papid> E03-2010 </papid>rayner et al, 2006), which extracts most ofthe structure of the model from general linguistically motivated resource grammar.</citsent>
<aftsection>
<nextsent>the result isa specialised version of the general grammar, tailored to the example corpus, which can then be compiled into an efficient recogniser or into genera language vocab wer semer english 441 6% 18% french 1025 8% 10% japanese 347 4% 4% table 2: recognition performance for english, french and japanese headache-domain recognisers.vocab?
</nextsent>
<nextsent>= number of surface words in source language recogniser vocabulary; wer?
</nextsent>
<nextsent>= word error rate for source language recogniser, on in-coverage material; semer?
</nextsent>
<nextsent>= semantic error rate for source language recogniser, on in-coverage material.tion module.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC399">
<title id=" W06-2007.xml">word sense disambiguation using automatically translated sense examples </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this is the so-called knowledge acquisition bottleneck.
</prevsent>
<prevsent>to overcome this bottleneck, unsupervised wsd approaches have been proposed.
</prevsent>
</prevsection>
<citsent citstr=" J94-4003 ">
among them, systems under the multilingual paradigm have shown great promise (gale et al, 1992; da gan and itai, 1994; <papid> J94-4003 </papid>diab and resnik, 2002; <papid> P02-1033 </papid>ng et al., 2003; <papid> P03-1058 </papid>li and li, 2004; <papid> J04-1001 </papid>chan and ng, 2005;wang and carroll, 2005).<papid> H05-1069 </papid></citsent>
<aftsection>
<nextsent>the underlying hypothesis is that mappings between word forms and meanings can be different from language tolanguage.
</nextsent>
<nextsent>much work have been done on extracting sense examples from parallel corpora for wsd.
</nextsent>
<nextsent>for example, ng et al (2003) <papid> P03-1058 </papid>proposed to train classifier on sense examples acquired from word-aligned english-chinese parallel cor pora.</nextsent>
<nextsent>they grouped senses that share the same chinese translation, and then the occurrences of the word on the english side of the parallel corpora were considered to have been disambiguated and sense tagged?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC400">
<title id=" W06-2007.xml">word sense disambiguation using automatically translated sense examples </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this is the so-called knowledge acquisition bottleneck.
</prevsent>
<prevsent>to overcome this bottleneck, unsupervised wsd approaches have been proposed.
</prevsent>
</prevsection>
<citsent citstr=" P02-1033 ">
among them, systems under the multilingual paradigm have shown great promise (gale et al, 1992; da gan and itai, 1994; <papid> J94-4003 </papid>diab and resnik, 2002; <papid> P02-1033 </papid>ng et al., 2003; <papid> P03-1058 </papid>li and li, 2004; <papid> J04-1001 </papid>chan and ng, 2005;wang and carroll, 2005).<papid> H05-1069 </papid></citsent>
<aftsection>
<nextsent>the underlying hypothesis is that mappings between word forms and meanings can be different from language tolanguage.
</nextsent>
<nextsent>much work have been done on extracting sense examples from parallel corpora for wsd.
</nextsent>
<nextsent>for example, ng et al (2003) <papid> P03-1058 </papid>proposed to train classifier on sense examples acquired from word-aligned english-chinese parallel cor pora.</nextsent>
<nextsent>they grouped senses that share the same chinese translation, and then the occurrences of the word on the english side of the parallel corpora were considered to have been disambiguated and sense tagged?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC401">
<title id=" W06-2007.xml">word sense disambiguation using automatically translated sense examples </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this is the so-called knowledge acquisition bottleneck.
</prevsent>
<prevsent>to overcome this bottleneck, unsupervised wsd approaches have been proposed.
</prevsent>
</prevsection>
<citsent citstr=" P03-1058 ">
among them, systems under the multilingual paradigm have shown great promise (gale et al, 1992; da gan and itai, 1994; <papid> J94-4003 </papid>diab and resnik, 2002; <papid> P02-1033 </papid>ng et al., 2003; <papid> P03-1058 </papid>li and li, 2004; <papid> J04-1001 </papid>chan and ng, 2005;wang and carroll, 2005).<papid> H05-1069 </papid></citsent>
<aftsection>
<nextsent>the underlying hypothesis is that mappings between word forms and meanings can be different from language tolanguage.
</nextsent>
<nextsent>much work have been done on extracting sense examples from parallel corpora for wsd.
</nextsent>
<nextsent>for example, ng et al (2003) <papid> P03-1058 </papid>proposed to train classifier on sense examples acquired from word-aligned english-chinese parallel cor pora.</nextsent>
<nextsent>they grouped senses that share the same chinese translation, and then the occurrences of the word on the english side of the parallel corpora were considered to have been disambiguated and sense tagged?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC402">
<title id=" W06-2007.xml">word sense disambiguation using automatically translated sense examples </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this is the so-called knowledge acquisition bottleneck.
</prevsent>
<prevsent>to overcome this bottleneck, unsupervised wsd approaches have been proposed.
</prevsent>
</prevsection>
<citsent citstr=" J04-1001 ">
among them, systems under the multilingual paradigm have shown great promise (gale et al, 1992; da gan and itai, 1994; <papid> J94-4003 </papid>diab and resnik, 2002; <papid> P02-1033 </papid>ng et al., 2003; <papid> P03-1058 </papid>li and li, 2004; <papid> J04-1001 </papid>chan and ng, 2005;wang and carroll, 2005).<papid> H05-1069 </papid></citsent>
<aftsection>
<nextsent>the underlying hypothesis is that mappings between word forms and meanings can be different from language tolanguage.
</nextsent>
<nextsent>much work have been done on extracting sense examples from parallel corpora for wsd.
</nextsent>
<nextsent>for example, ng et al (2003) <papid> P03-1058 </papid>proposed to train classifier on sense examples acquired from word-aligned english-chinese parallel cor pora.</nextsent>
<nextsent>they grouped senses that share the same chinese translation, and then the occurrences of the word on the english side of the parallel corpora were considered to have been disambiguated and sense tagged?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC403">
<title id=" W06-2007.xml">word sense disambiguation using automatically translated sense examples </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this is the so-called knowledge acquisition bottleneck.
</prevsent>
<prevsent>to overcome this bottleneck, unsupervised wsd approaches have been proposed.
</prevsent>
</prevsection>
<citsent citstr=" H05-1069 ">
among them, systems under the multilingual paradigm have shown great promise (gale et al, 1992; da gan and itai, 1994; <papid> J94-4003 </papid>diab and resnik, 2002; <papid> P02-1033 </papid>ng et al., 2003; <papid> P03-1058 </papid>li and li, 2004; <papid> J04-1001 </papid>chan and ng, 2005;wang and carroll, 2005).<papid> H05-1069 </papid></citsent>
<aftsection>
<nextsent>the underlying hypothesis is that mappings between word forms and meanings can be different from language tolanguage.
</nextsent>
<nextsent>much work have been done on extracting sense examples from parallel corpora for wsd.
</nextsent>
<nextsent>for example, ng et al (2003) <papid> P03-1058 </papid>proposed to train classifier on sense examples acquired from word-aligned english-chinese parallel cor pora.</nextsent>
<nextsent>they grouped senses that share the same chinese translation, and then the occurrences of the word on the english side of the parallel corpora were considered to have been disambiguated and sense tagged?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC415">
<title id=" W06-2007.xml">word sense disambiguation using automatically translated sense examples </title>
<section> word-segment these chinese text snippets..  </section>
<citcontext>
<prevsection>
<prevsent>figure 1 illustrates our adapted acquisition process.
</prevsent>
<prevsent>as described above, we prepared two sets of training examples for each english word senseto disambiguate: one set was translated word-byword by looking up bilingual dictionary, as proposed in (wang and carroll, 2005), <papid> H05-1069 </papid>and the other translated using mt software.</prevsent>
</prevsection>
<citsent citstr=" W04-0807 ">
in detail, we first mapped senses of ambiguous words, as defined in the gold-standard twa (mihalcea, 2003) and senseval-3 lexical sample (mihalcea et al, 2004) <papid> W04-0807 </papid>datasets (which we use for evaluation) onto their corresponding chinese translations.</citsent>
<aftsection>
<nextsent>we did thisby looking up an english-chinese dictionary pow erword 20022.
</nextsent>
<nextsent>this mapping process involved human intervention, but it only took an annotator (fluent speaker in both chinese and english) 4 hours.
</nextsent>
<nextsent>since some chinese translations arealso ambiguous, which may affect wsd performance, the annotator was asked to select the chinese words that are relatively unambiguous (or ideally monosemous) in chinese for the target word senses, when it was possible.
</nextsent>
<nextsent>sometimes multiple senses of an english word can map tothe same chinese word, according to the english chinese dictionary.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC416">
<title id=" W06-2007.xml">word sense disambiguation using automatically translated sense examples </title>
<section> word-segment these chinese text snippets..  </section>
<citcontext>
<prevsection>
<prevsent>3 experimental settings.
</prevsent>
<prevsent>3.1 training.
</prevsent>
</prevsection>
<citsent citstr=" W04-3204 ">
we applied the vector space model (vsm) algorithm on the two different kinds of sense examples (i.e., dictionary translated ones vs. mt software translated ones), as it has been shown to perform well with the features described below (agirre and martinez, 2004<papid> W04-3204 </papid>a).</citsent>
<aftsection>
<nextsent>in vsm, we represent each context as vector, where each feature has an 1 or 0 value to indicate its occurrence or absence.for each sense in training, centro id vector is obtained, and these centro ids are compared to the vectors that represent test examples, by means of the cosine similarity function.
</nextsent>
<nextsent>the closest centro id assigns its sense to the test example.for the sense examples translated by mt software, we analysed the sentences using different 3see: http://mtgroup.ict.ac.cn/  zhp/ictclas 47tools and extracted relevant features.
</nextsent>
<nextsent>we applied stemming and pos tagging, using the fntbltoolkit (ngai and florian, 2001), as well as shallow parsing4.
</nextsent>
<nextsent>then we extracted the following types of topical and domain features5, which were then fed to the vsm machine learner:   topical features: we extracted lemmas of the content words in two windows around the target word: the whole context and a
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC420">
<title id=" W06-2007.xml">word sense disambiguation using automatically translated sense examples </title>
<section> word-segment these chinese text snippets..  </section>
<citcontext>
<prevsection>
<prevsent>then we extracted the following types of topical and domain features5, which were then fed to the vsm machine learner:   topical features: we extracted lemmas of the content words in two windows around the target word: the whole context and a
</prevsent>
<prevsent>4 word window.
</prevsent>
</prevsection>
<citsent citstr=" N01-1011 ">
we also obtained salient bigrams inthe context, with the methods and the software described in (pedersen, 2001).<papid> N01-1011 </papid></citsent>
<aftsection>
<nextsent>we included another feature type, which match the closest words (for each pos and in both directions) to the target word (e.g. left noun dog?
</nextsent>
<nextsent>or left verb eat?).
</nextsent>
<nextsent>  domain features: the wordnet domains resource was used to identify the most relevant domains in the context.
</nextsent>
<nextsent>following the relevance formula presented in (magnini and cavaglia?, 2000), we defined two feature types: (1) the most relevant domain, and (2) list of domains above threshold6.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC422">
<title id=" W06-2007.xml">word sense disambiguation using automatically translated sense examples </title>
<section> word-segment these chinese text snippets..  </section>
<citcontext>
<prevsection>
<prevsent>6this software was kindly provided by gerard escuderos group at universitat politecnica de catalunya.
</prevsent>
<prevsent>the threshold was set in previous work.
</prevsent>
</prevsection>
<citsent citstr=" H93-1061 ">
7http://wordnet.princeton.edu 8http://www.wordsmyth.net we also used the semcor corpus (miller et al, 1993) <papid> H93-1061 </papid>for tuning our relative-threshold heuristic.</citsent>
<aftsection>
<nextsent>it contains number of texts, mainly from the brown corpus, comprising about 200,000 words, where all content words have been manually tagged with senses from wordnet.
</nextsent>
<nextsent>throughout the paper we will use the concepts of precision and recall to measure the performance of wsd systems, where precision refers to the ratio of correct answers to the total number of answers given by the system, and recall indicates the ratio of correct answers to the total number of instances.
</nextsent>
<nextsent>our ml systems attempt every instance and always give unique answer, and hence precision equals to recall.
</nextsent>
<nextsent>when comparing with other systems that participated in senseval-3 in table 7, both recall and precision are shown.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC423">
<title id=" W06-2007.xml">word sense disambiguation using automatically translated sense examples </title>
<section> experiments on senseval-3.  </section>
<citcontext>
<prevsection>
<prevsent>from this table, we chose 11 as threshold value for the test data, as it obtained the best performance on semcor.thus, we performed single run of the algorithm on the test data applying the chosen threshold.
</prevsent>
<prevsent>the performance for all nouns is given in table 6.
</prevsent>
</prevsection>
<citsent citstr=" P04-1036 ">
we can see that the recall has increased significantly, and is now closer to the mfs baseline, which is very hard baseline for unsupervised systems (mccarthy et al, 2004).<papid> P04-1036 </papid></citsent>
<aftsection>
<nextsent>still, the performance is significantly lower than the score achieved by supervised systems, which can reach above 72% recall (mihalcea et al, 2004).<papid> W04-0807 </papid></nextsent>
<nextsent>some of the reasons for the gap are the following:   the acquisition process: problems can arise 50 word test ex.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC430">
<title id=" W06-3208.xml">morphology induction from limited noisy data using approximate string matching </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>he used successor and predecessor frequencies of letters in given sequence of letters in identifying possible morpheme bound 60aries.
</prevsent>
<prevsent>the morpheme boundaries are where the predictability of the next letter in the letter sequence is the lowest.
</prevsent>
</prevsection>
<citsent citstr=" J01-2001 ">
several researchers (brent, 1993; brent et al, 1995; goldsmith, 2001) <papid> J01-2001 </papid>used minimum description length (mdl) for morphology learning.</citsent>
<aftsection>
<nextsent>snoverand brent (2001) <papid> P01-1063 </papid>proposed generative probability model to identify stems and suffixes.</nextsent>
<nextsent>schone and jurafsky (2001) <papid> N01-1024 </papid>used latent semantic analysis to find affixes.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC432">
<title id=" W06-3208.xml">morphology induction from limited noisy data using approximate string matching </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the morpheme boundaries are where the predictability of the next letter in the letter sequence is the lowest.
</prevsent>
<prevsent>several researchers (brent, 1993; brent et al, 1995; goldsmith, 2001) <papid> J01-2001 </papid>used minimum description length (mdl) for morphology learning.</prevsent>
</prevsection>
<citsent citstr=" P01-1063 ">
snoverand brent (2001) <papid> P01-1063 </papid>proposed generative probability model to identify stems and suffixes.</citsent>
<aftsection>
<nextsent>schone and jurafsky (2001) <papid> N01-1024 </papid>used latent semantic analysis to find affixes.</nextsent>
<nextsent>baroni et al (2002) <papid> W02-0606 </papid>produced ranked list of morphologically related pairs from corpus using orthographic or semantic similarity with minimum edit distance and mutual information metrics.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC433">
<title id=" W06-3208.xml">morphology induction from limited noisy data using approximate string matching </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>several researchers (brent, 1993; brent et al, 1995; goldsmith, 2001) <papid> J01-2001 </papid>used minimum description length (mdl) for morphology learning.</prevsent>
<prevsent>snoverand brent (2001) <papid> P01-1063 </papid>proposed generative probability model to identify stems and suffixes.</prevsent>
</prevsection>
<citsent citstr=" N01-1024 ">
schone and jurafsky (2001) <papid> N01-1024 </papid>used latent semantic analysis to find affixes.</citsent>
<aftsection>
<nextsent>baroni et al (2002) <papid> W02-0606 </papid>produced ranked list of morphologically related pairs from corpus using orthographic or semantic similarity with minimum edit distance and mutual information metrics.</nextsent>
<nextsent>creutz and lagus (2002) <papid> W02-0603 </papid>proposed two unsupervised methods for word segmentation, one based on maximum description length, and one based on maximum likelihood.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC434">
<title id=" W06-3208.xml">morphology induction from limited noisy data using approximate string matching </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>snoverand brent (2001) <papid> P01-1063 </papid>proposed generative probability model to identify stems and suffixes.</prevsent>
<prevsent>schone and jurafsky (2001) <papid> N01-1024 </papid>used latent semantic analysis to find affixes.</prevsent>
</prevsection>
<citsent citstr=" W02-0606 ">
baroni et al (2002) <papid> W02-0606 </papid>produced ranked list of morphologically related pairs from corpus using orthographic or semantic similarity with minimum edit distance and mutual information metrics.</citsent>
<aftsection>
<nextsent>creutz and lagus (2002) <papid> W02-0603 </papid>proposed two unsupervised methods for word segmentation, one based on maximum description length, and one based on maximum likelihood.</nextsent>
<nextsent>in their model, words consisted of lengthy sequences of segment sand there is no distinction between stems and af fixes.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC435">
<title id=" W06-3208.xml">morphology induction from limited noisy data using approximate string matching </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>schone and jurafsky (2001) <papid> N01-1024 </papid>used latent semantic analysis to find affixes.</prevsent>
<prevsent>baroni et al (2002) <papid> W02-0606 </papid>produced ranked list of morphologically related pairs from corpus using orthographic or semantic similarity with minimum edit distance and mutual information metrics.</prevsent>
</prevsection>
<citsent citstr=" W02-0603 ">
creutz and lagus (2002) <papid> W02-0603 </papid>proposed two unsupervised methods for word segmentation, one based on maximum description length, and one based on maximum likelihood.</citsent>
<aftsection>
<nextsent>in their model, words consisted of lengthy sequences of segment sand there is no distinction between stems and af fixes.
</nextsent>
<nextsent>the whole word morphologizer (neuvel andfulop, 2002) <papid> W02-0604 </papid>uses pos-tagged lexicon as input, induces morphological relationships without attempting to discover or identify morphemes.</nextsent>
<nextsent>it is also capable of generating new words beyond the learning sample.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC436">
<title id=" W06-3208.xml">morphology induction from limited noisy data using approximate string matching </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>creutz and lagus (2002) <papid> W02-0603 </papid>proposed two unsupervised methods for word segmentation, one based on maximum description length, and one based on maximum likelihood.</prevsent>
<prevsent>in their model, words consisted of lengthy sequences of segment sand there is no distinction between stems and af fixes.</prevsent>
</prevsection>
<citsent citstr=" W02-0604 ">
the whole word morphologizer (neuvel andfulop, 2002) <papid> W02-0604 </papid>uses pos-tagged lexicon as input, induces morphological relationships without attempting to discover or identify morphemes.</citsent>
<aftsection>
<nextsent>it is also capable of generating new words beyond the learning sample.
</nextsent>
<nextsent>mystem (segalovich, 2003) uses dictionary for unknown word guessing in morphological analysis algorithm for web search engines.
</nextsent>
<nextsent>using very simple idea of morphological similarity, unknown word morphology is taken from all the closest words in the dictionary, where the closeness is the number of letters on its end.
</nextsent>
<nextsent>the word frame model (wicentowski, 2004) <papid> W04-0109 </papid>uses inflection-root pairs, where unseen inflections are transformed into their corresponding root forms.the model works with imperfect data, and can handle prefixes, suffixes, stem-internal vowel shifts, and point-of-affixation stem changes.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC437">
<title id=" W06-3208.xml">morphology induction from limited noisy data using approximate string matching </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>mystem (segalovich, 2003) uses dictionary for unknown word guessing in morphological analysis algorithm for web search engines.
</prevsent>
<prevsent>using very simple idea of morphological similarity, unknown word morphology is taken from all the closest words in the dictionary, where the closeness is the number of letters on its end.
</prevsent>
</prevsection>
<citsent citstr=" W04-0109 ">
the word frame model (wicentowski, 2004) <papid> W04-0109 </papid>uses inflection-root pairs, where unseen inflections are transformed into their corresponding root forms.the model works with imperfect data, and can handle prefixes, suffixes, stem-internal vowel shifts, and point-of-affixation stem changes.</citsent>
<aftsection>
<nextsent>the word frame model can be used for co-training with low-accuracy unsupervised algorithms.
</nextsent>
<nextsent>monson (2004) <papid> P04-2012 </papid>concentrated on languages with limited resources.</nextsent>
<nextsent>the proposed language independent framework used corpus of full wordforms.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC438">
<title id=" W06-3208.xml">morphology induction from limited noisy data using approximate string matching </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the word frame model (wicentowski, 2004) <papid> W04-0109 </papid>uses inflection-root pairs, where unseen inflections are transformed into their corresponding root forms.the model works with imperfect data, and can handle prefixes, suffixes, stem-internal vowel shifts, and point-of-affixation stem changes.</prevsent>
<prevsent>the word frame model can be used for co-training with low-accuracy unsupervised algorithms.</prevsent>
</prevsection>
<citsent citstr=" P04-2012 ">
monson (2004) <papid> P04-2012 </papid>concentrated on languages with limited resources.</citsent>
<aftsection>
<nextsent>the proposed language independent framework used corpus of full wordforms.
</nextsent>
<nextsent>candidate suffixes are grouped into candidate inflection classes, which are then arranged in lattice structure.
</nextsent>
<nextsent>a recent work (goldsmith et al, 2005) proposed to use string edit distance algorithm as bootstrapping heuristic to analyze languages with rich morphologies.
</nextsent>
<nextsent>string edit distance is used for ranking and quantifying the robustness of morphological generalizations in set of clean data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC442">
<title id=" W06-3117.xml">stochastic inversion transduction grammars for obtaining word phrases for phrase based statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this work, we propose obtaining word phrases by means of stochastic inversion translation grammar.
</prevsent>
<prevsent>experiment son the shared task proposed in this work shop with the europarl corpus have been carried out and good results have been obtained.
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
phrase-based statistical translation systems are currently providing excellent results in real machine translation tasks (zens et al, 2002; och and ney, 2003; <papid> J03-1002 </papid>koehn, 2004).</citsent>
<aftsection>
<nextsent>in phrase-based statistical translation systems, the basic translation units are word phrases.an important problem that is related to phrase based statistical translation is to automatically obtain bilingual word phrases from parallel corpora.
</nextsent>
<nextsent>several methods have been defined for dealing with this problem (och and ney, 2003).<papid> J03-1002 </papid></nextsent>
<nextsent>in this work, we study method for obtaining word phrases that isbased on stochastic inversion transduction grammars that was proposed in (wu, 1997).<papid> J97-3002 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC444">
<title id=" W06-3117.xml">stochastic inversion transduction grammars for obtaining word phrases for phrase based statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in phrase-based statistical translation systems, the basic translation units are word phrases.an important problem that is related to phrase based statistical translation is to automatically obtain bilingual word phrases from parallel corpora.
</prevsent>
<prevsent>several methods have been defined for dealing with this problem (och and ney, 2003).<papid> J03-1002 </papid></prevsent>
</prevsection>
<citsent citstr=" J97-3002 ">
in this work, we study method for obtaining word phrases that isbased on stochastic inversion transduction grammars that was proposed in (wu, 1997).<papid> J97-3002 </papid></citsent>
<aftsection>
<nextsent>stochastic inversion transduction grammars(sitg) can be viewed as restricted stochastic context-free syntax-directed transductionscheme.
</nextsent>
<nextsent>sitgs can be used to carry out simultaneous parsing of both the input string and the output string.
</nextsent>
<nextsent>in this work, we apply this idea to obtain aligned word phrases to be used in phrase-based translation systems (snchez and bened?, 2006).
</nextsent>
<nextsent>in section 2, we review the phrase-based machine translation approach.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC447">
<title id=" W06-3117.xml">stochastic inversion transduction grammars for obtaining word phrases for phrase based statistical machine translation </title>
<section> phrase-based statistical machine.  </section>
<citcontext>
<prevsection>
<prevsent>these systems have demonstrated excellent translation performance in real tasks.the basic idea of phrase-based statistical machine translation system consists of the following steps (zens et al, 2002): first, the source sentence is segmented into phrases; second, each source phrase is translated into target phrase; and third, the target phrases are reordered in order to compose the target sentence.
</prevsent>
<prevsent>bilingual translation phrases are an important component of phrase-based system.
</prevsent>
</prevsection>
<citsent citstr=" P01-1067 ">
different methods have been defined to obtain bilingual translations phrases, mainly from word-based alignments and from syntax-based models (yamada and knight, 2001).<papid> P01-1067 </papid></citsent>
<aftsection>
<nextsent>in this work, we focus on learning bilingual word phrases by using stochastic inversion transduction grammars (sitgs) (wu, 1997).<papid> J97-3002 </papid></nextsent>
<nextsent>this formalism al 130lows us to obtain bilingual word phrases in natural way from the bilingual parsing of two sentences.in addition, the sitgs allow us to easily incorporate many desirable characteristics to word phrases such as length restrictions, selection according to the word alignment probability, bracketing information,etc. we review this formalism in the following sec tion.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC458">
<title id=" W06-3117.xml">stochastic inversion transduction grammars for obtaining word phrases for phrase based statistical machine translation </title>
<section> stochastic inversion transduction.  </section>
<citcontext>
<prevsection>
<prevsent>to note that this time complexity restricts the use of the algorithm to real tasks with short strings.if bracketed corpus is available, then modified version of the parsing algorithm can be defined to take into account the bracketing of the strings.
</prevsent>
<prevsent>1a normal form for sitgs can be defined (wu, 1997) <papid> J97-3002 </papid>by analogy to the chomsky normal form for stochastic context free grammars.</prevsent>
</prevsection>
<citsent citstr=" P92-1017 ">
the modifications are similar to those proposed in (pereira and schabes, 1992) <papid> P92-1017 </papid>for the inside algorithm.</citsent>
<aftsection>
<nextsent>following the notation that is presented in (pereiraand schabes, 1992), <papid> P92-1017 </papid>we can define partially bracketed corpus as set of sentence pairs that are annotated with parentheses that mark constituent fron tiers.</nextsent>
<nextsent>more precisely, bracketed corpus = is set of tuples  fl   ?   #   ?@  , where fl and # are strings,  6  is the bracketing of fl , and  6@ is the bracketing of # . let  b@ be parsing of fl and # with the sitg  </nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC460">
<title id=" W06-3811.xml">synonym extraction using a semantic distance on a dictionary </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we present an evaluation ofthe relevance of the candidates on sample of the lexicon.
</prevsent>
<prevsent>thesaurus are an important resource in many natural language processing tasks.
</prevsent>
</prevsection>
<citsent citstr=" W03-1613 ">
they are used to help information retrieval (zukerman et al, 2003), <papid> W03-1613 </papid>machine or semi-automated translation, (ploux and ji, 2003; <papid> J03-2001 </papid>barzilay and mckeown, 2001; <papid> P01-1008 </papid>edmonds and hirst, 2002) <papid> J02-2001 </papid>or generation (langkilde and knight, 1998).<papid> P98-1116 </papid></citsent>
<aftsection>
<nextsent>since the gathering of such lexical information is delicate and time-consuming endeavour, some effort has been devoted to the automatic building of sets of synonyms words or expressions.
</nextsent>
<nextsent>synonym extraction suffers from variety of methodological problems, however.
</nextsent>
<nextsent>synonymy itself is not an easily definable notion.
</nextsent>
<nextsent>totally equivalent words (in meaning and use) arguably do not exist, and some people prefer to talk about near synonyms (edmonds and hirst, 2002).<papid> J02-2001 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC461">
<title id=" W06-3811.xml">synonym extraction using a semantic distance on a dictionary </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we present an evaluation ofthe relevance of the candidates on sample of the lexicon.
</prevsent>
<prevsent>thesaurus are an important resource in many natural language processing tasks.
</prevsent>
</prevsection>
<citsent citstr=" J03-2001 ">
they are used to help information retrieval (zukerman et al, 2003), <papid> W03-1613 </papid>machine or semi-automated translation, (ploux and ji, 2003; <papid> J03-2001 </papid>barzilay and mckeown, 2001; <papid> P01-1008 </papid>edmonds and hirst, 2002) <papid> J02-2001 </papid>or generation (langkilde and knight, 1998).<papid> P98-1116 </papid></citsent>
<aftsection>
<nextsent>since the gathering of such lexical information is delicate and time-consuming endeavour, some effort has been devoted to the automatic building of sets of synonyms words or expressions.
</nextsent>
<nextsent>synonym extraction suffers from variety of methodological problems, however.
</nextsent>
<nextsent>synonymy itself is not an easily definable notion.
</nextsent>
<nextsent>totally equivalent words (in meaning and use) arguably do not exist, and some people prefer to talk about near synonyms (edmonds and hirst, 2002).<papid> J02-2001 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC462">
<title id=" W06-3811.xml">synonym extraction using a semantic distance on a dictionary </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we present an evaluation ofthe relevance of the candidates on sample of the lexicon.
</prevsent>
<prevsent>thesaurus are an important resource in many natural language processing tasks.
</prevsent>
</prevsection>
<citsent citstr=" P01-1008 ">
they are used to help information retrieval (zukerman et al, 2003), <papid> W03-1613 </papid>machine or semi-automated translation, (ploux and ji, 2003; <papid> J03-2001 </papid>barzilay and mckeown, 2001; <papid> P01-1008 </papid>edmonds and hirst, 2002) <papid> J02-2001 </papid>or generation (langkilde and knight, 1998).<papid> P98-1116 </papid></citsent>
<aftsection>
<nextsent>since the gathering of such lexical information is delicate and time-consuming endeavour, some effort has been devoted to the automatic building of sets of synonyms words or expressions.
</nextsent>
<nextsent>synonym extraction suffers from variety of methodological problems, however.
</nextsent>
<nextsent>synonymy itself is not an easily definable notion.
</nextsent>
<nextsent>totally equivalent words (in meaning and use) arguably do not exist, and some people prefer to talk about near synonyms (edmonds and hirst, 2002).<papid> J02-2001 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC463">
<title id=" W06-3811.xml">synonym extraction using a semantic distance on a dictionary </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we present an evaluation ofthe relevance of the candidates on sample of the lexicon.
</prevsent>
<prevsent>thesaurus are an important resource in many natural language processing tasks.
</prevsent>
</prevsection>
<citsent citstr=" J02-2001 ">
they are used to help information retrieval (zukerman et al, 2003), <papid> W03-1613 </papid>machine or semi-automated translation, (ploux and ji, 2003; <papid> J03-2001 </papid>barzilay and mckeown, 2001; <papid> P01-1008 </papid>edmonds and hirst, 2002) <papid> J02-2001 </papid>or generation (langkilde and knight, 1998).<papid> P98-1116 </papid></citsent>
<aftsection>
<nextsent>since the gathering of such lexical information is delicate and time-consuming endeavour, some effort has been devoted to the automatic building of sets of synonyms words or expressions.
</nextsent>
<nextsent>synonym extraction suffers from variety of methodological problems, however.
</nextsent>
<nextsent>synonymy itself is not an easily definable notion.
</nextsent>
<nextsent>totally equivalent words (in meaning and use) arguably do not exist, and some people prefer to talk about near synonyms (edmonds and hirst, 2002).<papid> J02-2001 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC465">
<title id=" W06-3811.xml">synonym extraction using a semantic distance on a dictionary </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we present an evaluation ofthe relevance of the candidates on sample of the lexicon.
</prevsent>
<prevsent>thesaurus are an important resource in many natural language processing tasks.
</prevsent>
</prevsection>
<citsent citstr=" P98-1116 ">
they are used to help information retrieval (zukerman et al, 2003), <papid> W03-1613 </papid>machine or semi-automated translation, (ploux and ji, 2003; <papid> J03-2001 </papid>barzilay and mckeown, 2001; <papid> P01-1008 </papid>edmonds and hirst, 2002) <papid> J02-2001 </papid>or generation (langkilde and knight, 1998).<papid> P98-1116 </papid></citsent>
<aftsection>
<nextsent>since the gathering of such lexical information is delicate and time-consuming endeavour, some effort has been devoted to the automatic building of sets of synonyms words or expressions.
</nextsent>
<nextsent>synonym extraction suffers from variety of methodological problems, however.
</nextsent>
<nextsent>synonymy itself is not an easily definable notion.
</nextsent>
<nextsent>totally equivalent words (in meaning and use) arguably do not exist, and some people prefer to talk about near synonyms (edmonds and hirst, 2002).<papid> J02-2001 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC467">
<title id=" W06-3811.xml">synonym extraction using a semantic distance on a dictionary </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>design of thesaurus,but can help evaluate the relevance of synonyms extracted automatically, and which could have been forgotten.
</prevsent>
<prevsent>one can hope at best for semi-automatic procedure were lexicographers have to weed out bad candidates in set of proposals that is hopefully not too noisy.a few studies have tried to use the lexical information available in general dictionary and find patterns that would indicate synonymy relations (blon 65 del et al, 2004; ho and cdrick, 2004).
</prevsent>
</prevsection>
<citsent citstr=" C90-2067 ">
the general idea is that words are related by the definition they appear in, in complex network that must be semantic in nature (this has been also applied to word sense disambiguation, albeit with limited success (veronis and ide, 1990; <papid> C90-2067 </papid>h.kozima and furugori, 1993)).we present here method exploiting the graph structure of dictionary, where words are related by the definition they appear in, to compute distance between words.</citsent>
<aftsection>
<nextsent>this distance is used to isolate candidate synonyms forgiven word.
</nextsent>
<nextsent>we present an evaluation of the relevance of the candidates on sample of the lexicon.we describe here our method (dubbed prox) to compute distance between nodes in graph.
</nextsent>
<nextsent>basically, nodes are derived from entries in the dictionary or words appearing in definitions, and there are edges between an entry and the word in its definition (more in section 3).
</nextsent>
<nextsent>such graphs are  small world networks with distinguishing features and we hypothetize these features reflect linguistic and semantic organisation that can be exploited (gaume et al, 2005).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC469">
<title id=" W06-3811.xml">synonym extraction using a semantic distance on a dictionary </title>
<section> trouble psychique [...].  </section>
<citcontext>
<prevsection>
<prevsent>5 related work.
</prevsent>
<prevsent>among the methods proposed to collect synonymyinformation, two families can be distinguished according to the input they consider.
</prevsent>
</prevsection>
<citsent citstr=" W03-1610 ">
either general dictionary is used (or more than one (wu and zhou, 2003)), <papid> W03-1610 </papid>or corpus of unconstrained texts from which lexical distributions are computed (sim ple collocations or syntactic dependencies) (lin,1998; <papid> P98-2127 </papid>freitag et al, 2005) . <papid> W05-0604 </papid>the approach of (barzilay and mckeown, 2001) <papid> P01-1008 </papid>uses related kind of re source: multiple translations of the same text, with additional constraints on availability, and problems of text alignment, for only third of the results being synonyms (when compared to wordnet).</citsent>
<aftsection>
<nextsent>a measure of similarity is almost always used torank possible candidates.
</nextsent>
<nextsent>in the case of distributional approaches, similarity if determined from the appearance in similar contexts (lin, 1998); <papid> P98-2127 </papid>in the case of dictionary-based methods, lexical relations are deduced from the links between words expressed in definitions of entries.</nextsent>
<nextsent>approaches that relyon distributional data have two major drawbacks: they need lot of data, generally syntactically parsed sentences, that is not always available forgiven language (english is an exception), and they do not discriminate well among lexical relations (mainly hyponyms, antonyms, hy pernyms) (weeds et al, 2004) . <papid> C04-1146 </papid>dictionary-based 70approaches address the first problem since dictionaries are readily available for lot of language, even electronically, and this is the raison dtre of our ef fort.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC470">
<title id=" W06-3811.xml">synonym extraction using a semantic distance on a dictionary </title>
<section> trouble psychique [...].  </section>
<citcontext>
<prevsection>
<prevsent>5 related work.
</prevsent>
<prevsent>among the methods proposed to collect synonymyinformation, two families can be distinguished according to the input they consider.
</prevsent>
</prevsection>
<citsent citstr=" P98-2127 ">
either general dictionary is used (or more than one (wu and zhou, 2003)), <papid> W03-1610 </papid>or corpus of unconstrained texts from which lexical distributions are computed (sim ple collocations or syntactic dependencies) (lin,1998; <papid> P98-2127 </papid>freitag et al, 2005) . <papid> W05-0604 </papid>the approach of (barzilay and mckeown, 2001) <papid> P01-1008 </papid>uses related kind of re source: multiple translations of the same text, with additional constraints on availability, and problems of text alignment, for only third of the results being synonyms (when compared to wordnet).</citsent>
<aftsection>
<nextsent>a measure of similarity is almost always used torank possible candidates.
</nextsent>
<nextsent>in the case of distributional approaches, similarity if determined from the appearance in similar contexts (lin, 1998); <papid> P98-2127 </papid>in the case of dictionary-based methods, lexical relations are deduced from the links between words expressed in definitions of entries.</nextsent>
<nextsent>approaches that relyon distributional data have two major drawbacks: they need lot of data, generally syntactically parsed sentences, that is not always available forgiven language (english is an exception), and they do not discriminate well among lexical relations (mainly hyponyms, antonyms, hy pernyms) (weeds et al, 2004) . <papid> C04-1146 </papid>dictionary-based 70approaches address the first problem since dictionaries are readily available for lot of language, even electronically, and this is the raison dtre of our ef fort.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC471">
<title id=" W06-3811.xml">synonym extraction using a semantic distance on a dictionary </title>
<section> trouble psychique [...].  </section>
<citcontext>
<prevsection>
<prevsent>5 related work.
</prevsent>
<prevsent>among the methods proposed to collect synonymyinformation, two families can be distinguished according to the input they consider.
</prevsent>
</prevsection>
<citsent citstr=" W05-0604 ">
either general dictionary is used (or more than one (wu and zhou, 2003)), <papid> W03-1610 </papid>or corpus of unconstrained texts from which lexical distributions are computed (sim ple collocations or syntactic dependencies) (lin,1998; <papid> P98-2127 </papid>freitag et al, 2005) . <papid> W05-0604 </papid>the approach of (barzilay and mckeown, 2001) <papid> P01-1008 </papid>uses related kind of re source: multiple translations of the same text, with additional constraints on availability, and problems of text alignment, for only third of the results being synonyms (when compared to wordnet).</citsent>
<aftsection>
<nextsent>a measure of similarity is almost always used torank possible candidates.
</nextsent>
<nextsent>in the case of distributional approaches, similarity if determined from the appearance in similar contexts (lin, 1998); <papid> P98-2127 </papid>in the case of dictionary-based methods, lexical relations are deduced from the links between words expressed in definitions of entries.</nextsent>
<nextsent>approaches that relyon distributional data have two major drawbacks: they need lot of data, generally syntactically parsed sentences, that is not always available forgiven language (english is an exception), and they do not discriminate well among lexical relations (mainly hyponyms, antonyms, hy pernyms) (weeds et al, 2004) . <papid> C04-1146 </papid>dictionary-based 70approaches address the first problem since dictionaries are readily available for lot of language, even electronically, and this is the raison dtre of our ef fort.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC475">
<title id=" W06-3811.xml">synonym extraction using a semantic distance on a dictionary </title>
<section> trouble psychique [...].  </section>
<citcontext>
<prevsection>
<prevsent>a measure of similarity is almost always used torank possible candidates.
</prevsent>
<prevsent>in the case of distributional approaches, similarity if determined from the appearance in similar contexts (lin, 1998); <papid> P98-2127 </papid>in the case of dictionary-based methods, lexical relations are deduced from the links between words expressed in definitions of entries.</prevsent>
</prevsection>
<citsent citstr=" C04-1146 ">
approaches that relyon distributional data have two major drawbacks: they need lot of data, generally syntactically parsed sentences, that is not always available forgiven language (english is an exception), and they do not discriminate well among lexical relations (mainly hyponyms, antonyms, hy pernyms) (weeds et al, 2004) . <papid> C04-1146 </papid>dictionary-based 70approaches address the first problem since dictionaries are readily available for lot of language, even electronically, and this is the raison dtre of our ef fort.</citsent>
<aftsection>
<nextsent>as we have seen here, it is not an obvious taskto sort related terms with respect to synonymy, hy pernymy, etc, just as with distribution approaches.a lot of work has been done to extract lexical relations from the definitions taken in isolation (mostly for ontology building), see recently (nichols et al, 2005), with syntactic/semantic parse, with usually results around 60% of precision (that can be compared with the same baseline we used, all words inthe definition with the same category), on dictionaries with very small definitions (and thus higher proportions of synonyms and hypernyms).
</nextsent>
<nextsent>estimating the recall of such methods have not been done.
</nextsent>
<nextsent>using dictionaries as network of lexical items or senses has been quite popular for word sense disambiguation (veronis and ide, 1990; <papid> C90-2067 </papid>h.kozima and furugori, 1993; niwa and nitta, 1994) <papid> C94-1049 </papid>before losing ground to statistical approaches, even though(gaume et al, 2004; <papid> C04-1173 </papid>mihalcea et al, 2004) <papid> C04-1162 </papid>tried revival of such methods.</nextsent>
<nextsent>both (ho and cdrick, 2004) and (blondel et al, 2004) build graph of lexical items from dictionary in manner similar to ours.in the first case, the method used to compute similarity between two concepts (or words) is restricted to neighbors, in the graph, of the two concepts; in the second case, only directly related words are considered as potential candidates for synonymy: for two words to be considered synonyms, one has to appear in the definition of another.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC478">
<title id=" W06-3811.xml">synonym extraction using a semantic distance on a dictionary </title>
<section> trouble psychique [...].  </section>
<citcontext>
<prevsection>
<prevsent>as we have seen here, it is not an obvious taskto sort related terms with respect to synonymy, hy pernymy, etc, just as with distribution approaches.a lot of work has been done to extract lexical relations from the definitions taken in isolation (mostly for ontology building), see recently (nichols et al, 2005), with syntactic/semantic parse, with usually results around 60% of precision (that can be compared with the same baseline we used, all words inthe definition with the same category), on dictionaries with very small definitions (and thus higher proportions of synonyms and hypernyms).
</prevsent>
<prevsent>estimating the recall of such methods have not been done.
</prevsent>
</prevsection>
<citsent citstr=" C94-1049 ">
using dictionaries as network of lexical items or senses has been quite popular for word sense disambiguation (veronis and ide, 1990; <papid> C90-2067 </papid>h.kozima and furugori, 1993; niwa and nitta, 1994) <papid> C94-1049 </papid>before losing ground to statistical approaches, even though(gaume et al, 2004; <papid> C04-1173 </papid>mihalcea et al, 2004) <papid> C04-1162 </papid>tried revival of such methods.</citsent>
<aftsection>
<nextsent>both (ho and cdrick, 2004) and (blondel et al, 2004) build graph of lexical items from dictionary in manner similar to ours.in the first case, the method used to compute similarity between two concepts (or words) is restricted to neighbors, in the graph, of the two concepts; in the second case, only directly related words are considered as potential candidates for synonymy: for two words to be considered synonyms, one has to appear in the definition of another.
</nextsent>
<nextsent>in both cases, only 6 or 7 words have been used as test of synonymy,with validation provided by the authors with  related terms  (an unclear notion) considered correct.
</nextsent>
<nextsent>the similarity measure itself was evaluated on set of related terms from (miller and charles, 1991), asin (budanitsky and hirst, 2001; banerjee and pedersen, 2003), with seemingly good results, but semantically related terms is very different notion ( car  and  tire  for instance are semantically related terms, and thus considered similar).we do not know of any dictionary-based graph approach which have been given larger evaluation of its results.
</nextsent>
<nextsent>parsing definitions in isolation prevents complete coverage (we estimated that only 30% of synonyms pairs in the tlf can be found from defi nitions).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC479">
<title id=" W06-3811.xml">synonym extraction using a semantic distance on a dictionary </title>
<section> trouble psychique [...].  </section>
<citcontext>
<prevsection>
<prevsent>as we have seen here, it is not an obvious taskto sort related terms with respect to synonymy, hy pernymy, etc, just as with distribution approaches.a lot of work has been done to extract lexical relations from the definitions taken in isolation (mostly for ontology building), see recently (nichols et al, 2005), with syntactic/semantic parse, with usually results around 60% of precision (that can be compared with the same baseline we used, all words inthe definition with the same category), on dictionaries with very small definitions (and thus higher proportions of synonyms and hypernyms).
</prevsent>
<prevsent>estimating the recall of such methods have not been done.
</prevsent>
</prevsection>
<citsent citstr=" C04-1173 ">
using dictionaries as network of lexical items or senses has been quite popular for word sense disambiguation (veronis and ide, 1990; <papid> C90-2067 </papid>h.kozima and furugori, 1993; niwa and nitta, 1994) <papid> C94-1049 </papid>before losing ground to statistical approaches, even though(gaume et al, 2004; <papid> C04-1173 </papid>mihalcea et al, 2004) <papid> C04-1162 </papid>tried revival of such methods.</citsent>
<aftsection>
<nextsent>both (ho and cdrick, 2004) and (blondel et al, 2004) build graph of lexical items from dictionary in manner similar to ours.in the first case, the method used to compute similarity between two concepts (or words) is restricted to neighbors, in the graph, of the two concepts; in the second case, only directly related words are considered as potential candidates for synonymy: for two words to be considered synonyms, one has to appear in the definition of another.
</nextsent>
<nextsent>in both cases, only 6 or 7 words have been used as test of synonymy,with validation provided by the authors with  related terms  (an unclear notion) considered correct.
</nextsent>
<nextsent>the similarity measure itself was evaluated on set of related terms from (miller and charles, 1991), asin (budanitsky and hirst, 2001; banerjee and pedersen, 2003), with seemingly good results, but semantically related terms is very different notion ( car  and  tire  for instance are semantically related terms, and thus considered similar).we do not know of any dictionary-based graph approach which have been given larger evaluation of its results.
</nextsent>
<nextsent>parsing definitions in isolation prevents complete coverage (we estimated that only 30% of synonyms pairs in the tlf can be found from defi nitions).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC480">
<title id=" W06-3811.xml">synonym extraction using a semantic distance on a dictionary </title>
<section> trouble psychique [...].  </section>
<citcontext>
<prevsection>
<prevsent>as we have seen here, it is not an obvious taskto sort related terms with respect to synonymy, hy pernymy, etc, just as with distribution approaches.a lot of work has been done to extract lexical relations from the definitions taken in isolation (mostly for ontology building), see recently (nichols et al, 2005), with syntactic/semantic parse, with usually results around 60% of precision (that can be compared with the same baseline we used, all words inthe definition with the same category), on dictionaries with very small definitions (and thus higher proportions of synonyms and hypernyms).
</prevsent>
<prevsent>estimating the recall of such methods have not been done.
</prevsent>
</prevsection>
<citsent citstr=" C04-1162 ">
using dictionaries as network of lexical items or senses has been quite popular for word sense disambiguation (veronis and ide, 1990; <papid> C90-2067 </papid>h.kozima and furugori, 1993; niwa and nitta, 1994) <papid> C94-1049 </papid>before losing ground to statistical approaches, even though(gaume et al, 2004; <papid> C04-1173 </papid>mihalcea et al, 2004) <papid> C04-1162 </papid>tried revival of such methods.</citsent>
<aftsection>
<nextsent>both (ho and cdrick, 2004) and (blondel et al, 2004) build graph of lexical items from dictionary in manner similar to ours.in the first case, the method used to compute similarity between two concepts (or words) is restricted to neighbors, in the graph, of the two concepts; in the second case, only directly related words are considered as potential candidates for synonymy: for two words to be considered synonyms, one has to appear in the definition of another.
</nextsent>
<nextsent>in both cases, only 6 or 7 words have been used as test of synonymy,with validation provided by the authors with  related terms  (an unclear notion) considered correct.
</nextsent>
<nextsent>the similarity measure itself was evaluated on set of related terms from (miller and charles, 1991), asin (budanitsky and hirst, 2001; banerjee and pedersen, 2003), with seemingly good results, but semantically related terms is very different notion ( car  and  tire  for instance are semantically related terms, and thus considered similar).we do not know of any dictionary-based graph approach which have been given larger evaluation of its results.
</nextsent>
<nextsent>parsing definitions in isolation prevents complete coverage (we estimated that only 30% of synonyms pairs in the tlf can be found from defi nitions).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC487">
<title id=" W06-1672.xml">discriminative methods for transliteration </title>
<section> global transliteration modeling.  </section>
<citcontext>
<prevsection>
<prevsent>features: (fm$,e$), (fm-1fm$,e$).
</prevsent>
<prevsent>the parameters w(e), w(f), and d(e,f) are, in general, language-specific, and we will show, in the experiments, that different values of the parameters are appropriate for different languages.
</prevsent>
</prevsection>
<citsent citstr=" W02-1001 ">
in global transliteration modeling, we directly model the agreement function between and e. we follow (collins 2002) <papid> W02-1001 </papid>and consider the global feature representation ?: f*e* ? rd. 613 each global feature corresponds to condition on the pair of strings.</citsent>
<aftsection>
<nextsent>the value of feature is the number of times the condition holds true forgiven pair of strings.
</nextsent>
<nextsent>in particular, for every local feature k((e[1,i-1], f),ei) we can define the corresponding global feature: )),],1,1[((),( ? ?=?
</nextsent>
<nextsent>i ikk ei feef ?
</nextsent>
<nextsent>(1) we seek transliteration model that is linear in the global features.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC493">
<title id=" W06-1672.xml">discriminative methods for transliteration </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>while for korean the number of training examples is sufficient to cope with the attribute noise, the relatively small arabic training sample is not.
</prevsent>
<prevsent>we hypothesize that with increasing the number of training examples for arabic, the optimal value of will also increase.
</prevsent>
</prevsection>
<citsent citstr=" J98-4003 ">
most work on name transliteration adopted source-channel approach (knight and grael 1998; <papid> J98-4003 </papid>al-onaizan and knight 2002a; virga and khudanpur 2003; <papid> W03-1508 </papid>oh and choi 2000) incorporat 615 ing phone tics as an intermediate representation.</citsent>
<aftsection>
<nextsent>(al-onaizan and knight 2002) showed that use of outside linguistic resources such as www counts of transliteration candidates can greatly boost transliteration accuracy.
</nextsent>
<nextsent>(li et al 2004) <papid> P04-1021 </papid>introduced the joint transliteration model whose variant augmented with adaptive re-ranking we used in our experiments.</nextsent>
<nextsent>among direct (non-source-channel) models, we note the work of (gao et al 2004) on applying maximum entropy to english-chinese transliteration, and the english-korean transliteration model of (kang and choi 2000) based on decision trees.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC494">
<title id=" W06-1672.xml">discriminative methods for transliteration </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>while for korean the number of training examples is sufficient to cope with the attribute noise, the relatively small arabic training sample is not.
</prevsent>
<prevsent>we hypothesize that with increasing the number of training examples for arabic, the optimal value of will also increase.
</prevsent>
</prevsection>
<citsent citstr=" W03-1508 ">
most work on name transliteration adopted source-channel approach (knight and grael 1998; <papid> J98-4003 </papid>al-onaizan and knight 2002a; virga and khudanpur 2003; <papid> W03-1508 </papid>oh and choi 2000) incorporat 615 ing phone tics as an intermediate representation.</citsent>
<aftsection>
<nextsent>(al-onaizan and knight 2002) showed that use of outside linguistic resources such as www counts of transliteration candidates can greatly boost transliteration accuracy.
</nextsent>
<nextsent>(li et al 2004) <papid> P04-1021 </papid>introduced the joint transliteration model whose variant augmented with adaptive re-ranking we used in our experiments.</nextsent>
<nextsent>among direct (non-source-channel) models, we note the work of (gao et al 2004) on applying maximum entropy to english-chinese transliteration, and the english-korean transliteration model of (kang and choi 2000) based on decision trees.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC495">
<title id=" W06-1672.xml">discriminative methods for transliteration </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>most work on name transliteration adopted source-channel approach (knight and grael 1998; <papid> J98-4003 </papid>al-onaizan and knight 2002a; virga and khudanpur 2003; <papid> W03-1508 </papid>oh and choi 2000) incorporat 615 ing phone tics as an intermediate representation.</prevsent>
<prevsent>(al-onaizan and knight 2002) showed that use of outside linguistic resources such as www counts of transliteration candidates can greatly boost transliteration accuracy.</prevsent>
</prevsection>
<citsent citstr=" P04-1021 ">
(li et al 2004) <papid> P04-1021 </papid>introduced the joint transliteration model whose variant augmented with adaptive re-ranking we used in our experiments.</citsent>
<aftsection>
<nextsent>among direct (non-source-channel) models, we note the work of (gao et al 2004) on applying maximum entropy to english-chinese transliteration, and the english-korean transliteration model of (kang and choi 2000) based on decision trees.
</nextsent>
<nextsent>all of the above models require alignment between names.
</nextsent>
<nextsent>we follow the recent work of (klementiev and roth 2006) <papid> N06-1011 </papid>who addressed the problem of discovery of transliterated named entities from comparable corpora and suggested that alignment may not be necessary for tran slit eration.</nextsent>
<nextsent>finally, our modeling approaches follow the recent work on both local classifier-based modeling of complex learning problems (mccallum et al 2000; punyakanok and roth 2001), as well as global discriminative approaches based on crfs (lafferty et al 2001), svm (taskar et al 2005<papid> H05-1010 </papid>and the perceptron algorithm (collins 2002) <papid> W02-1001 </papid>that we used in our experiments.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC496">
<title id=" W06-1672.xml">discriminative methods for transliteration </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>among direct (non-source-channel) models, we note the work of (gao et al 2004) on applying maximum entropy to english-chinese transliteration, and the english-korean transliteration model of (kang and choi 2000) based on decision trees.
</prevsent>
<prevsent>all of the above models require alignment between names.
</prevsent>
</prevsection>
<citsent citstr=" N06-1011 ">
we follow the recent work of (klementiev and roth 2006) <papid> N06-1011 </papid>who addressed the problem of discovery of transliterated named entities from comparable corpora and suggested that alignment may not be necessary for tran slit eration.</citsent>
<aftsection>
<nextsent>finally, our modeling approaches follow the recent work on both local classifier-based modeling of complex learning problems (mccallum et al 2000; punyakanok and roth 2001), as well as global discriminative approaches based on crfs (lafferty et al 2001), svm (taskar et al 2005<papid> H05-1010 </papid>and the perceptron algorithm (collins 2002) <papid> W02-1001 </papid>that we used in our experiments.</nextsent>
<nextsent>we presented two novel discriminative approaches to name transliteration that do not employ the notion of alignment.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC497">
<title id=" W06-1672.xml">discriminative methods for transliteration </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>all of the above models require alignment between names.
</prevsent>
<prevsent>we follow the recent work of (klementiev and roth 2006) <papid> N06-1011 </papid>who addressed the problem of discovery of transliterated named entities from comparable corpora and suggested that alignment may not be necessary for tran slit eration.</prevsent>
</prevsection>
<citsent citstr=" H05-1010 ">
finally, our modeling approaches follow the recent work on both local classifier-based modeling of complex learning problems (mccallum et al 2000; punyakanok and roth 2001), as well as global discriminative approaches based on crfs (lafferty et al 2001), svm (taskar et al 2005<papid> H05-1010 </papid>and the perceptron algorithm (collins 2002) <papid> W02-1001 </papid>that we used in our experiments.</citsent>
<aftsection>
<nextsent>we presented two novel discriminative approaches to name transliteration that do not employ the notion of alignment.
</nextsent>
<nextsent>we showed experimentally that the approaches lead to superior experimental results in all languages, with the global discriminative modeling approach achieving the best performance.
</nextsent>
<nextsent>the results are somewhat surprising, for the notion of alignment seems very intuitive and useful for transliteration.
</nextsent>
<nextsent>we will investigate whether similar alignment-free methodology can be extended to full-text translation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC503">
<title id=" W06-1672.xml">discriminative methods for transliteration </title>
<section> conclusions.  </section>
<citcontext>
<prevsection>
<prevsent>the results are somewhat surprising, for the notion of alignment seems very intuitive and useful for transliteration.
</prevsent>
<prevsent>we will investigate whether similar alignment-free methodology can be extended to full-text translation.
</prevsent>
</prevsection>
<citsent citstr=" H05-1011 ">
it will also be interesting to study the relationship between our discriminative alignment-free methods andre cently proposed discriminative alignment-based methods for transliteration and translation (taskar et al 2005<papid> H05-1010 </papid>a; moore 2005).<papid> H05-1011 </papid></citsent>
<aftsection>
<nextsent>we also showed that for name transliteration, global discriminative modeling is superior to local classifier-based discriminative modeling.
</nextsent>
<nextsent>this may have resulted from poor calibration of scores and probabilities produced by individual classifiers.
</nextsent>
<nextsent>we plan to further investigate the relationship between the local and global approaches to complex learning problems in natural language.
</nextsent>

</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC504">
<title id=" W06-1668.xml">competitive generative models with structure learning for nlp classification tasks </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in addition, as structure learning for generative models is far more efficient, they may be preferable for some tasks.
</prevsent>
<prevsent>discriminative models have become the models of choice for nlp tasks, because of their ability to easily incorporate non-independent features and to more directly optimize classification accuracy.state of the art models for many nlp tasks are either fully discriminative or trained using discriminative reranking (collins, 2000).
</prevsent>
</prevsection>
<citsent citstr=" N03-1033 ">
these include models for part-of-speech tagging (toutanova et al., 2003), <papid> N03-1033 </papid>semantic-role labeling (punyakanok etal., 2005; pradhan et al, 2005<papid> P05-1072 </papid>b) and penn tree bank parsing (charniak and johnson, 2005).<papid> P05-1022 </papid></citsent>
<aftsection>
<nextsent>the superiority of discriminative models hasbeen shown on many tasks when the discriminative and generative models use exactly the same model structure (klein and manning, 2002).<papid> W02-1002 </papid></nextsent>
<nextsent>however, the advantage of the discriminative models can be very slight (johnson, 2001) <papid> P01-1042 </papid>and for small training set sizes generative models canbe better because they need fewer training samples to converge to the optimal parameter setting(ng and jordan, 2002).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC505">
<title id=" W06-1668.xml">competitive generative models with structure learning for nlp classification tasks </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in addition, as structure learning for generative models is far more efficient, they may be preferable for some tasks.
</prevsent>
<prevsent>discriminative models have become the models of choice for nlp tasks, because of their ability to easily incorporate non-independent features and to more directly optimize classification accuracy.state of the art models for many nlp tasks are either fully discriminative or trained using discriminative reranking (collins, 2000).
</prevsent>
</prevsection>
<citsent citstr=" P05-1072 ">
these include models for part-of-speech tagging (toutanova et al., 2003), <papid> N03-1033 </papid>semantic-role labeling (punyakanok etal., 2005; pradhan et al, 2005<papid> P05-1072 </papid>b) and penn tree bank parsing (charniak and johnson, 2005).<papid> P05-1022 </papid></citsent>
<aftsection>
<nextsent>the superiority of discriminative models hasbeen shown on many tasks when the discriminative and generative models use exactly the same model structure (klein and manning, 2002).<papid> W02-1002 </papid></nextsent>
<nextsent>however, the advantage of the discriminative models can be very slight (johnson, 2001) <papid> P01-1042 </papid>and for small training set sizes generative models canbe better because they need fewer training samples to converge to the optimal parameter setting(ng and jordan, 2002).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC508">
<title id=" W06-1668.xml">competitive generative models with structure learning for nlp classification tasks </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in addition, as structure learning for generative models is far more efficient, they may be preferable for some tasks.
</prevsent>
<prevsent>discriminative models have become the models of choice for nlp tasks, because of their ability to easily incorporate non-independent features and to more directly optimize classification accuracy.state of the art models for many nlp tasks are either fully discriminative or trained using discriminative reranking (collins, 2000).
</prevsent>
</prevsection>
<citsent citstr=" P05-1022 ">
these include models for part-of-speech tagging (toutanova et al., 2003), <papid> N03-1033 </papid>semantic-role labeling (punyakanok etal., 2005; pradhan et al, 2005<papid> P05-1072 </papid>b) and penn tree bank parsing (charniak and johnson, 2005).<papid> P05-1022 </papid></citsent>
<aftsection>
<nextsent>the superiority of discriminative models hasbeen shown on many tasks when the discriminative and generative models use exactly the same model structure (klein and manning, 2002).<papid> W02-1002 </papid></nextsent>
<nextsent>however, the advantage of the discriminative models can be very slight (johnson, 2001) <papid> P01-1042 </papid>and for small training set sizes generative models canbe better because they need fewer training samples to converge to the optimal parameter setting(ng and jordan, 2002).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC509">
<title id=" W06-1668.xml">competitive generative models with structure learning for nlp classification tasks </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>discriminative models have become the models of choice for nlp tasks, because of their ability to easily incorporate non-independent features and to more directly optimize classification accuracy.state of the art models for many nlp tasks are either fully discriminative or trained using discriminative reranking (collins, 2000).
</prevsent>
<prevsent>these include models for part-of-speech tagging (toutanova et al., 2003), <papid> N03-1033 </papid>semantic-role labeling (punyakanok etal., 2005; pradhan et al, 2005<papid> P05-1072 </papid>b) and penn tree bank parsing (charniak and johnson, 2005).<papid> P05-1022 </papid></prevsent>
</prevsection>
<citsent citstr=" W02-1002 ">
the superiority of discriminative models hasbeen shown on many tasks when the discriminative and generative models use exactly the same model structure (klein and manning, 2002).<papid> W02-1002 </papid></citsent>
<aftsection>
<nextsent>however, the advantage of the discriminative models can be very slight (johnson, 2001) <papid> P01-1042 </papid>and for small training set sizes generative models canbe better because they need fewer training samples to converge to the optimal parameter setting(ng and jordan, 2002).</nextsent>
<nextsent>additionally, many discriminative models use generative model as base model and add discriminative features with reranking (collins, 2000; charniak and johnson, 2005; <papid> P05-1022 </papid>roark et al, 2004), <papid> P04-1007 </papid>or train discriminativelya small set of weights for features which are gener atively estimated probabilities (raina et al, 2004; och and ney, 2002).<papid> P02-1038 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC510">
<title id=" W06-1668.xml">competitive generative models with structure learning for nlp classification tasks </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these include models for part-of-speech tagging (toutanova et al., 2003), <papid> N03-1033 </papid>semantic-role labeling (punyakanok etal., 2005; pradhan et al, 2005<papid> P05-1072 </papid>b) and penn tree bank parsing (charniak and johnson, 2005).<papid> P05-1022 </papid></prevsent>
<prevsent>the superiority of discriminative models hasbeen shown on many tasks when the discriminative and generative models use exactly the same model structure (klein and manning, 2002).<papid> W02-1002 </papid></prevsent>
</prevsection>
<citsent citstr=" P01-1042 ">
however, the advantage of the discriminative models can be very slight (johnson, 2001) <papid> P01-1042 </papid>and for small training set sizes generative models canbe better because they need fewer training samples to converge to the optimal parameter setting(ng and jordan, 2002).</citsent>
<aftsection>
<nextsent>additionally, many discriminative models use generative model as base model and add discriminative features with reranking (collins, 2000; charniak and johnson, 2005; <papid> P05-1022 </papid>roark et al, 2004), <papid> P04-1007 </papid>or train discriminativelya small set of weights for features which are gener atively estimated probabilities (raina et al, 2004; och and ney, 2002).<papid> P02-1038 </papid></nextsent>
<nextsent>therefore it is important tostudy generative models and to find ways of making them better even when they are used only as components of discriminative models.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC512">
<title id=" W06-1668.xml">competitive generative models with structure learning for nlp classification tasks </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the superiority of discriminative models hasbeen shown on many tasks when the discriminative and generative models use exactly the same model structure (klein and manning, 2002).<papid> W02-1002 </papid></prevsent>
<prevsent>however, the advantage of the discriminative models can be very slight (johnson, 2001) <papid> P01-1042 </papid>and for small training set sizes generative models canbe better because they need fewer training samples to converge to the optimal parameter setting(ng and jordan, 2002).</prevsent>
</prevsection>
<citsent citstr=" P04-1007 ">
additionally, many discriminative models use generative model as base model and add discriminative features with reranking (collins, 2000; charniak and johnson, 2005; <papid> P05-1022 </papid>roark et al, 2004), <papid> P04-1007 </papid>or train discriminativelya small set of weights for features which are gener atively estimated probabilities (raina et al, 2004; och and ney, 2002).<papid> P02-1038 </papid></citsent>
<aftsection>
<nextsent>therefore it is important tostudy generative models and to find ways of making them better even when they are used only as components of discriminative models.
</nextsent>
<nextsent>generative models may often perform poorly due to making strong independence assumptions about the joint distribution of features and classes.
</nextsent>
<nextsent>to avoid this problem, generative models for nlp tasks have often been manually designed to achieve an appropriate representation of the joint distribution, such as in the parsing models of (collins, 1997; <papid> P97-1003 </papid>charniak, 2000).<papid> A00-2018 </papid></nextsent>
<nextsent>this shows that when the generative models have good model structure, they can perform quite well.in this paper, we look differently at comparing generative and discriminative models.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC513">
<title id=" W06-1668.xml">competitive generative models with structure learning for nlp classification tasks </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the superiority of discriminative models hasbeen shown on many tasks when the discriminative and generative models use exactly the same model structure (klein and manning, 2002).<papid> W02-1002 </papid></prevsent>
<prevsent>however, the advantage of the discriminative models can be very slight (johnson, 2001) <papid> P01-1042 </papid>and for small training set sizes generative models canbe better because they need fewer training samples to converge to the optimal parameter setting(ng and jordan, 2002).</prevsent>
</prevsection>
<citsent citstr=" P02-1038 ">
additionally, many discriminative models use generative model as base model and add discriminative features with reranking (collins, 2000; charniak and johnson, 2005; <papid> P05-1022 </papid>roark et al, 2004), <papid> P04-1007 </papid>or train discriminativelya small set of weights for features which are gener atively estimated probabilities (raina et al, 2004; och and ney, 2002).<papid> P02-1038 </papid></citsent>
<aftsection>
<nextsent>therefore it is important tostudy generative models and to find ways of making them better even when they are used only as components of discriminative models.
</nextsent>
<nextsent>generative models may often perform poorly due to making strong independence assumptions about the joint distribution of features and classes.
</nextsent>
<nextsent>to avoid this problem, generative models for nlp tasks have often been manually designed to achieve an appropriate representation of the joint distribution, such as in the parsing models of (collins, 1997; <papid> P97-1003 </papid>charniak, 2000).<papid> A00-2018 </papid></nextsent>
<nextsent>this shows that when the generative models have good model structure, they can perform quite well.in this paper, we look differently at comparing generative and discriminative models.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC515">
<title id=" W06-1668.xml">competitive generative models with structure learning for nlp classification tasks </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>therefore it is important tostudy generative models and to find ways of making them better even when they are used only as components of discriminative models.
</prevsent>
<prevsent>generative models may often perform poorly due to making strong independence assumptions about the joint distribution of features and classes.
</prevsent>
</prevsection>
<citsent citstr=" P97-1003 ">
to avoid this problem, generative models for nlp tasks have often been manually designed to achieve an appropriate representation of the joint distribution, such as in the parsing models of (collins, 1997; <papid> P97-1003 </papid>charniak, 2000).<papid> A00-2018 </papid></citsent>
<aftsection>
<nextsent>this shows that when the generative models have good model structure, they can perform quite well.in this paper, we look differently at comparing generative and discriminative models.
</nextsent>
<nextsent>we ask the question: given the same set of input features, what is the best generative model can do if it is allowed to learn an optimal structure for the joint distribution, and what is the best discriminative model can do if it is also allowed to learn an optimal structure.
</nextsent>
<nextsent>that is, we do not impose any independence assumptions on the generative or discriminative models and let them learn the best representation of the data they can.structure learning is very efficient for generative models in the form of directed graphical models (bayesian networks (pearl, 1988)), since the optimal parameters for such models can be estimated in closed form.
</nextsent>
<nextsent>we compare bayesian net 576works with structure learning to their closely related discriminative counterpart ? conditional loglinear models with structure learning.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC516">
<title id=" W06-1668.xml">competitive generative models with structure learning for nlp classification tasks </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>therefore it is important tostudy generative models and to find ways of making them better even when they are used only as components of discriminative models.
</prevsent>
<prevsent>generative models may often perform poorly due to making strong independence assumptions about the joint distribution of features and classes.
</prevsent>
</prevsection>
<citsent citstr=" A00-2018 ">
to avoid this problem, generative models for nlp tasks have often been manually designed to achieve an appropriate representation of the joint distribution, such as in the parsing models of (collins, 1997; <papid> P97-1003 </papid>charniak, 2000).<papid> A00-2018 </papid></citsent>
<aftsection>
<nextsent>this shows that when the generative models have good model structure, they can perform quite well.in this paper, we look differently at comparing generative and discriminative models.
</nextsent>
<nextsent>we ask the question: given the same set of input features, what is the best generative model can do if it is allowed to learn an optimal structure for the joint distribution, and what is the best discriminative model can do if it is also allowed to learn an optimal structure.
</nextsent>
<nextsent>that is, we do not impose any independence assumptions on the generative or discriminative models and let them learn the best representation of the data they can.structure learning is very efficient for generative models in the form of directed graphical models (bayesian networks (pearl, 1988)), since the optimal parameters for such models can be estimated in closed form.
</nextsent>
<nextsent>we compare bayesian net 576works with structure learning to their closely related discriminative counterpart ? conditional loglinear models with structure learning.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC520">
<title id=" W06-1668.xml">competitive generative models with structure learning for nlp classification tasks </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>this is why we can afford to perform complete re-estimation of the parameters of the model at each step.
</prevsent>
<prevsent>3.1 problems and datasets.
</prevsent>
</prevsection>
<citsent citstr=" J93-1005 ">
we study two classification problems ? prepositional phrase (pp) attachment, and semantic role labeling.following most of the literature on prepositional phrase attachment (e.g., (hindle and rooth,1993; <papid> J93-1005 </papid>collins and brooks, 1995; <papid> W95-0103 </papid>van schoen winkel and man derick, 2003)), we focus on themost common configuration that leads to ambi guities: np pp.</citsent>
<aftsection>
<nextsent>here, we are given verb phrase with following noun phrase and prepositional phrase.
</nextsent>
<nextsent>the goal is to determine if thepp should be attached to the verb or to the object noun phrase.
</nextsent>
<nextsent>for example, in the sentence: never [hang]v [a painting]np [with peg]pp , the prepositional phrase with peg can either modify the verb hang or the object noun phrase painting.
</nextsent>
<nextsent>here, clearly, with peg modifies the verb hang.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC521">
<title id=" W06-1668.xml">competitive generative models with structure learning for nlp classification tasks </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>this is why we can afford to perform complete re-estimation of the parameters of the model at each step.
</prevsent>
<prevsent>3.1 problems and datasets.
</prevsent>
</prevsection>
<citsent citstr=" W95-0103 ">
we study two classification problems ? prepositional phrase (pp) attachment, and semantic role labeling.following most of the literature on prepositional phrase attachment (e.g., (hindle and rooth,1993; <papid> J93-1005 </papid>collins and brooks, 1995; <papid> W95-0103 </papid>van schoen winkel and man derick, 2003)), we focus on themost common configuration that leads to ambi guities: np pp.</citsent>
<aftsection>
<nextsent>here, we are given verb phrase with following noun phrase and prepositional phrase.
</nextsent>
<nextsent>the goal is to determine if thepp should be attached to the verb or to the object noun phrase.
</nextsent>
<nextsent>for example, in the sentence: never [hang]v [a painting]np [with peg]pp , the prepositional phrase with peg can either modify the verb hang or the object noun phrase painting.
</nextsent>
<nextsent>here, clearly, with peg modifies the verb hang.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC523">
<title id=" W06-1668.xml">competitive generative models with structure learning for nlp classification tasks </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>we work with the standard dataset previously used for this task by other researchers (ratna task training devset test pp 20,801 4,039 3,097 srl 173,514 5,115 9,272 table 1: data sizes for the pp attachment and srl tasks.
</prevsent>
<prevsent>parkhi et al, 1994; collins and brooks, 1995).<papid> W95-0103 </papid></prevsent>
</prevsection>
<citsent citstr=" H94-1048 ">
it is extracted from the the penn treebank wall street journal data (ratnaparkhi et al, 1994).<papid> H94-1048 </papid></citsent>
<aftsection>
<nextsent>table 1 shows summary statistics for the dataset.
</nextsent>
<nextsent>the second task we concentrate on is semantic role labeling in the context of propbank (palmer et al, 2005).<papid> J05-1004 </papid></nextsent>
<nextsent>the propbank corpus annotates phrases which fill semantic roles for verbs on top of penn treebank parse trees.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC524">
<title id=" W06-1668.xml">competitive generative models with structure learning for nlp classification tasks </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>it is extracted from the the penn treebank wall street journal data (ratnaparkhi et al, 1994).<papid> H94-1048 </papid></prevsent>
<prevsent>table 1 shows summary statistics for the dataset.</prevsent>
</prevsection>
<citsent citstr=" J05-1004 ">
the second task we concentrate on is semantic role labeling in the context of propbank (palmer et al, 2005).<papid> J05-1004 </papid></citsent>
<aftsection>
<nextsent>the propbank corpus annotates phrases which fill semantic roles for verbs on top of penn treebank parse trees.
</nextsent>
<nextsent>the annotated roles specify agent, patient, direction, etc. the labels for semantic roles are grouped into two groups,core argument labels and modifier argument labels, which correspond approximately to the traditional distinction between arguments and adjuncts.
</nextsent>
<nextsent>there has been plenty of work on machine learning models for semantic role labeling, starting with the work of gildea and jurafsky (2002), <papid> J02-3001 </papid>and including conll shared tasks (carreras and ma`rquez, 2005).</nextsent>
<nextsent>the most successful formulation has been as learning to classify nodes in syntactic parse tree.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC525">
<title id=" W06-1668.xml">competitive generative models with structure learning for nlp classification tasks </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>the propbank corpus annotates phrases which fill semantic roles for verbs on top of penn treebank parse trees.
</prevsent>
<prevsent>the annotated roles specify agent, patient, direction, etc. the labels for semantic roles are grouped into two groups,core argument labels and modifier argument labels, which correspond approximately to the traditional distinction between arguments and adjuncts.
</prevsent>
</prevsection>
<citsent citstr=" J02-3001 ">
there has been plenty of work on machine learning models for semantic role labeling, starting with the work of gildea and jurafsky (2002), <papid> J02-3001 </papid>and including conll shared tasks (carreras and ma`rquez, 2005).</citsent>
<aftsection>
<nextsent>the most successful formulation has been as learning to classify nodes in syntactic parse tree.
</nextsent>
<nextsent>the possible labels are none,meaning that the corresponding phrase has no semantic role and the set of core and modifier labels.
</nextsent>
<nextsent>we concentrate on the sub problem of classification for core argument nodes.
</nextsent>
<nextsent>the problem is, given that node has core argument label, decide what the correct label is. other researchers have also looked at this sub problem (gildea and jurafsky, 2002; <papid> J02-3001 </papid>toutanova et al, 2005; <papid> P05-1073 </papid>pradhan et al., 2005<papid> P05-1072 </papid>a; xue and palmer, 2004).<papid> W04-3212 </papid>many features have been proposed for building models for semantic role labeling.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC527">
<title id=" W06-1668.xml">competitive generative models with structure learning for nlp classification tasks </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>the possible labels are none,meaning that the corresponding phrase has no semantic role and the set of core and modifier labels.
</prevsent>
<prevsent>we concentrate on the sub problem of classification for core argument nodes.
</prevsent>
</prevsection>
<citsent citstr=" P05-1073 ">
the problem is, given that node has core argument label, decide what the correct label is. other researchers have also looked at this sub problem (gildea and jurafsky, 2002; <papid> J02-3001 </papid>toutanova et al, 2005; <papid> P05-1073 </papid>pradhan et al., 2005<papid> P05-1072 </papid>a; xue and palmer, 2004).<papid> W04-3212 </papid>many features have been proposed for building models for semantic role labeling.</citsent>
<aftsection>
<nextsent>initially,7 features were proposed by (gildea and jurafsky, 2002), <papid> J02-3001 </papid>and all following research has used these features and some additional ones.</nextsent>
<nextsent>these are the features we use as well.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC529">
<title id=" W06-1668.xml">competitive generative models with structure learning for nlp classification tasks </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>the possible labels are none,meaning that the corresponding phrase has no semantic role and the set of core and modifier labels.
</prevsent>
<prevsent>we concentrate on the sub problem of classification for core argument nodes.
</prevsent>
</prevsection>
<citsent citstr=" W04-3212 ">
the problem is, given that node has core argument label, decide what the correct label is. other researchers have also looked at this sub problem (gildea and jurafsky, 2002; <papid> J02-3001 </papid>toutanova et al, 2005; <papid> P05-1073 </papid>pradhan et al., 2005<papid> P05-1072 </papid>a; xue and palmer, 2004).<papid> W04-3212 </papid>many features have been proposed for building models for semantic role labeling.</citsent>
<aftsection>
<nextsent>initially,7 features were proposed by (gildea and jurafsky, 2002), <papid> J02-3001 </papid>and all following research has used these features and some additional ones.</nextsent>
<nextsent>these are the features we use as well.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC544">
<title id=" W06-1607.xml">phrase table smoothing for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these conditional distributions are derived from the joint frequencies c(s?, t?)
</prevsent>
<prevsent>of source/target phrase pairs observed in word-aligned parallel corpus.
</prevsent>
</prevsection>
<citsent citstr=" N03-1017 ">
traditionally, maximum-likelihood estimation from relative frequencies is used to obtain conditional probabilities (koehn et al, 2003), <papid> N03-1017 </papid>eg, p(s?|t?) = c(s?, t?)/s? c(s?, t?)</citsent>
<aftsection>
<nextsent>(since the estimation problems for p(s?|t?) and p(t?|s?) are symmetrical, we will usually refer only to p(s?|t?) for brevity).
</nextsent>
<nextsent>the most obvious example of the over fitting this causes can be seen in phrase pairs whose constituent phrases occur only once in the corpus.
</nextsent>
<nextsent>these are assigned conditional probabilities of 1, higher than the estimated probabilities of pairs for which much more evidence exists, in the typical case where the latter have constituents that cooccur occasionally with other phrases.
</nextsent>
<nextsent>during decoding, overlapping phrase pairs are in direct competition, so estimation biases such as this one in favour of infrequent pairs have the potential to significantly degrade translation quality.an excellent discussion of smoothing techniques developed for ngram language models (lms) may be found in (chen and goodman, 1998; goodman, 2001).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC545">
<title id=" W06-1607.xml">phrase table smoothing for statistical machine translation </title>
<section> phrase-based statistical mt.  </section>
<citcontext>
<prevsection>
<prevsent>tk ; sk are source phrases such that = sj1 . . .
</prevsent>
<prevsent>sjk ; and sk is the translation of the kth target phrase tk.1this is first approximation; exceptions occur when different phrase tables are used in parallel, and when rules are used to translate certain classes of entities.
</prevsent>
</prevsection>
<citsent citstr=" P03-1021 ">
to model p(t,a|s), we use standard loglinear approach: p(t,a|s) ? exp [ ? ifi(s, t,a) ] where each fi(s, t,a) is feature function, and weights are set using ochs algorithm (och,2003) <papid> P03-1021 </papid>to maximize the systems bleu score (pa pineni et al, 2001) on development corpus.</citsent>
<aftsection>
<nextsent>the features used in this study are: the length of t; single-parameter distortion penalty on phrase reordering in a, as described in (koehn et al, 2003); <papid> N03-1017 </papid>phrase translation model probabilities; and trigram language model probabilities log p(t), using kneser-ney smoothing as implemented in the srilm toolkit (stolcke, 2002).phrase translation model probabilities are features of the form: log p(s|t,a) ? ? k=1 log p(sk|tk) ie, we assume that the phrases sk specified by are conditionally independent, and depend only on their aligned phrases tk. the forward?</nextsent>
<nextsent>phrase probabilities p(t?|s?) are not used as features, but only as filter on the set of possible translations: for each source phrase s?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC548">
<title id=" W06-1607.xml">phrase table smoothing for statistical machine translation </title>
<section> phrase-based statistical mt.  </section>
<citcontext>
<prevsection>
<prevsent>according to p(t?|s?) are retained.
</prevsent>
<prevsent>to derive the joint counts c(s?, t?)
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
from which p(s?|t?) and p(t?|s?) are estimated, we use the phrase induction algorithm described in (koehn et al,2003), <papid> N03-1017 </papid>with symmetrized word alignments generated using ibm model 2 (brown et al, 1993).<papid> J93-2003 </papid></citsent>
<aftsection>
<nextsent>smoothing involves some recipe for modifying conditional distributions away from pure relative frequency estimates made from joint counts, in order to compensate for data sparsity.
</nextsent>
<nextsent>in the spirit of((hastie et al, 2001), figure 2.11, pg.
</nextsent>
<nextsent>38) smoothing can be seen as way of combining the relative frequency estimate, which is model with high complexity, high variance, and low bias, with another model with lower complexity, lower variance, and high bias, in the hope of obtaining better performance on new data.
</nextsent>
<nextsent>there are two main ingredients in all such recipes: some probability distribution that is smoother than relative frequencies (ie, that has fewer parameters and is thus less 54 complex) and some technique for combining that distribution with relative frequency estimates.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC550">
<title id=" W06-1607.xml">phrase table smoothing for statistical machine translation </title>
<section> smoothing techniques.  </section>
<citcontext>
<prevsection>
<prevsent>since for phrase table smoothing, better prediction of unseen (zero-count) events has no direct impact only seen events are represented in the phrase table, and thus hypothesized during decoding interpolation seemed more suitable approach.
</prevsent>
<prevsent>for combining relative-frequency estimates with glass-box smoothing distributions, we employed loglinear interpolation.
</prevsent>
</prevsection>
<citsent citstr=" N04-1033 ">
this is the traditional approach for glass-box smoothing (koehn et al, 2003; <papid> N03-1017 </papid>zens and ney, 2004).<papid> N04-1033 </papid></citsent>
<aftsection>
<nextsent>to illustrate the difference between linear and loglinear interpolation, consider combining two bernoulli distributions p1(x) and p2(x) using each method: plinear(x) = p1(x) + (1?
</nextsent>
<nextsent>?)p2(x) ploglin(x) = p1(x)p2(x) p1(x)p2(x) + q1(x)q2(x) where qi(x) = 1 ? pi(x).
</nextsent>
<nextsent>setting p2(x) = 0.5 to simulate uniform smoothing gives ploglin(x) = p1(x)?/(p1(x)?
</nextsent>
<nextsent>+ q1(x)?).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC556">
<title id=" W06-1607.xml">phrase table smoothing for statistical machine translation </title>
<section> smoothing techniques.  </section>
<citcontext>
<prevsection>
<prevsent>we have not yet implemented this new glass-box smoothing technique, but it has considerable appeal.
</prevsent>
<prevsent>the idea is similar in spirit to collins?
</prevsent>
</prevsection>
<citsent citstr=" W95-0103 ">
backoff method for prepositional phrase attachment (collins and brooks, 1995).<papid> W95-0103 </papid></citsent>
<aftsection>
<nextsent>as mentioned previously, (chen and goodman,1998) give comprehensive survey and evaluation of smoothing techniques for language modeling.
</nextsent>
<nextsent>as also mentioned previously, there is relatively little published work on smoothing for statistical mt. for the ibm models, alignment probabilities need to be smoothed for combinations of sentence lengths and positions not encountered in training data (garca-varea et al, 1998).moore (2004) <papid> P04-1066 </papid>has found that smoothing to correct overestimated ibm1 lexical probabilities forrare words can improve word-alignment perfor mance.</nextsent>
<nextsent>langlais (2005) reports negative results for synonym-based smoothing of ibm2 lexical probabilities prior to extracting phrases for phrase based smt.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC557">
<title id=" W06-1607.xml">phrase table smoothing for statistical machine translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>backoff method for prepositional phrase attachment (collins and brooks, 1995).<papid> W95-0103 </papid></prevsent>
<prevsent>as mentioned previously, (chen and goodman,1998) give comprehensive survey and evaluation of smoothing techniques for language mod eling.</prevsent>
</prevsection>
<citsent citstr=" P04-1066 ">
as also mentioned previously, there is relatively little published work on smoothing for statistical mt. for the ibm models, alignment probabilities need to be smoothed for combinations of sentence lengths and positions not encountered in training data (garca-varea et al, 1998).moore (2004) <papid> P04-1066 </papid>has found that smoothing to correct overestimated ibm1 lexical probabilities forrare words can improve word-alignment perfor mance.</citsent>
<aftsection>
<nextsent>langlais (2005) reports negative results for synonym-based smoothing of ibm2 lexical probabilities prior to extracting phrases for phrase based smt.
</nextsent>
<nextsent>for phrase-based smt, the use of smoothing to avoid zero probabilities during phrase induction is reported in (marcu and wong, 2002), <papid> W02-1018 </papid>but no details are given.</nextsent>
<nextsent>as described above, (zens and 57ney, 2004) and (koehn et al, 2005) use two different variants of glass-box smoothing (which they call lexical smoothing?)</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC558">
<title id=" W06-1607.xml">phrase table smoothing for statistical machine translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>as also mentioned previously, there is relatively little published work on smoothing for statistical mt. for the ibm models, alignment probabilities need to be smoothed for combinations of sentence lengths and positions not encountered in training data (garca-varea et al, 1998).moore (2004) <papid> P04-1066 </papid>has found that smoothing to correct overestimated ibm1 lexical probabilities forrare words can improve word-alignment perfor mance.</prevsent>
<prevsent>langlais (2005) reports negative results for synonym-based smoothing of ibm2 lexical probabilities prior to extracting phrases for phrase based smt.</prevsent>
</prevsection>
<citsent citstr=" W02-1018 ">
for phrase-based smt, the use of smoothing to avoid zero probabilities during phrase induction is reported in (marcu and wong, 2002), <papid> W02-1018 </papid>but no details are given.</citsent>
<aftsection>
<nextsent>as described above, (zens and 57ney, 2004) and (koehn et al, 2005) use two different variants of glass-box smoothing (which they call lexical smoothing?)
</nextsent>
<nextsent>over the phrase table, and combine the resulting estimates with pure relative frequency ones in loglinear model.
</nextsent>
<nextsent>finally, (cet tollo et al, 2005) describes the use of witten-bell smoothing (a black-box technique) for phrase table counts, but does not give comparison to other methods.
</nextsent>
<nextsent>as witten-bell is reported by (chen and goodman, 1998) to be significantly worse than kneser-ney smoothing, we have not yet tested this method.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC559">
<title id=" W06-1650.xml">automatically assessing review helpfulness </title>
<section> relevant work.  </section>
<citcontext>
<prevsection>
<prevsent>our main contributions are: ? system for automatically ranking reviews according to helpfulness; using state of the art svm regression, we empirically evaluate our system on real world dataset collected from amazon.com on the task of reconstructing the helpfulness ranking; and ? an analysis of different classes of features most important to capture review helpful ness; including structural (e.g., html tags, punctuation, review length), lexical (e.g., grams), syntactic (e.g., percentage of verbs and nouns), semantic (e.g., product feature men tions), and meta-data (e.g., star rating).
</prevsent>
<prevsent>the task of automatically assessing product review helpfulness is related to these broader areas 423of research: automatic analysis of product reviews, opinion and sentiment analysis, and text classification.
</prevsent>
</prevsection>
<citsent citstr=" P02-1053 ">
in the thriving area of research on automatic analysis and processing of product reviews (hu and liu 2004; turney 2002; <papid> P02-1053 </papid>pang and lee 2005), <papid> P05-1015 </papid>little attention has been paid to the important task studied here ? assessing review helpfulness.</citsent>
<aftsection>
<nextsent>pang and lee (2005) <papid> P05-1015 </papid>have studied prediction of product ratings, which may be particularly relevant due to the correlation we find between product rating and the helpfulness of the review (dis cussed in section 5).</nextsent>
<nextsent>however, users overall rating for the product is often already available.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC560">
<title id=" W06-1650.xml">automatically assessing review helpfulness </title>
<section> relevant work.  </section>
<citcontext>
<prevsection>
<prevsent>our main contributions are: ? system for automatically ranking reviews according to helpfulness; using state of the art svm regression, we empirically evaluate our system on real world dataset collected from amazon.com on the task of reconstructing the helpfulness ranking; and ? an analysis of different classes of features most important to capture review helpful ness; including structural (e.g., html tags, punctuation, review length), lexical (e.g., grams), syntactic (e.g., percentage of verbs and nouns), semantic (e.g., product feature men tions), and meta-data (e.g., star rating).
</prevsent>
<prevsent>the task of automatically assessing product review helpfulness is related to these broader areas 423of research: automatic analysis of product reviews, opinion and sentiment analysis, and text classification.
</prevsent>
</prevsection>
<citsent citstr=" P05-1015 ">
in the thriving area of research on automatic analysis and processing of product reviews (hu and liu 2004; turney 2002; <papid> P02-1053 </papid>pang and lee 2005), <papid> P05-1015 </papid>little attention has been paid to the important task studied here ? assessing review helpfulness.</citsent>
<aftsection>
<nextsent>pang and lee (2005) <papid> P05-1015 </papid>have studied prediction of product ratings, which may be particularly relevant due to the correlation we find between product rating and the helpfulness of the review (dis cussed in section 5).</nextsent>
<nextsent>however, users overall rating for the product is often already available.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC562">
<title id=" W06-1650.xml">automatically assessing review helpfulness </title>
<section> relevant work.  </section>
<citcontext>
<prevsection>
<prevsent>in opinion and sentiment analysis, the focus is on distinguishing between statements of fact vs. opinion, and on detecting the polarity of sentiments being expressed.
</prevsent>
<prevsent>many researchers have worked in various facets of opinion analysis.
</prevsent>
</prevsection>
<citsent citstr=" W02-1011 ">
pang et al (2002) <papid> W02-1011 </papid>and turney (2002) <papid> P02-1053 </papid>classified sentiment polarity of reviews at the document level.</citsent>
<aftsection>
<nextsent>wiebe et al (1999) <papid> P99-1032 </papid>classified sentence level subjectivity using syntactic classes such as adjectives, pronouns and modal verbs as features.</nextsent>
<nextsent>riloff and wiebe (2003) <papid> W03-1014 </papid>extracted subjective expressions from sentences using bootstrapping pattern learning process.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC564">
<title id=" W06-1650.xml">automatically assessing review helpfulness </title>
<section> relevant work.  </section>
<citcontext>
<prevsection>
<prevsent>many researchers have worked in various facets of opinion analysis.
</prevsent>
<prevsent>pang et al (2002) <papid> W02-1011 </papid>and turney (2002) <papid> P02-1053 </papid>classified sentiment polarity of reviews at the document level.</prevsent>
</prevsection>
<citsent citstr=" P99-1032 ">
wiebe et al (1999) <papid> P99-1032 </papid>classified sentence level subjectivity using syntactic classes such as adjectives, pronouns and modal verbs as features.</citsent>
<aftsection>
<nextsent>riloff and wiebe (2003) <papid> W03-1014 </papid>extracted subjective expressions from sentences using bootstrapping pattern learning process.</nextsent>
<nextsent>yu and hatzivassiloglou (2003) <papid> W03-1017 </papid>identified the polarity of opinion sentences using semantically oriented words.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC565">
<title id=" W06-1650.xml">automatically assessing review helpfulness </title>
<section> relevant work.  </section>
<citcontext>
<prevsection>
<prevsent>pang et al (2002) <papid> W02-1011 </papid>and turney (2002) <papid> P02-1053 </papid>classified sentiment polarity of reviews at the document level.</prevsent>
<prevsent>wiebe et al (1999) <papid> P99-1032 </papid>classified sentence level subjectivity using syntactic classes such as adjectives, pronouns and modal verbs as features.</prevsent>
</prevsection>
<citsent citstr=" W03-1014 ">
riloff and wiebe (2003) <papid> W03-1014 </papid>extracted subjective expressions from sentences using bootstrapping pattern learning process.</citsent>
<aftsection>
<nextsent>yu and hatzivassiloglou (2003) <papid> W03-1017 </papid>identified the polarity of opinion sentences using semantically oriented words.</nextsent>
<nextsent>these techniques were applied and examined in different domains, such as customer reviews (hu and liu 2004) and news articles (trec novelty track 2003 and 2004).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC566">
<title id=" W06-1650.xml">automatically assessing review helpfulness </title>
<section> relevant work.  </section>
<citcontext>
<prevsection>
<prevsent>wiebe et al (1999) <papid> P99-1032 </papid>classified sentence level subjectivity using syntactic classes such as adjectives, pronouns and modal verbs as features.</prevsent>
<prevsent>riloff and wiebe (2003) <papid> W03-1014 </papid>extracted subjective expressions from sentences using bootstrapping pattern learning process.</prevsent>
</prevsection>
<citsent citstr=" W03-1017 ">
yu and hatzivassiloglou (2003) <papid> W03-1017 </papid>identified the polarity of opinion sentences using semantically oriented words.</citsent>
<aftsection>
<nextsent>these techniques were applied and examined in different domains, such as customer reviews (hu and liu 2004) and news articles (trec novelty track 2003 and 2004).
</nextsent>
<nextsent>in text classification, systems typically use bag-of-words models, although there is some evidence of benefits when introducing relevant semantic knowledge (gabrilovich and markovitch, 2005).
</nextsent>
<nextsent>in this paper, we explore the use of some semantic features for review helpfulness ranking.
</nextsent>
<nextsent>another potential relevant classification task is academic and commercial efforts on detecting email spam messages1, which aim to capture much broader notion of helpfulness.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC568">
<title id=" W06-1707.xml">corp orator a tool for creating rss based specialized corpora </title>
<section> introduction1.  </section>
<citcontext>
<prevsection>
<prevsent>examples of projects adopting the latter approach are numerous (among many sekigushi and yammoto, 2004; emirkanian et al 2004).
</prevsent>
<prevsent>it is also the goal of the wacky project for instance which aims at developing tools that will allow linguists to crawl section of the web, process the data, index them and search them2.
</prevsent>
</prevsection>
<citsent citstr=" J03-3001 ">
so we have the internet: it is immense, free, easily accessible and can be used for all manner of language research (kilgarriff and grefenstette, 2003).<papid> J03-3001 </papid></citsent>
<aftsection>
<nextsent>but text is so abundant, that it is not so easy to find appropriate textual data forgiven task.
</nextsent>
<nextsent>for this reason, researchers have been developing softwares that are able to crawl the web and find sources corresponding to specific criteria.
</nextsent>
<nextsent>using clustering algorithms or similarity measures, it is possible to select texts that are similar to training set.
</nextsent>
<nextsent>these techniques can achieve good results, but they are sometimes limited when it comes to distinguishing between well-written texts vs. poorly written, or other subtle criteria.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC569">
<title id=" W06-1703.xml">a comparative study on compositional translation estimation using a domaintopicspecific corpus collected from the web </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this paper studies issues related to the compilation of bilingual lexicon for technical terms.
</prevsent>
<prevsent>thus far, several techniques of estimating bilingual term correspondences from parallel/comparable corpus have been studied (matsumoto and utsuro, 2000).
</prevsent>
</prevsection>
<citsent citstr=" P98-1069 ">
for example, in the case of estimation from comparable corpora, (fung and yee, 1998; <papid> P98-1069 </papid>rapp, 1999) <papid> P99-1067 </papid>proposed standard techniques of estimating bilingual term correspondences from comparable corpora.</citsent>
<aftsection>
<nextsent>in their techniques, contextual similarity between source language term and its translation candidate is measured across the languages, andall the translation candidates are re-ranked according to their contextual similarities.
</nextsent>
<nextsent>however, there are limited number of parallel/comparable corpora that are available for the purpose of estimating bilingual term correspondences.
</nextsent>
<nextsent>therefore, even if one wants to apply those existing techniques tothe task of estimating bilingual term correspondences of technical terms, it is usually rather difficult to find an existing corpus for the domain of such technical terms.on the other hand, compositional translation estimation techniques that use monolingual corpus (fujii and ishikawa, 2001; tanaka and baldwin, 2003) are more practical.
</nextsent>
<nextsent>it is because collecting amonolingual corpus is less expensive than collecting parallel/comparable corpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC570">
<title id=" W06-1703.xml">a comparative study on compositional translation estimation using a domaintopicspecific corpus collected from the web </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this paper studies issues related to the compilation of bilingual lexicon for technical terms.
</prevsent>
<prevsent>thus far, several techniques of estimating bilingual term correspondences from parallel/comparable corpus have been studied (matsumoto and utsuro, 2000).
</prevsent>
</prevsection>
<citsent citstr=" P99-1067 ">
for example, in the case of estimation from comparable corpora, (fung and yee, 1998; <papid> P98-1069 </papid>rapp, 1999) <papid> P99-1067 </papid>proposed standard techniques of estimating bilingual term correspondences from comparable corpora.</citsent>
<aftsection>
<nextsent>in their techniques, contextual similarity between source language term and its translation candidate is measured across the languages, andall the translation candidates are re-ranked according to their contextual similarities.
</nextsent>
<nextsent>however, there are limited number of parallel/comparable corpora that are available for the purpose of estimating bilingual term correspondences.
</nextsent>
<nextsent>therefore, even if one wants to apply those existing techniques tothe task of estimating bilingual term correspondences of technical terms, it is usually rather difficult to find an existing corpus for the domain of such technical terms.on the other hand, compositional translation estimation techniques that use monolingual corpus (fujii and ishikawa, 2001; tanaka and baldwin, 2003) are more practical.
</nextsent>
<nextsent>it is because collecting amonolingual corpus is less expensive than collecting parallel/comparable corpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC571">
<title id=" W06-1703.xml">a comparative study on compositional translation estimation using a domaintopicspecific corpus collected from the web </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in the first approach, translation candidates are validated through the search engine (cao and li, 2002).
</prevsent>
<prevsent>in the second approach, domain/topic-specific corpus is collected from the web in advance and fixed 11 collecting terms of specific domain/topic (language ) xsu (# of translations is one) compiled bilingual lexicon process data collecting corpus (language ) domain/topic specific corpus (language ) sample terms of specific domain/topic (language ) xstu , xstm ,yst estimating bilingual term correspondences language pair (s,t ) term set (language ) xtu (lang.
</prevsent>
</prevsection>
<citsent citstr=" I05-2020 ">
t ) translation set (language ) web (language ) web (language ) existing bilingual lexicon xsm (# of translations is more than one) ys (# of translations is zero) web (language ) web (language ) looking up bilingual lexicon validating translation candidates web (language ) web (language )figure 1: compilation of domain/topic specific bilingual lexicon using the web before translation estimation, then generated translation candidates are validated against the domain/topic-specific corpus (tonoike et al, 2005).<papid> I05-2020 </papid></citsent>
<aftsection>
<nextsent>the first approach is preferable in terms of coverage, while the second is preferable in terms of computational efficiency.
</nextsent>
<nextsent>this paper mainly focuses on quantitatively comparing the two approaches in terms of coverage and precision of compositional translation estimation.
</nextsent>
<nextsent>more specifically, in compositional translation estimation, we decompose the scoring function of translation candidate into two components: bilingual lexicon score and corpus score.
</nextsent>
<nextsent>in this paper, we examine variants for those components and define 9 types of scoring functions in total.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC572">
<title id=" W06-1703.xml">a comparative study on compositional translation estimation using a domaintopicspecific corpus collected from the web </title>
<section> overall framework.  </section>
<citcontext>
<prevsection>
<prevsent>the experimental result shows that it is quite possible to achieve precision much higher than those of single scoring functions.
</prevsent>
<prevsent>the overall framework of compiling bilingual lexicon from the web is illustrated as in figure 1.
</prevsent>
</prevsection>
<citsent citstr=" P03-2020 ">
suppose that we have sample terms of specific domain/topic, then the technical terms that are to be listed as the headwords of bilingual lexicon are collected from the web by the related term collection method of (sato and sasaki, 2003).<papid> P03-2020 </papid></citsent>
<aftsection>
<nextsent>these collected technical terms can be divided into three subsets depending on the number of translation candidates present in an existing bilingual lexicon, i.e., the subset xus of terms for which the number of translations in the existing bilingual lexicon is one, the subset xms of terms for which the number of translations is more than one, and the subset ys of terms that are not found in the existing bilingual lexicon (henceforth, the union xus ? xms will be denoted as xs).
</nextsent>
<nextsent>here, the translation estimation task here is to estimate translations for the termsof the subsets xms and ys . new bilingual lexicon is compiled from the result of the translation estimation for the terms of the subsets xms and ys as well as the translation pairs that consist of the terms of the subset xus and their translations found in the existing bilingual lexicon.
</nextsent>
<nextsent>for the terms of the subset xms , it is required that an appropriate translation is selected from among the translation candidates found in the existing bilingual lexicon.
</nextsent>
<nextsent>for example, as translation of the japanese technical term ?????,?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC574">
<title id=" W06-1703.xml">a comparative study on compositional translation estimation using a domaintopicspecific corpus collected from the web </title>
<section> related works.  </section>
<citcontext>
<prevsection>
<prevsent>mains.
</prevsent>
<prevsent>another significant difference is that in (fujii and ishikawa, 2001), they evaluate only the performance of the cross-language information retrieval and not that of translation estimation.
</prevsent>
</prevsection>
<citsent citstr=" C02-1011 ">
(cao and li, 2002) <papid> C02-1011 </papid>also proposed method of compositional translation estimation for com pounds.</citsent>
<aftsection>
<nextsent>in the method of (cao and li, 2002), the translation candidates of term are compositionally generated by concatenating the translation ofthe constituents of the term and are validated directly through the search engine.
</nextsent>
<nextsent>in this paper, we evaluate the approach proposed in (cao and li, 2002) <papid> C02-1011 </papid>by introducing total scoring function 17 that is based on validating translation candidates directly through the search engine.</nextsent>
<nextsent>this paper studied issues related to the compilation bilingual lexicon for technical terms.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC578">
<title id=" W06-1703.xml">a comparative study on compositional translation estimation using a domaintopicspecific corpus collected from the web </title>
<section> conclusion.  </section>
<citcontext>
<prevsection>
<prevsent>this paper focused on quantitatively comparing variations of the component sin the scoring functions of compositional translation estimation.
</prevsent>
<prevsent>through experimental evaluation, we showed that the domain/topic specific corpus contributes to improving the performance of the compositional translation estimation.future work includes complement ally integrating the proposed framework of compositional translation estimation using the web with other translation estimation techniques.
</prevsent>
</prevsection>
<citsent citstr=" H05-1061 ">
one of them is that based on collecting partially bilingual texts through the search engine (nagata and others, 2001; huang et al, 2005).<papid> H05-1061 </papid></citsent>
<aftsection>
<nextsent>another technique which seems to be useful is that of transliteration of names (knight and graehl, 1998; <papid> J98-4003 </papid>oh and choi, 2005).<papid> I05-2012 </papid></nextsent>



</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC579">
<title id=" W06-1703.xml">a comparative study on compositional translation estimation using a domaintopicspecific corpus collected from the web </title>
<section> conclusion.  </section>
<citcontext>
<prevsection>
<prevsent>through experimental evaluation, we showed that the domain/topic specific corpus contributes to improving the performance of the compositional translation estimation.future work includes complement ally integrating the proposed framework of compositional translation estimation using the web with other translation estimation techniques.
</prevsent>
<prevsent>one of them is that based on collecting partially bilingual texts through the search engine (nagata and others, 2001; huang et al, 2005).<papid> H05-1061 </papid></prevsent>
</prevsection>
<citsent citstr=" J98-4003 ">
another technique which seems to be useful is that of transliteration of names (knight and graehl, 1998; <papid> J98-4003 </papid>oh and choi, 2005).<papid> I05-2012 </papid></citsent>
<aftsection>




</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC580">
<title id=" W06-1703.xml">a comparative study on compositional translation estimation using a domaintopicspecific corpus collected from the web </title>
<section> conclusion.  </section>
<citcontext>
<prevsection>
<prevsent>through experimental evaluation, we showed that the domain/topic specific corpus contributes to improving the performance of the compositional translation estimation.future work includes complement ally integrating the proposed framework of compositional translation estimation using the web with other translation estimation techniques.
</prevsent>
<prevsent>one of them is that based on collecting partially bilingual texts through the search engine (nagata and others, 2001; huang et al, 2005).<papid> H05-1061 </papid></prevsent>
</prevsection>
<citsent citstr=" I05-2012 ">
another technique which seems to be useful is that of transliteration of names (knight and graehl, 1998; <papid> J98-4003 </papid>oh and choi, 2005).<papid> I05-2012 </papid></citsent>
<aftsection>




</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC581">
<title id=" W06-1601.xml">unsupervised discovery of a statistical verb lexicon </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, while the subject of increase expresses the agent role in the first, it instead expresses the patient role in the second.
</prevsent>
<prevsent>pairs of linkings such as this allowed by asingle predicate are often called dia thesis alternations (levin, 1993).the current state-of-the-art approach to resolving this ambiguity is to use discriminative classifiers, trained on hand-tagged data, to classify the 1also called thematic roles, theta roles, or deep cases.
</prevsent>
</prevsection>
<citsent citstr=" J02-3001 ">
2sometimes called frames.semantic role of each dependent (gildea and jurafsky, 2002; <papid> J02-3001 </papid>pradhan et al, 2005; <papid> P05-1072 </papid>punyakanok et al, 2005).</citsent>
<aftsection>
<nextsent>a drawback of this approach is that evena relatively large training corpus exhibits considerable sparsity of evidence.
</nextsent>
<nextsent>the two main hand tagged corpora are propbank (palmer et al, 2003) and framenet (baker et al, 1998), <papid> P98-1013 </papid>the former of which currently has broader coverage.</nextsent>
<nextsent>however, even propbank, which is based on the 1m word wsj section of the penn treebank, is insufficient in quantity and genre to exhibit many things.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC582">
<title id=" W06-1601.xml">unsupervised discovery of a statistical verb lexicon </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, while the subject of increase expresses the agent role in the first, it instead expresses the patient role in the second.
</prevsent>
<prevsent>pairs of linkings such as this allowed by asingle predicate are often called dia thesis alternations (levin, 1993).the current state-of-the-art approach to resolving this ambiguity is to use discriminative classifiers, trained on hand-tagged data, to classify the 1also called thematic roles, theta roles, or deep cases.
</prevsent>
</prevsection>
<citsent citstr=" P05-1072 ">
2sometimes called frames.semantic role of each dependent (gildea and jurafsky, 2002; <papid> J02-3001 </papid>pradhan et al, 2005; <papid> P05-1072 </papid>punyakanok et al, 2005).</citsent>
<aftsection>
<nextsent>a drawback of this approach is that evena relatively large training corpus exhibits considerable sparsity of evidence.
</nextsent>
<nextsent>the two main hand tagged corpora are propbank (palmer et al, 2003) and framenet (baker et al, 1998), <papid> P98-1013 </papid>the former of which currently has broader coverage.</nextsent>
<nextsent>however, even propbank, which is based on the 1m word wsj section of the penn treebank, is insufficient in quantity and genre to exhibit many things.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC583">
<title id=" W06-1601.xml">unsupervised discovery of a statistical verb lexicon </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>2sometimes called frames.semantic role of each dependent (gildea and jurafsky, 2002; <papid> J02-3001 </papid>pradhan et al, 2005; <papid> P05-1072 </papid>punyakanok et al, 2005).</prevsent>
<prevsent>a drawback of this approach is that evena relatively large training corpus exhibits considerable sparsity of evidence.</prevsent>
</prevsection>
<citsent citstr=" P98-1013 ">
the two main hand tagged corpora are propbank (palmer et al, 2003) and framenet (baker et al, 1998), <papid> P98-1013 </papid>the former of which currently has broader coverage.</citsent>
<aftsection>
<nextsent>however, even propbank, which is based on the 1m word wsj section of the penn treebank, is insufficient in quantity and genre to exhibit many things.
</nextsent>
<nextsent>a perfectly common verb like flap occurs only twice, across all morphological forms.
</nextsent>
<nextsent>the first example is an adjectival use (flapping wings), and the second is rare in transitive use with an agent argument and path (ducks flapping over washington).from this data, one cannot learn the basic alternation pattern for flap: the bird flapped its wings vs. the wings flapped.
</nextsent>
<nextsent>we propose to address the challenge of datasparsity by learning models of verb behavior directly from raw unannotated text, of which thereis plenty.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC584">
<title id=" W06-1601.xml">unsupervised discovery of a statistical verb lexicon </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the models learned by our unsupervised approach provide new broad-coverage lexical resource which gives statistics about verb behavior, information that may prove useful in other language processing tasks, such as parsing.
</prevsent>
<prevsent>moreover, they may be used discriminatively to label novel verb instances for semantic role.
</prevsent>
</prevsection>
<citsent citstr=" P93-1032 ">
thus we evaluate them both in terms of the verb alternations that they learn and their accuracy as semantic role labelers.this work bears some similarity to the substantial literature on automatic subcategorization frame acquisition (see, e.g., manning (1993), <papid> P93-1032 </papid>briscoe and carroll (1997), <papid> A97-1052 </papid>and korhonen(2002)).</citsent>
<aftsection>
<nextsent>however, that research is focused on acquiring verbs?
</nextsent>
<nextsent>syntactic behavior, and we are focused on the acquisition of verbs?
</nextsent>
<nextsent>linking behavior.
</nextsent>
<nextsent>more relevant is the work of mccarthy and 1 relation description subj np preceding verb np#n np in the nth position following verb np np that is not the subject and not immediately following verb cl#n complement clause in the nth position following verb cl complement clause not immediately following verb xcl#n complement clause without subject in the nth position following verb xcl complement clause without subject not immediately following verb acomp#n adjectival complement in the nth position following verb acomp adjectival complement not immediately following verb prep prepositional modifier with preposition advmod adverbial modifier advcl adverbial clause table 1: the set of syntactic relations we use, where ? {1, 2, 3} and is preposition.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC585">
<title id=" W06-1601.xml">unsupervised discovery of a statistical verb lexicon </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the models learned by our unsupervised approach provide new broad-coverage lexical resource which gives statistics about verb behavior, information that may prove useful in other language processing tasks, such as parsing.
</prevsent>
<prevsent>moreover, they may be used discriminatively to label novel verb instances for semantic role.
</prevsent>
</prevsection>
<citsent citstr=" A97-1052 ">
thus we evaluate them both in terms of the verb alternations that they learn and their accuracy as semantic role labelers.this work bears some similarity to the substantial literature on automatic subcategorization frame acquisition (see, e.g., manning (1993), <papid> P93-1032 </papid>briscoe and carroll (1997), <papid> A97-1052 </papid>and korhonen(2002)).</citsent>
<aftsection>
<nextsent>however, that research is focused on acquiring verbs?
</nextsent>
<nextsent>syntactic behavior, and we are focused on the acquisition of verbs?
</nextsent>
<nextsent>linking behavior.
</nextsent>
<nextsent>more relevant is the work of mccarthy and 1 relation description subj np preceding verb np#n np in the nth position following verb np np that is not the subject and not immediately following verb cl#n complement clause in the nth position following verb cl complement clause not immediately following verb xcl#n complement clause without subject in the nth position following verb xcl complement clause without subject not immediately following verb acomp#n adjectival complement in the nth position following verb acomp adjectival complement not immediately following verb prep prepositional modifier with preposition advmod adverbial modifier advcl adverbial clause table 1: the set of syntactic relations we use, where ? {1, 2, 3} and is preposition.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC586">
<title id=" W06-1601.xml">unsupervised discovery of a statistical verb lexicon </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>linking behavior.
</prevsent>
<prevsent>more relevant is the work of mccarthy and 1 relation description subj np preceding verb np#n np in the nth position following verb np np that is not the subject and not immediately following verb cl#n complement clause in the nth position following verb cl complement clause not immediately following verb xcl#n complement clause without subject in the nth position following verb xcl complement clause without subject not immediately following verb acomp#n adjectival complement in the nth position following verb acomp adjectival complement not immediately following verb prep prepositional modifier with preposition advmod adverbial modifier advcl adverbial clause table 1: the set of syntactic relations we use, where ? {1, 2, 3} and is preposition.
</prevsent>
</prevsection>
<citsent citstr=" P99-1051 ">
korhonen (1998), which used statistical model to identify verb alternations, relying on an existing taxonomy of possible alternations, as well as la pata (1999), <papid> P99-1051 </papid>which searched large corpus to find evidence of two particular verb alternations.</citsent>
<aftsection>
<nextsent>there has also been some work on both clustering and supervised classification of verbs based on their alternation behavior (stevenson and merlo, 1999; <papid> E99-1007 </papid>schulte im walde, 2000; merlo and stevenson,2001).<papid> J01-3003 </papid></nextsent>
<nextsent>finally, swier and stevenson (2004) <papid> W04-3213 </papid>perform unsupervised semantic role labeling by using hand-crafted verb lexicons to replace supervised semantic role training data.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC587">
<title id=" W06-1601.xml">unsupervised discovery of a statistical verb lexicon </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>more relevant is the work of mccarthy and 1 relation description subj np preceding verb np#n np in the nth position following verb np np that is not the subject and not immediately following verb cl#n complement clause in the nth position following verb cl complement clause not immediately following verb xcl#n complement clause without subject in the nth position following verb xcl complement clause without subject not immediately following verb acomp#n adjectival complement in the nth position following verb acomp adjectival complement not immediately following verb prep prepositional modifier with preposition advmod adverbial modifier advcl adverbial clause table 1: the set of syntactic relations we use, where ? {1, 2, 3} and is preposition.
</prevsent>
<prevsent>korhonen (1998), which used statistical model to identify verb alternations, relying on an existing taxonomy of possible alternations, as well as la pata (1999), <papid> P99-1051 </papid>which searched large corpus to find evidence of two particular verb alternations.</prevsent>
</prevsection>
<citsent citstr=" E99-1007 ">
there has also been some work on both clustering and supervised classification of verbs based on their alternation behavior (stevenson and merlo, 1999; <papid> E99-1007 </papid>schulte im walde, 2000; merlo and stevenson,2001).<papid> J01-3003 </papid></citsent>
<aftsection>
<nextsent>finally, swier and stevenson (2004) <papid> W04-3213 </papid>perform unsupervised semantic role labeling by using hand-crafted verb lexicons to replace supervised semantic role training data.</nextsent>
<nextsent>however, we believe this is the first system to simultaneously discover verb roles and verb linking patterns from unsupervised data using unified probabilistic model.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC588">
<title id=" W06-1601.xml">unsupervised discovery of a statistical verb lexicon </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>more relevant is the work of mccarthy and 1 relation description subj np preceding verb np#n np in the nth position following verb np np that is not the subject and not immediately following verb cl#n complement clause in the nth position following verb cl complement clause not immediately following verb xcl#n complement clause without subject in the nth position following verb xcl complement clause without subject not immediately following verb acomp#n adjectival complement in the nth position following verb acomp adjectival complement not immediately following verb prep prepositional modifier with preposition advmod adverbial modifier advcl adverbial clause table 1: the set of syntactic relations we use, where ? {1, 2, 3} and is preposition.
</prevsent>
<prevsent>korhonen (1998), which used statistical model to identify verb alternations, relying on an existing taxonomy of possible alternations, as well as la pata (1999), <papid> P99-1051 </papid>which searched large corpus to find evidence of two particular verb alternations.</prevsent>
</prevsection>
<citsent citstr=" J01-3003 ">
there has also been some work on both clustering and supervised classification of verbs based on their alternation behavior (stevenson and merlo, 1999; <papid> E99-1007 </papid>schulte im walde, 2000; merlo and stevenson,2001).<papid> J01-3003 </papid></citsent>
<aftsection>
<nextsent>finally, swier and stevenson (2004) <papid> W04-3213 </papid>perform unsupervised semantic role labeling by using hand-crafted verb lexicons to replace supervised semantic role training data.</nextsent>
<nextsent>however, we believe this is the first system to simultaneously discover verb roles and verb linking patterns from unsupervised data using unified probabilistic model.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC589">
<title id=" W06-1601.xml">unsupervised discovery of a statistical verb lexicon </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>korhonen (1998), which used statistical model to identify verb alternations, relying on an existing taxonomy of possible alternations, as well as la pata (1999), <papid> P99-1051 </papid>which searched large corpus to find evidence of two particular verb alternations.</prevsent>
<prevsent>there has also been some work on both clustering and supervised classification of verbs based on their alternation behavior (stevenson and merlo, 1999; <papid> E99-1007 </papid>schulte im walde, 2000; merlo and stevenson,2001).<papid> J01-3003 </papid></prevsent>
</prevsection>
<citsent citstr=" W04-3213 ">
finally, swier and stevenson (2004) <papid> W04-3213 </papid>perform unsupervised semantic role labeling by using hand-crafted verb lexicons to replace supervised semantic role training data.</citsent>
<aftsection>
<nextsent>however, we believe this is the first system to simultaneously discover verb roles and verb linking patterns from unsupervised data using unified probabilistic model.
</nextsent>
<nextsent>our goal is to learn model which relates verb,its semantic roles, and their possible syntactic realizations.
</nextsent>
<nextsent>as is the case with most semantic role labeling research, we do not attempt to model the syntax itself, and instead assume the existence of syntactic parse of the sentence.
</nextsent>
<nextsent>the parse may be from human annotator, where available, or from an automatic parser.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC590">
<title id=" W06-1601.xml">unsupervised discovery of a statistical verb lexicon </title>
<section> learning setting.  </section>
<citcontext>
<prevsection>
<prevsent>our goal was to choose set that provides sufficient syntactic information for these mantic role decision, while remaining accurately computable from any reasonable parse tree using simple deterministic rules.
</prevsent>
<prevsent>our set does not include the relations direct object or indirect object,since this distinction can not be made deterministically on the basis of syntactic structure alone; instead, we opted to number the noun phrase (np),complement clause (cl, xcl), and adjectival complements (acomp) appearing in an unbroken sequence directly after the verb, since this is sufficient to capture the necessary syntactic information.
</prevsent>
</prevsection>
<citsent citstr=" P03-1054 ">
the syntactic relations used in our experiments are computed from the typed dependencies returned by the stanford parser (klein and manning, 2003).<papid> P03-1054 </papid>we also must choose representation for semantic roles.</citsent>
<aftsection>
<nextsent>we allow each verb small fixed number of roles, in the manner similar to prop banks arg0 . . .
</nextsent>
<nextsent>arg5.
</nextsent>
<nextsent>we also designate single adjunct role which is shared by all verbs,similar to prop banks argm role.
</nextsent>
<nextsent>we say similar?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC591">
<title id=" W06-1601.xml">unsupervised discovery of a statistical verb lexicon </title>
<section> datasets and evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>adeeper problem is that data likelihood may not correspond well to linguists assessment of model quality.
</prevsent>
<prevsent>as evidence that this is not the case, we have observed strong correlation between data log likelihood and labeling accuracy.
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
we train our models with verb instances extracted from three parsed corpora: (1) the wall street journal section of the penn treebank (ptb), which was parsed by human annotators (marcus etal., 1993), (<papid> J93-2004 </papid>2) the brown laboratory for linguistic information processing corpus of wall street journal text (bllip), which was parsed automatically by the charniak parser (charniak, 2000), <papid> A00-2018 </papid>and (3) the gigaword corpus of raw newswire text(gw), which we parsed ourselves with the stanford parser.</citsent>
<aftsection>
<nextsent>in all cases, when training model, coarse roles core roles sec.
</nextsent>
<nextsent>23 r f1 r f1 id only .957 .802 .873 .944 .843 .891 cl only baseline .856 .856 .856 .975 .820 .886 ptb tr.
</nextsent>
<nextsent>.889 .889 .889 .928 .898 .911 1000 tr.
</nextsent>
<nextsent>.897 .897 .897 .947 .898 .920.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC592">
<title id=" W06-1601.xml">unsupervised discovery of a statistical verb lexicon </title>
<section> datasets and evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>adeeper problem is that data likelihood may not correspond well to linguists assessment of model quality.
</prevsent>
<prevsent>as evidence that this is not the case, we have observed strong correlation between data log likelihood and labeling accuracy.
</prevsent>
</prevsection>
<citsent citstr=" A00-2018 ">
we train our models with verb instances extracted from three parsed corpora: (1) the wall street journal section of the penn treebank (ptb), which was parsed by human annotators (marcus etal., 1993), (<papid> J93-2004 </papid>2) the brown laboratory for linguistic information processing corpus of wall street journal text (bllip), which was parsed automatically by the charniak parser (charniak, 2000), <papid> A00-2018 </papid>and (3) the gigaword corpus of raw newswire text(gw), which we parsed ourselves with the stanford parser.</citsent>
<aftsection>
<nextsent>in all cases, when training model, coarse roles core roles sec.
</nextsent>
<nextsent>23 r f1 r f1 id only .957 .802 .873 .944 .843 .891 cl only baseline .856 .856 .856 .975 .820 .886 ptb tr.
</nextsent>
<nextsent>.889 .889 .889 .928 .898 .911 1000 tr.
</nextsent>
<nextsent>.897 .897 .897 .947 .898 .920.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC593">
<title id=" W06-1661.xml">statistical ranking in tactical generation </title>
<section> models.  </section>
<citcontext>
<prevsection>
<prevsent>other choices involve, for example, the optionality of complementizers and relative pronouns, permutation of (intersective) modifiers, and lexical and orthographic alternations.
</prevsent>
<prevsent>2.1 language models.
</prevsent>
</prevsection>
<citsent citstr=" W98-1426 ">
the use of n-gram language models is the most common approach to statistical selection in generation (langkilde &amp; knight, 1998; <papid> W98-1426 </papid>and white (2004); inter alios).</citsent>
<aftsection>
<nextsent>in order to better assert the relative performance of the discriminative models and the structural features we present below, we also apply trigram model to the ranking problem.
</nextsent>
<nextsent>using the freely available cmu slm toolkit (clarkson &amp; rosenfeld, 1997), we trained trigram model on an unannotated version of the british national corpus (bnc), containing roughly 100 million words (using witten-bell discounting and back-off).
</nextsent>
<nextsent>given such model n , the score of realization i with surface form k i1 = (w i1 ; : : : ; ik ) is then computed as (1) (s; i ) = x j=1 n (w i;j jw i;j n ; : : : ; i;j 1 )given the scoring function , the best realization is selected according to the following decision function: (2) r^ = argmax 0 2y(s) (s; 0 ) although in this case scoring is not conditioned on the input semantics at all, we still include it to make the function formulation more general as we will be reusing it later.note that, as the realizations in our symmetric treebank also include punctuation marks, these are also treated as separate tokens by the language model (in addition to pseudo-tokens marking sentence boundaries).
</nextsent>
<nextsent>2.2 maximum entropy models.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC594">
<title id=" W06-1661.xml">statistical ranking in tactical generation </title>
<section> models.  </section>
<citcontext>
<prevsection>
<prevsent>given such model n , the score of realization i with surface form k i1 = (w i1 ; : : : ; ik ) is then computed as (1) (s; i ) = x j=1 n (w i;j jw i;j n ; : : : ; i;j 1 )given the scoring function , the best realization is selected according to the following decision function: (2) r^ = argmax 0 2y(s) (s; 0 ) although in this case scoring is not conditioned on the input semantics at all, we still include it to make the function formulation more general as we will be reusing it later.note that, as the realizations in our symmetric treebank also include punctuation marks, these are also treated as separate tokens by the language model (in addition to pseudo-tokens marking sentence boundaries).
</prevsent>
<prevsent>2.2 maximum entropy models.
</prevsent>
</prevsection>
<citsent citstr=" N04-1021 ">
maximum entropy modeling provides very flexible framework that has been widely used for range of tasks in nlp, including parse selection (e.g. johnson, geman, canon, chi, &amp; riezler, 1999; malouf &amp; noord, 2004) and reranking for machine translation (e.g. och et al, 2004).<papid> N04-1021 </papid></citsent>
<aftsection>
<nextsent>a model is specified by set of real-valued feature functions that describe properties of the data, and an associated set of learned weights that determine the contribution of each feature.
</nextsent>
<nextsent>let us first introduce some notation before we go on.
</nextsent>
<nextsent>let y(s ) = fr 1 ; : : : ; mg be the set of realizations licensed by the grammar for semantic representation i . now, let our (positive) training.
</nextsent>
<nextsent>data be given as p = fx 1 ; : : : ; n where each i is pair (s ; j ) for which j 2 y(s ) and jis annotated in the treebank as being correct realization of i . note that we might have several.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC595">
<title id=" W06-1661.xml">statistical ranking in tactical generation </title>
<section> models.  </section>
<citcontext>
<prevsection>
<prevsent>(s; r) the normalization term w is defined as (5) w (s) = r 0 2y(s) f (s;r) when we want to find the best realization forgiven input semantics according to model w , it is sufficient to compute the score function as in equation (4) and then use the decision function previously given in equation (2) above.
</prevsent>
<prevsent>when it 518comes to estimating1 the parameters w, the procedure seeks to maximize the (log of) penalized likelihood function as in (6) w^ = argmax logl(w)   d i=1 2 2 2 where l(w) is the conditionalized?
</prevsent>
</prevsection>
<citsent citstr=" P99-1069 ">
likelihood of the training data p(johnson et al, 1999), <papid> P99-1069 </papid>computed as l(w) = n i=1 w (r js ).</citsent>
<aftsection>
<nextsent>the second term of the likelihood function in equation (6) is penalty term that is commonly used for reducing the tendency of log-linear models to over-fit, especially when training on sparse data using many features (chen &amp; rosenfeld, 1999; johnson et al, 1999; <papid> P99-1069 </papid>malouf &amp; noord, 2004).</nextsent>
<nextsent>more specifically it defines zero-mean gaussian prior on the feature weights which effectively leads to less extreme values.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC597">
<title id=" W06-1661.xml">statistical ranking in tactical generation </title>
<section> models.  </section>
<citcontext>
<prevsection>
<prevsent>however, instead of maximizing the probability of the preferred or positive realizations, we try to maximize their value for w directly.recall our definition of the set of positive training examples in section 2.2.
</prevsent>
<prevsent>let us here analogously define n = fx 1 ; : : : ; qg to be the negative counterpart, so that forgiven pair = (s ; j ) 2 n , we have that j 2 y(s ) but j is not annotated as preferred realization of i . fol-.
</prevsent>
</prevsection>
<citsent citstr=" W02-2018 ">
1we use the tadm open-source package (malouf, 2002) <papid> W02-2018 </papid>for training the models, using its limited-memory variable metric as the optimization method and experimentally determine the optimal convergence threshold and variance of the prior.</citsent>
<aftsection>
<nextsent>lowing joachims (2002), the goal is to minimize (7) (w; ) = 1 2 w
</nextsent>
<nextsent>w + x  i;j;k subject to the following constraints, 8ijk s.t.
</nextsent>
<nextsent>(s ; i ) 2 p ^ (s ; j ) 2 n :(8) w
</nextsent>
<nextsent>(s ; i )  w
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC598">
<title id=" W06-1661.xml">statistical ranking in tactical generation </title>
<section> datasets and features.  </section>
<citcontext>
<prevsection>
<prevsent>for fur 520 table 2 summarizes the two resulting datasets, which are both comprised of instructional texts on tourist activities, the application domain of the background mt system.
</prevsent>
<prevsent>3.2 feature templates.
</prevsent>
</prevsection>
<citsent citstr=" W02-2030 ">
for the purpose of parse selection, toutanova, manning, shieber, flickinger, &amp; oepen (2002)and toutanova &amp; manning (2002) <papid> W02-2030 </papid>train discriminative log-linear model on the redwood sparse treebank, using features defined over derivation trees with non-terminals representing the construction types and lexical types of the hpsg grammar (see figure 1).</citsent>
<aftsection>
<nextsent>the basic feature set of our maxent realization ranker is defined in the same way (corresponding to the pcfg-s model of toutanova &amp; manning, 2002), <papid> W02-2030 </papid>each feature capturing sub-tree from the derivation limited to depthone.</nextsent>
<nextsent>table 3 shows example features in our max ent and svm models, where the feature template # 1 corresponds to local derivation sub-trees.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC600">
<title id=" W07-0714.xml">labelled dependencies in machine translation evaluation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our dependency based method, in contrast to most popular string-based evaluation metrics, does not unfairly penalize perfectly valid syntactic variations in the translation, and the addition of wordnet provides way to accommodate lexical variation.
</prevsent>
<prevsent>in comparison with other metrics on 16,800 sentences of chinese-english newswire text, our method reaches high correlation with human scores.
</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
since the creation of bleu (papineni et al, 2002) <papid> P02-1040 </papid>and nist (doddington, 2002), the subject of automatic evaluation metrics for mt has been given quite lot of attention.</citsent>
<aftsection>
<nextsent>although widely popular thanks to their speed and efficiency, both bleu and nist have been criticized for inadequate accuracy of evaluation at the segment level (callison-burch et al, 2006).
</nextsent>
<nextsent>as string based-metrics, they are limited to superficial comparison of word sequences between translated sentence and one or more reference sentences, and are unable to accommodate any legitimate grammatical variation when it comes to lexical choices or syntactic structure of the translation, beyond what can be found in the multiple references.
</nextsent>
<nextsent>a natural next step in the field of evaluation was to introduce metrics that would better reflect our human judgement by accepting synonyms in the translated sentence or evaluating the translation on the basis of what syntactic features it shares with the reference.
</nextsent>
<nextsent>our method follows and substantially extends the earlier work of liu and gildea (2005), <papid> W05-0904 </papid>who use syntactic features and un labelled dependencies to evaluate mt quality, outperforming bleu on segment-level correlation with human judgement.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC601">
<title id=" W07-0714.xml">labelled dependencies in machine translation evaluation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>as string based-metrics, they are limited to superficial comparison of word sequences between translated sentence and one or more reference sentences, and are unable to accommodate any legitimate grammatical variation when it comes to lexical choices or syntactic structure of the translation, beyond what can be found in the multiple references.
</prevsent>
<prevsent>a natural next step in the field of evaluation was to introduce metrics that would better reflect our human judgement by accepting synonyms in the translated sentence or evaluating the translation on the basis of what syntactic features it shares with the reference.
</prevsent>
</prevsection>
<citsent citstr=" W05-0904 ">
our method follows and substantially extends the earlier work of liu and gildea (2005), <papid> W05-0904 </papid>who use syntactic features and un labelled dependencies to evaluate mt quality, outperforming bleu on segment-level correlation with human judgement.</citsent>
<aftsection>
<nextsent>dependencies abstract away from the particulars of the surface string (and syntactic tree) realization and provide normalized?
</nextsent>
<nextsent>representation of (some) syntactic variants of given sentence.
</nextsent>
<nextsent>while liu and gildea (2005) <papid> W05-0904 </papid>calculate n-gram matches on non-labelled head-modifier sequences derived by head-extraction rules from syntactic trees, we automatically evaluate the quality of translation by calculating an f-score on labelled dependency structures produced by lexical functional grammar (lfg) parser.</nextsent>
<nextsent>these dependencies differ from those used by liu and gildea (2005), <papid> W05-0904 </papid>in that they are extracted according to the rules of the lfg grammar and they are labelled with type of grammatical relation that connects the head and the modifier, such as subject, determiner, etc. the presence of grammatical relation labels adds another layer of important linguistic information into the comparison and allows us to account for partial matches, for example when lexical item finds itself in correct relation but with an incorrect partner.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC607">
<title id=" W07-0714.xml">labelled dependencies in machine translation evaluation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these dependencies differ from those used by liu and gildea (2005), <papid> W05-0904 </papid>in that they are extracted according to the rules of the lfg grammar and they are labelled with type of grammatical relation that connects the head and the modifier, such as subject, determiner, etc. the presence of grammatical relation labels adds another layer of important linguistic information into the comparison and allows us to account for partial matches, for example when lexical item finds itself in correct relation but with an incorrect partner.</prevsent>
<prevsent>moreover, we use number of best parses for the translation and the reference, which serves to decrease the amount of noise that can be introduced by the process of parsing and extracting dependency information.</prevsent>
</prevsection>
<citsent citstr=" P04-1041 ">
the translation and reference files are analyzed by treebank-based, probabilistic lfg parser (cahill et al, 2004), <papid> P04-1041 </papid>which produces set of dependency triples for each input.</citsent>
<aftsection>
<nextsent>the translation set is compared to the reference set, and the number of matches is calculated, giving the 104 precision, recall, and f-score for each particular translation.
</nextsent>
<nextsent>in addition, to allow for the possibility of valid lexical differences between the translation and the references, we follow kauchak and barzilay (2006) <papid> N06-1058 </papid>in adding number of synonyms in the process of evaluation to raise the number of matches between the translation and the reference, leading to higher score.</nextsent>
<nextsent>in an experiment on 16,800 sentences of chinese-english newswire text with segment-level human evaluation from the linguistic data consortiums (ldc) multiple translation project, we compare the lfg-based evaluation method with other popular metrics like bleu, nist, general text matcher (gtm) (turian et al, 2003), translation error rate (ter) (snover et al, 2006)1, and meteor (banerjee and lavie, 2005), <papid> W05-0909 </papid>and we show that combining dependency representations with synonyms leads to more accurate evaluation that correlates better with human judgment.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC608">
<title id=" W07-0714.xml">labelled dependencies in machine translation evaluation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the translation and reference files are analyzed by treebank-based, probabilistic lfg parser (cahill et al, 2004), <papid> P04-1041 </papid>which produces set of dependency triples for each input.</prevsent>
<prevsent>the translation set is compared to the reference set, and the number of matches is calculated, giving the 104 precision, recall, and f-score for each particular translation.</prevsent>
</prevsection>
<citsent citstr=" N06-1058 ">
in addition, to allow for the possibility of valid lexical differences between the translation and the references, we follow kauchak and barzilay (2006) <papid> N06-1058 </papid>in adding number of synonyms in the process of evaluation to raise the number of matches between the translation and the reference, leading to higher score.</citsent>
<aftsection>
<nextsent>in an experiment on 16,800 sentences of chinese-english newswire text with segment-level human evaluation from the linguistic data consortiums (ldc) multiple translation project, we compare the lfg-based evaluation method with other popular metrics like bleu, nist, general text matcher (gtm) (turian et al, 2003), translation error rate (ter) (snover et al, 2006)1, and meteor (banerjee and lavie, 2005), <papid> W05-0909 </papid>and we show that combining dependency representations with synonyms leads to more accurate evaluation that correlates better with human judgment.</nextsent>
<nextsent>although evaluated on different test set, our method also outperforms the correlation with human scores reported in liu and gildea (2005).<papid> W05-0904 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC611">
<title id=" W07-0714.xml">labelled dependencies in machine translation evaluation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the translation set is compared to the reference set, and the number of matches is calculated, giving the 104 precision, recall, and f-score for each particular translation.
</prevsent>
<prevsent>in addition, to allow for the possibility of valid lexical differences between the translation and the references, we follow kauchak and barzilay (2006) <papid> N06-1058 </papid>in adding number of synonyms in the process of evaluation to raise the number of matches between the translation and the reference, leading to higher score.</prevsent>
</prevsection>
<citsent citstr=" W05-0909 ">
in an experiment on 16,800 sentences of chinese-english newswire text with segment-level human evaluation from the linguistic data consortiums (ldc) multiple translation project, we compare the lfg-based evaluation method with other popular metrics like bleu, nist, general text matcher (gtm) (turian et al, 2003), translation error rate (ter) (snover et al, 2006)1, and meteor (banerjee and lavie, 2005), <papid> W05-0909 </papid>and we show that combining dependency representations with synonyms leads to more accurate evaluation that correlates better with human judgment.</citsent>
<aftsection>
<nextsent>although evaluated on different test set, our method also outperforms the correlation with human scores reported in liu and gildea (2005).<papid> W05-0904 </papid></nextsent>
<nextsent>the remainder of this paper is organized as follows: section 2 gives basic introduction to lfg; section 3 describes related work; section 4 describes our method and gives results of the experiment on the multiple translation data; section 5 discusses ongoing work; section 6 concludes.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC618">
<title id=" W07-0714.xml">labelled dependencies in machine translation evaluation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>recently number of attempts to remedy these shortcomings have led to the development of other automatic mt evaluation metrics.
</prevsent>
<prevsent>some of them concentrate mainly on word order, like general text matcher (turian et al, 2003), which calculates precision and recall for translation reference pairs, weighting contiguous matches more than non-sequential matches, or translation error rate (snover et al, 2006), which computes the number of substitutions, insertions, deletions, and shifts necessary to transform the translation text to match the reference.
</prevsent>
</prevsection>
<citsent citstr=" E06-1031 ">
others try to accommodate both syntactic and lexical differences between the candidate translation and the reference, like cder (leusch et al, 2006), <papid> E06-1031 </papid>which employs version of edit distance for word substitution and reordering; or meteor (banerjee and lavie, 2005), <papid> W05-0909 </papid>which uses stemming and wordnet synonymy.</citsent>
<aftsection>
<nextsent>kauchak and barzilay (2006) <papid> N06-1058 </papid>and owczarzak et al (2006) <papid> W06-3112 </papid>use paraphrases during bleu and nist evaluation to increase the number of matches between the translation and the reference; the paraphrases are either taken from wordnet4 in kauchak and barzilay (2006) <papid> N06-1058 </papid>or derived from the test set itself through automatic word and phrase alignment in owczarzak et al (2006).<papid> W06-3112 </papid></nextsent>
<nextsent>another metric making use of synonyms is the linear regression model developed by russo-lassner et al (2005), which makes use of stemming, wordnet synonymy, verb class synonymy, matching noun phrase heads, and proper name matching.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC624">
<title id=" W07-0714.xml">labelled dependencies in machine translation evaluation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>some of them concentrate mainly on word order, like general text matcher (turian et al, 2003), which calculates precision and recall for translation reference pairs, weighting contiguous matches more than non-sequential matches, or translation error rate (snover et al, 2006), which computes the number of substitutions, insertions, deletions, and shifts necessary to transform the translation text to match the reference.
</prevsent>
<prevsent>others try to accommodate both syntactic and lexical differences between the candidate translation and the reference, like cder (leusch et al, 2006), <papid> E06-1031 </papid>which employs version of edit distance for word substitution and reordering; or meteor (banerjee and lavie, 2005), <papid> W05-0909 </papid>which uses stemming and wordnet synonymy.</prevsent>
</prevsection>
<citsent citstr=" W06-3112 ">
kauchak and barzilay (2006) <papid> N06-1058 </papid>and owczarzak et al (2006) <papid> W06-3112 </papid>use paraphrases during bleu and nist evaluation to increase the number of matches between the translation and the reference; the paraphrases are either taken from wordnet4 in kauchak and barzilay (2006) <papid> N06-1058 </papid>or derived from the test set itself through automatic word and phrase alignment in owczarzak et al (2006).<papid> W06-3112 </papid></citsent>
<aftsection>
<nextsent>another metric making use of synonyms is the linear regression model developed by russo-lassner et al (2005), which makes use of stemming, wordnet synonymy, verb class synonymy, matching noun phrase heads, and proper name matching.
</nextsent>
<nextsent>kulesza and shieber (2004), on the other hand, train support vector machine using features such as proportion of gram matches and word error rate to judge given translations distance from human-level quality.
</nextsent>
<nextsent>3.2 dependency-based metric.
</nextsent>
<nextsent>the metrics described above use only string-based comparisons, even while taking into consideration reordering.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC641">
<title id=" W06-3203.xml">learning quantity insensitive stress systems via local inference </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for example, latin is quantity-sensitive (qs) because stress assignment depends on the syllable type: if the penultimate syllable is heavy (i.e. has long vowel or coda) then it receives stress, but otherwise the antepenult does.
</prevsent>
<prevsent>the stress systems under consideration here, unlike latin, do not distinguish syllable types.
</prevsent>
</prevsection>
<citsent citstr=" W97-1107 ">
1in this respect, this work departs from (or is special case of) gradient phonotactic models (coleman and pierrehumbert, 1997; <papid> W97-1107 </papid>frisch et al, 2000; albright, 2006; hayes and wilson, 2006) 21 there are 27 types of qi stress systems found in gordons (2002) typology.</citsent>
<aftsection>
<nextsent>gordon adds six plausibly at testable qi systems by considering the behavior of all-light-syllabled words from qs systems.these 33 patterns are divided into four kinds: single, dual, binary and ternary.
</nextsent>
<nextsent>single systems have one stressed syllable per word, and dual systems upto two.
</nextsent>
<nextsent>binary and ternary systems stress every second (binary) or third (ternary) syllable.
</nextsent>
<nextsent>the choice to study qi stress systems was made for three reasons.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC642">
<title id=" W06-3203.xml">learning quantity insensitive stress systems via local inference </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a finite state machine is 5-tuple (?, q, 0 , f, ?) where ? is finite alphabet, is set of states, 0 ? is the start state,f ? is set of final states, and ? is set of transitions.
</prevsent>
<prevsent>each transition has an origin and terminus and is labeled with symbol of the alphabet; i.e. transition is 3-tuple (o, a, t) where o, ? and ? ?.
</prevsent>
</prevsection>
<citsent citstr=" J94-3001 ">
empirically, it has been observed that most phonological phenomena are regular (johnson, 1972; kaplan and kay, 1981; kaplan and kay, 1994; <papid> J94-3001 </papid>ellison, 1994; eisner, 1997; karttunen, 1998).<papid> W98-1301 </papid></citsent>
<aftsection>
<nextsent>this is especially true of phonotactics: reduplication and metathesis, which have higher complexity, are not phonotactic patterns as they involve alternations.3 formally, regular languages are widely studied in computer science, and their basic properties are well understood (hopcroft et al, 2001).
</nextsent>
<nextsent>also, learning literature exists.
</nextsent>
<nextsent>e.g. the class of regular languages is not exactly identifiable in the limit (gold, 1967), but certain subsets of it are (angluin, 1980; angluin, 1982).
</nextsent>
<nextsent>thus it is becomes possible to ask: what subset of the regular languages deli mits the class of possible human phonotactics and can properties of this class be exploited by learner this perspective also connects to finite state models of optimality theory (ot) (prince and smolensky, 1993).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC643">
<title id=" W06-3203.xml">learning quantity insensitive stress systems via local inference </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a finite state machine is 5-tuple (?, q, 0 , f, ?) where ? is finite alphabet, is set of states, 0 ? is the start state,f ? is set of final states, and ? is set of transitions.
</prevsent>
<prevsent>each transition has an origin and terminus and is labeled with symbol of the alphabet; i.e. transition is 3-tuple (o, a, t) where o, ? and ? ?.
</prevsent>
</prevsection>
<citsent citstr=" W98-1301 ">
empirically, it has been observed that most phonological phenomena are regular (johnson, 1972; kaplan and kay, 1981; kaplan and kay, 1994; <papid> J94-3001 </papid>ellison, 1994; eisner, 1997; karttunen, 1998).<papid> W98-1301 </papid></citsent>
<aftsection>
<nextsent>this is especially true of phonotactics: reduplication and metathesis, which have higher complexity, are not phonotactic patterns as they involve alternations.3 formally, regular languages are widely studied in computer science, and their basic properties are well understood (hopcroft et al, 2001).
</nextsent>
<nextsent>also, learning literature exists.
</nextsent>
<nextsent>e.g. the class of regular languages is not exactly identifiable in the limit (gold, 1967), but certain subsets of it are (angluin, 1980; angluin, 1982).
</nextsent>
<nextsent>thus it is becomes possible to ask: what subset of the regular languages deli mits the class of possible human phonotactics and can properties of this class be exploited by learner this perspective also connects to finite state models of optimality theory (ot) (prince and smolensky, 1993).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC644">
<title id=" W06-1623.xml">inducing temporal graphs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>understanding the temporal flow of discourse isa significant aspect of text comprehension.
</prevsent>
<prevsent>consequently, temporal analysis has been focus of linguistic research for quite some time.
</prevsent>
</prevsection>
<citsent citstr=" P87-1001 ">
temporal interpretation encompasses levels ranging from the syntactic to the lexico-semantic (re ichenbach, 1947; moens and steedman, 1987)<papid> P87-1001 </papid>and includes the characterization of temporal discourse in terms of rhetorical structure and pragmatic relations (dowty, 1986; webber, 1987; <papid> P87-1021 </papid>passonneau, 1988; <papid> J88-2005 </papid>lascarides and asher, 1993).</citsent>
<aftsection>
<nextsent>besides its linguistic significance, temporal analysis has important practical implications.
</nextsent>
<nextsent>in multi document summarization, knowledge about the temporal order of events can enhance both the content selection and the summary generation processes (barzilay et al, 2002).
</nextsent>
<nextsent>in question answering, temporal analysis is needed to determine when particular event occurs and how events relate to each other.
</nextsent>
<nextsent>some of these needs can be addressed by emerging technologies for temporal analysis (wilson et al, 2001; <papid> W01-1312 </papid>mani et al, 2003; <papid> N03-2019 </papid>lapata and lascarides, 2004; <papid> N04-1020 </papid>boguraev and ando, 2005).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC645">
<title id=" W06-1623.xml">inducing temporal graphs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>understanding the temporal flow of discourse isa significant aspect of text comprehension.
</prevsent>
<prevsent>consequently, temporal analysis has been focus of linguistic research for quite some time.
</prevsent>
</prevsection>
<citsent citstr=" P87-1021 ">
temporal interpretation encompasses levels ranging from the syntactic to the lexico-semantic (re ichenbach, 1947; moens and steedman, 1987)<papid> P87-1001 </papid>and includes the characterization of temporal discourse in terms of rhetorical structure and pragmatic relations (dowty, 1986; webber, 1987; <papid> P87-1021 </papid>passonneau, 1988; <papid> J88-2005 </papid>lascarides and asher, 1993).</citsent>
<aftsection>
<nextsent>besides its linguistic significance, temporal analysis has important practical implications.
</nextsent>
<nextsent>in multi document summarization, knowledge about the temporal order of events can enhance both the content selection and the summary generation processes (barzilay et al, 2002).
</nextsent>
<nextsent>in question answering, temporal analysis is needed to determine when particular event occurs and how events relate to each other.
</nextsent>
<nextsent>some of these needs can be addressed by emerging technologies for temporal analysis (wilson et al, 2001; <papid> W01-1312 </papid>mani et al, 2003; <papid> N03-2019 </papid>lapata and lascarides, 2004; <papid> N04-1020 </papid>boguraev and ando, 2005).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC646">
<title id=" W06-1623.xml">inducing temporal graphs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>understanding the temporal flow of discourse isa significant aspect of text comprehension.
</prevsent>
<prevsent>consequently, temporal analysis has been focus of linguistic research for quite some time.
</prevsent>
</prevsection>
<citsent citstr=" J88-2005 ">
temporal interpretation encompasses levels ranging from the syntactic to the lexico-semantic (re ichenbach, 1947; moens and steedman, 1987)<papid> P87-1001 </papid>and includes the characterization of temporal discourse in terms of rhetorical structure and pragmatic relations (dowty, 1986; webber, 1987; <papid> P87-1021 </papid>passonneau, 1988; <papid> J88-2005 </papid>lascarides and asher, 1993).</citsent>
<aftsection>
<nextsent>besides its linguistic significance, temporal analysis has important practical implications.
</nextsent>
<nextsent>in multi document summarization, knowledge about the temporal order of events can enhance both the content selection and the summary generation processes (barzilay et al, 2002).
</nextsent>
<nextsent>in question answering, temporal analysis is needed to determine when particular event occurs and how events relate to each other.
</nextsent>
<nextsent>some of these needs can be addressed by emerging technologies for temporal analysis (wilson et al, 2001; <papid> W01-1312 </papid>mani et al, 2003; <papid> N03-2019 </papid>lapata and lascarides, 2004; <papid> N04-1020 </papid>boguraev and ando, 2005).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC647">
<title id=" W06-1623.xml">inducing temporal graphs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in multi document summarization, knowledge about the temporal order of events can enhance both the content selection and the summary generation processes (barzilay et al, 2002).
</prevsent>
<prevsent>in question answering, temporal analysis is needed to determine when particular event occurs and how events relate to each other.
</prevsent>
</prevsection>
<citsent citstr=" W01-1312 ">
some of these needs can be addressed by emerging technologies for temporal analysis (wilson et al, 2001; <papid> W01-1312 </papid>mani et al, 2003; <papid> N03-2019 </papid>lapata and lascarides, 2004; <papid> N04-1020 </papid>boguraev and ando, 2005).</citsent>
<aftsection>
<nextsent>this paper characterizes the temporal flow of discourse in terms of temporal segments and their ordering.
</nextsent>
<nextsent>we define temporal segment to be fragment of text that does not exhibit abrupt changes in temporal focus (webber, 1988).<papid> J88-2006 </papid></nextsent>
<nextsent>a segment may contain more than one event or state, but the key requirement is that its elements maintain temporal coherence.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC648">
<title id=" W06-1623.xml">inducing temporal graphs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in multi document summarization, knowledge about the temporal order of events can enhance both the content selection and the summary generation processes (barzilay et al, 2002).
</prevsent>
<prevsent>in question answering, temporal analysis is needed to determine when particular event occurs and how events relate to each other.
</prevsent>
</prevsection>
<citsent citstr=" N03-2019 ">
some of these needs can be addressed by emerging technologies for temporal analysis (wilson et al, 2001; <papid> W01-1312 </papid>mani et al, 2003; <papid> N03-2019 </papid>lapata and lascarides, 2004; <papid> N04-1020 </papid>boguraev and ando, 2005).</citsent>
<aftsection>
<nextsent>this paper characterizes the temporal flow of discourse in terms of temporal segments and their ordering.
</nextsent>
<nextsent>we define temporal segment to be fragment of text that does not exhibit abrupt changes in temporal focus (webber, 1988).<papid> J88-2006 </papid></nextsent>
<nextsent>a segment may contain more than one event or state, but the key requirement is that its elements maintain temporal coherence.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC651">
<title id=" W06-1623.xml">inducing temporal graphs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in multi document summarization, knowledge about the temporal order of events can enhance both the content selection and the summary generation processes (barzilay et al, 2002).
</prevsent>
<prevsent>in question answering, temporal analysis is needed to determine when particular event occurs and how events relate to each other.
</prevsent>
</prevsection>
<citsent citstr=" N04-1020 ">
some of these needs can be addressed by emerging technologies for temporal analysis (wilson et al, 2001; <papid> W01-1312 </papid>mani et al, 2003; <papid> N03-2019 </papid>lapata and lascarides, 2004; <papid> N04-1020 </papid>boguraev and ando, 2005).</citsent>
<aftsection>
<nextsent>this paper characterizes the temporal flow of discourse in terms of temporal segments and their ordering.
</nextsent>
<nextsent>we define temporal segment to be fragment of text that does not exhibit abrupt changes in temporal focus (webber, 1988).<papid> J88-2006 </papid></nextsent>
<nextsent>a segment may contain more than one event or state, but the key requirement is that its elements maintain temporal coherence.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC655">
<title id=" W06-1623.xml">inducing temporal graphs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>some of these needs can be addressed by emerging technologies for temporal analysis (wilson et al, 2001; <papid> W01-1312 </papid>mani et al, 2003; <papid> N03-2019 </papid>lapata and lascarides, 2004; <papid> N04-1020 </papid>boguraev and ando, 2005).</prevsent>
<prevsent>this paper characterizes the temporal flow of discourse in terms of temporal segments and their ordering.</prevsent>
</prevsection>
<citsent citstr=" J88-2006 ">
we define temporal segment to be fragment of text that does not exhibit abrupt changes in temporal focus (webber, 1988).<papid> J88-2006 </papid></citsent>
<aftsection>
<nextsent>a segment may contain more than one event or state, but the key requirement is that its elements maintain temporal coherence.
</nextsent>
<nextsent>for instance, medical case summary may contain segments describing patients admission, his previous hospital visit, and the onset of his original symptoms.
</nextsent>
<nextsent>each of these segments corresponds to different timeframe, and is clearly delineated as such in text.
</nextsent>
<nextsent>our ultimate goal is to automatically construct graph that encodes ordering between temporal segments.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC658">
<title id=" W06-1623.xml">inducing temporal graphs </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in contrast to existing methods that focus on pairwise ordering, we explore strategies for global temporal inference.the strength of the proposed model lies in its ability to simultaneously optimize pairwise ordering preferences and global constraints on graph topology.
</prevsent>
<prevsent>while the algorithm has been applied at the segment level, it can be used with other temporal annotation schemes.
</prevsent>
</prevsection>
<citsent citstr=" P92-1030 ">
temporal ordering has been extensively studied in computational linguistics (passonneau, 1988;<papid> J88-2005 </papid>webber, 1988; <papid> J88-2006 </papid>hwang and schubert, 1992; <papid> P92-1030 </papid>lascarides and asher, 1993; lascarides and oberlander, 1993).<papid> E93-1031 </papid></citsent>
<aftsection>
<nextsent>prior research has investigateda variety of language mechanisms and knowledge sources that guide interpretation of temporal ordering, including tense, aspect, temporal adverbials, rhetorical relations and pragmatic constraints.
</nextsent>
<nextsent>in recent years, the availability of annotated corpora, such as time bank (pustejovskyet al, 2003), has triggered the use of machine learning methods for temporal analysis (mani et al., 2003; <papid> N03-2019 </papid>lapata and lascarides, 2004; <papid> N04-1020 </papid>boguraev and ando, 2005).</nextsent>
<nextsent>typical tasks include identification of temporal anchors, linking events to times, and temporal ordering of events.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC659">
<title id=" W06-1623.xml">inducing temporal graphs </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in contrast to existing methods that focus on pairwise ordering, we explore strategies for global temporal inference.the strength of the proposed model lies in its ability to simultaneously optimize pairwise ordering preferences and global constraints on graph topology.
</prevsent>
<prevsent>while the algorithm has been applied at the segment level, it can be used with other temporal annotation schemes.
</prevsent>
</prevsection>
<citsent citstr=" E93-1031 ">
temporal ordering has been extensively studied in computational linguistics (passonneau, 1988;<papid> J88-2005 </papid>webber, 1988; <papid> J88-2006 </papid>hwang and schubert, 1992; <papid> P92-1030 </papid>lascarides and asher, 1993; lascarides and oberlander, 1993).<papid> E93-1031 </papid></citsent>
<aftsection>
<nextsent>prior research has investigateda variety of language mechanisms and knowledge sources that guide interpretation of temporal ordering, including tense, aspect, temporal adverbials, rhetorical relations and pragmatic constraints.
</nextsent>
<nextsent>in recent years, the availability of annotated corpora, such as time bank (pustejovskyet al, 2003), has triggered the use of machine learning methods for temporal analysis (mani et al., 2003; <papid> N03-2019 </papid>lapata and lascarides, 2004; <papid> N04-1020 </papid>boguraev and ando, 2005).</nextsent>
<nextsent>typical tasks include identification of temporal anchors, linking events to times, and temporal ordering of events.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC665">
<title id=" W06-1623.xml">inducing temporal graphs </title>
<section> method for temporal segmentation.  </section>
<citcontext>
<prevsection>
<prevsent>to implement this approach, we first identify set of potential boundaries.
</prevsent>
<prevsent>our analysis of the manually-annotated corpus reveals that boundaries can occur not only between sentences, but also within sentence, at the boundary of syntactic clauses.
</prevsent>
</prevsection>
<citsent citstr=" A00-2018 ">
we automatically segment sentences into clauses using robust statistical parser (charniak, 2000).<papid> A00-2018 </papid></citsent>
<aftsection>
<nextsent>next, we encode each boundary as vector of features.
</nextsent>
<nextsent>given set of annotated examples, we train classifier2 to predict boundaries based on the following feature set: lexical features temporal expressions, such as tomorrow and earlier, are among the strongest markers of temporal discontinuity (passonneau, 1988;<papid> J88-2005 </papid> bestgen and vonk, 1995).</nextsent>
<nextsent>in addition toa well-studied set of domain-independent temporal markers, there are variety of domain-specifictemporal markers.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC667">
<title id=" W06-1623.xml">inducing temporal graphs </title>
<section> method for temporal segmentation.  </section>
<citcontext>
<prevsection>
<prevsent>191identifying such transitions is relevant for temporal segmentation.
</prevsent>
<prevsent>we quantify the strength of topic chang eby computing cosine similarity between sentences bordering the proposed segmentation.
</prevsent>
</prevsection>
<citsent citstr=" P94-1002 ">
this measure is commonly used in topic segmentation (hearst, 1994) <papid> P94-1002 </papid>under the assumption that change in lexical distribution corresponds to topical change.positional features some parts of the document are more likely to exhibit temporal change than others.</citsent>
<aftsection>
<nextsent>this property is related to patterns in discourse organization of document as whole.for instance, medical case summary first discusses various developments in the medical history of patient and then focuses on his currentconditions.
</nextsent>
<nextsent>as result, the first part of the summary contains many short temporal segments.
</nextsent>
<nextsent>we encode positional features by recording the relative position of sentence in document.
</nextsent>
<nextsent>syntactic features because our segment boundaries are considered at the clausal level,rather than at the sentence level, the syntax surrounding hypothesized boundary may be indicative of temporal shifts.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC683">
<title id=" W06-1710.xml">web corpus mining by instance of wikipedia </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>consequently, corpus analysis requires, amongst others, comparison of occurrences in given text with typical occurrences in other texts of the same genre (stubbs, 2001, p. 120).this raises the question how to judge the membership of texts, in which occurrences of linguistic items are observed, to the genres involved.
</prevsent>
<prevsent>evidently, because of the size of the corpora involved,this question is only adequately answered by reference to the area of automatic classification.
</prevsent>
</prevsection>
<citsent citstr=" J03-3001 ">
this holds all the more for web corpus linguistics (kilgarriff and grefenstette, 2003; <papid> J03-3001 </papid>baroni and bernar dini, 2006) where large corpora of web pages,whose membership in web genres is presently unknown, have to be analyzed.</citsent>
<aftsection>
<nextsent>consequently, web corpus linguistics faces two related task: 1.
</nextsent>
<nextsent>exploration: the task of initially exploring.
</nextsent>
<nextsent>which web genres actually exist.
</nextsent>
<nextsent>per textual units according to their member ship in the genres being explored in the latter step.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC684">
<title id=" W06-1419.xml">evaluations of nlg systems common corpus and tasks or common dimensions and metrics </title>
<section> usability evaluations of nlg systems.  </section>
<citcontext>
<prevsection>
<prevsent>in asking these questions, we believe it is also useful to decouple specific system and its underlying architecture, and ask the appropriate questions to both.
</prevsent>
<prevsent>when talking about evaluation of nlg systems, we should also remember that usability evaluations are crucial, as they can confirm the usefulness of system for its purpose and look at the impact of the generated text on its intended audience.
</prevsent>
</prevsection>
<citsent citstr=" P01-1057 ">
there has been an increasing number of such evaluations ? e.g., (reiter et al, 2001, <papid> P01-1057 </papid>paris et al, 2001, colineau et al, 2002, kushniruk et al, 2002, elhadad et al, 2005) ? and we should continue to encourage them as well as develop and share methodologies (and pitfalls) for performing these evaluations.</citsent>
<aftsection>
<nextsent>it is interesting, in fact, to note that communities that have emphasized common task and corpus evaluations, such as their community, are now turning their attention to stakeholder-based evaluations such as task-based evaluations.
</nextsent>
<nextsent>in looking at ways to evaluate nlg systems, we might again enlarge our view beyond reader/listener-oriented usability evaluations, as readers are not the only persons potentially affected by our technology.
</nextsent>
<nextsent>when doing our evaluations, then, we must also consider other parties.
</nextsent>
<nextsent>considering nlg systems as information systems, we might consider the following stakeholders beyond the reader: ? the creators of the information: for some applications, this may refer to the person creating the resources or the information required for the nlg system.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC685">
<title id=" W06-1404.xml">over generation and ranking for spoken dialogue systems </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we find that over generation basedon bottom-up chart generation is well suited to a) model phenomena such as alignment and variation in dialogue, and b)address robustness issues in the face of imperfect generation input.
</prevsent>
<prevsent>we report evaluation results of first user study involving 20 subjects.
</prevsent>
</prevsection>
<citsent citstr=" C02-1005 ">
over generation and ranking approaches have become increasingly popular in recent years (langk ilde, 2002; varges, 2002).<papid> C02-1005 </papid></citsent>
<aftsection>
<nextsent>however, most work on generation for practical dialogue systems makes use of generation components that work toward single output, often using simple templates.
</nextsent>
<nextsent>inthe following, we first describe our dialogue system and then turn to the generator which is based on the over generation and ranking paradigm.
</nextsent>
<nextsent>we outline the results of user study, followed by discussion section.
</nextsent>
<nextsent>the dialogue system: dialogue processing starts with the output of speech recognizer (nuance) which is analyzed by both statistical dependency parser and topic classifier.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC687">
<title id=" W06-1404.xml">over generation and ranking for spoken dialogue systems </title>
<section> over generation for spoken dialogue.  </section>
<citcontext>
<prevsection>
<prevsent>the highest-ranked candidate is selected for output.
</prevsent>
<prevsent>2.1 chart generation.
</prevsent>
</prevsection>
<citsent citstr=" P96-1027 ">
we follow bottom-up chart generation approach (kay, 1996) <papid> P96-1027 </papid>for production systems similar to(varges, 2005).<papid> W05-1626 </papid></citsent>
<aftsection>
<nextsent>the rule-based core of the generator is set of productions written in production system.
</nextsent>
<nextsent>productions map individual database constraints to phrases such as open for lunch?,within 3 miles?, formal dress code?, andre cursively combine them into nps.
</nextsent>
<nextsent>this includes the use of coordination to produce restaurants with 5-star rating and formal dress code?,for example.
</nextsent>
<nextsent>the nps are integrated into sentence templates, several of which can be combined 20 |result| mod example realization fexp s1 0 no im sorry but found no restaurants on mayfield road that serve mediterranean food . 0 s2 small: no there are 2 cheap thai restaurants in lincoln in my database : thai mee choke and 61   0,   t1 noodle house . s3 medium: no found 9 restaurants with two star rating and formal dress code that are open 212  = t1,   t2 for dinner and serve french food . here are the first ones : s4 large: no found 258 restaurants on page mill road, for example maya restaurant , 300  = t2 green frog and pho hoa restaurant . would you like to try searching by cuisine ? s5 large yes found no restaurants that ...
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC688">
<title id=" W06-1404.xml">over generation and ranking for spoken dialogue systems </title>
<section> over generation for spoken dialogue.  </section>
<citcontext>
<prevsection>
<prevsent>the highest-ranked candidate is selected for output.
</prevsent>
<prevsent>2.1 chart generation.
</prevsent>
</prevsection>
<citsent citstr=" W05-1626 ">
we follow bottom-up chart generation approach (kay, 1996) <papid> P96-1027 </papid>for production systems similar to(varges, 2005).<papid> W05-1626 </papid></citsent>
<aftsection>
<nextsent>the rule-based core of the generator is set of productions written in production system.
</nextsent>
<nextsent>productions map individual database constraints to phrases such as open for lunch?,within 3 miles?, formal dress code?, andre cursively combine them into nps.
</nextsent>
<nextsent>this includes the use of coordination to produce restaurants with 5-star rating and formal dress code?,for example.
</nextsent>
<nextsent>the nps are integrated into sentence templates, several of which can be combined 20 |result| mod example realization fexp s1 0 no im sorry but found no restaurants on mayfield road that serve mediterranean food . 0 s2 small: no there are 2 cheap thai restaurants in lincoln in my database : thai mee choke and 61   0,   t1 noodle house . s3 medium: no found 9 restaurants with two star rating and formal dress code that are open 212  = t1,   t2 for dinner and serve french food . here are the first ones : s4 large: no found 258 restaurants on page mill road, for example maya restaurant , 300  = t2 green frog and pho hoa restaurant . would you like to try searching by cuisine ? s5 large yes found no restaurants that ...
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC691">
<title id=" W06-3813.xml">matching syntactic semantic graphs for semantic relation assignment </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the idea resurfaced forcefully at several points in the more recent history of linguistic research (tesnie`re, 1959; gruber, 1965; fillmore, 1968).
</prevsent>
<prevsent>now it has the attention of many researchers in natural language processing, as shown by recent research in semantic parsing and semantic 1the sources date his work variously between the 5th and 7th century.
</prevsent>
</prevsection>
<citsent citstr=" P98-1013 ">
role labelling (baker et al, 1998; <papid> P98-1013 </papid>kipper et al, 2000; carreras and marquez, 2004; carreras and marquez, 2005; atserias et al, 2001; shi and mihalcea, 2005).graph-like structures are natural way of organising ones impressions of text seen from the perspective of connections between its simpler constituents of varying granularity, from sections through paragraphs, sentences, clauses, phrases, words to morphemes.in this work we pursue well-known and of ten tacitly assumed line of thinking: connections atthe syntactic level reflect connections at the semantic level (in other words, syntax carries meaning).</citsent>
<aftsection>
<nextsent>anecdotal support for this stance comes from the fact that the grammatical notion of case is the basis for semantic relations (misra, 1966; gruber, 1965; fillmore, 1968).
</nextsent>
<nextsent>tesnie`re (1959), who proposes agrouping of verb arguments into actants and circumstances, gives set of rules to connect specific types of actants ? for example, agent or instrument ? to such grammatical elements as subject, direct object, indirect object.
</nextsent>
<nextsent>this idea was expanded to include nouns and their modifiers through verb nominaliza tions (chomsky, 1970; quirk et al, 1985).
</nextsent>
<nextsent>we work with sentences, clauses, phrases and words, using syntactic structures generated by parser.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC693">
<title id=" W06-3813.xml">matching syntactic semantic graphs for semantic relation assignment </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>discussion and conclusions appear in section 6.
</prevsent>
<prevsent>some methods of semantic relation analysis relyon predefined templates filled with information from processed texts (baker et al, 1998).<papid> P98-1013 </papid></prevsent>
</prevsection>
<citsent citstr=" W01-0511 ">
in other methods, lexical resources are specifically tailored to meet the requirements of the domain (rosario and hearst, 2001) <papid> W01-0511 </papid>or the system (gomez, 1998).</citsent>
<aftsection>
<nextsent>such systems extract information from some types of syntactic units (clauses in (fillmore and atkins, 1998; gildea and jurafsky, 2002; <papid> J02-3001 </papid>hull and gomez, 1996); noun phrases in (hull and gomez, 1996; rosario et al., 2002)).<papid> P02-1032 </papid></nextsent>
<nextsent>lists of semantic relations are designed to capture salient domain information.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC694">
<title id=" W06-3813.xml">matching syntactic semantic graphs for semantic relation assignment </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>some methods of semantic relation analysis relyon predefined templates filled with information from processed texts (baker et al, 1998).<papid> P98-1013 </papid></prevsent>
<prevsent>in other methods, lexical resources are specifically tailored to meet the requirements of the domain (rosario and hearst, 2001) <papid> W01-0511 </papid>or the system (gomez, 1998).</prevsent>
</prevsection>
<citsent citstr=" J02-3001 ">
such systems extract information from some types of syntactic units (clauses in (fillmore and atkins, 1998; gildea and jurafsky, 2002; <papid> J02-3001 </papid>hull and gomez, 1996); noun phrases in (hull and gomez, 1996; rosario et al., 2002)).<papid> P02-1032 </papid></citsent>
<aftsection>
<nextsent>lists of semantic relations are designed to capture salient domain information.
</nextsent>
<nextsent>in the rapid knowledge formation project (rkf) support system was developed for domain experts.
</nextsent>
<nextsent>it helps them build complex knowledge bases by combining components: events, entities and modifiers (clark and porter, 1997).
</nextsent>
<nextsent>the systems interface facilitates the experts task of creating and manipulating structures which represent domain concepts, and assigning them relations from relation dictionary.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC696">
<title id=" W06-3813.xml">matching syntactic semantic graphs for semantic relation assignment </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>some methods of semantic relation analysis relyon predefined templates filled with information from processed texts (baker et al, 1998).<papid> P98-1013 </papid></prevsent>
<prevsent>in other methods, lexical resources are specifically tailored to meet the requirements of the domain (rosario and hearst, 2001) <papid> W01-0511 </papid>or the system (gomez, 1998).</prevsent>
</prevsection>
<citsent citstr=" P02-1032 ">
such systems extract information from some types of syntactic units (clauses in (fillmore and atkins, 1998; gildea and jurafsky, 2002; <papid> J02-3001 </papid>hull and gomez, 1996); noun phrases in (hull and gomez, 1996; rosario et al., 2002)).<papid> P02-1032 </papid></citsent>
<aftsection>
<nextsent>lists of semantic relations are designed to capture salient domain information.
</nextsent>
<nextsent>in the rapid knowledge formation project (rkf) support system was developed for domain experts.
</nextsent>
<nextsent>it helps them build complex knowledge bases by combining components: events, entities and modifiers (clark and porter, 1997).
</nextsent>
<nextsent>the systems interface facilitates the experts task of creating and manipulating structures which represent domain concepts, and assigning them relations from relation dictionary.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC700">
<title id=" W06-3813.xml">matching syntactic semantic graphs for semantic relation assignment </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the systems interface facilitates the experts task of creating and manipulating structures which represent domain concepts, and assigning them relations from relation dictionary.
</prevsent>
<prevsent>in current work on semantic relation analysis, the focus is on semantic roles ? relations between verb sand their arguments.
</prevsent>
</prevsection>
<citsent citstr=" P05-1072 ">
most approaches relyon verb net (kipper et al, 2000) and framenet (baker et al., 1998) <papid> P98-1013 </papid>to provide associations between verbs and semantic roles, that are then mapped onto the current instance, as shown by the systems competing in semantic role labelling competitions (carreras and marquez, 2004; carreras and marquez, 2005) and also (gildea and jurafsky, 2002; <papid> J02-3001 </papid>pradhan et al, 2005; <papid> P05-1072 </papid>shi and mihalcea, 2005).</citsent>
<aftsection>
<nextsent>these systems share two ideas which make them different from the approach presented here: they all analyse verb-argument relations, and they all use machine learning or probabilistic approaches (prad han et al, 2005) <papid> P05-1072 </papid>to assign label to new instance.labelling every instance relies on the same previously encoded knowledge (see (carreras and marquez, 2004; carreras and marquez, 2005) for an overview of the systems in the semantic role labelling competitions from 2004 and 2005).</nextsent>
<nextsent>pradhan et al (2005) <papid> P05-1072 </papid>combine the outputs of multiple parsers to extract reliable syntactic information, which is translated into features for machine learning experiment in assigning semantic roles.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC703">
<title id=" W06-2608.xml">syntagmatic kernels a word sense disambiguation case study </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in computational linguistics, it is usual to deal withsequences: words are sequences of letters and syn tag matic relations are established by sequences ofwords.
</prevsent>
<prevsent>sequences are analyzed to measure morphological similarity, to detect multi words, to represent syntagmatic relations, and so on.
</prevsent>
</prevsection>
<citsent citstr=" P05-1050 ">
hence modeling syntagmatic relations is crucial for wide variety of nlp tasks, such as named entity recognition(gliozzo et al, 2005<papid> P05-1050 </papid>a) and word sense disambiguation (wsd) (strapparava et al, 2004).<papid> W04-0856 </papid>in general, the strategy adopted to model syntag matic relations is to provide bigrams and trigrams of collocated words as features to describe local contexts (yarowsky, 1994), <papid> P94-1013 </papid>and each word is regard edas different instance to classify.</citsent>
<aftsection>
<nextsent>for instance, occurrences of given class of named entities (such as names of persons) can be discriminated in texts by recognizing word patterns in their local contexts.
</nextsent>
<nextsent>for example the token rossi, whenever is preceded by the token prof., often represents the name of person.
</nextsent>
<nextsent>another task that can benefit from modeling this kind of relations is wsd.
</nextsent>
<nextsent>to solve ambiguity it is necessary to analyze syntagmatic relations in the local context of the word to be disambiguated.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC705">
<title id=" W06-2608.xml">syntagmatic kernels a word sense disambiguation case study </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in computational linguistics, it is usual to deal withsequences: words are sequences of letters and syn tag matic relations are established by sequences ofwords.
</prevsent>
<prevsent>sequences are analyzed to measure morphological similarity, to detect multi words, to represent syntagmatic relations, and so on.
</prevsent>
</prevsection>
<citsent citstr=" W04-0856 ">
hence modeling syntagmatic relations is crucial for wide variety of nlp tasks, such as named entity recognition(gliozzo et al, 2005<papid> P05-1050 </papid>a) and word sense disambiguation (wsd) (strapparava et al, 2004).<papid> W04-0856 </papid>in general, the strategy adopted to model syntag matic relations is to provide bigrams and trigrams of collocated words as features to describe local contexts (yarowsky, 1994), <papid> P94-1013 </papid>and each word is regard edas different instance to classify.</citsent>
<aftsection>
<nextsent>for instance, occurrences of given class of named entities (such as names of persons) can be discriminated in texts by recognizing word patterns in their local contexts.
</nextsent>
<nextsent>for example the token rossi, whenever is preceded by the token prof., often represents the name of person.
</nextsent>
<nextsent>another task that can benefit from modeling this kind of relations is wsd.
</nextsent>
<nextsent>to solve ambiguity it is necessary to analyze syntagmatic relations in the local context of the word to be disambiguated.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC707">
<title id=" W06-2608.xml">syntagmatic kernels a word sense disambiguation case study </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in computational linguistics, it is usual to deal withsequences: words are sequences of letters and syn tag matic relations are established by sequences ofwords.
</prevsent>
<prevsent>sequences are analyzed to measure morphological similarity, to detect multi words, to represent syntagmatic relations, and so on.
</prevsent>
</prevsection>
<citsent citstr=" P94-1013 ">
hence modeling syntagmatic relations is crucial for wide variety of nlp tasks, such as named entity recognition(gliozzo et al, 2005<papid> P05-1050 </papid>a) and word sense disambiguation (wsd) (strapparava et al, 2004).<papid> W04-0856 </papid>in general, the strategy adopted to model syntag matic relations is to provide bigrams and trigrams of collocated words as features to describe local contexts (yarowsky, 1994), <papid> P94-1013 </papid>and each word is regard edas different instance to classify.</citsent>
<aftsection>
<nextsent>for instance, occurrences of given class of named entities (such as names of persons) can be discriminated in texts by recognizing word patterns in their local contexts.
</nextsent>
<nextsent>for example the token rossi, whenever is preceded by the token prof., often represents the name of person.
</nextsent>
<nextsent>another task that can benefit from modeling this kind of relations is wsd.
</nextsent>
<nextsent>to solve ambiguity it is necessary to analyze syntagmatic relations in the local context of the word to be disambiguated.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC713">
<title id=" W06-1615.xml">domain adaptation with structural correspondence learning </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we test our technique on part of speech tagging and show performance gains for varying amounts of source and target training data, as well as improvements in target domain parsing accuracy using our improved tagger.
</prevsent>
<prevsent>discriminative learning methods are ubiquitous in natural language processing.
</prevsent>
</prevsection>
<citsent citstr=" W96-0213 ">
discriminative taggers and chunk ers have been the state-of-the-art for more than decade (ratnaparkhi, 1996; <papid> W96-0213 </papid>shaand pereira, 2003).<papid> N03-1028 </papid></citsent>
<aftsection>
<nextsent>furthermore, end-to-end systems like speech recognizers (roark et al, 2004)<papid> P04-1007 </papid>and automatic translators (och, 2003) <papid> P03-1021 </papid>use increasingly sophisticated discriminative models, which generalize well to new data that is drawn from the same distribution as the training data.</nextsent>
<nextsent>however, in many situations we may have source domain with plentiful labeled training data,but we need to process material from target do main with different distribution from the source domain and no labeled data.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC715">
<title id=" W06-1615.xml">domain adaptation with structural correspondence learning </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we test our technique on part of speech tagging and show performance gains for varying amounts of source and target training data, as well as improvements in target domain parsing accuracy using our improved tagger.
</prevsent>
<prevsent>discriminative learning methods are ubiquitous in natural language processing.
</prevsent>
</prevsection>
<citsent citstr=" N03-1028 ">
discriminative taggers and chunk ers have been the state-of-the-art for more than decade (ratnaparkhi, 1996; <papid> W96-0213 </papid>shaand pereira, 2003).<papid> N03-1028 </papid></citsent>
<aftsection>
<nextsent>furthermore, end-to-end systems like speech recognizers (roark et al, 2004)<papid> P04-1007 </papid>and automatic translators (och, 2003) <papid> P03-1021 </papid>use increasingly sophisticated discriminative models, which generalize well to new data that is drawn from the same distribution as the training data.</nextsent>
<nextsent>however, in many situations we may have source domain with plentiful labeled training data,but we need to process material from target do main with different distribution from the source domain and no labeled data.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC716">
<title id=" W06-1615.xml">domain adaptation with structural correspondence learning </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>discriminative learning methods are ubiquitous in natural language processing.
</prevsent>
<prevsent>discriminative taggers and chunk ers have been the state-of-the-art for more than decade (ratnaparkhi, 1996; <papid> W96-0213 </papid>shaand pereira, 2003).<papid> N03-1028 </papid></prevsent>
</prevsection>
<citsent citstr=" P04-1007 ">
furthermore, end-to-end systems like speech recognizers (roark et al, 2004)<papid> P04-1007 </papid>and automatic translators (och, 2003) <papid> P03-1021 </papid>use increasingly sophisticated discriminative models, which generalize well to new data that is drawn from the same distribution as the training data.</citsent>
<aftsection>
<nextsent>however, in many situations we may have source domain with plentiful labeled training data,but we need to process material from target do main with different distribution from the source domain and no labeled data.
</nextsent>
<nextsent>in such cases, we must take steps to adapt model trained on the source domain for use in the target domain (roark and bacchiani, 2003; <papid> N03-1027 </papid>florian et al, 2004; <papid> N04-1001 </papid>chelbaand acero, 2004; <papid> W04-3237 </papid>ando, 2004; <papid> P04-3013 </papid>lease and charniak, 2005; daume?</nextsent>
<nextsent>iii and marcu, 2006).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC717">
<title id=" W06-1615.xml">domain adaptation with structural correspondence learning </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>discriminative learning methods are ubiquitous in natural language processing.
</prevsent>
<prevsent>discriminative taggers and chunk ers have been the state-of-the-art for more than decade (ratnaparkhi, 1996; <papid> W96-0213 </papid>shaand pereira, 2003).<papid> N03-1028 </papid></prevsent>
</prevsection>
<citsent citstr=" P03-1021 ">
furthermore, end-to-end systems like speech recognizers (roark et al, 2004)<papid> P04-1007 </papid>and automatic translators (och, 2003) <papid> P03-1021 </papid>use increasingly sophisticated discriminative models, which generalize well to new data that is drawn from the same distribution as the training data.</citsent>
<aftsection>
<nextsent>however, in many situations we may have source domain with plentiful labeled training data,but we need to process material from target do main with different distribution from the source domain and no labeled data.
</nextsent>
<nextsent>in such cases, we must take steps to adapt model trained on the source domain for use in the target domain (roark and bacchiani, 2003; <papid> N03-1027 </papid>florian et al, 2004; <papid> N04-1001 </papid>chelbaand acero, 2004; <papid> W04-3237 </papid>ando, 2004; <papid> P04-3013 </papid>lease and charniak, 2005; daume?</nextsent>
<nextsent>iii and marcu, 2006).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC718">
<title id=" W06-1615.xml">domain adaptation with structural correspondence learning </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>furthermore, end-to-end systems like speech recognizers (roark et al, 2004)<papid> P04-1007 </papid>and automatic translators (och, 2003) <papid> P03-1021 </papid>use increasingly sophisticated discriminative models, which generalize well to new data that is drawn from the same distribution as the training data.</prevsent>
<prevsent>however, in many situations we may have source domain with plentiful labeled training data,but we need to process material from target do main with different distribution from the source domain and no labeled data.</prevsent>
</prevsection>
<citsent citstr=" N03-1027 ">
in such cases, we must take steps to adapt model trained on the source domain for use in the target domain (roark and bacchiani, 2003; <papid> N03-1027 </papid>florian et al, 2004; <papid> N04-1001 </papid>chelbaand acero, 2004; <papid> W04-3237 </papid>ando, 2004; <papid> P04-3013 </papid>lease and charniak, 2005; daume?</citsent>
<aftsection>
<nextsent>iii and marcu, 2006).
</nextsent>
<nextsent>this work focuses on using unlabeled data from both the source and target domains to learn common feature representation that is meaningful across both domains.
</nextsent>
<nextsent>we hypothesize that discriminative model trained in the source domain using this common feature representation will generalize better to the target domain.
</nextsent>
<nextsent>this representation is learned using method we call structural correspondence learning (scl).the key idea of scl is to identify correspondences among features from different domains by modeling their correlations with pivot features.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC719">
<title id=" W06-1615.xml">domain adaptation with structural correspondence learning </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>furthermore, end-to-end systems like speech recognizers (roark et al, 2004)<papid> P04-1007 </papid>and automatic translators (och, 2003) <papid> P03-1021 </papid>use increasingly sophisticated discriminative models, which generalize well to new data that is drawn from the same distribution as the training data.</prevsent>
<prevsent>however, in many situations we may have source domain with plentiful labeled training data,but we need to process material from target do main with different distribution from the source domain and no labeled data.</prevsent>
</prevsection>
<citsent citstr=" N04-1001 ">
in such cases, we must take steps to adapt model trained on the source domain for use in the target domain (roark and bacchiani, 2003; <papid> N03-1027 </papid>florian et al, 2004; <papid> N04-1001 </papid>chelbaand acero, 2004; <papid> W04-3237 </papid>ando, 2004; <papid> P04-3013 </papid>lease and charniak, 2005; daume?</citsent>
<aftsection>
<nextsent>iii and marcu, 2006).
</nextsent>
<nextsent>this work focuses on using unlabeled data from both the source and target domains to learn common feature representation that is meaningful across both domains.
</nextsent>
<nextsent>we hypothesize that discriminative model trained in the source domain using this common feature representation will generalize better to the target domain.
</nextsent>
<nextsent>this representation is learned using method we call structural correspondence learning (scl).the key idea of scl is to identify correspondences among features from different domains by modeling their correlations with pivot features.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC721">
<title id=" W06-1615.xml">domain adaptation with structural correspondence learning </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>furthermore, end-to-end systems like speech recognizers (roark et al, 2004)<papid> P04-1007 </papid>and automatic translators (och, 2003) <papid> P03-1021 </papid>use increasingly sophisticated discriminative models, which generalize well to new data that is drawn from the same distribution as the training data.</prevsent>
<prevsent>however, in many situations we may have source domain with plentiful labeled training data,but we need to process material from target do main with different distribution from the source domain and no labeled data.</prevsent>
</prevsection>
<citsent citstr=" W04-3237 ">
in such cases, we must take steps to adapt model trained on the source domain for use in the target domain (roark and bacchiani, 2003; <papid> N03-1027 </papid>florian et al, 2004; <papid> N04-1001 </papid>chelbaand acero, 2004; <papid> W04-3237 </papid>ando, 2004; <papid> P04-3013 </papid>lease and charniak, 2005; daume?</citsent>
<aftsection>
<nextsent>iii and marcu, 2006).
</nextsent>
<nextsent>this work focuses on using unlabeled data from both the source and target domains to learn common feature representation that is meaningful across both domains.
</nextsent>
<nextsent>we hypothesize that discriminative model trained in the source domain using this common feature representation will generalize better to the target domain.
</nextsent>
<nextsent>this representation is learned using method we call structural correspondence learning (scl).the key idea of scl is to identify correspondences among features from different domains by modeling their correlations with pivot features.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC722">
<title id=" W06-1615.xml">domain adaptation with structural correspondence learning </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>furthermore, end-to-end systems like speech recognizers (roark et al, 2004)<papid> P04-1007 </papid>and automatic translators (och, 2003) <papid> P03-1021 </papid>use increasingly sophisticated discriminative models, which generalize well to new data that is drawn from the same distribution as the training data.</prevsent>
<prevsent>however, in many situations we may have source domain with plentiful labeled training data,but we need to process material from target do main with different distribution from the source domain and no labeled data.</prevsent>
</prevsection>
<citsent citstr=" P04-3013 ">
in such cases, we must take steps to adapt model trained on the source domain for use in the target domain (roark and bacchiani, 2003; <papid> N03-1027 </papid>florian et al, 2004; <papid> N04-1001 </papid>chelbaand acero, 2004; <papid> W04-3237 </papid>ando, 2004; <papid> P04-3013 </papid>lease and charniak, 2005; daume?</citsent>
<aftsection>
<nextsent>iii and marcu, 2006).
</nextsent>
<nextsent>this work focuses on using unlabeled data from both the source and target domains to learn common feature representation that is meaningful across both domains.
</nextsent>
<nextsent>we hypothesize that discriminative model trained in the source domain using this common feature representation will generalize better to the target domain.
</nextsent>
<nextsent>this representation is learned using method we call structural correspondence learning (scl).the key idea of scl is to identify correspondences among features from different domains by modeling their correlations with pivot features.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC723">
<title id=" W06-1615.xml">domain adaptation with structural correspondence learning </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>non-pivot features from different domains which are correlated with many of the same pivot features are assumed to correspond, and we treat them similarly in discriminative learner.
</prevsent>
<prevsent>even on the unlabeled data, the co-occurrence statistics of pivot and non-pivot features are likely to be sparse, and we must model them in compact way.
</prevsent>
</prevsection>
<citsent citstr=" J92-4003 ">
there are many choices for modeling co-occurrence data (brown et al, 1992; <papid> J92-4003 </papid>pereira et al, 1993; <papid> P93-1024 </papid>blei et al, 2003).</citsent>
<aftsection>
<nextsent>in this work we choose to use the technique of structural learning (ando and zhang, 2005<papid> P05-1001 </papid>a; ando and zhang,2005<papid> P05-1001 </papid>b).</nextsent>
<nextsent>structural learning models the correlations which are most useful for semi-supervisedlearning.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC724">
<title id=" W06-1615.xml">domain adaptation with structural correspondence learning </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>non-pivot features from different domains which are correlated with many of the same pivot features are assumed to correspond, and we treat them similarly in discriminative learner.
</prevsent>
<prevsent>even on the unlabeled data, the co-occurrence statistics of pivot and non-pivot features are likely to be sparse, and we must model them in compact way.
</prevsent>
</prevsection>
<citsent citstr=" P93-1024 ">
there are many choices for modeling co-occurrence data (brown et al, 1992; <papid> J92-4003 </papid>pereira et al, 1993; <papid> P93-1024 </papid>blei et al, 2003).</citsent>
<aftsection>
<nextsent>in this work we choose to use the technique of structural learning (ando and zhang, 2005<papid> P05-1001 </papid>a; ando and zhang,2005<papid> P05-1001 </papid>b).</nextsent>
<nextsent>structural learning models the correlations which are most useful for semi-supervisedlearning.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC725">
<title id=" W06-1615.xml">domain adaptation with structural correspondence learning </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>even on the unlabeled data, the co-occurrence statistics of pivot and non-pivot features are likely to be sparse, and we must model them in compact way.
</prevsent>
<prevsent>there are many choices for modeling co-occurrence data (brown et al, 1992; <papid> J92-4003 </papid>pereira et al, 1993; <papid> P93-1024 </papid>blei et al, 2003).</prevsent>
</prevsection>
<citsent citstr=" P05-1001 ">
in this work we choose to use the technique of structural learning (ando and zhang, 2005<papid> P05-1001 </papid>a; ando and zhang,2005<papid> P05-1001 </papid>b).</citsent>
<aftsection>
<nextsent>structural learning models the correlations which are most useful for semi-supervisedlearning.
</nextsent>
<nextsent>we demonstrate how to adapt it for transfer learning, and consequently the structural part of structural correspondence learning is borrowed from it.1scl is general technique, which one can apply to feature based classifiers for any task.
</nextsent>
<nextsent>here,1structural learning is different from learning with structured outputs, common paradigm for discriminative natural language processing models.
</nextsent>
<nextsent>to avoid terminological confusion, we refer throughout the paper to specific structural learning method, alternating structural optimization (aso) (ando and zhang, 2005<papid> P05-1001 </papid>a).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC740">
<title id=" W06-1615.xml">domain adaptation with structural correspondence learning </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>to avoid terminological confusion, we refer throughout the paper to specific structural learning method, alternating structural optimization (aso) (ando and zhang, 2005<papid> P05-1001 </papid>a).</prevsent>
<prevsent>120 (a) wall street journal dt jj vbz dt nn in dt jj nn the clash is sign of new toughness cc nn in nnp pos jj jj nn . and divisiveness in japan once-cozy financial circles .</prevsent>
</prevsection>
<citsent citstr=" N03-1033 ">
(b) medline dt jj vbn nns in dt nn nns vbp the oncogenic mutated forms of the ras proteins are rb jj cc vbp in jj nn nn . constitutively active and interfere with normal signal transduction . figure 1: part of speech-tagged sentences from both corpor awe investigate its use in part of speech (pos) tagging (ratnaparkhi, 1996; <papid> W96-0213 </papid>toutanova et al, 2003).<papid> N03-1033 </papid></citsent>
<aftsection>
<nextsent>while pos tagging has been heavily studied, many domains lack appropriate training corpora for postagging.
</nextsent>
<nextsent>nevertheless, pos tagging is an important stage in pipe lined language processing systems, from information extractors to speech synthesizers.
</nextsent>
<nextsent>we show how to use scl to transfer pos tagger from the wall street journal (financial news) to medline (biomedical abstracts), which use very different vocabularies, and we demonstrate not only improved pos accuracy but also improved end-to-end parsing accuracy while using the improved tagger.an important but rarely-explored setting in do main adaptation is when we have no labeled training data for the target domain.
</nextsent>
<nextsent>we first demonstrate that in this situation scl significantly improves performance over both supervised and semi-supervised taggers.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC777">
<title id=" W06-1615.xml">domain adaptation with structural correspondence learning </title>
<section> datasets and supervised tagger.  </section>
<citcontext>
<prevsection>
<prevsent>this cut inhalf training time and marginally improved performance in all our experiments.
</prevsent>
<prevsent>5.1 source domain: wsj.
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
we used sections 02-21 of the penn treebank (marcus et al, 1993) <papid> J93-2004 </papid>for training.</citsent>
<aftsection>
<nextsent>this resulted in 39,832 training sentences.
</nextsent>
<nextsent>for the unlabeled data, we used 100,000 sentences from 1988 subset of the wsj.
</nextsent>
<nextsent>5.2 target domain: biomedical text.
</nextsent>
<nextsent>for unlabeled data we used 200,000 sentences that were chosen by searching medline for abstracts pertaining to cancer, in particular genomic varia 123 company transaction investors officials your pretty short-term political receptors mutation assays lesions functional transient neuronal metastatic wsj only medline only figure 4: an example projection of word features onto r. words on the left (negative valued) behave similarly to each other for classification, but differently from words on the right (positive valued).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC778">
<title id=" W06-1615.xml">domain adaptation with structural correspondence learning </title>
<section> datasets and supervised tagger.  </section>
<citcontext>
<prevsection>
<prevsent>mira learns and outputs linear classification score, s(x,y;w) = ? f(x,y), where the feature representation can contain arbitrary features of the input, including the correspondence features described earlier.
</prevsent>
<prevsent>in particular, mira aims to learn weights so that the score of correct output, yt, for input xt is separated from the highest scoring incorrect outputs2, with margin proportional to their hamming losses.
</prevsent>
</prevsection>
<citsent citstr=" P05-1012 ">
mira has been used successfully for both sequence analysis (mcdonald et al, 2005<papid> P05-1012 </papid>a) and dependency parsing (mcdonald et al, 2005<papid> P05-1012 </papid>b).</citsent>
<aftsection>
<nextsent>as with any structured predictor, we need to factor the output space to make inference tractable.we use first-order markov factor ization, allowing for an efficient viterbi inference procedure.
</nextsent>
<nextsent>2we fix the number of high scoring incorrect outputs to 5.
</nextsent>
<nextsent>in section 2 we claimed that good representations should encode correspondences between words like signal?
</nextsent>
<nextsent>from medline and investment from the wsj.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC804">
<title id=" W06-1615.xml">domain adaptation with structural correspondence learning </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>by using just the unlabeled data from the target domain, however, we can view domain adaptation as standard semisupervisedlearning problem.
</prevsent>
<prevsent>there are many possible approaches for semisupservised learning in natural language processing, and it is beyond the scope of this paper to address them all.
</prevsent>
</prevsection>
<citsent citstr=" N04-1043 ">
we chose to compare with aso because it consistently outperforms co training (blum and mitchell, 1998) and clustering methods (miller et al, 2004).<papid> N04-1043 </papid></citsent>
<aftsection>
<nextsent>we did run experiments with the top-k version of aso (ando and zhang, 2005<papid> P05-1001 </papid>a), which is inspired by co training but consistently outperforms it.</nextsent>
<nextsent>thisdid not outperform the supervised method for do main adaptation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC834">
<title id=" W06-2809.xml">a proposal to automatically build and maintain gazette ers for named entity recognition by using wikipedia </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this restricts the techniques that can be used but makes the method useful for languages with few resources.
</prevsent>
<prevsent>the evaluation carried out proves that this approach can be successfully used to build ner gazette ers for location (f 78%) and person (f 68%) categories.
</prevsent>
</prevsection>
<citsent citstr=" M98-1001 ">
named entity recognition (ner) was defined at the muc conferences (chinchor, 1998) <papid> M98-1001 </papid>as the task consisting of detecting and classifying strings of text which are considered to belong to different classes (e.g. person, location, organization, date, time).</citsent>
<aftsection>
<nextsent>named entities are theoretically identified and classified by using evidence.
</nextsent>
<nextsent>two kinds of evidence have been defined (mcdonald, 1996).
</nextsent>
<nextsent>these are internal and external evidence.
</nextsent>
<nextsent>internal evidence is the one provided from within the sequence of words that constitute the entity.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC835">
<title id=" W06-2809.xml">a proposal to automatically build and maintain gazette ers for named entity recognition by using wikipedia </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>e.g. portugal could be an instance in location gazetteer.initially, and specially for the muc conferences, most of the ner systems developed did belong to the knowledge-based approach.
</prevsent>
<prevsent>this approach proved to be able to obtain high scores.
</prevsent>
</prevsection>
<citsent citstr=" M98-1021 ">
infact, the highest score obtained by knowledge based system in muc-7 reached 93.39 % (mikheev et al, 1998).<papid> M98-1021 </papid></citsent>
<aftsection>
<nextsent>however, this approach has an important problem: gazette ers and rules are difficult and tedious to develop and to maintain.
</nextsent>
<nextsent>ifthe system is to be used for an open domain, linguistic experts are needed to build the rules, and besides, it takes too much time to tune these resources in order to obtain satisfactory results.
</nextsent>
<nextsent>be cause of this, lately most of the research falls into the learning-based paradigm.
</nextsent>
<nextsent>regarding the creation and maintenance of gazette ers, several problems have been identified, these are mainly: ? creation and maintenance effort ? overlaps between gazette ers the first problem identified assumes that the gazette ers are manually created and maintained.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC836">
<title id=" W06-2809.xml">a proposal to automatically build and maintain gazette ers for named entity recognition by using wikipedia </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>several research works have been carried out inthis direction.
</prevsent>
<prevsent>an example of this is ner system which uses trigger gazette ers automatically extracted from wordnet (magnini et al, 2002) by using wordnet predicates.
</prevsent>
</prevsection>
<citsent citstr=" W03-0103 ">
the advantage in this case is that the resource used is multilingual and thus, porting it to another language is almost straightforward (negri and magnini, 2004).there is also work that deals with automatically building location gazette ers from internet texts by applying text mining procedures (ouri oupina, 2002), (uryupina, 2003).<papid> W03-0103 </papid></citsent>
<aftsection>
<nextsent>however, this work uses linguistic patterns, and thus is language dependent.
</nextsent>
<nextsent>the author claims that the approach may successfully be used to create gazette ers for ner.we agree with (magnini et al, 2002) that in order to automatically create and maintain trigger gazette ers, using hierarchy of common nouns is good approach.
</nextsent>
<nextsent>therefore, we want to focus onthe automatically creation and maintenance of entity gazetteers.
</nextsent>
<nextsent>another reason for this is that the class of common nouns (the ones being triggers) is much more stable than the class of proper names (the ones in entity gazetteers).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC837">
<title id=" W06-1602.xml">an empirical approach to the interpretation of superlatives </title>
<section> annotated corpus of superlatives.  </section>
<citcontext>
<prevsection>
<prevsent>] 22 def sg no ord 37 the agriculture department reported that feedlots in the 13 biggest ranch states held [.
</prevsent>
<prevsent>] 910 def pl yes no 1112the failed takeover would have given ual employees 75 % voting control of the nation second-largest airline [.
</prevsent>
</prevsection>
<citsent citstr=" J96-2004 ">
] 1717 pos sg no ord 1418the kappa statistics (k), where applicable (car letta, 1996).<papid> J96-2004 </papid></citsent>
<aftsection>
<nextsent>in using f-score, we arbitrarily takeone of the annotators?
</nextsent>
<nextsent>decisions (a) as gold standard and compare them with the other annotators decisions (b).
</nextsent>
<nextsent>note that here f-score is symmetric, since precision(a,b) = recall(b,a), and (balanced)f-score is the harmonic mean of precision andre call (tjong kim sang, 2002; hachey et al, 2005, <papid> W05-0619 </papid>see also section 5).</nextsent>
<nextsent>we evaluated three levels of agreement on sample of 1967 sentences (one full wsj section).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC838">
<title id=" W06-1602.xml">an empirical approach to the interpretation of superlatives </title>
<section> annotated corpus of superlatives.  </section>
<citcontext>
<prevsection>
<prevsent>in using f-score, we arbitrarily takeone of the annotators?
</prevsent>
<prevsent>decisions (a) as gold standard and compare them with the other annotators decisions (b).
</prevsent>
</prevsection>
<citsent citstr=" W05-0619 ">
note that here f-score is symmetric, since precision(a,b) = recall(b,a), and (balanced)f-score is the harmonic mean of precision andre call (tjong kim sang, 2002; hachey et al, 2005, <papid> W05-0619 </papid>see also section 5).</citsent>
<aftsection>
<nextsent>we evaluated three levels of agreement on sample of 1967 sentences (one full wsj section).
</nextsent>
<nextsent>the first level concerns superlative detection: to what extent different human judges can agree onwhat constitutes superlative.
</nextsent>
<nextsent>for this task, fscore was measured at 0.963 with total of 79 superlative phrases agreed upon.
</nextsent>
<nextsent>the second level of agreement is relative to type identification (attributive, predicative, adverbial, idiomatic), and is only calculated on the subset of cases both annotators recognised as superlatives (79 instances, as mentioned).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC839">
<title id=" W06-1602.xml">an empirical approach to the interpretation of superlatives </title>
<section> annotated corpus of superlatives.  </section>
<citcontext>
<prevsection>
<prevsent>we also assessed agreement for each class, and the attributive type resulted themost reliable with an f-score of 1 (total agreement on 64 cases), whereas there was some disagreement in classifying predicative and adverbial cases (0.9 and 0.8 f-score, respectively).
</prevsent>
<prevsent>idiomatic uses where not detected in this portion of the data.
</prevsent>
</prevsection>
<citsent citstr=" J04-1005 ">
to assess this classification task we also used the kappa statistics which yielded kco=0.922 (fol lowing (eugenio and glass, 2004) <papid> J04-1005 </papid>we report kas kco, indicating that we calculate a` la cohen (cohen, 1960).</citsent>
<aftsection>
<nextsent>kco over 0.9 is considered to signal very good agreement (krippendorff, 1980).
</nextsent>
<nextsent>the third and last level of agreement deals with the span of the comparison set and only concernsattributive cases (64 out of 79).
</nextsent>
<nextsent>percentage agreement was used since this is not classification task and was measured at 95.31%.the agreement results show that the task appears quite easy to perform for linguists.
</nextsent>
<nextsent>despite the limited number of instances compared, this has also emerged from the annotators?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC840">
<title id=" W06-1602.xml">an empirical approach to the interpretation of superlatives </title>
<section> automatic analysis of superlatives.  </section>
<citcontext>
<prevsection>
<prevsent>4.1 combinatory categorial grammar.
</prevsent>
<prevsent>(ccg)ccg is lexicalised theory of grammar (steedman, 2001).
</prevsent>
</prevsection>
<citsent citstr=" P04-1014 ">
we used clark &amp; currans wide coverage statistical parser (clark and curran,2004) <papid> P04-1014 </papid>trained on ccg-bank, which in turn is derived from the penn-treebank (hockenmaier and steedman, 2002).<papid> P02-1043 </papid></citsent>
<aftsection>
<nextsent>in ccg-bank, the majority of superlative adjective of cases are analysed as fol lows: the tallest woman np/n n/n n np most devastating droughts (n/n)/(n/n) n/n n/n third largest bank n/n (n/n)\(n/n) n/n clark &amp; currans parser outputs besides ccgderivation of the input sentence also part-of speech (pos) tag and lemmatised form for each input token.
</nextsent>
<nextsent>to recognise attributive superlatives in the output of the parser, we look both at the pos tag and the ccg-category assigned to word.
</nextsent>
<nextsent>words with pos-tag jjs and ccg category n/n, (n/n)/(n/n), or (n/n)\(n/n) are considered attributive superlatives adjectives, andso are the words most?
</nextsent>
<nextsent>and least?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC841">
<title id=" W06-1602.xml">an empirical approach to the interpretation of superlatives </title>
<section> automatic analysis of superlatives.  </section>
<citcontext>
<prevsection>
<prevsent>4.1 combinatory categorial grammar.
</prevsent>
<prevsent>(ccg)ccg is lexicalised theory of grammar (steedman, 2001).
</prevsent>
</prevsection>
<citsent citstr=" P02-1043 ">
we used clark &amp; currans wide coverage statistical parser (clark and curran,2004) <papid> P04-1014 </papid>trained on ccg-bank, which in turn is derived from the penn-treebank (hockenmaier and steedman, 2002).<papid> P02-1043 </papid></citsent>
<aftsection>
<nextsent>in ccg-bank, the majority of superlative adjective of cases are analysed as fol lows: the tallest woman np/n n/n n np most devastating droughts (n/n)/(n/n) n/n n/n third largest bank n/n (n/n)\(n/n) n/n clark &amp; currans parser outputs besides ccgderivation of the input sentence also part-of speech (pos) tag and lemmatised form for each input token.
</nextsent>
<nextsent>to recognise attributive superlatives in the output of the parser, we look both at the pos tag and the ccg-category assigned to word.
</nextsent>
<nextsent>words with pos-tag jjs and ccg category n/n, (n/n)/(n/n), or (n/n)\(n/n) are considered attributive superlatives adjectives, andso are the words most?
</nextsent>
<nextsent>and least?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC842">
<title id=" W06-1602.xml">an empirical approach to the interpretation of superlatives </title>
<section> automatic analysis of superlatives.  </section>
<citcontext>
<prevsection>
<prevsent>the output of the parser, ccg derivation of the input sentence, is used to construct discourse representation structure (drs, the semantic representation proposed by drt (kamp and reyle, 2this is due to the fact that the penn-treebank annotation guidelines prescribe that all hyphenated adjectives ought to be tagged as jj.
</prevsent>
<prevsent>1993)).
</prevsent>
</prevsection>
<citsent citstr=" C04-1180 ">
we follow (bos et al, 2004; <papid> C04-1180 </papid>bos, 2005) in automatically building semantic representation on the basis of ccg derivations in compositional fashion.</citsent>
<aftsection>
<nextsent>we briefly summarise the approach here.the semantic representation for word is determined by its ccg category, pos-tag, and lemma.
</nextsent>
<nextsent>consider the following lexical entries: the: p.q.( ;p(x);q(x)) tallest: p.x.( ( y 6=x ;p(y))?
</nextsent>
<nextsent>taller(x,y) ;p(x)) man: x. man(x)these lexical entries are combined in compositional fashion following the ccg derivation, using the ?-calculus as glue language: tallest man: x. man(x) y6=x man(y) ? taller(x,y) the tallest man: q.( man(x) y6=x man(y) ? taller(x,y) ;q(x)) in this way drss can be produced in robust way, achieving high-coverage.
</nextsent>
<nextsent>an example output representation of the complete system is shown in figure 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC843">
<title id=" W06-1602.xml">an empirical approach to the interpretation of superlatives </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>5.1 superlative detection.
</prevsent>
<prevsent>baseline system for superlative detection we generated baseline that solely relies on part-of speech information.
</prevsent>
</prevsection>
<citsent citstr=" A00-1031 ">
the data was tagged using tnt (brants, 2000), <papid> A00-1031 </papid>using model trained on thewall street journal.</citsent>
<aftsection>
<nextsent>in the wsj tagset, superlatives can be marked in two different ways, depending on whether the adjective is inflected or modified by most/least.
</nextsent>
<nextsent>so, largest?, for instance, is tagged as jjs, whereas most beautiful?
</nextsent>
<nextsent>is sequence of rbs (most) and jj (beautiful).
</nextsent>
<nextsent>we also checked that they are followed by common or proper noun (nn.*), allowing one word to occur in between.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC845">
<title id=" W06-1636.xml">learning phrasal categories </title>
<section> introduction and previous research.  </section>
<citcontext>
<prevsection>


</prevsection>
<citsent citstr=" J93-2004 ">
it is by now commonplace knowledge that accurate syntactic parsing is not possible given onlya context-free grammar with standard penn tree bank (marcus et al, 1993) <papid> J93-2004 </papid>labels (e.g., s, np , etc.)</citsent>
<aftsection>
<nextsent>(charniak, 1996).
</nextsent>
<nextsent>instead researchers condition parsing decisions on many other features, such as parent phrase-marker, and, famously, the lexical-head of the phrase (mager man, 1995; <papid> P95-1037 </papid>collins, 1996; <papid> P96-1025 </papid>collins, 1997; <papid> P97-1003 </papid>johnson, 1998; <papid> J98-4004 </papid>charniak, 2000; <papid> A00-2018 </papid>henderson, 2003; <papid> N03-1014 </papid>klein and manning, 2003; <papid> P03-1054 </papid>matsuzaki et al, 2005) (<papid> P05-1010 </papid>and others).</nextsent>
<nextsent>one particularly perspicuous way to view the use of extra conditioning information is that of tree-transformation (johnson, 1998; <papid> J98-4004 </papid>klein and manning, 2003).<papid> P03-1054 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC846">
<title id=" W06-1636.xml">learning phrasal categories </title>
<section> introduction and previous research.  </section>
<citcontext>
<prevsection>
<prevsent>it is by now commonplace knowledge that accurate syntactic parsing is not possible given onlya context-free grammar with standard penn tree bank (marcus et al, 1993) <papid> J93-2004 </papid>labels (e.g., s, np , etc.)</prevsent>
<prevsent>(charniak, 1996).</prevsent>
</prevsection>
<citsent citstr=" P95-1037 ">
instead researchers condition parsing decisions on many other features, such as parent phrase-marker, and, famously, the lexical-head of the phrase (mager man, 1995; <papid> P95-1037 </papid>collins, 1996; <papid> P96-1025 </papid>collins, 1997; <papid> P97-1003 </papid>johnson, 1998; <papid> J98-4004 </papid>charniak, 2000; <papid> A00-2018 </papid>henderson, 2003; <papid> N03-1014 </papid>klein and manning, 2003; <papid> P03-1054 </papid>matsuzaki et al, 2005) (<papid> P05-1010 </papid>and others).</citsent>
<aftsection>
<nextsent>one particularly perspicuous way to view the use of extra conditioning information is that of tree-transformation (johnson, 1998; <papid> J98-4004 </papid>klein and manning, 2003).<papid> P03-1054 </papid></nextsent>
<nextsent>rather than imagining the parser roaming around the tree for picking up the information it needs, we rather relabel the nodes to directly encode this information.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC848">
<title id=" W06-1636.xml">learning phrasal categories </title>
<section> introduction and previous research.  </section>
<citcontext>
<prevsection>
<prevsent>it is by now commonplace knowledge that accurate syntactic parsing is not possible given onlya context-free grammar with standard penn tree bank (marcus et al, 1993) <papid> J93-2004 </papid>labels (e.g., s, np , etc.)</prevsent>
<prevsent>(charniak, 1996).</prevsent>
</prevsection>
<citsent citstr=" P96-1025 ">
instead researchers condition parsing decisions on many other features, such as parent phrase-marker, and, famously, the lexical-head of the phrase (mager man, 1995; <papid> P95-1037 </papid>collins, 1996; <papid> P96-1025 </papid>collins, 1997; <papid> P97-1003 </papid>johnson, 1998; <papid> J98-4004 </papid>charniak, 2000; <papid> A00-2018 </papid>henderson, 2003; <papid> N03-1014 </papid>klein and manning, 2003; <papid> P03-1054 </papid>matsuzaki et al, 2005) (<papid> P05-1010 </papid>and others).</citsent>
<aftsection>
<nextsent>one particularly perspicuous way to view the use of extra conditioning information is that of tree-transformation (johnson, 1998; <papid> J98-4004 </papid>klein and manning, 2003).<papid> P03-1054 </papid></nextsent>
<nextsent>rather than imagining the parser roaming around the tree for picking up the information it needs, we rather relabel the nodes to directly encode this information.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC849">
<title id=" W06-1636.xml">learning phrasal categories </title>
<section> introduction and previous research.  </section>
<citcontext>
<prevsection>
<prevsent>it is by now commonplace knowledge that accurate syntactic parsing is not possible given onlya context-free grammar with standard penn tree bank (marcus et al, 1993) <papid> J93-2004 </papid>labels (e.g., s, np , etc.)</prevsent>
<prevsent>(charniak, 1996).</prevsent>
</prevsection>
<citsent citstr=" P97-1003 ">
instead researchers condition parsing decisions on many other features, such as parent phrase-marker, and, famously, the lexical-head of the phrase (mager man, 1995; <papid> P95-1037 </papid>collins, 1996; <papid> P96-1025 </papid>collins, 1997; <papid> P97-1003 </papid>johnson, 1998; <papid> J98-4004 </papid>charniak, 2000; <papid> A00-2018 </papid>henderson, 2003; <papid> N03-1014 </papid>klein and manning, 2003; <papid> P03-1054 </papid>matsuzaki et al, 2005) (<papid> P05-1010 </papid>and others).</citsent>
<aftsection>
<nextsent>one particularly perspicuous way to view the use of extra conditioning information is that of tree-transformation (johnson, 1998; <papid> J98-4004 </papid>klein and manning, 2003).<papid> P03-1054 </papid></nextsent>
<nextsent>rather than imagining the parser roaming around the tree for picking up the information it needs, we rather relabel the nodes to directly encode this information.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC850">
<title id=" W06-1636.xml">learning phrasal categories </title>
<section> introduction and previous research.  </section>
<citcontext>
<prevsection>
<prevsent>it is by now commonplace knowledge that accurate syntactic parsing is not possible given onlya context-free grammar with standard penn tree bank (marcus et al, 1993) <papid> J93-2004 </papid>labels (e.g., s, np , etc.)</prevsent>
<prevsent>(charniak, 1996).</prevsent>
</prevsection>
<citsent citstr=" J98-4004 ">
instead researchers condition parsing decisions on many other features, such as parent phrase-marker, and, famously, the lexical-head of the phrase (mager man, 1995; <papid> P95-1037 </papid>collins, 1996; <papid> P96-1025 </papid>collins, 1997; <papid> P97-1003 </papid>johnson, 1998; <papid> J98-4004 </papid>charniak, 2000; <papid> A00-2018 </papid>henderson, 2003; <papid> N03-1014 </papid>klein and manning, 2003; <papid> P03-1054 </papid>matsuzaki et al, 2005) (<papid> P05-1010 </papid>and others).</citsent>
<aftsection>
<nextsent>one particularly perspicuous way to view the use of extra conditioning information is that of tree-transformation (johnson, 1998; <papid> J98-4004 </papid>klein and manning, 2003).<papid> P03-1054 </papid></nextsent>
<nextsent>rather than imagining the parser roaming around the tree for picking up the information it needs, we rather relabel the nodes to directly encode this information.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC852">
<title id=" W06-1636.xml">learning phrasal categories </title>
<section> introduction and previous research.  </section>
<citcontext>
<prevsection>
<prevsent>it is by now commonplace knowledge that accurate syntactic parsing is not possible given onlya context-free grammar with standard penn tree bank (marcus et al, 1993) <papid> J93-2004 </papid>labels (e.g., s, np , etc.)</prevsent>
<prevsent>(charniak, 1996).</prevsent>
</prevsection>
<citsent citstr=" A00-2018 ">
instead researchers condition parsing decisions on many other features, such as parent phrase-marker, and, famously, the lexical-head of the phrase (mager man, 1995; <papid> P95-1037 </papid>collins, 1996; <papid> P96-1025 </papid>collins, 1997; <papid> P97-1003 </papid>johnson, 1998; <papid> J98-4004 </papid>charniak, 2000; <papid> A00-2018 </papid>henderson, 2003; <papid> N03-1014 </papid>klein and manning, 2003; <papid> P03-1054 </papid>matsuzaki et al, 2005) (<papid> P05-1010 </papid>and others).</citsent>
<aftsection>
<nextsent>one particularly perspicuous way to view the use of extra conditioning information is that of tree-transformation (johnson, 1998; <papid> J98-4004 </papid>klein and manning, 2003).<papid> P03-1054 </papid></nextsent>
<nextsent>rather than imagining the parser roaming around the tree for picking up the information it needs, we rather relabel the nodes to directly encode this information.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC853">
<title id=" W06-1636.xml">learning phrasal categories </title>
<section> introduction and previous research.  </section>
<citcontext>
<prevsection>
<prevsent>it is by now commonplace knowledge that accurate syntactic parsing is not possible given onlya context-free grammar with standard penn tree bank (marcus et al, 1993) <papid> J93-2004 </papid>labels (e.g., s, np , etc.)</prevsent>
<prevsent>(charniak, 1996).</prevsent>
</prevsection>
<citsent citstr=" N03-1014 ">
instead researchers condition parsing decisions on many other features, such as parent phrase-marker, and, famously, the lexical-head of the phrase (mager man, 1995; <papid> P95-1037 </papid>collins, 1996; <papid> P96-1025 </papid>collins, 1997; <papid> P97-1003 </papid>johnson, 1998; <papid> J98-4004 </papid>charniak, 2000; <papid> A00-2018 </papid>henderson, 2003; <papid> N03-1014 </papid>klein and manning, 2003; <papid> P03-1054 </papid>matsuzaki et al, 2005) (<papid> P05-1010 </papid>and others).</citsent>
<aftsection>
<nextsent>one particularly perspicuous way to view the use of extra conditioning information is that of tree-transformation (johnson, 1998; <papid> J98-4004 </papid>klein and manning, 2003).<papid> P03-1054 </papid></nextsent>
<nextsent>rather than imagining the parser roaming around the tree for picking up the information it needs, we rather relabel the nodes to directly encode this information.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC855">
<title id=" W06-1636.xml">learning phrasal categories </title>
<section> introduction and previous research.  </section>
<citcontext>
<prevsection>
<prevsent>it is by now commonplace knowledge that accurate syntactic parsing is not possible given onlya context-free grammar with standard penn tree bank (marcus et al, 1993) <papid> J93-2004 </papid>labels (e.g., s, np , etc.)</prevsent>
<prevsent>(charniak, 1996).</prevsent>
</prevsection>
<citsent citstr=" P03-1054 ">
instead researchers condition parsing decisions on many other features, such as parent phrase-marker, and, famously, the lexical-head of the phrase (mager man, 1995; <papid> P95-1037 </papid>collins, 1996; <papid> P96-1025 </papid>collins, 1997; <papid> P97-1003 </papid>johnson, 1998; <papid> J98-4004 </papid>charniak, 2000; <papid> A00-2018 </papid>henderson, 2003; <papid> N03-1014 </papid>klein and manning, 2003; <papid> P03-1054 </papid>matsuzaki et al, 2005) (<papid> P05-1010 </papid>and others).</citsent>
<aftsection>
<nextsent>one particularly perspicuous way to view the use of extra conditioning information is that of tree-transformation (johnson, 1998; <papid> J98-4004 </papid>klein and manning, 2003).<papid> P03-1054 </papid></nextsent>
<nextsent>rather than imagining the parser roaming around the tree for picking up the information it needs, we rather relabel the nodes to directly encode this information.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC856">
<title id=" W06-1636.xml">learning phrasal categories </title>
<section> introduction and previous research.  </section>
<citcontext>
<prevsection>
<prevsent>it is by now commonplace knowledge that accurate syntactic parsing is not possible given onlya context-free grammar with standard penn tree bank (marcus et al, 1993) <papid> J93-2004 </papid>labels (e.g., s, np , etc.)</prevsent>
<prevsent>(charniak, 1996).</prevsent>
</prevsection>
<citsent citstr=" P05-1010 ">
instead researchers condition parsing decisions on many other features, such as parent phrase-marker, and, famously, the lexical-head of the phrase (mager man, 1995; <papid> P95-1037 </papid>collins, 1996; <papid> P96-1025 </papid>collins, 1997; <papid> P97-1003 </papid>johnson, 1998; <papid> J98-4004 </papid>charniak, 2000; <papid> A00-2018 </papid>henderson, 2003; <papid> N03-1014 </papid>klein and manning, 2003; <papid> P03-1054 </papid>matsuzaki et al, 2005) (<papid> P05-1010 </papid>and others).</citsent>
<aftsection>
<nextsent>one particularly perspicuous way to view the use of extra conditioning information is that of tree-transformation (johnson, 1998; <papid> J98-4004 </papid>klein and manning, 2003).<papid> P03-1054 </papid></nextsent>
<nextsent>rather than imagining the parser roaming around the tree for picking up the information it needs, we rather relabel the nodes to directly encode this information.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC883">
<title id=" W06-1636.xml">learning phrasal categories </title>
<section> introduction and previous research.  </section>
<citcontext>
<prevsection>
<prevsent>perhaps more substantial difference is that by not casting his problem as one of learning phrasal categories magerman loses all of the free pcfg technology that we can leverage.
</prevsent>
<prevsent>for instance, magerman must use heuristic search to find his parses and incurs search errors because ofit.
</prevsent>
</prevsection>
<citsent citstr=" C02-1068 ">
we use an efficient cky algorithm to do exhaustive search in reasonable time.belz (2002) <papid> C02-1068 </papid>considers the problem in manner more similar to our approach.</citsent>
<aftsection>
<nextsent>beginning with both non-annotated grammar and parent annotated grammar, using beam search they search the space of grammars which can be attained via merging nonterminals.
</nextsent>
<nextsent>they guide the search using the performance on parsing (and several other tasks) of the grammar at each stage in the search.
</nextsent>
<nextsent>in contrast, our approach explores the space of grammars by starting with few nonterminals and splitting them.
</nextsent>
<nextsent>we also consider much wider range of contextual information than just parent phrase-markers.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC955">
<title id=" W06-1636.xml">learning phrasal categories </title>
<section> conclusion.  </section>
<citcontext>
<prevsection>
<prevsent>decisions like this abound, and are worth exploring.
</prevsent>
<prevsent>more radically, it is also possible to grow many decision trees, and thus many alternative grammars.
</prevsent>
</prevsection>
<citsent citstr=" W04-3242 ">
we have been impressed by the success of random-forest methods in language modeling (xu and jelinek, 2004).<papid> W04-3242 </papid></citsent>
<aftsection>
<nextsent>in these methods many trees (the forest) are grown, each trying to predict the next word.
</nextsent>
<nextsent>the multiple trees together are much more powerful than any one individually.
</nextsent>
<nextsent>the same might be true for grammars.
</nextsent>
<nextsent>acknowledgement the research presented here was funded in part by darpa gale contract hr 0011-06-20001.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC956">
<title id=" W06-3119.xml">syntax augmented machine translation via chart parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we present results on the french-to-english task for this workshop, representing significant improvements over the workshops baselinesystem.
</prevsent>
<prevsent>our translation system is available open-source under the gnu general public license.
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
recent work in machine translation has evolved from the traditional word (brown et al, 1993) <papid> J93-2003 </papid>and phrase based (koehn et al, 2003<papid> N03-1017 </papid>a) models to include hierarchical phrase models (chiang, 2005) <papid> P05-1033 </papid>and bilingual synchronous grammars (melamed, 2004).<papid> P04-1083 </papid>these advances are motivated by the desire to integrate richer knowledge sources within the translation process with the explicit goal of producing more fluent translations in the target language.</citsent>
<aftsection>
<nextsent>the hierarchical translation operations introduced in these methods call for extensions to the traditional beam decoder (koehn et al, 2003<papid> N03-1017 </papid>a).</nextsent>
<nextsent>in this work we introduce techniques to generate syntactically motivated generalized phrases and discuss issues inchart parser based decoding in the statistical machine translation environment.(chiang, 2005) <papid> P05-1033 </papid>generates synchronous context free grammar (syncfg) rules from an existing phrase translation table.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC957">
<title id=" W06-3119.xml">syntax augmented machine translation via chart parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we present results on the french-to-english task for this workshop, representing significant improvements over the workshops baselinesystem.
</prevsent>
<prevsent>our translation system is available open-source under the gnu general public license.
</prevsent>
</prevsection>
<citsent citstr=" N03-1017 ">
recent work in machine translation has evolved from the traditional word (brown et al, 1993) <papid> J93-2003 </papid>and phrase based (koehn et al, 2003<papid> N03-1017 </papid>a) models to include hierarchical phrase models (chiang, 2005) <papid> P05-1033 </papid>and bilingual synchronous grammars (melamed, 2004).<papid> P04-1083 </papid>these advances are motivated by the desire to integrate richer knowledge sources within the translation process with the explicit goal of producing more fluent translations in the target language.</citsent>
<aftsection>
<nextsent>the hierarchical translation operations introduced in these methods call for extensions to the traditional beam decoder (koehn et al, 2003<papid> N03-1017 </papid>a).</nextsent>
<nextsent>in this work we introduce techniques to generate syntactically motivated generalized phrases and discuss issues inchart parser based decoding in the statistical machine translation environment.(chiang, 2005) <papid> P05-1033 </papid>generates synchronous context free grammar (syncfg) rules from an existing phrase translation table.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC963">
<title id=" W06-3119.xml">syntax augmented machine translation via chart parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we present results on the french-to-english task for this workshop, representing significant improvements over the workshops baselinesystem.
</prevsent>
<prevsent>our translation system is available open-source under the gnu general public license.
</prevsent>
</prevsection>
<citsent citstr=" P05-1033 ">
recent work in machine translation has evolved from the traditional word (brown et al, 1993) <papid> J93-2003 </papid>and phrase based (koehn et al, 2003<papid> N03-1017 </papid>a) models to include hierarchical phrase models (chiang, 2005) <papid> P05-1033 </papid>and bilingual synchronous grammars (melamed, 2004).<papid> P04-1083 </papid>these advances are motivated by the desire to integrate richer knowledge sources within the translation process with the explicit goal of producing more fluent translations in the target language.</citsent>
<aftsection>
<nextsent>the hierarchical translation operations introduced in these methods call for extensions to the traditional beam decoder (koehn et al, 2003<papid> N03-1017 </papid>a).</nextsent>
<nextsent>in this work we introduce techniques to generate syntactically motivated generalized phrases and discuss issues inchart parser based decoding in the statistical machine translation environment.(chiang, 2005) <papid> P05-1033 </papid>generates synchronous context free grammar (syncfg) rules from an existing phrase translation table.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC967">
<title id=" W06-3119.xml">syntax augmented machine translation via chart parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we present results on the french-to-english task for this workshop, representing significant improvements over the workshops baselinesystem.
</prevsent>
<prevsent>our translation system is available open-source under the gnu general public license.
</prevsent>
</prevsection>
<citsent citstr=" P04-1083 ">
recent work in machine translation has evolved from the traditional word (brown et al, 1993) <papid> J93-2003 </papid>and phrase based (koehn et al, 2003<papid> N03-1017 </papid>a) models to include hierarchical phrase models (chiang, 2005) <papid> P05-1033 </papid>and bilingual synchronous grammars (melamed, 2004).<papid> P04-1083 </papid>these advances are motivated by the desire to integrate richer knowledge sources within the translation process with the explicit goal of producing more fluent translations in the target language.</citsent>
<aftsection>
<nextsent>the hierarchical translation operations introduced in these methods call for extensions to the traditional beam decoder (koehn et al, 2003<papid> N03-1017 </papid>a).</nextsent>
<nextsent>in this work we introduce techniques to generate syntactically motivated generalized phrases and discuss issues inchart parser based decoding in the statistical machine translation environment.(chiang, 2005) <papid> P05-1033 </papid>generates synchronous context free grammar (syncfg) rules from an existing phrase translation table.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC982">
<title id=" W06-3119.xml">syntax augmented machine translation via chart parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these rules can be viewed as phrase pairs with mixed lexical and non-terminal entries, where non-terminal entries (occurring aspairs in the source and target side) represent place holders for inserting additional phrases pairs (which again may contain nonterminals) at decoding time.
</prevsent>
<prevsent>while (chiang, 2005) <papid> P05-1033 </papid>uses only two nonterminal symbols in his grammar, we introduce multiple syntactic categories, taking advantage of target language parser for this information.</prevsent>
</prevsection>
<citsent citstr=" P02-1039 ">
while (yamada and knight, 2002) <papid> P02-1039 </papid>represent syntactical information in the decoding process through series of transformation operations, we operate directly at the phrase level.</citsent>
<aftsection>
<nextsent>in addition to the benefits that come from more structured hierarchical rule set, we believe that these restrictions serve as syntax driven language model that can guide the decoding process,as n-gram context based language models do intra ditional decoding.
</nextsent>
<nextsent>in the following sections, we describe our phrase annotation and generalization process followed by the design and pruning decisions in our chart parser.
</nextsent>
<nextsent>we give results on the french-english europarl data and conclude with prospects for future work.
</nextsent>
<nextsent>138
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC989">
<title id=" W06-3119.xml">syntax augmented machine translation via chart parsing </title>
<section> rule generation.  </section>
<citcontext>
<prevsection>
<prevsent>we start with phrase translations on the parallel training data using the techniques and implementation described in (koehn et al, 2003<papid> N03-1017 </papid>a).</prevsent>
<prevsent>this phrase table provides the purely lexical entries in the final hierarchical rule set that will be used in decoding.</prevsent>
</prevsection>
<citsent citstr=" A00-2018 ">
we then use charniaks parser (charniak, 2000) <papid> A00-2018 </papid>to generate the most likely parse tree for each english target sentence in the training corpus.</citsent>
<aftsection>
<nextsent>next, we determine all phrase pairs in the phrase table whose source and target side occur in each respective source and target sentence pair defining the scope of the initial rules in our syncfg.annotation if the target side of any of these initial rules correspond to syntactic category of the target side parse tree, we label the phrase pair with that syntactic category.
</nextsent>
<nextsent>this label corresponds to the left-hand side of our synchronous grammar.
</nextsent>
<nextsent>phrase pairs that do not correspond to span in the parse tree are given default category x?, and can still play role in the decoding process.
</nextsent>
<nextsent>in work done after submission to the 2006 data track, we assign such phrases an extended category of the form c1 + c2, c1/c2, or c2\c1, indicating that the phrase pairs target side spans two adjacent syntactic categories(e.g., she went: np+v), partial syntactic category c1 missing c2 to the right (e.g., the great: np/nn), or partial c1 missing c2 to the left (e.g., great wall: dt\np), respectively.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC994">
<title id=" W06-3119.xml">syntax augmented machine translation via chart parsing </title>
<section> scoring.  </section>
<citcontext>
<prevsection>
<prevsent>i=1 n?
</prevsent>
<prevsent>j=1 (vj)i .here, 1, . . .
</prevsent>
</prevsection>
<citsent citstr=" P03-1021 ">
, are the parameters of the loglinear model, which we optimize on held-out portion of the training set (2005 development data) using minimum-error-rate training (och, 2003).<papid> P03-1021 </papid></citsent>
<aftsection>
<nextsent>we use the following features for our rules: ? source- and target-conditioned neg-log lexical weights as described in (koehn et al, 2003<papid> N03-1017 </papid>b)?</nextsent>
<nextsent>neg-log relative frequencies: left-hand side-conditioned, target-phrase-conditioned, source-phrase-conditioned ? counters: n.o. rule applications, n.o. target words?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1001">
<title id=" W06-3119.xml">syntax augmented machine translation via chart parsing </title>
<section> parsing.  </section>
<citcontext>
<prevsection>
<prevsent>our syncfg rules are equivalent to probabilistic context-free grammar and decoding is therefore an application of chart parsing.
</prevsent>
<prevsent>instead of the common method of converting the cfg grammar into chomsky normal form and applying cky algorithm to produce the most likely parse forgiven source sentence, we avoided the explosion of the rule set caused by the introduction of new non-terminals in the conversion process and implemented variant of the cky+ algorithm as described in (j.earley, 1970).
</prevsent>
</prevsection>
<citsent citstr=" W05-1506 ">
each cell of the parsing process in (j.earley, 1970) contains set of hypergraph nodes (huangand chiang, 2005).<papid> W05-1506 </papid></citsent>
<aftsection>
<nextsent>a hypergraph node is an equivalence class of complete hypotheses (derivations) with identical production results (left-hand sides ofthe corresponding applied rules).
</nextsent>
<nextsent>complete hypotheses point directly to nodes in their backwards star, and the cost of the complete hypothesis is calculated with respect to each back pointer nodes best cost.this structure affords efficient parsing with minimal pruning (we use single parameter to restrict the number of hierarchical rules applied), but sacrifices effective management of unique language model states contributing to significant search errors during parsing.
</nextsent>
<nextsent>at initial submission time we simplyre-scored k-best list extracted after first best parsing using the lazy retrieval process in (huang and chiang, 2005).<papid> W05-1506 </papid></nextsent>
<nextsent>post-submission after our workshop submission,we modified the k-best list extraction process to integrate an n-gram language model during k-best extraction.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1021">
<title id=" W06-1910.xml">experiments adapting an open domain question answering system tothe geographical domain using scope based resources </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this paper we describe our experiments in the adaptation and evaluation of an odqa system to restricted domain, the geographical domain.geotalp-qa is multilingual geographical domain question answering (gdqa) system.
</prevsent>
<prevsent>this restricted domain question answering (rdqa) system has been built over an existing odqa system, talp-qa, multilingual odqa system that processes both factoid and definition questions (see (ferres et al, 2005) and (ferres et al., 2004)).
</prevsent>
</prevsection>
<citsent citstr=" W04-0506 ">
the system was evaluated for spanish and english in the context of our participation in the conferences trec and clef in 2005 and has been adapted to multilingual gdqa system for factoid questions.as pointed out in (benamara, 2004), <papid> W04-0506 </papid>the geographical domain (gd) can be considered middle way between real restricted domains andopen ones because many open domain texts contain high density of geographical terms.</citsent>
<aftsection>
<nextsent>although the basic architecture of talp-qa has remained unchanged, set of qa components were redesigned and modified and we had to add some specific components for the gd to our qa system.
</nextsent>
<nextsent>the basic approach in talp-qa consists of applying language-dependent processes on both question and passages for getting language independent semantic representation, and then extracting set of semantic constraints (sc) for each question.
</nextsent>
<nextsent>then, an answer extraction algorithm extracts and ranks sentences that satisfy the scsof the question.
</nextsent>
<nextsent>finally, an answer selection module chooses the most appropriate answer.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1022">
<title id=" W06-1910.xml">experiments adapting an open domain question answering system tothe geographical domain using scope based resources </title>
<section> restricted domain qa systems.  </section>
<citcontext>
<prevsection>
<prevsent>so, the repertory of question patterns is limited allowing good accuracy in question processing with limited effort.
</prevsent>
<prevsent>user requirements regarding the quality of the answer tend to be higher in rdqa.
</prevsent>
</prevsection>
<citsent citstr=" W04-0507 ">
as (chung et al, 2004) <papid> W04-0507 </papid>pointed out, no answer is preferred to wrong answer.?</citsent>
<aftsection>
<nextsent>in rdqa not only nes but also domain specific terminology plays central role.
</nextsent>
<nextsent>this fact usually implies that domain specific lexicons and gazette ers have to be used.?
</nextsent>
<nextsent>in some cases, as in gd, many documents included in the collections are far to be standard nl texts but contain tables, lists, illformed sentences, etc. sometimes follow inga more or less defined structure.
</nextsent>
<nextsent>thus, extraction systems based, as our, on the linguistic structure of the sentences have to be relaxed in some way to deal with this kind of texts.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1023">
<title id=" W06-1910.xml">experiments adapting an open domain question answering system tothe geographical domain using scope based resources </title>
<section> resources for scope-based.  </section>
<citcontext>
<prevsection>
<prevsent>first, we obtained the original set of documents (26235 files).
</prevsent>
<prevsent>then, we selected two sets of 120 documents about the spanish geography domain and the non-spanish geography domain.
</prevsent>
</prevsection>
<citsent citstr=" C00-1072 ">
using these sets we obtained set of topic signatures (ts) (lin and hovy, 2000) <papid> C00-1072 </papid>for the spanish geography domain and another set of ts for the non-spanish geography domain.</citsent>
<aftsection>
<nextsent>then, we used these ts to filter the documents from wikipedia, and we obtained set of 8851 documents pertaining to the spanish geography do main.
</nextsent>
<nextsent>these documents were pre-processed and indexed.
</nextsent>
<nextsent>4.3 geographical scope-based resources.
</nextsent>
<nextsent>a knowledge base (kb) of spanish geography has been built using four resources:?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1024">
<title id=" W07-0711.xml">using word dependent transition models in hmm based word alignment for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we also conducted machine translation (mt) experiments on the europarl corpus.
</prevsent>
<prevsent>mt results show that word alignment based on this method can be used in phrase-based machine translation system to yield up to 1% absolute improvement in bleu score, compared to conventional hmm, and 0.8% compared to ibm model 4 based word alignment.
</prevsent>
</prevsection>
<citsent citstr=" N03-1017 ">
word alignment is an important step of most modern approaches to statistical machine translation (koehn et al, 2003).<papid> N03-1017 </papid></citsent>
<aftsection>
<nextsent>the classical approaches to word alignment are based on ibm models 1-5 (brown et al, 1994) and the hmm based alignment model (vogel et al, 1996) (<papid> C96-2141 </papid>och and ney, 2000<papid> P00-1056 </papid>a, 2000b), while recently discriminative approaches (moore, 2006) and syntax based approaches (zhang and gildea, 2005) <papid> P05-1059 </papid>for word alignment are also studied.</nextsent>
<nextsent>in this paper, we present improvements to the hmm based alignment model originally proposed by (vogel et al., 1996, <papid> C96-2141 </papid>och and ney, 2000<papid> P00-1056 </papid>a).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1025">
<title id=" W07-0711.xml">using word dependent transition models in hmm based word alignment for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>mt results show that word alignment based on this method can be used in phrase-based machine translation system to yield up to 1% absolute improvement in bleu score, compared to conventional hmm, and 0.8% compared to ibm model 4 based word alignment.
</prevsent>
<prevsent>word alignment is an important step of most modern approaches to statistical machine translation (koehn et al, 2003).<papid> N03-1017 </papid></prevsent>
</prevsection>
<citsent citstr=" C96-2141 ">
the classical approaches to word alignment are based on ibm models 1-5 (brown et al, 1994) and the hmm based alignment model (vogel et al, 1996) (<papid> C96-2141 </papid>och and ney, 2000<papid> P00-1056 </papid>a, 2000b), while recently discriminative approaches (moore, 2006) and syntax based approaches (zhang and gildea, 2005) <papid> P05-1059 </papid>for word alignment are also studied.</citsent>
<aftsection>
<nextsent>in this paper, we present improvements to the hmm based alignment model originally proposed by (vogel et al., 1996, <papid> C96-2141 </papid>och and ney, 2000<papid> P00-1056 </papid>a).</nextsent>
<nextsent>although hmm based word alignment approaches give good performance, one weakness of it is the coarse transition models.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1026">
<title id=" W07-0711.xml">using word dependent transition models in hmm based word alignment for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>mt results show that word alignment based on this method can be used in phrase-based machine translation system to yield up to 1% absolute improvement in bleu score, compared to conventional hmm, and 0.8% compared to ibm model 4 based word alignment.
</prevsent>
<prevsent>word alignment is an important step of most modern approaches to statistical machine translation (koehn et al, 2003).<papid> N03-1017 </papid></prevsent>
</prevsection>
<citsent citstr=" P00-1056 ">
the classical approaches to word alignment are based on ibm models 1-5 (brown et al, 1994) and the hmm based alignment model (vogel et al, 1996) (<papid> C96-2141 </papid>och and ney, 2000<papid> P00-1056 </papid>a, 2000b), while recently discriminative approaches (moore, 2006) and syntax based approaches (zhang and gildea, 2005) <papid> P05-1059 </papid>for word alignment are also studied.</citsent>
<aftsection>
<nextsent>in this paper, we present improvements to the hmm based alignment model originally proposed by (vogel et al., 1996, <papid> C96-2141 </papid>och and ney, 2000<papid> P00-1056 </papid>a).</nextsent>
<nextsent>although hmm based word alignment approaches give good performance, one weakness of it is the coarse transition models.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1038">
<title id=" W07-0711.xml">using word dependent transition models in hmm based word alignment for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>mt results show that word alignment based on this method can be used in phrase-based machine translation system to yield up to 1% absolute improvement in bleu score, compared to conventional hmm, and 0.8% compared to ibm model 4 based word alignment.
</prevsent>
<prevsent>word alignment is an important step of most modern approaches to statistical machine translation (koehn et al, 2003).<papid> N03-1017 </papid></prevsent>
</prevsection>
<citsent citstr=" P05-1059 ">
the classical approaches to word alignment are based on ibm models 1-5 (brown et al, 1994) and the hmm based alignment model (vogel et al, 1996) (<papid> C96-2141 </papid>och and ney, 2000<papid> P00-1056 </papid>a, 2000b), while recently discriminative approaches (moore, 2006) and syntax based approaches (zhang and gildea, 2005) <papid> P05-1059 </papid>for word alignment are also studied.</citsent>
<aftsection>
<nextsent>in this paper, we present improvements to the hmm based alignment model originally proposed by (vogel et al., 1996, <papid> C96-2141 </papid>och and ney, 2000<papid> P00-1056 </papid>a).</nextsent>
<nextsent>although hmm based word alignment approaches give good performance, one weakness of it is the coarse transition models.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1064">
<title id=" W07-0711.xml">using word dependent transition models in hmm based word alignment for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a word dependent self-transition model p(stay|e) is introduced to decide whether to stay at the current source word at the next step, or jump to different word.
</prevsent>
<prevsent>it was also shown that with the assumption that source word with fertility greater than one generates consecutive words in the target language, this probability approximates fertility modeling.
</prevsent>
</prevsection>
<citsent citstr=" H05-1022 ">
deng and byrne in (2005) <papid> H05-1022 </papid>improved this idea.</citsent>
<aftsection>
<nextsent>they proposed word-to-phrase hmm in which source word dependent phrase length model is used to model the approximate fertility, i.e., the length of consecutive target words generated by the source word.
</nextsent>
<nextsent>it provides more powerful modeling of approximate fertility than the single p(stay|e) parameter.
</nextsent>
<nextsent>however, these methods only model the probability of state occupancy rather than full set of transition probabilities.
</nextsent>
<nextsent>important knowledge of jumping from to another position, e.g., jumping 80forward (monotonic alignment) or jumping backward (non-monotonic alignment), is not modeled.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1065">
<title id=" W07-0711.xml">using word dependent transition models in hmm based word alignment for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for this purpose, we estimate full transition model for each source word.
</prevsent>
<prevsent>a key problem for detailed word-dependent transition modeling is data sparsity.
</prevsent>
</prevsection>
<citsent citstr=" W02-1012 ">
in (toutanova et al, 2002), <papid> W02-1012 </papid>the word dependent self-transition probability p(stay|e) is interpolated with the global hmm self-transition probability to alleviate the data sparsity problem, where an interpolation weight is used for all words and that weight is tuned on hold-out set.</citsent>
<aftsection>
<nextsent>in the proposed word dependent transition model, because there are large number of parameters to estimate, the data sparsity problem is even more severe.
</nextsent>
<nextsent>moreover, since the sparsity of different words are very different, it is difficult to find one-size-fits-all interpolation weight, and therefore simple linear interpolation is not optimal.
</nextsent>
<nextsent>in order to address this problem, we use bayesian learning so that the transition model parameters are estimated by maximum posteriori (map) training.
</nextsent>
<nextsent>with the help of the prior distribution of the model, the training is regularized and results in robust models.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1105">
<title id=" W07-0711.xml">using word dependent transition models in hmm based word alignment for statistical machine translation </title>
<section> baseline hmm alignment model.  </section>
<citcontext>
<prevsection>
<prevsent>is mul tinomial distribution estimated according to (2), where at each iteration the distortion set {c(i - )} is the fractional count of transitions with jump width = - , i.e., 1 1 1 1 1 1 ( ) pr( , | , , ) i i j i d i i f ? + = = ?= = = + ???
</prevsent>
<prevsent>(4) where ?  is the model obtained from the immediate previous iteration and these terms in (4) can be efficiently computed by using the forward backward algorithm (rabiner 1989).
</prevsent>
</prevsection>
<citsent citstr=" N06-1014 ">
in practice, we can bucket the distortion parameters {c(d)} into few buckets as implemented in (liang et al, 2006).<papid> N06-1014 </papid></citsent>
<aftsection>
<nextsent>in our implementation, 15 buckets are used for c(?-7), c(-6), ... c(0), ..., c(7).
</nextsent>
<nextsent>the probability mass for transitions with jump width larger than 6 is uniformly divided.
</nextsent>
<nextsent>as suggested in (liang et al, 2006), <papid> N06-1014 </papid>we also use two separate sets of distortion parameters for transitioning into the first state, and for transitioning out of the last state, respectively.</nextsent>
<nextsent>finally, we further smooth transition probabilities with uniform distribution as described in (och and ney, 2000<papid> P00-1056 </papid>a), _ _ 1( | , ) (1 ) ( | , )j j jp a p a ii?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1153">
<title id=" W07-0711.xml">using word dependent transition models in hmm based word alignment for statistical machine translation </title>
<section> experimental results.  </section>
<citcontext>
<prevsection>
<prevsent>all numbers are in percentage.
</prevsent>
<prevsent>84 5.2 machine translation on europarl corpus.
</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
we further tested our wdhmm on phrase-based machine translation system to see whether our improvement on word alignment can also improve mt accuracy measured by bleu score (papineni et al, 2002).<papid> P02-1040 </papid></citsent>
<aftsection>
<nextsent>the machine translation experiment was conducted on the english-to-french track of naacl 2006 europarl evaluation workshop.
</nextsent>
<nextsent>the supplied training corpus contains 688k sentence pairs.
</nextsent>
<nextsent>text data are already tokenized.
</nextsent>
<nextsent>in our experiment, we first lower-cased all text, then word clustering was performed to cluster words of english and french into 32 word classes respectively using the tool provided by (j. goodman).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1158">
<title id=" W07-0711.xml">using word dependent transition models in hmm based word alignment for statistical machine translation </title>
<section> experimental results.  </section>
<citcontext>
<prevsection>
<prevsent>they are direct maximum likelihood estimate of the probability of target phrase given source phrase, and the same estimate of source given target; we also compute the lexicon weighting features for source given target and target given source, respectively.
</prevsent>
<prevsent>other models include word count and phrase count, and 3-gram language model provided by the workshop.
</prevsent>
</prevsection>
<citsent citstr=" P02-1038 ">
these models are combined in log-linear framework with different weights (och and ney, 2002).<papid> P02-1038 </papid></citsent>
<aftsection>
<nextsent>the model weight vector is trained on dev set with 2000 english sentences, each of which has one french translation reference.
</nextsent>
<nextsent>in the experiment, only the first 500 sentences were used to train the log-linear model weight vector, where minimum error rate (mer) training was used (och, 2003).<papid> P03-1021 </papid></nextsent>
<nextsent>after mer training, the weight vector that gives the best accuracy on the development set was se lected.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1159">
<title id=" W07-0711.xml">using word dependent transition models in hmm based word alignment for statistical machine translation </title>
<section> experimental results.  </section>
<citcontext>
<prevsection>
<prevsent>these models are combined in log-linear framework with different weights (och and ney, 2002).<papid> P02-1038 </papid></prevsent>
<prevsent>the model weight vector is trained on dev set with 2000 english sentences, each of which has one french translation reference.</prevsent>
</prevsection>
<citsent citstr=" P03-1021 ">
in the experiment, only the first 500 sentences were used to train the log-linear model weight vector, where minimum error rate (mer) training was used (och, 2003).<papid> P03-1021 </papid></citsent>
<aftsection>
<nextsent>after mer training, the weight vector that gives the best accuracy on the development set was selected.
</nextsent>
<nextsent>we then applied it to tests.
</nextsent>
<nextsent>there are 2000 sentences in the development-test set devtest, 2000 sentences in test set test, and 1064 out-of-domain sentences called nc-test.
</nextsent>
<nextsent>the pharaoh phrase-based decoder (koehn 2004<papid> W04-3250 </papid>b) was used for decoding.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1160">
<title id=" W07-0711.xml">using word dependent transition models in hmm based word alignment for statistical machine translation </title>
<section> experimental results.  </section>
<citcontext>
<prevsection>
<prevsent>we then applied it to tests.
</prevsent>
<prevsent>there are 2000 sentences in the development-test set devtest, 2000 sentences in test set test, and 1064 out-of-domain sentences called nc-test.
</prevsent>
</prevsection>
<citsent citstr=" W04-3250 ">
the pharaoh phrase-based decoder (koehn 2004<papid> W04-3250 </papid>b) was used for decoding.</citsent>
<aftsection>
<nextsent>the maximum re-ordering limit for decoding was set to 7.
</nextsent>
<nextsent>we used default settings for all other parameters.
</nextsent>
<nextsent>we present bleu scores of mt systems using different word alignments on all three test sets, where fig 2 shows bleu scores of the two in domain tests, and fig 3 shows mt results on the out-of-domain test set.
</nextsent>
<nextsent>in testing, the prior parameter ? of wdhmm was varied in the range of [20, 5000].
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1175">
<title id=" W06-1605.xml">distributional measures of concept distance a task oriented evaluation </title>
<section> semantic and distributional measures.  </section>
<citcontext>
<prevsection>
<prevsent>theco-occurring words may be all those in predetermined window around the target, or may be restricted to those that have certain syntactic (e.g.,verbobject) or semantic (e.g., agenttheme) relation with the target word.
</prevsent>
<prevsent>we will refer to the former kind of dps as relation-free.
</prevsent>
</prevsection>
<citsent citstr=" J90-1003 ">
usually in1in our experiments, we set negative pmi values to 0, be cause church and hanks (1990), <papid> J90-1003 </papid>in their seminal paper on word association ratio, show that negative pmi values are not expected to be accurate unless co-occurrence counts are made from an extremely large corpus.</citsent>
<aftsection>
<nextsent>35 table 1: measures of dp distance and measures of strength of association.
</nextsent>
<nextsent>dp distance strength of association ?-skew divergence conditional probability cosine pointwise mutual information jensen shannon divergence linthe latter case, separate association values are calculated for each of the different relations between the target and the co-occurring units.
</nextsent>
<nextsent>we will refer to such dps as relation-constrained.
</nextsent>
<nextsent>typical relation-free dps are those of schutze and pedersen (1997) and yoshida et al (2003).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1176">
<title id=" W06-1605.xml">distributional measures of concept distance a task oriented evaluation </title>
<section> semantic and distributional measures.  </section>
<citcontext>
<prevsection>
<prevsent>we will refer to such dps as relation-constrained.
</prevsent>
<prevsent>typical relation-free dps are those of schutze and pedersen (1997) and yoshida et al (2003).
</prevsent>
</prevsection>
<citsent citstr=" P98-2127 ">
typical relation-constrained dps are those of lin (1998) <papid> P98-2127 </papid>and lee (2001).</citsent>
<aftsection>
<nextsent>below are contrived, but plausible, examples of each for the word pulse; the numbers are conditional probabilities.
</nextsent>
<nextsent>relation-free dp pulse: beat (.28), racing (.2), grow (.13), beans (.09), heart (.04), . . .
</nextsent>
<nextsent>relation-constrained dp pulse:  beat, subject verb  (.34),  racing, noun qualifying adjective  (.22),  grow, subject verb  (.14), . . .
</nextsent>
<nextsent>the distance between two words, given their dps, is calculated using measure of dp distance, such as cosine.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1179">
<title id=" W06-1605.xml">distributional measures of concept distance a task oriented evaluation </title>
<section> conceptual grain size and storage.  </section>
<citcontext>
<prevsection>
<prevsent>36
</prevsent>
<prevsent>requirements as applications for linguistic distance become more sophisticated and demanding, it becomes attractive to pre-compute and store the distance values between all possible pairs of words or senses.but both kinds of measures have large space requirements to do this, requiring matrices of sizen n, where is the size of the vocabulary (per haps 100,000 for most languages) in the case of distributional measures and the number of senses (75,000 just for nouns in wordnet) in the case of semantic measures.
</prevsent>
</prevsection>
<citsent citstr=" C92-2070 ">
it is generally accepted, however, that wordnet senses are far too fine-grained (agirre and lopez de lacalle lekuona (2003) and citations therein).on the other hand, published thesauri, such as ro gets and macquarie, group near-synonymous and semantically related words into relatively small number of categories typically between 800 and 1100that roughly correspond to very coarse concepts or senses (yarowsky, 1992).<papid> C92-2070 </papid></citsent>
<aftsection>
<nextsent>words with more than one sense are listed in more than one category.
</nextsent>
<nextsent>a published thesaurus thus provides uswith very coarse human-developed set or inventory of word senses or concepts2 that are more intuitive and discernible than the concepts?
</nextsent>
<nextsent>generated by dimensionality-reduction methods such as latent semantic analysis.
</nextsent>
<nextsent>using coarse senses from known inventory means that the senses can be represented unambiguously by large number of possibly ambiguous words (conveniently available in the thesaurus)a feature that we exploited in our earlier work (mohammad and hirst, 2006) <papid> E06-1016 </papid>to determine useful estimates of the strength of association between concept and co-occurring words.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1180">
<title id=" W06-1605.xml">distributional measures of concept distance a task oriented evaluation </title>
<section> conceptual grain size and storage.  </section>
<citcontext>
<prevsection>
<prevsent>a published thesaurus thus provides uswith very coarse human-developed set or inventory of word senses or concepts2 that are more intuitive and discernible than the concepts?
</prevsent>
<prevsent>generated by dimensionality-reduction methods such as latent semantic analysis.
</prevsent>
</prevsection>
<citsent citstr=" E06-1016 ">
using coarse senses from known inventory means that the senses can be represented unambiguously by large number of possibly ambiguous words (conveniently available in the thesaurus)a feature that we exploited in our earlier work (mohammad and hirst, 2006) <papid> E06-1016 </papid>to determine useful estimates of the strength of association between concept and co-occurring words.</citsent>
<aftsection>
<nextsent>in this paper, we go one step further and usethe idea of very coarse sense inventory to develop framework for distributional measures of concepts that can more naturally and more accurately be used in place of semantic measures of word senses.
</nextsent>
<nextsent>we use the macquarie thesaurus (bernard, 1986) as sense inventory and repository of words pertaining to each sense.
</nextsent>
<nextsent>it has 812 categories with around 176,000 word tokens and 98,000 word types.
</nextsent>
<nextsent>this allows us to have much smaller concept concept distance matrices of size just 812 812 (roughly .01% the size 2we use the terms senses and concepts interchangeably.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1182">
<title id=" W06-1605.xml">distributional measures of concept distance a task oriented evaluation </title>
<section> conceptual grain size and storage.  </section>
<citcontext>
<prevsection>
<prevsent>it has 812 categories with around 176,000 word tokens and 98,000 word types.
</prevsent>
<prevsent>this allows us to have much smaller concept concept distance matrices of size just 812 812 (roughly .01% the size 2we use the terms senses and concepts interchangeably.
</prevsent>
</prevsection>
<citsent citstr=" J05-2004 ">
this is in contrast to studies, such as that of cooper (2005), <papid> J05-2004 </papid>that attempt to make principled distinction between them.</citsent>
<aftsection>
<nextsent>of matrices required by existing measures).
</nextsent>
<nextsent>we evaluate our distributional concept-distance measures on two tasks: ranking word pairs in orderof their semantic distance, and correcting real word spelling errors.
</nextsent>
<nextsent>we compare performance with distributional word-distance measures and the wordnet-based concept-distance measures.
</nextsent>
<nextsent>concept-distance 4.1 capturing distributional profiles of.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1188">
<title id=" W06-1605.xml">distributional measures of concept distance a task oriented evaluation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>however, no matter which of the two is chosen as the bottom-line performance statistic,the results show that the newly proposed distributional concept-distance measures are clearly superior to word-distance measures.
</prevsent>
<prevsent>further, of all the wordnet-based measures, only that proposed by jiang and conrath outperforms the best distributional concept-distance measures consistently with respect to both bottom-line statistics.
</prevsent>
</prevsection>
<citsent citstr=" W06-2501 ">
patwardhan and pedersen (2006) <papid> W06-2501 </papid>create aggregate co-occurrence vectors for wordnet sense by adding the co-occurrence vectors of the words in its wordnet gloss.</citsent>
<aftsection>
<nextsent>the distance between two senses is then determined by the cosine of the an 41 gle between their aggregate vectors.
</nextsent>
<nextsent>however, as we pointed out in mohammad and hirst (2005), such aggregate co-occurrence vectors are expected to be noisy because they are created from data that is not sense-annotated.
</nextsent>
<nextsent>therefore, we employed simple word sense disambiguation and bootstrapping techniques on our base wccm to create more-accurate co-occurrence vectors, which gave markedly higher accuracies in the task of determining word sense dominance.
</nextsent>
<nextsent>in the experiments described in this paper, we used these boot strapped co-occurrence vectors to determine concept-distance.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1189">
<title id=" W06-1605.xml">distributional measures of concept distance a task oriented evaluation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>therefore, we employed simple word sense disambiguation and bootstrapping techniques on our base wccm to create more-accurate co-occurrence vectors, which gave markedly higher accuracies in the task of determining word sense dominance.
</prevsent>
<prevsent>in the experiments described in this paper, we used these boot strapped co-occurrence vectors to determine concept-distance.
</prevsent>
</prevsection>
<citsent citstr=" P05-1016 ">
pantel (2005) <papid> P05-1016 </papid>also provides way to create co-occurrence vectors for wordnet senses.</citsent>
<aftsection>
<nextsent>the lexical co-occurrence vectors of words in leaf node are propagated up the wordnet hierarchy.
</nextsent>
<nextsent>a parent node inherits those co-occurrences that are shared by its children.
</nextsent>
<nextsent>lastly, co-occurrences not pertaining to the leaf nodes are removed fromits vector.
</nextsent>
<nextsent>even though the methodology attempts at associating wordnet node or sense with only those co-occurrences that pertain to it, no attempt is made at correcting the frequencycounts.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1190">
<title id=" W06-2208.xml">expanding the recall of relation extraction by bootstrapping </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>relation extraction is task to extract tu ples of entities that satisfy given relation from textual documents.
</prevsent>
<prevsent>examples of relations include ceoof(company, ceo) and acquisi tion(organization, organization).
</prevsent>
</prevsection>
<citsent citstr=" P05-1052 ">
there has been much work on relation extraction; most of it employs knowledge engineering or supervised machine learning approaches (feldman et al, 2002; zhao and grishman, 2005).<papid> P05-1052 </papid></citsent>
<aftsection>
<nextsent>both approaches are labor intensive.
</nextsent>
<nextsent>we begin with baseline information extraction system, knowitall (etzioni et al, 2005), that does unsupervised information extraction at web scale.knowitall uses set of generic extraction patterns, and automatically instantiates rules by combining these patterns with user supplied relation labels.
</nextsent>
<nextsent>for example, knowitall has patterns for generic of?
</nextsent>
<nextsent>relation: np1  relation
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1191">
<title id=" W06-2208.xml">expanding the recall of relation extraction by bootstrapping </title>
<section> string pattern learning (spl).  </section>
<citcontext>
<prevsection>
<prevsent>spl tabulates the occurrence of such pattern sin the set of positive training sentences (all sentences from the reservoir that contain both argument values from seed tuple in either order), and also tabulates their occurrence in negative training sentences.
</prevsent>
<prevsent>the negative training are sentences that have one argument value from seed tuple and nearest simple np in place of the other argumentvalue.
</prevsent>
</prevsection>
<citsent citstr=" P02-1006 ">
this idea is based on that of (ravichan dran and hovy, 2002) <papid> P02-1006 </papid>for qa system.</citsent>
<aftsection>
<nextsent>spl learns possibly large set of strict extraction rules that have alternating context strings and extraction slots, with no gaps or wild cards in the rules.
</nextsent>
<nextsent>spl selects the best patterns as follows: 1.
</nextsent>
<nextsent>groups the context strings that have the exact.
</nextsent>
<nextsent>same middle string.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1192">
<title id=" W06-2208.xml">expanding the recall of relation extraction by bootstrapping </title>
<section> less restrictive pattern learning.  </section>
<citcontext>
<prevsection>
<prevsent>tagging, given sequence of tokens, it automatically generates sequence of feature sets; each set is corresponding to token.
</prevsent>
<prevsent>it can incorporate any properties that can be represented as binary feature into the model, such as words, capitalized patterns, part-of-speech tags and the existence of the word in dictionary.
</prevsent>
</prevsection>
<citsent citstr=" W03-0430 ">
it works quite well on ne tagging tasks (mccallum and li, 2003).<papid> W03-0430 </papid></citsent>
<aftsection>
<nextsent>3.1.2 how to train supervised ner in bootstrapping we use bootstrapping to train crf for relation specific ne tagging as follows: 1) select the sentences that include all the entity values of seed tuple, 2) automatically mark the argument values in each sentence, and 3)train crf on the seed marked sentences.
</nextsent>
<nextsent>an example of seed marked sentence is the following: seed tuple:  microsoft, steve ballmer  seed marked sentence:  yesterday,  arg2 steve ballmer /arg2 , ceo of  arg1 microsoft /arg1  announced that ... because of redundancy, we can expect to generate fairly large number of seed marked sentences by using few highly frequent seed tuples.to avoid over fitting on terms from these seed tu ples, we substitute the actual argument values with random characters for each training sentence, preserving capitalization patterns and number of characters in each token.
</nextsent>
<nextsent>3.2 relation assessor.
</nextsent>
<nextsent>relation assessor employs several snowball-like techniques including making rules by clustering and em-estimation for the confidence of the rules and tuples.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1193">
<title id=" W06-2208.xml">expanding the recall of relation extraction by bootstrapping </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>in the em algorithm, the match score
</prevsent>
<prevsent>between learned pattern  and tuple  is set to constant   .lrpl uses minor third (cohen, 2004) implementation of crf for relation ner.
</prevsent>
</prevsection>
<citsent citstr=" H05-1056 ">
the features used in the experiments are the lower-case word,capitalize pattern, part of speech tag of the current and +-2 tokens, and the previous state (tag) referring to (minkov et al, 2005; <papid> H05-1056 </papid>rosenfeld et al, 2005).</citsent>
<aftsection>
<nextsent>the parameters used for spl and lrpl are experimentally set as follows:     ,     ,
</nextsent>
<nextsent>  ,   ,    and the context weights for lrpl shown in table 1.
</nextsent>
<nextsent>figure 2-6 show the recall-precision curves.
</nextsent>
<nextsent>we use the number of correct extractions to serve asa surrogate for recall, since computing actual recall would require extensive manual inspection of the large datasets.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1197">
<title id=" W06-2208.xml">expanding the recall of relation extraction by bootstrapping </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>lrpl is similar to snowball(agichtein, 2005), which employs generic ner, and reported that most errors come from ner errors.
</prevsent>
<prevsent>because our evaluation showed that relation ner works better than generic ner, combination of relation nerand snowball can make better result in other settings.
</prevsent>
</prevsection>
<citsent citstr=" W99-0613 ">
3 (collins and singer, 1999) <papid> W99-0613 </papid>and (jones, 2005) describe self-training and co-training methods for named entity classification.</citsent>
<aftsection>
<nextsent>however, the problem of nec task, where the boundary of entities are given by np chunker or parser, is different from ne tagging task.
</nextsent>
<nextsent>because the boundary of an entity is often different from np boundary, the technique can not be used for our purpose; microsoft ceo steve ballmer?
</nextsent>
<nextsent>is tagged as single noun phrase.
</nextsent>
<nextsent>this paper describes two bootstrapping strategies, spl, which learns simple string patterns, and lrpl, which trains relation ner and uses it with less restrictive patterns.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1198">
<title id=" W06-2407.xml">extending corpus based identification of light verb constructions using a supervised learning framework </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the use of this complementary information improves lvc identification, as it models the inherent bias of some verbs to be used more often as light verbs than others.
</prevsent>
<prevsent>let f(v, n) be the count of verb-object pairs occurring in the corpus, such that is the verb, is deverbal noun.
</prevsent>
</prevsection>
<citsent citstr=" W04-0401 ">
then, the most probable light verb for is given by: dj96(n) = argmax f(v, n) ? f(v, n) (2)stevenson et al (2004)<papid> W04-0401 </papid>s research examines evidence from constructions featuring determiners.</citsent>
<aftsection>
<nextsent>they focused on expressions of the form v-a-nand v-det-n, where is light verb, is de verbal noun, is an indefinite determiner (namely, a? or an?), and det is any determiner other than the indefinite.
</nextsent>
<nextsent>examples of such constructions are give speech?
</nextsent>
<nextsent>and take walk?.
</nextsent>
<nextsent>they employ mutual information which measures the frequency of co-occurrences of two variables, corrected for random agreement.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1199">
<title id=" W06-2407.xml">extending corpus based identification of light verb constructions using a supervised learning framework </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>also, they suggested that the determiner the?
</prevsent>
<prevsent>be excluded from the development data since it frequently occurred in their data.
</prevsent>
</prevsection>
<citsent citstr=" W05-1005 ">
recently, fazly et al (2005) <papid> W05-1005 </papid>have proposed statistical measure for the detection of lvcs.</citsent>
<aftsection>
<nextsent>the probability that verb-object pair v-n (where is light verb) is lvc can be expressed as product of three probabilities: (1) probability of the object 50 occurring in the corpus, (2) the probability that is part of any lvc given n, and (3) the probability of occurring given and that v-n is lvc.
</nextsent>
<nextsent>each of these three probabilities can then be estimated by the frequency of occurrence in the corpus, using the assumption that all instances of v?-a-n is alvc, where v?
</nextsent>
<nextsent>is any light verb and is an indefinite determiner.
</nextsent>
<nextsent>to summarize, research in lvc detection started by developing single measures that utilized simple frequency counts of verbs and their complements.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1208">
<title id=" W06-3125.xml">ngrambased smt system enhanced with reordering patterns </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for both spanish-english translation directions and the english-to-frenchtranslation task, the baseline system allows for linguistically motivated source side reorderings.
</prevsent>
<prevsent>the statistical machine translation approach used in this work implements log-linear combination of feature functions along with translation model which is based on bilingual n-grams (de gispert and marino, 2002).
</prevsent>
</prevsection>
<citsent citstr=" N03-1017 ">
this translation model differs from the well known phrase-based translation approach (koehn et al , 2003) <papid> N03-1017 </papid>in two basic issues: first, training data is monotonously segmented into bilingual units; and second, the model considers n-gram probabilities instead of relative frequencies.</citsent>
<aftsection>
<nextsent>this translation approach is described in detail in (marino et al , 2005).for those translation tasks with spanish or english as target language, an additional tagged (us ing pos information) target language model is used.
</nextsent>
<nextsent>additionally reordering strategy that includes pos information is described and evaluated.
</nextsent>
<nextsent>translation results for all six translation directions proposed in the shared task are presented and discussed.
</nextsent>
<nextsent>both translation directions are considered for the pairs: english-spanish, english-french, and english-german.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1209">
<title id=" W06-3125.xml">ngrambased smt system enhanced with reordering patterns </title>
<section> baseline n-gram-based smt system.  </section>
<citcontext>
<prevsection>
<prevsent>based on the number of target words in the partial-translation hypothesis,to compensate the lm preference for short sentences.
</prevsent>
<prevsent>a source-to-target lexicon model.
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
based on ibm model 1 lexical parameters(brown et al , 1993), <papid> J93-2003 </papid>providing complementary probability for each tuple in the translation table.</citsent>
<aftsection>
<nextsent>these parameters are obtained from source-to-target al ignments.?
</nextsent>
<nextsent>a target-to-source lexicon model.
</nextsent>
<nextsent>analogous to the previous feature, but obtained from target-to-source alignments.
</nextsent>
<nextsent>a tagged (pos) target language model.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1210">
<title id=" W06-3125.xml">ngrambased smt system enhanced with reordering patterns </title>
<section> baseline n-gram-based smt system.  </section>
<citcontext>
<prevsection>
<prevsent>this baseline system is actually very similar tothe system used for last years shared task exploiting parallel texts for statistical machine translation?
</prevsent>
<prevsent>of acl05 workshop on building and using parallel texts: data-driven machine translation and beyond (banchs et al , 2005), whose results are available at: http://www.statmt.org/wpt05/ mt-shared-task/.
</prevsent>
</prevsection>
<citsent citstr=" A00-1031 ">
a more detailed description of the system can be found in (2005).the tools used for pos-tagging were freel ing (carreras et al , 2004) for spanish andtnt (brants, 2000) <papid> A00-1031 </papid>for english.</citsent>
<aftsection>
<nextsent>all language models were estimated using the sri language modeling toolkit.
</nextsent>
<nextsent>word-to-word alignments were extracted with giza++.
</nextsent>
<nextsent>improvements in word-to word alignments were achieved through verb group classification as described in (de gispert, 2005).
</nextsent>
<nextsent>in this section we outline the reordering framework used for the experiments (crego and marino, 2006).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1211">
<title id=" W06-3604.xml">allword prediction as the ultimate con fusible disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(even-zohar and roth, 2000).
</prevsent>
<prevsent>it is usually not an engineering end in itself to predict the next word in sequence, or fill in blanked-out word in sequence.
</prevsent>
</prevsection>
<citsent citstr=" P01-1005 ">
yet, it could be an asset in higher-level proofing or authoring tools, e.g. to be able to automatically discern among confusables and thereby to detect con fusable errors (golding and roth, 1999; even-zohar and roth, 2000; banko and brill, 2001; <papid> P01-1005 </papid>huang and powers, 2001).</citsent>
<aftsection>
<nextsent>it could alleviate problems with low frequency and unknown words in natural language processing and information retrieval, by replacing them with likely and higher-frequency alternatives that carry similar information.
</nextsent>
<nextsent>and also, since the task of word prediction is direct interpretation of language modeling, word prediction system could provide useful information for to be used in speech recognition systems.
</nextsent>
<nextsent>a unique aspect of the word prediction task, as compared to most other tasks in natural language processing, is that real-world examples abound in large amounts.
</nextsent>
<nextsent>any digitized text can be used as training material for word prediction system capable of learning from examples, and nowadays gigas cale and tera scale document collections are available for research purposes.a specific type of word prediction is conf usable prediction, i.e., learn to predict among limited sets of conf usable words such as to/two/too and there/their/theyre (golding and roth, 1999; bankoand brill, 2001).<papid> P01-1005 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1213">
<title id=" W06-3604.xml">allword prediction as the ultimate con fusible disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>prediction is to encode context only by words, and not by any higher-level linguistic non-terminals which have been investigated in related work on word prediction (wu et al, 1999; even-zohar and roth, 2000).
</prevsent>
<prevsent>this choice leaves open the question how the same tasks can be learned from examples when non-terminal symbols are taken into account as well.the choice for our algorithm, decision-tree approximation of k-nearest-neigbor (k-nn) based or memory-based learning, is motivated by the fact that, as we describe later in this paper, this particular algorithm can scale up to predicting tens of thousands of words, while simultaneously being able to scale up to tens of millions of examples as training material, predicting words at useful rates of hundreds to thousands of words per second.
</prevsent>
</prevsection>
<citsent citstr=" P97-1056 ">
another motivation for our choice is that our decision-tree approximation of k-nearest neighbor classification is functionally equivalent to back-off smoothing (za vrel and daelemans, 1997); <papid> P97-1056 </papid>not only does it share its performance capacities with n-gram models withback-off smoothing, it also shares its scaling abilities with these models, while being able to handle large values of n. the article is structured as follows.</citsent>
<aftsection>
<nextsent>in section 2we describe what data we selected for our experiments, and we provide an overview of the experimental methodology used throughout the experiments, including description of the igtree algorithm central to our study.
</nextsent>
<nextsent>in section 3 the results of the word prediction experiments are presented, andthe subsequent section 4 contains the experimental results of the experiments on confusables.
</nextsent>
<nextsent>we briefly relate our work to earlier work that inspired the current study in section 5.
</nextsent>
<nextsent>the results are discussed, and conclusions are drawn in section 6.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1214">
<title id=" W06-3604.xml">allword prediction as the ultimate con fusible disambiguation </title>
<section> data preparation and experimental.  </section>
<citcontext>
<prevsection>
<prevsent>additionally, we selected two test sets taken from different corpora.
</prevsent>
<prevsent>first, we used the project gutenberg2 version of the novel alices adventures in wonderland by lewis carroll (carroll, 1865), henceforth alice.
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
as the third test set we selected all tokens of the brown corpus part of the penn tree bank (marcus et al, 1993), <papid> J93-2004 </papid>selected portion of the original one-million word brown corpus (kuceraand francis, 1967), collection of samples of american english in many different genres, from sources printed in 1961; we refer to this test set as brown.</citsent>
<aftsection>
<nextsent>in sum, we have three test sets, covering texts from the same genre and source as the training data, fictional novel, and mix of genres wider than the training set.
</nextsent>
<nextsent>table 1 summarizes the key training and test setstatistics.
</nextsent>
<nextsent>as the table shows, the cross-domain coverages for unigrams and bigrams are rather low; not only are these numbers the best-case performance ceilings, they also imply that lot of contextual information used by the machine learning method used in this paper will be partly unknown to the learner, especially in texts from other domains than the training set.
</nextsent>
<nextsent>1for availability of the reuters corpus, see http://about.reuters.com/researchandstandards/corpus/.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1225">
<title id=" W06-3604.xml">allword prediction as the ultimate con fusible disambiguation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>golding and roth illustrate that multiplicative weight-updating algorithms such as winnow can deal with immense input feature spaces, where for each single classification only small number of features is actually relevant (golding and roth, 1999).
</prevsent>
<prevsent>with igtree we have an arguably competitive efficient, but one-shotlearning algorithm; igtree does not need an iterative procedure to set weights, and can also handle large feature space.
</prevsent>
</prevsection>
<citsent citstr=" P94-1013 ">
instead of viewing all positional features as containers of thousands of atomic word features, it treats the positional features as the basic tests, branching on the word values in the tree.more generally, as precursor to the abovementioned work, conf usable disambiguation has been investigated in string of papers discussing the application of various machine learning algorithms to the task (yarowsky, 1994; <papid> P94-1013 </papid>golding, 1995; <papid> W95-0104 </papid>mangu 31 and brill, 1997; huang and powers, 2001).</citsent>
<aftsection>
<nextsent>in this article we explored the scaling abilities ofigtree, simple decision-tree algorithm with favorable asymptotic complexities with respect to multi-label classification tasks.
</nextsent>
<nextsent>igtree is applied to word prediction, task for which virtually unlimited amounts of training examples are available, with very large amounts of predictable class labels; and conf usable disambiguation, specialization of word prediction focusing on small sets of confusablewords.
</nextsent>
<nextsent>best results are 42.2% correctly predicted tokens (words and punctuation markers) when training and testing on data from the reuters newswire cor pus; and conf usable disambiguation accuracies of well above 90%.
</nextsent>
<nextsent>memory requirements and speeds were shown to be realistic.analysing the results of the learning curve experiments with increasing amounts of training examples, we observe that better word prediction accuracy canbe attained simply by adding more training examples, and that the progress inaccuracy proceeds at log-linear rate.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1226">
<title id=" W06-3604.xml">allword prediction as the ultimate con fusible disambiguation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>golding and roth illustrate that multiplicative weight-updating algorithms such as winnow can deal with immense input feature spaces, where for each single classification only small number of features is actually relevant (golding and roth, 1999).
</prevsent>
<prevsent>with igtree we have an arguably competitive efficient, but one-shotlearning algorithm; igtree does not need an iterative procedure to set weights, and can also handle large feature space.
</prevsent>
</prevsection>
<citsent citstr=" W95-0104 ">
instead of viewing all positional features as containers of thousands of atomic word features, it treats the positional features as the basic tests, branching on the word values in the tree.more generally, as precursor to the abovementioned work, conf usable disambiguation has been investigated in string of papers discussing the application of various machine learning algorithms to the task (yarowsky, 1994; <papid> P94-1013 </papid>golding, 1995; <papid> W95-0104 </papid>mangu 31 and brill, 1997; huang and powers, 2001).</citsent>
<aftsection>
<nextsent>in this article we explored the scaling abilities ofigtree, simple decision-tree algorithm with favorable asymptotic complexities with respect to multi-label classification tasks.
</nextsent>
<nextsent>igtree is applied to word prediction, task for which virtually unlimited amounts of training examples are available, with very large amounts of predictable class labels; and conf usable disambiguation, specialization of word prediction focusing on small sets of confusablewords.
</nextsent>
<nextsent>best results are 42.2% correctly predicted tokens (words and punctuation markers) when training and testing on data from the reuters newswire cor pus; and conf usable disambiguation accuracies of well above 90%.
</nextsent>
<nextsent>memory requirements and speeds were shown to be realistic.analysing the results of the learning curve experiments with increasing amounts of training examples, we observe that better word prediction accuracy canbe attained simply by adding more training examples, and that the progress inaccuracy proceeds at log-linear rate.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1227">
<title id=" W06-3104.xml">quasi synchronous grammars alignment by soft projection of syntactic dependencies </title>
<section> motivation and related work.  </section>
<citcontext>
<prevsection>
<prevsent>equation (1) does not assume that t1 and t2 are isomorphic.
</prevsent>
<prevsent>for example, model might judge t2 and to be likely, given t1, provided that manybut not necessarily allof the syntactic dependencies in t1 are aligned with corresponding dependencies in t2.
</prevsent>
</prevsection>
<citsent citstr=" P02-1050 ">
hwa et al (2002) <papid> P02-1050 </papid>found that human translations from chinese to english preserved only 3942% of the unlabeled chinese dependen cies.</citsent>
<aftsection>
<nextsent>they increased this figure to 67% by using more involved heuristics for aligning dependencies across these two languages.
</nextsent>
<nextsent>that suggests that (1)should be defined to consider more than one dependency at time.
</nextsent>
<nextsent>this inspires the key novel feature of our models: does not have to be well-behaved?
</nextsent>
<nextsent>syntacticalignment.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1228">
<title id=" W06-3104.xml">quasi synchronous grammars alignment by soft projection of syntactic dependencies </title>
<section> motivation and related work.  </section>
<citcontext>
<prevsection>
<prevsent>any portion of t2 can align to any portion of t1, or to null.
</prevsent>
<prevsent>nodes that are syntactically related in t1 do not have to translate into nodes thatare syntactically related in t2although (1) is usually higher if they do.
</prevsent>
</prevsection>
<citsent citstr=" J94-4004 ">
this property makes our approach especially promising for aligning freely, or erroneously, translated sentences, and for coping with syntactic diver 23gences observed between even closely related languages (dorr, 1994; <papid> J94-4004 </papid>fox, 2002).<papid> W02-1039 </papid></citsent>
<aftsection>
<nextsent>we can patch together an alignment without accounting for all the details of the translation process.
</nextsent>
<nextsent>for instance, perhaps source np (figure 1) or pp (figure 2) appears out of place?
</nextsent>
<nextsent>in the target sentence.
</nextsent>
<nextsent>a linguist might account for the position of the pp auf diese frage either syntactically (by invoking scrambling)or semantically (by describing deep analysis transfer-synthesis process in the translators head).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1229">
<title id=" W06-3104.xml">quasi synchronous grammars alignment by soft projection of syntactic dependencies </title>
<section> motivation and related work.  </section>
<citcontext>
<prevsection>
<prevsent>any portion of t2 can align to any portion of t1, or to null.
</prevsent>
<prevsent>nodes that are syntactically related in t1 do not have to translate into nodes thatare syntactically related in t2although (1) is usually higher if they do.
</prevsent>
</prevsection>
<citsent citstr=" W02-1039 ">
this property makes our approach especially promising for aligning freely, or erroneously, translated sentences, and for coping with syntactic diver 23gences observed between even closely related languages (dorr, 1994; <papid> J94-4004 </papid>fox, 2002).<papid> W02-1039 </papid></citsent>
<aftsection>
<nextsent>we can patch together an alignment without accounting for all the details of the translation process.
</nextsent>
<nextsent>for instance, perhaps source np (figure 1) or pp (figure 2) appears out of place?
</nextsent>
<nextsent>in the target sentence.
</nextsent>
<nextsent>a linguist might account for the position of the pp auf diese frage either syntactically (by invoking scrambling)or semantically (by describing deep analysis transfer-synthesis process in the translators head).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1230">
<title id=" W06-3104.xml">quasi synchronous grammars alignment by soft projection of syntactic dependencies </title>
<section> motivation and related work.  </section>
<citcontext>
<prevsection>
<prevsent>transfer.
</prevsent>
<prevsent>1.2 from synchronous to quasi-synchronous.
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
grammars because our approach will let anything align to anything, it is reminiscent of ibm models 15(brown et al, 1993).<papid> J93-2003 </papid></citsent>
<aftsection>
<nextsent>it differs from the many approaches where (1) is defined by stochastic synchronous grammar (wu, 1997; <papid> J97-3002 </papid>alshawi et al, 2000; <papid> J00-1004 </papid>yamada and knight, 2001; <papid> P01-1067 </papid>eisner, 2003; <papid> P03-2041 </papid>gildea,2003; <papid> P03-1011 </papid>melamed, 2004) <papid> P04-1083 </papid>and from transfer-based systems defined by context-free grammars (lavie et al, 2003).</nextsent>
<nextsent>the synchronous grammar approach, originally due to shieber and schabes (1990), <papid> C90-3045 </papid>supposes that t2 is generated in lockstep to t1.1 when choosing how to expand certain vp node in t2, synchronous cfg process would observe that this node is aligned to node vp?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1231">
<title id=" W06-3104.xml">quasi synchronous grammars alignment by soft projection of syntactic dependencies </title>
<section> motivation and related work.  </section>
<citcontext>
<prevsection>
<prevsent>1.2 from synchronous to quasi-synchronous.
</prevsent>
<prevsent>grammars because our approach will let anything align to anything, it is reminiscent of ibm models 15(brown et al, 1993).<papid> J93-2003 </papid></prevsent>
</prevsection>
<citsent citstr=" J97-3002 ">
it differs from the many approaches where (1) is defined by stochastic synchronous grammar (wu, 1997; <papid> J97-3002 </papid>alshawi et al, 2000; <papid> J00-1004 </papid>yamada and knight, 2001; <papid> P01-1067 </papid>eisner, 2003; <papid> P03-2041 </papid>gildea,2003; <papid> P03-1011 </papid>melamed, 2004) <papid> P04-1083 </papid>and from transfer-based systems defined by context-free grammars (lavie et al, 2003).</citsent>
<aftsection>
<nextsent>the synchronous grammar approach, originally due to shieber and schabes (1990), <papid> C90-3045 </papid>supposes that t2 is generated in lockstep to t1.1 when choosing how to expand certain vp node in t2, synchronous cfg process would observe that this node is aligned to node vp?</nextsent>
<nextsent>in t1, which had been expanded in t1by vp?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1232">
<title id=" W06-3104.xml">quasi synchronous grammars alignment by soft projection of syntactic dependencies </title>
<section> motivation and related work.  </section>
<citcontext>
<prevsection>
<prevsent>1.2 from synchronous to quasi-synchronous.
</prevsent>
<prevsent>grammars because our approach will let anything align to anything, it is reminiscent of ibm models 15(brown et al, 1993).<papid> J93-2003 </papid></prevsent>
</prevsection>
<citsent citstr=" J00-1004 ">
it differs from the many approaches where (1) is defined by stochastic synchronous grammar (wu, 1997; <papid> J97-3002 </papid>alshawi et al, 2000; <papid> J00-1004 </papid>yamada and knight, 2001; <papid> P01-1067 </papid>eisner, 2003; <papid> P03-2041 </papid>gildea,2003; <papid> P03-1011 </papid>melamed, 2004) <papid> P04-1083 </papid>and from transfer-based systems defined by context-free grammars (lavie et al, 2003).</citsent>
<aftsection>
<nextsent>the synchronous grammar approach, originally due to shieber and schabes (1990), <papid> C90-3045 </papid>supposes that t2 is generated in lockstep to t1.1 when choosing how to expand certain vp node in t2, synchronous cfg process would observe that this node is aligned to node vp?</nextsent>
<nextsent>in t1, which had been expanded in t1by vp?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1233">
<title id=" W06-3104.xml">quasi synchronous grammars alignment by soft projection of syntactic dependencies </title>
<section> motivation and related work.  </section>
<citcontext>
<prevsection>
<prevsent>1.2 from synchronous to quasi-synchronous.
</prevsent>
<prevsent>grammars because our approach will let anything align to anything, it is reminiscent of ibm models 15(brown et al, 1993).<papid> J93-2003 </papid></prevsent>
</prevsection>
<citsent citstr=" P01-1067 ">
it differs from the many approaches where (1) is defined by stochastic synchronous grammar (wu, 1997; <papid> J97-3002 </papid>alshawi et al, 2000; <papid> J00-1004 </papid>yamada and knight, 2001; <papid> P01-1067 </papid>eisner, 2003; <papid> P03-2041 </papid>gildea,2003; <papid> P03-1011 </papid>melamed, 2004) <papid> P04-1083 </papid>and from transfer-based systems defined by context-free grammars (lavie et al, 2003).</citsent>
<aftsection>
<nextsent>the synchronous grammar approach, originally due to shieber and schabes (1990), <papid> C90-3045 </papid>supposes that t2 is generated in lockstep to t1.1 when choosing how to expand certain vp node in t2, synchronous cfg process would observe that this node is aligned to node vp?</nextsent>
<nextsent>in t1, which had been expanded in t1by vp?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1234">
<title id=" W06-3104.xml">quasi synchronous grammars alignment by soft projection of syntactic dependencies </title>
<section> motivation and related work.  </section>
<citcontext>
<prevsection>
<prevsent>1.2 from synchronous to quasi-synchronous.
</prevsent>
<prevsent>grammars because our approach will let anything align to anything, it is reminiscent of ibm models 15(brown et al, 1993).<papid> J93-2003 </papid></prevsent>
</prevsection>
<citsent citstr=" P03-2041 ">
it differs from the many approaches where (1) is defined by stochastic synchronous grammar (wu, 1997; <papid> J97-3002 </papid>alshawi et al, 2000; <papid> J00-1004 </papid>yamada and knight, 2001; <papid> P01-1067 </papid>eisner, 2003; <papid> P03-2041 </papid>gildea,2003; <papid> P03-1011 </papid>melamed, 2004) <papid> P04-1083 </papid>and from transfer-based systems defined by context-free grammars (lavie et al, 2003).</citsent>
<aftsection>
<nextsent>the synchronous grammar approach, originally due to shieber and schabes (1990), <papid> C90-3045 </papid>supposes that t2 is generated in lockstep to t1.1 when choosing how to expand certain vp node in t2, synchronous cfg process would observe that this node is aligned to node vp?</nextsent>
<nextsent>in t1, which had been expanded in t1by vp?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1236">
<title id=" W06-3104.xml">quasi synchronous grammars alignment by soft projection of syntactic dependencies </title>
<section> motivation and related work.  </section>
<citcontext>
<prevsection>
<prevsent>1.2 from synchronous to quasi-synchronous.
</prevsent>
<prevsent>grammars because our approach will let anything align to anything, it is reminiscent of ibm models 15(brown et al, 1993).<papid> J93-2003 </papid></prevsent>
</prevsection>
<citsent citstr=" P03-1011 ">
it differs from the many approaches where (1) is defined by stochastic synchronous grammar (wu, 1997; <papid> J97-3002 </papid>alshawi et al, 2000; <papid> J00-1004 </papid>yamada and knight, 2001; <papid> P01-1067 </papid>eisner, 2003; <papid> P03-2041 </papid>gildea,2003; <papid> P03-1011 </papid>melamed, 2004) <papid> P04-1083 </papid>and from transfer-based systems defined by context-free grammars (lavie et al, 2003).</citsent>
<aftsection>
<nextsent>the synchronous grammar approach, originally due to shieber and schabes (1990), <papid> C90-3045 </papid>supposes that t2 is generated in lockstep to t1.1 when choosing how to expand certain vp node in t2, synchronous cfg process would observe that this node is aligned to node vp?</nextsent>
<nextsent>in t1, which had been expanded in t1by vp?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1237">
<title id=" W06-3104.xml">quasi synchronous grammars alignment by soft projection of syntactic dependencies </title>
<section> motivation and related work.  </section>
<citcontext>
<prevsection>
<prevsent>1.2 from synchronous to quasi-synchronous.
</prevsent>
<prevsent>grammars because our approach will let anything align to anything, it is reminiscent of ibm models 15(brown et al, 1993).<papid> J93-2003 </papid></prevsent>
</prevsection>
<citsent citstr=" P04-1083 ">
it differs from the many approaches where (1) is defined by stochastic synchronous grammar (wu, 1997; <papid> J97-3002 </papid>alshawi et al, 2000; <papid> J00-1004 </papid>yamada and knight, 2001; <papid> P01-1067 </papid>eisner, 2003; <papid> P03-2041 </papid>gildea,2003; <papid> P03-1011 </papid>melamed, 2004) <papid> P04-1083 </papid>and from transfer-based systems defined by context-free grammars (lavie et al, 2003).</citsent>
<aftsection>
<nextsent>the synchronous grammar approach, originally due to shieber and schabes (1990), <papid> C90-3045 </papid>supposes that t2 is generated in lockstep to t1.1 when choosing how to expand certain vp node in t2, synchronous cfg process would observe that this node is aligned to node vp?</nextsent>
<nextsent>in t1, which had been expanded in t1by vp?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1238">
<title id=" W06-3104.xml">quasi synchronous grammars alignment by soft projection of syntactic dependencies </title>
<section> motivation and related work.  </section>
<citcontext>
<prevsection>
<prevsent>grammars because our approach will let anything align to anything, it is reminiscent of ibm models 15(brown et al, 1993).<papid> J93-2003 </papid></prevsent>
<prevsent>it differs from the many approaches where (1) is defined by stochastic synchronous grammar (wu, 1997; <papid> J97-3002 </papid>alshawi et al, 2000; <papid> J00-1004 </papid>yamada and knight, 2001; <papid> P01-1067 </papid>eisner, 2003; <papid> P03-2041 </papid>gildea,2003; <papid> P03-1011 </papid>melamed, 2004) <papid> P04-1083 </papid>and from transfer-based systems defined by context-free grammars (lavie et al, 2003).</prevsent>
</prevsection>
<citsent citstr=" C90-3045 ">
the synchronous grammar approach, originally due to shieber and schabes (1990), <papid> C90-3045 </papid>supposes that t2 is generated in lockstep to t1.1 when choosing how to expand certain vp node in t2, synchronous cfg process would observe that this node is aligned to node vp?</citsent>
<aftsection>
<nextsent>in t1, which had been expanded in t1by vp?
</nextsent>
<nextsent>np?
</nextsent>
<nextsent>v?.
</nextsent>
<nextsent>this might bias it toward choosing to expand the vp in t2 as vp ? np, with the new children aligned to v?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1243">
<title id=" W06-3104.xml">quasi synchronous grammars alignment by soft projection of syntactic dependencies </title>
<section> motivation and related work.  </section>
<citcontext>
<prevsection>
<prevsent>in the text, we directly describe an equivalent conditional process for generating t2, given t1.
</prevsent>
<prevsent>deterministically assembles the latter rules into an actual tree t2 and reads off its yield s2.what is worrisome about the synchronous process is that it can only produce trees t2 that are perfectly isomorphic to t1.
</prevsent>
</prevsection>
<citsent citstr=" P05-1067 ">
it is possible to relax this requirement by using synchronous grammar formalisms more sophisticated than cfg:2 one can permit unaligned nodes (yamada and knight, 2001), <papid> P01-1067 </papid>duplicated children (gildea, 2003)<papid> P03-1011 </papid>3, or alignment between elementary trees of differing sizes rather than between single rules (eisner, 2003; <papid> P03-2041 </papid>ding and palmer, 2005; <papid> P05-1067 </papid>quirk et al, 2005).<papid> P05-1034 </papid></citsent>
<aftsection>
<nextsent>however, one would need rather powerful and slow grammar formalisms (shieber and schabes, 1990; <papid> C90-3045 </papid>melamed et al., 2004), <papid> P04-1084 </papid>often with dis contiguous constituents, to account for all the linguistic divergences that could arise from different movement patterns (scrambling,wh-in situ) or free translation.</nextsent>
<nextsent>in particular, synchronous grammar cannot practically allow s2 to be any permutation of s1, as ibm models 15 do.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1244">
<title id=" W06-3104.xml">quasi synchronous grammars alignment by soft projection of syntactic dependencies </title>
<section> motivation and related work.  </section>
<citcontext>
<prevsection>
<prevsent>in the text, we directly describe an equivalent conditional process for generating t2, given t1.
</prevsent>
<prevsent>deterministically assembles the latter rules into an actual tree t2 and reads off its yield s2.what is worrisome about the synchronous process is that it can only produce trees t2 that are perfectly isomorphic to t1.
</prevsent>
</prevsection>
<citsent citstr=" P05-1034 ">
it is possible to relax this requirement by using synchronous grammar formalisms more sophisticated than cfg:2 one can permit unaligned nodes (yamada and knight, 2001), <papid> P01-1067 </papid>duplicated children (gildea, 2003)<papid> P03-1011 </papid>3, or alignment between elementary trees of differing sizes rather than between single rules (eisner, 2003; <papid> P03-2041 </papid>ding and palmer, 2005; <papid> P05-1067 </papid>quirk et al, 2005).<papid> P05-1034 </papid></citsent>
<aftsection>
<nextsent>however, one would need rather powerful and slow grammar formalisms (shieber and schabes, 1990; <papid> C90-3045 </papid>melamed et al., 2004), <papid> P04-1084 </papid>often with dis contiguous constituents, to account for all the linguistic divergences that could arise from different movement patterns (scrambling,wh-in situ) or free translation.</nextsent>
<nextsent>in particular, synchronous grammar cannot practically allow s2 to be any permutation of s1, as ibm models 15 do.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1246">
<title id=" W06-3104.xml">quasi synchronous grammars alignment by soft projection of syntactic dependencies </title>
<section> motivation and related work.  </section>
<citcontext>
<prevsection>
<prevsent>deterministically assembles the latter rules into an actual tree t2 and reads off its yield s2.what is worrisome about the synchronous process is that it can only produce trees t2 that are perfectly isomorphic to t1.
</prevsent>
<prevsent>it is possible to relax this requirement by using synchronous grammar formalisms more sophisticated than cfg:2 one can permit unaligned nodes (yamada and knight, 2001), <papid> P01-1067 </papid>duplicated children (gildea, 2003)<papid> P03-1011 </papid>3, or alignment between elementary trees of differing sizes rather than between single rules (eisner, 2003; <papid> P03-2041 </papid>ding and palmer, 2005; <papid> P05-1067 </papid>quirk et al, 2005).<papid> P05-1034 </papid></prevsent>
</prevsection>
<citsent citstr=" P04-1084 ">
however, one would need rather powerful and slow grammar formalisms (shieber and schabes, 1990; <papid> C90-3045 </papid>melamed et al., 2004), <papid> P04-1084 </papid>often with dis contiguous constituents, to account for all the linguistic divergences that could arise from different movement patterns (scrambling,wh-in situ) or free translation.</citsent>
<aftsection>
<nextsent>in particular, synchronous grammar cannot practically allow s2 to be any permutation of s1, as ibm models 15 do.
</nextsent>
<nextsent>our alternative is to define quasi-synchronous?
</nextsent>
<nextsent>stochastic process.
</nextsent>
<nextsent>it generates t2 in way that is not in thrall to t1 but is inspired by it.?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1247">
<title id=" W06-3104.xml">quasi synchronous grammars alignment by soft projection of syntactic dependencies </title>
<section> parameterizing qcfg.  </section>
<citcontext>
<prevsection>
<prevsent>given parent node (p, h, h?)
</prevsent>
<prevsent>in t2, we wish to generate sequences of left and right child nodes, of the form (c, a, a?).
</prevsent>
</prevsection>
<citsent citstr=" P04-1061 ">
our monolingual parameters come from simple generative model of syntax used for grammar induc tion: the dependency model with valence (dmv) of klein and manning (2004).<papid> P04-1061 </papid></citsent>
<aftsection>
<nextsent>in scoring dependency attachments, dmv uses tags rather than words.
</nextsent>
<nextsent>the parameters of the model are:1.
</nextsent>
<nextsent>pchoose(c | p, dir): the probability of generating as the next child tag in the sequence of dir children, where dir ? {left, right}.2.
</nextsent>
<nextsent>pstop(s | h, dir, adj): the probability of generating no more child tags in the sequence of dirchildren.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1248">
<title id=" W06-3104.xml">quasi synchronous grammars alignment by soft projection of syntactic dependencies </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>to evaluate this claim, we train quasi-synchronous dependency grammars that allow progressively more divergence from monotonic tree alignment.
</prevsent>
<prevsent>we evaluate these models on cross-entropy over held-out data and on error rate in word-alignment task.
</prevsent>
</prevsection>
<citsent citstr=" W04-3228 ">
one might doubt the use of dependency trees for alignment, since gildea (2004) <papid> W04-3228 </papid>found that constituency trees aligned better.</citsent>
<aftsection>
<nextsent>that experiment, how ever, aligned only the 1-best parse trees.
</nextsent>
<nextsent>we too will consider only the 1-best source tree t1, but in con strast to gildea, we will search for the target tree t2that aligns best with t1.
</nextsent>
<nextsent>finding t2 and the alignment is simply matter of parsing s2 with the qg derived from t1.
</nextsent>
<nextsent>4.1 data and training.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1249">
<title id=" W06-3104.xml">quasi synchronous grammars alignment by soft projection of syntactic dependencies </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>4.1 data and training.
</prevsent>
<prevsent>we performed our modeling experiments with the german-english portion of the europarl european parliament transcripts (koehn, 2002).
</prevsent>
</prevsection>
<citsent citstr=" P03-1054 ">
we obtained monolingual parse trees from the stanford german and english parsers (klein and manning, 2003).<papid> P03-1054 </papid></citsent>
<aftsection>
<nextsent>initial estimates of lexical translation probabilities came from the ibm model 4 translation tables produced by giza++ (brown et al, 1993; <papid> J93-2003 </papid>och and ney, 2003).<papid> J03-1002 </papid></nextsent>
<nextsent>all text was lower cased and numbers of two or more digits were converted to an equal number of hash signs.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1251">
<title id=" W06-3104.xml">quasi synchronous grammars alignment by soft projection of syntactic dependencies </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>we performed our modeling experiments with the german-english portion of the europarl european parliament transcripts (koehn, 2002).
</prevsent>
<prevsent>we obtained monolingual parse trees from the stanford german and english parsers (klein and manning, 2003).<papid> P03-1054 </papid></prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
initial estimates of lexical translation probabilities came from the ibm model 4 translation tables produced by giza++ (brown et al, 1993; <papid> J93-2003 </papid>och and ney, 2003).<papid> J03-1002 </papid></citsent>
<aftsection>
<nextsent>all text was lower cased and numbers of two or more digits were converted to an equal number of hash signs.
</nextsent>
<nextsent>the bitext was divided into training sets of 1k, 10k, and 100k sentence pairs.
</nextsent>
<nextsent>we heldout one thousand sentences for evaluating the cross entropy of the various models and hand-aligned 100 sentence pairs to evaluate alignment error rate (aer).
</nextsent>
<nextsent>we trained the model parameters on bitext using the expectation-maximization (em) algorithm.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1253">
<title id=" W06-3103.xml">morphosyntactic arabic preprocessing for arabic to english statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in order to deal with this problem and to improve the performance of statistical machine translation, each word must be decomposed into its parts.
</prevsent>
<prevsent>in (larkey et al, 2002) it was already shown that word segmentation for arabic improves information retrieval.
</prevsent>
</prevsection>
<citsent citstr=" P03-1051 ">
in (lee et al, 2003) <papid> P03-1051 </papid>statistical approach for arabic word segmentation was presented.</citsent>
<aftsection>
<nextsent>it decomposes each word into sequence of morphemes (prefixes-stem-suffixes), where all possible prefixes and suffixes (not only those we described in table 1and 2) are split from the original word.
</nextsent>
<nextsent>a comparable work was done by (diab et al, 2004), <papid> N04-4038 </papid>where pos tagging method for arabic is also discussed.</nextsent>
<nextsent>as we have access to this tool, we test its impact on the performance of our translation system.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1254">
<title id=" W06-3103.xml">morphosyntactic arabic preprocessing for arabic to english statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in (lee et al, 2003) <papid> P03-1051 </papid>statistical approach for arabic word segmentation was presented.</prevsent>
<prevsent>it decomposes each word into sequence of morphemes (prefixes-stem-suffixes), where all possible prefixes and suffixes (not only those we described in table 1and 2) are split from the original word.</prevsent>
</prevsection>
<citsent citstr=" N04-4038 ">
a comparable work was done by (diab et al, 2004), <papid> N04-4038 </papid>where pos tagging method for arabic is also discussed.</citsent>
<aftsection>
<nextsent>as we have access to this tool, we test its impact on the performance of our translation system.
</nextsent>
<nextsent>in 15 table 1: prefixes handled in this work and their meanings.
</nextsent>
<nextsent>prefix ? ?
</nextsent>
<nextsent>h. ?@ transliteration f l al meaning and and then as, like in order to with, in the (habash and rambow, 2005) <papid> P05-1071 </papid>morphology analyzer was used for the segementation and pos tagging.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1255">
<title id=" W06-3103.xml">morphosyntactic arabic preprocessing for arabic to english statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in 15 table 1: prefixes handled in this work and their meanings.
</prevsent>
<prevsent>prefix ? ?
</prevsent>
</prevsection>
<citsent citstr=" P05-1071 ">
h. ?@ transliteration f l al meaning and and then as, like in order to with, in the (habash and rambow, 2005) <papid> P05-1071 </papid>morphology analyzer was used for the segementation and pos tagging.</citsent>
<aftsection>
<nextsent>in contrast to the methods mentioned above, our segmentation method is unsupervised and rule based.in this paper we first explain our statistical machine translation (smt) system used for testing the impact of the different segmentation methods, then we introduce some preprocessing and normalization tools for arabic and explain the linguistic motivation beyond them.
</nextsent>
<nextsent>afterwards, we present three word segmentation methods, supervised learning approach, finite state automaton-based segmentation, and frequency-based method.
</nextsent>
<nextsent>in section 5, the experimental results are presented.
</nextsent>
<nextsent>finally, the paper is summarized in section 6 .
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1257">
<title id=" W06-3103.xml">morphosyntactic arabic preprocessing for arabic to english statistical machine translation </title>
<section> baseline smt system.  </section>
<citcontext>
<prevsection>
<prevsent>fj ,which is to be translated into target language sentence ei1 = e1 . . .
</prevsent>
<prevsent>ei . . .
</prevsent>
</prevsection>
<citsent citstr=" P02-1038 ">
ei . among all possible target language sentences, we will choose the sentence with the highest probability: ei1 = argmax i,ei1 { pr(ei1|f 1 ) } (1)the posterior probability pr(ei1|fj1 ) is modeled directly using log-linear combination of several models (och and ney, 2002): <papid> P02-1038 </papid>pr(ei1|f 1 ) = exp (m m=1 mhm(e 1, j 1 ) ) ? ei ? 1 exp (m m=1 mhm(e i? 1 , j 1 ) ) (2) the denominator represents normalization factor that depends only on the source sentence fj1 . therefore, we can omit it during the search process.</citsent>
<aftsection>
<nextsent>as decision rule, we obtain: ei1 = argmax i,ei1 { m?
</nextsent>
<nextsent>m=1 mhm(e 1, j 1 ) } (3)this approach is generalization of the source channel approach (brown et al, 1990).<papid> J90-2002 </papid></nextsent>
<nextsent>it has the advantage that additional models h(?)</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1258">
<title id=" W06-3103.xml">morphosyntactic arabic preprocessing for arabic to english statistical machine translation </title>
<section> baseline smt system.  </section>
<citcontext>
<prevsection>
<prevsent>ei . among all possible target language sentences, we will choose the sentence with the highest probability: ei1 = argmax i,ei1 { pr(ei1|f 1 ) } (1)the posterior probability pr(ei1|fj1 ) is modeled directly using log-linear combination of several models (och and ney, 2002): <papid> P02-1038 </papid>pr(ei1|f 1 ) = exp (m m=1 mhm(e 1, j 1 ) ) ? ei ? 1 exp (m m=1 mhm(e i? 1 , j 1 ) ) (2) the denominator represents normalization factor that depends only on the source sentence fj1 . therefore, we can omit it during the search process.</prevsent>
<prevsent>as decision rule, we obtain: ei1 = argmax i,ei1 { m?</prevsent>
</prevsection>
<citsent citstr=" J90-2002 ">
m=1 mhm(e 1, j 1 ) } (3)this approach is generalization of the source channel approach (brown et al, 1990).<papid> J90-2002 </papid></citsent>
<aftsection>
<nextsent>it has the advantage that additional models h(?)
</nextsent>
<nextsent>can be easily integrated into the overall system.
</nextsent>
<nextsent>the model scaling factors m1 are trained with respect to the final translation quality measured by an error criterion (och, 2003).<papid> P03-1021 </papid></nextsent>
<nextsent>we use state-of-the-art phrase-based translation system including the following models: an n-gram language model, phrase translation model and aword-based lexicon model.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1259">
<title id=" W06-3103.xml">morphosyntactic arabic preprocessing for arabic to english statistical machine translation </title>
<section> baseline smt system.  </section>
<citcontext>
<prevsection>
<prevsent>it has the advantage that additional models h(?)
</prevsent>
<prevsent>can be easily integrated into the overall system.
</prevsent>
</prevsection>
<citsent citstr=" P03-1021 ">
the model scaling factors m1 are trained with respect to the final translation quality measured by an error criterion (och, 2003).<papid> P03-1021 </papid></citsent>
<aftsection>
<nextsent>we use state-of-the-art phrase-based translation system including the following models: an n-gram language model, phrase translation model and aword-based lexicon model.
</nextsent>
<nextsent>the latter two models are used for both directions: p(f |e) and p(e|f).
</nextsent>
<nextsent>additionally, we use word penalty and phrase penalty.
</nextsent>
<nextsent>more details about the baseline system can be found in (zens and ney, 2004; <papid> N04-1033 </papid>zens et al, 2005).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1260">
<title id=" W06-3103.xml">morphosyntactic arabic preprocessing for arabic to english statistical machine translation </title>
<section> baseline smt system.  </section>
<citcontext>
<prevsection>
<prevsent>the latter two models are used for both directions: p(f |e) and p(e|f).
</prevsent>
<prevsent>additionally, we use word penalty and phrase penalty.
</prevsent>
</prevsection>
<citsent citstr=" N04-1033 ">
more details about the baseline system can be found in (zens and ney, 2004; <papid> N04-1033 </papid>zens et al, 2005).</citsent>
<aftsection>
<nextsent>3.1 tokenizer.
</nextsent>
<nextsent>as for other languages, the corpora must be first tokenized.
</nextsent>
<nextsent>here words and punctuations (except ab breviation) must be separated.
</nextsent>
<nextsent>another criterion is that arabic has some characters that appear only at the end of word.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1261">
<title id=" W06-3103.xml">morphosyntactic arabic preprocessing for arabic to english statistical machine translation </title>
<section> preprocessing and normalization tools.  </section>
<citcontext>
<prevsection>
<prevsent>so, if word has the prefix al, then its adjective will also have al as prefix.
</prevsent>
<prevsent>in order to reduce the sentence size we decide to remove all these articles that are supposed to be attached to an adjective.
</prevsent>
</prevsection>
<citsent citstr=" N04-4015 ">
another way for determiner deletion is described in (lee, 2004).<papid> N04-4015 </papid></citsent>
<aftsection>
<nextsent>one way to simplify inflected arabic text for smt system is to split the words in prefixes, stem and suffixes.
</nextsent>
<nextsent>in (lee et al, 2003), (<papid> P03-1051 </papid>diab et al, 2004) <papid> N04-4038 </papid>and (habash and rambow, 2005) <papid> P05-1071 </papid>three supervised segmentation methods are introduced.</nextsent>
<nextsent>however, in these works the impact of the segmentation on the translation quality is not studied.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1270">
<title id=" W06-3103.xml">morphosyntactic arabic preprocessing for arabic to english statistical machine translation </title>
<section> word segmentation.  </section>
<citcontext>
<prevsection>
<prevsent>we decide whether and where to split the composite word based on the frequency of different resulting stems and on the frequency of the compound word, e.g. if the compound word has higher frequency than all possible stems, it will notbe split.
</prevsent>
<prevsent>this simple heuristic harmonizes the corpus by reducing the size of vocabulary, singletons and also unseen words from the test corpus.
</prevsent>
</prevsection>
<citsent citstr=" E03-1076 ">
this method is very similar to the method used for splitting german compound words (koehn and knight, 2003).<papid> E03-1076 </papid></citsent>
<aftsection>
<nextsent>4.3 finite state automaton-based approach.
</nextsent>
<nextsent>(fsa) to segment arabic words into prefixes, stem and one suffix, we implemented two finite state automata.
</nextsent>
<nextsent>one for stripping the prefixes and the other for the suffixes.
</nextsent>
<nextsent>then, we append the suffix automaton to the other one for stripping prefixes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1271">
<title id=" W06-3103.xml">morphosyntactic arabic preprocessing for arabic to english statistical machine translation </title>
<section> experimental results.  </section>
<citcontext>
<prevsection>
<prevsent>in the btec task, c-star03 and iwslt04 coporaare considered as development and test sets, respectively.
</prevsent>
<prevsent>5.2 evaluation metrics.
</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
the commonly used criteria to evaluate the translation results in the machine translation community are: wer (word error rate), per (position independent word error rate), bleu (papineni et al., 2002), <papid> P02-1040 </papid>and nist (doddington, 2002).</citsent>
<aftsection>
<nextsent>the four criteria are computed with respect to multiple references.
</nextsent>
<nextsent>the number of reference translations per source sentence varies from 4 to 16 references.
</nextsent>
<nextsent>the evaluation is case-insensitive for btec and case sensitive for nist task.
</nextsent>
<nextsent>as the bleu and nist scores measure accuracy, higher scores are better.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1272">
<title id=" W06-3103.xml">morphosyntactic arabic preprocessing for arabic to english statistical machine translation </title>
<section> conclusion.  </section>
<citcontext>
<prevsection>
<prevsent>arabic english tokenized ifsa train: sentences 8.5m running words 260.5m 316.8m 279.2m vocabulary 510.3k 411.2k 301.2k dev: sentences 1043 running words 30.2k 33.3k 33k oovs (running words) 809 399 na test: sentences 1353 running words 40k 47.9k 48.3k oovs (running words) 871 505 na table 6: case insensitive evaluation results for translating the development and test data of btec task after performing divers preprocessing.
</prevsent>
<prevsent>dev test mper mwer bleu nist mper mwer bleu nist [%] [%] [%] [%] [%] [%] non-segmented data 21.4 24.6 63.9 10.0 23.5 27.2 58.1 9.6 sl segmenter 21.2 24.4 62.5 9.7 23.4 27.4 59.2 9.7 fb segmenter 20.9 24.4 65.3 10.1 22.1 25.8 59.8 9.7 fsa segmenter 20.1 23.4 64.8 10.2 21.1 25.2 61.3 10.2 ifsa segmenter 20.0 23.3 65.0 10.4 21.2 25.3 61.3 10.2 based approach and finite state automaton-basedapproach.
</prevsent>
</prevsection>
<citsent citstr=" W00-0801 ">
we explained that the best of our proposed methods, the improved finite state automaton, has three advantages over the state-of-the-art arabic word segmentation method (diab, 2000), <papid> W00-0801 </papid>supervised learning.</citsent>
<aftsection>
<nextsent>they are: consistency in improving the baselines system over different tasks, its capability to be efficiently applied on the large corpora, and its ability to cope with different tasks.
</nextsent>
<nextsent>this material is based upon work supported by the defense advanced research projects agency (darpa) under contract no.
</nextsent>
<nextsent>hr0011-06-c-0023.any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the defense advanced research projects agency (darpa).
</nextsent>

</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1273">
<title id=" W06-2207.xml">a hybrid approach for the acquisition of information extraction patterns </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>forex ample, for financial market change?
</prevsent>
<prevsent>domain one relevant pattern is  noun fall money to money .
</prevsent>
</prevsection>
<citsent citstr=" C00-2136 ">
when this pattern is matched on the text london gold fell $4.70 to $308.35?, achange of $4.70 is detected for the financial instrument london gold?.domain-specific patterns are either handcrafted or acquired automatically (riloff, 1996;yangarber et al, 2000; <papid> C00-2136 </papid>yangarber, 2003; <papid> P03-1044 </papid>stevenson and greenwood, 2005).<papid> P05-1047 </papid></citsent>
<aftsection>
<nextsent>to minimize annotation costs, some of the latter approaches use lightly supervised bootstrapping algorithms that require as input only small set of documents annotated with their corresponding category label.
</nextsent>
<nextsent>the focus of this paper is to improve such lightly supervised pattern acquisition methods.
</nextsent>
<nextsent>moreover, we focuson robust bootstrapping algorithms that can handle real-world document collections, which contain many domains.although rich literature covers bootstrapping methods applied to natural language problems (yarowsky, 1995; <papid> P95-1026 </papid>riloff, 1996; collins and singer, 1999; <papid> W99-0613 </papid>yangarber et al, 2000; <papid> C00-2136 </papid>yangarber, 2003; <papid> P03-1044 </papid>abney, 2004) <papid> J04-3004 </papid>several questions remain unanswered when these methods are applied to syntactic or semantic pattern acquisition.</nextsent>
<nextsent>in this paper we answer two of these questions: (1) can pattern acquisition be improved with text categorization techniquesbootstrapping-based pattern acquisition algorithms can also be regarded as incremental text categorization (tc), since in each iteration documents containing certain patterns are assigned the corresponding category label.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1278">
<title id=" W06-2207.xml">a hybrid approach for the acquisition of information extraction patterns </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>forex ample, for financial market change?
</prevsent>
<prevsent>domain one relevant pattern is  noun fall money to money .
</prevsent>
</prevsection>
<citsent citstr=" P03-1044 ">
when this pattern is matched on the text london gold fell $4.70 to $308.35?, achange of $4.70 is detected for the financial instrument london gold?.domain-specific patterns are either handcrafted or acquired automatically (riloff, 1996;yangarber et al, 2000; <papid> C00-2136 </papid>yangarber, 2003; <papid> P03-1044 </papid>stevenson and greenwood, 2005).<papid> P05-1047 </papid></citsent>
<aftsection>
<nextsent>to minimize annotation costs, some of the latter approaches use lightly supervised bootstrapping algorithms that require as input only small set of documents annotated with their corresponding category label.
</nextsent>
<nextsent>the focus of this paper is to improve such lightly supervised pattern acquisition methods.
</nextsent>
<nextsent>moreover, we focuson robust bootstrapping algorithms that can handle real-world document collections, which contain many domains.although rich literature covers bootstrapping methods applied to natural language problems (yarowsky, 1995; <papid> P95-1026 </papid>riloff, 1996; collins and singer, 1999; <papid> W99-0613 </papid>yangarber et al, 2000; <papid> C00-2136 </papid>yangarber, 2003; <papid> P03-1044 </papid>abney, 2004) <papid> J04-3004 </papid>several questions remain unanswered when these methods are applied to syntactic or semantic pattern acquisition.</nextsent>
<nextsent>in this paper we answer two of these questions: (1) can pattern acquisition be improved with text categorization techniquesbootstrapping-based pattern acquisition algorithms can also be regarded as incremental text categorization (tc), since in each iteration documents containing certain patterns are assigned the corresponding category label.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1280">
<title id=" W06-2207.xml">a hybrid approach for the acquisition of information extraction patterns </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>forex ample, for financial market change?
</prevsent>
<prevsent>domain one relevant pattern is  noun fall money to money .
</prevsent>
</prevsection>
<citsent citstr=" P05-1047 ">
when this pattern is matched on the text london gold fell $4.70 to $308.35?, achange of $4.70 is detected for the financial instrument london gold?.domain-specific patterns are either handcrafted or acquired automatically (riloff, 1996;yangarber et al, 2000; <papid> C00-2136 </papid>yangarber, 2003; <papid> P03-1044 </papid>stevenson and greenwood, 2005).<papid> P05-1047 </papid></citsent>
<aftsection>
<nextsent>to minimize annotation costs, some of the latter approaches use lightly supervised bootstrapping algorithms that require as input only small set of documents annotated with their corresponding category label.
</nextsent>
<nextsent>the focus of this paper is to improve such lightly supervised pattern acquisition methods.
</nextsent>
<nextsent>moreover, we focuson robust bootstrapping algorithms that can handle real-world document collections, which contain many domains.although rich literature covers bootstrapping methods applied to natural language problems (yarowsky, 1995; <papid> P95-1026 </papid>riloff, 1996; collins and singer, 1999; <papid> W99-0613 </papid>yangarber et al, 2000; <papid> C00-2136 </papid>yangarber, 2003; <papid> P03-1044 </papid>abney, 2004) <papid> J04-3004 </papid>several questions remain unanswered when these methods are applied to syntactic or semantic pattern acquisition.</nextsent>
<nextsent>in this paper we answer two of these questions: (1) can pattern acquisition be improved with text categorization techniquesbootstrapping-based pattern acquisition algorithms can also be regarded as incremental text categorization (tc), since in each iteration documents containing certain patterns are assigned the corresponding category label.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1284">
<title id=" W06-2207.xml">a hybrid approach for the acquisition of information extraction patterns </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>to minimize annotation costs, some of the latter approaches use lightly supervised bootstrapping algorithms that require as input only small set of documents annotated with their corresponding category label.
</prevsent>
<prevsent>the focus of this paper is to improve such lightly supervised pattern acquisition methods.
</prevsent>
</prevsection>
<citsent citstr=" P95-1026 ">
moreover, we focuson robust bootstrapping algorithms that can handle real-world document collections, which contain many domains.although rich literature covers bootstrapping methods applied to natural language problems (yarowsky, 1995; <papid> P95-1026 </papid>riloff, 1996; collins and singer, 1999; <papid> W99-0613 </papid>yangarber et al, 2000; <papid> C00-2136 </papid>yangarber, 2003; <papid> P03-1044 </papid>abney, 2004) <papid> J04-3004 </papid>several questions remain unanswered when these methods are applied to syntactic or semantic pattern acquisition.</citsent>
<aftsection>
<nextsent>in this paper we answer two of these questions: (1) can pattern acquisition be improved with text categorization techniquesbootstrapping-based pattern acquisition algorithms can also be regarded as incremental text categorization (tc), since in each iteration documents containing certain patterns are assigned the corresponding category label.
</nextsent>
<nextsent>although tc is obviously not the main goal of pattern acquisition methodologies, it is nevertheless an integral part ofthe learning algorithm: each iteration of the acquisition algorithm depends on the previous assignments of category labels to documents.
</nextsent>
<nextsent>hence, if the quality of the tc solution proposed is bad, the quality of the acquired patterns will suffer.
</nextsent>
<nextsent>motivated by this observation, we introduce co-training-based algorithm (blum and mitchell, 1998) that uses text categorization algorithm as reinforcement for pattern acquisition.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1285">
<title id=" W06-2207.xml">a hybrid approach for the acquisition of information extraction patterns </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>to minimize annotation costs, some of the latter approaches use lightly supervised bootstrapping algorithms that require as input only small set of documents annotated with their corresponding category label.
</prevsent>
<prevsent>the focus of this paper is to improve such lightly supervised pattern acquisition methods.
</prevsent>
</prevsection>
<citsent citstr=" W99-0613 ">
moreover, we focuson robust bootstrapping algorithms that can handle real-world document collections, which contain many domains.although rich literature covers bootstrapping methods applied to natural language problems (yarowsky, 1995; <papid> P95-1026 </papid>riloff, 1996; collins and singer, 1999; <papid> W99-0613 </papid>yangarber et al, 2000; <papid> C00-2136 </papid>yangarber, 2003; <papid> P03-1044 </papid>abney, 2004) <papid> J04-3004 </papid>several questions remain unanswered when these methods are applied to syntactic or semantic pattern acquisition.</citsent>
<aftsection>
<nextsent>in this paper we answer two of these questions: (1) can pattern acquisition be improved with text categorization techniquesbootstrapping-based pattern acquisition algorithms can also be regarded as incremental text categorization (tc), since in each iteration documents containing certain patterns are assigned the corresponding category label.
</nextsent>
<nextsent>although tc is obviously not the main goal of pattern acquisition methodologies, it is nevertheless an integral part ofthe learning algorithm: each iteration of the acquisition algorithm depends on the previous assignments of category labels to documents.
</nextsent>
<nextsent>hence, if the quality of the tc solution proposed is bad, the quality of the acquired patterns will suffer.
</nextsent>
<nextsent>motivated by this observation, we introduce co-training-based algorithm (blum and mitchell, 1998) that uses text categorization algorithm as reinforcement for pattern acquisition.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1292">
<title id=" W06-2207.xml">a hybrid approach for the acquisition of information extraction patterns </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>to minimize annotation costs, some of the latter approaches use lightly supervised bootstrapping algorithms that require as input only small set of documents annotated with their corresponding category label.
</prevsent>
<prevsent>the focus of this paper is to improve such lightly supervised pattern acquisition methods.
</prevsent>
</prevsection>
<citsent citstr=" J04-3004 ">
moreover, we focuson robust bootstrapping algorithms that can handle real-world document collections, which contain many domains.although rich literature covers bootstrapping methods applied to natural language problems (yarowsky, 1995; <papid> P95-1026 </papid>riloff, 1996; collins and singer, 1999; <papid> W99-0613 </papid>yangarber et al, 2000; <papid> C00-2136 </papid>yangarber, 2003; <papid> P03-1044 </papid>abney, 2004) <papid> J04-3004 </papid>several questions remain unanswered when these methods are applied to syntactic or semantic pattern acquisition.</citsent>
<aftsection>
<nextsent>in this paper we answer two of these questions: (1) can pattern acquisition be improved with text categorization techniquesbootstrapping-based pattern acquisition algorithms can also be regarded as incremental text categorization (tc), since in each iteration documents containing certain patterns are assigned the corresponding category label.
</nextsent>
<nextsent>although tc is obviously not the main goal of pattern acquisition methodologies, it is nevertheless an integral part ofthe learning algorithm: each iteration of the acquisition algorithm depends on the previous assignments of category labels to documents.
</nextsent>
<nextsent>hence, if the quality of the tc solution proposed is bad, the quality of the acquired patterns will suffer.
</nextsent>
<nextsent>motivated by this observation, we introduce co-training-based algorithm (blum and mitchell, 1998) that uses text categorization algorithm as reinforcement for pattern acquisition.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1332">
<title id=" W06-2207.xml">a hybrid approach for the acquisition of information extraction patterns </title>
<section> experimental evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>all charts show precision/recall points starting after 100 learning iterations with 100 iteration increments.
</prevsent>
<prevsent>it is immediately obvious that the collins selection criterion performs significantly better than the other three criteria.
</prevsent>
</prevsection>
<citsent citstr=" P02-1046 ">
forthe same recall point, collins yields classification model with much higher precision, with differences ranging from 5% in the reuters collection to 20% in the ap collection.theorem 5 in (abney, 2002) <papid> P02-1046 </papid>provides theoretical explanation for these results: if certain independence conditions between the classifier rules are satisfied and the precision of each rule is larger than threshold , then the precision of the final classifier is larger than . although the rule independence conditions are certainly not satisfied inour real-world evaluation, the above theorem indicates that there is strong relation between the precision of the classifier rules on labeled data andthe precision of the final classifier.</citsent>
<aftsection>
<nextsent>our results provide the empirical proof that controling the precision of the acquired rules (i.e. the collins crite rion) is important.
</nextsent>
<nextsent>the collins criterion controls the recall of the learned model by favoring rules with high frequency in the collection.
</nextsent>
<nextsent>however, since the other two criteria do not use high precision threshold, they will acquire more rules, which translate sin better recall.
</nextsent>
<nextsent>for two out of the three collections, riloff and chi obtain slightly better recall, about 2% higher than collins?, albeit with much lower precision.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1344">
<title id=" W07-0401.xml">chunk level reordering of source language sentences with automatically learned rules for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the experiments also show that the reordering at the chunk-level performs better than at the pos-level.
</prevsent>
<prevsent>in machine translation, reordering is one of the major problems, since different languages have different word order requirements.
</prevsent>
</prevsection>
<citsent citstr=" P96-1021 ">
many reordering constraints have been used for word reorderings, such as itg constraints (wu, 1996), <papid> P96-1021 </papid>ibm constraints (berger et al , 1996) <papid> J96-1002 </papid>and local constraints (kanthak et al , 2005).<papid> W05-0831 </papid></citsent>
<aftsection>
<nextsent>these approaches do not make use of any linguistic knowledge.several methods have been proposed to use syntactic information to handle the reordering problem, e.g.
</nextsent>
<nextsent>(wu, 1997; <papid> J97-3002 </papid>yamada and knight, 2001; <papid> P01-1067 </papid>gildea, 2003; <papid> P03-1011 </papid>melamed, 2004; <papid> P04-1083 </papid>graehl and knight, 2004; <papid> N04-1014 </papid>galley et al , 2006).<papid> P06-1121 </papid></nextsent>
<nextsent>one approach makes use ofbitext grammars to parse both the source and target languages.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1345">
<title id=" W07-0401.xml">chunk level reordering of source language sentences with automatically learned rules for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the experiments also show that the reordering at the chunk-level performs better than at the pos-level.
</prevsent>
<prevsent>in machine translation, reordering is one of the major problems, since different languages have different word order requirements.
</prevsent>
</prevsection>
<citsent citstr=" J96-1002 ">
many reordering constraints have been used for word reorderings, such as itg constraints (wu, 1996), <papid> P96-1021 </papid>ibm constraints (berger et al , 1996) <papid> J96-1002 </papid>and local constraints (kanthak et al , 2005).<papid> W05-0831 </papid></citsent>
<aftsection>
<nextsent>these approaches do not make use of any linguistic knowledge.several methods have been proposed to use syntactic information to handle the reordering problem, e.g.
</nextsent>
<nextsent>(wu, 1997; <papid> J97-3002 </papid>yamada and knight, 2001; <papid> P01-1067 </papid>gildea, 2003; <papid> P03-1011 </papid>melamed, 2004; <papid> P04-1083 </papid>graehl and knight, 2004; <papid> N04-1014 </papid>galley et al , 2006).<papid> P06-1121 </papid></nextsent>
<nextsent>one approach makes use ofbitext grammars to parse both the source and target languages.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1346">
<title id=" W07-0401.xml">chunk level reordering of source language sentences with automatically learned rules for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the experiments also show that the reordering at the chunk-level performs better than at the pos-level.
</prevsent>
<prevsent>in machine translation, reordering is one of the major problems, since different languages have different word order requirements.
</prevsent>
</prevsection>
<citsent citstr=" W05-0831 ">
many reordering constraints have been used for word reorderings, such as itg constraints (wu, 1996), <papid> P96-1021 </papid>ibm constraints (berger et al , 1996) <papid> J96-1002 </papid>and local constraints (kanthak et al , 2005).<papid> W05-0831 </papid></citsent>
<aftsection>
<nextsent>these approaches do not make use of any linguistic knowledge.several methods have been proposed to use syntactic information to handle the reordering problem, e.g.
</nextsent>
<nextsent>(wu, 1997; <papid> J97-3002 </papid>yamada and knight, 2001; <papid> P01-1067 </papid>gildea, 2003; <papid> P03-1011 </papid>melamed, 2004; <papid> P04-1083 </papid>graehl and knight, 2004; <papid> N04-1014 </papid>galley et al , 2006).<papid> P06-1121 </papid></nextsent>
<nextsent>one approach makes use ofbitext grammars to parse both the source and target languages.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1348">
<title id=" W07-0401.xml">chunk level reordering of source language sentences with automatically learned rules for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>many reordering constraints have been used for word reorderings, such as itg constraints (wu, 1996), <papid> P96-1021 </papid>ibm constraints (berger et al , 1996) <papid> J96-1002 </papid>and local constraints (kanthak et al , 2005).<papid> W05-0831 </papid></prevsent>
<prevsent>these approaches do not make use of any linguistic knowledge.several methods have been proposed to use syntactic information to handle the reordering problem, e.g.</prevsent>
</prevsection>
<citsent citstr=" J97-3002 ">
(wu, 1997; <papid> J97-3002 </papid>yamada and knight, 2001; <papid> P01-1067 </papid>gildea, 2003; <papid> P03-1011 </papid>melamed, 2004; <papid> P04-1083 </papid>graehl and knight, 2004; <papid> N04-1014 </papid>galley et al , 2006).<papid> P06-1121 </papid></citsent>
<aftsection>
<nextsent>one approach makes use ofbitext grammars to parse both the source and target languages.
</nextsent>
<nextsent>another approach makes use of syntactic information only in the target language.
</nextsent>
<nextsent>note that these models have radically different structures and parameterizations than phrase-based models for smt.another kind of approaches is to use syntactic information in rescoring methods.
</nextsent>
<nextsent>(koehn and knight, 2003) <papid> E03-1076 </papid>apply reranking approach to the sub-task of noun-phrase translation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1349">
<title id=" W07-0401.xml">chunk level reordering of source language sentences with automatically learned rules for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>many reordering constraints have been used for word reorderings, such as itg constraints (wu, 1996), <papid> P96-1021 </papid>ibm constraints (berger et al , 1996) <papid> J96-1002 </papid>and local constraints (kanthak et al , 2005).<papid> W05-0831 </papid></prevsent>
<prevsent>these approaches do not make use of any linguistic knowledge.several methods have been proposed to use syntactic information to handle the reordering problem, e.g.</prevsent>
</prevsection>
<citsent citstr=" P01-1067 ">
(wu, 1997; <papid> J97-3002 </papid>yamada and knight, 2001; <papid> P01-1067 </papid>gildea, 2003; <papid> P03-1011 </papid>melamed, 2004; <papid> P04-1083 </papid>graehl and knight, 2004; <papid> N04-1014 </papid>galley et al , 2006).<papid> P06-1121 </papid></citsent>
<aftsection>
<nextsent>one approach makes use ofbitext grammars to parse both the source and target languages.
</nextsent>
<nextsent>another approach makes use of syntactic information only in the target language.
</nextsent>
<nextsent>note that these models have radically different structures and parameterizations than phrase-based models for smt.another kind of approaches is to use syntactic information in rescoring methods.
</nextsent>
<nextsent>(koehn and knight, 2003) <papid> E03-1076 </papid>apply reranking approach to the sub-task of noun-phrase translation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1350">
<title id=" W07-0401.xml">chunk level reordering of source language sentences with automatically learned rules for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>many reordering constraints have been used for word reorderings, such as itg constraints (wu, 1996), <papid> P96-1021 </papid>ibm constraints (berger et al , 1996) <papid> J96-1002 </papid>and local constraints (kanthak et al , 2005).<papid> W05-0831 </papid></prevsent>
<prevsent>these approaches do not make use of any linguistic knowledge.several methods have been proposed to use syntactic information to handle the reordering problem, e.g.</prevsent>
</prevsection>
<citsent citstr=" P03-1011 ">
(wu, 1997; <papid> J97-3002 </papid>yamada and knight, 2001; <papid> P01-1067 </papid>gildea, 2003; <papid> P03-1011 </papid>melamed, 2004; <papid> P04-1083 </papid>graehl and knight, 2004; <papid> N04-1014 </papid>galley et al , 2006).<papid> P06-1121 </papid></citsent>
<aftsection>
<nextsent>one approach makes use ofbitext grammars to parse both the source and target languages.
</nextsent>
<nextsent>another approach makes use of syntactic information only in the target language.
</nextsent>
<nextsent>note that these models have radically different structures and parameterizations than phrase-based models for smt.another kind of approaches is to use syntactic information in rescoring methods.
</nextsent>
<nextsent>(koehn and knight, 2003) <papid> E03-1076 </papid>apply reranking approach to the sub-task of noun-phrase translation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1351">
<title id=" W07-0401.xml">chunk level reordering of source language sentences with automatically learned rules for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>many reordering constraints have been used for word reorderings, such as itg constraints (wu, 1996), <papid> P96-1021 </papid>ibm constraints (berger et al , 1996) <papid> J96-1002 </papid>and local constraints (kanthak et al , 2005).<papid> W05-0831 </papid></prevsent>
<prevsent>these approaches do not make use of any linguistic knowledge.several methods have been proposed to use syntactic information to handle the reordering problem, e.g.</prevsent>
</prevsection>
<citsent citstr=" P04-1083 ">
(wu, 1997; <papid> J97-3002 </papid>yamada and knight, 2001; <papid> P01-1067 </papid>gildea, 2003; <papid> P03-1011 </papid>melamed, 2004; <papid> P04-1083 </papid>graehl and knight, 2004; <papid> N04-1014 </papid>galley et al , 2006).<papid> P06-1121 </papid></citsent>
<aftsection>
<nextsent>one approach makes use ofbitext grammars to parse both the source and target languages.
</nextsent>
<nextsent>another approach makes use of syntactic information only in the target language.
</nextsent>
<nextsent>note that these models have radically different structures and parameterizations than phrase-based models for smt.another kind of approaches is to use syntactic information in rescoring methods.
</nextsent>
<nextsent>(koehn and knight, 2003) <papid> E03-1076 </papid>apply reranking approach to the sub-task of noun-phrase translation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1352">
<title id=" W07-0401.xml">chunk level reordering of source language sentences with automatically learned rules for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>many reordering constraints have been used for word reorderings, such as itg constraints (wu, 1996), <papid> P96-1021 </papid>ibm constraints (berger et al , 1996) <papid> J96-1002 </papid>and local constraints (kanthak et al , 2005).<papid> W05-0831 </papid></prevsent>
<prevsent>these approaches do not make use of any linguistic knowledge.several methods have been proposed to use syntactic information to handle the reordering problem, e.g.</prevsent>
</prevsection>
<citsent citstr=" N04-1014 ">
(wu, 1997; <papid> J97-3002 </papid>yamada and knight, 2001; <papid> P01-1067 </papid>gildea, 2003; <papid> P03-1011 </papid>melamed, 2004; <papid> P04-1083 </papid>graehl and knight, 2004; <papid> N04-1014 </papid>galley et al , 2006).<papid> P06-1121 </papid></citsent>
<aftsection>
<nextsent>one approach makes use ofbitext grammars to parse both the source and target languages.
</nextsent>
<nextsent>another approach makes use of syntactic information only in the target language.
</nextsent>
<nextsent>note that these models have radically different structures and parameterizations than phrase-based models for smt.another kind of approaches is to use syntactic information in rescoring methods.
</nextsent>
<nextsent>(koehn and knight, 2003) <papid> E03-1076 </papid>apply reranking approach to the sub-task of noun-phrase translation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1353">
<title id=" W07-0401.xml">chunk level reordering of source language sentences with automatically learned rules for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>many reordering constraints have been used for word reorderings, such as itg constraints (wu, 1996), <papid> P96-1021 </papid>ibm constraints (berger et al , 1996) <papid> J96-1002 </papid>and local constraints (kanthak et al , 2005).<papid> W05-0831 </papid></prevsent>
<prevsent>these approaches do not make use of any linguistic knowledge.several methods have been proposed to use syntactic information to handle the reordering problem, e.g.</prevsent>
</prevsection>
<citsent citstr=" P06-1121 ">
(wu, 1997; <papid> J97-3002 </papid>yamada and knight, 2001; <papid> P01-1067 </papid>gildea, 2003; <papid> P03-1011 </papid>melamed, 2004; <papid> P04-1083 </papid>graehl and knight, 2004; <papid> N04-1014 </papid>galley et al , 2006).<papid> P06-1121 </papid></citsent>
<aftsection>
<nextsent>one approach makes use ofbitext grammars to parse both the source and target languages.
</nextsent>
<nextsent>another approach makes use of syntactic information only in the target language.
</nextsent>
<nextsent>note that these models have radically different structures and parameterizations than phrase-based models for smt.another kind of approaches is to use syntactic information in rescoring methods.
</nextsent>
<nextsent>(koehn and knight, 2003) <papid> E03-1076 </papid>apply reranking approach to the sub-task of noun-phrase translation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1354">
<title id=" W07-0401.xml">chunk level reordering of source language sentences with automatically learned rules for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>another approach makes use of syntactic information only in the target language.
</prevsent>
<prevsent>note that these models have radically different structures and parameterizations than phrase-based models for smt.another kind of approaches is to use syntactic information in rescoring methods.
</prevsent>
</prevsection>
<citsent citstr=" E03-1076 ">
(koehn and knight, 2003) <papid> E03-1076 </papid>apply reranking approach to the sub-task of noun-phrase translation.</citsent>
<aftsection>
<nextsent>(och et al , 2004) <papid> N04-1021 </papid>and(shen et al , 2004) <papid> N04-1023 </papid>describe the use of syntactic features in reranking the output of full translation system, but the syntactic features give very small gains.</nextsent>
<nextsent>in this paper, we present strategy to reorder source sentence using rules based on syntacticchunks.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1355">
<title id=" W07-0401.xml">chunk level reordering of source language sentences with automatically learned rules for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>note that these models have radically different structures and parameterizations than phrase-based models for smt.another kind of approaches is to use syntactic information in rescoring methods.
</prevsent>
<prevsent>(koehn and knight, 2003) <papid> E03-1076 </papid>apply reranking approach to the sub-task of noun-phrase translation.</prevsent>
</prevsection>
<citsent citstr=" N04-1021 ">
(och et al , 2004) <papid> N04-1021 </papid>and(shen et al , 2004) <papid> N04-1023 </papid>describe the use of syntactic features in reranking the output of full translation system, but the syntactic features give very small gains.</citsent>
<aftsection>
<nextsent>in this paper, we present strategy to reorder source sentence using rules based on syntacticchunks.
</nextsent>
<nextsent>it is possible to integrate reordering rules directly into the search process, but here, we considera more modular approach: easy to exchange reordering strategy.
</nextsent>
<nextsent>to avoid hard decisions before smt, we generate source-reordering lattice instead of single reordered source sentence as input to the smt system.
</nextsent>
<nextsent>then, the decoder uses the reordered source language model as an additional feature function.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1356">
<title id=" W07-0401.xml">chunk level reordering of source language sentences with automatically learned rules for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>note that these models have radically different structures and parameterizations than phrase-based models for smt.another kind of approaches is to use syntactic information in rescoring methods.
</prevsent>
<prevsent>(koehn and knight, 2003) <papid> E03-1076 </papid>apply reranking approach to the sub-task of noun-phrase translation.</prevsent>
</prevsection>
<citsent citstr=" N04-1023 ">
(och et al , 2004) <papid> N04-1021 </papid>and(shen et al , 2004) <papid> N04-1023 </papid>describe the use of syntactic features in reranking the output of full translation system, but the syntactic features give very small gains.</citsent>
<aftsection>
<nextsent>in this paper, we present strategy to reorder source sentence using rules based on syntacticchunks.
</nextsent>
<nextsent>it is possible to integrate reordering rules directly into the search process, but here, we considera more modular approach: easy to exchange reordering strategy.
</nextsent>
<nextsent>to avoid hard decisions before smt, we generate source-reordering lattice instead of single reordered source sentence as input to the smt system.
</nextsent>
<nextsent>then, the decoder uses the reordered source language model as an additional feature function.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1357">
<title id=" W07-0401.xml">chunk level reordering of source language sentences with automatically learned rules for statistical machine translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>finally, we conclude this paper and discuss future work in section 7.
</prevsent>
<prevsent>beside the reordering methods during decoding, an alternative approach is to reorder the input source sentence to match the word order of the target sen tence.some reordering methods are carried out on syntactic source trees.
</prevsent>
</prevsection>
<citsent citstr=" P05-1066 ">
(collins et al , 2005) <papid> P05-1066 </papid>describea method for reordering german for german-to english translation, where six transformations are applied to the surface string of the parsed sourcesentence.</citsent>
<aftsection>
<nextsent>(xia and mccord, 2004) <papid> C04-1073 </papid>propose an approach for translation from french-to-english.</nextsent>
<nextsent>this approach automatically extracts rewrite patterns by parsing the source and target sides of the training corpus.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1358">
<title id=" W07-0401.xml">chunk level reordering of source language sentences with automatically learned rules for statistical machine translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>beside the reordering methods during decoding, an alternative approach is to reorder the input source sentence to match the word order of the target sen tence.some reordering methods are carried out on syntactic source trees.
</prevsent>
<prevsent>(collins et al , 2005) <papid> P05-1066 </papid>describea method for reordering german for german-to english translation, where six transformations are applied to the surface string of the parsed sourcesentence.</prevsent>
</prevsection>
<citsent citstr=" C04-1073 ">
(xia and mccord, 2004) <papid> C04-1073 </papid>propose an approach for translation from french-to-english.</citsent>
<aftsection>
<nextsent>this approach automatically extracts rewrite patterns by parsing the source and target sides of the training corpus.
</nextsent>
<nextsent>these rewrite patterns can be applied to any input source sentence so that the rewritten source and target sentences have similar word order.
</nextsent>
<nextsent>both methods need parser to generate trees of source sentences and are applied only as preprocessing step.another kind of source reordering methods besides full parsing is based on part-of-speech (pos) tags or word classes.
</nextsent>
<nextsent>(costa-jussa` and fonollosa, 2006) view the source reordering as translation task that translate the source language into reordered source language.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1362">
<title id=" W07-0401.xml">chunk level reordering of source language sentences with automatically learned rules for statistical machine translation </title>
<section> system overview.  </section>
<citcontext>
<prevsection>
<prevsent>fj ,which is to be translated into target language sentence ei1 = e1 . . .
</prevsent>
<prevsent>ei . . .
</prevsent>
</prevsection>
<citsent citstr=" J90-2002 ">
ei . among all possible target language sentences, we will choose the sentence with the highest probability: ei1 = argmax i,ei1 { pr(ei1|fj1 ) } (1) = argmax i,ei1 { pr(ei1) ? pr(fj1 |ei1) } (2) this decomposition into two knowledge source sis known as the source-channel approach to statistical machine translation (brown et al , 1990).<papid> J90-2002 </papid></citsent>
<aftsection>
<nextsent>it allows an independent modeling of the target language model pr(ei1) and the translation model pr(fj1 |ei1).
</nextsent>
<nextsent>the target language model describes the well-formedness of the target language sentence.the translation model links the source language sentence to the target language sentence.
</nextsent>
<nextsent>the argmaxoperation denotes the search problem, i.e., the generation of the output sentence in the target language.
</nextsent>
<nextsent>a generalization of the classical source-channel approach is the direct modeling of the posterior probability pr(ei1|fj1 ).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1363">
<title id=" W07-0401.xml">chunk level reordering of source language sentences with automatically learned rules for statistical machine translation </title>
<section> system overview.  </section>
<citcontext>
<prevsection>
<prevsent>the argmaxoperation denotes the search problem, i.e., the generation of the output sentence in the target language.
</prevsent>
<prevsent>a generalization of the classical source-channel approach is the direct modeling of the posterior probability pr(ei1|fj1 ).
</prevsent>
</prevsection>
<citsent citstr=" P02-1038 ">
using log-linear model 2 (och and ney, 2002), <papid> P02-1038 </papid>we obtain: pr(ei1|fj1 ) = exp ( m=1 mhm(ei1, fj1 ) ) ? i?,ei1 exp ( m=1 mhm(ei ? 1 , fj1 ) ) (3) the denominator represents normalization factor that depends only on the source sentence fj1 . therefore, we can omit it during the search process.</citsent>
<aftsection>
<nextsent>as decision rule, we obtain: ei1 = argmax i,ei1 { ? m=1 mhm(ei1, fj1 ) } (4)the log-linear model has the advantage that additional models h(?)
</nextsent>
<nextsent>can be easily integrated into the overall system.
</nextsent>
<nextsent>the model scaling factors m1 are trained according to the maximum entropy principle, e.g., using the gis algorithm.
</nextsent>
<nextsent>alternatively, one can train them with respect to the final translation quality measured by an error criterion (och, 2003).<papid> P03-1021 </papid>the log-linear model is natural framework to integrate many models.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1364">
<title id=" W07-0401.xml">chunk level reordering of source language sentences with automatically learned rules for statistical machine translation </title>
<section> system overview.  </section>
<citcontext>
<prevsection>
<prevsent>can be easily integrated into the overall system.
</prevsent>
<prevsent>the model scaling factors m1 are trained according to the maximum entropy principle, e.g., using the gis algorithm.
</prevsent>
</prevsection>
<citsent citstr=" P03-1021 ">
alternatively, one can train them with respect to the final translation quality measured by an error criterion (och, 2003).<papid> P03-1021 </papid>the log-linear model is natural framework to integrate many models.</citsent>
<aftsection>
<nextsent>the baseline system uses the following models: ? phrase translation model ? phrase count features ? word-based translation model ? word and phrase penalty ? target language model (6-gram) ? distortion model (assigning costs based on the jump width)all the experiments in the paper are evaluated without rescoring.
</nextsent>
<nextsent>more details about the baseline system can be found in (mauser et al , 2006) 3.2 source sentence reordering framework.
</nextsent>
<nextsent>encouraged by the work of (xia and mccord, 2004) <papid> C04-1073 </papid>and (crego and marino, 2006), we also reorder the source language side.</nextsent>
<nextsent>compared to reordering onthe target language side, one advantage is the efficiency since the reordering lattice can be translatedmonotonically as in (zens et al , 2002).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1369">
<title id=" W07-0401.xml">chunk level reordering of source language sentences with automatically learned rules for statistical machine translation </title>
<section> reordering rules.  </section>
<citcontext>
<prevsection>
<prevsent>4.2 extraction of reordering rules.
</prevsent>
<prevsent>the extraction of reordering rules is based on the word alignment and the source sentence chunks.
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
here, we train word alignments in both directions with giza++ (och and ney, 2003).<papid> J03-1002 </papid></citsent>
<aftsection>
<nextsent>to get al gn ment with high accuracy, we use the intersection alignment here.
</nextsent>
<nextsent>forgiven word-aligned sentence pair (fj1 , ei1, aj1 ), the source word sequence fj1 isfirst parsed into chunk sequence fk1 . accordingly, the word-to-word alignment aj1 is changed to chunk-to-word alignment ak1 which is the combination of the target words aligned to the source words in chunk.
</nextsent>
<nextsent>it is defined as: ak = {i|i = aj ? ? [jk, jk+1 ? 1]} figure 2: illustration of three kinds of phrases: (a)monotone phrase, (b)reordering phrase, (c)cross phrase.
</nextsent>
<nextsent>the black box is word-to-word alignment.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1370">
<title id=" W07-0401.xml">chunk level reordering of source language sentences with automatically learned rules for statistical machine translation </title>
<section> reordering lattice generation.  </section>
<citcontext>
<prevsection>
<prevsent>in our experiments, we use the tool of inst.
</prevsent>
<prevsent>of computing tech., chinese lexical analysis system (ictclas)?
</prevsent>
</prevsection>
<citsent citstr=" W03-1709 ">
(zhang et al , 2003), <papid> W03-1709 </papid>which does the two tasks in one pass.</citsent>
<aftsection>
<nextsent>referring to the description of the chunking task in conll-20001, instead of english, chinese chunker is processed and evaluated.
</nextsent>
<nextsent>each word is assigned chunk tag, which contains the name of the chunk type and b? for the first word of the chunk and i? for each other word in the chunk.
</nextsent>
<nextsent>the o? chunk tag is used for tokens which are not part ofany chunk.
</nextsent>
<nextsent>we use the maximum entropy tool yas 1http://www.cnts.ua.ac.be/conll2000/chunking/ 4 figure 3: example of applying rules.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1371">
<title id=" W07-0401.xml">chunk level reordering of source language sentences with automatically learned rules for statistical machine translation </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>the results at the chunk level are worse than at the word level, because chunk is counted as correct only if the chunk tag and the chunk boundaries are both correct.
</prevsent>
<prevsent>6.2 translation results.
</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
for the translation experiments, we report the two accuracy measures bleu (papineni et al , 2002) <papid> P02-1040 </papid>and nist (doddington, 2002) as well as the two error rates word error rate (wer) and position independent word error rate (per).we perform translation experiments on the basic traveling expression corpus (btec) for the chinese-english task.</citsent>
<aftsection>
<nextsent>it is speech translation task in the domain of tourism-related information.
</nextsent>
<nextsent>we report results on the iwslt 2004, 2005 and 2006evaluation test sets.
</nextsent>
<nextsent>there are 16 reference translations for the iwslt 2004 and 2005 tasks and 7 reference translations for the iwslt 2006 task.
</nextsent>
<nextsent>table 4 shows the corpus statistics of the task.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1374">
<title id=" W06-1411.xml">group based generation of referring expressions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>to over come this limitation, this paper proposesa novel generation method utilizing perceptual groups of objects and n-ary relations among them.
</prevsent>
<prevsent>the evaluation using 18 subjects showed that the proposed method could effectively generate proper referring expressions.
</prevsent>
</prevsection>
<citsent citstr=" E91-1028 ">
in the last two decades, many researchers have studied the generation of referring expressions to enable computers to communicate with humans about objects in the world.in order to refer to an intended object (the tar get) among others (distractors), most past work (appelt, 1985; dale and haddock, 1991; <papid> E91-1028 </papid>dale, 1992; dale and reiter, 1995; heeman and hirst, 1995; <papid> J95-3003 </papid>horacek, 1997; <papid> P97-1027 </papid>krahmer and theune, 2002; van deemter, 2002; krahmer et al, 2003) <papid> J03-1003 </papid>utilized attributes of the target and binary relations between the target and distractors.</citsent>
<aftsection>
<nextsent>therefore, these methods cannot generate proper referring expressions in situations where there is no significant surface difference between the target and distrac tors, and no binary relation is useful to distinguish the target.
</nextsent>
<nextsent>here, proper referring expression currently at honda research institute japan co., ltd. currently at hitachi, ltd. means concise and natural linguistic expression enabling hearers to identify the target.for example, consider indicating object to person in the situation of figure 1.
</nextsent>
<nextsent>note that labels a, and are assigned for explanation to the readers, and person does not share these labels with the speaker.
</nextsent>
<nextsent>because object is not distinguishable from objects or by means of their appearance, one would try to use binary relation between object and the table, i.e., ball to the right of the table?.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1375">
<title id=" W06-1411.xml">group based generation of referring expressions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>to over come this limitation, this paper proposesa novel generation method utilizing perceptual groups of objects and n-ary relations among them.
</prevsent>
<prevsent>the evaluation using 18 subjects showed that the proposed method could effectively generate proper referring expressions.
</prevsent>
</prevsection>
<citsent citstr=" J95-3003 ">
in the last two decades, many researchers have studied the generation of referring expressions to enable computers to communicate with humans about objects in the world.in order to refer to an intended object (the tar get) among others (distractors), most past work (appelt, 1985; dale and haddock, 1991; <papid> E91-1028 </papid>dale, 1992; dale and reiter, 1995; heeman and hirst, 1995; <papid> J95-3003 </papid>horacek, 1997; <papid> P97-1027 </papid>krahmer and theune, 2002; van deemter, 2002; krahmer et al, 2003) <papid> J03-1003 </papid>utilized attributes of the target and binary relations between the target and distractors.</citsent>
<aftsection>
<nextsent>therefore, these methods cannot generate proper referring expressions in situations where there is no significant surface difference between the target and distrac tors, and no binary relation is useful to distinguish the target.
</nextsent>
<nextsent>here, proper referring expression currently at honda research institute japan co., ltd. currently at hitachi, ltd. means concise and natural linguistic expression enabling hearers to identify the target.for example, consider indicating object to person in the situation of figure 1.
</nextsent>
<nextsent>note that labels a, and are assigned for explanation to the readers, and person does not share these labels with the speaker.
</nextsent>
<nextsent>because object is not distinguishable from objects or by means of their appearance, one would try to use binary relation between object and the table, i.e., ball to the right of the table?.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1376">
<title id=" W06-1411.xml">group based generation of referring expressions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>to over come this limitation, this paper proposesa novel generation method utilizing perceptual groups of objects and n-ary relations among them.
</prevsent>
<prevsent>the evaluation using 18 subjects showed that the proposed method could effectively generate proper referring expressions.
</prevsent>
</prevsection>
<citsent citstr=" P97-1027 ">
in the last two decades, many researchers have studied the generation of referring expressions to enable computers to communicate with humans about objects in the world.in order to refer to an intended object (the tar get) among others (distractors), most past work (appelt, 1985; dale and haddock, 1991; <papid> E91-1028 </papid>dale, 1992; dale and reiter, 1995; heeman and hirst, 1995; <papid> J95-3003 </papid>horacek, 1997; <papid> P97-1027 </papid>krahmer and theune, 2002; van deemter, 2002; krahmer et al, 2003) <papid> J03-1003 </papid>utilized attributes of the target and binary relations between the target and distractors.</citsent>
<aftsection>
<nextsent>therefore, these methods cannot generate proper referring expressions in situations where there is no significant surface difference between the target and distrac tors, and no binary relation is useful to distinguish the target.
</nextsent>
<nextsent>here, proper referring expression currently at honda research institute japan co., ltd. currently at hitachi, ltd. means concise and natural linguistic expression enabling hearers to identify the target.for example, consider indicating object to person in the situation of figure 1.
</nextsent>
<nextsent>note that labels a, and are assigned for explanation to the readers, and person does not share these labels with the speaker.
</nextsent>
<nextsent>because object is not distinguishable from objects or by means of their appearance, one would try to use binary relation between object and the table, i.e., ball to the right of the table?.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1377">
<title id=" W06-1411.xml">group based generation of referring expressions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>to over come this limitation, this paper proposesa novel generation method utilizing perceptual groups of objects and n-ary relations among them.
</prevsent>
<prevsent>the evaluation using 18 subjects showed that the proposed method could effectively generate proper referring expressions.
</prevsent>
</prevsection>
<citsent citstr=" J03-1003 ">
in the last two decades, many researchers have studied the generation of referring expressions to enable computers to communicate with humans about objects in the world.in order to refer to an intended object (the tar get) among others (distractors), most past work (appelt, 1985; dale and haddock, 1991; <papid> E91-1028 </papid>dale, 1992; dale and reiter, 1995; heeman and hirst, 1995; <papid> J95-3003 </papid>horacek, 1997; <papid> P97-1027 </papid>krahmer and theune, 2002; van deemter, 2002; krahmer et al, 2003) <papid> J03-1003 </papid>utilized attributes of the target and binary relations between the target and distractors.</citsent>
<aftsection>
<nextsent>therefore, these methods cannot generate proper referring expressions in situations where there is no significant surface difference between the target and distrac tors, and no binary relation is useful to distinguish the target.
</nextsent>
<nextsent>here, proper referring expression currently at honda research institute japan co., ltd. currently at hitachi, ltd. means concise and natural linguistic expression enabling hearers to identify the target.for example, consider indicating object to person in the situation of figure 1.
</nextsent>
<nextsent>note that labels a, and are assigned for explanation to the readers, and person does not share these labels with the speaker.
</nextsent>
<nextsent>because object is not distinguishable from objects or by means of their appearance, one would try to use binary relation between object and the table, i.e., ball to the right of the table?.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1378">
<title id=" W06-1411.xml">group based generation of referring expressions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>is not discriminatory relation, for objects and are also located to the right of the table.
</prevsent>
<prevsent>using and as reference object instead of the table does not make sense, since and cannot be uniquely identified because of the same reason that cannot be identified.
</prevsent>
</prevsection>
<citsent citstr=" W00-1416 ">
such situations have drawn less attention (stone, 2000), <papid> W00-1416 </papid>but can frequently occur in some domains such as object arrangement (tanaka et al, 2004).</citsent>
<aftsection>
<nextsent>p b table figure 1: an example of problematic situation sin the situation of figure 1, the speaker can indicate object to person with simple expression the front ball?.
</nextsent>
<nextsent>in order to generate such an expression, one must be able to recognize the salient perceptual group of the objects and use the n-ary relative relations in the group.
</nextsent>
<nextsent>73to overcome the problem described above, funakoshi et al (2004) proposed method of generating japanese referring expressions that utilizes n-ary relations among members of group.
</nextsent>
<nextsent>they, however, dealt with the limited situations where only homogeneous objects are randomly arranged(see figure 2).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1379">
<title id=" W07-0716.xml">using paraphrases for parameter tuning in statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, obtaining reference translations is expensive.
</prevsent>
<prevsent>in this paper, we introduce new full-sentence paraphrase technique, based on english-to-english decoding with an mtsystem, and we demonstrate that the resulting paraphrases can be used to drastically reduce the number of human reference translations needed for parameter tuning, without significant decrease in translation quality.
</prevsent>
</prevsection>
<citsent citstr=" N03-1017 ">
viewed at very high level, statistical machine translation involves four phases: language and translation model training, parameter tuning, decoding, and evaluation (lopez, 2007; koehn et al, 2003).<papid> N03-1017 </papid></citsent>
<aftsection>
<nextsent>since their introduction in statistical mt by och and ney (2002), <papid> P02-1038 </papid>log-linear models have been standard way to combine sub-models in mt systems.</nextsent>
<nextsent>typically such model takes the form ? ii(f? , e?)</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1380">
<title id=" W07-0716.xml">using paraphrases for parameter tuning in statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this paper, we introduce new full-sentence paraphrase technique, based on english-to-english decoding with an mtsystem, and we demonstrate that the resulting paraphrases can be used to drastically reduce the number of human reference translations needed for parameter tuning, without significant decrease in translation quality.
</prevsent>
<prevsent>viewed at very high level, statistical machine translation involves four phases: language and translation model training, parameter tuning, decoding, and evaluation (lopez, 2007; koehn et al, 2003).<papid> N03-1017 </papid></prevsent>
</prevsection>
<citsent citstr=" P02-1038 ">
since their introduction in statistical mt by och and ney (2002), <papid> P02-1038 </papid>log-linear models have been standard way to combine sub-models in mt systems.</citsent>
<aftsection>
<nextsent>typically such model takes the form ? ii(f? , e?)
</nextsent>
<nextsent>(1) where are features of the hypothesis and are weights associated with those features.
</nextsent>
<nextsent>selecting appropriate weights is essential in order to obtain good translation performance.och (2003) <papid> P03-1021 </papid>introduced minimum error rate training (mert), technique for optimizing log-linear model parameters relative to measure of translation quality.</nextsent>
<nextsent>this has become much more standard than optimizing the conditional probability of the training data given the model (i.e., maximum likelihood criterion), as was common previously.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1381">
<title id=" W07-0716.xml">using paraphrases for parameter tuning in statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>typically such model takes the form ? ii(f? , e?)
</prevsent>
<prevsent>(1) where are features of the hypothesis and are weights associated with those features.
</prevsent>
</prevsection>
<citsent citstr=" P03-1021 ">
selecting appropriate weights is essential in order to obtain good translation performance.och (2003) <papid> P03-1021 </papid>introduced minimum error rate training (mert), technique for optimizing log-linear model parameters relative to measure of translation quality.</citsent>
<aftsection>
<nextsent>this has become much more standard than optimizing the conditional probability of the training data given the model (i.e., maximum likelihood criterion), as was common previously.
</nextsent>
<nextsent>och showed that system performance is best when parameters are optimized using the same objective function that will be used for evaluation; bleu (papineni et al, 2002)<papid> P02-1040 </papid>remains common for both purposes and is often retained for parameter optimization even when alternative evaluation measures are used, e.g., (banerjee and lavie, 2005; <papid> W05-0909 </papid>snover et al, 2006).minimum error rate training and more generally, optimization of parameters relative to translation quality measure relies on datasets in which source language sentences are paired with (sets of) reference translations.</nextsent>
<nextsent>it is widely agreed that, at least for the widely used bleu criterion, which is based on n-gram overlap between hypotheses and reference translations, the criterion is most accurate when computed with as many distinct reference translations as possible.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1382">
<title id=" W07-0716.xml">using paraphrases for parameter tuning in statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>selecting appropriate weights is essential in order to obtain good translation performance.och (2003) <papid> P03-1021 </papid>introduced minimum error rate training (mert), technique for optimizing log-linear model parameters relative to measure of translation quality.</prevsent>
<prevsent>this has become much more standard than optimizing the conditional probability of the training data given the model (i.e., maximum likelihood criterion), as was common previously.</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
och showed that system performance is best when parameters are optimized using the same objective function that will be used for evaluation; bleu (papineni et al, 2002)<papid> P02-1040 </papid>remains common for both purposes and is often retained for parameter optimization even when alternative evaluation measures are used, e.g., (banerjee and lavie, 2005; <papid> W05-0909 </papid>snover et al, 2006).minimum error rate training and more generally, optimization of parameters relative to translation quality measure relies on datasets in which source language sentences are paired with (sets of) reference translations.</citsent>
<aftsection>
<nextsent>it is widely agreed that, at least for the widely used bleu criterion, which is based on n-gram overlap between hypotheses and reference translations, the criterion is most accurate when computed with as many distinct reference translations as possible.
</nextsent>
<nextsent>intuitively this makes sense: if there are alternative ways to phrase the meaning of the source sentence in the target language, then the translation quality criterion should take as many of those variations into account as possible.
</nextsent>
<nextsent>to do otherwise is to risk the possibility that the criterion might judge good translations to be poor when they fail to match the exact wording within the reference translations that have been provided.
</nextsent>
<nextsent>this reliance on multiple reference translations creates problem, because reference translations are labor intensive and expensive to obtain.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1383">
<title id=" W07-0716.xml">using paraphrases for parameter tuning in statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>selecting appropriate weights is essential in order to obtain good translation performance.och (2003) <papid> P03-1021 </papid>introduced minimum error rate training (mert), technique for optimizing log-linear model parameters relative to measure of translation quality.</prevsent>
<prevsent>this has become much more standard than optimizing the conditional probability of the training data given the model (i.e., maximum likelihood criterion), as was common previously.</prevsent>
</prevsection>
<citsent citstr=" W05-0909 ">
och showed that system performance is best when parameters are optimized using the same objective function that will be used for evaluation; bleu (papineni et al, 2002)<papid> P02-1040 </papid>remains common for both purposes and is often retained for parameter optimization even when alternative evaluation measures are used, e.g., (banerjee and lavie, 2005; <papid> W05-0909 </papid>snover et al, 2006).minimum error rate training and more generally, optimization of parameters relative to translation quality measure relies on datasets in which source language sentences are paired with (sets of) reference translations.</citsent>
<aftsection>
<nextsent>it is widely agreed that, at least for the widely used bleu criterion, which is based on n-gram overlap between hypotheses and reference translations, the criterion is most accurate when computed with as many distinct reference translations as possible.
</nextsent>
<nextsent>intuitively this makes sense: if there are alternative ways to phrase the meaning of the source sentence in the target language, then the translation quality criterion should take as many of those variations into account as possible.
</nextsent>
<nextsent>to do otherwise is to risk the possibility that the criterion might judge good translations to be poor when they fail to match the exact wording within the reference translations that have been provided.
</nextsent>
<nextsent>this reliance on multiple reference translations creates problem, because reference translations are labor intensive and expensive to obtain.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1384">
<title id=" W07-0716.xml">using paraphrases for parameter tuning in statistical machine translation </title>
<section> translation framework.  </section>
<citcontext>
<prevsection>
<prevsent>, x) denotes feature function defined on the pair of hierarchical phrases.1 feature functions represent conditional and joint co-occurrence probabilities over the hierarchical paraphrase pair.
</prevsent>
<prevsent>the hiero framework includes methods to learn grammars and feature values from unannotated parallel corpora, without requiring syntactic annotation of the data.
</prevsent>
</prevsection>
<citsent citstr=" P00-1056 ">
briefly, training hiero model proceeds as follows: ? giza++ (och and ney, 2000) <papid> P00-1056 </papid>is run on the parallel corpus in both directions, followed by an alignment refinement heuristic that yields amany-to-many alignment for each parallel sen tence.</citsent>
<aftsection>
<nextsent>initial phrase pairs are identified following the procedure typically employed in phrase based systems (koehn et al, 2003; <papid> N03-1017 </papid>och and ney, 2004).<papid> J04-4002 </papid></nextsent>
<nextsent>grammar rules in the form of equation (2) are induced by subtracting?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1386">
<title id=" W07-0716.xml">using paraphrases for parameter tuning in statistical machine translation </title>
<section> translation framework.  </section>
<citcontext>
<prevsection>
<prevsent>the hiero framework includes methods to learn grammars and feature values from unannotated parallel corpora, without requiring syntactic annotation of the data.
</prevsent>
<prevsent>briefly, training hiero model proceeds as follows: ? giza++ (och and ney, 2000) <papid> P00-1056 </papid>is run on the parallel corpus in both directions, followed by an alignment refinement heuristic that yields amany-to-many alignment for each parallel sen tence.</prevsent>
</prevsection>
<citsent citstr=" J04-4002 ">
initial phrase pairs are identified following the procedure typically employed in phrase based systems (koehn et al, 2003; <papid> N03-1017 </papid>och and ney, 2004).<papid> J04-4002 </papid></citsent>
<aftsection>
<nextsent>grammar rules in the form of equation (2) are induced by subtracting?
</nextsent>
<nextsent>out hierarchical phrase pairs from these initial phrase pairs.?
</nextsent>
<nextsent>fractional counts are assigned to each produced rule: c(x ? e?, f??)
</nextsent>
<nextsent>= m?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1388">
<title id=" W07-0716.xml">using paraphrases for parameter tuning in statistical machine translation </title>
<section> generating paraphrases.  </section>
<citcontext>
<prevsection>
<prevsent>as discussed in section 1, our goal is to make it possible to accomplish the parameter-tuning phase using fewer human reference translations.
</prevsent>
<prevsent>we accomplish this by beginning with small set of human reference translations for each sentence in the development set, and expanding that set by automatically paraphrasing each member of the set rather than by acquiring more human translations.
</prevsent>
</prevsection>
<citsent citstr=" N03-1003 ">
most previous work on paraphrase has focused on high quality rather than coverage (barzilay andlee, 2003; <papid> N03-1003 </papid>quirk et al, 2004), <papid> W04-3219 </papid>but generating artificial references for mt parameter tuning in our setting has two unique properties compared to other paraphrase applications.</citsent>
<aftsection>
<nextsent>first, we would like to obtain 100% coverage, in order to avoid modifications to our minimum error rate training infrastructure.2 second, we prefer that paraphrases be as distinct as possible from the original sentences, while retaining as much of the original meaning as possible.in order to satisfy these two properties, we approach sentence-level paraphrase for english asa problem of english-to-english translation, constructing the model using english-f translation, fora second language , as pivot.
</nextsent>
<nextsent>following bannard and callison-burch (2005), we first identifyenglish-to-f correspondences, then map from english to english by following translation units from english to and back.
</nextsent>
<nextsent>then, generalizing their approach, we use those mappings to create well defined english-to-english translation model.
</nextsent>
<nextsent>the parameters of this model are tuned using mert, andthen the model is used in an the (unmodified) statistical mt system, yielding sentence-level english paraphrases by means of decoding input english sentences.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1389">
<title id=" W07-0716.xml">using paraphrases for parameter tuning in statistical machine translation </title>
<section> generating paraphrases.  </section>
<citcontext>
<prevsection>
<prevsent>as discussed in section 1, our goal is to make it possible to accomplish the parameter-tuning phase using fewer human reference translations.
</prevsent>
<prevsent>we accomplish this by beginning with small set of human reference translations for each sentence in the development set, and expanding that set by automatically paraphrasing each member of the set rather than by acquiring more human translations.
</prevsent>
</prevsection>
<citsent citstr=" W04-3219 ">
most previous work on paraphrase has focused on high quality rather than coverage (barzilay andlee, 2003; <papid> N03-1003 </papid>quirk et al, 2004), <papid> W04-3219 </papid>but generating artificial references for mt parameter tuning in our setting has two unique properties compared to other paraphrase applications.</citsent>
<aftsection>
<nextsent>first, we would like to obtain 100% coverage, in order to avoid modifications to our minimum error rate training infrastructure.2 second, we prefer that paraphrases be as distinct as possible from the original sentences, while retaining as much of the original meaning as possible.in order to satisfy these two properties, we approach sentence-level paraphrase for english asa problem of english-to-english translation, constructing the model using english-f translation, fora second language , as pivot.
</nextsent>
<nextsent>following bannard and callison-burch (2005), we first identifyenglish-to-f correspondences, then map from english to english by following translation units from english to and back.
</nextsent>
<nextsent>then, generalizing their approach, we use those mappings to create well defined english-to-english translation model.
</nextsent>
<nextsent>the parameters of this model are tuned using mert, andthen the model is used in an the (unmodified) statistical mt system, yielding sentence-level english paraphrases by means of decoding input english sentences.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1391">
<title id=" W07-0716.xml">using paraphrases for parameter tuning in statistical machine translation </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>note that since we have only four human references per item, this fourth condition does not permit comparison with an upper bound of eight human references.
</prevsent>
<prevsent>table 4 shows bleu and ter scores on the testset for all four conditions.5 if only two human references were available (simulated by using only twoof the available four), expanding to four using paraphrases would yield clear improvement.
</prevsent>
</prevsection>
<citsent citstr=" W04-3250 ">
using bootstrap re sampling to compute confidence intervals (koehn, 2004), <papid> W04-3250 </papid>we find that the improvement in bleu score is statistically significant at   .01.equally interesting, expanding the number of reference translations from two to four using paraphrases yields performance that approaches the upper bound obtained by doing mert using all four human reference translations.</citsent>
<aftsection>
<nextsent>the difference in bleu between conditions 2 and 3 is not significant.
</nextsent>
<nextsent>finally, our fourth condition asks whether it is possible to improve mt performance given the typical four human reference translations used for mert in most statistical mt systems, by adding paraphrase to each one for total eight references per translation.
</nextsent>
<nextsent>there is indeed further improvement, although the difference in bleu score does not reach significance.
</nextsent>
<nextsent>5we plan to include meteor scores in future experiments.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1392">
<title id=" W07-0716.xml">using paraphrases for parameter tuning in statistical machine translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>however, this output is chosen from space in which the number of possible outputs is exponential in the input size, and inwhich there are many good outputs in this space (although they are vastly outnumbered by the bad out puts).
</prevsent>
<prevsent>various discriminative learning methods have attempted to deal with the first of these issues, often by restricting the space of examples.
</prevsent>
</prevsection>
<citsent citstr=" P06-1091 ">
for instance,some max-margin methods restrict their computations to set of examples from feasible set,where they are expected to be maximally discriminative (tillmann and zhang, 2006).<papid> P06-1091 </papid></citsent>
<aftsection>
<nextsent>the present approach deals with the second issue: in learning problem where the use of single positive example is likely to be highly biased, how can we produce set of positive examples that is more representative of the space of correct outcomes?
</nextsent>
<nextsent>our method exploits alternative sources of information to produce new positive examples that are, we hope, reasonably likely to represent consensus of good examples.
</nextsent>
<nextsent>quite bit of work has been done on paraphrase, 6we anticipate doing significance tests for differences inter in future work.
</nextsent>
<nextsent>125 some clearly related to our technique, although in general previous work has been focused on human readability rather than high coverage, noisy paraphrases for use downstream in an automatic process.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1396">
<title id=" W07-0716.xml">using paraphrases for parameter tuning in statistical machine translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>and, in fact, almost third of their best sentential paraphrases?
</prevsent>
<prevsent>are identical to the in put sentence.
</prevsent>
</prevsection>
<citsent citstr=" W03-1608 ">
a number of other approaches relyon parallel monolingual data and, additionally, require parsing of the training sentences (ibrahim et al, 2003; <papid> W03-1608 </papid>pang et al, 2003).<papid> N03-1024 </papid></citsent>
<aftsection>
<nextsent>lin and pantel (2001) use non-parallel corpus and employ dependency parser and computation of distributional similarity to learn paraphrases.there has also been recent work on using paraphrases to improve statistical machine translation.
</nextsent>
<nextsent>callison-burch et al (2006) extract phrase-level paraphrases by mapping input phrases into phrase table and then mapping back to the source language.
</nextsent>
<nextsent>however, they do not generate paraphrases of entire sentences, but instead employ paraphrases to add entries to an existing phrase table solely for the purpose of increasing source-language coverage.
</nextsent>
<nextsent>other work has incorporated paraphrases into mtevaluation: russo-lassner et al (2005) use combination of paraphrase-based features to evaluate translation output; zhou et al (2006) <papid> N06-1057 </papid>propose new metric that extends n-gram matching to include synonyms and paraphrases; and lavies meteor metric (banerjee and lavie, 2005) <papid> W05-0909 </papid>can be used with additional knowledge such as wordnet in order to support inexact lexical matches.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1397">
<title id=" W07-0716.xml">using paraphrases for parameter tuning in statistical machine translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>and, in fact, almost third of their best sentential paraphrases?
</prevsent>
<prevsent>are identical to the in put sentence.
</prevsent>
</prevsection>
<citsent citstr=" N03-1024 ">
a number of other approaches relyon parallel monolingual data and, additionally, require parsing of the training sentences (ibrahim et al, 2003; <papid> W03-1608 </papid>pang et al, 2003).<papid> N03-1024 </papid></citsent>
<aftsection>
<nextsent>lin and pantel (2001) use non-parallel corpus and employ dependency parser and computation of distributional similarity to learn paraphrases.there has also been recent work on using paraphrases to improve statistical machine translation.
</nextsent>
<nextsent>callison-burch et al (2006) extract phrase-level paraphrases by mapping input phrases into phrase table and then mapping back to the source language.
</nextsent>
<nextsent>however, they do not generate paraphrases of entire sentences, but instead employ paraphrases to add entries to an existing phrase table solely for the purpose of increasing source-language coverage.
</nextsent>
<nextsent>other work has incorporated paraphrases into mtevaluation: russo-lassner et al (2005) use combination of paraphrase-based features to evaluate translation output; zhou et al (2006) <papid> N06-1057 </papid>propose new metric that extends n-gram matching to include synonyms and paraphrases; and lavies meteor metric (banerjee and lavie, 2005) <papid> W05-0909 </papid>can be used with additional knowledge such as wordnet in order to support inexact lexical matches.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1398">
<title id=" W07-0716.xml">using paraphrases for parameter tuning in statistical machine translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>callison-burch et al (2006) extract phrase-level paraphrases by mapping input phrases into phrase table and then mapping back to the source language.
</prevsent>
<prevsent>however, they do not generate paraphrases of entire sentences, but instead employ paraphrases to add entries to an existing phrase table solely for the purpose of increasing source-language coverage.
</prevsent>
</prevsection>
<citsent citstr=" N06-1057 ">
other work has incorporated paraphrases into mtevaluation: russo-lassner et al (2005) use combination of paraphrase-based features to evaluate translation output; zhou et al (2006) <papid> N06-1057 </papid>propose new metric that extends n-gram matching to include synonyms and paraphrases; and lavies meteor metric (banerjee and lavie, 2005) <papid> W05-0909 </papid>can be used with additional knowledge such as wordnet in order to support inexact lexical matches.</citsent>
<aftsection>
<nextsent>we introduced an automatic paraphrasing technique based on english-to-english translation of full sentences using statistical mt system, and demonstrated that, using this technique, it is possible tocut in half the usual number of reference translations used for minimum error rate training with no significant loss in translation quality.
</nextsent>
<nextsent>our method enables the generation of paraphrases for thousands of sentences in very short amount of time (muchshorter than creating other low-cost human references).
</nextsent>
<nextsent>this might prove beneficial for various discriminative training methods (tillmann and zhang, 2006).<papid> P06-1091 </papid>this has important implications for data acquisition strategies for example, it suggests that rather than obtaining four reference translations per sentence for development sets, it may be more worth while to obtain fewer translations for wider range of sentences, e.g., expanding into new topics and genres.</nextsent>
<nextsent>in addition, this approach can significantly increase the utility of datasets which include only single reference translation.a number of future research directions are pos sible.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1402">
<title id=" W06-1657.xml">short text authorship attribution via sequence kernels markov chains and author unmasking an investigation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>specific examples are the federalist papers (hanus and hagen auer, 2005; mos teller, 1984) and the forensic analysis of the una bomber manifesto (foster, 2001).within the area of automatic author attribution, recently it has been shown that encouraging performance can be achieved via the use of probabilistic models based on n-grams (clement and sharp, 2003) and markov chains of characters and words (peng et al, 2004).
</prevsent>
<prevsent>diederich et al (2003) showed that support vector machines (svms), using the bag-of-words kernel,can obtain promising performance, while in an other study, svms with kernels based on character collocations obtained mixed performance (cor ney, 2003).
</prevsent>
</prevsection>
<citsent citstr=" C04-1088 ">
gamon (2004) <papid> C04-1088 </papid>utilised svms with syntactic and semantic features to obtain relatively minor accuracy improvements over the use of function word frequencies and part-of-speech trigrams.</citsent>
<aftsection>
<nextsent>koppel &amp; schler (2004) proposed word-level heuristic, resembling recursive feature elimination used for cancer classification (guyon et al, 2002; huang and kecman, 2005), to obtain author unmasking curves.
</nextsent>
<nextsent>the curves were processed to obtain feature vectors that were in turn classified in traditional svm setting.the studies listed above have several limitations.
</nextsent>
<nextsent>in (clement and sharp, 2003), rudimentary probability smoothing technique was used to handle n-grams which were unseen during the training phase.
</nextsent>
<nextsent>in the dataset used by (peng et al, 2004) each author tended to stick to one or two topics, raising the possibility that the discrimination was based on topic rather than by author style.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1407">
<title id=" W06-1807.xml">merging stories with shallow semantics </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this technique is extended with reasoning engine that borrows techniques from dynamic ontology refinement to discover the semantic similarity of stories and to merge them together.
</prevsent>
<prevsent>many nlp applications would benefit from the availability of broad-coverage knowledge extraction from natural language text.
</prevsent>
</prevsection>
<citsent citstr=" C04-1180 ">
despite some recent advances in this direction (bos et al, 2004), <papid> C04-1180 </papid>itis still the case that it is hard to obtain deep semantic analyses which are accurate enough to support logical inference (lev et al, 2004).<papid> W04-0902 </papid></citsent>
<aftsection>
<nextsent>our problem can be stated in abstract terms.given formalism for the semantic representation of natural language, such as first-order clauses in predicate calculus, and set of sentences s, give translation function (s) ? . the goal of such translation would be to solve problem (such as paraphrasing or question-answering)where allows to be solved by some reasoning process, or else the domain exhibits type of structure easily represented in the formalism . if we accept that current parsing technology cannot reliably combine accurate semantic analysis with robustness, then the question arises whether noisy?
</nextsent>
<nextsent>semantics can be ameliorated using some other techniques.
</nextsent>
<nextsent>in this paper, we adopt the hypothesis that methods drawn from dynamic ontology refinement (mcneill et al, 2004) can indeed help with this task.
</nextsent>
<nextsent>in the limit, we would like to be able to show that semantic content drawn from wide variety of sources can be compared and merged to reveal the shared common ground.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1408">
<title id=" W06-1807.xml">merging stories with shallow semantics </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this technique is extended with reasoning engine that borrows techniques from dynamic ontology refinement to discover the semantic similarity of stories and to merge them together.
</prevsent>
<prevsent>many nlp applications would benefit from the availability of broad-coverage knowledge extraction from natural language text.
</prevsent>
</prevsection>
<citsent citstr=" W04-0902 ">
despite some recent advances in this direction (bos et al, 2004), <papid> C04-1180 </papid>itis still the case that it is hard to obtain deep semantic analyses which are accurate enough to support logical inference (lev et al, 2004).<papid> W04-0902 </papid></citsent>
<aftsection>
<nextsent>our problem can be stated in abstract terms.given formalism for the semantic representation of natural language, such as first-order clauses in predicate calculus, and set of sentences s, give translation function (s) ? . the goal of such translation would be to solve problem (such as paraphrasing or question-answering)where allows to be solved by some reasoning process, or else the domain exhibits type of structure easily represented in the formalism . if we accept that current parsing technology cannot reliably combine accurate semantic analysis with robustness, then the question arises whether noisy?
</nextsent>
<nextsent>semantics can be ameliorated using some other techniques.
</nextsent>
<nextsent>in this paper, we adopt the hypothesis that methods drawn from dynamic ontology refinement (mcneill et al, 2004) can indeed help with this task.
</nextsent>
<nextsent>in the limit, we would like to be able to show that semantic content drawn from wide variety of sources can be compared and merged to reveal the shared common ground.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1409">
<title id=" W06-1807.xml">merging stories with shallow semantics </title>
<section> extracting clauses from text.  </section>
<citcontext>
<prevsection>
<prevsent>2.
</prevsent>
<prevsent>the words are tagged for their part of speech.
</prevsent>
</prevsection>
<citsent citstr=" P04-1014 ">
using the candc tagger (clark and curran, 2004) <papid> P04-1014 </papid>and the penn treebank tagset.</citsent>
<aftsection>
<nextsent>the glencova pronoun resolution algorithm (halpin et al, 2004), <papid> W04-3217 </papid>based on series ofrules similar to the cogniac engine (baldwin, 1997), <papid> W97-1306 </papid>but without gender information based rules since this is not provided by the penn treebank tagset.</nextsent>
<nextsent>4.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1410">
<title id=" W06-1807.xml">merging stories with shallow semantics </title>
<section> extracting clauses from text.  </section>
<citcontext>
<prevsection>
<prevsent>the words are tagged for their part of speech.
</prevsent>
<prevsent>using the candc tagger (clark and curran, 2004) <papid> P04-1014 </papid>and the penn treebank tagset.</prevsent>
</prevsection>
<citsent citstr=" W04-3217 ">
the glencova pronoun resolution algorithm (halpin et al, 2004), <papid> W04-3217 </papid>based on series ofrules similar to the cogniac engine (baldwin, 1997), <papid> W97-1306 </papid>but without gender information based rules since this is not provided by the penn treebank tagset.</citsent>
<aftsection>
<nextsent>4.
</nextsent>
<nextsent>the words are then reduced to their morpho-.
</nextsent>
<nextsent>logical stem (lemma) using morpha (minnen et al, 2001).
</nextsent>
<nextsent>5.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1411">
<title id=" W06-1807.xml">merging stories with shallow semantics </title>
<section> extracting clauses from text.  </section>
<citcontext>
<prevsection>
<prevsent>the words are tagged for their part of speech.
</prevsent>
<prevsent>using the candc tagger (clark and curran, 2004) <papid> P04-1014 </papid>and the penn treebank tagset.</prevsent>
</prevsection>
<citsent citstr=" W97-1306 ">
the glencova pronoun resolution algorithm (halpin et al, 2004), <papid> W04-3217 </papid>based on series ofrules similar to the cogniac engine (baldwin, 1997), <papid> W97-1306 </papid>but without gender information based rules since this is not provided by the penn treebank tagset.</citsent>
<aftsection>
<nextsent>4.
</nextsent>
<nextsent>the words are then reduced to their morpho-.
</nextsent>
<nextsent>logical stem (lemma) using morpha (minnen et al, 2001).
</nextsent>
<nextsent>5.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1412">
<title id=" W06-1807.xml">merging stories with shallow semantics </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in general, recent work in natural language processing has currently relied heavily on purely statisti cal? methods for tasks such as text similarity and 39 kraq06.
</prevsent>
<prevsent>summarization.
</prevsent>
</prevsection>
<citsent citstr=" W03-0907 ">
however, there is also rich logical tradition in linguistic semantics, and work in this vein can bring the two closer together.current work in story understanding is focusing on the use of logical forms, yet these are not extracted from text automatically (mueller, 2003).<papid> W03-0907 </papid>the natural language processing and story conversion pipeline are improvements over pipeline that was shown to successfully compare stories in manner similar to teacher (halpin et al, 2004).<papid> W04-3217 </papid>the merging task is more logic-based approach than similar techniques like information fusion used in multi-document summarization (barzilay et al, 1999).<papid> P99-1071 </papid></citsent>
<aftsection>
<nextsent>our approach has some features in common with (wan and dale, 2001), however, we have chosen to focus exclusively on merger at the semantic level, rather than trying to also incorporate syntactic structure.
</nextsent>
<nextsent>there has also been revival in using weighted logical forms in structured relational learning, such as markov logic networks (domingos and kok, 2005), and this is related to the scoring of facts used by the current system in merging texts.
</nextsent>
<nextsent>as mentioned at the beginning of this paper,the conversion of unrestricted text to some logical form has experienced recent revival recently(bos et al, 2004).<papid> C04-1180 </papid></nextsent>
<nextsent>although our approach deliberately ignores much semantic detail, this may be compensated for by increased robustness due to the reliance on finite-state methods for semantic translation and chunking.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1414">
<title id=" W06-1807.xml">merging stories with shallow semantics </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in general, recent work in natural language processing has currently relied heavily on purely statisti cal? methods for tasks such as text similarity and 39 kraq06.
</prevsent>
<prevsent>summarization.
</prevsent>
</prevsection>
<citsent citstr=" P99-1071 ">
however, there is also rich logical tradition in linguistic semantics, and work in this vein can bring the two closer together.current work in story understanding is focusing on the use of logical forms, yet these are not extracted from text automatically (mueller, 2003).<papid> W03-0907 </papid>the natural language processing and story conversion pipeline are improvements over pipeline that was shown to successfully compare stories in manner similar to teacher (halpin et al, 2004).<papid> W04-3217 </papid>the merging task is more logic-based approach than similar techniques like information fusion used in multi-document summarization (barzilay et al, 1999).<papid> P99-1071 </papid></citsent>
<aftsection>
<nextsent>our approach has some features in common with (wan and dale, 2001), however, we have chosen to focus exclusively on merger at the semantic level, rather than trying to also incorporate syntactic structure.
</nextsent>
<nextsent>there has also been revival in using weighted logical forms in structured relational learning, such as markov logic networks (domingos and kok, 2005), and this is related to the scoring of facts used by the current system in merging texts.
</nextsent>
<nextsent>as mentioned at the beginning of this paper,the conversion of unrestricted text to some logical form has experienced recent revival recently(bos et al, 2004).<papid> C04-1180 </papid></nextsent>
<nextsent>although our approach deliberately ignores much semantic detail, this may be compensated for by increased robustness due to the reliance on finite-state methods for semantic translation and chunking.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1416">
<title id=" W06-1669.xml">two graph based algorithms for stateoftheart wsd </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>word sense disambiguation (wsd) is keyenabling-technology.
</prevsent>
<prevsent>supervised wsd techniques are the best performing in public evaluations, but need large amounts of hand-taggeddata.
</prevsent>
</prevsection>
<citsent citstr=" H93-1061 ">
existing hand-annotated corpora like sem cor (miller et al, 1993), <papid> H93-1061 </papid>which is annotated with wordnet senses (fellbaum, 1998) allow for small improvement over the simple most frequent sense heuristic, as attested in the all-words track of the last senseval competition (snyder and palmer, 2004).<papid> W04-0811 </papid></citsent>
<aftsection>
<nextsent>in theory, larger amounts of training data (semcor has approx.
</nextsent>
<nextsent>700k words) would improve the performance of supervised wsd, but no current project exists to provide such an expensive resource.
</nextsent>
<nextsent>supervised wsd is based on the fixed-list ofsenses?
</nextsent>
<nextsent>paradigm, where the senses for target word are closed list coming from dictionary or lexicon.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1418">
<title id=" W06-1669.xml">two graph based algorithms for stateoftheart wsd </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>word sense disambiguation (wsd) is keyenabling-technology.
</prevsent>
<prevsent>supervised wsd techniques are the best performing in public evaluations, but need large amounts of hand-taggeddata.
</prevsent>
</prevsection>
<citsent citstr=" W04-0811 ">
existing hand-annotated corpora like sem cor (miller et al, 1993), <papid> H93-1061 </papid>which is annotated with wordnet senses (fellbaum, 1998) allow for small improvement over the simple most frequent sense heuristic, as attested in the all-words track of the last senseval competition (snyder and palmer, 2004).<papid> W04-0811 </papid></citsent>
<aftsection>
<nextsent>in theory, larger amounts of training data (semcor has approx.
</nextsent>
<nextsent>700k words) would improve the performance of supervised wsd, but no current project exists to provide such an expensive resource.
</nextsent>
<nextsent>supervised wsd is based on the fixed-list ofsenses?
</nextsent>
<nextsent>paradigm, where the senses for target word are closed list coming from dictionary or lexicon.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1419">
<title id=" W06-1669.xml">two graph based algorithms for stateoftheart wsd </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>typical unsupervised wsd systems involve clustering techniques, which group together similar examples.
</prevsent>
<prevsent>given set of induced clusters (which represent word uses or senses1), each new occurrence of the target word will be compared to the clusters and the most similar cluster will be selected as its sense.
</prevsent>
</prevsection>
<citsent citstr=" W04-2406 ">
most of the unsupervised wsd work has been based on the vector space model, where each example is represented by vector of features (e.g. the words occurring in the context), andthe induced senses are either clusters of examples (schutze, 1998; purandare and pedersen, 2004) <papid> W04-2406 </papid>or clusters of words (pantel and lin,2002).</citsent>
<aftsection>
<nextsent>recently, veronis (veronis, 2004) has proposed hyper lex, an application of graph models to wsd based on the small-world properties of cooccurrence graphs.
</nextsent>
<nextsent>graph-based methods have gained attention in several areas of nlp, including knowledge-based wsd (mihalcea, 2005; <papid> H05-1052 </papid>navigli and velardi, 2005) and summarization (erkan and radev, 2004; mihalcea and tarau, 2004).<papid> W04-3252 </papid></nextsent>
<nextsent>the hyper lex algorithm presented in (veronis,2004) is entirely corpus-based.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1420">
<title id=" W06-1669.xml">two graph based algorithms for stateoftheart wsd </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>most of the unsupervised wsd work has been based on the vector space model, where each example is represented by vector of features (e.g. the words occurring in the context), andthe induced senses are either clusters of examples (schutze, 1998; purandare and pedersen, 2004) <papid> W04-2406 </papid>or clusters of words (pantel and lin,2002).</prevsent>
<prevsent>recently, veronis (veronis, 2004) has proposed hyper lex, an application of graph models to wsd based on the small-world properties of cooccurrence graphs.</prevsent>
</prevsection>
<citsent citstr=" H05-1052 ">
graph-based methods have gained attention in several areas of nlp, including knowledge-based wsd (mihalcea, 2005; <papid> H05-1052 </papid>navigli and velardi, 2005) and summarization (erkan and radev, 2004; mihalcea and tarau, 2004).<papid> W04-3252 </papid></citsent>
<aftsection>
<nextsent>the hyper lex algorithm presented in (veronis,2004) is entirely corpus-based.
</nextsent>
<nextsent>it builds cooccurrence graph for all pairs of words cooccurring in the context of the target word.
</nextsent>
<nextsent>veronis shows that this kind of graph fulfills the properties of small world graphs, and thus possesses highly connected 1unsupervised wsd approaches prefer the term worduses?
</nextsent>
<nextsent>to word senses?.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1421">
<title id=" W06-1669.xml">two graph based algorithms for stateoftheart wsd </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>most of the unsupervised wsd work has been based on the vector space model, where each example is represented by vector of features (e.g. the words occurring in the context), andthe induced senses are either clusters of examples (schutze, 1998; purandare and pedersen, 2004) <papid> W04-2406 </papid>or clusters of words (pantel and lin,2002).</prevsent>
<prevsent>recently, veronis (veronis, 2004) has proposed hyper lex, an application of graph models to wsd based on the small-world properties of cooccurrence graphs.</prevsent>
</prevsection>
<citsent citstr=" W04-3252 ">
graph-based methods have gained attention in several areas of nlp, including knowledge-based wsd (mihalcea, 2005; <papid> H05-1052 </papid>navigli and velardi, 2005) and summarization (erkan and radev, 2004; mihalcea and tarau, 2004).<papid> W04-3252 </papid></citsent>
<aftsection>
<nextsent>the hyper lex algorithm presented in (veronis,2004) is entirely corpus-based.
</nextsent>
<nextsent>it builds cooccurrence graph for all pairs of words cooccurring in the context of the target word.
</nextsent>
<nextsent>veronis shows that this kind of graph fulfills the properties of small world graphs, and thus possesses highly connected 1unsupervised wsd approaches prefer the term worduses?
</nextsent>
<nextsent>to word senses?.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1423">
<title id=" W06-1669.xml">two graph based algorithms for stateoftheart wsd </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these hubs are used as are presentation of the senses induced by the system, the same way that clusters of examples are used to represent senses in clustering approaches to wsd (purandare and pedersen, 2004).<papid> W04-2406 </papid></prevsent>
<prevsent>one of the problems of unsupervised systems is that of managing to do fair evaluation.most of current unsupervised systems are evaluated in-house, with brief comparison to re implementation of former system, leading to proliferation of unsupervised systems with little ground to compare among them.</prevsent>
</prevsection>
<citsent citstr=" W06-3814 ">
in preliminary work (agirre et al, 2006), <papid> W06-3814 </papid>we have shown that hyper lex compares favorably to other unsupervised systems.</citsent>
<aftsection>
<nextsent>we defined asemi-supervised setting for optimizing the free parameters of hyper lex on the senseval-2 english lexical sample task (s2ls), which consisted on mapping the induced senses onto the official sense inventory using the training part of s2ls.
</nextsent>
<nextsent>the best parameters were then used on the senseval-3 english lexical sample task (s3ls), where similar semi-supervised method was used to output the official sense inventory.this paper extends the previous work in several aspects.
</nextsent>
<nextsent>first of all, we adapted the pager ank graph-based method (brin and page, 1998) for wsd and compared it with hyperlex.
</nextsent>
<nextsent>we also extend the previous evaluation scheme, using measures in the clustering community which only require gold standard clustering and no mapping step.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1425">
<title id=" W06-1669.xml">two graph based algorithms for stateoftheart wsd </title>
<section> evaluating unsupervised wsd systems.  </section>
<citcontext>
<prevsection>
<prevsent>a third alternative would be to devise method to map the hubs (clusters) returned by the system to the senses in lexicon.
</prevsent>
<prevsent>pantel and lin (2002) automatically mapped the senses to wordnet, and then measured the quality of the mapping.
</prevsent>
</prevsection>
<citsent citstr=" W05-0605 ">
more recently, tagged corpora have been used to mapthe induced senses, and then compare the systems over publicly available benchmarks (puran 587 dare and pedersen, 2004; <papid> W04-2406 </papid>niu et al, 2005; <papid> W05-0605 </papid>agirreet al, 2006), <papid> W06-3814 </papid>which offers the advantage of comparing to other systems, but converts the whole system into semi-supervised.</citsent>
<aftsection>
<nextsent>see section 5 formore details on these systems.
</nextsent>
<nextsent>note that the mapping introduces noise and information loss, whichis disadvantage when comparing to other systems that relyon the gold-standard senses.
</nextsent>
<nextsent>yet another possibility is to evaluate the induced senses against gold standard as clustering task.
</nextsent>
<nextsent>induced senses are clusters, gold standard senses are classes, and measures from the clustering literature like entropy or purity can be used.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1436">
<title id=" W06-1669.xml">two graph based algorithms for stateoftheart wsd </title>
<section> experiment setting and results.  </section>
<citcontext>
<prevsection>
<prevsent>page rank with frequency threshold (pr fr) and the optimized veronis (vr opt) obtain 10 point improvement over the mfs baseline with very similar results(the difference is not statistically significant according to mcnemars test at 95% confidence 589 level).table 2 also shows the results of three supervised systems.
</prevsent>
<prevsent>these results (and those of the other unsupervised systems in the table) where obtained from the senseval website, and the only processing we did was to filter nouns.
</prevsent>
</prevsection>
<citsent citstr=" W04-0807 ">
s3ls-best stands for the the winner of s3ls (mihalcea et al, 2004), <papid> W04-0807 </papid>which is 8.3 points over our method.</citsent>
<aftsection>
<nextsent>wealso include the results of two of our in-house systems.
</nextsent>
<nextsent>knn-all is state-of-the-art system (agirreet al, 2005) using wide range of local and topical features, and only 2.3 points below the bests3ls system.
</nextsent>
<nextsent>knn-bow which is the same supervised system, but restricted to bag-of-words features only, which are the ones used by our graph based systems.
</nextsent>
<nextsent>the table shows that vr opt and pr fr are one single point from knn-bow, which is an impressive result if we take into account the information loss of the mapping step and that we tuned our parameters on different set of words.the last 5 rows of table 2 show several unsupervised systems, all of which except cymfony (niu et al, 2005) <papid> W05-0605 </papid>and (purandare and pedersen, 2004) <papid> W04-2406 </papid>participated in s3ls (check (mihal cea et al, 2004) <papid> W04-0807 </papid>for further details on the systems).we classify them according to the amount of supervision?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1445">
<title id=" W06-1502.xml">a constraint driven meta grammar </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this paper shows that the meta grammar can be used to put model theoretic syntax at work while preserving reasonably efficient processing properties.
</prevsent>
<prevsent>the strategy used here builds on constraining offline tag whose units are elementary trees the other option is to formulate constraints applied on-line, in the course of parsing, applying on the whole syntactic structure.
</prevsent>
</prevsection>
<citsent citstr=" W04-1510 ">
in dependency framework, xdg followed this path (debusmann et al, 2004), <papid> W04-1510 </papid>however it remains unknown to us whether this approach remains computationally tractable for parsing with real scale grammars.</citsent>
<aftsection>




</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1446">
<title id=" W06-1704.xml">cucweb a catalan corpus built from the web </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the project set up an architecture to retrieve portion of the web roughly corresponding to the web in spain, in order to study its formal properties (analysing its link distribution as graph) and its characteristics in terms of pages, sites, and domains (size, kind of software used, language, among other aspects).
</prevsent>
<prevsent>one of the by-products of the project is 166 million word corpus for catalan.1 the biggest annotated catalan corpus before cucweb is the ctilc corpus (rafel, 1994), consisting of about 50 million words.
</prevsent>
</prevsection>
<citsent citstr=" J03-3001 ">
in recent years, the web has been increasingly used as source of linguistic data (kilgarriff and grefenstette, 2003).<papid> J03-3001 </papid></citsent>
<aftsection>
<nextsent>the most straightforward approach to using the web as corpus is to gather data online (grefenstette, 1998), or estimate counts 1catalan is relatively minor language.
</nextsent>
<nextsent>there are currently about 10.8 million catalan speakers, similar to serbian (12), greek (10.2), or swedish (9.3).
</nextsent>
<nextsent>see http://www.upc.es/slt/alatac/cat/dades/catala-04.html (keller and lapata, 2003) <papid> J03-3005 </papid>using available searchengines.</nextsent>
<nextsent>this approach has number of drawbacks, e.g. the data one looks for has to be known beforehand, and the queries have to consist of lexical material.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1447">
<title id=" W06-1704.xml">cucweb a catalan corpus built from the web </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the most straightforward approach to using the web as corpus is to gather data online (grefenstette, 1998), or estimate counts 1catalan is relatively minor language.
</prevsent>
<prevsent>there are currently about 10.8 million catalan speakers, similar to serbian (12), greek (10.2), or swedish (9.3).
</prevsent>
</prevsection>
<citsent citstr=" J03-3005 ">
see http://www.upc.es/slt/alatac/cat/dades/catala-04.html (keller and lapata, 2003) <papid> J03-3005 </papid>using available searchengines.</citsent>
<aftsection>
<nextsent>this approach has number of drawbacks, e.g. the data one looks for has to be known beforehand, and the queries have to consist of lexical material.
</nextsent>
<nextsent>in other words, it is not possible to perform structural searches or proper language modeling.current technology makes it feasible and relatively cheap to crawl and store tera bytes of data.
</nextsent>
<nextsent>in addition, crawling the data and processing itoff-line provides more potential for its exploitation, as well as more control over the data selection and pruning processes.
</nextsent>
<nextsent>however, this approach is more challenging from technological viewpoint.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1448">
<title id=" W06-1704.xml">cucweb a catalan corpus built from the web </title>
<section> corpus exploitation.  </section>
<citcontext>
<prevsection>
<prevsent>using catcg,the performance decreased only 2 points (0.82 score).in subsequent experiment, the data were extracted from the cucweb corpus.
</prevsent>
<prevsent>given that it is 12 times larger than the traditional corpus, the question was whether more data is better data?
</prevsent>
</prevsection>
<citsent citstr=" J93-1001 ">
(church and mercer, 1993, <papid> J93-1001 </papid>18-19).</citsent>
<aftsection>
<nextsent>banko and brill (2001) <papid> P01-1005 </papid>present case study on confusion set disambiguation that supports this slogan.</nextsent>
<nextsent>surprisingly enough, results using cucweb were significantly worse than those using the traditional corpus, even with automatic linguistic processing: cucweb lead to an average 0.71 f-score, so an 11 point difference resulted.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1449">
<title id=" W06-1704.xml">cucweb a catalan corpus built from the web </title>
<section> corpus exploitation.  </section>
<citcontext>
<prevsection>
<prevsent>given that it is 12 times larger than the traditional corpus, the question was whether more data is better data?
</prevsent>
<prevsent>(church and mercer, 1993, <papid> J93-1001 </papid>18-19).</prevsent>
</prevsection>
<citsent citstr=" P01-1005 ">
banko and brill (2001) <papid> P01-1005 </papid>present case study on confusion set disambiguation that supports this slogan.</citsent>
<aftsection>
<nextsent>surprisingly enough, results using cucweb were significantly worse than those using the traditional corpus, even with automatic linguistic processing: cucweb lead to an average 0.71 f-score, so an 11 point difference resulted.
</nextsent>
<nextsent>these results somewhat question the quality of the cucweb corpus, particularly so as the authors attribute the difference to noise in the cucweb and difficulties in linguistic processing (see section 4).
</nextsent>
<nextsent>however, 0.71 is still well beyond the 0.33 f-score baseline, so that our analysis is that cucweb can be successfully used in lexical acquisition tasks.
</nextsent>
<nextsent>improvement in both filtering and linguistic processing is still must, though.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1450">
<title id=" W06-1635.xml">protein folding and chart parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>more interestingly, in the protein folding case, such recursive hierarchical search strategies, which imply tree-shaped folding routes, have been postulated independently for biological and biophysical reasons.
</prevsent>
<prevsent>this may indicate deeper, natural connection between these two processes.given that hierarchical search strategies for protein folding have been proposed in the biological literature, our primary interest here has been the question of whether greedy, hierarchical search as implemented in cky is able to identify the native state of proteins in the hp model.
</prevsent>
</prevsection>
<citsent citstr=" J98-2004 ">
the research presented here aims to verify these predictions with an explicit computational model.therefore, we were less concerned with improving efficiency, and more with the properties of this algorithm, which we consider baseline method upon which more sophisticated techniques such as best-first parsing (caraballo and charniak, 1998) <papid> J98-2004 </papid>or   search (klein and manning, 2003) <papid> N03-1016 </papid>may well be able to improve.</citsent>
<aftsection>
<nextsent>we also plan to adapt this technique to other, more realistic, representations of proteins, and to longer sequences.
</nextsent>
<nextsent>for longer sequences, we will take advantage of the fact that cky is easily paral lelizable, since any operation which combines the entries of two cells chart    and chart   1   is completely independent of other parts of the chart.
</nextsent>
<nextsent>if the routes by which proteins fold really are trees, dynamic programming technique such as cky is inherently suited to model this process,since it is the most efficient way to search all possible trees.
</nextsent>
<nextsent>this distinguishes it from more established techniques such as monte carlo, which can only follow one trajectory at time, and require multiple runs to sample the underlying landscape to sufficient degree.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1451">
<title id=" W06-1635.xml">protein folding and chart parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>more interestingly, in the protein folding case, such recursive hierarchical search strategies, which imply tree-shaped folding routes, have been postulated independently for biological and biophysical reasons.
</prevsent>
<prevsent>this may indicate deeper, natural connection between these two processes.given that hierarchical search strategies for protein folding have been proposed in the biological literature, our primary interest here has been the question of whether greedy, hierarchical search as implemented in cky is able to identify the native state of proteins in the hp model.
</prevsent>
</prevsection>
<citsent citstr=" N03-1016 ">
the research presented here aims to verify these predictions with an explicit computational model.therefore, we were less concerned with improving efficiency, and more with the properties of this algorithm, which we consider baseline method upon which more sophisticated techniques such as best-first parsing (caraballo and charniak, 1998) <papid> J98-2004 </papid>or   search (klein and manning, 2003) <papid> N03-1016 </papid>may well be able to improve.</citsent>
<aftsection>
<nextsent>we also plan to adapt this technique to other, more realistic, representations of proteins, and to longer sequences.
</nextsent>
<nextsent>for longer sequences, we will take advantage of the fact that cky is easily paral lelizable, since any operation which combines the entries of two cells chart    and chart   1   is completely independent of other parts of the chart.
</nextsent>
<nextsent>if the routes by which proteins fold really are trees, dynamic programming technique such as cky is inherently suited to model this process,since it is the most efficient way to search all possible trees.
</nextsent>
<nextsent>this distinguishes it from more established techniques such as monte carlo, which can only follow one trajectory at time, and require multiple runs to sample the underlying landscape to sufficient degree.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1452">
<title id=" W06-2204.xml">transductive pattern learning for information extraction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>most recent research has focused on learning algorithms that automatically acquire extraction patterns from manually labelled training data (e.g., (riloff, 1993; califf and mooney, 1999; soderland, 1999; freitag and kushmerick, 2000; ciravegna, 2001; finn and kushmerick, 2004)).
</prevsent>
<prevsent>this training data takes the form of the original text, annotated with the fragments to be extracted.due to the expense and tedious nature of this labelling process, it is widely recognized that key bottleneck in deploying such algorithms is the need to create sufficiently large training corpus for each new do main.
</prevsent>
</prevsection>
<citsent citstr=" C02-1154 ">
in response to this challenge, many researchers have investigated semi-supervised learning algorithms that learn from (relatively small) set of labelled texts in conjunction with (relatively large) set of un labelled texts (e.g., (riloff, 1996; brin, 1998; yangarber et al, 2002)).<papid> C02-1154 </papid></citsent>
<aftsection>
<nextsent>in this paper, we present tplex, semi-supervised algorithm for learning information extraction patterns.the key idea is to exploit the following recursive defini tions: good patterns extract good fragments, and good fragments are extracted by good patterns.
</nextsent>
<nextsent>to ope rationalize this recursive definition, we initialize the pattern and fragment scores with labelled data, and then iterate until the scores have converged.most prior semi-supervised approaches to information extraction assume that fragments are essentially named entities, so that there will be many occurrences of any given fragment.
</nextsent>
<nextsent>for example, for the task of discovering diseases (influenza?, ebola?, etc), prior algorithms assume that each disease will be mentioned many times, and that every occurrence of such disease in un labelled text should be extracted.
</nextsent>
<nextsent>however, it may not be the case that fragments to be extracted occur more than once in the corpus, or that every occurrence of labelled fragment should be extracted.for example, in the well-known cmu seminars corpus, any given person usually gives just one seminar, and fragment such as 3pm?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1455">
<title id=" W06-2204.xml">transductive pattern learning for information extraction </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>(cardie, 1997; kushmerick and thomas, 2003).
</prevsent>
<prevsent>a number of researchers have previously developed bootstrapping or semi-supervised approaches to information extraction, named entity recognition, andre lated tasks (riloff, 1996; brin, 1998; riloff and jones, 1999; agichtein et al, 2001; yangarber et al, 2002; <papid> C02-1154 </papid>stevenson and greenwood, 2005; etzioni et al, 2005).</prevsent>
</prevsection>
<citsent citstr=" P95-1026 ">
several approaches for learning from both labeled and unlabeled data have been proposed (yarowsky, 1995; <papid> P95-1026 </papid>blum and mitchell, 1998; collins and singer, 1999) <papid> W99-0613 </papid>where the unlabeled data is utilised to boost the performance of the algorithm.</citsent>
<aftsection>
<nextsent>in (collins and singer, 1999) <papid> W99-0613 </papid>collins and singer show that unlabeled data can be used to reduce the level of supervision required for named entity classification.</nextsent>
<nextsent>however, their approach is reliant on the presence of redundancy in the named entities to be identified.tplex is most closely related to the nomen algorithm (yangarber et al, 2002).<papid> C02-1154 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1456">
<title id=" W06-2204.xml">transductive pattern learning for information extraction </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>(cardie, 1997; kushmerick and thomas, 2003).
</prevsent>
<prevsent>a number of researchers have previously developed bootstrapping or semi-supervised approaches to information extraction, named entity recognition, andre lated tasks (riloff, 1996; brin, 1998; riloff and jones, 1999; agichtein et al, 2001; yangarber et al, 2002; <papid> C02-1154 </papid>stevenson and greenwood, 2005; etzioni et al, 2005).</prevsent>
</prevsection>
<citsent citstr=" W99-0613 ">
several approaches for learning from both labeled and unlabeled data have been proposed (yarowsky, 1995; <papid> P95-1026 </papid>blum and mitchell, 1998; collins and singer, 1999) <papid> W99-0613 </papid>where the unlabeled data is utilised to boost the performance of the algorithm.</citsent>
<aftsection>
<nextsent>in (collins and singer, 1999) <papid> W99-0613 </papid>collins and singer show that unlabeled data can be used to reduce the level of supervision required for named entity classification.</nextsent>
<nextsent>however, their approach is reliant on the presence of redundancy in the named entities to be identified.tplex is most closely related to the nomen algorithm (yangarber et al, 2002).<papid> C02-1154 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1463">
<title id=" W07-0413.xml">three models for discriminative machine translation using global lexical selection and sentence reconstruction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we show that hierarchical model performs best when compared to the other two models.
</prevsent>
<prevsent>the problem of machine translation can be view edas consisting of two subproblems: (a) lexical selection, where appropriate target language lexical items are chosen for each source language lexical item and (b) lexical reordering, where the chosen target language lexical items are rearranged to produce meaningful target languagestring.
</prevsent>
</prevsection>
<citsent citstr=" W99-0604 ">
most of the previous work on statistical machine translation, as exemplified in (brown et al, 1993), employs word alignment algorithm (such as giza++ (och et al, 1999)) <papid> W99-0604 </papid>that provides local associations between source words and target words.</citsent>
<aftsection>
<nextsent>the sourcetotarget word alignments are sometimes augmented with targettosource word alignments in order to improve the precision of these local associations.
</nextsent>
<nextsent>further, the wordlevelalignments are extended to phrase level alignments in order to increase the extent of local associations.
</nextsent>
<nextsent>the phrasal associations compile some amount of (local) lexical reordering of the target words those permitted by the size of the phrase.
</nextsent>
<nextsent>most of the stateoftheart machine translation systems use these phrase level associations in conjunction with target language model to produce the target sentence.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1464">
<title id=" W07-0413.xml">three models for discriminative machine translation using global lexical selection and sentence reconstruction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>most of the stateoftheart machine translation systems use these phrase level associations in conjunction with target language model to produce the target sentence.
</prevsent>
<prevsent>there is relatively little emphasis on (global) lexical reordering other than the local re-orderings permitted within the phrasal alignments.
</prevsent>
</prevsection>
<citsent citstr=" J97-3002 ">
a few exceptions are the hierarchical (possibly syntaxbased) transduction models (wu, 1997; <papid> J97-3002 </papid>alshawi et al, 1998; <papid> P98-1006 </papid>yamada and knight, 2001; <papid> P01-1067 </papid>chiang, 2005) <papid> P05-1033 </papid>and the string transduction models (kanthak et al, 2005).<papid> W05-0831 </papid></citsent>
<aftsection>
<nextsent>in this paper, we present three models for doing discriminative machine translation using global lexical selection and lexical reordering.
</nextsent>
<nextsent>1.
</nextsent>
<nextsent>bagofwords model : given source sen-.
</nextsent>
<nextsent>tence, each of the target words are chosen by looking at the entire source sentence.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1465">
<title id=" W07-0413.xml">three models for discriminative machine translation using global lexical selection and sentence reconstruction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>most of the stateoftheart machine translation systems use these phrase level associations in conjunction with target language model to produce the target sentence.
</prevsent>
<prevsent>there is relatively little emphasis on (global) lexical reordering other than the local re-orderings permitted within the phrasal alignments.
</prevsent>
</prevsection>
<citsent citstr=" P98-1006 ">
a few exceptions are the hierarchical (possibly syntaxbased) transduction models (wu, 1997; <papid> J97-3002 </papid>alshawi et al, 1998; <papid> P98-1006 </papid>yamada and knight, 2001; <papid> P01-1067 </papid>chiang, 2005) <papid> P05-1033 </papid>and the string transduction models (kanthak et al, 2005).<papid> W05-0831 </papid></citsent>
<aftsection>
<nextsent>in this paper, we present three models for doing discriminative machine translation using global lexical selection and lexical reordering.
</nextsent>
<nextsent>1.
</nextsent>
<nextsent>bagofwords model : given source sen-.
</nextsent>
<nextsent>tence, each of the target words are chosen by looking at the entire source sentence.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1466">
<title id=" W07-0413.xml">three models for discriminative machine translation using global lexical selection and sentence reconstruction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>most of the stateoftheart machine translation systems use these phrase level associations in conjunction with target language model to produce the target sentence.
</prevsent>
<prevsent>there is relatively little emphasis on (global) lexical reordering other than the local re-orderings permitted within the phrasal alignments.
</prevsent>
</prevsection>
<citsent citstr=" P01-1067 ">
a few exceptions are the hierarchical (possibly syntaxbased) transduction models (wu, 1997; <papid> J97-3002 </papid>alshawi et al, 1998; <papid> P98-1006 </papid>yamada and knight, 2001; <papid> P01-1067 </papid>chiang, 2005) <papid> P05-1033 </papid>and the string transduction models (kanthak et al, 2005).<papid> W05-0831 </papid></citsent>
<aftsection>
<nextsent>in this paper, we present three models for doing discriminative machine translation using global lexical selection and lexical reordering.
</nextsent>
<nextsent>1.
</nextsent>
<nextsent>bagofwords model : given source sen-.
</nextsent>
<nextsent>tence, each of the target words are chosen by looking at the entire source sentence.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1467">
<title id=" W07-0413.xml">three models for discriminative machine translation using global lexical selection and sentence reconstruction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>most of the stateoftheart machine translation systems use these phrase level associations in conjunction with target language model to produce the target sentence.
</prevsent>
<prevsent>there is relatively little emphasis on (global) lexical reordering other than the local re-orderings permitted within the phrasal alignments.
</prevsent>
</prevsection>
<citsent citstr=" P05-1033 ">
a few exceptions are the hierarchical (possibly syntaxbased) transduction models (wu, 1997; <papid> J97-3002 </papid>alshawi et al, 1998; <papid> P98-1006 </papid>yamada and knight, 2001; <papid> P01-1067 </papid>chiang, 2005) <papid> P05-1033 </papid>and the string transduction models (kanthak et al, 2005).<papid> W05-0831 </papid></citsent>
<aftsection>
<nextsent>in this paper, we present three models for doing discriminative machine translation using global lexical selection and lexical reordering.
</nextsent>
<nextsent>1.
</nextsent>
<nextsent>bagofwords model : given source sen-.
</nextsent>
<nextsent>tence, each of the target words are chosen by looking at the entire source sentence.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1468">
<title id=" W07-0413.xml">three models for discriminative machine translation using global lexical selection and sentence reconstruction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>most of the stateoftheart machine translation systems use these phrase level associations in conjunction with target language model to produce the target sentence.
</prevsent>
<prevsent>there is relatively little emphasis on (global) lexical reordering other than the local re-orderings permitted within the phrasal alignments.
</prevsent>
</prevsection>
<citsent citstr=" W05-0831 ">
a few exceptions are the hierarchical (possibly syntaxbased) transduction models (wu, 1997; <papid> J97-3002 </papid>alshawi et al, 1998; <papid> P98-1006 </papid>yamada and knight, 2001; <papid> P01-1067 </papid>chiang, 2005) <papid> P05-1033 </papid>and the string transduction models (kanthak et al, 2005).<papid> W05-0831 </papid></citsent>
<aftsection>
<nextsent>in this paper, we present three models for doing discriminative machine translation using global lexical selection and lexical reordering.
</nextsent>
<nextsent>1.
</nextsent>
<nextsent>bagofwords model : given source sen-.
</nextsent>
<nextsent>tence, each of the target words are chosen by looking at the entire source sentence.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1469">
<title id=" W07-0413.xml">three models for discriminative machine translation using global lexical selection and sentence reconstruction </title>
<section> hierarchical lexical association and re-.  </section>
<citcontext>
<prevsection>
<prevsent>the goal is, given source sentence s, to estimate the probability that we find given word (tj) in its translation ie.., we need to estimate the probabilities (true|tj , s) and (false|tj, s).
</prevsent>
<prevsent>to train such model, we need to build binary classifiers for all the words in the target language vocabulary.
</prevsent>
</prevsection>
<citsent citstr=" J96-1002 ">
the probability distributions of these binary classifiers are learnt using maximum entropy model (berger et al, 1996; <papid> J96-1002 </papid>haffner, 2006).</citsent>
<aftsection>
<nextsent>for the word tj , the training sentence pairs are considered as positive examples where the word appears in the target, and negative otherwise.
</nextsent>
<nextsent>thus, the number of training examples foreach binary classifier equals the number of training examples.
</nextsent>
<nextsent>in this model, classifiers are training using ngram features (bograms(s)).during decoding, instead of producing the target sentence directly, what we initially obtain is the target bag of words.
</nextsent>
<nextsent>each word in the target vocabulary is detected independently, so we have here very simple use of binary static classifiers.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1471">
<title id=" W07-0104.xml">active learning for the identification of non literal language </title>
<section> literal vs. non literal identification.  </section>
<citcontext>
<prevsection>
<prevsent>our identification model for literal vs. non literal usage of verbs is described in detail in previous publication (birke &amp; sarkar, 2006).
</prevsent>
<prevsent>here we provide brief description of the model so that the useof this model in our proposed active learning approach can be explained.
</prevsent>
</prevsection>
<citsent citstr=" J98-1002 ">
since we are attempting to reduce the problem of literal/nonliteral recognition to one of word-sense disambiguation, we use an existing similarity-based word-sense disambiguation algorithm developed by (karov &amp; edelman, 1998), <papid> J98-1002 </papid>henceforth ke.</citsent>
<aftsection>
<nextsent>the ke algorithm is based on the principle of attraction:similarities are calculated between sentences containing the word we wish to disambiguate (the target word) and collections of seed sentences (feedback sets).
</nextsent>
<nextsent>it requires target set ? the set of sentences containing the verbs to be classified into literal or non literal ? and the seed sets: the literal feedback set and the non literal feedback set.
</nextsent>
<nextsent>a target set sentence is considered to be attracted to the feedback set containing the sentence to which it shows the highest similarity.
</nextsent>
<nextsent>two sentences are similar if they contain similar words and two words are similar if they are contained in similar sentences.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1474">
<title id=" W07-0104.xml">active learning for the identification of non literal language </title>
<section> literal vs. non literal identification.  </section>
<citcontext>
<prevsection>
<prevsent>then 11: break # algorithm converges in 1?
</prevsent>
<prevsent>steps.
</prevsent>
</prevsection>
<citsent citstr=" J99-2004 ">
12: end if 13: := + 1 14: end whileby statistical tagger (ratnaparkhi, 1996) and supertags (bangalore &amp; joshi, 1999).<papid> J99-2004 </papid></citsent>
<aftsection>
<nextsent>this model was evaluated on 25 target verbs: absorb, assault, die, drag, drown, escape, examine, fill, fix, flow, grab, grasp, kick, knock, lend, miss, pass, rest, ride, roll, smooth, step, stick, strike, touch the verbs were carefully chosen to have varying token frequencies (we do not simply learn on frequently occurring verbs).
</nextsent>
<nextsent>as result, the target sets contain from 1 to 115 manually annotated sentences for each verb to enable us to measure accuracy.
</nextsent>
<nextsent>the annotations were not provided to the learning algorithm: they were only used to evaluate the test data performance.
</nextsent>
<nextsent>the first round of annotations was done by the first annotator.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1475">
<title id=" W07-0104.xml">active learning for the identification of non literal language </title>
<section> literal vs. non literal identification.  </section>
<citcontext>
<prevsection>
<prevsent>our inter-annotator agree menton the annotations used as test data in the experiments in this paper is quite high.
</prevsent>
<prevsent>(cohen) and ?(s&c;) on random sample of 200 annotated examples annotated by two different annotators was found to be 0.77.
</prevsent>
</prevsection>
<citsent citstr=" J04-1005 ">
as per ((di eugenio &amp; glass, 2004), <papid> J04-1005 </papid>cf.</citsent>
<aftsection>
<nextsent>refs therein), the standard assessment for ? values is that tentative conclusions on agreement exists when.67 ? ?
</nextsent>
<nextsent>  .8, and definite conclusion on agreement exists when ? ?
</nextsent>
<nextsent>.8.in the case of larger scale annotation effort, having the person leading the effort provide one or two examples of literal and non literal usages for each target verb to each annotator would almost certainly improve inter-annotator agreement.
</nextsent>
<nextsent>the algorithms were evaluated based on how accurately they clustered the hand-annotated sentences.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1478">
<title id=" W07-0104.xml">active learning for the identification of non literal language </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>further unsupervised expansion of the existing clusters as well as the production of additional clusters is possibility.
</prevsent>
<prevsent>to our knowledge there has not been any previous work done on taking model for literal/nonliteral language and augmenting it with an active learning approach which allows human expert knowledge to become part of the learning process.
</prevsent>
</prevsection>
<citsent citstr=" J98-4002 ">
our approach to active learning is similar to the uncertainty sampling approach of (lewis &amp; gale, 1994) and (fujii et. al., 1998) <papid> J98-4002 </papid>in that we pick those examples that we could not classify due to low confidence in the labeling at particular point.</citsent>
<aftsection>
<nextsent>we employ resource-limited version in which onlya small fixed sample is ever annotated by human.
</nextsent>
<nextsent>some of the criticisms leveled against uncertainty sampling and in favor of committee-based sampling (engelson &amp; dagan, 1996) (and see refs therein) do not apply in our case.
</nextsent>
<nextsent>our similarity measure is based on two views of sentence- and word-level similarity and hence we get an estimate of appropriate identification rather than just correct classification.
</nextsent>
<nextsent>as result, by embedding an uncertainty sampling active learning model within two-view clustering algorithm, wegain the same advantages as other uncertainty sampling methods obtain when used in bootstrapping methods (e.g.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1480">
<title id=" W07-0104.xml">active learning for the identification of non literal language </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>(dolan, 1995)).
</prevsent>
<prevsent>corpus-based systems primarily extract orlearn the necessary metaphor-processing information from large corpora, thus avoiding the need for manual annotation or metaphor-map construction.
</prevsent>
</prevsection>
<citsent citstr=" P03-1008 ">
examples of such systems are (murata et. al., 2000; nissim &amp; markert, 2003; <papid> P03-1008 </papid>mason, 2004).<papid> J04-1002 </papid></citsent>
<aftsection>
<nextsent>27 nissim &amp; markert (2003) <papid> P03-1008 </papid>approach metonymy resolution with machine learning methods, which[exploit] the similarity between examples of conventional metonymy?</nextsent>
<nextsent>((nissim &amp; markert, 2003),<papid> P03-1008 </papid>p. 56).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1481">
<title id=" W07-0104.xml">active learning for the identification of non literal language </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>(dolan, 1995)).
</prevsent>
<prevsent>corpus-based systems primarily extract orlearn the necessary metaphor-processing information from large corpora, thus avoiding the need for manual annotation or metaphor-map construction.
</prevsent>
</prevsection>
<citsent citstr=" J04-1002 ">
examples of such systems are (murata et. al., 2000; nissim &amp; markert, 2003; <papid> P03-1008 </papid>mason, 2004).<papid> J04-1002 </papid></citsent>
<aftsection>
<nextsent>27 nissim &amp; markert (2003) <papid> P03-1008 </papid>approach metonymy resolution with machine learning methods, which[exploit] the similarity between examples of conventional metonymy?</nextsent>
<nextsent>((nissim &amp; markert, 2003),<papid> P03-1008 </papid>p. 56).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1489">
<title id=" W06-2714.xml">middle ware for creating and combining multidimensional nlp markup </title>
<section> middle ware architecture.  </section>
<citcontext>
<prevsection>
<prevsent>structured meta data like configuration and processing parameters (e.g. processing time and date, language id etc.) are always stored within the annotation markup as first root daughter element.
</prevsent>
<prevsent>2.2 xml standoff markup as first-class citizen.
</prevsent>
</prevsection>
<citsent citstr=" P02-1022 ">
unlike other nlp architectures (e.g. gate (cun ningham et al, 2002) <papid> P02-1022 </papid>etc.), heart of gold treats xml standoff annotations (thompson and mckelvie, 1997) as first class citizens and nat ively supports xml (and only xml) markup of any kind.moreover, heart of gold does not prescribe specific dtds or schemata for annotations, provided that the markup is well-formed.</citsent>
<aftsection>
<nextsent>in this sense, itis completely open framework that may however be constrained by requirements of the actually configured components.
</nextsent>
<nextsent>the advantage ofthis openness is easy integration of new components.
</nextsent>
<nextsent>mappings need only be defined for the immediately depending annotations (see next section) which is by far not an n-to-n mapping in practical applications.
</nextsent>
<nextsent>however, the fact that specific dtd or schema is not imposed by the middle ware does not mean that there are no minimal requirements.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1490">
<title id=" W06-2714.xml">middle ware for creating and combining multidimensional nlp markup </title>
<section> scenario 2: shallow cascades.  </section>
<citcontext>
<prevsection>
<prevsent>the second scenario is described in (frank et al., 2004) in detail.
</prevsent>
<prevsent>a robust, partial semantics representation is generated from shallow chun kers output and morphological analysis (english and german) by means of processing cascade consisting of four sprout grammar instances with four interleaved xslt transformations.
</prevsent>
</prevsection>
<citsent citstr=" W03-0812 ">
the cascade is defined using the declarative system description language sdl (krieger, 2003).<papid> W03-0812 </papid></citsent>
<aftsection>
<nextsent>an sdl architecture description is compiled into java class which is integrated in heart of goldas sub-architecture module (fig.
</nextsent>
<nextsent>5).
</nextsent>
<nextsent>the scenario is equally good example for xslt-based annotation integration.
</nextsent>
<nextsent>chunker analysis results are included in the rmrs to be built through an xslt style sheet using the xpath expression document($uri)/chunkie/chunks/chunk[ @cstart=$beginspan and @cend=$endspan] where $uri is variable containing an annotation identifier of the form hog://sid/acid/aid as explained in section 2.1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1491">
<title id=" W06-1622.xml">semantic role labeling via instance based learning </title>
<section> abstract </section>
<citcontext>
<prevsection>
<prevsent>two ibl algorithms are utilized: k-nearest neighbor (knn), and priority maximum likelihood (pml) with modified back-off combination method.
</prevsent>
<prevsent>the experimental data are the wsj23 and brown corpus test sets from the conll2005 shared task.
</prevsent>
</prevsection>
<citsent citstr=" J05-1004 ">
it is shown that applying the tree-based predicate argument recognition algorithm (para) to the data as preprocessing stage allows knn and pml to deliver f1: 68.61 and 71.02 respectively on the wsj23, and f1: 56.96 and 60.55 on the brown corpus; an increase of 8.28 in f1 measurement over the most recent published pml results for this problem (palmer et al, 2005).<papid> J05-1004 </papid></citsent>
<aftsection>
<nextsent>training times for ibl algorithms are very much faster than for other widely used techniques for srl (e.g. parsing, support vector machines, perceptrons, etc); and the feature reduction effects of para yield testing and processing speeds of around 1.0 second per sentence for knn and 0.9 second per sentence for pml respectively, suggesting that ibl could be more practical way to perform srl for nlp applications where it is employed; such as real time machine translation or automatic speech recognition.
</nextsent>
<nextsent>the proceedings from conll2004 and conll2005 detail wide variety of approaches to semantic role labeling (srl).
</nextsent>
<nextsent>many research efforts utilize machine learning (ml) approaches; such as support vector machines (moschitti et al, 2004; pradhan et al, 2004), <papid> N04-1030 </papid>percep trons (carreras et al, 2004), <papid> W04-2415 </papid>the snow learning architecture (punyakanok et al, 2004), <papid> C04-1197 </papid>embased clustering (baldewein et al, 2004), <papid> W04-0817 </papid>transformation-based learning (higgins, 2004), <papid> W04-2417 </papid>memory-based learning (kouchnir, 2004), <papid> W04-2418 </papid>and inductive learning (surdeanu et al, 2003).<papid> P03-1002 </papid></nextsent>
<nextsent>this paper compares two instance-based learning approaches, knn and pml.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1492">
<title id=" W06-1622.xml">semantic role labeling via instance based learning </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>training times for ibl algorithms are very much faster than for other widely used techniques for srl (e.g. parsing, support vector machines, perceptrons, etc); and the feature reduction effects of para yield testing and processing speeds of around 1.0 second per sentence for knn and 0.9 second per sentence for pml respectively, suggesting that ibl could be more practical way to perform srl for nlp applications where it is employed; such as real time machine translation or automatic speech recognition.
</prevsent>
<prevsent>the proceedings from conll2004 and conll2005 detail wide variety of approaches to semantic role labeling (srl).
</prevsent>
</prevsection>
<citsent citstr=" N04-1030 ">
many research efforts utilize machine learning (ml) approaches; such as support vector machines (moschitti et al, 2004; pradhan et al, 2004), <papid> N04-1030 </papid>percep trons (carreras et al, 2004), <papid> W04-2415 </papid>the snow learning architecture (punyakanok et al, 2004), <papid> C04-1197 </papid>embased clustering (baldewein et al, 2004), <papid> W04-0817 </papid>transformation-based learning (higgins, 2004), <papid> W04-2417 </papid>memory-based learning (kouchnir, 2004), <papid> W04-2418 </papid>and inductive learning (surdeanu et al, 2003).<papid> P03-1002 </papid></citsent>
<aftsection>
<nextsent>this paper compares two instance-based learning approaches, knn and pml.
</nextsent>
<nextsent>the pml method used here utilizes modification of the backoff lattice method used by gildea &amp; jurafsky (2002) <papid> J02-3001 </papid>to use set of basic features specifically, the features employed for learning in this paper are predicate (pr), voice (vo), phrase type (pt), distance (di), head word (hw), path (pa), preposition in pp (pp), and an actor?</nextsent>
<nextsent>heuristic.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1493">
<title id=" W06-1622.xml">semantic role labeling via instance based learning </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>training times for ibl algorithms are very much faster than for other widely used techniques for srl (e.g. parsing, support vector machines, perceptrons, etc); and the feature reduction effects of para yield testing and processing speeds of around 1.0 second per sentence for knn and 0.9 second per sentence for pml respectively, suggesting that ibl could be more practical way to perform srl for nlp applications where it is employed; such as real time machine translation or automatic speech recognition.
</prevsent>
<prevsent>the proceedings from conll2004 and conll2005 detail wide variety of approaches to semantic role labeling (srl).
</prevsent>
</prevsection>
<citsent citstr=" W04-2415 ">
many research efforts utilize machine learning (ml) approaches; such as support vector machines (moschitti et al, 2004; pradhan et al, 2004), <papid> N04-1030 </papid>percep trons (carreras et al, 2004), <papid> W04-2415 </papid>the snow learning architecture (punyakanok et al, 2004), <papid> C04-1197 </papid>embased clustering (baldewein et al, 2004), <papid> W04-0817 </papid>transformation-based learning (higgins, 2004), <papid> W04-2417 </papid>memory-based learning (kouchnir, 2004), <papid> W04-2418 </papid>and inductive learning (surdeanu et al, 2003).<papid> P03-1002 </papid></citsent>
<aftsection>
<nextsent>this paper compares two instance-based learning approaches, knn and pml.
</nextsent>
<nextsent>the pml method used here utilizes modification of the backoff lattice method used by gildea &amp; jurafsky (2002) <papid> J02-3001 </papid>to use set of basic features specifically, the features employed for learning in this paper are predicate (pr), voice (vo), phrase type (pt), distance (di), head word (hw), path (pa), preposition in pp (pp), and an actor?</nextsent>
<nextsent>heuristic.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1494">
<title id=" W06-1622.xml">semantic role labeling via instance based learning </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>training times for ibl algorithms are very much faster than for other widely used techniques for srl (e.g. parsing, support vector machines, perceptrons, etc); and the feature reduction effects of para yield testing and processing speeds of around 1.0 second per sentence for knn and 0.9 second per sentence for pml respectively, suggesting that ibl could be more practical way to perform srl for nlp applications where it is employed; such as real time machine translation or automatic speech recognition.
</prevsent>
<prevsent>the proceedings from conll2004 and conll2005 detail wide variety of approaches to semantic role labeling (srl).
</prevsent>
</prevsection>
<citsent citstr=" C04-1197 ">
many research efforts utilize machine learning (ml) approaches; such as support vector machines (moschitti et al, 2004; pradhan et al, 2004), <papid> N04-1030 </papid>percep trons (carreras et al, 2004), <papid> W04-2415 </papid>the snow learning architecture (punyakanok et al, 2004), <papid> C04-1197 </papid>embased clustering (baldewein et al, 2004), <papid> W04-0817 </papid>transformation-based learning (higgins, 2004), <papid> W04-2417 </papid>memory-based learning (kouchnir, 2004), <papid> W04-2418 </papid>and inductive learning (surdeanu et al, 2003).<papid> P03-1002 </papid></citsent>
<aftsection>
<nextsent>this paper compares two instance-based learning approaches, knn and pml.
</nextsent>
<nextsent>the pml method used here utilizes modification of the backoff lattice method used by gildea &amp; jurafsky (2002) <papid> J02-3001 </papid>to use set of basic features specifically, the features employed for learning in this paper are predicate (pr), voice (vo), phrase type (pt), distance (di), head word (hw), path (pa), preposition in pp (pp), and an actor?</nextsent>
<nextsent>heuristic.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1495">
<title id=" W06-1622.xml">semantic role labeling via instance based learning </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>training times for ibl algorithms are very much faster than for other widely used techniques for srl (e.g. parsing, support vector machines, perceptrons, etc); and the feature reduction effects of para yield testing and processing speeds of around 1.0 second per sentence for knn and 0.9 second per sentence for pml respectively, suggesting that ibl could be more practical way to perform srl for nlp applications where it is employed; such as real time machine translation or automatic speech recognition.
</prevsent>
<prevsent>the proceedings from conll2004 and conll2005 detail wide variety of approaches to semantic role labeling (srl).
</prevsent>
</prevsection>
<citsent citstr=" W04-0817 ">
many research efforts utilize machine learning (ml) approaches; such as support vector machines (moschitti et al, 2004; pradhan et al, 2004), <papid> N04-1030 </papid>percep trons (carreras et al, 2004), <papid> W04-2415 </papid>the snow learning architecture (punyakanok et al, 2004), <papid> C04-1197 </papid>embased clustering (baldewein et al, 2004), <papid> W04-0817 </papid>transformation-based learning (higgins, 2004), <papid> W04-2417 </papid>memory-based learning (kouchnir, 2004), <papid> W04-2418 </papid>and inductive learning (surdeanu et al, 2003).<papid> P03-1002 </papid></citsent>
<aftsection>
<nextsent>this paper compares two instance-based learning approaches, knn and pml.
</nextsent>
<nextsent>the pml method used here utilizes modification of the backoff lattice method used by gildea &amp; jurafsky (2002) <papid> J02-3001 </papid>to use set of basic features specifically, the features employed for learning in this paper are predicate (pr), voice (vo), phrase type (pt), distance (di), head word (hw), path (pa), preposition in pp (pp), and an actor?</nextsent>
<nextsent>heuristic.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1496">
<title id=" W06-1622.xml">semantic role labeling via instance based learning </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>training times for ibl algorithms are very much faster than for other widely used techniques for srl (e.g. parsing, support vector machines, perceptrons, etc); and the feature reduction effects of para yield testing and processing speeds of around 1.0 second per sentence for knn and 0.9 second per sentence for pml respectively, suggesting that ibl could be more practical way to perform srl for nlp applications where it is employed; such as real time machine translation or automatic speech recognition.
</prevsent>
<prevsent>the proceedings from conll2004 and conll2005 detail wide variety of approaches to semantic role labeling (srl).
</prevsent>
</prevsection>
<citsent citstr=" W04-2417 ">
many research efforts utilize machine learning (ml) approaches; such as support vector machines (moschitti et al, 2004; pradhan et al, 2004), <papid> N04-1030 </papid>percep trons (carreras et al, 2004), <papid> W04-2415 </papid>the snow learning architecture (punyakanok et al, 2004), <papid> C04-1197 </papid>embased clustering (baldewein et al, 2004), <papid> W04-0817 </papid>transformation-based learning (higgins, 2004), <papid> W04-2417 </papid>memory-based learning (kouchnir, 2004), <papid> W04-2418 </papid>and inductive learning (surdeanu et al, 2003).<papid> P03-1002 </papid></citsent>
<aftsection>
<nextsent>this paper compares two instance-based learning approaches, knn and pml.
</nextsent>
<nextsent>the pml method used here utilizes modification of the backoff lattice method used by gildea &amp; jurafsky (2002) <papid> J02-3001 </papid>to use set of basic features specifically, the features employed for learning in this paper are predicate (pr), voice (vo), phrase type (pt), distance (di), head word (hw), path (pa), preposition in pp (pp), and an actor?</nextsent>
<nextsent>heuristic.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1497">
<title id=" W06-1622.xml">semantic role labeling via instance based learning </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>training times for ibl algorithms are very much faster than for other widely used techniques for srl (e.g. parsing, support vector machines, perceptrons, etc); and the feature reduction effects of para yield testing and processing speeds of around 1.0 second per sentence for knn and 0.9 second per sentence for pml respectively, suggesting that ibl could be more practical way to perform srl for nlp applications where it is employed; such as real time machine translation or automatic speech recognition.
</prevsent>
<prevsent>the proceedings from conll2004 and conll2005 detail wide variety of approaches to semantic role labeling (srl).
</prevsent>
</prevsection>
<citsent citstr=" W04-2418 ">
many research efforts utilize machine learning (ml) approaches; such as support vector machines (moschitti et al, 2004; pradhan et al, 2004), <papid> N04-1030 </papid>percep trons (carreras et al, 2004), <papid> W04-2415 </papid>the snow learning architecture (punyakanok et al, 2004), <papid> C04-1197 </papid>embased clustering (baldewein et al, 2004), <papid> W04-0817 </papid>transformation-based learning (higgins, 2004), <papid> W04-2417 </papid>memory-based learning (kouchnir, 2004), <papid> W04-2418 </papid>and inductive learning (surdeanu et al, 2003).<papid> P03-1002 </papid></citsent>
<aftsection>
<nextsent>this paper compares two instance-based learning approaches, knn and pml.
</nextsent>
<nextsent>the pml method used here utilizes modification of the backoff lattice method used by gildea &amp; jurafsky (2002) <papid> J02-3001 </papid>to use set of basic features specifically, the features employed for learning in this paper are predicate (pr), voice (vo), phrase type (pt), distance (di), head word (hw), path (pa), preposition in pp (pp), and an actor?</nextsent>
<nextsent>heuristic.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1499">
<title id=" W06-1622.xml">semantic role labeling via instance based learning </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>training times for ibl algorithms are very much faster than for other widely used techniques for srl (e.g. parsing, support vector machines, perceptrons, etc); and the feature reduction effects of para yield testing and processing speeds of around 1.0 second per sentence for knn and 0.9 second per sentence for pml respectively, suggesting that ibl could be more practical way to perform srl for nlp applications where it is employed; such as real time machine translation or automatic speech recognition.
</prevsent>
<prevsent>the proceedings from conll2004 and conll2005 detail wide variety of approaches to semantic role labeling (srl).
</prevsent>
</prevsection>
<citsent citstr=" P03-1002 ">
many research efforts utilize machine learning (ml) approaches; such as support vector machines (moschitti et al, 2004; pradhan et al, 2004), <papid> N04-1030 </papid>percep trons (carreras et al, 2004), <papid> W04-2415 </papid>the snow learning architecture (punyakanok et al, 2004), <papid> C04-1197 </papid>embased clustering (baldewein et al, 2004), <papid> W04-0817 </papid>transformation-based learning (higgins, 2004), <papid> W04-2417 </papid>memory-based learning (kouchnir, 2004), <papid> W04-2418 </papid>and inductive learning (surdeanu et al, 2003).<papid> P03-1002 </papid></citsent>
<aftsection>
<nextsent>this paper compares two instance-based learning approaches, knn and pml.
</nextsent>
<nextsent>the pml method used here utilizes modification of the backoff lattice method used by gildea &amp; jurafsky (2002) <papid> J02-3001 </papid>to use set of basic features specifically, the features employed for learning in this paper are predicate (pr), voice (vo), phrase type (pt), distance (di), head word (hw), path (pa), preposition in pp (pp), and an actor?</nextsent>
<nextsent>heuristic.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1500">
<title id=" W06-1622.xml">semantic role labeling via instance based learning </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>many research efforts utilize machine learning (ml) approaches; such as support vector machines (moschitti et al, 2004; pradhan et al, 2004), <papid> N04-1030 </papid>percep trons (carreras et al, 2004), <papid> W04-2415 </papid>the snow learning architecture (punyakanok et al, 2004), <papid> C04-1197 </papid>embased clustering (baldewein et al, 2004), <papid> W04-0817 </papid>transformation-based learning (higgins, 2004), <papid> W04-2417 </papid>memory-based learning (kouchnir, 2004), <papid> W04-2418 </papid>and inductive learning (surdeanu et al, 2003).<papid> P03-1002 </papid></prevsent>
<prevsent>this paper compares two instance-based learning approaches, knn and pml.</prevsent>
</prevsection>
<citsent citstr=" J02-3001 ">
the pml method used here utilizes modification of the backoff lattice method used by gildea &amp; jurafsky (2002) <papid> J02-3001 </papid>to use set of basic features specifically, the features employed for learning in this paper are predicate (pr), voice (vo), phrase type (pt), distance (di), head word (hw), path (pa), preposition in pp (pp), and an actor?</citsent>
<aftsection>
<nextsent>heuristic.
</nextsent>
<nextsent>the general approach presented here is an example of memory-based learning.
</nextsent>
<nextsent>many existing srl systems are also memory-based (bosch et al, 2004;kouchnir, 2004), <papid> W04-2418 </papid>implemented using tilmbl software (http://ilk.kub.nl/software.html) with advanced methods such as feature weighting, and so forth.</nextsent>
<nextsent>this paper measures the performance of knn and pml for comparison in terms of accuracy and processing speed, both against each other and against previously published results.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1507">
<title id=" W06-1622.xml">semantic role labeling via instance based learning </title>
<section> classification function.  </section>
<citcontext>
<prevsection>
<prevsent>computational complexity for knn is linear, such that tknn -  o( * ), which is proportional to the product of the number of features (m) and the number of training instances (n).
</prevsent>
<prevsent>2.2 priority maximum likelihood (pml).
</prevsent>
</prevsection>
<citsent citstr=" W03-1008 ">
estimation gildea &amp; jurafsky (2002), <papid> J02-3001 </papid>gildea &amp; hockenmaier (2003) <papid> W03-1008 </papid>and palmer et al, (2005) <papid> J05-1004 </papid>use statistical approach based on maximum likelihood method for srl, with different backoff combina predicate arg0 argm-loc 181 p(r | hw, pt, pre ,pp) p(r | pt, pa, pr, pp) p(r | pt, di, vo, pr, pp) p(r | hw, pr, pp) p(r | pt, pr, pp) p(r | pr, pp) local global p(r | hw, pp) p(r | pt, di, vo, pp) tion methods in which selected probabilities are combined with linear interpolation.</citsent>
<aftsection>
<nextsent>the probability estimation or maximum likelihood is based on the number of known features available.
</nextsent>
<nextsent>if the full feature set is selected the probability is calculated by (r | pr, vo, pt, di, hw, pa, pp) = # (r, pr, vo, pt, di, hw, pa, pp) / # (pr, vo, pt, di, hw, pa, pp) gildea &amp; jurafsky (2002) <papid> J02-3001 </papid>claims there is trade-off between more-specific distributions, which have higher accuracy but lower coverage, and less-specific distributions, which have lower accuracy but higher coverage?</nextsent>
<nextsent>and that these lection of feature subsets is exponential; and that selection of combinations of different feature subsets is doubly exponential, which is np complete.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1518">
<title id=" W06-1622.xml">semantic role labeling via instance based learning </title>
<section> classification function.  </section>
<citcontext>
<prevsection>
<prevsent>examination of the details of execution time (described in the results section of this paper) show that plot of the execution time exhibits logarithmic characteristics, implying that the computational complexity for pml is log-linear, such that tpml -  o( * log ) where denotes the size of features and denotes the size of training data.
</prevsent>
<prevsent>2.3 predicate-argument recognition algo-.
</prevsent>
</prevsection>
<citsent citstr=" W05-0626 ">
rithm (para) lin &amp; smith (2005),  <papid> W05-0626 </papid>smith (2006) describe tree-based predicate-argument recognition algorithm (para).</citsent>
<aftsection>
<nextsent>para simply finds all boundaries forgiven predicates by browsing input parse-trees, such as given by charniaks parser or hand corrected parses.
</nextsent>
<nextsent>there are three major types of phrases including given predicates, which are vp, np, and pp.
</nextsent>
<nextsent>boundaries can be recognized within boundary areas or from the top levels of clauses (as in xue &amp; palmer, 2004).
</nextsent>
<nextsent>figure 3 shows the basic algorithm of para, and more details can be found in lin &amp; smith (2006).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1526">
<title id=" W06-3307.xml">integrating cooccurrence statistics with information extraction for robust retrieval of protein interactions from medline </title>
<section> sentence-level relation extraction.  </section>
<citcontext>
<prevsection>
<prevsent>a model that integrates the two approaches is then introduced in section 4.
</prevsent>
<prevsent>this is followed by adescription of the dataset used for evaluation in section 5, and experimental results in section 6.
</prevsent>
</prevsection>
<citsent citstr=" P05-1052 ">
most systems that identify relations between entities mentioned in text documents consider only pair of entities that are mentioned in the same sentence (ray and craven, 2001; zhao and grishman, 2005;<papid> P05-1052 </papid>bunescu and mooney, 2005).</citsent>
<aftsection>
<nextsent>to decide the existence and the type of relationship, these systems generally use lexico-semantic clues inferred fromthe sentence context of the two entities.
</nextsent>
<nextsent>much research has been focused recently on automatically identifying biologically relevant entities and their relationships such as protein-protein interactions or sub cellular localizations.
</nextsent>
<nextsent>for example, the sentencetr6 specifically binds fas ligand?, states an interaction between the two proteins tr6 and fas ligand.
</nextsent>
<nextsent>one of the first systems for extracting interactions between proteins is described in (blaschke and valencia, 2001).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1527">
<title id=" W06-3307.xml">integrating cooccurrence statistics with information extraction for robust retrieval of protein interactions from medline </title>
<section> co-occurrence statistics.  </section>
<citcontext>
<prevsection>
<prevsent>thus, if is the total number of abstracts, of which cite the first protein, cite the second protein, and cite both, then the probability of co-citation under random model is: (kjn;m; n) =  k    m    m  (1)the approach that we take in this paper is to constrain the two proteins to be mentioned in the same sentence, based on the assumption that if there is reason for two protein names to co-occur in the same sentence, then in most cases that is caused bytheir interaction.
</prevsent>
<prevsent>to compute the degree of inter action?
</prevsent>
</prevsection>
<citsent citstr=" J90-1003 ">
between two proteins 1 and 2 , we use the information-theoretic measure of pointwise mutual information (church and hanks, 1990; <papid> J90-1003 </papid>manning and schutze, 1999), which is computed based on the following quantities:1.</citsent>
<aftsection>
<nextsent>n : the total number of protein pairs cooccurring in the same sentence in the corpus.
</nextsent>
<nextsent>2.
</nextsent>
<nextsent>p (p 1 ; 2 )   12 =n : the probability that 1 and 2 co-occur in the same sentence; 12 = the number of sentences mentioning both 1 and 2 . 3.
</nextsent>
<nextsent>p (p 1 ; p)   1 =n : the probability that 1cooccurs with any other protein in the same sen tence; 1= the number of sentences mentioning 1 and p.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1528">
<title id=" W06-3307.xml">integrating cooccurrence statistics with information extraction for robust retrieval of protein interactions from medline </title>
<section> evaluation corpus.  </section>
<citcontext>
<prevsection>
<prevsent>the heuristic can be made even more accurate if pairof genesis considered as interacting only if they cooccur in (predefined) minimum number of sentences in the entire corpus ? with the evaluation modified accordingly, as described later in section 6.
</prevsent>
<prevsent>5.2 gene name annotation and normalization.
</prevsent>
</prevsection>
<citsent citstr=" W05-1303 ">
for the annotation of gene names and their normalization, we use dictionary-based approach similar to (cohen, 2005).<papid> W05-1303 </papid></citsent>
<aftsection>
<nextsent>ncbi1 provides comprehensive dictionary of human genes, where each gene is specified by its unique identifier, and qualified with an official name, description, synonym names and one or more protein names, as illustrated in table 2.
</nextsent>
<nextsent>all of these names, including the description, are considered as potential referential expressions for the gene entity.
</nextsent>
<nextsent>each name string is reduced to anormal form by: replacing dashes with spaces, introducing spaces between sequences of letters and se 1url: http://www.ncbi.nih.gov quences of digits, replacing greek letters with their latin counterparts (capitalized), substituting roman numerals with arabic numerals, decapitalizing the first word if capitalized.
</nextsent>
<nextsent>all names are further tokenized, and checked against dictionary of close to 100k english nouns.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1529">
<title id=" W06-3307.xml">integrating cooccurrence statistics with information extraction for robust retrieval of protein interactions from medline </title>
<section> experimental methodology.  </section>
<citcontext>
<prevsection>
<prevsent>all abstracts, either from the hprd corpus, or from the entire medline, are annotated using the dictionary-based approach described in section 5.2.the sentence-level extraction is done with the sub sequence kernel (ssk) approach from (bunescu and mooney, 2005), which was shown to give good results on extracting interactions from biomedical abstracts.
</prevsent>
<prevsent>the sub sequence kernel was trained on set of 225 medline abstracts which were manually 2url: http://opennlp.sourceforge.net 53 annotated with protein names and their interactions.
</prevsent>
</prevsection>
<citsent citstr=" J93-1003 ">
it is known that pmi gives undue importance to low frequency events (dunning, 1993), <papid> J93-1003 </papid>therefore the evaluation considers only pairs of genes that occur at least 5 times in the whole corpus.</citsent>
<aftsection>
<nextsent>when evaluating corpus-level extraction on hprd, because the quasi-exact?
</nextsent>
<nextsent>list of interactions is known, we report the precision-recall (pr) graphs, where the precision (p) and recall (r) are computed as follows: = #true interactions extracted #total interaction extracted = #true interactions extracted #true interactions all pairs of proteins are ranked based on each scoring method, and precision recall points are computed by considering the top pairs, where varies from 1 to the total number of pairs.
</nextsent>
<nextsent>when evaluating on the entire medline, we used the shared protein function benchmark described in (ramani et al, 2005).
</nextsent>
<nextsent>given the set of interacting pairs recovered at each recall level, this benchmark calculates the extent to which interaction partners in dataset share functional annotation, measure previously shown to correlate with the accuracy of functional genomics datasets (lee et al, 2004).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1530">
<title id=" W06-2903.xml">non local modeling with a mixture of pcfgs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the probabilistic context-free grammar (pcfg) formalism is the basis of most modern statisticalparsers.
</prevsent>
<prevsent>the symbols in pcfg encode context freedom assumptions about statistical dependencies in the derivations of sentences, and the relative conditional probabilities of the grammar rules induce scores on trees.
</prevsent>
</prevsection>
<citsent citstr=" A00-2018 ">
compared to basic treebank grammar (charniak, 1996), the grammars of high accuracy parsers weaken independence assumptions by splitting grammar symbols and rules with either lexical (charniak, 2000; <papid> A00-2018 </papid>collins, 1999) or non lexical (klein and manning, 2003; <papid> P03-1054 </papid>matsuzaki et al,2005) <papid> P05-1010 </papid>conditioning information.</citsent>
<aftsection>
<nextsent>while such splitting, or conditioning, can cause problems forsta tistical estimation, it can dramatically improve the accuracy of parser.
</nextsent>
<nextsent>however, the configurations exploited in pcfg parsers are quite local: rules?
</nextsent>
<nextsent>probabilities may depend on parents or headwords, but do not depend on arbitrarily distant tree configurations.
</nextsent>
<nextsent>for example, it is generally not modeled that if one quantifier phrase (qp in the penn treebank) appears in sentence, the likelihood of finding another qp in that same sentence is greatly increased.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1531">
<title id=" W06-2903.xml">non local modeling with a mixture of pcfgs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the probabilistic context-free grammar (pcfg) formalism is the basis of most modern statisticalparsers.
</prevsent>
<prevsent>the symbols in pcfg encode context freedom assumptions about statistical dependencies in the derivations of sentences, and the relative conditional probabilities of the grammar rules induce scores on trees.
</prevsent>
</prevsection>
<citsent citstr=" P03-1054 ">
compared to basic treebank grammar (charniak, 1996), the grammars of high accuracy parsers weaken independence assumptions by splitting grammar symbols and rules with either lexical (charniak, 2000; <papid> A00-2018 </papid>collins, 1999) or non lexical (klein and manning, 2003; <papid> P03-1054 </papid>matsuzaki et al,2005) <papid> P05-1010 </papid>conditioning information.</citsent>
<aftsection>
<nextsent>while such splitting, or conditioning, can cause problems forsta tistical estimation, it can dramatically improve the accuracy of parser.
</nextsent>
<nextsent>however, the configurations exploited in pcfg parsers are quite local: rules?
</nextsent>
<nextsent>probabilities may depend on parents or headwords, but do not depend on arbitrarily distant tree configurations.
</nextsent>
<nextsent>for example, it is generally not modeled that if one quantifier phrase (qp in the penn treebank) appears in sentence, the likelihood of finding another qp in that same sentence is greatly increased.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1532">
<title id=" W06-2903.xml">non local modeling with a mixture of pcfgs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the probabilistic context-free grammar (pcfg) formalism is the basis of most modern statisticalparsers.
</prevsent>
<prevsent>the symbols in pcfg encode context freedom assumptions about statistical dependencies in the derivations of sentences, and the relative conditional probabilities of the grammar rules induce scores on trees.
</prevsent>
</prevsection>
<citsent citstr=" P05-1010 ">
compared to basic treebank grammar (charniak, 1996), the grammars of high accuracy parsers weaken independence assumptions by splitting grammar symbols and rules with either lexical (charniak, 2000; <papid> A00-2018 </papid>collins, 1999) or non lexical (klein and manning, 2003; <papid> P03-1054 </papid>matsuzaki et al,2005) <papid> P05-1010 </papid>conditioning information.</citsent>
<aftsection>
<nextsent>while such splitting, or conditioning, can cause problems forsta tistical estimation, it can dramatically improve the accuracy of parser.
</nextsent>
<nextsent>however, the configurations exploited in pcfg parsers are quite local: rules?
</nextsent>
<nextsent>probabilities may depend on parents or headwords, but do not depend on arbitrarily distant tree configurations.
</nextsent>
<nextsent>for example, it is generally not modeled that if one quantifier phrase (qp in the penn treebank) appears in sentence, the likelihood of finding another qp in that same sentence is greatly increased.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1534">
<title id=" W06-2903.xml">non local modeling with a mixture of pcfgs </title>
<section> empirical motivation.  </section>
<citcontext>
<prevsection>
<prevsent>there is also similar, but weaker, correlation for the american financial ($).
</prevsent>
<prevsent>on the right hand side we show the ten rules whose likelihoods are most increased in sentence containing this rule.
</prevsent>
</prevsection>
<citsent citstr=" J98-4004 ">
strong and that weakening them results in better models of language (johnson, 1998; <papid> J98-4004 </papid>gildea, 2001; <papid> W01-0521 </papid>klein and manning, 2003).<papid> P03-1054 </papid></citsent>
<aftsection>
<nextsent>in particular, certain grammar productions often cooccur with other productions, which may be either near or distant in the parse tree.
</nextsent>
<nextsent>in general, there exist three types of cor relations: (i) local (e.g. parent-child), (ii) non-local, and (iii) self correlations (which may be local or non-local).
</nextsent>
<nextsent>in order to quantify the strength of correlation, we use likelihood ratio (lr).
</nextsent>
<nextsent>for two rules x?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1536">
<title id=" W06-2903.xml">non local modeling with a mixture of pcfgs </title>
<section> empirical motivation.  </section>
<citcontext>
<prevsection>
<prevsent>there is also similar, but weaker, correlation for the american financial ($).
</prevsent>
<prevsent>on the right hand side we show the ten rules whose likelihoods are most increased in sentence containing this rule.
</prevsent>
</prevsection>
<citsent citstr=" W01-0521 ">
strong and that weakening them results in better models of language (johnson, 1998; <papid> J98-4004 </papid>gildea, 2001; <papid> W01-0521 </papid>klein and manning, 2003).<papid> P03-1054 </papid></citsent>
<aftsection>
<nextsent>in particular, certain grammar productions often cooccur with other productions, which may be either near or distant in the parse tree.
</nextsent>
<nextsent>in general, there exist three types of cor relations: (i) local (e.g. parent-child), (ii) non-local, and (iii) self correlations (which may be local or non-local).
</nextsent>
<nextsent>in order to quantify the strength of correlation, we use likelihood ratio (lr).
</nextsent>
<nextsent>for two rules x?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1541">
<title id=" W06-2903.xml">non local modeling with a mixture of pcfgs </title>
<section> mixtures of pcfgs.  </section>
<citcontext>
<prevsection>
<prevsent>however, for amixture of grammars we need to sum over the individual grammars: ? p(t, i) = ? p(i) ? xt p(?|x, i).because of the outer sum, this expression unfortunately does not decompose into product over scores of subparts.
</prevsent>
<prevsent>in particular, tree which maximizes the sum need not be top tree for any single component.
</prevsent>
</prevsection>
<citsent citstr=" C92-3126 ">
as is true for many other grammar formalisms inwhich there is derivation / parse distinction, an alternative to finding the most probable parse is to find the most probable derivation (vijay-shankar and joshi, 1985; bod, 1992; <papid> C92-3126 </papid>steedman, 2000).</citsent>
<aftsection>
<nextsent>instead of finding the tree which maximizes ? p(t, i),we find both the tree and component which maximize p(t, i).
</nextsent>
<nextsent>the most probable derivation can be found by simply doing standard pcfg parsing once for each component, then comparing the resulting trees?
</nextsent>
<nextsent>likelihoods.
</nextsent>
<nextsent>3.3 learning: training.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1544">
<title id=" W06-1706.xml">web coverage of the 2004 us presidential election </title>
<section> refining attitude measures.  </section>
<citcontext>
<prevsection>
<prevsent>as an adjective, good assigns desirable or positive qualities.
</prevsent>
<prevsent>part of speech tagging considers this variability by annotating web content and distinguishing between nouns, verbs, adjectives and other parts of speech.
</prevsent>
</prevsection>
<citsent citstr=" H05-2018 ">
besides differences in word-sense, analysts also encounter other types of ambiguities ? e.g., idiomatic versus non-idiomatic term usage, or various pragmatic ambiguities involving irony, sarcasm, and metaphor (wiebe, wilson et al 2005).<papid> H05-2018 </papid></citsent>
<aftsection>
<nextsent>given the considerable size of the corpus and the need to publish results in weekly intervals, the system was designed to maximize throughput in terms of documents per second.
</nextsent>
<nextsent>a comparably simple approach restricted to single words and without sentence parsing ensured the timely completion of the weekly calculations.
</nextsent>
<nextsent>planned extensions will add multiple-word combinations to the tagged dictionary to discern morphologically similar but semantically different terms such as fuel cell and prison cell.
</nextsent>
<nextsent>yet the lexis of web content only partially determines its semantic orientation, despite using multi-word units of meaning instead of single words or lemmas (danielsson 2004).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1552">
<title id=" W07-0204.xml">time stamped graphs evolutionary models of text for multi document summarization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>recently, number of graph-based approaches have been suggested for nlp applications.
</prevsent>
<prevsent>erkan and radev (2004) introduced lexrank for multi-document text summarization.
</prevsent>
</prevsection>
<citsent citstr=" W04-3252 ">
mihalcea and tarau (2004) <papid> W04-3252 </papid>introduced text rank for keyword and sentence extractions.</citsent>
<aftsection>
<nextsent>both lexrank and text rank assume fully connected, undirected graph, with text units as nodes and similarity as edges.
</nextsent>
<nextsent>after graph construction, both algorithms use random walk on the graph to redistribute the node weights.
</nextsent>
<nextsent>many graph-based algorithms feature an evolutionary model, in which the graph changes over timesteps.
</nextsent>
<nextsent>an example is citation network whose edges point backward in time: papers (usually) only reference older published works.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1554">
<title id=" W07-0204.xml">time stamped graphs evolutionary models of text for multi document summarization </title>
<section> random walk.  </section>
<citcontext>
<prevsection>
<prevsent>equations 1 and 2 assume random walker jumps from the current node to random node with probability ?.
</prevsent>
<prevsent>the key to creating topic-sensitive pag erank is that we can bias the computation by restricting the user to jump only to random node which has non-zero similarity with the query.
</prevsent>
</prevsection>
<citsent citstr=" H05-1115 ">
otterbacher et al (2005) <papid> H05-1115 </papid>gives an equation for topic sensitive and weighted page rank as: ? ??</citsent>
<aftsection>
<nextsent>?+= )( )( )()1( ),( ),( )( uinv voutx vx vu sy vpr w qysim qusim upr ??
</nextsent>
<nextsent>(3) 29 where is the set of all nodes in the graph, and sim(u, q) is the similarity score between node and the query q.
</nextsent>
<nextsent>we have generalized and formalized evolutionary time stamped graph model.
</nextsent>
<nextsent>we want to apply it on automatic text summarization to confirm that these evolutionary models help in extracting important sentences.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1557">
<title id=" W06-1403.xml">ccg chart realization from disjunctive inputs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this paradigm, symbolic methods are used to generate space of possible phrasings, and statistical methods are used to select one or more outputs from this space.
</prevsent>
<prevsent>to specify the desired paraphrase space, one may either provide an input logical form that underspecifiescertain realization choices, or include explicit dis junctions in the input lf (or both).
</prevsent>
</prevsection>
<citsent citstr=" N03-1003 ">
our experience suggests that disjunctive lfs are an important capability, especially as one seeks to make grammars reusable across applications, and to employ domain-specific, sentence-level paraphrases (barzilay and lee, 2003).<papid> N03-1003 </papid></citsent>
<aftsection>
<nextsent>prominent examples of surface realizers inthe generate-and-select paradigm include nitro gen/halogen (langkilde, 2000; <papid> A00-2023 </papid>langkilde-geary, 2002) and fergus (bangalore and rambow, 2000).<papid> C00-1007 </papid></nextsent>
<nextsent>more recently, generate-and-select realizers in the chart realization tradition (kay, 1996) <papid> P96-1027 </papid>have appeared, including the openccg (white, 2004) and lingo (carroll and oepen, 2005) realizers.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1558">
<title id=" W06-1403.xml">ccg chart realization from disjunctive inputs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>to specify the desired paraphrase space, one may either provide an input logical form that underspecifiescertain realization choices, or include explicit dis junctions in the input lf (or both).
</prevsent>
<prevsent>our experience suggests that disjunctive lfs are an important capability, especially as one seeks to make grammars reusable across applications, and to employ domain-specific, sentence-level paraphrases (barzilay and lee, 2003).<papid> N03-1003 </papid></prevsent>
</prevsection>
<citsent citstr=" A00-2023 ">
prominent examples of surface realizers inthe generate-and-select paradigm include nitro gen/halogen (langkilde, 2000; <papid> A00-2023 </papid>langkilde-geary, 2002) and fergus (bangalore and rambow, 2000).<papid> C00-1007 </papid></citsent>
<aftsection>
<nextsent>more recently, generate-and-select realizers in the chart realization tradition (kay, 1996) <papid> P96-1027 </papid>have appeared, including the openccg (white, 2004) and lingo (carroll and oepen, 2005) realizers.</nextsent>
<nextsent>chart realizers make it possible to use the same reversible grammar for both parsing and realization, and employ well-defined methods of semantic composition to construct semantic representations that can properly represent the scope of logical operators.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1559">
<title id=" W06-1403.xml">ccg chart realization from disjunctive inputs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>to specify the desired paraphrase space, one may either provide an input logical form that underspecifiescertain realization choices, or include explicit dis junctions in the input lf (or both).
</prevsent>
<prevsent>our experience suggests that disjunctive lfs are an important capability, especially as one seeks to make grammars reusable across applications, and to employ domain-specific, sentence-level paraphrases (barzilay and lee, 2003).<papid> N03-1003 </papid></prevsent>
</prevsection>
<citsent citstr=" C00-1007 ">
prominent examples of surface realizers inthe generate-and-select paradigm include nitro gen/halogen (langkilde, 2000; <papid> A00-2023 </papid>langkilde-geary, 2002) and fergus (bangalore and rambow, 2000).<papid> C00-1007 </papid></citsent>
<aftsection>
<nextsent>more recently, generate-and-select realizers in the chart realization tradition (kay, 1996) <papid> P96-1027 </papid>have appeared, including the openccg (white, 2004) and lingo (carroll and oepen, 2005) realizers.</nextsent>
<nextsent>chart realizers make it possible to use the same reversible grammar for both parsing and realization, and employ well-defined methods of semantic composition to construct semantic representations that can properly represent the scope of logical operators.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1560">
<title id=" W06-1403.xml">ccg chart realization from disjunctive inputs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our experience suggests that disjunctive lfs are an important capability, especially as one seeks to make grammars reusable across applications, and to employ domain-specific, sentence-level paraphrases (barzilay and lee, 2003).<papid> N03-1003 </papid></prevsent>
<prevsent>prominent examples of surface realizers inthe generate-and-select paradigm include nitro gen/halogen (langkilde, 2000; <papid> A00-2023 </papid>langkilde-geary, 2002) and fergus (bangalore and rambow, 2000).<papid> C00-1007 </papid></prevsent>
</prevsection>
<citsent citstr=" P96-1027 ">
more recently, generate-and-select realizers in the chart realization tradition (kay, 1996) <papid> P96-1027 </papid>have appeared, including the openccg (white, 2004) and lingo (carroll and oepen, 2005) realizers.</citsent>
<aftsection>
<nextsent>chart realizers make it possible to use the same reversible grammar for both parsing and realization, and employ well-defined methods of semantic composition to construct semantic representations that can properly represent the scope of logical operators.
</nextsent>
<nextsent>in the chart realization tradition, previous work has not generally supported disjunctive logical forms, with (shemtov, 1997) as the only published exception (to the authors knowledge).
</nextsent>
<nextsent>arguably, part of the reason that disjunctive lfs have notyet been embraced more broadly by those working on chart realization is that shemtovs solution,while ingenious, is dauntingly complex.
</nextsent>
<nextsent>looking beyond chart realizers, both nitrogen/halogenand fergus support some forms of disjunctive in put; however, in comparison to shemtovs inputs,theirs are less expressive, in that they do not allow dis junctions across different levels of the input structure.as an alternative to shemtovs method, this paper presents chart realization algorithm for generating paraphrases from disjunctive logical forms that is more straightforward to implement, together with an initial case study of the algorithmsefficiency.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1561">
<title id=" W06-1403.xml">ccg chart realization from disjunctive inputs </title>
<section> disjunctive logical forms.  </section>
<citcontext>
<prevsection>
<prevsent>this example shows some of the phrasings that may be used in comic to describe the style of design that has not been discussed previously.the example includes top-level disjunction between the use of deictic np this design | this one| this (with an accompanying pointing gesture) followed by the copula, or the use of the phrase herewe have to introduce the design.
</prevsent>
<prevsent>while these alternatives can function as paraphrases in this context, it is difficult to see how one might specify them in single underspecified (and application neutral) logical form.graphs such as those in figure 1 are represented internally using hybrid logic dependency semantics (hlds), as in figure 2.
</prevsent>
</prevsection>
<citsent citstr=" P02-1041 ">
hlds is adependency-based approach to representing linguistic meaning developed by baldridge and kruijff (2002).<papid> P02-1041 </papid></citsent>
<aftsection>
<nextsent>in hlds, hybrid logic (blackburn, 2000) terms3 are used to describe dependency 3hybrid logic extends modal logic with nominals, new sort of basic formula that explicitly names states/nodes.
</nextsent>
<nextsent>like propositions, nominals are first-class citizens of the objectgraphs.
</nextsent>
<nextsent>these graphs have been suggested as representations for discourse structure, and have their own underlying semantics (white, 2006).
</nextsent>
<nextsent>in hlds, as can be seen in figure 2(a), each semantic head is associated with nominal that identifies its discourse referent, and heads are connected to their dependents via dependency relations, which are modeled as modal relations.modal relations are also used to represent semantic features.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1563">
<title id=" W06-1403.xml">ccg chart realization from disjunctive inputs </title>
<section> case study.  </section>
<citcontext>
<prevsection>
<prevsent>in this grammar, there are relatively few categories per lexeme on average, but the boundary tone categories engender great deal of non-determinism.
</prevsent>
<prevsent>with other grammars, run times can be expected to vary.
</prevsent>
</prevsection>
<citsent citstr=" W04-0601 ">
in anticipation of the present work, foster and white (2004) <papid> W04-0601 </papid>generated disjunctive logical forms during sentence planning, then (as stopgap measure) multiplied out the dis junctions and sequentially realized the top-level alternatives until an overall time limit was reached.</citsent>
<aftsection>
<nextsent>taking the previous logical forms as starting point, 104 sentences from the evaluation in (foster and white,2005) were selected, and their lfs were manually augmented to cover greater range of paraphrases allowed by the grammar.9 to obtain the corresponding top-level lf alternatives, 100-best realization was performed, and the unique lfs appearing in the top 100 realizations were gath ered; on average, there were 29 such unique lfs.we then compared the present algorithms performance against sequential realization in producing 10-best outputs and single-best outputs.
</nextsent>
<nextsent>inthe 10-best case, we used the two-stage pack ing/unpacking mode; for the single-best case, we used the anytime mode with 3-best pruning.
</nextsent>
<nextsent>with both cases, the run times include scoring with trigram language model, and were measured on a2.8ghz linux pc.
</nextsent>
<nextsent>realization quality was not assessed as part of the study, though manual inspection indicated that it was very high.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1564">
<title id=" W06-1403.xml">ccg chart realization from disjunctive inputs </title>
<section> conclusions.  </section>
<citcontext>
<prevsection>
<prevsent>consequently, to prevent over generation, his algorithm has to solve during the enumeration phase system of constraints (potentially exponential in size)formed from the conditions in the contexted coverage arraysa process which is far from straight forward.
</prevsent>
<prevsent>this paper has presented new chart realization algorithm for efficiently generating surface realizations from disjunctive logical forms, and has argued that the approach represents an improvement over that of (shemtov, 1997) in terms of both usability and simplicity.
</prevsent>
</prevsection>
<citsent citstr=" P06-1140 ">
the algorithm has been implemented as an extension to the openccg hybrid symbolic/statistical realizer, and has recently been employed to generate n-best realization lists for reranking according to their predicted synthesis quality (nakatsu and white, 2006), <papid> P06-1140 </papid>as well as to generate dialogues exhibiting individuality and alignment(brockmann et al, 2005; isard et al, 2005).</citsent>
<aftsection>
<nextsent>an initial case study has shown that the algorithm works many times faster than sequential realization, with run times suitable for use india logue systems; more comprehensive study of the algorithms efficiency is planned for future work.
</nextsent>
<nextsent>acknowledgements the author thanks mary ellen foster, amy isard,johanna moore, mark steedman and the anonymous reviewers for helpful feedback and discussion, and the university of edinburghs institute for communicating and collaborative systems for partially supporting this work.
</nextsent>


</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1565">
<title id=" W06-1909.xml">adapting a semantic question answering system to the web </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the system described by neumann and xu (2003) workson german web pages.
</prevsent>
<prevsent>its general approach differs from insicht because it relies on shallow, but robust methods, while insicht builds on sentence parsing and semantic representations.
</prevsent>
</prevsection>
<citsent citstr=" P01-1037 ">
in this respect, insicht resembles the (english) textual qa system presented by harabagiu et al (2001).<papid> P01-1037 </papid></citsent>
<aftsection>
<nextsent>in contrast to insicht, this system applies theoremprover and large knowledge base to validate candidate answers.
</nextsent>
<nextsent>an interesting combination of web and textual qa is presented by vicedo et al (2005): english and spanish web documents are used to enhance textual qa in spanish.
</nextsent>
<nextsent>6this high number is in part an artifact of the pre selection query generation.
</nextsent>
<nextsent>in more thorough analysis, human users could be given the nl question and be asked to formulate corresponding search engine query.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1566">
<title id=" W06-1660.xml">empirical study on the performance stability of named entity recognition model across domains </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>machine learning approaches are becoming more attractive for ner in recent years since they are trainable and adaptable.
</prevsent>
<prevsent>recent research on english ner has focused on the machine learning approach (sang and meulder, 2003).
</prevsent>
</prevsection>
<citsent citstr=" W03-0428 ">
the relevant algorithms include maximum entropy (borth wick, 1999; klein et al, 2003), <papid> W03-0428 </papid>hidden markov model (hmm) (bikel et al, 1999; klein et al,2003), <papid> W03-0428 </papid>ada boost (carreras et al, 2003), <papid> W03-0421 </papid>memory based learning (meulder and daelemans, 2003), support vector machine (isozaki and kazawa,2002), <papid> C02-1054 </papid>robust risk minimization (rrm) classification method (florian et al, 2003), <papid> W03-0425 </papid>etc.for chinese ner, most of the existing approaches use hand-crafted rules with word (or character) frequency statistics.</citsent>
<aftsection>
<nextsent>some machine learning algorithms also have been investigated in chinese ner, including hmm (yu et al, 1998; <papid> M98-1016 </papid>jing et al, 2003), <papid> W03-1026 </papid>class-based language model (gao et al, 2005; <papid> J05-4005 </papid>wu et al, 2005), <papid> H05-1054 </papid>rrm (guo et al, 2005; jing et al, 2003), <papid> W03-1026 </papid>etc. however, when machine learning-based ner system is directly employed in new domain, its performance usually degrades.</nextsent>
<nextsent>in order to avoid the performance degrading, the ner model is often retrained with domain-specific annotated cor pus.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1568">
<title id=" W06-1660.xml">empirical study on the performance stability of named entity recognition model across domains </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>machine learning approaches are becoming more attractive for ner in recent years since they are trainable and adaptable.
</prevsent>
<prevsent>recent research on english ner has focused on the machine learning approach (sang and meulder, 2003).
</prevsent>
</prevsection>
<citsent citstr=" W03-0421 ">
the relevant algorithms include maximum entropy (borth wick, 1999; klein et al, 2003), <papid> W03-0428 </papid>hidden markov model (hmm) (bikel et al, 1999; klein et al,2003), <papid> W03-0428 </papid>ada boost (carreras et al, 2003), <papid> W03-0421 </papid>memory based learning (meulder and daelemans, 2003), support vector machine (isozaki and kazawa,2002), <papid> C02-1054 </papid>robust risk minimization (rrm) classification method (florian et al, 2003), <papid> W03-0425 </papid>etc.for chinese ner, most of the existing approaches use hand-crafted rules with word (or character) frequency statistics.</citsent>
<aftsection>
<nextsent>some machine learning algorithms also have been investigated in chinese ner, including hmm (yu et al, 1998; <papid> M98-1016 </papid>jing et al, 2003), <papid> W03-1026 </papid>class-based language model (gao et al, 2005; <papid> J05-4005 </papid>wu et al, 2005), <papid> H05-1054 </papid>rrm (guo et al, 2005; jing et al, 2003), <papid> W03-1026 </papid>etc. however, when machine learning-based ner system is directly employed in new domain, its performance usually degrades.</nextsent>
<nextsent>in order to avoid the performance degrading, the ner model is often retrained with domain-specific annotated cor pus.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1569">
<title id=" W06-1660.xml">empirical study on the performance stability of named entity recognition model across domains </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>machine learning approaches are becoming more attractive for ner in recent years since they are trainable and adaptable.
</prevsent>
<prevsent>recent research on english ner has focused on the machine learning approach (sang and meulder, 2003).
</prevsent>
</prevsection>
<citsent citstr=" C02-1054 ">
the relevant algorithms include maximum entropy (borth wick, 1999; klein et al, 2003), <papid> W03-0428 </papid>hidden markov model (hmm) (bikel et al, 1999; klein et al,2003), <papid> W03-0428 </papid>ada boost (carreras et al, 2003), <papid> W03-0421 </papid>memory based learning (meulder and daelemans, 2003), support vector machine (isozaki and kazawa,2002), <papid> C02-1054 </papid>robust risk minimization (rrm) classification method (florian et al, 2003), <papid> W03-0425 </papid>etc.for chinese ner, most of the existing approaches use hand-crafted rules with word (or character) frequency statistics.</citsent>
<aftsection>
<nextsent>some machine learning algorithms also have been investigated in chinese ner, including hmm (yu et al, 1998; <papid> M98-1016 </papid>jing et al, 2003), <papid> W03-1026 </papid>class-based language model (gao et al, 2005; <papid> J05-4005 </papid>wu et al, 2005), <papid> H05-1054 </papid>rrm (guo et al, 2005; jing et al, 2003), <papid> W03-1026 </papid>etc. however, when machine learning-based ner system is directly employed in new domain, its performance usually degrades.</nextsent>
<nextsent>in order to avoid the performance degrading, the ner model is often retrained with domain-specific annotated cor pus.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1570">
<title id=" W06-1660.xml">empirical study on the performance stability of named entity recognition model across domains </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>machine learning approaches are becoming more attractive for ner in recent years since they are trainable and adaptable.
</prevsent>
<prevsent>recent research on english ner has focused on the machine learning approach (sang and meulder, 2003).
</prevsent>
</prevsection>
<citsent citstr=" W03-0425 ">
the relevant algorithms include maximum entropy (borth wick, 1999; klein et al, 2003), <papid> W03-0428 </papid>hidden markov model (hmm) (bikel et al, 1999; klein et al,2003), <papid> W03-0428 </papid>ada boost (carreras et al, 2003), <papid> W03-0421 </papid>memory based learning (meulder and daelemans, 2003), support vector machine (isozaki and kazawa,2002), <papid> C02-1054 </papid>robust risk minimization (rrm) classification method (florian et al, 2003), <papid> W03-0425 </papid>etc.for chinese ner, most of the existing approaches use hand-crafted rules with word (or character) frequency statistics.</citsent>
<aftsection>
<nextsent>some machine learning algorithms also have been investigated in chinese ner, including hmm (yu et al, 1998; <papid> M98-1016 </papid>jing et al, 2003), <papid> W03-1026 </papid>class-based language model (gao et al, 2005; <papid> J05-4005 </papid>wu et al, 2005), <papid> H05-1054 </papid>rrm (guo et al, 2005; jing et al, 2003), <papid> W03-1026 </papid>etc. however, when machine learning-based ner system is directly employed in new domain, its performance usually degrades.</nextsent>
<nextsent>in order to avoid the performance degrading, the ner model is often retrained with domain-specific annotated cor pus.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1571">
<title id=" W06-1660.xml">empirical study on the performance stability of named entity recognition model across domains </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>recent research on english ner has focused on the machine learning approach (sang and meulder, 2003).
</prevsent>
<prevsent>the relevant algorithms include maximum entropy (borth wick, 1999; klein et al, 2003), <papid> W03-0428 </papid>hidden markov model (hmm) (bikel et al, 1999; klein et al,2003), <papid> W03-0428 </papid>ada boost (carreras et al, 2003), <papid> W03-0421 </papid>memory based learning (meulder and daelemans, 2003), support vector machine (isozaki and kazawa,2002), <papid> C02-1054 </papid>robust risk minimization (rrm) classification method (florian et al, 2003), <papid> W03-0425 </papid>etc.for chinese ner, most of the existing approaches use hand-crafted rules with word (or character) frequency statistics.</prevsent>
</prevsection>
<citsent citstr=" M98-1016 ">
some machine learning algorithms also have been investigated in chinese ner, including hmm (yu et al, 1998; <papid> M98-1016 </papid>jing et al, 2003), <papid> W03-1026 </papid>class-based language model (gao et al, 2005; <papid> J05-4005 </papid>wu et al, 2005), <papid> H05-1054 </papid>rrm (guo et al, 2005; jing et al, 2003), <papid> W03-1026 </papid>etc. however, when machine learning-based ner system is directly employed in new domain, its performance usually degrades.</citsent>
<aftsection>
<nextsent>in order to avoid the performance degrading, the ner model is often retrained with domain-specific annotated corpus.
</nextsent>
<nextsent>this retraining process usually needs more efforts and costs.
</nextsent>
<nextsent>in order to enhance the performance stability of ner models with less efforts, some issues have to be considered in practice.
</nextsent>
<nextsent>for example, how much training data is enough for building stable and applicable ner model?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1572">
<title id=" W06-1660.xml">empirical study on the performance stability of named entity recognition model across domains </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>recent research on english ner has focused on the machine learning approach (sang and meulder, 2003).
</prevsent>
<prevsent>the relevant algorithms include maximum entropy (borth wick, 1999; klein et al, 2003), <papid> W03-0428 </papid>hidden markov model (hmm) (bikel et al, 1999; klein et al,2003), <papid> W03-0428 </papid>ada boost (carreras et al, 2003), <papid> W03-0421 </papid>memory based learning (meulder and daelemans, 2003), support vector machine (isozaki and kazawa,2002), <papid> C02-1054 </papid>robust risk minimization (rrm) classification method (florian et al, 2003), <papid> W03-0425 </papid>etc.for chinese ner, most of the existing approaches use hand-crafted rules with word (or character) frequency statistics.</prevsent>
</prevsection>
<citsent citstr=" W03-1026 ">
some machine learning algorithms also have been investigated in chinese ner, including hmm (yu et al, 1998; <papid> M98-1016 </papid>jing et al, 2003), <papid> W03-1026 </papid>class-based language model (gao et al, 2005; <papid> J05-4005 </papid>wu et al, 2005), <papid> H05-1054 </papid>rrm (guo et al, 2005; jing et al, 2003), <papid> W03-1026 </papid>etc. however, when machine learning-based ner system is directly employed in new domain, its performance usually degrades.</citsent>
<aftsection>
<nextsent>in order to avoid the performance degrading, the ner model is often retrained with domain-specific annotated corpus.
</nextsent>
<nextsent>this retraining process usually needs more efforts and costs.
</nextsent>
<nextsent>in order to enhance the performance stability of ner models with less efforts, some issues have to be considered in practice.
</nextsent>
<nextsent>for example, how much training data is enough for building stable and applicable ner model?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1574">
<title id=" W06-1660.xml">empirical study on the performance stability of named entity recognition model across domains </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>recent research on english ner has focused on the machine learning approach (sang and meulder, 2003).
</prevsent>
<prevsent>the relevant algorithms include maximum entropy (borth wick, 1999; klein et al, 2003), <papid> W03-0428 </papid>hidden markov model (hmm) (bikel et al, 1999; klein et al,2003), <papid> W03-0428 </papid>ada boost (carreras et al, 2003), <papid> W03-0421 </papid>memory based learning (meulder and daelemans, 2003), support vector machine (isozaki and kazawa,2002), <papid> C02-1054 </papid>robust risk minimization (rrm) classification method (florian et al, 2003), <papid> W03-0425 </papid>etc.for chinese ner, most of the existing approaches use hand-crafted rules with word (or character) frequency statistics.</prevsent>
</prevsection>
<citsent citstr=" J05-4005 ">
some machine learning algorithms also have been investigated in chinese ner, including hmm (yu et al, 1998; <papid> M98-1016 </papid>jing et al, 2003), <papid> W03-1026 </papid>class-based language model (gao et al, 2005; <papid> J05-4005 </papid>wu et al, 2005), <papid> H05-1054 </papid>rrm (guo et al, 2005; jing et al, 2003), <papid> W03-1026 </papid>etc. however, when machine learning-based ner system is directly employed in new domain, its performance usually degrades.</citsent>
<aftsection>
<nextsent>in order to avoid the performance degrading, the ner model is often retrained with domain-specific annotated corpus.
</nextsent>
<nextsent>this retraining process usually needs more efforts and costs.
</nextsent>
<nextsent>in order to enhance the performance stability of ner models with less efforts, some issues have to be considered in practice.
</nextsent>
<nextsent>for example, how much training data is enough for building stable and applicable ner model?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1575">
<title id=" W06-1660.xml">empirical study on the performance stability of named entity recognition model across domains </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>recent research on english ner has focused on the machine learning approach (sang and meulder, 2003).
</prevsent>
<prevsent>the relevant algorithms include maximum entropy (borth wick, 1999; klein et al, 2003), <papid> W03-0428 </papid>hidden markov model (hmm) (bikel et al, 1999; klein et al,2003), <papid> W03-0428 </papid>ada boost (carreras et al, 2003), <papid> W03-0421 </papid>memory based learning (meulder and daelemans, 2003), support vector machine (isozaki and kazawa,2002), <papid> C02-1054 </papid>robust risk minimization (rrm) classification method (florian et al, 2003), <papid> W03-0425 </papid>etc.for chinese ner, most of the existing approaches use hand-crafted rules with word (or character) frequency statistics.</prevsent>
</prevsection>
<citsent citstr=" H05-1054 ">
some machine learning algorithms also have been investigated in chinese ner, including hmm (yu et al, 1998; <papid> M98-1016 </papid>jing et al, 2003), <papid> W03-1026 </papid>class-based language model (gao et al, 2005; <papid> J05-4005 </papid>wu et al, 2005), <papid> H05-1054 </papid>rrm (guo et al, 2005; jing et al, 2003), <papid> W03-1026 </papid>etc. however, when machine learning-based ner system is directly employed in new domain, its performance usually degrades.</citsent>
<aftsection>
<nextsent>in order to avoid the performance degrading, the ner model is often retrained with domain-specific annotated corpus.
</nextsent>
<nextsent>this retraining process usually needs more efforts and costs.
</nextsent>
<nextsent>in order to enhance the performance stability of ner models with less efforts, some issues have to be considered in practice.
</nextsent>
<nextsent>for example, how much training data is enough for building stable and applicable ner model?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1578">
<title id=" W06-1660.xml">empirical study on the performance stability of named entity recognition model across domains </title>
<section> use informative training samples to.  </section>
<citcontext>
<prevsection>
<prevsent>within the limitation of available training data and computational resources, it is necessary for us to either limit the number of features or select more informative data which can be efficiently handled by the training algorithm.
</prevsent>
<prevsent>active learning method is usually employed in text classification (mccal lum and nigam et al, 1998).
</prevsent>
</prevsection>
<citsent citstr=" P04-1075 ">
it is only recently employed inner (shen et al, 2004).<papid> P04-1075 </papid>in order to enhance the performance and over come the limitation of available training data and computational resources, we present an informative sample selection method using variant of uncertainty-sampling (lewis and catlett, 1994).</citsent>
<aftsection>
<nextsent>the main steps are described as follows.
</nextsent>
<nextsent>1.
</nextsent>
<nextsent>build an initial ner model (f-.
</nextsent>
<nextsent>measure=76.24%) using an initial dataset.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1579">
<title id=" W06-3401.xml">prosodic correlates of rhetorical relations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for example, one sentence might make claim and the following sentence give evidence for the claim, with the second sentence being satellite and the evidence relation existing between the two spans.in text containing many sentences, these nucleus satellite pairs can be built up to produce document wide rhetorical tree.
</prevsent>
<prevsent>figure 1 gives an example of rhetorical tree for three-sentence text1.
</prevsent>
</prevsection>
<citsent citstr=" W97-0713 ">
theories such as rst have been popular for sometime as way of describing the multi-levelled rhetorical relations that exist in text, with relevant applications such as automatic summarization (marcu, 1997) <papid> W97-0713 </papid>and natural language generation (knott and dale, 1996).</citsent>
<aftsection>
<nextsent>however, implementing automatic rhetorical parsers has been problematic area of research.
</nextsent>
<nextsent>techniques that rely heavily on explicit signals, such as discourse markers, are of limited use both because only small percentage of rhetorical relations are signalled explicitly and because explicit markers can be ambiguous.
</nextsent>
<nextsent>rst trees are binary branching trees distinguishing between nuclei and satellites, and automatically determining nucle arity is also far from trivial.
</nextsent>
<nextsent>furthermore, there are some documents which are simply not amenable to being described by document-wide rhetorical tree (mann and thompson, 1988).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1580">
<title id=" W06-3401.xml">prosodic correlates of rhetorical relations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>rst trees are binary branching trees distinguishing between nuclei and satellites, and automatically determining nucle arity is also far from trivial.
</prevsent>
<prevsent>furthermore, there are some documents which are simply not amenable to being described by document-wide rhetorical tree (mann and thompson, 1988).
</prevsent>
</prevsection>
<citsent citstr=" J92-4007 ">
finally, sometimes more than one relation can hold between two given units (moore and pollack, 1992).<papid> J92-4007 </papid></citsent>
<aftsection>
<nextsent>given the problems of automatically parsing text for rhetorical relations, it seems prohibitively difficult to attempt rhetorical parsing of speech documents - data which are marked by disfluencies, low information density, and sometimes little cohesion.
</nextsent>
<nextsent>for that reason, this pilot study sets out comparatively modest task: to determine whether one of five relations holds between two adjacent dialogue acts in meeting speech.
</nextsent>
<nextsent>all relations are of the form nucleus satellite, and the five relation types are contrast,1contrast is in fact often realized with multi-nuclear structure 1
</nextsent>
<nextsent>       ff flfi ffi  !
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1582">
<title id=" W06-3401.xml">prosodic correlates of rhetorical relations </title>
<section> previous research.  </section>
<citcontext>
<prevsection>
<prevsent>this work solely investigates the usefulness of prosodic features in classifying these five relations, rather than relying on discourse or lexical cues.
</prevsent>
<prevsent>a central motivation for this study is the hope that rhetorical parsing using prosodic features might aidan automatic summarization system.
</prevsent>
</prevsection>
<citsent citstr=" P99-1047 ">
early work on automatic rst analysis relied heavily on discourse cues to identify relations (corston oliver, 1998; knott and sanders, 1998; marcu, 1997; <papid> W97-0713 </papid>marcu, 1999; <papid> P99-1047 </papid>marcu, 2000) (<papid> J00-3005 </papid>e.g., however signaling an antithesis or contrast relation.</citsent>
<aftsection>
<nextsent>as mentioned above, this approach is limited by the fact that rhetorical relations are often not explicitly signalled,and discourse markers can nevertheless be ambiguous.
</nextsent>
<nextsent>a novel approach was described in (marcu and echihabi, 2002), <papid> P02-1047 </papid>which used an unsupervised training technique, extracting relations that were explicitly and unamibiguously signalled and automatically labelling those examples as the training set.</nextsent>
<nextsent>this unsupervised technique allowed the authors to label very large amount of data and pairs of words foundin the nucleus and satellite as the features of inter est.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1583">
<title id=" W06-3401.xml">prosodic correlates of rhetorical relations </title>
<section> previous research.  </section>
<citcontext>
<prevsection>
<prevsent>this work solely investigates the usefulness of prosodic features in classifying these five relations, rather than relying on discourse or lexical cues.
</prevsent>
<prevsent>a central motivation for this study is the hope that rhetorical parsing using prosodic features might aidan automatic summarization system.
</prevsent>
</prevsection>
<citsent citstr=" J00-3005 ">
early work on automatic rst analysis relied heavily on discourse cues to identify relations (corston oliver, 1998; knott and sanders, 1998; marcu, 1997; <papid> W97-0713 </papid>marcu, 1999; <papid> P99-1047 </papid>marcu, 2000) (<papid> J00-3005 </papid>e.g., however signaling an antithesis or contrast relation.</citsent>
<aftsection>
<nextsent>as mentioned above, this approach is limited by the fact that rhetorical relations are often not explicitly signalled,and discourse markers can nevertheless be ambiguous.
</nextsent>
<nextsent>a novel approach was described in (marcu and echihabi, 2002), <papid> P02-1047 </papid>which used an unsupervised training technique, extracting relations that were explicitly and unamibiguously signalled and automatically labelling those examples as the training set.</nextsent>
<nextsent>this unsupervised technique allowed the authors to label very large amount of data and pairs of words foundin the nucleus and satellite as the features of inter est.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1584">
<title id=" W06-3401.xml">prosodic correlates of rhetorical relations </title>
<section> previous research.  </section>
<citcontext>
<prevsection>
<prevsent>early work on automatic rst analysis relied heavily on discourse cues to identify relations (corston oliver, 1998; knott and sanders, 1998; marcu, 1997; <papid> W97-0713 </papid>marcu, 1999; <papid> P99-1047 </papid>marcu, 2000) (<papid> J00-3005 </papid>e.g., however signaling an antithesis or contrast relation.</prevsent>
<prevsent>as mentioned above, this approach is limited by the fact that rhetorical relations are often not explicitly signalled,and discourse markers can nevertheless be ambigu ous.</prevsent>
</prevsection>
<citsent citstr=" P02-1047 ">
a novel approach was described in (marcu and echihabi, 2002), <papid> P02-1047 </papid>which used an unsupervised training technique, extracting relations that were explicitly and unamibiguously signalled and automatically labelling those examples as the training set.</citsent>
<aftsection>
<nextsent>this unsupervised technique allowed the authors to label very large amount of data and pairs of words foundin the nucleus and satellite as the features of interest.
</nextsent>
<nextsent>the authors reported very encouraging pairwise classification results using these word-pair features,though subsequent work using the same bootstrapping technique has fared less well (sporleder and lascarides, to appear 2006).
</nextsent>
<nextsent>there is little precedent for applying rst to speech dialogues, though (taboada, 2004) describes rhetorical analyses of spanish and english spoken dialogues, with in-depth corpus analyses of discourse markers and their corresponding relations.
</nextsent>
<nextsent>the work in (noordman et al, 1999) uses short read texts to explore the relationship between prosody and the level of hierarchy in an rst tree.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1585">
<title id=" W06-3401.xml">prosodic correlates of rhetorical relations </title>
<section> experimental setup.  </section>
<citcontext>
<prevsection>
<prevsent>all data was taken from the icsi meetings corpus(janin et al, 2003), corpus of 75 unrestricted do main meetings averaging about an hour in length each.
</prevsent>
<prevsent>both native and non-native english speakers participate in the meetings.
</prevsent>
</prevsection>
<citsent citstr=" W04-2319 ">
the following experiments used manual meeting transcripts and relied on manual dialogue act segmentation (shriberg et al, 2004).<papid> W04-2319 </papid></citsent>
<aftsection>
<nextsent>a given meeting can contain between 1000and 1600 dialogue acts.
</nextsent>
<nextsent>all rhetorical relation examples in the training and test sets are pairs of adjacent dialogue acts.
</nextsent>
<nextsent>4.2 features.
</nextsent>
<nextsent>seventy-five prosodic features were extracted in all,relating to pitch (or f0) contour, pitch variance, energy, rate-of-speech, pause and duration.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1586">
<title id=" W06-2005.xml">tagging portuguese with a spanish tagger </title>
<section> abstract </section>
<citcontext>
<prevsection>
<prevsent>we describe knowledge and resource light system for an automatic morphological analysis and tagging of brazilian portuguese.1 we avoid the use of labor intensive resources; particularly, large annotated corpora and lexicons.
</prevsent>
<prevsent>instead, we use (i) an annotated corpus of peninsular spanish, language related to portuguese, (ii) an unannotated corpus of portuguese,(iii) description of portuguese morphology on the level of basic grammar book.
</prevsent>
</prevsection>
<citsent citstr=" W04-3229 ">
we extend the similar work that we have done (hana et al, 2004; <papid> W04-3229 </papid>feldman et al,2006) by proposing an alternative algorithm for cognate transfer that effectively projects the spanish emission probabilities into portuguese.</citsent>
<aftsection>
<nextsent>our experiments use minimal new human effort and show 21% error reduction over even emissions on fine-grained tagset.
</nextsent>
<nextsent>part of speech (pos) tagging is an important step in natural language processing.
</nextsent>
<nextsent>corpora that have been pos-tagged are very useful both for linguistic research, e.g. finding instances or frequencies of particular constructions (meurers, 2004) and for further computational processing, such as syntactic parsing, speech recognition, stemming, word sense disambiguation.
</nextsent>
<nextsent>morphological tagging isthe process of assigning pos, case, number, gender and other morphological information to each word in corpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1590">
<title id=" W06-2005.xml">tagging portuguese with a spanish tagger </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>lack annotated resources of this kind, mainly due to the lack of training corpora which are usually required for applying standard statistical taggers.applications of taggers include syntactic parsing, stemming, text-to-speech synthesis, word sense disambiguation, information extraction.
</prevsent>
<prevsent>forsome of these getting all the tags right is inessential, e.g. the input to noun phrase chunking does not necessarily require high accuracy fine-grained tag resolution.
</prevsent>
</prevsection>
<citsent citstr=" N01-1026 ">
cross-language information transfer is not new;however, most of the existing work relies on parallel corpora (e.g. hwa et al, 2004; yarowsky and ngai, 2001) <papid> N01-1026 </papid>which are difficult to find, especially for lesser studied languages.</citsent>
<aftsection>
<nextsent>in this paper, we describe cross-language method that requires neither training data of the target language nor bilingual lexicons or parallel corpora.
</nextsent>
<nextsent>we report the results of the experiments done on brazilian portuguese and peninsular spanish, however, our system is not tied to these particular languages.
</nextsent>
<nextsent>the method is easily portable to other (inflected)languages.
</nextsent>
<nextsent>our method assumes that an annotated corpus exists for the source language (here,spanish) and that textbook with basic linguistic facts about the source language is available (here, portuguese).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1599">
<title id=" W06-2005.xml">tagging portuguese with a spanish tagger </title>
<section> number 5 5.  </section>
<citcontext>
<prevsection>
<prevsent>that, however, is very costly.
</prevsent>
<prevsent>instead, we created such lexicon automatically.
</prevsent>
</prevsection>
<citsent citstr=" J97-3003 ">
usually, automatically acquired lexicons and similar systems are used as backup for largehigh-precision high-cost manually created lexicons (e.g. mikheev, 1997; <papid> J97-3003 </papid>hlavacova?, 2001).</citsent>
<aftsection>
<nextsent>such systems extrapolate the information about the words known by the lexicon (e.g. distributional properties of endings) to unknown words.
</nextsent>
<nextsent>since our approach is resource light, we do not have any such large lexicon to extrapolate from.the general idea of our system is very simple.
</nextsent>
<nextsent>the paradigm-based guesser, provides all the possible analyses of word consistent with portuguese paradigms.
</nextsent>
<nextsent>obviously, this approach mas 35sively overgenerates.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1600">
<title id=" W06-2005.xml">tagging portuguese with a spanish tagger </title>
<section> tagging.  </section>
<citcontext>
<prevsection>
<prevsent>it is calculated assuming an oracle portuguese tagger that is always able to select the correct pos tag if it is in the set of options given by the morphological analyzer.
</prevsent>
<prevsent>notice also that for the tagging accuracy, the drop of recall is less important than the drop of ambiguity.
</prevsent>
</prevsection>
<citsent citstr=" A00-1031 ">
we used the tnt tagger (brants, 2000), <papid> A00-1031 </papid>an implementation of the viterbi algorithm for second order markov model.</citsent>
<aftsection>
<nextsent>in the traditional approach,we would train the taggers transitional and emission probabilities on large annotated corpus of portuguese.
</nextsent>
<nextsent>however, our resource-light approach means that such corpus is not available to us andwe need to use different ways to obtain this information.
</nextsent>
<nextsent>we assume that syntactic properties of spanish and portuguese are similar enough to be able touse the transitional probabilities trained on spanish (after simple tagset mapping).the situation with the lexical properties ascap tured by emission probabilities is more complex.below we present three different ways how to obtains emissions, assuming:1.
</nextsent>
<nextsent>they are the same: we use the spanish emissions directly (5.1).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1601">
<title id=" W06-2005.xml">tagging portuguese with a spanish tagger </title>
<section> tagging.  </section>
<citcontext>
<prevsection>
<prevsent>in the absence of this knowledge, we automatically identify cog nates, using the edit distance measure (normalized by word length).
</prevsent>
<prevsent>unlike in the standard edit distance, the cost of operations is dependent on the arguments.
</prevsent>
</prevsection>
<citsent citstr=" P00-1027 ">
similarly as yarowsky and wicentowski (2000), <papid> P00-1027 </papid>we assume that, in any language, vowels are more muta ble in inflection than consonants, thus for example replacing for is cheaper that replacing by r. in addition, costs are refined based on some well known and common phonetic-orthographic regularities, e.g. replacing q with is less costly than replacing with, say s. however, we do not want to do detailed contrastive morpho-phonological analysis, since we want our system to be portable to other languages.</citsent>
<aftsection>
<nextsent>so, some facts from simple grammar reference book should be enough.using cognates.
</nextsent>
<nextsent>having list of spanish portuguese cognate pairs, we can use these tomap the emission probabilities acquired on spanish corpus to portuguese.
</nextsent>
<nextsent>37 lets assume spanish word ws and portuguese word wp are cognates.
</nextsent>
<nextsent>letts denote the tags that ws occurs within the spanish corpus, and let ps(t) be the emission probability of tag (t 6?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1602">
<title id=" W06-2005.xml">tagging portuguese with a spanish tagger </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the details of the tagset used in the experiments are not provided, so precise comparison with our results is difficult.
</prevsent>
<prevsent>previous research in resource-light language learning has defined resource-light in different ways.
</prevsent>
</prevsection>
<citsent citstr=" J94-2001 ">
some have assumed only partially tagged training corpora (merialdo, 1994); <papid> J94-2001 </papid>some have be gun with small tagged seed word lists (cucerzan and yarowsky, 1999) <papid> W99-0612 </papid>for named-entity tagging,while others have exploited the automatic transfer of an already existing annotated resource in min e-even e-cognates tag: 56.9 77.2 82.1 pos: 65.3 84.2 87.6 subpos: 61.7 83.3 86.9 gender: 70.4 87.3 90.2 number: 78.3 95.3 96.0 case: 93.8 96.8 97.2 possessors num: 85.4 96.7 97.0 form: 92.9 99.2 99.2 person: 74.5 91.2 92.7 tense: 90.7 95.1 96.1 mood: 91.5 95.0 96.0 participle: 99.9 100.0 100.0 table 4: tagging brazilian portuguese different genres or different language (e.g. cross language projection of morphological and syntactic information in (yarowsky et al, 2001;<papid> H01-1035 </papid>yarowsky and ngai, 2001), <papid> N01-1026 </papid>requiring no direct supervision in the target language).ngai and yarowsky (2000) <papid> P00-1016 </papid>observe that the total weighted human and resource costs is the most practical measure of the degree of supervision.cucerzan and yarowsky (2002) <papid> W02-2006 </papid>observe that an other useful measure of minimal supervision is the additional cost of obtaining desired functionality from existing commonly available knowledge sources.</citsent>
<aftsection>
<nextsent>they note that for remarkably wide range of languages, there exist plenty of reference grammar books and dictionaries which is an invaluable linguistic resource.
</nextsent>
<nextsent>7.1 resource-light approaches to romance.
</nextsent>
<nextsent>languages cucerzan and yarowsky (2002) <papid> W02-2006 </papid>present method for bootstrapping fine-grained, broad coverage pos tagger in new language using only one person-day of data acquisition effort.</nextsent>
<nextsent>similarly to us, they use basic library reference grammar book, and access to an existing monolingual text corpus in the language, but they also use medium-sized bilingual dictionary.in our work, we use paradigm-based morphology, including only the basic paradigms from standard grammar textbook.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1603">
<title id=" W06-2005.xml">tagging portuguese with a spanish tagger </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the details of the tagset used in the experiments are not provided, so precise comparison with our results is difficult.
</prevsent>
<prevsent>previous research in resource-light language learning has defined resource-light in different ways.
</prevsent>
</prevsection>
<citsent citstr=" W99-0612 ">
some have assumed only partially tagged training corpora (merialdo, 1994); <papid> J94-2001 </papid>some have be gun with small tagged seed word lists (cucerzan and yarowsky, 1999) <papid> W99-0612 </papid>for named-entity tagging,while others have exploited the automatic transfer of an already existing annotated resource in min e-even e-cognates tag: 56.9 77.2 82.1 pos: 65.3 84.2 87.6 subpos: 61.7 83.3 86.9 gender: 70.4 87.3 90.2 number: 78.3 95.3 96.0 case: 93.8 96.8 97.2 possessors num: 85.4 96.7 97.0 form: 92.9 99.2 99.2 person: 74.5 91.2 92.7 tense: 90.7 95.1 96.1 mood: 91.5 95.0 96.0 participle: 99.9 100.0 100.0 table 4: tagging brazilian portuguese different genres or different language (e.g. cross language projection of morphological and syntactic information in (yarowsky et al, 2001;<papid> H01-1035 </papid>yarowsky and ngai, 2001), <papid> N01-1026 </papid>requiring no direct supervision in the target language).ngai and yarowsky (2000) <papid> P00-1016 </papid>observe that the total weighted human and resource costs is the most practical measure of the degree of supervision.cucerzan and yarowsky (2002) <papid> W02-2006 </papid>observe that an other useful measure of minimal supervision is the additional cost of obtaining desired functionality from existing commonly available knowledge sources.</citsent>
<aftsection>
<nextsent>they note that for remarkably wide range of languages, there exist plenty of reference grammar books and dictionaries which is an invaluable linguistic resource.
</nextsent>
<nextsent>7.1 resource-light approaches to romance.
</nextsent>
<nextsent>languages cucerzan and yarowsky (2002) <papid> W02-2006 </papid>present method for bootstrapping fine-grained, broad coverage pos tagger in new language using only one person-day of data acquisition effort.</nextsent>
<nextsent>similarly to us, they use basic library reference grammar book, and access to an existing monolingual text corpus in the language, but they also use medium-sized bilingual dictionary.in our work, we use paradigm-based morphology, including only the basic paradigms from standard grammar textbook.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1604">
<title id=" W06-2005.xml">tagging portuguese with a spanish tagger </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the details of the tagset used in the experiments are not provided, so precise comparison with our results is difficult.
</prevsent>
<prevsent>previous research in resource-light language learning has defined resource-light in different ways.
</prevsent>
</prevsection>
<citsent citstr=" H01-1035 ">
some have assumed only partially tagged training corpora (merialdo, 1994); <papid> J94-2001 </papid>some have be gun with small tagged seed word lists (cucerzan and yarowsky, 1999) <papid> W99-0612 </papid>for named-entity tagging,while others have exploited the automatic transfer of an already existing annotated resource in min e-even e-cognates tag: 56.9 77.2 82.1 pos: 65.3 84.2 87.6 subpos: 61.7 83.3 86.9 gender: 70.4 87.3 90.2 number: 78.3 95.3 96.0 case: 93.8 96.8 97.2 possessors num: 85.4 96.7 97.0 form: 92.9 99.2 99.2 person: 74.5 91.2 92.7 tense: 90.7 95.1 96.1 mood: 91.5 95.0 96.0 participle: 99.9 100.0 100.0 table 4: tagging brazilian portuguese different genres or different language (e.g. cross language projection of morphological and syntactic information in (yarowsky et al, 2001;<papid> H01-1035 </papid>yarowsky and ngai, 2001), <papid> N01-1026 </papid>requiring no direct supervision in the target language).ngai and yarowsky (2000) <papid> P00-1016 </papid>observe that the total weighted human and resource costs is the most practical measure of the degree of supervision.cucerzan and yarowsky (2002) <papid> W02-2006 </papid>observe that an other useful measure of minimal supervision is the additional cost of obtaining desired functionality from existing commonly available knowledge sources.</citsent>
<aftsection>
<nextsent>they note that for remarkably wide range of languages, there exist plenty of reference grammar books and dictionaries which is an invaluable linguistic resource.
</nextsent>
<nextsent>7.1 resource-light approaches to romance.
</nextsent>
<nextsent>languages cucerzan and yarowsky (2002) <papid> W02-2006 </papid>present method for bootstrapping fine-grained, broad coverage pos tagger in new language using only one person-day of data acquisition effort.</nextsent>
<nextsent>similarly to us, they use basic library reference grammar book, and access to an existing monolingual text corpus in the language, but they also use medium-sized bilingual dictionary.in our work, we use paradigm-based morphology, including only the basic paradigms from standard grammar textbook.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1606">
<title id=" W06-2005.xml">tagging portuguese with a spanish tagger </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the details of the tagset used in the experiments are not provided, so precise comparison with our results is difficult.
</prevsent>
<prevsent>previous research in resource-light language learning has defined resource-light in different ways.
</prevsent>
</prevsection>
<citsent citstr=" P00-1016 ">
some have assumed only partially tagged training corpora (merialdo, 1994); <papid> J94-2001 </papid>some have be gun with small tagged seed word lists (cucerzan and yarowsky, 1999) <papid> W99-0612 </papid>for named-entity tagging,while others have exploited the automatic transfer of an already existing annotated resource in min e-even e-cognates tag: 56.9 77.2 82.1 pos: 65.3 84.2 87.6 subpos: 61.7 83.3 86.9 gender: 70.4 87.3 90.2 number: 78.3 95.3 96.0 case: 93.8 96.8 97.2 possessors num: 85.4 96.7 97.0 form: 92.9 99.2 99.2 person: 74.5 91.2 92.7 tense: 90.7 95.1 96.1 mood: 91.5 95.0 96.0 participle: 99.9 100.0 100.0 table 4: tagging brazilian portuguese different genres or different language (e.g. cross language projection of morphological and syntactic information in (yarowsky et al, 2001;<papid> H01-1035 </papid>yarowsky and ngai, 2001), <papid> N01-1026 </papid>requiring no direct supervision in the target language).ngai and yarowsky (2000) <papid> P00-1016 </papid>observe that the total weighted human and resource costs is the most practical measure of the degree of supervision.cucerzan and yarowsky (2002) <papid> W02-2006 </papid>observe that an other useful measure of minimal supervision is the additional cost of obtaining desired functionality from existing commonly available knowledge sources.</citsent>
<aftsection>
<nextsent>they note that for remarkably wide range of languages, there exist plenty of reference grammar books and dictionaries which is an invaluable linguistic resource.
</nextsent>
<nextsent>7.1 resource-light approaches to romance.
</nextsent>
<nextsent>languages cucerzan and yarowsky (2002) <papid> W02-2006 </papid>present method for bootstrapping fine-grained, broad coverage pos tagger in new language using only one person-day of data acquisition effort.</nextsent>
<nextsent>similarly to us, they use basic library reference grammar book, and access to an existing monolingual text corpus in the language, but they also use medium-sized bilingual dictionary.in our work, we use paradigm-based morphology, including only the basic paradigms from standard grammar textbook.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1607">
<title id=" W06-2005.xml">tagging portuguese with a spanish tagger </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the details of the tagset used in the experiments are not provided, so precise comparison with our results is difficult.
</prevsent>
<prevsent>previous research in resource-light language learning has defined resource-light in different ways.
</prevsent>
</prevsection>
<citsent citstr=" W02-2006 ">
some have assumed only partially tagged training corpora (merialdo, 1994); <papid> J94-2001 </papid>some have be gun with small tagged seed word lists (cucerzan and yarowsky, 1999) <papid> W99-0612 </papid>for named-entity tagging,while others have exploited the automatic transfer of an already existing annotated resource in min e-even e-cognates tag: 56.9 77.2 82.1 pos: 65.3 84.2 87.6 subpos: 61.7 83.3 86.9 gender: 70.4 87.3 90.2 number: 78.3 95.3 96.0 case: 93.8 96.8 97.2 possessors num: 85.4 96.7 97.0 form: 92.9 99.2 99.2 person: 74.5 91.2 92.7 tense: 90.7 95.1 96.1 mood: 91.5 95.0 96.0 participle: 99.9 100.0 100.0 table 4: tagging brazilian portuguese different genres or different language (e.g. cross language projection of morphological and syntactic information in (yarowsky et al, 2001;<papid> H01-1035 </papid>yarowsky and ngai, 2001), <papid> N01-1026 </papid>requiring no direct supervision in the target language).ngai and yarowsky (2000) <papid> P00-1016 </papid>observe that the total weighted human and resource costs is the most practical measure of the degree of supervision.cucerzan and yarowsky (2002) <papid> W02-2006 </papid>observe that an other useful measure of minimal supervision is the additional cost of obtaining desired functionality from existing commonly available knowledge sources.</citsent>
<aftsection>
<nextsent>they note that for remarkably wide range of languages, there exist plenty of reference grammar books and dictionaries which is an invaluable linguistic resource.
</nextsent>
<nextsent>7.1 resource-light approaches to romance.
</nextsent>
<nextsent>languages cucerzan and yarowsky (2002) <papid> W02-2006 </papid>present method for bootstrapping fine-grained, broad coverage pos tagger in new language using only one person-day of data acquisition effort.</nextsent>
<nextsent>similarly to us, they use basic library reference grammar book, and access to an existing monolingual text corpus in the language, but they also use medium-sized bilingual dictionary.in our work, we use paradigm-based morphology, including only the basic paradigms from standard grammar textbook.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1612">
<title id=" W06-2005.xml">tagging portuguese with a spanish tagger </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the accuracy of their model is comparable to ours.
</prevsent>
<prevsent>on fine-grained (up to 5-feature) pos space, they achieve 86.5% for spanish and 75.5% for romanian.
</prevsent>
</prevsection>
<citsent citstr=" E03-1038 ">
with tagset of similar size (11features) we obtain the accuracy of 82.1% for por tuguese.carreras et al (2003) <papid> E03-1038 </papid>present work on developing low-cost named entity recognizers (ner) for language with no available annotated resources, using as starting point existing resources for similar language.</citsent>
<aftsection>
<nextsent>they devise and evaluate several strategies to build catalan ner system using only annotated spanish data and unlabeled catalan text, and compare their approach with classical bootstrapping setting where small initial corpus in the target language is hand tagged.
</nextsent>
<nextsent>it turns out that the hand translation of spanish model is better than model directly learned from small hand annotated training corpus of catalan.
</nextsent>
<nextsent>thebest result is achieved using cross-linguistic features.
</nextsent>
<nextsent>solorio and lopez (2005) follow their ap proach; however, they apply the ner system for spanish directly to portuguese and train classifier using the output and the real classes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1613">
<title id=" W06-2005.xml">tagging portuguese with a spanish tagger </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>solorio and lopez (2005) follow their ap proach; however, they apply the ner system for spanish directly to portuguese and train classifier using the output and the real classes.
</prevsent>
<prevsent>7.2 cognates.
</prevsent>
</prevsection>
<citsent citstr=" N01-1020 ">
mann and yarowsky (2001) <papid> N01-1020 </papid>present method for inducing translation lexicons based on trasduction modules of cognate pairs via bridge languages.bilingual lexicons within language families are induced using probabilistic string edit distance models.</citsent>
<aftsection>
<nextsent>translation lexicons for abitrary distant language pairs are then generated by combination of these intra-family translation models and oneor more cross-family online dictionaries.
</nextsent>
<nextsent>similarly to mann and yarowsky (2001), <papid> N01-1020 </papid>we show that languages are often close enough to others within their language family so that cognate pairs between the two are common, and significant portions of the translation lexicon can be induced with high accuracy where no bilingual dictionary or parallel corpora may exist.</nextsent>
<nextsent>we have shown that tagging system with small amount of manually created resources can be successful.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1619">
<title id=" W06-1637.xml">priming effects in combinatory categorial grammar </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>structural priming occurs when speaker repeats syntactic decision, and has been demonstrated in numerous experiments over the past two decades (e.g., bock, 1986; branigan et al, 2000).
</prevsent>
<prevsent>these experimental findings show that subjects are more likely to choose, e.g., passive voice construction if they have previously comprehended or produced such construction.recent studies have used syntactically annotated corpora to investigate structural priming.
</prevsent>
</prevsection>
<citsent citstr=" N06-2031 ">
the results have demonstrated the existence of priming effects in corpus data: they occur for specific syntactic constructions (gries, 2005; szmrecsanyi, 2005), consistent with the experimental literature, but also generalize to syntactic rules across the board, which repeated more often than expected by chance (reitter et al, 2006<papid> N06-2031 </papid>b; dubey et al, 2006).<papid> P06-1053 </papid></citsent>
<aftsection>
<nextsent>in the present paper, we build on this corpus-based approach to priming, but focuson the role of the underlying syntactic representations.
</nextsent>
<nextsent>in particular, we use priming to evaluate claims resulting from particular syntactic theory,which is way of testing the representational assumptions it makes.using priming effects to inform syntactic theory is novel idea; previous corpus-based priming studies have simply worked with uncontroversial classes of constructions (e.g., passive/active).
</nextsent>
<nextsent>the contribution of this paper is to overcome this limitation by defining computational model of priming with clear interface to particular syntactic framework.
</nextsent>
<nextsent>the general assumption we makeis that priming is phenomenon relating to grammatical constituents ? these constituents determine the syntactic choices whose repetition can lead topriming.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1621">
<title id=" W06-1637.xml">priming effects in combinatory categorial grammar </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>structural priming occurs when speaker repeats syntactic decision, and has been demonstrated in numerous experiments over the past two decades (e.g., bock, 1986; branigan et al, 2000).
</prevsent>
<prevsent>these experimental findings show that subjects are more likely to choose, e.g., passive voice construction if they have previously comprehended or produced such construction.recent studies have used syntactically annotated corpora to investigate structural priming.
</prevsent>
</prevsection>
<citsent citstr=" P06-1053 ">
the results have demonstrated the existence of priming effects in corpus data: they occur for specific syntactic constructions (gries, 2005; szmrecsanyi, 2005), consistent with the experimental literature, but also generalize to syntactic rules across the board, which repeated more often than expected by chance (reitter et al, 2006<papid> N06-2031 </papid>b; dubey et al, 2006).<papid> P06-1053 </papid></citsent>
<aftsection>
<nextsent>in the present paper, we build on this corpus-based approach to priming, but focuson the role of the underlying syntactic representations.
</nextsent>
<nextsent>in particular, we use priming to evaluate claims resulting from particular syntactic theory,which is way of testing the representational assumptions it makes.using priming effects to inform syntactic theory is novel idea; previous corpus-based priming studies have simply worked with uncontroversial classes of constructions (e.g., passive/active).
</nextsent>
<nextsent>the contribution of this paper is to overcome this limitation by defining computational model of priming with clear interface to particular syntactic framework.
</nextsent>
<nextsent>the general assumption we makeis that priming is phenomenon relating to grammatical constituents ? these constituents determine the syntactic choices whose repetition can lead topriming.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1625">
<title id=" W06-1637.xml">priming effects in combinatory categorial grammar </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>2.3 combinatory categorial grammar.
</prevsent>
<prevsent>ccg (steedman, 2000) is mildly context sensitive, lexicalized grammar formalism with atransparent syntax-semantics interface and flexible constituent structure that is of particular interest to psycho linguistics, since it allows the construction of incremental derivations.
</prevsent>
</prevsection>
<citsent citstr=" P04-1014 ">
ccg has also enjoyed the interest of the nlp community, with high-accuracy wide-coverage parsers(clark and curran, 2004; <papid> P04-1014 </papid>hockenmaier and steedman, 2002) <papid> P02-1043 </papid>and generators1 available (white and baldridge, 2003).</citsent>
<aftsection>
<nextsent>words are associated with lexical categories which specify their subcategorization behaviour, eg.
</nextsent>
<nextsent>((s[dcl]\np)/np)/np is the lexical category for (tensed) ditransitive verbs in english such as gives or send, which expect two np objects totheir right, and one np subject to their left.
</nextsent>
<nextsent>complex categories x/y or x\y are functors which yield constituent with category x, if they are applied to constituent with category to their right (/y) or to their left (\y).
</nextsent>
<nextsent>constituents are combined via small set of combinatory rule schemata: forward application: x/y ?  1http://opennlp.sourceforge.net/ backward application: x\y ?  forward composition: x/y y/z x/z backward composition: y\z x\y x\z backw.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1626">
<title id=" W06-1637.xml">priming effects in combinatory categorial grammar </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>2.3 combinatory categorial grammar.
</prevsent>
<prevsent>ccg (steedman, 2000) is mildly context sensitive, lexicalized grammar formalism with atransparent syntax-semantics interface and flexible constituent structure that is of particular interest to psycho linguistics, since it allows the construction of incremental derivations.
</prevsent>
</prevsection>
<citsent citstr=" P02-1043 ">
ccg has also enjoyed the interest of the nlp community, with high-accuracy wide-coverage parsers(clark and curran, 2004; <papid> P04-1014 </papid>hockenmaier and steedman, 2002) <papid> P02-1043 </papid>and generators1 available (white and baldridge, 2003).</citsent>
<aftsection>
<nextsent>words are associated with lexical categories which specify their subcategorization behaviour, eg.
</nextsent>
<nextsent>((s[dcl]\np)/np)/np is the lexical category for (tensed) ditransitive verbs in english such as gives or send, which expect two np objects totheir right, and one np subject to their left.
</nextsent>
<nextsent>complex categories x/y or x\y are functors which yield constituent with category x, if they are applied to constituent with category to their right (/y) or to their left (\y).
</nextsent>
<nextsent>constituents are combined via small set of combinatory rule schemata: forward application: x/y ?  1http://opennlp.sourceforge.net/ backward application: x\y ?  forward composition: x/y y/z x/z backward composition: y\z x\y x\z backw.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1627">
<title id=" W06-1637.xml">priming effects in combinatory categorial grammar </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>ccg uses thesame lexical categories for long-range dependencies that arise eg.
</prevsent>
<prevsent>in wh-movement or coordination as for local dependencies, and does notre quire traces: the man that saw np (np\np)/(s/np) np (s\np)/np  s/(s\np)  s/np   np\np saw and you heard the man np (s\np)/np conj np (s\np)/np   s/(s\np) s/(s\np)   s/np s/np  ?  s/np   the combinatory rules of ccg allow multiple, semantically equivalent, syntactic derivations of the same sentence.
</prevsent>
</prevsection>
<citsent citstr=" P96-1011 ">
this spurious ambiguity is the result of ccgs flexible constituent structure, which can account for long-range dependencies and coordination (as in the above example), and also for interaction with information structure.ccg parsers often limit the use of the combi natory rules (in particular: type-raising) to obtain single right-branching normal form derivation(eisner, 1996) <papid> P96-1011 </papid>for each possible semantic inter pretation.</citsent>
<aftsection>
<nextsent>such normal form derivations only use composition and type-raising where syntactically necessary (eg.
</nextsent>
<nextsent>in relative clauses).
</nextsent>
<nextsent>3.1 priming effects.
</nextsent>
<nextsent>we expect priming effects to apply to ccg categories, which describe the type of constituent including the arguments it expects.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1628">
<title id=" W06-1637.xml">priming effects in combinatory categorial grammar </title>
<section> predictions.  </section>
<citcontext>
<prevsection>
<prevsent>the interaction of such conditions withthe priming effect can be quantified in the statistical model.
</prevsent>
<prevsent>3.3 incrementality of analyses.
</prevsent>
</prevsection>
<citsent citstr=" P94-1018 ">
type-raising and composition allow derivations that are mostly left-branching, or incremental.adopting left-to-right processing order for sentence is important, if the syntactic theory is to make psycholinguistically viable predictions (niv, 1994; <papid> P94-1018 </papid>steedman, 2000).pickering et al (2002) present priming experiments that suggest that, in production, structural dominance and linearization do not take place in different stages.</citsent>
<aftsection>
<nextsent>their argument involves verbal phrases with shifted prepositional object such as showed to the mechanic torn overall.
</nextsent>
<nextsent>at dominance-only level, such phrases are equivalent to non-shifted prepositional constructions (showeda torn overall to the mechanic), but the two variants may be differentiated at linearization stage.
</nextsent>
<nextsent>shifted primes do not prime prepositional object sin their canonical position, thus priming must occur at linear ized level, and separate dominance level seems unlikely (unless priming is selective).
</nextsent>
<nextsent>ccg is compatible with one-stage formulations of syntax, as no transformation is assumed and categories encode linearization together with subcategorization.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1629">
<title id=" W06-1637.xml">priming effects in combinatory categorial grammar </title>
<section> corpus data.  </section>
<citcontext>
<prevsection>
<prevsent>we hypothesize that priming effects predicted on the basis of incremental ccg analyses will be as strong than those predicted on the basis of their normal form equivalents.
</prevsent>
<prevsent>4.1 the switchboard corpus.
</prevsent>
</prevsection>
<citsent citstr=" H94-1020 ">
the switchboard (marcus et al, 1994) <papid> H94-1020 </papid>corpus contains transcriptions of spoken, spontaneous conversation annotated with phrase-structure trees.</citsent>
<aftsection>
<nextsent>dialogues were recorded over the telephone among randomly paired north american speakers, who were just given general topic to talk about.
</nextsent>
<nextsent>80,000 utterances of the corpus have been annotated with syntactic structure.
</nextsent>
<nextsent>this portion,included in the penn treebank, has been time aligned (per word) in the paraphrase project (car letta et al, 2004).using the same regression technique as employed here, reitter et al (2006<papid> N06-2031 </papid>b) found marked structural priming effect for penn-treebank style phrase structure rules in switchboard.</nextsent>
<nextsent>4.2 disfluencies.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1633">
<title id=" W06-1637.xml">priming effects in combinatory categorial grammar </title>
<section> corpus data.  </section>
<citcontext>
<prevsection>
<prevsent>this portion,included in the penn treebank, has been time aligned (per word) in the paraphrase project (car letta et al, 2004).using the same regression technique as employed here, reitter et al (2006<papid> N06-2031 </papid>b) found marked structural priming effect for penn-treebank style phrase structure rules in switchboard.</prevsent>
<prevsent>4.2 disfluencies.</prevsent>
</prevsection>
<citsent citstr=" P04-1005 ">
speech is often dis fluent, and speech repairs are known to repeat large portions of the preceding context (johnson and charniak, 2004).<papid> P04-1005 </papid></citsent>
<aftsection>
<nextsent>the original switchboard transcripts contains these disflu encies (marked up as edited): ( (s    (edited (rm (-dfl- \bs [) ) (edited (rm (-dfl- \bs [) ) (cc and) (, ,) (ip (-dfl- \bs +) )) (cc and) (, ,) (rs (-dfl- \bs ]) ) (ip (-dfl- \bs +) ))   2selectional criteria such as information structure and intonation allow to distinguish between semantically different analyses.
</nextsent>
<nextsent>311 (cc and)    (rs (-dfl- \bs ]) )    (np-sbj (prp i) ) (vp (vbp guess) (sbar (-none- 0) (s (np-sbj (dt that) ) (vp (bes s) (sbar-nom-prd (whnp-1 (wp what) ) (s (np-sbj (prp i) ) (advp (rb really) ) (vp (vbp like) (np (-none- *t*-1) )))))))) (.
</nextsent>
<nextsent>(-dfl- e_s) )) it is unclear to what extent these repetitions are due to priming rather than simple correction.
</nextsent>
<nextsent>in dis fluent utterances, we therefore eliminate rep aranda and only keep repairs (the portions marked with  ...  are removed).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1639">
<title id=" W06-3605.xml">a probabilistic search for the best solution among partially completed candidates </title>
<section> background </section>
<citcontext>
<prevsection>
<prevsent>the parser produces set of candidate parses and typically also implements target function that scores the parses based on their structural and lexical features.
</prevsent>
<prevsent>each parse that is compatible with both the pos tagger and the parse ris structurally correct.
</prevsent>
</prevsection>
<citsent citstr=" C90-2040 ">
the best solution may be defined, for instance, as such solution that maximizes the sum of the scores of the pos- and parser-centric target functions.in practice, the set of structurally correct solutions may be computed, for example, through the intersection or composition of finite-state automata as in the formalism of finite-state intersection grammars (koskenniemi, 1990).<papid> C90-2040 </papid></citsent>
<aftsection>
<nextsent>finding the best so 33 lution may be implemented as best-path search through viterbi decoding, given target function that satisfies the viterbi condition.
</nextsent>
<nextsent>most of the recent approaches to nlp tasks like parse re-ranking make, however, use of feature based representations and machine-learning induced target functions, which do not allow efficient search strategies that are guaranteed to find the global optimum.
</nextsent>
<nextsent>in general case, all structurally correct solutions have to be generated and scored by the target functions in order to guarantee that the globally optimal solution is found.
</nextsent>
<nextsent>further, each of the various problems in natural language processing is typically approached with different class of models, ranging from n-gram statistics to complex regress ors and classifiers such as the support vector machines.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1640">
<title id=" W06-3605.xml">a probabilistic search for the best solution among partially completed candidates </title>
<section> background </section>
<citcontext>
<prevsection>
<prevsent>these different approaches need to be combined inorder to find the globally optimal solution.
</prevsent>
<prevsent>therefore, in our study we aim to develop search strategy that allows to combine wider range of target functions.
</prevsent>
</prevsection>
<citsent citstr=" H05-1003 ">
an alternative approach is that of propagating best solutions through the pipeline system, where each step re-ranks the solutions by local criteria (ji et al, 2005).<papid> H05-1003 </papid></citsent>
<aftsection>
<nextsent>incorporating wide range of features representing information from all levels of analysis into single master classifier is other commonly used method (kambhatla, 2004; <papid> P04-3022 </papid>zelenko et al., 2004).in this paper, we assume the possibility of generating the structurally correct solutions incrementally, through sequence of partially completed solutions.</nextsent>
<nextsent>we then derive probabilistic search algorithm that attempts to identify the globally best solution, without fully completing all structurally correctsolutions.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1641">
<title id=" W06-3605.xml">a probabilistic search for the best solution among partially completed candidates </title>
<section> background </section>
<citcontext>
<prevsection>
<prevsent>therefore, in our study we aim to develop search strategy that allows to combine wider range of target functions.
</prevsent>
<prevsent>an alternative approach is that of propagating best solutions through the pipeline system, where each step re-ranks the solutions by local criteria (ji et al, 2005).<papid> H05-1003 </papid></prevsent>
</prevsection>
<citsent citstr=" P04-3022 ">
incorporating wide range of features representing information from all levels of analysis into single master classifier is other commonly used method (kambhatla, 2004; <papid> P04-3022 </papid>zelenko et al., 2004).in this paper, we assume the possibility of generating the structurally correct solutions incrementally, through sequence of partially completed solutions.</citsent>
<aftsection>
<nextsent>we then derive probabilistic search algorithm that attempts to identify the globally best solution, without fully completing all structurally correctsolutions.
</nextsent>
<nextsent>further, we do not impose strong restrictions, such as the viterbi assumption, on the target functions.
</nextsent>
<nextsent>to certain extent, this approach is related to the problem of cost-sensitive learning, where obtaining feature value is associated with cost and the objective is to minimize the cost of training data acquisition and the cost of instance classification(melville et al, 2004).
</nextsent>
<nextsent>however, the crucial difference is that we do not assume the possibility to influence when advancing partial solution, which feature will be obtained next.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1642">
<title id=" W06-2930.xml">investigating multilingual dependency parsing </title>
<section> extensions to nivres algorithm.  </section>
<citcontext>
<prevsection>
<prevsent>this may not fit all the languages.
</prevsent>
<prevsent>some dependencies may be easier to capture when proceeding from the reverse direction.
</prevsent>
</prevsection>
<citsent citstr=" I05-2044 ">
jin et al (2005) <papid> I05-2044 </papid>is an example of it for chinese, where the authors describe an adaptation of nivres parser to bidirectionality.</citsent>
<aftsection>
<nextsent>we trained the model and ran the algorithm in both directions (left to right and right to left).
</nextsent>
<nextsent>we used voting strategy based on probability scores.
</nextsent>
<nextsent>each link was assigned probability score (simply by using the probability of the la or ra actions for each link).
</nextsent>
<nextsent>we then summed the probability scores of the links from all four trees.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1643">
<title id=" W06-2930.xml">investigating multilingual dependency parsing </title>
<section> analysis.  </section>
<citcontext>
<prevsection>
<prevsent>5.1 experimental settings.
</prevsent>
<prevsent>we trained the models on projectivized?
</prevsent>
</prevsection>
<citsent citstr=" P05-1013 ">
graphs following nivre and nilsson (2005) <papid> P05-1013 </papid>method.</citsent>
<aftsection>
<nextsent>we used the complete annotated data for nine langagues.
</nextsent>
<nextsent>dueto time limitations, we could not complete the training for three languages, chinese, czech, and german. 5.2 overview of the results.
</nextsent>
<nextsent>we parsed the 12 languages using exactly the same algorithms and parameters.
</nextsent>
<nextsent>we obtained an average score of 74.93 for the labeled arcs and of 80.39 for the unlabeled ones (resp.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1644">
<title id=" W06-2915.xml">which side are you on identifying perspectives at the document and sentence levels </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>identifying the perspective from which document is written is subtask in the growing area of automatic opinion recognition and extraction.
</prevsent>
<prevsent>subjective language is used to express opinions, emotions, and sentiments.
</prevsent>
</prevsection>
<citsent citstr=" J04-3002 ">
so far, research in automatic opinion recognition has primarily addressed learning subjective language (wiebe et al, 2004; <papid> J04-3002 </papid>riloff et al, 2003), <papid> W03-0404 </papid>identifying opinionated documents (yu and hatzivassiloglou, 2003) <papid> W03-1017 </papid>and sentences (yu and hatzivassiloglou, 2003; <papid> W03-1017 </papid>riloff et al, 2003), <papid> W03-0404 </papid>and discriminating between positive and negative language (pang et al, 2002; <papid> W02-1011 </papid>morinaga et al, 2002; yu and hatzivassiloglou, 2003; <papid> W03-1017 </papid>turney and littman, 2003; dave et al, 2003; nasukawa and yi, 2003; popescu and etzioni, 2005; <papid> H05-1043 </papid>wilson et al, 2005).<papid> H05-1044 </papid></citsent>
<aftsection>
<nextsent>while by its very nature we expect much of the language that is used when presenting perspective or point-of-view to be subjective, labeling document or sentence as subjective is not enough to identify the perspective from which it is written.
</nextsent>
<nextsent>moreover, the ideology and beliefs authors possess are often expressed in ways other than positive or negative language toward specific targets.
</nextsent>
<nextsent>research on the automatic classification of movie or product reviews as positive or negative (e.g., (pang et al, 2002; <papid> W02-1011 </papid>morinaga et al, 2002; turney and littman, 2003; nasukawa and yi, 2003; mullen and collier, 2004; <papid> W04-3253 </papid>beineke et al, 2004; <papid> P04-1034 </papid>hu and liu, 2004)) is perhaps the most similar to our work.</nextsent>
<nextsent>aswith review classification, we treat perspective identification as document-level classification task, discriminating, in sense, between different types ofopinions.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1645">
<title id=" W06-2915.xml">which side are you on identifying perspectives at the document and sentence levels </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>identifying the perspective from which document is written is subtask in the growing area of automatic opinion recognition and extraction.
</prevsent>
<prevsent>subjective language is used to express opinions, emotions, and sentiments.
</prevsent>
</prevsection>
<citsent citstr=" W03-0404 ">
so far, research in automatic opinion recognition has primarily addressed learning subjective language (wiebe et al, 2004; <papid> J04-3002 </papid>riloff et al, 2003), <papid> W03-0404 </papid>identifying opinionated documents (yu and hatzivassiloglou, 2003) <papid> W03-1017 </papid>and sentences (yu and hatzivassiloglou, 2003; <papid> W03-1017 </papid>riloff et al, 2003), <papid> W03-0404 </papid>and discriminating between positive and negative language (pang et al, 2002; <papid> W02-1011 </papid>morinaga et al, 2002; yu and hatzivassiloglou, 2003; <papid> W03-1017 </papid>turney and littman, 2003; dave et al, 2003; nasukawa and yi, 2003; popescu and etzioni, 2005; <papid> H05-1043 </papid>wilson et al, 2005).<papid> H05-1044 </papid></citsent>
<aftsection>
<nextsent>while by its very nature we expect much of the language that is used when presenting perspective or point-of-view to be subjective, labeling document or sentence as subjective is not enough to identify the perspective from which it is written.
</nextsent>
<nextsent>moreover, the ideology and beliefs authors possess are often expressed in ways other than positive or negative language toward specific targets.
</nextsent>
<nextsent>research on the automatic classification of movie or product reviews as positive or negative (e.g., (pang et al, 2002; <papid> W02-1011 </papid>morinaga et al, 2002; turney and littman, 2003; nasukawa and yi, 2003; mullen and collier, 2004; <papid> W04-3253 </papid>beineke et al, 2004; <papid> P04-1034 </papid>hu and liu, 2004)) is perhaps the most similar to our work.</nextsent>
<nextsent>aswith review classification, we treat perspective identification as document-level classification task, discriminating, in sense, between different types ofopinions.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1647">
<title id=" W06-2915.xml">which side are you on identifying perspectives at the document and sentence levels </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>identifying the perspective from which document is written is subtask in the growing area of automatic opinion recognition and extraction.
</prevsent>
<prevsent>subjective language is used to express opinions, emotions, and sentiments.
</prevsent>
</prevsection>
<citsent citstr=" W03-1017 ">
so far, research in automatic opinion recognition has primarily addressed learning subjective language (wiebe et al, 2004; <papid> J04-3002 </papid>riloff et al, 2003), <papid> W03-0404 </papid>identifying opinionated documents (yu and hatzivassiloglou, 2003) <papid> W03-1017 </papid>and sentences (yu and hatzivassiloglou, 2003; <papid> W03-1017 </papid>riloff et al, 2003), <papid> W03-0404 </papid>and discriminating between positive and negative language (pang et al, 2002; <papid> W02-1011 </papid>morinaga et al, 2002; yu and hatzivassiloglou, 2003; <papid> W03-1017 </papid>turney and littman, 2003; dave et al, 2003; nasukawa and yi, 2003; popescu and etzioni, 2005; <papid> H05-1043 </papid>wilson et al, 2005).<papid> H05-1044 </papid></citsent>
<aftsection>
<nextsent>while by its very nature we expect much of the language that is used when presenting perspective or point-of-view to be subjective, labeling document or sentence as subjective is not enough to identify the perspective from which it is written.
</nextsent>
<nextsent>moreover, the ideology and beliefs authors possess are often expressed in ways other than positive or negative language toward specific targets.
</nextsent>
<nextsent>research on the automatic classification of movie or product reviews as positive or negative (e.g., (pang et al, 2002; <papid> W02-1011 </papid>morinaga et al, 2002; turney and littman, 2003; nasukawa and yi, 2003; mullen and collier, 2004; <papid> W04-3253 </papid>beineke et al, 2004; <papid> P04-1034 </papid>hu and liu, 2004)) is perhaps the most similar to our work.</nextsent>
<nextsent>aswith review classification, we treat perspective identification as document-level classification task, discriminating, in sense, between different types ofopinions.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1651">
<title id=" W06-2915.xml">which side are you on identifying perspectives at the document and sentence levels </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>identifying the perspective from which document is written is subtask in the growing area of automatic opinion recognition and extraction.
</prevsent>
<prevsent>subjective language is used to express opinions, emotions, and sentiments.
</prevsent>
</prevsection>
<citsent citstr=" W02-1011 ">
so far, research in automatic opinion recognition has primarily addressed learning subjective language (wiebe et al, 2004; <papid> J04-3002 </papid>riloff et al, 2003), <papid> W03-0404 </papid>identifying opinionated documents (yu and hatzivassiloglou, 2003) <papid> W03-1017 </papid>and sentences (yu and hatzivassiloglou, 2003; <papid> W03-1017 </papid>riloff et al, 2003), <papid> W03-0404 </papid>and discriminating between positive and negative language (pang et al, 2002; <papid> W02-1011 </papid>morinaga et al, 2002; yu and hatzivassiloglou, 2003; <papid> W03-1017 </papid>turney and littman, 2003; dave et al, 2003; nasukawa and yi, 2003; popescu and etzioni, 2005; <papid> H05-1043 </papid>wilson et al, 2005).<papid> H05-1044 </papid></citsent>
<aftsection>
<nextsent>while by its very nature we expect much of the language that is used when presenting perspective or point-of-view to be subjective, labeling document or sentence as subjective is not enough to identify the perspective from which it is written.
</nextsent>
<nextsent>moreover, the ideology and beliefs authors possess are often expressed in ways other than positive or negative language toward specific targets.
</nextsent>
<nextsent>research on the automatic classification of movie or product reviews as positive or negative (e.g., (pang et al, 2002; <papid> W02-1011 </papid>morinaga et al, 2002; turney and littman, 2003; nasukawa and yi, 2003; mullen and collier, 2004; <papid> W04-3253 </papid>beineke et al, 2004; <papid> P04-1034 </papid>hu and liu, 2004)) is perhaps the most similar to our work.</nextsent>
<nextsent>aswith review classification, we treat perspective identification as document-level classification task, discriminating, in sense, between different types ofopinions.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1654">
<title id=" W06-2915.xml">which side are you on identifying perspectives at the document and sentence levels </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>identifying the perspective from which document is written is subtask in the growing area of automatic opinion recognition and extraction.
</prevsent>
<prevsent>subjective language is used to express opinions, emotions, and sentiments.
</prevsent>
</prevsection>
<citsent citstr=" H05-1043 ">
so far, research in automatic opinion recognition has primarily addressed learning subjective language (wiebe et al, 2004; <papid> J04-3002 </papid>riloff et al, 2003), <papid> W03-0404 </papid>identifying opinionated documents (yu and hatzivassiloglou, 2003) <papid> W03-1017 </papid>and sentences (yu and hatzivassiloglou, 2003; <papid> W03-1017 </papid>riloff et al, 2003), <papid> W03-0404 </papid>and discriminating between positive and negative language (pang et al, 2002; <papid> W02-1011 </papid>morinaga et al, 2002; yu and hatzivassiloglou, 2003; <papid> W03-1017 </papid>turney and littman, 2003; dave et al, 2003; nasukawa and yi, 2003; popescu and etzioni, 2005; <papid> H05-1043 </papid>wilson et al, 2005).<papid> H05-1044 </papid></citsent>
<aftsection>
<nextsent>while by its very nature we expect much of the language that is used when presenting perspective or point-of-view to be subjective, labeling document or sentence as subjective is not enough to identify the perspective from which it is written.
</nextsent>
<nextsent>moreover, the ideology and beliefs authors possess are often expressed in ways other than positive or negative language toward specific targets.
</nextsent>
<nextsent>research on the automatic classification of movie or product reviews as positive or negative (e.g., (pang et al, 2002; <papid> W02-1011 </papid>morinaga et al, 2002; turney and littman, 2003; nasukawa and yi, 2003; mullen and collier, 2004; <papid> W04-3253 </papid>beineke et al, 2004; <papid> P04-1034 </papid>hu and liu, 2004)) is perhaps the most similar to our work.</nextsent>
<nextsent>aswith review classification, we treat perspective identification as document-level classification task, discriminating, in sense, between different types ofopinions.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1655">
<title id=" W06-2915.xml">which side are you on identifying perspectives at the document and sentence levels </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>identifying the perspective from which document is written is subtask in the growing area of automatic opinion recognition and extraction.
</prevsent>
<prevsent>subjective language is used to express opinions, emotions, and sentiments.
</prevsent>
</prevsection>
<citsent citstr=" H05-1044 ">
so far, research in automatic opinion recognition has primarily addressed learning subjective language (wiebe et al, 2004; <papid> J04-3002 </papid>riloff et al, 2003), <papid> W03-0404 </papid>identifying opinionated documents (yu and hatzivassiloglou, 2003) <papid> W03-1017 </papid>and sentences (yu and hatzivassiloglou, 2003; <papid> W03-1017 </papid>riloff et al, 2003), <papid> W03-0404 </papid>and discriminating between positive and negative language (pang et al, 2002; <papid> W02-1011 </papid>morinaga et al, 2002; yu and hatzivassiloglou, 2003; <papid> W03-1017 </papid>turney and littman, 2003; dave et al, 2003; nasukawa and yi, 2003; popescu and etzioni, 2005; <papid> H05-1043 </papid>wilson et al, 2005).<papid> H05-1044 </papid></citsent>
<aftsection>
<nextsent>while by its very nature we expect much of the language that is used when presenting perspective or point-of-view to be subjective, labeling document or sentence as subjective is not enough to identify the perspective from which it is written.
</nextsent>
<nextsent>moreover, the ideology and beliefs authors possess are often expressed in ways other than positive or negative language toward specific targets.
</nextsent>
<nextsent>research on the automatic classification of movie or product reviews as positive or negative (e.g., (pang et al, 2002; <papid> W02-1011 </papid>morinaga et al, 2002; turney and littman, 2003; nasukawa and yi, 2003; mullen and collier, 2004; <papid> W04-3253 </papid>beineke et al, 2004; <papid> P04-1034 </papid>hu and liu, 2004)) is perhaps the most similar to our work.</nextsent>
<nextsent>aswith review classification, we treat perspective identification as document-level classification task, discriminating, in sense, between different types ofopinions.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1658">
<title id=" W06-2915.xml">which side are you on identifying perspectives at the document and sentence levels </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>while by its very nature we expect much of the language that is used when presenting perspective or point-of-view to be subjective, labeling document or sentence as subjective is not enough to identify the perspective from which it is written.
</prevsent>
<prevsent>moreover, the ideology and beliefs authors possess are often expressed in ways other than positive or negative language toward specific targets.
</prevsent>
</prevsection>
<citsent citstr=" W04-3253 ">
research on the automatic classification of movie or product reviews as positive or negative (e.g., (pang et al, 2002; <papid> W02-1011 </papid>morinaga et al, 2002; turney and littman, 2003; nasukawa and yi, 2003; mullen and collier, 2004; <papid> W04-3253 </papid>beineke et al, 2004; <papid> P04-1034 </papid>hu and liu, 2004)) is perhaps the most similar to our work.</citsent>
<aftsection>
<nextsent>aswith review classification, we treat perspective identification as document-level classification task, discriminating, in sense, between different types ofopinions.
</nextsent>
<nextsent>however, there is key difference.
</nextsent>
<nextsent>a positive or negative opinion toward particular movie or product is fundamentally different from an overall perspective.
</nextsent>
<nextsent>ones opinion will change from movie to movie, whereas ones perspective can be seen as more static, often underpinned by ones ideology or beliefs about the world.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1659">
<title id=" W06-2915.xml">which side are you on identifying perspectives at the document and sentence levels </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>while by its very nature we expect much of the language that is used when presenting perspective or point-of-view to be subjective, labeling document or sentence as subjective is not enough to identify the perspective from which it is written.
</prevsent>
<prevsent>moreover, the ideology and beliefs authors possess are often expressed in ways other than positive or negative language toward specific targets.
</prevsent>
</prevsection>
<citsent citstr=" P04-1034 ">
research on the automatic classification of movie or product reviews as positive or negative (e.g., (pang et al, 2002; <papid> W02-1011 </papid>morinaga et al, 2002; turney and littman, 2003; nasukawa and yi, 2003; mullen and collier, 2004; <papid> W04-3253 </papid>beineke et al, 2004; <papid> P04-1034 </papid>hu and liu, 2004)) is perhaps the most similar to our work.</citsent>
<aftsection>
<nextsent>aswith review classification, we treat perspective identification as document-level classification task, discriminating, in sense, between different types ofopinions.
</nextsent>
<nextsent>however, there is key difference.
</nextsent>
<nextsent>a positive or negative opinion toward particular movie or product is fundamentally different from an overall perspective.
</nextsent>
<nextsent>ones opinion will change from movie to movie, whereas ones perspective can be seen as more static, often underpinned by ones ideology or beliefs about the world.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1660">
<title id=" W06-2915.xml">which side are you on identifying perspectives at the document and sentence levels </title>
<section> corpus.  </section>
<citcontext>
<prevsection>
<prevsent>the distribution of documents and sentences are listed intable 1.
</prevsent>
<prevsent>we removed meta data from all articles, in palestinian israeli written by editors 148 149 written by guests 149 148 total number of documents 297 297 average document length 740.4 816.1 number of sentences 8963 9640 table 1: the basic statistics of the corpus cluding edition numbers, publication dates, topics, titles, author names and biographic information.
</prevsent>
</prevsection>
<citsent citstr=" W03-1014 ">
weused opennlp tools4 to automatically extract sentence boundaries, and reduced word variants using the porter stemming algorithm.we evaluated the subjectivity of each sentence using the automatic subjective sentence classifier from (riloff and wiebe, 2003), <papid> W03-1014 </papid>and find that 65.6% of palestinian sentences and 66.2% of israeli sentences are classified as subjective.</citsent>
<aftsection>
<nextsent>the high but almost equivalent percentages of subjective sentences in thetwo perspectives support our observation in section 2 that perspective is largely expressed using subjective language, but that the amount of subjectivity in document is not necessarily indicative of 4http://sourceforge.net/projects/ opennlp/ its perspective.
</nextsent>
<nextsent>we develop algorithms for learning perspectives using statistical framework.
</nextsent>
<nextsent>denote training corp usas set of documents wn and their perspectives labels dn, = 1, . . .
</nextsent>
<nextsent>,n , where is the total number of documents in the corpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1661">
<title id=" W07-0208.xml">learning to transform linguistic graphs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we argue in favor of the the use of labeled directed graph to represent various types of linguistic structures, and illustrate how this allows one to view nlp tasks asgraph transformations.
</prevsent>
<prevsent>we present general method for learning such transformations from an annotated corpus and describe experiments with two applications of the method: identification of non-local depenencies (using penn treebank data)and semantic role labeling (using proposition bank data).
</prevsent>
</prevsection>
<citsent citstr=" J05-1004 ">
availability of linguistically annotated corpora such as the penn treebank (bies et al, 1995), proposition bank (palmer et al, 2005), <papid> J05-1004 </papid>and framenet (john sonet al, 2003) has stimulated much research on methods for automatic syntactic and semantic analysis of text.</citsent>
<aftsection>
<nextsent>rich annotations of corpora has allowed for the development of techniques for recovering deep linguistic structures: syntactic non-local dependencies (johnson, 2002; <papid> P02-1018 </papid>hockenmaier, 2003;<papid> P03-1046 </papid>dienes, 2004; jijkoun and de rijke, 2004) <papid> P04-1040 </papid>and semantic arguments (gildea, 2001; pradhan et al, 2005; <papid> P05-1072 </papid>toutanova et al, 2005; <papid> P05-1073 </papid>giuglea and moschitti, 2006).<papid> P06-1117 </papid></nextsent>
<nextsent>most state-of-the-art methods for the latter two tasks use cascaded architecture: they employ syntactic parsers and re-cast the corresponding tasksas pattern matching (johnson, 2002) <papid> P02-1018 </papid>or classification (pradhan et al, 2005) <papid> P05-1072 </papid>problems.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1663">
<title id=" W07-0208.xml">learning to transform linguistic graphs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we present general method for learning such transformations from an annotated corpus and describe experiments with two applications of the method: identification of non-local depenencies (using penn treebank data)and semantic role labeling (using proposition bank data).
</prevsent>
<prevsent>availability of linguistically annotated corpora such as the penn treebank (bies et al, 1995), proposition bank (palmer et al, 2005), <papid> J05-1004 </papid>and framenet (john sonet al, 2003) has stimulated much research on methods for automatic syntactic and semantic analysis of text.</prevsent>
</prevsection>
<citsent citstr=" P02-1018 ">
rich annotations of corpora has allowed for the development of techniques for recovering deep linguistic structures: syntactic non-local dependencies (johnson, 2002; <papid> P02-1018 </papid>hockenmaier, 2003;<papid> P03-1046 </papid>dienes, 2004; jijkoun and de rijke, 2004) <papid> P04-1040 </papid>and semantic arguments (gildea, 2001; pradhan et al, 2005; <papid> P05-1072 </papid>toutanova et al, 2005; <papid> P05-1073 </papid>giuglea and moschitti, 2006).<papid> P06-1117 </papid></citsent>
<aftsection>
<nextsent>most state-of-the-art methods for the latter two tasks use cascaded architecture: they employ syntactic parsers and re-cast the corresponding tasksas pattern matching (johnson, 2002) <papid> P02-1018 </papid>or classification (pradhan et al, 2005) <papid> P05-1072 </papid>problems.</nextsent>
<nextsent>other methods (jijkoun and de rijke, 2004) <papid> P04-1040 </papid>use combinations of pattern matching and classification.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1664">
<title id=" W07-0208.xml">learning to transform linguistic graphs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we present general method for learning such transformations from an annotated corpus and describe experiments with two applications of the method: identification of non-local depenencies (using penn treebank data)and semantic role labeling (using proposition bank data).
</prevsent>
<prevsent>availability of linguistically annotated corpora such as the penn treebank (bies et al, 1995), proposition bank (palmer et al, 2005), <papid> J05-1004 </papid>and framenet (john sonet al, 2003) has stimulated much research on methods for automatic syntactic and semantic analysis of text.</prevsent>
</prevsection>
<citsent citstr=" P03-1046 ">
rich annotations of corpora has allowed for the development of techniques for recovering deep linguistic structures: syntactic non-local dependencies (johnson, 2002; <papid> P02-1018 </papid>hockenmaier, 2003;<papid> P03-1046 </papid>dienes, 2004; jijkoun and de rijke, 2004) <papid> P04-1040 </papid>and semantic arguments (gildea, 2001; pradhan et al, 2005; <papid> P05-1072 </papid>toutanova et al, 2005; <papid> P05-1073 </papid>giuglea and moschitti, 2006).<papid> P06-1117 </papid></citsent>
<aftsection>
<nextsent>most state-of-the-art methods for the latter two tasks use cascaded architecture: they employ syntactic parsers and re-cast the corresponding tasksas pattern matching (johnson, 2002) <papid> P02-1018 </papid>or classification (pradhan et al, 2005) <papid> P05-1072 </papid>problems.</nextsent>
<nextsent>other methods (jijkoun and de rijke, 2004) <papid> P04-1040 </papid>use combinations of pattern matching and classification.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1665">
<title id=" W07-0208.xml">learning to transform linguistic graphs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we present general method for learning such transformations from an annotated corpus and describe experiments with two applications of the method: identification of non-local depenencies (using penn treebank data)and semantic role labeling (using proposition bank data).
</prevsent>
<prevsent>availability of linguistically annotated corpora such as the penn treebank (bies et al, 1995), proposition bank (palmer et al, 2005), <papid> J05-1004 </papid>and framenet (john sonet al, 2003) has stimulated much research on methods for automatic syntactic and semantic analysis of text.</prevsent>
</prevsection>
<citsent citstr=" P04-1040 ">
rich annotations of corpora has allowed for the development of techniques for recovering deep linguistic structures: syntactic non-local dependencies (johnson, 2002; <papid> P02-1018 </papid>hockenmaier, 2003;<papid> P03-1046 </papid>dienes, 2004; jijkoun and de rijke, 2004) <papid> P04-1040 </papid>and semantic arguments (gildea, 2001; pradhan et al, 2005; <papid> P05-1072 </papid>toutanova et al, 2005; <papid> P05-1073 </papid>giuglea and moschitti, 2006).<papid> P06-1117 </papid></citsent>
<aftsection>
<nextsent>most state-of-the-art methods for the latter two tasks use cascaded architecture: they employ syntactic parsers and re-cast the corresponding tasksas pattern matching (johnson, 2002) <papid> P02-1018 </papid>or classification (pradhan et al, 2005) <papid> P05-1072 </papid>problems.</nextsent>
<nextsent>other methods (jijkoun and de rijke, 2004) <papid> P04-1040 </papid>use combinations of pattern matching and classification.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1668">
<title id=" W07-0208.xml">learning to transform linguistic graphs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we present general method for learning such transformations from an annotated corpus and describe experiments with two applications of the method: identification of non-local depenencies (using penn treebank data)and semantic role labeling (using proposition bank data).
</prevsent>
<prevsent>availability of linguistically annotated corpora such as the penn treebank (bies et al, 1995), proposition bank (palmer et al, 2005), <papid> J05-1004 </papid>and framenet (john sonet al, 2003) has stimulated much research on methods for automatic syntactic and semantic analysis of text.</prevsent>
</prevsection>
<citsent citstr=" P05-1072 ">
rich annotations of corpora has allowed for the development of techniques for recovering deep linguistic structures: syntactic non-local dependencies (johnson, 2002; <papid> P02-1018 </papid>hockenmaier, 2003;<papid> P03-1046 </papid>dienes, 2004; jijkoun and de rijke, 2004) <papid> P04-1040 </papid>and semantic arguments (gildea, 2001; pradhan et al, 2005; <papid> P05-1072 </papid>toutanova et al, 2005; <papid> P05-1073 </papid>giuglea and moschitti, 2006).<papid> P06-1117 </papid></citsent>
<aftsection>
<nextsent>most state-of-the-art methods for the latter two tasks use cascaded architecture: they employ syntactic parsers and re-cast the corresponding tasksas pattern matching (johnson, 2002) <papid> P02-1018 </papid>or classification (pradhan et al, 2005) <papid> P05-1072 </papid>problems.</nextsent>
<nextsent>other methods (jijkoun and de rijke, 2004) <papid> P04-1040 </papid>use combinations of pattern matching and classification.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1669">
<title id=" W07-0208.xml">learning to transform linguistic graphs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we present general method for learning such transformations from an annotated corpus and describe experiments with two applications of the method: identification of non-local depenencies (using penn treebank data)and semantic role labeling (using proposition bank data).
</prevsent>
<prevsent>availability of linguistically annotated corpora such as the penn treebank (bies et al, 1995), proposition bank (palmer et al, 2005), <papid> J05-1004 </papid>and framenet (john sonet al, 2003) has stimulated much research on methods for automatic syntactic and semantic analysis of text.</prevsent>
</prevsection>
<citsent citstr=" P05-1073 ">
rich annotations of corpora has allowed for the development of techniques for recovering deep linguistic structures: syntactic non-local dependencies (johnson, 2002; <papid> P02-1018 </papid>hockenmaier, 2003;<papid> P03-1046 </papid>dienes, 2004; jijkoun and de rijke, 2004) <papid> P04-1040 </papid>and semantic arguments (gildea, 2001; pradhan et al, 2005; <papid> P05-1072 </papid>toutanova et al, 2005; <papid> P05-1073 </papid>giuglea and moschitti, 2006).<papid> P06-1117 </papid></citsent>
<aftsection>
<nextsent>most state-of-the-art methods for the latter two tasks use cascaded architecture: they employ syntactic parsers and re-cast the corresponding tasksas pattern matching (johnson, 2002) <papid> P02-1018 </papid>or classification (pradhan et al, 2005) <papid> P05-1072 </papid>problems.</nextsent>
<nextsent>other methods (jijkoun and de rijke, 2004) <papid> P04-1040 </papid>use combinations of pattern matching and classification.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1670">
<title id=" W07-0208.xml">learning to transform linguistic graphs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we present general method for learning such transformations from an annotated corpus and describe experiments with two applications of the method: identification of non-local depenencies (using penn treebank data)and semantic role labeling (using proposition bank data).
</prevsent>
<prevsent>availability of linguistically annotated corpora such as the penn treebank (bies et al, 1995), proposition bank (palmer et al, 2005), <papid> J05-1004 </papid>and framenet (john sonet al, 2003) has stimulated much research on methods for automatic syntactic and semantic analysis of text.</prevsent>
</prevsection>
<citsent citstr=" P06-1117 ">
rich annotations of corpora has allowed for the development of techniques for recovering deep linguistic structures: syntactic non-local dependencies (johnson, 2002; <papid> P02-1018 </papid>hockenmaier, 2003;<papid> P03-1046 </papid>dienes, 2004; jijkoun and de rijke, 2004) <papid> P04-1040 </papid>and semantic arguments (gildea, 2001; pradhan et al, 2005; <papid> P05-1072 </papid>toutanova et al, 2005; <papid> P05-1073 </papid>giuglea and moschitti, 2006).<papid> P06-1117 </papid></citsent>
<aftsection>
<nextsent>most state-of-the-art methods for the latter two tasks use cascaded architecture: they employ syntactic parsers and re-cast the corresponding tasksas pattern matching (johnson, 2002) <papid> P02-1018 </papid>or classification (pradhan et al, 2005) <papid> P05-1072 </papid>problems.</nextsent>
<nextsent>other methods (jijkoun and de rijke, 2004) <papid> P04-1040 </papid>use combinations of pattern matching and classification.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1678">
<title id=" W07-0208.xml">learning to transform linguistic graphs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>an example of such transformation is adding level ofthe predicate argument structure or semantic arguments to syntactically annotated sentences.
</prevsent>
<prevsent>furthermore, we describe general method to automatically learn such transformations from annotated corpora.
</prevsent>
</prevsection>
<citsent citstr=" J95-4004 ">
our method combines pattern matching on graphs and machine learning (classification) and canbe viewed as an extension of the transformation based learning paradigm (brill, 1995).<papid> J95-4004 </papid></citsent>
<aftsection>
<nextsent>after describing the method for learning graph transformations we demonstrate its applicability on two tasks: identification of non-local dependencies (using penn treebank data) and semantic roles labeling (using proposition bank data).
</nextsent>
<nextsent>the paper is organized as follows.
</nextsent>
<nextsent>in section 2 we give our motivations for using graphs to encode linguistic data.
</nextsent>
<nextsent>in section 3 we describe our method for learning graph transformations and in section 4 we report on experiments with applications of our method.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1679">
<title id=" W07-0208.xml">learning to transform linguistic graphs </title>
<section> graphs for linguistic structures and.  </section>
<citcontext>
<prevsection>
<prevsent>in this example, the co-indexing-based implicit annotation of the non-local dependency (sub ject control) in the penn treebank (bies et al, 1995) is made explicit in the graph-based encoding.
</prevsent>
<prevsent>figure 2 shows graph encoding of linguistic structures for the sentence lorillard inc stopped using crocodolite in sigarette filters in 1956.
</prevsent>
</prevsection>
<citsent citstr=" A00-2018 ">
here, solid lines correspond to surface syntactic structure, produced by charniaks parser (charniak, 2000), <papid> A00-2018 </papid>and dashed lines are an encoding of the proposition bank annotation of the semantic roles with respect to the verb stopped.</citsent>
<aftsection>
<nextsent>graph-based representations allow for uniform view on the linguistic structures on different layers.an advantage of such uniform view is that apparently different nlp tasks can be considered as vp to seek np seats vp planned directors this month np np figure 3: output of syntactic parser.
</nextsent>
<nextsent>manipulations with graphs, in other words, as graph transformation problems.consider the task of recovering non-local dependencies (such as control, wh-extraction, topicaliza tion) in the surface syntactic phrase trees produced by the state-of-the-art parser of (charniak, 2000).<papid> A00-2018 </papid></nextsent>
<nextsent>figure 3 shows graph-based encoding of the output of the parser, and the task in question would consist in transforming the graph in figure 3 into the graph in figure 1.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1682">
<title id=" W07-0208.xml">learning to transform linguistic graphs </title>
<section> applications.  </section>
<citcontext>
<prevsection>
<prevsent>parsers trained on the penn treebank, produce syntactic parse trees with bare phrase labels, (np, pp, s, see figure 3),i.e., providing surface grammatical analysis of sentences, even though the training corpus, the penn treebank, is richer and contains additional grammatical and semantic information: it distinguishes various types of modifiers, complements, subjects, objects and annotates non-local dependencies, i.e., relations between phrases not adjacent in the parse tree (see figure 1).
</prevsent>
<prevsent>the task of recovering this information in the parsers output has received good deal of attention.
</prevsent>
</prevsection>
<citsent citstr=" P04-1082 ">
(campbell, 2004) <papid> P04-1082 </papid>presents rule based algorithm for empty node identification in syntactic trees, competitive with the machine learning methods we mention next.</citsent>
<aftsection>
<nextsent>in (johnson, 2002) <papid> P02-1018 </papid>simple pattern-matching algorithm was proposed for inserting empty nodes into syntactic trees, with patterns extracted from the penn treebank.</nextsent>
<nextsent>(dienes,2004) used pre processor that identified surface location of empty nodes and syntactic parser incorporating non-local dependencies into its probabilistic model.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1694">
<title id=" W06-2936.xml">maximum spanning tree algorithm for non projective labeled dependency parsing </title>
<section> abstract </section>
<citcontext>
<prevsection>


</prevsection>
<citsent citstr=" H05-1066 ">
following (mcdonald et al, 2005), <papid> H05-1066 </papid>we present an application of maximum spanning tree algorithm for directed graph to non-projective labeled dependency parsing.</citsent>
<aftsection>
<nextsent>using variant of the voted perceptron (collins, 2002; <papid> W02-1001 </papid>collins and roark, 2004; <papid> P04-1015 </papid>crammer and singer, 2003), we discriminatively trained our parser in an on-line fashion.</nextsent>
<nextsent>after just one epoch of training, we were generally able to attain average results in the conll 2006 shared task.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1698">
<title id=" W06-2936.xml">maximum spanning tree algorithm for non projective labeled dependency parsing </title>
<section> abstract </section>
<citcontext>
<prevsection>

<prevsent>following (mcdonald et al, 2005), <papid> H05-1066 </papid>we present an application of maximum spanning tree algorithm for directed graph to non-projective labeled dependency parsing.</prevsent>
</prevsection>
<citsent citstr=" W02-1001 ">
using variant of the voted perceptron (collins, 2002; <papid> W02-1001 </papid>collins and roark, 2004; <papid> P04-1015 </papid>crammer and singer, 2003), we discriminatively trained our parser in an on-line fashion.</citsent>
<aftsection>
<nextsent>after just one epoch of training, we were generally able to attain average results in the conll 2006 shared task.
</nextsent>
<nextsent>recently, we have seen dependency parsing grow more popular.
</nextsent>
<nextsent>it is not rare to see dependency relations used as features, in tasks such as relation extraction (bunescu and mooney, 2005) <papid> H05-1091 </papid>and machine translation (ding and palmer, 2005).<papid> P05-1067 </papid></nextsent>
<nextsent>although english dependency relations are mostly projective, in other languages with more flexible word order, suchas czech, non-projective dependencies are more frequent.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1700">
<title id=" W06-2936.xml">maximum spanning tree algorithm for non projective labeled dependency parsing </title>
<section> abstract </section>
<citcontext>
<prevsection>

<prevsent>following (mcdonald et al, 2005), <papid> H05-1066 </papid>we present an application of maximum spanning tree algorithm for directed graph to non-projective labeled dependency parsing.</prevsent>
</prevsection>
<citsent citstr=" P04-1015 ">
using variant of the voted perceptron (collins, 2002; <papid> W02-1001 </papid>collins and roark, 2004; <papid> P04-1015 </papid>crammer and singer, 2003), we discriminatively trained our parser in an on-line fashion.</citsent>
<aftsection>
<nextsent>after just one epoch of training, we were generally able to attain average results in the conll 2006 shared task.
</nextsent>
<nextsent>recently, we have seen dependency parsing grow more popular.
</nextsent>
<nextsent>it is not rare to see dependency relations used as features, in tasks such as relation extraction (bunescu and mooney, 2005) <papid> H05-1091 </papid>and machine translation (ding and palmer, 2005).<papid> P05-1067 </papid></nextsent>
<nextsent>although english dependency relations are mostly projective, in other languages with more flexible word order, suchas czech, non-projective dependencies are more frequent.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1701">
<title id=" W06-2936.xml">maximum spanning tree algorithm for non projective labeled dependency parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>after just one epoch of training, we were generally able to attain average results in the conll 2006 shared task.
</prevsent>
<prevsent>recently, we have seen dependency parsing grow more popular.
</prevsent>
</prevsection>
<citsent citstr=" H05-1091 ">
it is not rare to see dependency relations used as features, in tasks such as relation extraction (bunescu and mooney, 2005) <papid> H05-1091 </papid>and machine translation (ding and palmer, 2005).<papid> P05-1067 </papid></citsent>
<aftsection>
<nextsent>although english dependency relations are mostly projective, in other languages with more flexible word order, suchas czech, non-projective dependencies are more frequent.
</nextsent>
<nextsent>there are generally two methods for learning non-projective dependencies.
</nextsent>
<nextsent>you could map non-projective dependency tree to projective one, learn and predict the tree, then bring it back to the non-projective dependency tree (nivre and nilsson, 2005).<papid> P05-1013 </papid></nextsent>
<nextsent>non-projective dependency parsing can also be represented as search for maximum spanning tree in directed graph, and this technique has been shown to perform well in czech (mcdonald et al,2005).<papid> H05-1066 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1702">
<title id=" W06-2936.xml">maximum spanning tree algorithm for non projective labeled dependency parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>after just one epoch of training, we were generally able to attain average results in the conll 2006 shared task.
</prevsent>
<prevsent>recently, we have seen dependency parsing grow more popular.
</prevsent>
</prevsection>
<citsent citstr=" P05-1067 ">
it is not rare to see dependency relations used as features, in tasks such as relation extraction (bunescu and mooney, 2005) <papid> H05-1091 </papid>and machine translation (ding and palmer, 2005).<papid> P05-1067 </papid></citsent>
<aftsection>
<nextsent>although english dependency relations are mostly projective, in other languages with more flexible word order, suchas czech, non-projective dependencies are more frequent.
</nextsent>
<nextsent>there are generally two methods for learning non-projective dependencies.
</nextsent>
<nextsent>you could map non-projective dependency tree to projective one, learn and predict the tree, then bring it back to the non-projective dependency tree (nivre and nilsson, 2005).<papid> P05-1013 </papid></nextsent>
<nextsent>non-projective dependency parsing can also be represented as search for maximum spanning tree in directed graph, and this technique has been shown to perform well in czech (mcdonald et al,2005).<papid> H05-1066 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1703">
<title id=" W06-2936.xml">maximum spanning tree algorithm for non projective labeled dependency parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>although english dependency relations are mostly projective, in other languages with more flexible word order, suchas czech, non-projective dependencies are more frequent.
</prevsent>
<prevsent>there are generally two methods for learning non-projective dependencies.
</prevsent>
</prevsection>
<citsent citstr=" P05-1013 ">
you could map non-projective dependency tree to projective one, learn and predict the tree, then bring it back to the non-projective dependency tree (nivre and nilsson, 2005).<papid> P05-1013 </papid></citsent>
<aftsection>
<nextsent>non-projective dependency parsing can also be represented as search for maximum spanning tree in directed graph, and this technique has been shown to perform well in czech (mcdonald et al,2005).<papid> H05-1066 </papid></nextsent>
<nextsent>in this paper, we investigate the effectiveness of (mcdonald et al, 2005) <papid> H05-1066 </papid>in the various languages given by the conll 2006 shared task for non-projective labeled dependency parsing.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1711">
<title id=" W06-2936.xml">maximum spanning tree algorithm for non projective labeled dependency parsing </title>
<section> non-projective dependency parsing.  </section>
<citcontext>
<prevsection>
<prevsent>the paper is structured as follows: in section 2 and 3, we review the decoding and learning aspects of (mcdonald et al, 2005), <papid> H05-1066 </papid>and in section 4, we describe the extension of the algorithm and the features needed for the conll 2006 shared task.</prevsent>
<prevsent>2.1 dependency structure.</prevsent>
</prevsection>
<citsent citstr=" W04-3201 ">
let us define to be generic sequence of input tokens together with their pos tags and other morphological features, and to be generic dependency structure, that is, set of edges for x. we use the terminology in (taskar et al, 2004) <papid> W04-3201 </papid>for generic structured output prediction, and define part.</citsent>
<aftsection>
<nextsent>a part represents an edge together with its label.
</nextsent>
<nextsent>a part is tuple deprel, i, j?
</nextsent>
<nextsent>where is the start point of the edge, is the end point, and deprel is the label of the edge.
</nextsent>
<nextsent>the token at is the head of the token at j.table 1 shows our formulation of building non projective dependency tree as prediction problem.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1726">
<title id=" W06-1653.xml">relevance feedback models for recommendation </title>
<section> relevance feedback models.  </section>
<citcontext>
<prevsection>
<prevsent>clearly, p2   p1.5; therefore, the polya model assigns higher probability.
</prevsent>
<prevsent>in this example, the polya model assigns probability to the first occurrence and p0.5(  p) to the second.
</prevsent>
</prevsection>
<citsent citstr=" C00-1027 ">
since words that occur once are likely to occur again (church, 2000),<papid> C00-1027 </papid>the polya model is better suited to model the occurrences of words and users.</citsent>
<aftsection>
<nextsent>see yamamoto and sadamitsu (2005) for further discussion on applying the polya distribution to text modeling.zaragoza et al(2003) applied the polya distribution to ad hoc ir.
</nextsent>
<nextsent>they introduced the exact polya distribution (see eq.
</nextsent>
<nextsent>9) as an extension to the dirichlet prior method (zhai and lafferty,2001).
</nextsent>
<nextsent>however, we have introduced multino mial approximation of the polya distribution.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1727">
<title id=" W06-1653.xml">relevance feedback models for recommendation </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>wx was the sequence of words that were typical in x. to make wx, we removed stop words and stemmed the remaining words with porter stemmer.
</prevsent>
<prevsent>next, we identified 100 typical words in each article and extracted only those words (|wx| ? 100 because some of them occurred multiple times).
</prevsent>
</prevsection>
<citsent citstr=" J93-1003 ">
typicality was measured using the log-likelihood ratio test (dunning, 1993).<papid> J93-1003 </papid></citsent>
<aftsection>
<nextsent>we needed to reduce the number of words to speed up our recommender system.
</nextsent>
<nextsent>to make our dataset, we first extracted 302,606 articles, which had more than 100 tokens after thestop words were removed.
</nextsent>
<nextsent>we then selected typical words in each article.
</nextsent>
<nextsent>the implicit rating data were obtained from the histories of users editing these articles.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1728">
<title id=" W07-0210.xml">correlations in the organization of largescale syntactic dependency networks </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>during the last decade, the study of the statistical properties of networks as different as technical,biological and social networks has grown tremendously.
</prevsent>
<prevsent>see (barabasi and albert, 2002; doro govt sev and mendes, 2002; newman, 2003) for review.
</prevsent>
</prevsection>
<citsent citstr=" W06-2801 ">
among them many kinds of linguistic networks have been studied: e.g., free word association networks (steyvers and tenenbaum, 2005), syllable networks (soares et al, 2005), thesaurus networks (sigman 65 and cecchi, 2002), and document networks (mehler,2006).<papid> W06-2801 </papid></citsent>
<aftsection>
<nextsent>see (mehler, 2007a) for review of linguistic network studies.
</nextsent>
<nextsent>here we focus on the so called global syntactic dependency networks (gsdn) (fer rer cancho et al, 2004; ferrer cancho, 2005).
</nextsent>
<nextsent>a gsdn is induced from dependency treebank in two steps: 1.
</nextsent>
<nextsent>the vertices of the network are obtained from.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1731">
<title id=" W07-0201.xml">analysis of the wikipedia category graph for nlp applications </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>article links are established because of any kind of relation between 3wikipedia can be downloaded from http: //download.wikimedia.org/ articles, while links between categories are typically established because of hyponymy or meronymy re lations.holloway et al (2005) create and visualize category map based on co-occurrence of categories.
</prevsent>
<prevsent>voss (2006) pointed out that the wcg is kind of thesaurus that combines collaborative tagging and hierarchical indexing.
</prevsent>
</prevsection>
<citsent citstr=" N07-2052 ">
zesch et al (2007<papid> N07-2052 </papid>a) identified the wcg as value able source of lexical semantic knowledge, but did not analytically analyze its prop erties.</citsent>
<aftsection>
<nextsent>however, even if the wcg seems to be very similar to other semantic wordnets, graph-theoretic analysis of the wcg is necessary to substantiate this claim.
</nextsent>
<nextsent>it is carried out in the next section.
</nextsent>
<nextsent>a graph-theoretic analysis of the wcg is required to estimate, whether graph based semantic relatedness measures developed for semantic wordnets can be transferred to the wcg.
</nextsent>
<nextsent>this is substantiated in case study on computing semantic relatedness in section 4.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1733">
<title id=" W07-0201.xml">analysis of the wikipedia category graph for nlp applications </title>
<section> graph based semantic relatedness.  </section>
<citcontext>
<prevsection>
<prevsent>distpl = l(n1, n2) leacock and chodorow (1998, lc) normalize the path-length with the depth of the graph, simlc(n1, n2) = ? log l(n1, n2) 2?
</prevsent>
<prevsent>depth where depth is the length of the longest path in the graph.
</prevsent>
</prevsection>
<citsent citstr=" P94-1019 ">
wu and palmer (1994, <papid> P94-1019 </papid>wp) introduce measure that uses the notion of lowest common subsumer of two nodes lcs(n1, n2).</citsent>
<aftsection>
<nextsent>in directed graph, lcs is the parent of both child nodes with the largest depth in the graph.
</nextsent>
<nextsent>simwp = 2 depth(lcs) l(n1, lcs) + l(n2, lcs) + 2 depth(lcs)resnik (1995, res), defines semantic similarity between two nodes as the information content (ic)value of their lcs.
</nextsent>
<nextsent>he used the relative corpus frequency to estimate the information content value.
</nextsent>
<nextsent>jiang and conrath (1997, jc) additionally use the ic of the nodes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1735">
<title id=" W07-0201.xml">analysis of the wikipedia category graph for nlp applications </title>
<section> semantic relatedness experiments.  </section>
<citcontext>
<prevsection>
<prevsent>5 correlation dataset year language # pairs pos type scores # subjects inter intra rg65 1965 english 65 ss continuous 04 51 - .850mc30 1991 english 30 ss continuous 04 38 - res30 1995 english 30 ss continuous 04 10 .903 fin353 2002 english 353 n, v, sr continuous 010 13/16 - 153 13 .731 200 16 .549 gur65 2005 german 65 ss discrete {0,1,2,3,4} 24 .810 gur350 2006 german 350 n, v, sr discrete {0,1,2,3,4} 8 .690 zg222 2006 german 222 n, v, sr discrete {0,1,2,3,4} 21 .490 .647 table 2: comparison of german datasets used for evaluating semantic relatedness.by either synonymy or hyponymy.
</prevsent>
<prevsent>thus, she created larger german dataset containing 350 word pairs (gur350).
</prevsent>
</prevsection>
<citsent citstr=" W04-2607 ">
it contains nouns, verbs and adjectives that are connected by classical and non classical relations (morris and hirst, 2004).<papid> W04-2607 </papid></citsent>
<aftsection>
<nextsent>however, word pairs for this dataset are biased towards strong classical relations, as they were manually selected.
</nextsent>
<nextsent>thus, zesch and gurevych (2006) <papid> W06-1104 </papid>used semi-automatic process to create word pairs from domain-specific corpora.</nextsent>
<nextsent>the resulting zg222 dataset contains 222 word pairs that are connected by all kinds of lexical semantic relations.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1736">
<title id=" W07-0201.xml">analysis of the wikipedia category graph for nlp applications </title>
<section> semantic relatedness experiments.  </section>
<citcontext>
<prevsection>
<prevsent>it contains nouns, verbs and adjectives that are connected by classical and non classical relations (morris and hirst, 2004).<papid> W04-2607 </papid></prevsent>
<prevsent>however, word pairs for this dataset are biased towards strong classical relations, as they were manually selected.</prevsent>
</prevsection>
<citsent citstr=" W06-1104 ">
thus, zesch and gurevych (2006) <papid> W06-1104 </papid>used semi-automatic process to create word pairs from domain-specific corpora.</citsent>
<aftsection>
<nextsent>the resulting zg222 dataset contains 222 word pairs that are connected by all kinds of lexical semantic relations.
</nextsent>
<nextsent>hence, it is particularly suited for analyzing the capability of measure to estimate sr. 4.2 results and discussion.
</nextsent>
<nextsent>figure 4 gives an overview of our experimental results of evaluating sr measures based on the wcgon three german datasets.
</nextsent>
<nextsent>we use pearsons product moment correlation to compare the results with human judgments.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1739">
<title id=" W06-1640.xml">partially supervised coreference resolution for opinion summarization through structured rule learning </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>sentiment analysis is concerned with extracting attitudes, opinions, evaluations, and sentiment from text.
</prevsent>
<prevsent>work in this area has been motivated by the desire to provide information analysis applications in the arenas of government, business,and politics (e.g. coglianese (2004)).
</prevsent>
</prevsection>
<citsent citstr=" H05-1116 ">
additionally, sentiment analysis can augment existing nlp applications such as question answering, information retrieval, summarization, and clustering by providing information about sentiment (e.g. stoyanov et al (2005), <papid> H05-1116 </papid>riloff et al (2005)).</citsent>
<aftsection>
<nextsent>to date, research in the area (see related work section)has focused on the problem of extracting sentiment both at the document level (coarse-grainedsentiment information), and at the level of sentences, clauses, or individual expressions (fine grained sentiment information).in contrast, our work concerns the summarization of fine-grained information about opinions.
</nextsent>
<nextsent>in particular, while recent research efforts have shown that fine-grained opinions (e.g. riloff and wiebe (2003), <papid> W03-1014 </papid>bethard et al (2004), wiebe and riloff (2005)) as well as their sources (e.g. bethard et al (2004), choi et al (2005),<papid> H05-1045 </papid>kim and hovy (2005)) can be extracted automatically, little has been done to create opinion summaries, where opinions from the samesource/target are combined, statistics are computed for each source/target and multiple opinions from the same source to the same target are ag gregated.</nextsent>
<nextsent>a simple opinion summary is shown in figure 1.1 we expect that this type of opinion summary, based on fine-grained opinion information,will be important for information analysis applications in any domain where the analysis of opinions is critical.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1740">
<title id=" W06-1640.xml">partially supervised coreference resolution for opinion summarization through structured rule learning </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>additionally, sentiment analysis can augment existing nlp applications such as question answering, information retrieval, summarization, and clustering by providing information about sentiment (e.g. stoyanov et al (2005), <papid> H05-1116 </papid>riloff et al (2005)).</prevsent>
<prevsent>to date, research in the area (see related work section)has focused on the problem of extracting sentiment both at the document level (coarse-grainedsentiment information), and at the level of sentences, clauses, or individual expressions (fine grained sentiment information).in contrast, our work concerns the summarization of fine-grained information about opinions.</prevsent>
</prevsection>
<citsent citstr=" W03-1014 ">
in particular, while recent research efforts have shown that fine-grained opinions (e.g. riloff and wiebe (2003), <papid> W03-1014 </papid>bethard et al (2004), wiebe and riloff (2005)) as well as their sources (e.g. bethard et al (2004), choi et al (2005),<papid> H05-1045 </papid>kim and hovy (2005)) can be extracted automatically, little has been done to create opinion summaries, where opinions from the samesource/target are combined, statistics are computed for each source/target and multiple opinions from the same source to the same target are ag gregated.</citsent>
<aftsection>
<nextsent>a simple opinion summary is shown in figure 1.1 we expect that this type of opinion summary, based on fine-grained opinion information,will be important for information analysis applications in any domain where the analysis of opinions is critical.
</nextsent>
<nextsent>this paper addresses the problem of opinion summarization by considering the creation of simple opinion summaries like those of figure 1.
</nextsent>
<nextsent>we propose source coreference resolution ? the task of determining which mentions of opinion sources refer to the same entity ? as the primary mechanism for identifying the set of opinions attributed to each real-world source.
</nextsent>
<nextsent>for this type of summary, source coreference resolution constitutes an integral step in the process of generating full opinion summaries.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1742">
<title id=" W06-1640.xml">partially supervised coreference resolution for opinion summarization through structured rule learning </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>additionally, sentiment analysis can augment existing nlp applications such as question answering, information retrieval, summarization, and clustering by providing information about sentiment (e.g. stoyanov et al (2005), <papid> H05-1116 </papid>riloff et al (2005)).</prevsent>
<prevsent>to date, research in the area (see related work section)has focused on the problem of extracting sentiment both at the document level (coarse-grainedsentiment information), and at the level of sentences, clauses, or individual expressions (fine grained sentiment information).in contrast, our work concerns the summarization of fine-grained information about opinions.</prevsent>
</prevsection>
<citsent citstr=" H05-1045 ">
in particular, while recent research efforts have shown that fine-grained opinions (e.g. riloff and wiebe (2003), <papid> W03-1014 </papid>bethard et al (2004), wiebe and riloff (2005)) as well as their sources (e.g. bethard et al (2004), choi et al (2005),<papid> H05-1045 </papid>kim and hovy (2005)) can be extracted automatically, little has been done to create opinion summaries, where opinions from the samesource/target are combined, statistics are computed for each source/target and multiple opinions from the same source to the same target are ag gregated.</citsent>
<aftsection>
<nextsent>a simple opinion summary is shown in figure 1.1 we expect that this type of opinion summary, based on fine-grained opinion information,will be important for information analysis applications in any domain where the analysis of opinions is critical.
</nextsent>
<nextsent>this paper addresses the problem of opinion summarization by considering the creation of simple opinion summaries like those of figure 1.
</nextsent>
<nextsent>we propose source coreference resolution ? the task of determining which mentions of opinion sources refer to the same entity ? as the primary mechanism for identifying the set of opinions attributed to each real-world source.
</nextsent>
<nextsent>for this type of summary, source coreference resolution constitutes an integral step in the process of generating full opinion summaries.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1743">
<title id=" W06-1640.xml">partially supervised coreference resolution for opinion summarization through structured rule learning </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>and zerkin?
</prevsent>
<prevsent>refer to the same person.2 1for simplicity, the example summary does not contain any source/target statistics.2in addition, the summary would require the closely related task of target coreference resolution and means for aggregating the conflicting opinions from zerkin toward mous saoui.
</prevsent>
</prevsection>
<citsent citstr=" P02-1014 ">
336 at first glance, source coreference resolution appears equivalent to the task of noun phrase coreference resolution and therefore amenable to traditional coreference resolution techniques (e.g.ng and cardie (2002), <papid> P02-1014 </papid>morton (2000)).<papid> P00-1023 </papid></citsent>
<aftsection>
<nextsent>we hypothesize in section 3, however, that the task is likely to succumb to better solution by treating it in the context of new machine learning setting that we refer to as partially supervised clustering.
</nextsent>
<nextsent>in particular, due to high coreference annotation costs, datasets that are annotated with opinion information (like ours) do not typically include supervisory coreference information for all noun phrases in document (as would be required for the application of traditional coreference resolution techniques), but only for noun phrases that act as opinion sources (or targets).as result, we define the task of partially supervised clustering, the goal of which is to learna clustering function from set of partially specified clustering examples (section 4).
</nextsent>
<nextsent>we are not aware of prior work on the problem of partially supervised clustering and argue that it differs substantially from that of semi-supervised clustering.
</nextsent>
<nextsent>we propose an algorithm for partially supervised clustering that extends rule learner with structure information and is generally applicable to problems that fit the partially supervised clustering definition (section 5).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1745">
<title id=" W06-1640.xml">partially supervised coreference resolution for opinion summarization through structured rule learning </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>and zerkin?
</prevsent>
<prevsent>refer to the same person.2 1for simplicity, the example summary does not contain any source/target statistics.2in addition, the summary would require the closely related task of target coreference resolution and means for aggregating the conflicting opinions from zerkin toward mous saoui.
</prevsent>
</prevsection>
<citsent citstr=" P00-1023 ">
336 at first glance, source coreference resolution appears equivalent to the task of noun phrase coreference resolution and therefore amenable to traditional coreference resolution techniques (e.g.ng and cardie (2002), <papid> P02-1014 </papid>morton (2000)).<papid> P00-1023 </papid></citsent>
<aftsection>
<nextsent>we hypothesize in section 3, however, that the task is likely to succumb to better solution by treating it in the context of new machine learning setting that we refer to as partially supervised clustering.
</nextsent>
<nextsent>in particular, due to high coreference annotation costs, datasets that are annotated with opinion information (like ours) do not typically include supervisory coreference information for all noun phrases in document (as would be required for the application of traditional coreference resolution techniques), but only for noun phrases that act as opinion sources (or targets).as result, we define the task of partially supervised clustering, the goal of which is to learna clustering function from set of partially specified clustering examples (section 4).
</nextsent>
<nextsent>we are not aware of prior work on the problem of partially supervised clustering and argue that it differs substantially from that of semi-supervised clustering.
</nextsent>
<nextsent>we propose an algorithm for partially supervised clustering that extends rule learner with structure information and is generally applicable to problems that fit the partially supervised clustering definition (section 5).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1746">
<title id=" W06-1640.xml">partially supervised coreference resolution for opinion summarization through structured rule learning </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the underlined phrase will be explained later in the paper.a document either positive (thumbs up?)
</prevsent>
<prevsent>or negative (thumbs down?)
</prevsent>
</prevsection>
<citsent citstr=" W02-1011 ">
polarity (e.g. das and chen (2001), pang et al (2002), <papid> W02-1011 </papid>turney (2002), <papid> P02-1053 </papid>dave et al (2003)).</citsent>
<aftsection>
<nextsent>other research has concentrated on analyzing fine-grained opinions at, or below,the sentence level.
</nextsent>
<nextsent>recent work, for example, indicates that systems can be trained to recognize opinions and their polarity, strength, and sources to reasonable degree of accuracy (e.g. dave et al.
</nextsent>
<nextsent>(2003), riloff and wiebe (2003), <papid> W03-1014 </papid>bethard etal.</nextsent>
<nextsent>(2004), wilson et al (2004), yu and hatzivassiloglou (2003), <papid> W03-1017 </papid>choi et al (2005),<papid> H05-1045 </papid> kim and hovy(2005), wiebe and riloff (2005)).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1747">
<title id=" W06-1640.xml">partially supervised coreference resolution for opinion summarization through structured rule learning </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the underlined phrase will be explained later in the paper.a document either positive (thumbs up?)
</prevsent>
<prevsent>or negative (thumbs down?)
</prevsent>
</prevsection>
<citsent citstr=" P02-1053 ">
polarity (e.g. das and chen (2001), pang et al (2002), <papid> W02-1011 </papid>turney (2002), <papid> P02-1053 </papid>dave et al (2003)).</citsent>
<aftsection>
<nextsent>other research has concentrated on analyzing fine-grained opinions at, or below,the sentence level.
</nextsent>
<nextsent>recent work, for example, indicates that systems can be trained to recognize opinions and their polarity, strength, and sources to reasonable degree of accuracy (e.g. dave et al.
</nextsent>
<nextsent>(2003), riloff and wiebe (2003), <papid> W03-1014 </papid>bethard etal.</nextsent>
<nextsent>(2004), wilson et al (2004), yu and hatzivassiloglou (2003), <papid> W03-1017 </papid>choi et al (2005),<papid> H05-1045 </papid> kim and hovy(2005), wiebe and riloff (2005)).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1750">
<title id=" W06-1640.xml">partially supervised coreference resolution for opinion summarization through structured rule learning </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>recent work, for example, indicates that systems can be trained to recognize opinions and their polarity, strength, and sources to reasonable degree of accuracy (e.g. dave et al.
</prevsent>
<prevsent>(2003), riloff and wiebe (2003), <papid> W03-1014 </papid>bethard etal.</prevsent>
</prevsection>
<citsent citstr=" W03-1017 ">
(2004), wilson et al (2004), yu and hatzivassiloglou (2003), <papid> W03-1017 </papid>choi et al (2005),<papid> H05-1045 </papid> kim and hovy(2005), wiebe and riloff (2005)).</citsent>
<aftsection>
<nextsent>our work extends research on fine-grained opinion extraction by augmenting the opinions with additional information that allows the creation of concise opinionsummaries.
</nextsent>
<nextsent>in contrast to the opinion extracts produced by pang and lee (2004), <papid> P04-1035 </papid>our summaries are not text extracts, but rather explicitly identify and 337 characterize the relations between opinions and their sources.</nextsent>
<nextsent>coreference resolution.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1752">
<title id=" W06-1640.xml">partially supervised coreference resolution for opinion summarization through structured rule learning </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>(2004), wilson et al (2004), yu and hatzivassiloglou (2003), <papid> W03-1017 </papid>choi et al (2005),<papid> H05-1045 </papid> kim and hovy(2005), wiebe and riloff (2005)).</prevsent>
<prevsent>our work extends research on fine-grained opinion extraction by augmenting the opinions with additional information that allows the creation of concise opinionsummaries.</prevsent>
</prevsection>
<citsent citstr=" P04-1035 ">
in contrast to the opinion extracts produced by pang and lee (2004), <papid> P04-1035 </papid>our summaries are not text extracts, but rather explicitly identify and 337 characterize the relations between opinions and their sources.</citsent>
<aftsection>
<nextsent>coreference resolution.
</nextsent>
<nextsent>coreference resolution is relatively well studied nlp problem (e.g. morton (2000), <papid> P00-1023 </papid>ng and cardie (2002), <papid> P02-1014 </papid>iida et al(2003), mccallum and wellner (2003)).</nextsent>
<nextsent>coreference resolution is defined as the problem of deciding which noun phrases in the text (mentions) refer to the same real world entities (are coreferent).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1756">
<title id=" W06-1640.xml">partially supervised coreference resolution for opinion summarization through structured rule learning </title>
<section> source coreference resolution.  </section>
<citcontext>
<prevsection>
<prevsent>we then map the sources to the nps.
</prevsent>
<prevsent>since there is no one-to-one correspondence, we use setof heuristics to create the mapping.
</prevsent>
</prevsection>
<citsent citstr=" W06-0302 ">
more details about why heuristics are needed and the process used to map sources to nps can be found in stoyanov and cardie (2006).<papid> W06-0302 </papid></citsent>
<aftsection>
<nextsent>2.
</nextsent>
<nextsent>feature vector creation: we extract fea-.
</nextsent>
<nextsent>ture vector for every pair of nps from the preprocessed corpus.
</nextsent>
<nextsent>we use the features introduced by ng and cardie (2002) <papid> P02-1014 </papid>for the task of coreference resolution.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1759">
<title id=" W06-1640.xml">partially supervised coreference resolution for opinion summarization through structured rule learning </title>
<section> source coreference resolution.  </section>
<citcontext>
<prevsection>
<prevsent>unfortunately, this proper noun phrase might be critical in establishing the coreference of the final source reference he with the other mentions of the source moussaoui.
</prevsent>
<prevsent>as mentioned previously, in order to utilize the unlabeled data, our approach differs from traditional coreference resolution, which uses np pairs as training instances.
</prevsent>
</prevsection>
<citsent citstr=" W05-0609 ">
we instead follow the framework of supervised clustering (finley and joachims, 2005; li and roth, 2005) <papid> W05-0609 </papid>and consider each document as training example.</citsent>
<aftsection>
<nextsent>as in supervised clustering, this framework has the additional advantage that the learning algorithm can consider the clustering algorithm when making decisions about pairwise classification, which could lead to improvements in the classifier.
</nextsent>
<nextsent>in the next section we describe our approach to classifier construction for step 3 and compare our problem to traditional weakly supervised clustering, characterizing it asan instance of the novel problem of partially supervised clustering.
</nextsent>
<nextsent>in our desire to perform effective source coreference resolution we arrive at the following learning problem ? the learning algorithm is presented with set of partially specified examples of clusterings and acquires function that can cluster accurately an unseen set of items, while taking advantage of the unlabeled information in the examples.this setting is to be contrasted with semi supervised clustering (or clustering with constraints), which has received much research attention (e.g. demiriz et al (1999), wagstaff and cardie (2000), basu (2005), davidson and ravi(2005)).
</nextsent>
<nextsent>semi-supervised clustering can be defined as the problem of clustering set of itemsin the presence of limited supervisory information such as pairwise constraints (e.g. two items must/cannot be in the same cluster) or labeledpoints.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1774">
<title id=" W06-1640.xml">partially supervised coreference resolution for opinion summarization through structured rule learning </title>
<section> evaluation and results.  </section>
<citcontext>
<prevsection>
<prevsent>6.4 evaluation.
</prevsent>
<prevsent>in addition to the baselines described above, we evaluate strip both with and without unlabeled data.
</prevsent>
</prevsection>
<citsent citstr=" P98-1012 ">
that is, we train on the mpqa corpus strip using either all nps or just opinion source nps.we use the b3 (bagga and baldwin, 1998) <papid> P98-1012 </papid>evaluation measure as well as precision, recall, and f1 measured on the (positive) pairwise decisions.b3 is measure widely used for evaluating coreference resolution algorithms.</citsent>
<aftsection>
<nextsent>the measure computes the precision and recall for each np mention in document, and then averages them to produce combined results for the entire output.
</nextsent>
<nextsent>more precisely, given mention that has been assigned to chain ci, the precision for mention is defined as the number of correctly identified mentions in ci divided by the total number of mentions in ci.
</nextsent>
<nextsent>recall for is defined as the number of correctly identified mentions in ci divided by the number of mentions in the gold standard chain for i. results are shown in table 1.
</nextsent>
<nextsent>the first six rows of results correspond to the fully supervised baseline systems trained on different corpora ? muc6, muc7, and mpqa src.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1777">
<title id=" W06-1628.xml">a discriminative model for treetotree translation </title>
<section> abstract </section>
<citcontext>
<prevsection>
<prevsent>this paper proposes statistical, tree to-tree model for producing translations.
</prevsent>
<prevsent>two main contributions are as follows:(1) method for the extraction of syntactic structures with alignment information from parallel corpus of translations,and (2) use of discriminative, feature based model for prediction of these target language syntactic structures which we call aligned extended projections, or aeps.
</prevsent>
</prevsection>
<citsent citstr=" N03-1017 ">
an evaluation of the method on translation from german to english shows similar performance to the phrase-based model of koehn et al (2003).<papid> N03-1017 </papid></citsent>
<aftsection>
<nextsent>phrase-based approaches (och and ney, 2004)<papid> J04-4002 </papid>to statistical machine translation (smt) have recently achieved impressive results, leading to significant improvements inaccuracy over the original ibm models (brown et al, 1993).<papid> J93-2003 </papid></nextsent>
<nextsent>however, phrase-based models lack direct representation of syntactic information in the source or target lan guages; this has prompted several researchers to consider various approaches that make use of syntactic information.this paper describes framework for tree-to tree based statistical translation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1781">
<title id=" W06-1628.xml">a discriminative model for treetotree translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>two main contributions are as follows:(1) method for the extraction of syntactic structures with alignment information from parallel corpus of translations,and (2) use of discriminative, feature based model for prediction of these target language syntactic structures which we call aligned extended projections, or aeps.
</prevsent>
<prevsent>an evaluation of the method on translation from german to english shows similar performance to the phrase-based model of koehn et al (2003).<papid> N03-1017 </papid></prevsent>
</prevsection>
<citsent citstr=" J04-4002 ">
phrase-based approaches (och and ney, 2004)<papid> J04-4002 </papid>to statistical machine translation (smt) have recently achieved impressive results, leading to significant improvements inaccuracy over the original ibm models (brown et al, 1993).<papid> J93-2003 </papid></citsent>
<aftsection>
<nextsent>however, phrase-based models lack direct representation of syntactic information in the source or target lan guages; this has prompted several researchers to consider various approaches that make use of syntactic information.this paper describes framework for tree-to tree based statistical translation.
</nextsent>
<nextsent>our goal is to learn model that maps parse trees in the source language to parse trees in the target language.the model is learned from corpus of translation pairs, where each sentence in the source or target language has an associated parse tree.
</nextsent>
<nextsent>wesee two major benefits of tree-to-tree based translation.
</nextsent>
<nextsent>first, it is possible to explicitly model the syntax of the target language, thereby improving grammaticality.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1782">
<title id=" W06-1628.xml">a discriminative model for treetotree translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>two main contributions are as follows:(1) method for the extraction of syntactic structures with alignment information from parallel corpus of translations,and (2) use of discriminative, feature based model for prediction of these target language syntactic structures which we call aligned extended projections, or aeps.
</prevsent>
<prevsent>an evaluation of the method on translation from german to english shows similar performance to the phrase-based model of koehn et al (2003).<papid> N03-1017 </papid></prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
phrase-based approaches (och and ney, 2004)<papid> J04-4002 </papid>to statistical machine translation (smt) have recently achieved impressive results, leading to significant improvements inaccuracy over the original ibm models (brown et al, 1993).<papid> J93-2003 </papid></citsent>
<aftsection>
<nextsent>however, phrase-based models lack direct representation of syntactic information in the source or target lan guages; this has prompted several researchers to consider various approaches that make use of syntactic information.this paper describes framework for tree-to tree based statistical translation.
</nextsent>
<nextsent>our goal is to learn model that maps parse trees in the source language to parse trees in the target language.the model is learned from corpus of translation pairs, where each sentence in the source or target language has an associated parse tree.
</nextsent>
<nextsent>wesee two major benefits of tree-to-tree based translation.
</nextsent>
<nextsent>first, it is possible to explicitly model the syntax of the target language, thereby improving grammaticality.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1783">
<title id=" W06-1628.xml">a discriminative model for treetotree translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our translation framework involves process where the target-language parse tree is broken down into sequence of clauses, and each clause is then translated separately.
</prevsent>
<prevsent>a central concept we introduce in the translation of clauses is that of an aligned extended projection (aep).
</prevsent>
</prevsection>
<citsent citstr=" C90-3045 ">
aeps are derived from the concept of an extended projection in lexicalized tree adjoining grammars (ltag)(frank, 2002), with the addition of alignment information that is based on work in synchronousltag (shieber and schabes, 1990).<papid> C90-3045 </papid></citsent>
<aftsection>
<nextsent>a key contribution of this paper is method for learning to map german clauses to aeps using feature based model with perceptron learning algorithm.
</nextsent>
<nextsent>we performed experiments on translation from german to english on the europarl dataset.
</nextsent>
<nextsent>evaluation in terms of both bleu scores and human judgments shows that our system performs similarly to the phrase-based model of koehn et al (2003).<papid> N03-1017 </papid></nextsent>
<nextsent>1.1 sketch of the approach this section provides an overview of the translation process.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1792">
<title id=" W06-1628.xml">a discriminative model for treetotree translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>it also means that clause-level aeps are good test-bed for the discriminative approach to aep prediction; future work may consider applying these methods to other structures such as nps, pps, adjps, and so on.
</prevsent>
<prevsent>there has been substantial amount of previous work on approaches that make use of syntactic information in statistical machine translation.
</prevsent>
</prevsection>
<citsent citstr=" J97-3002 ">
wu (1997) <papid> J97-3002 </papid>and alshawi (1996) <papid> P96-1023 </papid>describe early work on formalisms that make use of transductive grammars; graehl and knight (2004) <papid> N04-1014 </papid>describe methods for training tree transducers.</citsent>
<aftsection>
<nextsent>melamed (2004)<papid> P04-1083 </papid>establishes theoretical framework for generalized synchronous parsing and translation.</nextsent>
<nextsent>eisner(2003) <papid> P03-2041 </papid>discusses methods for learning synchronized elementary tree pairs from parallel corpus of parsed sentences.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1793">
<title id=" W06-1628.xml">a discriminative model for treetotree translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>it also means that clause-level aeps are good test-bed for the discriminative approach to aep prediction; future work may consider applying these methods to other structures such as nps, pps, adjps, and so on.
</prevsent>
<prevsent>there has been substantial amount of previous work on approaches that make use of syntactic information in statistical machine translation.
</prevsent>
</prevsection>
<citsent citstr=" P96-1023 ">
wu (1997) <papid> J97-3002 </papid>and alshawi (1996) <papid> P96-1023 </papid>describe early work on formalisms that make use of transductive grammars; graehl and knight (2004) <papid> N04-1014 </papid>describe methods for training tree transducers.</citsent>
<aftsection>
<nextsent>melamed (2004)<papid> P04-1083 </papid>establishes theoretical framework for generalized synchronous parsing and translation.</nextsent>
<nextsent>eisner(2003) <papid> P03-2041 </papid>discusses methods for learning synchronized elementary tree pairs from parallel corpus of parsed sentences.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1794">
<title id=" W06-1628.xml">a discriminative model for treetotree translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>it also means that clause-level aeps are good test-bed for the discriminative approach to aep prediction; future work may consider applying these methods to other structures such as nps, pps, adjps, and so on.
</prevsent>
<prevsent>there has been substantial amount of previous work on approaches that make use of syntactic information in statistical machine translation.
</prevsent>
</prevsection>
<citsent citstr=" N04-1014 ">
wu (1997) <papid> J97-3002 </papid>and alshawi (1996) <papid> P96-1023 </papid>describe early work on formalisms that make use of transductive grammars; graehl and knight (2004) <papid> N04-1014 </papid>describe methods for training tree transducers.</citsent>
<aftsection>
<nextsent>melamed (2004)<papid> P04-1083 </papid>establishes theoretical framework for generalized synchronous parsing and translation.</nextsent>
<nextsent>eisner(2003) <papid> P03-2041 </papid>discusses methods for learning synchronized elementary tree pairs from parallel corpus of parsed sentences.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1795">
<title id=" W06-1628.xml">a discriminative model for treetotree translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>there has been substantial amount of previous work on approaches that make use of syntactic information in statistical machine translation.
</prevsent>
<prevsent>wu (1997) <papid> J97-3002 </papid>and alshawi (1996) <papid> P96-1023 </papid>describe early work on formalisms that make use of transductive grammars; graehl and knight (2004) <papid> N04-1014 </papid>describe methods for training tree transducers.</prevsent>
</prevsection>
<citsent citstr=" P04-1083 ">
melamed (2004)<papid> P04-1083 </papid>establishes theoretical framework for generalized synchronous parsing and translation.</citsent>
<aftsection>
<nextsent>eisner(2003) <papid> P03-2041 </papid>discusses methods for learning synchronized elementary tree pairs from parallel corpus of parsed sentences.</nextsent>
<nextsent>chiang (2005) <papid> P05-1033 </papid>has recently shown significant improvements in translation accuracy, using synchronous grammars.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1796">
<title id=" W06-1628.xml">a discriminative model for treetotree translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>wu (1997) <papid> J97-3002 </papid>and alshawi (1996) <papid> P96-1023 </papid>describe early work on formalisms that make use of transductive grammars; graehl and knight (2004) <papid> N04-1014 </papid>describe methods for training tree transducers.</prevsent>
<prevsent>melamed (2004)<papid> P04-1083 </papid>establishes theoretical framework for generalized synchronous parsing and translation.</prevsent>
</prevsection>
<citsent citstr=" P03-2041 ">
eisner(2003) <papid> P03-2041 </papid>discusses methods for learning synchronized elementary tree pairs from parallel corpus of parsed sentences.</citsent>
<aftsection>
<nextsent>chiang (2005) <papid> P05-1033 </papid>has recently shown significant improvements in translation accuracy, using synchronous grammars.</nextsent>
<nextsent>riezler and maxwell (2006) <papid> N06-1032 </papid>describe method for learn inga probabilistic model that maps lfg parse structures in german into lfg parse structures in en glish.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1797">
<title id=" W06-1628.xml">a discriminative model for treetotree translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>melamed (2004)<papid> P04-1083 </papid>establishes theoretical framework for generalized synchronous parsing and translation.</prevsent>
<prevsent>eisner(2003) <papid> P03-2041 </papid>discusses methods for learning synchronized elementary tree pairs from parallel corpus of parsed sentences.</prevsent>
</prevsection>
<citsent citstr=" P05-1033 ">
chiang (2005) <papid> P05-1033 </papid>has recently shown significant improvements in translation accuracy, using synchronous grammars.</citsent>
<aftsection>
<nextsent>riezler and maxwell (2006) <papid> N06-1032 </papid>describe method for learn inga probabilistic model that maps lfg parse structures in german into lfg parse structures in en glish.</nextsent>
<nextsent>yamada and knight (2001) <papid> P01-1067 </papid>and galley et al(2004) <papid> N04-1035 </papid>describe methods that make use of syntactic information in the target language alone; quirk et al (2005) <papid> P05-1034 </papid>describe similar methods that make use of dependency representations.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1798">
<title id=" W06-1628.xml">a discriminative model for treetotree translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>eisner(2003) <papid> P03-2041 </papid>discusses methods for learning synchronized elementary tree pairs from parallel corpus of parsed sentences.</prevsent>
<prevsent>chiang (2005) <papid> P05-1033 </papid>has recently shown significant improvements in translation accuracy, using synchronous grammars.</prevsent>
</prevsection>
<citsent citstr=" N06-1032 ">
riezler and maxwell (2006) <papid> N06-1032 </papid>describe method for learn inga probabilistic model that maps lfg parse structures in german into lfg parse structures in en glish.</citsent>
<aftsection>
<nextsent>yamada and knight (2001) <papid> P01-1067 </papid>and galley et al(2004) <papid> N04-1035 </papid>describe methods that make use of syntactic information in the target language alone; quirk et al (2005) <papid> P05-1034 </papid>describe similar methods that make use of dependency representations.</nextsent>
<nextsent>syntactic parsers in the target language have been used as language models in translation, giving some improvement inaccuracy (charniak et al, 2001).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1799">
<title id=" W06-1628.xml">a discriminative model for treetotree translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>chiang (2005) <papid> P05-1033 </papid>has recently shown significant improvements in translation accuracy, using synchronous grammars.</prevsent>
<prevsent>riezler and maxwell (2006) <papid> N06-1032 </papid>describe method for learn inga probabilistic model that maps lfg parse structures in german into lfg parse structures in en glish.</prevsent>
</prevsection>
<citsent citstr=" P01-1067 ">
yamada and knight (2001) <papid> P01-1067 </papid>and galley et al(2004) <papid> N04-1035 </papid>describe methods that make use of syntactic information in the target language alone; quirk et al (2005) <papid> P05-1034 </papid>describe similar methods that make use of dependency representations.</citsent>
<aftsection>
<nextsent>syntactic parsers in the target language have been used as language models in translation, giving some improvement inaccuracy (charniak et al, 2001).
</nextsent>
<nextsent>the work of gildea (2003) <papid> P03-1011 </papid>involves methods that make use of syntactic information in both the source and target languages.other work has attempted to incorporate syntac np-a vp know sbar-a sbar-a in that np-a vp has vp been np-a np the obstacle figure 1: extended projections for the verbs know and been, and for the noun obstacle.</nextsent>
<nextsent>the eps were taken from the parse tree for the sentence we know that the main obstacle has been the predictable resistance of manufacturers.tic information through reranking approaches applied to n-best output from phrase-based systems (och et al, 2004).<papid> N04-1021 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1800">
<title id=" W06-1628.xml">a discriminative model for treetotree translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>chiang (2005) <papid> P05-1033 </papid>has recently shown significant improvements in translation accuracy, using synchronous grammars.</prevsent>
<prevsent>riezler and maxwell (2006) <papid> N06-1032 </papid>describe method for learn inga probabilistic model that maps lfg parse structures in german into lfg parse structures in en glish.</prevsent>
</prevsection>
<citsent citstr=" N04-1035 ">
yamada and knight (2001) <papid> P01-1067 </papid>and galley et al(2004) <papid> N04-1035 </papid>describe methods that make use of syntactic information in the target language alone; quirk et al (2005) <papid> P05-1034 </papid>describe similar methods that make use of dependency representations.</citsent>
<aftsection>
<nextsent>syntactic parsers in the target language have been used as language models in translation, giving some improvement inaccuracy (charniak et al, 2001).
</nextsent>
<nextsent>the work of gildea (2003) <papid> P03-1011 </papid>involves methods that make use of syntactic information in both the source and target languages.other work has attempted to incorporate syntac np-a vp know sbar-a sbar-a in that np-a vp has vp been np-a np the obstacle figure 1: extended projections for the verbs know and been, and for the noun obstacle.</nextsent>
<nextsent>the eps were taken from the parse tree for the sentence we know that the main obstacle has been the predictable resistance of manufacturers.tic information through reranking approaches applied to n-best output from phrase-based systems (och et al, 2004).<papid> N04-1021 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1801">
<title id=" W06-1628.xml">a discriminative model for treetotree translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>chiang (2005) <papid> P05-1033 </papid>has recently shown significant improvements in translation accuracy, using synchronous grammars.</prevsent>
<prevsent>riezler and maxwell (2006) <papid> N06-1032 </papid>describe method for learn inga probabilistic model that maps lfg parse structures in german into lfg parse structures in en glish.</prevsent>
</prevsection>
<citsent citstr=" P05-1034 ">
yamada and knight (2001) <papid> P01-1067 </papid>and galley et al(2004) <papid> N04-1035 </papid>describe methods that make use of syntactic information in the target language alone; quirk et al (2005) <papid> P05-1034 </papid>describe similar methods that make use of dependency representations.</citsent>
<aftsection>
<nextsent>syntactic parsers in the target language have been used as language models in translation, giving some improvement inaccuracy (charniak et al, 2001).
</nextsent>
<nextsent>the work of gildea (2003) <papid> P03-1011 </papid>involves methods that make use of syntactic information in both the source and target languages.other work has attempted to incorporate syntac np-a vp know sbar-a sbar-a in that np-a vp has vp been np-a np the obstacle figure 1: extended projections for the verbs know and been, and for the noun obstacle.</nextsent>
<nextsent>the eps were taken from the parse tree for the sentence we know that the main obstacle has been the predictable resistance of manufacturers.tic information through reranking approaches applied to n-best output from phrase-based systems (och et al, 2004).<papid> N04-1021 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1802">
<title id=" W06-1628.xml">a discriminative model for treetotree translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>yamada and knight (2001) <papid> P01-1067 </papid>and galley et al(2004) <papid> N04-1035 </papid>describe methods that make use of syntactic information in the target language alone; quirk et al (2005) <papid> P05-1034 </papid>describe similar methods that make use of dependency representations.</prevsent>
<prevsent>syntactic parsers in the target language have been used as language models in translation, giving some improvement inaccuracy (charniak et al, 2001).</prevsent>
</prevsection>
<citsent citstr=" P03-1011 ">
the work of gildea (2003) <papid> P03-1011 </papid>involves methods that make use of syntactic information in both the source and target languages.other work has attempted to incorporate syntac np-a vp know sbar-a sbar-a in that np-a vp has vp been np-a np the obstacle figure 1: extended projections for the verbs know and been, and for the noun obstacle.</citsent>
<aftsection>
<nextsent>the eps were taken from the parse tree for the sentence we know that the main obstacle has been the predictable resistance of manufacturers.tic information through reranking approaches applied to n-best output from phrase-based systems (och et al, 2004).<papid> N04-1021 </papid></nextsent>
<nextsent>another class of approaches has shown improvements in translation through reordering, where source language strings are parsed and then reordered, in an attempt to recover word order that is closer to the target language (collins et al, 2005; <papid> P05-1066 </papid>xia and mccord, 2004).<papid> C04-1073 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1803">
<title id=" W06-1628.xml">a discriminative model for treetotree translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>syntactic parsers in the target language have been used as language models in translation, giving some improvement inaccuracy (charniak et al, 2001).
</prevsent>
<prevsent>the work of gildea (2003) <papid> P03-1011 </papid>involves methods that make use of syntactic information in both the source and target languages.other work has attempted to incorporate syntac np-a vp know sbar-a sbar-a in that np-a vp has vp been np-a np the obstacle figure 1: extended projections for the verbs know and been, and for the noun obstacle.</prevsent>
</prevsection>
<citsent citstr=" N04-1021 ">
the eps were taken from the parse tree for the sentence we know that the main obstacle has been the predictable resistance of manufacturers.tic information through reranking approaches applied to n-best output from phrase-based systems (och et al, 2004).<papid> N04-1021 </papid></citsent>
<aftsection>
<nextsent>another class of approaches has shown improvements in translation through reordering, where source language strings are parsed and then reordered, in an attempt to recover word order that is closer to the target language (collins et al, 2005; <papid> P05-1066 </papid>xia and mccord, 2004).<papid> C04-1073 </papid></nextsent>
<nextsent>our approach is closely related to previous work on synchronous tree adjoining grammars (shieber and schabes, 1990; <papid> C90-3045 </papid>shieber, 2004), and the work on tag approaches to syntax described by frank (2002).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1804">
<title id=" W06-1628.xml">a discriminative model for treetotree translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the work of gildea (2003) <papid> P03-1011 </papid>involves methods that make use of syntactic information in both the source and target languages.other work has attempted to incorporate syntac np-a vp know sbar-a sbar-a in that np-a vp has vp been np-a np the obstacle figure 1: extended projections for the verbs know and been, and for the noun obstacle.</prevsent>
<prevsent>the eps were taken from the parse tree for the sentence we know that the main obstacle has been the predictable resistance of manufacturers.tic information through reranking approaches applied to n-best output from phrase-based systems (och et al, 2004).<papid> N04-1021 </papid></prevsent>
</prevsection>
<citsent citstr=" P05-1066 ">
another class of approaches has shown improvements in translation through reordering, where source language strings are parsed and then reordered, in an attempt to recover word order that is closer to the target language (collins et al, 2005; <papid> P05-1066 </papid>xia and mccord, 2004).<papid> C04-1073 </papid></citsent>
<aftsection>
<nextsent>our approach is closely related to previous work on synchronous tree adjoining grammars (shieber and schabes, 1990; <papid> C90-3045 </papid>shieber, 2004), and the work on tag approaches to syntax described by frank (2002).</nextsent>
<nextsent>a major departure from previous work on synchronous tags is in our use of discriminative model that incrementally predicts the information in the aep.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1805">
<title id=" W06-1628.xml">a discriminative model for treetotree translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the work of gildea (2003) <papid> P03-1011 </papid>involves methods that make use of syntactic information in both the source and target languages.other work has attempted to incorporate syntac np-a vp know sbar-a sbar-a in that np-a vp has vp been np-a np the obstacle figure 1: extended projections for the verbs know and been, and for the noun obstacle.</prevsent>
<prevsent>the eps were taken from the parse tree for the sentence we know that the main obstacle has been the predictable resistance of manufacturers.tic information through reranking approaches applied to n-best output from phrase-based systems (och et al, 2004).<papid> N04-1021 </papid></prevsent>
</prevsection>
<citsent citstr=" C04-1073 ">
another class of approaches has shown improvements in translation through reordering, where source language strings are parsed and then reordered, in an attempt to recover word order that is closer to the target language (collins et al, 2005; <papid> P05-1066 </papid>xia and mccord, 2004).<papid> C04-1073 </papid></citsent>
<aftsection>
<nextsent>our approach is closely related to previous work on synchronous tree adjoining grammars (shieber and schabes, 1990; <papid> C90-3045 </papid>shieber, 2004), and the work on tag approaches to syntax described by frank (2002).</nextsent>
<nextsent>a major departure from previous work on synchronous tags is in our use of discriminative model that incrementally predicts the information in the aep.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1807">
<title id=" W06-1628.xml">a discriminative model for treetotree translation </title>
<section> extracting aeps from corpus.  </section>
<citcontext>
<prevsection>
<prevsent>a crucial step in our approach is the extraction of training examples from translation corpus.
</prevsent>
<prevsent>each training example consists of german clause paired with an english aep (see figure 2).in our experiments, we used the europarl corpus (koehn, 2005).
</prevsent>
</prevsection>
<citsent citstr=" P05-1039 ">
for each sentence pair from this data, we used version of the german parser described by dubey (2005) <papid> P05-1039 </papid>to parse the german component, and version of the english parser described by collins (1999) to parse the englishcomponent.</citsent>
<aftsection>
<nextsent>to extract aeps, we perform the following steps: np and pp alignment to align nps and pps, first all german and english nouns, personal and possessive pronouns, numbers, and adjectives are identified in each sentence and aligned using giza++ (och and ney, 2003).<papid> J03-1002 </papid></nextsent>
<nextsent>next, each np in an english tree is aligned to an np or pp in the corresponding german tree in way that is consistent with the word-alignment information.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1808">
<title id=" W06-1628.xml">a discriminative model for treetotree translation </title>
<section> extracting aeps from corpus.  </section>
<citcontext>
<prevsection>
<prevsent>each training example consists of german clause paired with an english aep (see figure 2).in our experiments, we used the europarl corpus (koehn, 2005).
</prevsent>
<prevsent>for each sentence pair from this data, we used version of the german parser described by dubey (2005) <papid> P05-1039 </papid>to parse the german component, and version of the english parser described by collins (1999) to parse the englishcomponent.</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
to extract aeps, we perform the following steps: np and pp alignment to align nps and pps, first all german and english nouns, personal and possessive pronouns, numbers, and adjectives are identified in each sentence and aligned using giza++ (och and ney, 2003).<papid> J03-1002 </papid></citsent>
<aftsection>
<nextsent>next, each np in an english tree is aligned to an np or pp in the corresponding german tree in way that is consistent with the word-alignment information.
</nextsent>
<nextsent>that is, the words dominated by the english node must be aligned only to words dominated by the german node, and vice versa.
</nextsent>
<nextsent>note that if there is more than one german node that is consistent, then the one rooted at the minimal subtree is selected.
</nextsent>
<nextsent>clause alignment, and aep extraction the next step in the training process is to identifygerman/english clause pairs which are translations of each other.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1809">
<title id=" W06-1628.xml">a discriminative model for treetotree translation </title>
<section> the model.  </section>
<citcontext>
<prevsection>
<prevsent>the score for any partial path is defined in eq.
</prevsent>
<prevsent>1.
</prevsent>
</prevsection>
<citsent citstr=" W02-1001 ">
the advance function is used to specify the set of possible decisions that can extend any given path in the beam.to train the model, we use the averaged perceptron algorithm described by collins (2002).<papid> W02-1001 </papid></citsent>
<aftsection>
<nextsent>this combination of the perceptron algorithm with beam-search is similar to that described by collins and roark (2004).<papid> P04-1015 </papid>5 the perceptron algorithm is convenient choice because it converges quickly usually taking only few iterations over the training set (collins, 2002; <papid> W02-1001 </papid>collins and roark, 2004).<papid> P04-1015 </papid></nextsent>
<nextsent>5.2 the features of the model.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1810">
<title id=" W06-1628.xml">a discriminative model for treetotree translation </title>
<section> the model.  </section>
<citcontext>
<prevsection>
<prevsent>1.
</prevsent>
<prevsent>the advance function is used to specify the set of possible decisions that can extend any given path in the beam.to train the model, we use the averaged perceptron algorithm described by collins (2002).<papid> W02-1001 </papid></prevsent>
</prevsection>
<citsent citstr=" P04-1015 ">
this combination of the perceptron algorithm with beam-search is similar to that described by collins and roark (2004).<papid> P04-1015 </papid>5 the perceptron algorithm is convenient choice because it converges quickly usually taking only few iterations over the training set (collins, 2002; <papid> W02-1001 </papid>collins and roark, 2004).<papid> P04-1015 </papid></citsent>
<aftsection>
<nextsent>5.2 the features of the model.
</nextsent>
<nextsent>the models features allow it to capture dependencies between the aep and the german clause, as well as dependencies between different parts of the aep itself.
</nextsent>
<nextsent>the features included in ??
</nextsent>
<nextsent>5future work may consider alternative algorithms, such as those described by daume?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1822">
<title id=" W06-2810.xml">finding similar sentences across multiple languages in wikipedia </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in wikipedia, two versions of wikipedia topic in two different languages are good starting point for searching similar sentences.
</prevsent>
<prevsent>however, these pages may not always conform to the typical definitions of bitextwhich current techniques assume.
</prevsent>
</prevsection>
<citsent citstr=" W96-0201 ">
bitext generally refers to two versions of text in two different languages (melamed, 1996).<papid> W96-0201 </papid></citsent>
<aftsection>
<nextsent>though it is not known how information is shared among the different languages in wikipedia, some pages tend to be translations of each other whereas the majority of the pages tend to be written independently of each other.
</nextsent>
<nextsent>therefore, two versions of the same topic in two different languages can not simply be taken as parallel corpora.
</nextsent>
<nextsent>this in turn limits the application of some of the currently available techniques.
</nextsent>
<nextsent>in this paper, we present two approaches for finding similar sentences across multiple languages in wikipedia.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1823">
<title id=" W06-1627.xml">efficient search for inversion transduction grammar </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these techniques make it possible to find optimal alignments much more quickly, andmake it possible to find optimal translations for the first time.
</prevsent>
<prevsent>even in the presence of pruning, we are able to achieve higher bleu scores with the same amount of computation.
</prevsent>
</prevsection>
<citsent citstr=" J97-3002 ">
the inversion transduction grammar (itg) of wu (1997) <papid> J97-3002 </papid>is syntactically motivated algorithm for producing word-level alignments of pairs oftranslationally equivalent sentences in two lan guages.</citsent>
<aftsection>
<nextsent>the algorithm builds synchronous parse tree for both sentences, and assumes that the tree shave the same underlying structure but that the ordering of constituents may differ in the two languages.
</nextsent>
<nextsent>itg imposes constraints on which alignments are possible, and these constraints have been shown to be good match for real bitext data (zens and ney, 2003).<papid> P03-1019 </papid></nextsent>
<nextsent>a major motivation for the introduction of itg was the existence of polynomial-time algorithms both for alignment and translation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1824">
<title id=" W06-1627.xml">efficient search for inversion transduction grammar </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the inversion transduction grammar (itg) of wu (1997) <papid> J97-3002 </papid>is syntactically motivated algorithm for producing word-level alignments of pairs oftranslationally equivalent sentences in two lan guages.</prevsent>
<prevsent>the algorithm builds synchronous parse tree for both sentences, and assumes that the tree shave the same underlying structure but that the ordering of constituents may differ in the two languages.</prevsent>
</prevsection>
<citsent citstr=" P03-1019 ">
itg imposes constraints on which alignments are possible, and these constraints have been shown to be good match for real bitext data (zens and ney, 2003).<papid> P03-1019 </papid></citsent>
<aftsection>
<nextsent>a major motivation for the introduction of itg was the existence of polynomial-time algorithms both for alignment and translation.
</nextsent>
<nextsent>alignment, whether for training translation model using em or for finding the viterbi alignment of test data,is o(n6) (wu, 1997), <papid> J97-3002 </papid>while translation (decod ing) is o(n7) using bigram language model, ando(n11) with trigrams.</nextsent>
<nextsent>while polynomial-time algorithms are major improvement over the np complete problems posed by the alignment models of brown et al (1993), <papid> J93-2003 </papid>the degree of these polynomials is high, making both alignment and decoding infeasible for realistic sentences without very significant pruning.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1826">
<title id=" W06-1627.xml">efficient search for inversion transduction grammar </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a major motivation for the introduction of itg was the existence of polynomial-time algorithms both for alignment and translation.
</prevsent>
<prevsent>alignment, whether for training translation model using em or for finding the viterbi alignment of test data,is o(n6) (wu, 1997), <papid> J97-3002 </papid>while translation (decod ing) is o(n7) using bigram language model, ando(n11) with trigrams.</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
while polynomial-time algorithms are major improvement over the np complete problems posed by the alignment models of brown et al (1993), <papid> J93-2003 </papid>the degree of these polynomials is high, making both alignment and decoding infeasible for realistic sentences without very significant pruning.</citsent>
<aftsection>
<nextsent>in this paper, we explore use of the hook trick?
</nextsent>
<nextsent>(eisner and satta, 1999; <papid> P99-1059 </papid>huang et al, 2005) <papid> W05-1507 </papid>to reduce the asymptotic complexity of decoding, and the use of heuristics to guide the search.our search heuristics are conservative estimate of the outside probability of bitext cell in the complete synchronous parse.</nextsent>
<nextsent>some estimate of this outside probability is common element of modern statistical (monolingual) parsers (char niak et al, 1998; <papid> W98-1115 </papid>collins, 1999), and recent work has developed heuristics that are admissible for a* search, guaranteeing that the optimal parse will be found (klein and manning, 2003).<papid> N03-1016 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1827">
<title id=" W06-1627.xml">efficient search for inversion transduction grammar </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>while polynomial-time algorithms are major improvement over the np complete problems posed by the alignment models of brown et al (1993), <papid> J93-2003 </papid>the degree of these polynomials is high, making both alignment and decoding infeasible for realistic sentences without very significant pruning.</prevsent>
<prevsent>in this paper, we explore use of the hook trick?</prevsent>
</prevsection>
<citsent citstr=" P99-1059 ">
(eisner and satta, 1999; <papid> P99-1059 </papid>huang et al, 2005) <papid> W05-1507 </papid>to reduce the asymptotic complexity of decoding, and the use of heuristics to guide the search.our search heuristics are conservative estimate of the outside probability of bitext cell in the complete synchronous parse.</citsent>
<aftsection>
<nextsent>some estimate of this outside probability is common element of modern statistical (monolingual) parsers (char niak et al, 1998; <papid> W98-1115 </papid>collins, 1999), and recent work has developed heuristics that are admissible for a* search, guaranteeing that the optimal parse will be found (klein and manning, 2003).<papid> N03-1016 </papid></nextsent>
<nextsent>we extend this type of outside probability estimate to include both word translation and n-gram language model probabilities.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1828">
<title id=" W06-1627.xml">efficient search for inversion transduction grammar </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>while polynomial-time algorithms are major improvement over the np complete problems posed by the alignment models of brown et al (1993), <papid> J93-2003 </papid>the degree of these polynomials is high, making both alignment and decoding infeasible for realistic sentences without very significant pruning.</prevsent>
<prevsent>in this paper, we explore use of the hook trick?</prevsent>
</prevsection>
<citsent citstr=" W05-1507 ">
(eisner and satta, 1999; <papid> P99-1059 </papid>huang et al, 2005) <papid> W05-1507 </papid>to reduce the asymptotic complexity of decoding, and the use of heuristics to guide the search.our search heuristics are conservative estimate of the outside probability of bitext cell in the complete synchronous parse.</citsent>
<aftsection>
<nextsent>some estimate of this outside probability is common element of modern statistical (monolingual) parsers (char niak et al, 1998; <papid> W98-1115 </papid>collins, 1999), and recent work has developed heuristics that are admissible for a* search, guaranteeing that the optimal parse will be found (klein and manning, 2003).<papid> N03-1016 </papid></nextsent>
<nextsent>we extend this type of outside probability estimate to include both word translation and n-gram language model probabilities.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1829">
<title id=" W06-1627.xml">efficient search for inversion transduction grammar </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this paper, we explore use of the hook trick?
</prevsent>
<prevsent>(eisner and satta, 1999; <papid> P99-1059 </papid>huang et al, 2005) <papid> W05-1507 </papid>to reduce the asymptotic complexity of decoding, and the use of heuristics to guide the search.our search heuristics are conservative estimate of the outside probability of bitext cell in the complete synchronous parse.</prevsent>
</prevsection>
<citsent citstr=" W98-1115 ">
some estimate of this outside probability is common element of modern statistical (monolingual) parsers (char niak et al, 1998; <papid> W98-1115 </papid>collins, 1999), and recent work has developed heuristics that are admissible for a* search, guaranteeing that the optimal parse will be found (klein and manning, 2003).<papid> N03-1016 </papid></citsent>
<aftsection>
<nextsent>we extend this type of outside probability estimate to include both word translation and n-gram language model probabilities.
</nextsent>
<nextsent>these measures have been used toguide search in word- or phrase-based mt systems (wang and waibel, 1997; <papid> P97-1047 </papid>och et al, 2001), <papid> W01-1408 </papid>but in such models optimal search is generally not practical even with good heuristics.</nextsent>
<nextsent>in this paper, we show that the same assumptions that make itgpolynomial-time can be used to efficiently compute heuristics which guarantee us that we will find the optimal alignment or translation, while significantly speeding the search.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1830">
<title id=" W06-1627.xml">efficient search for inversion transduction grammar </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this paper, we explore use of the hook trick?
</prevsent>
<prevsent>(eisner and satta, 1999; <papid> P99-1059 </papid>huang et al, 2005) <papid> W05-1507 </papid>to reduce the asymptotic complexity of decoding, and the use of heuristics to guide the search.our search heuristics are conservative estimate of the outside probability of bitext cell in the complete synchronous parse.</prevsent>
</prevsection>
<citsent citstr=" N03-1016 ">
some estimate of this outside probability is common element of modern statistical (monolingual) parsers (char niak et al, 1998; <papid> W98-1115 </papid>collins, 1999), and recent work has developed heuristics that are admissible for a* search, guaranteeing that the optimal parse will be found (klein and manning, 2003).<papid> N03-1016 </papid></citsent>
<aftsection>
<nextsent>we extend this type of outside probability estimate to include both word translation and n-gram language model probabilities.
</nextsent>
<nextsent>these measures have been used toguide search in word- or phrase-based mt systems (wang and waibel, 1997; <papid> P97-1047 </papid>och et al, 2001), <papid> W01-1408 </papid>but in such models optimal search is generally not practical even with good heuristics.</nextsent>
<nextsent>in this paper, we show that the same assumptions that make itgpolynomial-time can be used to efficiently compute heuristics which guarantee us that we will find the optimal alignment or translation, while significantly speeding the search.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1831">
<title id=" W06-1627.xml">efficient search for inversion transduction grammar </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>some estimate of this outside probability is common element of modern statistical (monolingual) parsers (char niak et al, 1998; <papid> W98-1115 </papid>collins, 1999), and recent work has developed heuristics that are admissible for a* search, guaranteeing that the optimal parse will be found (klein and manning, 2003).<papid> N03-1016 </papid></prevsent>
<prevsent>we extend this type of outside probability estimate to include both word translation and n-gram language model probabilities.</prevsent>
</prevsection>
<citsent citstr=" P97-1047 ">
these measures have been used toguide search in word- or phrase-based mt systems (wang and waibel, 1997; <papid> P97-1047 </papid>och et al, 2001), <papid> W01-1408 </papid>but in such models optimal search is generally not practical even with good heuristics.</citsent>
<aftsection>
<nextsent>in this paper, we show that the same assumptions that make itgpolynomial-time can be used to efficiently compute heuristics which guarantee us that we will find the optimal alignment or translation, while significantly speeding the search.
</nextsent>
<nextsent>an inversion transduction grammar can generate pairs of sentences in two languages by recursively applying context-free bilingual production rules.
</nextsent>
<nextsent>most work on itg has focused on the 2-normal form, which consists of unary production rules that are responsible for generating word pairs: ? e/f 224 and binary production rules in two forms that are responsible for generating syntactic subtree pairs: ? [y z] and ? z?
</nextsent>
<nextsent>the rules with square brackets enclosing the right hand side expand the left hand side symbol into the two symbols on the right hand side in the same order in the two languages, whereas the rules with pointed brackets expand the left hand side symbol into the two right hand side symbols in reverse order in the two languages.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1832">
<title id=" W06-1627.xml">efficient search for inversion transduction grammar </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>some estimate of this outside probability is common element of modern statistical (monolingual) parsers (char niak et al, 1998; <papid> W98-1115 </papid>collins, 1999), and recent work has developed heuristics that are admissible for a* search, guaranteeing that the optimal parse will be found (klein and manning, 2003).<papid> N03-1016 </papid></prevsent>
<prevsent>we extend this type of outside probability estimate to include both word translation and n-gram language model probabilities.</prevsent>
</prevsection>
<citsent citstr=" W01-1408 ">
these measures have been used toguide search in word- or phrase-based mt systems (wang and waibel, 1997; <papid> P97-1047 </papid>och et al, 2001), <papid> W01-1408 </papid>but in such models optimal search is generally not practical even with good heuristics.</citsent>
<aftsection>
<nextsent>in this paper, we show that the same assumptions that make itgpolynomial-time can be used to efficiently compute heuristics which guarantee us that we will find the optimal alignment or translation, while significantly speeding the search.
</nextsent>
<nextsent>an inversion transduction grammar can generate pairs of sentences in two languages by recursively applying context-free bilingual production rules.
</nextsent>
<nextsent>most work on itg has focused on the 2-normal form, which consists of unary production rules that are responsible for generating word pairs: ? e/f 224 and binary production rules in two forms that are responsible for generating syntactic subtree pairs: ? [y z] and ? z?
</nextsent>
<nextsent>the rules with square brackets enclosing the right hand side expand the left hand side symbol into the two symbols on the right hand side in the same order in the two languages, whereas the rules with pointed brackets expand the left hand side symbol into the two right hand side symbols in reverse order in the two languages.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1833">
<title id=" W06-1627.xml">efficient search for inversion transduction grammar </title>
<section> a* viterbi alignment selection </section>
<citcontext>
<prevsection>
<prevsent>the word sequence on the top is the viterbi translation of the sentence on the bottom.
</prevsent>
<prevsent>wide range word order change may happen.
</prevsent>
</prevsection>
<citsent citstr=" P05-1059 ">
nature of the cells, we can compute values for the inside and outside components of each cell using dynamic programming in o(n4) time (zhang and gildea, 2005).<papid> P05-1059 </papid></citsent>
<aftsection>
<nextsent>the of itg decoding algorithm of wu (1996) <papid> P96-1021 </papid>canbe viewed as variant of the viterbi parsing algorithm for alignment selection.</nextsent>
<nextsent>the task of standard alignment is to find word level links between two fixed-order strings.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1834">
<title id=" W06-1627.xml">efficient search for inversion transduction grammar </title>
<section> a* decoding </section>
<citcontext>
<prevsection>
<prevsent>wide range word order change may happen.
</prevsent>
<prevsent>nature of the cells, we can compute values for the inside and outside components of each cell using dynamic programming in o(n4) time (zhang and gildea, 2005).<papid> P05-1059 </papid></prevsent>
</prevsection>
<citsent citstr=" P96-1021 ">
the of itg decoding algorithm of wu (1996) <papid> P96-1021 </papid>canbe viewed as variant of the viterbi parsing algorithm for alignment selection.</citsent>
<aftsection>
<nextsent>the task of standard alignment is to find word level links between two fixed-order strings.
</nextsent>
<nextsent>in the decoding situation, while the input side is fixed sequence of words, the output side is bag of words to be linked with the input words and then reordered.
</nextsent>
<nextsent>under the itg constraint, if the target language substring [i, j] is translated into s1 in the source language and the target substring [j, k] is translated into s2, then s1and s2 must be consecutive in the source language as well and two possible orderings, s1s2 and s2s1,are allowed.
</nextsent>
<nextsent>finding the best translation of the substring of [i, k] involves searching over all possible split points and two possible reorderings for each split.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1837">
<title id=" W06-3206.xml">improved morpho phonological sequence processing with constraint satisfaction inference </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>if letter-phoneme conversion system predicts schwas on every vowel in polysyllabic word such as parameter because it is uncertain about the ambiguous mapping of each of the as andes, it produces bad pronunciation.
</prevsent>
<prevsent>likewise, if morphological analysis system segments word such as being as prefix followed by an inflection, making the locally most likely guesses, it generates an analysis that could never exist, since it lacks stem.global models that coordinate, mediate, or enforce that the output is valid sequence are typically formulated in the form of linguistic rules, applied during processing or in post-processing, that constrain the space of possible output sequences.
</prevsent>
</prevsection>
<citsent citstr=" W04-3246 ">
some present-day research in machine learning of morpho-phonology indeed focuses on satisfying linguistically-motivated constraints as postprocessing or filtering step; e.g., see (daya et al, 2004) <papid> W04-3246 </papid>on identifying roots in hebrew word forms.</citsent>
<aftsection>
<nextsent>optimality theory (prince and smolensky, 2004) can also be seen as constraint-based approach to language processing based on linguistically motivated constraints.
</nextsent>
<nextsent>in contrast to being motivated by linguistic theory, constraints in global model can be learned automatically from data as well.
</nextsent>
<nextsent>in this paper we propose such data-driven constraint satisfaction inference method, that finds globally appropriate output sequence on the basis of space of possible sequences generated by locally-operating classifier predicting output subsequences.
</nextsent>
<nextsent>we show that the method significantly improves on the basic method of predicting single output tokens at atime, on english and dutch letter-phoneme conversion and morphological analysis.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1838">
<title id=" W07-0211.xml">dlsite2 semantic similarity based on syntactic dependency trees applied to textual entailment </title>
<section> methods.  </section>
<citcontext>
<prevsection>
<prevsent>if they coincide, we then proceed to compare their respective child nodes, which are the tokens that have some sort of dependency with their respective root token.
</prevsent>
<prevsent>in order to add more flexibility to our system,we do not require the pair of tokens to be exactly the same, but rather set threshold that represents the minimum similarity value between them.
</prevsent>
</prevsection>
<citsent citstr=" N04-3012 ">
this is difference between our approach and the one described in (katrenko and adriaans, 2006).such similarity is calculated by using the word net::similarity tool (pedersen et al, 2004), <papid> N04-3012 </papid>and, concretely, the wu-palmer measure, as defined in equation 1 (wu and palmer, 1994).<papid> P94-1019 </papid></citsent>
<aftsection>
<nextsent>sim(c1, c2) = 2n3 n1 +n2 + 2n3 (1) where c1 and c2 are the synsets whose similarity we want to calculate, c3 is their least common su per concept, n1 is the number of nodes on the path from c1 to c3, n2 is the number of nodes on the path from c2 to c3, and n3 is the number of nodes on the path from c3 to the root.
</nextsent>
<nextsent>all these synsets and distances can be observed in figure 2.
</nextsent>
<nextsent>figure 2: distance between two synsets.
</nextsent>
<nextsent>if the similarity rate is greater or equal than the established threshold, which we have set empirically to 80%, we will consider the corresponding hypoth esis?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1839">
<title id=" W07-0211.xml">dlsite2 semantic similarity based on syntactic dependency trees applied to textual entailment </title>
<section> methods.  </section>
<citcontext>
<prevsection>
<prevsent>if they coincide, we then proceed to compare their respective child nodes, which are the tokens that have some sort of dependency with their respective root token.
</prevsent>
<prevsent>in order to add more flexibility to our system,we do not require the pair of tokens to be exactly the same, but rather set threshold that represents the minimum similarity value between them.
</prevsent>
</prevsection>
<citsent citstr=" P94-1019 ">
this is difference between our approach and the one described in (katrenko and adriaans, 2006).such similarity is calculated by using the word net::similarity tool (pedersen et al, 2004), <papid> N04-3012 </papid>and, concretely, the wu-palmer measure, as defined in equation 1 (wu and palmer, 1994).<papid> P94-1019 </papid></citsent>
<aftsection>
<nextsent>sim(c1, c2) = 2n3 n1 +n2 + 2n3 (1) where c1 and c2 are the synsets whose similarity we want to calculate, c3 is their least common su per concept, n1 is the number of nodes on the path from c1 to c3, n2 is the number of nodes on the path from c2 to c3, and n3 is the number of nodes on the path from c3 to the root.
</nextsent>
<nextsent>all these synsets and distances can be observed in figure 2.
</nextsent>
<nextsent>figure 2: distance between two synsets.
</nextsent>
<nextsent>if the similarity rate is greater or equal than the established threshold, which we have set empirically to 80%, we will consider the corresponding hypoth esis?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1840">
<title id=" W07-0211.xml">dlsite2 semantic similarity based on syntactic dependency trees applied to textual entailment </title>
<section> methods.  </section>
<citcontext>
<prevsection>
<prevsent>this operation consists in finding pairs of tokens in both trees whose lemmas are identical, no matter whether they are in the same position within the tree.
</prevsent>
<prevsent>we would like to point out that in this step we do not use the wordnet::similarity tool.
</prevsent>
</prevsection>
<citsent citstr=" N06-1006 ">
some authors have already designed similar matching techniques, such as the ones described in (maccartney et al, 2006) <papid> N06-1006 </papid>and (snow et al, 2006).<papid> N06-1005 </papid></citsent>
<aftsection>
<nextsent>however, these include semantic constraints that we have decided not to consider.
</nextsent>
<nextsent>the reason of this decision is that we desired to overcome the textual entailment recognition from an exclusively syntactic perspective.
</nextsent>
<nextsent>therefore, we did not want this module to include any kind of semantic knowledge.
</nextsent>
<nextsent>the weight given to token that has been foundin both trees will depend on the depth in the hypoth esis?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1841">
<title id=" W07-0211.xml">dlsite2 semantic similarity based on syntactic dependency trees applied to textual entailment </title>
<section> methods.  </section>
<citcontext>
<prevsection>
<prevsent>this operation consists in finding pairs of tokens in both trees whose lemmas are identical, no matter whether they are in the same position within the tree.
</prevsent>
<prevsent>we would like to point out that in this step we do not use the wordnet::similarity tool.
</prevsent>
</prevsection>
<citsent citstr=" N06-1005 ">
some authors have already designed similar matching techniques, such as the ones described in (maccartney et al, 2006) <papid> N06-1006 </papid>and (snow et al, 2006).<papid> N06-1005 </papid></citsent>
<aftsection>
<nextsent>however, these include semantic constraints that we have decided not to consider.
</nextsent>
<nextsent>the reason of this decision is that we desired to overcome the textual entailment recognition from an exclusively syntactic perspective.
</nextsent>
<nextsent>therefore, we did not want this module to include any kind of semantic knowledge.
</nextsent>
<nextsent>the weight given to token that has been foundin both trees will depend on the depth in the hypoth esis?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1843">
<title id=" W06-2205.xml">recognition of synonyms by a lexical graph </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the electronic online thesauri such as wordnet (2005) and open thesaurus (2005) have been increasingly employed for many ir and nlpproblems.
</prevsent>
<prevsent>however, considerable human effort is required to keep up with the evolving language and many sub domains are not sufficiently covered (turney, 2001).
</prevsent>
</prevsection>
<citsent citstr=" W02-0908 ">
many domain specific words or word senses are not included;inconsistency and bias are often cited asfur ther major deficiencies of hand-made thesauri (curran and moens, 2002), (<papid> W02-0908 </papid>senellart and blondel, 2003).</citsent>
<aftsection>
<nextsent>there is continuous demand for automatic identification of semantic relations and thesaurus generation.
</nextsent>
<nextsent>such tools do not only produce thesauri that are more adapted to particular application in certain domain, but provide also assistance for lexicographers in manual creation and keeping the hand-written thesauri up to date.
</nextsent>
<nextsent>numerous applications in ir (e.g. query expansion)and text mining (identification of relevant content by patterns) underline their usefulness.
</nextsent>
<nextsent>identification of semantic relations has been approached by different communities ascom ponent of knowledge management system or application of developed nlp framework.many approaches are guided by the assumption that similar terms occur in similar context and obtain context representation of termsas attribute vectors or relation tuples (cur ran and moens, 2002), (<papid> W02-0908 </papid>ruge, 1997), (lin,1998).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1846">
<title id=" W06-2205.xml">recognition of synonyms by a lexical graph </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the patterns contain lexical and syntactic elements and are acquired 32from text corpus by identifying common context of word pairs for which semantic relation holds.
</prevsent>
<prevsent>identified patterns are applied to large text corpus to detect new relations.
</prevsent>
</prevsection>
<citsent citstr=" W02-1017 ">
the method can be enhanced by applying filtering steps and ite rating over new found instances (phillips and riloff, 2002).<papid> W02-1017 </papid></citsent>
<aftsection>
<nextsent>lafourcade and prince base their approach on reduction of word semantics to conceptual vectors (vector space is spanned by hierarchy of concepts provided by thesaurus, (lafour cade, 2001)).
</nextsent>
<nextsent>every term is projected in the vector space and can be expressed by the linear combination of conceptual vectors.
</nextsent>
<nextsent>the angle between the vector ial representations of two terms is used in calculation of thematic closeness (lafourcade and prince, 2001).
</nextsent>
<nextsent>the approach is more closely related to our approach since it offers quantitative metric to measure the degree of synonymy between two lexical items.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1849">
<title id=" W06-2205.xml">recognition of synonyms by a lexical graph </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the least independent word is regarded synonymous.
</prevsent>
<prevsent>blondell et al (blondel et al, 2004) encodea monolingual dictionary as graph and identify synonyms by finding subgraphs that are similar to the subgraph corresponding to the queried term.the common evaluation method for similarity metrics is comparing their performance onthe same test set with the same context representations with some manually created semantic source as the gold standard (curran and moens, 2002).<papid> W02-0908 </papid></prevsent>
</prevsection>
<citsent citstr=" C04-1146 ">
abstracting from results for concrete test sets, weeds et al (2004) <papid> C04-1146 </papid>try to identify statistical and linguistic properties onthat the performance of similarity metrics generally depends.</citsent>
<aftsection>
<nextsent>different bias towards words with high or low frequency is recognized as one reason for the significant variance of k-nearest neighbors sets of different similarity metrics.
</nextsent>
<nextsent>the assumption that similar terms occur in similar context leads to the establishing of explicit context models (e.g. in form of vectors or relation tuples) by most researchers.
</nextsent>
<nextsent>we build an implicit context representation connecting lexical items in way corresponding to the sentence structure (as opposed to (blondel et al, 2004)), where term is linked to every word in its definition).
</nextsent>
<nextsent>the advantage of the graph model is its transitivity: not only terms in the immediate context but also semantically related terms that have short path to the examined term (but perhaps have never occurred in its immediate context) can contribute to identification of related terms.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1854">
<title id=" W06-3602.xml">efficient dynamic programming search algorithms for phrase based smt </title>
<section> abstract </section>
<citcontext>
<prevsection>
<prevsent>this paper presents series of efficient dynamic-programming (dp) based algorithms for phrase-based decoding and alignment computation in statistical machine translation (smt).
</prevsent>
<prevsent>the dp-based decoding algorithms are analyzed in terms of shortest path-finding algorithms, where the similarity to dp-based decoding algorithms in speech recognition isdemonstrated.
</prevsent>
</prevsection>
<citsent citstr=" J03-1005 ">
the paper contains the following original contributions: 1) the dp-based decoding algorithm in (tillmann and ney, 2003) <papid> J03-1005 </papid>is extended informal way to handle phrases and novel pruning strategy with increased translation speed is presented 2) novel alignment algorithm is presented that computes phrase alignment efficiently in the case that itis consistent with an underlying word alignment.</citsent>
<aftsection>
<nextsent>under certain restrictions, both algorithms handle mt-related problems efficiently that are generally np complete (knight, 1999).<papid> J99-4005 </papid></nextsent>
<nextsent>this paper deals with dynamic programming based decoding and alignment algorithms for phrase-based smt.dynamic programming based search algorithms are being used in speech recognition (jelinek, 1998; ney et al ., 1992) as well as in statistical machine translation (tillmann et al , 1997; <papid> P97-1037 </papid>niessen et al , 1998; tillmann and ney, 2003).<papid> J03-1005 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1858">
<title id=" W06-3602.xml">efficient dynamic programming search algorithms for phrase based smt </title>
<section> abstract </section>
<citcontext>
<prevsection>
<prevsent>the dp-based decoding algorithms are analyzed in terms of shortest path-finding algorithms, where the similarity to dp-based decoding algorithms in speech recognition isdemonstrated.
</prevsent>
<prevsent>the paper contains the following original contributions: 1) the dp-based decoding algorithm in (tillmann and ney, 2003) <papid> J03-1005 </papid>is extended informal way to handle phrases and novel pruning strategy with increased translation speed is presented 2) novel alignment algorithm is presented that computes phrase alignment efficiently in the case that itis consistent with an underlying word alignment.</prevsent>
</prevsection>
<citsent citstr=" J99-4005 ">
under certain restrictions, both algorithms handle mt-related problems efficiently that are generally np complete (knight, 1999).<papid> J99-4005 </papid></citsent>
<aftsection>
<nextsent>this paper deals with dynamic programming based decoding and alignment algorithms for phrase-based smt.dynamic programming based search algorithms are being used in speech recognition (jelinek, 1998; ney et al ., 1992) as well as in statistical machine translation (tillmann et al , 1997; <papid> P97-1037 </papid>niessen et al , 1998; tillmann and ney, 2003).<papid> J03-1005 </papid></nextsent>
<nextsent>here, the decoding algorithms are described as shortest path finding algorithms in regularly structured search graphs or search grids.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1860">
<title id=" W06-3602.xml">efficient dynamic programming search algorithms for phrase based smt </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the paper contains the following original contributions: 1) the dp-based decoding algorithm in (tillmann and ney, 2003) <papid> J03-1005 </papid>is extended informal way to handle phrases and novel pruning strategy with increased translation speed is presented 2) novel alignment algorithm is presented that computes phrase alignment efficiently in the case that itis consistent with an underlying word alignment.</prevsent>
<prevsent>under certain restrictions, both algorithms handle mt-related problems efficiently that are generally np complete (knight, 1999).<papid> J99-4005 </papid></prevsent>
</prevsection>
<citsent citstr=" P97-1037 ">
this paper deals with dynamic programming based decoding and alignment algorithms for phrase-based smt.dynamic programming based search algorithms are being used in speech recognition (jelinek, 1998; ney et al ., 1992) as well as in statistical machine translation (tillmann et al , 1997; <papid> P97-1037 </papid>niessen et al , 1998; tillmann and ney, 2003).<papid> J03-1005 </papid></citsent>
<aftsection>
<nextsent>here, the decoding algorithms are described as shortest path finding algorithms in regularly structured search graphs or search grids.
</nextsent>
<nextsent>under certain restrictions, e.g. start and end point restrictions for thepath, the shortest path computed corresponds to recognized word sequence or generated target language translation.
</nextsent>
<nextsent>in these algorithms, shortest-path search ({1},1) ({1,3},3) ({1,2},2) ({1,4},4) ({1,5},5) ({1,2,4},4) ({1,2,3},3) ({1,2,5},5) ({1,3,4},4) ({1,2,3},2) ({1,3,5},5) ({1,3,4},3) ({1,2,4},2) ({1,4,5},5) ({1,3,5},3) ({1,2,5},2) ({1,4,5},4) ({1,2,3,5},5) ({1,2,4,5},5) ({1,3,4,5},5) ({1,2,3,4},4) ({1,2,4,5},4) ({1,3,4,5},4) ({1,2,3,4},3) ({1,2,3,5},3) ({1,3,4,5},3) ({1,2,3,4},2) ({1,2,3,5},2) ({1,2,4,5},2) ({1,2,3,4,5},2) ({1,2,3,4,5},3) ({1,2,3,4,5},4) ({1,2,3,4,5},5) final figure 1: illustration of dp-based algorithm to solve traveling salesman problem with   cities.
</nextsent>
<nextsent>the visited cities correspond to processed source positions.is carried out in one pass over some input along specific direction?: in speech recognition the search is time synchronous, the single-word based search algorithm in (tillmann et al , 1997) <papid> P97-1037 </papid>is (source) position-synchronous or left-to-right, the search algorithm in (niessen et al , 1998) is (target) position-synchronous or bottom-to-top, and the search algorithm in (tillmann and ney, 2003) <papid> J03-1005 </papid>is so-called cardinality-synchronous.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1876">
<title id=" W06-3602.xml">efficient dynamic programming search algorithms for phrase based smt </title>
<section> efficient block alignment algorithm.  </section>
<citcontext>
<prevsection>
<prevsent>efficient search algorithms canbe derived by the restricting the allowable coverage vectors (tillmann, 2001) to local word re-ordering only.
</prevsent>
<prevsent>an efficient phrase alignment method that does not make use of re-ordering restriction is demonstrated in the following section.
</prevsent>
</prevsection>
<citsent citstr=" J04-4002 ">
a common approach to phrase-based smt is to learn phrasal translation pairs from word-aligned training data (och and ney, 2004).<papid> J04-4002 </papid></citsent>
<aftsection>
<nextsent>here, word alignment ? is asubset of the cartesian product of source and target posi tions: gwq  ffi2ffiaffi ?
</nextsent>
<nextsent>j??{gwq  ffi2ffiaffi 2s j# here, ? is the target sentence length and is the source sentence length.
</nextsent>
<nextsent>the phrase learning approach in (och and ney, 2004) <papid> J04-4002 </papid>takes two alignments: source-to-target alignment ? and target-to-source alignment ? ?</nextsent>
<nextsent>the.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1879">
<title id=" W06-3602.xml">efficient dynamic programming search algorithms for phrase based smt </title>
<section> efficient block alignment algorithm.  </section>
<citcontext>
<prevsection>
<prevsent>intersection of these two alignments is computed to obtain high-precision word alignment.
</prevsent>
<prevsent>here, we note that if the intersection covers all source and target positions (as shown in fig.
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
4), it constitutes bijection between source and target sentence positions, since the intersecting alignments are functions according to their definition in (brown et al , 1993) <papid> J93-2003 </papid>3.</citsent>
<aftsection>
<nextsent>in this paper, an algorithmic justification for restricting blocks based on word alignments is given.
</nextsent>
<nextsent>we assume that source and target sentence are given, and the task is to compute the lowest scoring block alignment.
</nextsent>
<nextsent>such an algorithm might be important in some discriminative training procedure that relies on decoding the training data efficiently.
</nextsent>
<nextsent>to restrict the block selection based on word aligned training data, interval projection functions are defined as follows 4: ? is source interval and ? is an target inter 3(tillmann, 2003) <papid> W03-1001 </papid>reports an intersection coverage of about ???</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1880">
<title id=" W06-3602.xml">efficient dynamic programming search algorithms for phrase based smt </title>
<section> efficient block alignment algorithm.  </section>
<citcontext>
<prevsection>
<prevsent>we assume that source and target sentence are given, and the task is to compute the lowest scoring block alignment.
</prevsent>
<prevsent>such an algorithm might be important in some discriminative training procedure that relies on decoding the training data efficiently.
</prevsent>
</prevsection>
<citsent citstr=" W03-1001 ">
to restrict the block selection based on word aligned training data, interval projection functions are defined as follows 4: ? is source interval and ? is an target inter 3(tillmann, 2003) <papid> W03-1001 </papid>reports an intersection coverage of about ???</citsent>
<aftsection>
<nextsent>% for arabic-english parallel data, and coverage of a?% for chinese-english data.
</nextsent>
<nextsent>in the case of un complete coverage, the current algorithm can be extended as described in section 4.1.
</nextsent>
<nextsent>4(och and ney, 2004) <papid> J04-4002 </papid>defines the notion of consistency for the set of phrasal translations that are learned from word figure 3: following the definition in eq.</nextsent>
<nextsent>6, the left picture shows three admissible block links while the right picture shows three non-admissible block links.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1887">
<title id=" W06-3602.xml">efficient dynamic programming search algorithms for phrase based smt </title>
<section> beam-search results.  </section>
<citcontext>
<prevsection>
<prevsent>, distortion model   ??
</prevsent>
<prevsent>as well as the negative target phrase length  $w . the transition cost is computed as       ? ff fi ffiy     , where ff ???
</prevsent>
</prevsection>
<citsent citstr=" P03-1021 ">
is weight vector that sums up to qy# : ?  ff ?q# . the weights are trained using procedure similar to (och, 2003) <papid> P03-1021 </papid>on held-out test data.</citsent>
<aftsection>
<nextsent>a block set of ??#   million blocks, which are not filtered according to any particular test set is used, which has been generated by phrase-pair selection algorithm similar to (al-onaizan et al , 2004).
</nextsent>
<nextsent>the training data issentence-aligned consisting of n?# million training sentence pairs.
</nextsent>
<nextsent>beam-search results are presented in terms of two pruning thresholds: the coverage pruning threshold ? ?
</nextsent>
<nextsent>and the cardinality pruning threshold ? _ (tillmann and ney, 2003).<papid> J03-1005 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1891">
<title id=" W06-3602.xml">efficient dynamic programming search algorithms for phrase based smt </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>(knight, 1999) <papid> J99-4005 </papid>shows that the decoding problem for smt as well as some bilingual tiling problems arenp-complete, so no efficient algorithm exists in the general case.</prevsent>
<prevsent>but using dp-based optimization techniques and appropriate restrictions leads to efficient dp-based decoding algorithms as shown in this paper.</prevsent>
</prevsection>
<citsent citstr=" J97-3002 ">
the efficient block alignment algorithm in section 4 is related to the inversion transduction grammar approach to bilingual parsing described in (wu, 1997): <papid> J97-3002 </papid>in both cases the number of alignments is drastically reduced by introducing appropriate re-ordering restrictions.</citsent>
<aftsection>
<nextsent>the list based decoding algorithms can also be compared to an earley-style parsing algorithm that processes list of parse states in single left-to-right run over the input sentence.for this algorithm, the comparison in terms of shortest path algorithm is less obvious: in the so-called completion step the parser re-visits states in previous stacks.
</nextsent>
<nextsent>butit is interesting to note that there is no multiple lists variant of that parser.
</nextsent>
<nextsent>in phrase-based decoding, multiple list decoder is feasible only because exact phrase matches occur.
</nextsent>
<nextsent>a block decoding algorithm that would allow fora fuzzy?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1892">
<title id=" W07-0103.xml">hunting elusive metaphors using lexical resources </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>abstractions and enormously complex situations are routinely understood via metaphors (lakoff and johnson, 1980).
</prevsent>
<prevsent>metaphors begin their lives as novel poetic creations with marked rhetoric effects whose comprehension requires special imaginative leap.
</prevsent>
</prevsection>
<citsent citstr=" T87-1040 ">
as time goes by, they become part of general use and their comprehension becomes automatic and idiomatic and rhetoric effect is dulled (nunberg,1987).<papid> T87-1040 </papid></citsent>
<aftsection>
<nextsent>we term such metaphors whose idiomatic effects are dulled because of common usage as dead metaphors while metaphors with novel usages as live metaphors.
</nextsent>
<nextsent>in this paper we are interested only in identifying live metaphors.
</nextsent>
<nextsent>the first author is currently affiliated with google inc, mountain view, ca.
</nextsent>
<nextsent>metaphors have interesting applications in manynlp problems like machine translation, text summarization, information retrieval and question answering.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1893">
<title id=" W07-0103.xml">hunting elusive metaphors using lexical resources </title>
<section> noun-form metaphors.  </section>
<citcontext>
<prevsection>
<prevsent>we use hyponym together with word co-occurrence information, in this case bigrams from the web 1t corpus (brants and franz, 2006).
</prevsent>
<prevsent>sections 3.1 and 3.2 discuss the two algorithms, respectively.
</prevsent>
</prevsection>
<citsent citstr=" P03-1054 ">
we use parser (klein and manning, 2003) <papid> P03-1054 </papid>to obtain the relationships between nouns, verbs and adjectives in sentence.</citsent>
<aftsection>
<nextsent>3.1 identifying type metaphors.
</nextsent>
<nextsent>we identify the wordnet hyponym relationship (or the lack thereof) between the subject and the object in type sentence.
</nextsent>
<nextsent>we classify the sentence as metaphoric, if the subject and object does not havea hyponym relation.
</nextsent>
<nextsent>a hyponym relation exists between pair of words if and only if one word is asubclass of another word.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1894">
<title id=" W07-0103.xml">hunting elusive metaphors using lexical resources </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>there has been long history of research in metaphors.
</prevsent>
<prevsent>we briefly review some of them here.one thing that sets our work apart is that most previous literatures in this area tend to give little empirical evaluation of their approaches.
</prevsent>
</prevsection>
<citsent citstr=" J83-3004 ">
in contrast, inthis study we provide detailed analysis of the effectiveness of our approaches.(fass and wilks, 1983) <papid> J83-3004 </papid>proposes the use of preference semantics for metaphor recognition.</citsent>
<aftsection>
<nextsent>techniques for automatically detecting selections preferences have been discussed in (mccarthy and carrol, 2003) <papid> J03-4004 </papid>and (resnik, 1997).<papid> W97-0209 </papid></nextsent>
<nextsent>type ii and type iii approaches discussed in this paper uses both these ideas for detecting live metaphors.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1895">
<title id=" W07-0103.xml">hunting elusive metaphors using lexical resources </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>we briefly review some of them here.one thing that sets our work apart is that most previous literatures in this area tend to give little empirical evaluation of their approaches.
</prevsent>
<prevsent>in contrast, inthis study we provide detailed analysis of the effectiveness of our approaches.(fass and wilks, 1983) <papid> J83-3004 </papid>proposes the use of preference semantics for metaphor recognition.</prevsent>
</prevsection>
<citsent citstr=" J03-4004 ">
techniques for automatically detecting selections preferences have been discussed in (mccarthy and carrol, 2003) <papid> J03-4004 </papid>and (resnik, 1997).<papid> W97-0209 </papid></citsent>
<aftsection>
<nextsent>type ii and type iii approaches discussed in this paper uses both these ideas for detecting live metaphors.
</nextsent>
<nextsent>fass (fass, 1991) <papid> J91-1003 </papid>uses selectional preference violation technique to detect metaphors.</nextsent>
<nextsent>however they relyon hand-codeddeclarative knowledge bases.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1896">
<title id=" W07-0103.xml">hunting elusive metaphors using lexical resources </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>we briefly review some of them here.one thing that sets our work apart is that most previous literatures in this area tend to give little empirical evaluation of their approaches.
</prevsent>
<prevsent>in contrast, inthis study we provide detailed analysis of the effectiveness of our approaches.(fass and wilks, 1983) <papid> J83-3004 </papid>proposes the use of preference semantics for metaphor recognition.</prevsent>
</prevsection>
<citsent citstr=" W97-0209 ">
techniques for automatically detecting selections preferences have been discussed in (mccarthy and carrol, 2003) <papid> J03-4004 </papid>and (resnik, 1997).<papid> W97-0209 </papid></citsent>
<aftsection>
<nextsent>type ii and type iii approaches discussed in this paper uses both these ideas for detecting live metaphors.
</nextsent>
<nextsent>fass (fass, 1991) <papid> J91-1003 </papid>uses selectional preference violation technique to detect metaphors.</nextsent>
<nextsent>however they relyon hand-codeddeclarative knowledge bases.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1897">
<title id=" W07-0103.xml">hunting elusive metaphors using lexical resources </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>techniques for automatically detecting selections preferences have been discussed in (mccarthy and carrol, 2003) <papid> J03-4004 </papid>and (resnik, 1997).<papid> W97-0209 </papid></prevsent>
<prevsent>type ii and type iii approaches discussed in this paper uses both these ideas for detecting live metaphors.</prevsent>
</prevsection>
<citsent citstr=" J91-1003 ">
fass (fass, 1991) <papid> J91-1003 </papid>uses selectional preference violation technique to detect metaphors.</citsent>
<aftsection>
<nextsent>however they relyon hand-codeddeclarative knowledge bases.
</nextsent>
<nextsent>our technique depends only on wordnet and we use selection preference violation based on the knowledge learned from the bigram frequencies on the web.
</nextsent>
<nextsent>markert and nissim (markert and nissim, 2002)<papid> W02-1027 </papid>presents supervised classification algorithm for resolving metonymy.</nextsent>
<nextsent>metonymy is closely related figure of speech to metaphors where word is substituted by another with which it is associated.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1898">
<title id=" W07-0103.xml">hunting elusive metaphors using lexical resources </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>however they relyon hand-codeddeclarative knowledge bases.
</prevsent>
<prevsent>our technique depends only on wordnet and we use selection preference violation based on the knowledge learned from the bigram frequencies on the web.
</prevsent>
</prevsection>
<citsent citstr=" W02-1027 ">
markert and nissim (markert and nissim, 2002)<papid> W02-1027 </papid>presents supervised classification algorithm for resolving metonymy.</citsent>
<aftsection>
<nextsent>metonymy is closely related figure of speech to metaphors where word is substituted by another with which it is associated.
</nextsent>
<nextsent>example, penis mightier than sword.
</nextsent>
<nextsent>here sword is metonymy for war and penis ametonymy for articles.
</nextsent>
<nextsent>they use collocation, cooccurrence and grammatical features in their classification algorithm.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1899">
<title id=" W07-0103.xml">hunting elusive metaphors using lexical resources </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>and lexical markers like literally?, illustrating?, metaphorically?
</prevsent>
<prevsent>etc. these would be useful for identifying simile and explicit metaphoric relations but not metaphors where the relation between the target concept and the source concept is not explicit.
</prevsent>
</prevsection>
<citsent citstr=" J04-1002 ">
the cormet system (mason, 2004) <papid> J04-1002 </papid>dynamically mines domain specific corpora to find less frequent 19usages and identifies conceptual metaphors.</citsent>
<aftsection>
<nextsent>however the system is limited to extracting only selectional preferences of verbs.
</nextsent>
<nextsent>verbal selectional preference is the verbs preference for the type of argument it takes.
</nextsent>
<nextsent>dolan (dolan, 1995) uses the path and path length between words in the knowledge base derived from lexical resources for interpreting the interrelationship between the component parts of metaphor.
</nextsent>
<nextsent>the effectiveness of this technique relies on whether the metaphoric sense is encoded in the dictionaries.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1900">
<title id=" W06-2105.xml">semantic interpretation of prepositions for nlp applications </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(3) prepositions often occur in collocations, where their interpretation is irregular.
</prevsent>
<prevsent>although large amount of work in the nlp community has focused on resolving attachment ambiguities, there are only first steps towardsa systematic description of preposition semantics which has sufficient coverage for nlp applications (litkowski and hargraves, 2005; saint dizier, 2005).
</prevsent>
</prevsection>
<citsent citstr=" W02-0802 ">
the automatic interpretation of prepositions in english has been tackled, for example, by litkowski (2002), <papid> W02-0802 </papid>who presents handcrafted disambiguation rules, and ohara and wiebe (2003), <papid> W03-0411 </papid>who propose statistical approach based on collocations.</citsent>
<aftsection>
<nextsent>however, in order to be applicable for semantic inference, the representation of preposition semantics should ideally be integrated within full-fledged knowledge representation formalism.
</nextsent>
<nextsent>inspite of the broad linguistic investigation son preposition semantics,1 the corresponding results have seldom found their way into real nlp applications.
</nextsent>
<nextsent>information retrieval systems, onthe other hand, which claim to use nlp techniques often do not cope with the semantic content of prepositions at all (even if they bear theterm semantic in their title, as with latent semantic analysis (letsche and berry, 1997)).
</nextsent>
<nextsent>in many cases prepositions are even dropped as stop words in such systems.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1901">
<title id=" W06-2105.xml">semantic interpretation of prepositions for nlp applications </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(3) prepositions often occur in collocations, where their interpretation is irregular.
</prevsent>
<prevsent>although large amount of work in the nlp community has focused on resolving attachment ambiguities, there are only first steps towardsa systematic description of preposition semantics which has sufficient coverage for nlp applications (litkowski and hargraves, 2005; saint dizier, 2005).
</prevsent>
</prevsection>
<citsent citstr=" W03-0411 ">
the automatic interpretation of prepositions in english has been tackled, for example, by litkowski (2002), <papid> W02-0802 </papid>who presents handcrafted disambiguation rules, and ohara and wiebe (2003), <papid> W03-0411 </papid>who propose statistical approach based on collocations.</citsent>
<aftsection>
<nextsent>however, in order to be applicable for semantic inference, the representation of preposition semantics should ideally be integrated within full-fledged knowledge representation formalism.
</nextsent>
<nextsent>inspite of the broad linguistic investigation son preposition semantics,1 the corresponding results have seldom found their way into real nlp applications.
</nextsent>
<nextsent>information retrieval systems, onthe other hand, which claim to use nlp techniques often do not cope with the semantic content of prepositions at all (even if they bear theterm semantic in their title, as with latent semantic analysis (letsche and berry, 1997)).
</nextsent>
<nextsent>in many cases prepositions are even dropped as stop words in such systems.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1902">
<title id=" W06-2105.xml">semantic interpretation of prepositions for nlp applications </title>
<section> preposition interpretation within.  </section>
<citcontext>
<prevsection>
<prevsent>(hirst, 1987).
</prevsent>
<prevsent>the prepositions complement, and the semantics of the possible syntactic head.
</prevsent>
</prevsection>
<citsent citstr=" W99-0614 ">
if no exact matchis found in the disambiguation statistics the number of considered alternatives and the granularity of the description of an alternative are reduced by backing off in these two orthogonal dimensions;see (hartrumpf, 2003; hartrumpf, 1999) <papid> W99-0614 </papid>forde tails.</citsent>
<aftsection>
<nextsent>6.1 intrinsic evaluation.
</nextsent>
<nextsent>experiments showed that 24.2% of verb complement interpretations are equally well produced by adjunct rules (see column 5 in table 3).
</nextsent>
<nextsent>nevertheless the pp interpretation disambiguation task profits from complement information because inmany of these overlap cases more than one pp interpretation was possible.
</nextsent>
<nextsent>also the pp attachment disambiguation task can benefit from the complement vs. adjunct distinction because complement hood is strong indicator of the correct attachmentplace.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1903">
<title id=" W06-3318.xml">recognizing nested named entities in genia corpus </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>1.1 related work.
</prevsent>
<prevsent>overall, our work is an application of machine learning methods to biomedical ner.
</prevsent>
</prevsection>
<citsent citstr=" W04-1221 ">
while most of earlier approaches relyon handcrafted rules or dictionaries, many recent works adopt machine learning approaches, e.g, svm (lee, 2003), hmm (zhou, 2004), maximum entropy (lin, 2004) and crf (settles,2004), <papid> W04-1221 </papid>especially with the availability of annotated corpora such as genia, achieving state-of-the-art performance.</citsent>
<aftsection>
<nextsent>we know only one work (zhou,2004) that deals with nested nes to improve the overall ner performance.
</nextsent>
<nextsent>however, their approach is basically rule-based and they did not report how well the nested nes are recognized.
</nextsent>
<nextsent>we use svm-light (http://svmlight.joachims.org/) to train binary classifier on the genia corpus.
</nextsent>
<nextsent>2.1 dataset.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1904">
<title id=" W07-0605.xml">i will shoot your shopping down and you can shoot all my tins automatic lexical acquisition from the childes database </title>
<section> abstract </section>
<citcontext>
<prevsection>
<prevsent>empirical data regarding the syntactic complexity of childrens speech is important for theories of language acquisition.
</prevsent>
<prevsent>currently much of this data is absent in the annotated versions of the childes database.
</prevsent>
</prevsection>
<citsent citstr=" P07-1115 ">
in thisperliminary study, we show that state-of the-art subcategorization acquisition system ofpreiss et al (2007) <papid> P07-1115 </papid>can be used to extract largescale subcategorization (frequency) information from the (i) child and (ii) child-directed speech within the childes database without any domain-specific tuning.</citsent>
<aftsection>
<nextsent>we demonstrate that the acquired information is sufficiently accurate to confirm and extend previously reported research findings.
</nextsent>
<nextsent>we also report qualitative results which can be used to further improve parsing and lexical acquisition technology for child language data in the future.
</nextsent>
<nextsent>large empirical data containing childrens speech arethe key to developing and evaluating different theories of child language acquisition (cla).
</nextsent>
<nextsent>particularly important are data related to syntactic complexity of child language since considerable evidence suggests that syntactic information plays central role during language acquisition, e.g.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1906">
<title id=" W07-0605.xml">i will shoot your shopping down and you can shoot all my tins automatic lexical acquisition from the childes database </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>they also gather important qualitative and quantitative information, which is difficult for humans to obtain, as side-effect of the acquisition process.
</prevsent>
<prevsent>for instance, state-of-the-art statistical parsers, e.g.
</prevsent>
</prevsection>
<citsent citstr=" A00-2018 ">
(charniak, 2000; <papid> A00-2018 </papid>briscoe et al, 2006), <papid> P06-4020 </papid>have wide coverage and yield grammatical representations capable of supporting various applications (e.g. summarization, information extraction).</citsent>
<aftsection>
<nextsent>in addition, lexical information (e.g. subcategorization, lexical classes) can now be acquired automatically from parsed data (mccarthy and carroll, 2003; <papid> J03-4004 </papid>schulte im walde,2006; preiss et al, 2007).<papid> P07-1115 </papid></nextsent>
<nextsent>this information complements the basic grammatical analysis and provides access to the underlying predicate-argument structure.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1907">
<title id=" W07-0605.xml">i will shoot your shopping down and you can shoot all my tins automatic lexical acquisition from the childes database </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>they also gather important qualitative and quantitative information, which is difficult for humans to obtain, as side-effect of the acquisition process.
</prevsent>
<prevsent>for instance, state-of-the-art statistical parsers, e.g.
</prevsent>
</prevsection>
<citsent citstr=" P06-4020 ">
(charniak, 2000; <papid> A00-2018 </papid>briscoe et al, 2006), <papid> P06-4020 </papid>have wide coverage and yield grammatical representations capable of supporting various applications (e.g. summarization, information extraction).</citsent>
<aftsection>
<nextsent>in addition, lexical information (e.g. subcategorization, lexical classes) can now be acquired automatically from parsed data (mccarthy and carroll, 2003; <papid> J03-4004 </papid>schulte im walde,2006; preiss et al, 2007).<papid> P07-1115 </papid></nextsent>
<nextsent>this information complements the basic grammatical analysis and provides access to the underlying predicate-argument structure.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1908">
<title id=" W07-0605.xml">i will shoot your shopping down and you can shoot all my tins automatic lexical acquisition from the childes database </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for instance, state-of-the-art statistical parsers, e.g.
</prevsent>
<prevsent>(charniak, 2000; <papid> A00-2018 </papid>briscoe et al, 2006), <papid> P06-4020 </papid>have wide coverage and yield grammatical representations capable of supporting various applications (e.g. summarization, information extraction).</prevsent>
</prevsection>
<citsent citstr=" J03-4004 ">
in addition, lexical information (e.g. subcategorization, lexical classes) can now be acquired automatically from parsed data (mccarthy and carroll, 2003; <papid> J03-4004 </papid>schulte im walde,2006; preiss et al, 2007).<papid> P07-1115 </papid></citsent>
<aftsection>
<nextsent>this information complements the basic grammatical analysis and provides access to the underlying predicate-argument structure.
</nextsent>
<nextsent>containing considerable ellipsis and error, spoken child language can be challenging for current nlp techniques which are typically optimized for written adult language.
</nextsent>
<nextsent>yet sagae et al (2005) <papid> P05-1025 </papid>have recently demonstrated that existing statistical parsing techniques can be usefully modified to analyse childes 33with promising accuracy.</nextsent>
<nextsent>although further improvements are still required for optimal accuracy, this research has opened up the exciting possibility of automatic grammatical annotation of the entire childes database in the future.however, no work has yet been conducted on automatic acquisition of lexical information from child speech.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1910">
<title id=" W07-0605.xml">i will shoot your shopping down and you can shoot all my tins automatic lexical acquisition from the childes database </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this information complements the basic grammatical analysis and provides access to the underlying predicate-argument structure.
</prevsent>
<prevsent>containing considerable ellipsis and error, spoken child language can be challenging for current nlp techniques which are typically optimized for written adult language.
</prevsent>
</prevsection>
<citsent citstr=" P05-1025 ">
yet sagae et al (2005) <papid> P05-1025 </papid>have recently demonstrated that existing statistical parsing techniques can be usefully modified to analyse childes 33with promising accuracy.</citsent>
<aftsection>
<nextsent>although further improvements are still required for optimal accuracy, this research has opened up the exciting possibility of automatic grammatical annotation of the entire childes database in the future.however, no work has yet been conducted on automatic acquisition of lexical information from child speech.
</nextsent>
<nextsent>the only automatic lexical acquisition study involving childes that we are aware of is that of buttery and korhonen (2005).
</nextsent>
<nextsent>the study involved extracting subcategorization information from (some of) the adult (child-directed) speech in the database,and showing that this information differs from that extracted from the spoken part of the british national corpus (bnc) (burnard, 1995).in this paper, we investigate whether state-of-the art subcategorization acquisition technology can be used without any domain-specific tuning to obtainlarge-scale verb subcategorization frequency information from childes which is accurate enough to show differences and similarities between child and adult speech, and thus be able to provide support for syntactic complexity studies in cla.
</nextsent>
<nextsent>we use the new system of preiss et al (2007) <papid> P07-1115 </papid>to extract scf frequency data from the (i) child and (ii) child-directed speech within childes.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1913">
<title id=" W07-0605.xml">i will shoot your shopping down and you can shoot all my tins automatic lexical acquisition from the childes database </title>
<section> subcategorization acquisition system.  </section>
<citcontext>
<prevsection>
<prevsent>in particular, we demonstrate that children and adults have different preferences for certain types of verbs, andthat these preferences seem to influence the way children acquire subcategorization.
</prevsent>
<prevsent>in addition, we report qualitative results which can be used to further im prove parsing and lexical acquisition technology for spoken child language data in the future.
</prevsent>
</prevsection>
<citsent citstr=" A97-1052 ">
we used for subcategorization acquisition the new system of preiss, briscoe and korhonen (2007) which is essentially much improved and extended version of briscoe and carrolls (1997) <papid> A97-1052 </papid>system.</citsent>
<aftsection>
<nextsent>it incorporates 168 scf distinctions, superset of those found in the comlex syntax (grishman et al, 1994) <papid> C94-1042 </papid>and anlt (boguraev et al, 1987) dictionaries.</nextsent>
<nextsent>currently,scfs abstract over specific lexically governed particles and prepositions and specific predicate selectional preferences but include some derived semi-predictable bounded dependency constructions, such as particle and dative movement this will be revised in future versions of the scf system.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1914">
<title id=" W07-0605.xml">i will shoot your shopping down and you can shoot all my tins automatic lexical acquisition from the childes database </title>
<section> subcategorization acquisition system.  </section>
<citcontext>
<prevsection>
<prevsent>in addition, we report qualitative results which can be used to further im prove parsing and lexical acquisition technology for spoken child language data in the future.
</prevsent>
<prevsent>we used for subcategorization acquisition the new system of preiss, briscoe and korhonen (2007) which is essentially much improved and extended version of briscoe and carrolls (1997) <papid> A97-1052 </papid>system.</prevsent>
</prevsection>
<citsent citstr=" C94-1042 ">
it incorporates 168 scf distinctions, superset of those found in the comlex syntax (grishman et al, 1994) <papid> C94-1042 </papid>and anlt (boguraev et al, 1987) dictionaries.</citsent>
<aftsection>
<nextsent>currently,scfs abstract over specific lexically governed particles and prepositions and specific predicate selectional preferences but include some derived semi-predictable bounded dependency constructions, such as particle and dative movement this will be revised in future versions of the scf system.
</nextsent>
<nextsent>the system tokenizes, tags, lemmatizes and parses input sentences using the recent (second) release of the rasp (robust accurate statistical parsing) system (briscoe et al, 2006) <papid> P06-4020 </papid>which parses arbitrary english text with state-of-the-art levels of accuracy.</nextsent>
<nextsent>scfs are extracted from the grammatical relations (grs) output of the parser using rule-based classifier.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1916">
<title id=" W07-0605.xml">i will shoot your shopping down and you can shoot all my tins automatic lexical acquisition from the childes database </title>
<section> analysis.  </section>
<citcontext>
<prevsection>
<prevsent>4.1 methods for analysis.
</prevsent>
<prevsent>the similarity between verb and scf distributions inthe lexicons was examined.
</prevsent>
</prevsection>
<citsent citstr=" W02-2014 ">
to maintain robust analysis in the presence of noise, multiple similarity measures were used to compare the verb and scf distributions (korhonen and krymolowski, 2002).<papid> W02-2014 </papid></citsent>
<aftsection>
<nextsent>in the following = (p ) and = (q ) where i and i are the probabilities associated with scf in distributions (lexicons) and q: ? intersection (is) - the intersection of non-zero probability scfs in and q; ? spearman rank correlation (rc) - lies in the range [1; 1], with values near 0 denoting low degree of association and values near -1 and 1 denoting strong association; ? kullback-leibler (kl) distance - measure of the additional information needed to describe using q, kl is always ? 0 and = 0 only when ? q; the scfs distributions acquired from the corpora for the chosen words were evaluated against: (i) gold standard scf lexicon created by merging the scfs inthe comlex and anlt syntax dictionaries this enabled us to determine the accuracy of the acquired scfs; (ii) another acquired scf lexicon (as if it werea gold standard)this enabled us to determine similarity of scf types between two lexicons.
</nextsent>
<nextsent>in each case verb child cds go 1 1 want 2 2 get 3 3 know 4 4 put 5 6 see 6 5 come 7 10 like 8 7 make 9 11 say 10 8 take 11 13 eat 12 14 play 13 15 need 14 16 look 15 12 fall 16 22 sit 17 21 think 18 9 break 19 27 give 20 17 table 1: ranks of the 20 most frequent verbs in child and in cds we recorded the number of true positives (tps), correct scfs, false positives (fps), incorrect scfs, and false negatives (fns), correct scfs not in the gold standard.
</nextsent>
<nextsent>using these counts, we calculated type precision (the percentage of scf types in the acquired lexicon which are correct), type recall (the percentage of scf types in the gold standard that are in the lexicon) and f-measure: = 2 ? precision ? recall precision + recall (1) 4.2 verb analysis.
</nextsent>
<nextsent>before conducting the scf comparisons we first compared (i) our 161 test verbs and (ii) all the 1212 common verbs and their frequencies in child and cds using the spearman rank correlation (rc) and the kullback-leibler distance (kl).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1917">
<title id=" W06-1802.xml">linguistic knowledge and question answering </title>
<section> off-line answer extraction.  </section>
<citcontext>
<prevsection>
<prevsent>or what is mtv?).
</prevsent>
<prevsent>off-line extraction of answers to frequent question types can be based on dependency patterns and coreference resolution (bouma et al, 2005; mur and vander plas, 2006), leading to higher recall (compared to systems using surface patterns).
</prevsent>
</prevsection>
<citsent citstr=" W06-2609 ">
closed-domain (medical) qa can benefit from the fact that dependency relations allow answers to be identified for questions which are not restricted to specific named entity classes, i.e. definitions, causes, symptoms, etc. answering definition questions, for instance, is task which has motivated approaches that go well beyond the techniques used for answering factoid questions.in fahmi and bouma (2006) <papid> W06-2609 </papid>it is shown that syntactic patterns can be used to extract potential definition sentences from wikipedia, and that syntactic features of these sentences (in combination with obvious clues such as the position of the sentence in the document) can be used to improve the accuracy of an automatic classifier which distinguishes definitions from non-definitions in the extracted data set.</citsent>
<aftsection>
<nextsent>joost is qa system for dutch which incorporates the features mentioned above, using the alpino parser for dutch to parse (offline) the document collections as well as (interactively) user questions.
</nextsent>
<nextsent>it has been used for the open-domain monolingual qa task of clef 2005, as well as for closed domain medical qa.
</nextsent>
<nextsent>for clef, the full dutch text collection (4 years of newspaper text, approximately 80 million words) has been parsed.
</nextsent>
<nextsent>for the medical qa system, we have been usinga mixture of texts from general and medical encyclopedia, medical reference works, and web pages 2 kraq06.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1918">
<title id=" W06-1803.xml">interpretation and generation in a knowledge based tutorial system </title>
<section> discussion and related work.  </section>
<citcontext>
<prevsection>
<prevsent>however,once the misconceptions are diagnosed, the remediation is done via kcds, with very limited language input and pre-authored responses, and without allowing students to ask questions.
</prevsent>
<prevsent>our approach attempts to address issues which arise in making remediation more flexible and dependent on context, while still relying on wide-coverage language interpretation and generation.the issues we encountered in integrating compositional interpretation and reference resolution with efficient knowledge representation is similar to known problem in natural language interfaces to databases which may contain slots with complex meanings.
</prevsent>
</prevsection>
<citsent citstr=" P86-1036 ">
(stallard, 1986) <papid> P86-1036 </papid>solves this problem by providing inference schemas linkingcomplex-valued slots with compositional representations.</citsent>
<aftsection>
<nextsent>our solution in mapping domain independent to domain-specific representation is similar, but stricter compositionality is needed for reference resolution support, placing additional constraints on knowledge engineering as we discussed in section 4.
</nextsent>
<nextsent>we glossed over the interpretation issues related to metonymy and other imprecise formulations in questions (aleven et al, 2002).
</nextsent>
<nextsent>a taxonomy of imprecise manual question encodings by domain experts is presented in (fan and porter, 2004).
</nextsent>
<nextsent>they also propose an algorithm to address loosely encoded questions using onto logical knowledge.this algorithm in effect performs question interpretation, and we are planning to incorporate itinto our interpretation mechanism to help interpret question representations obtained automatically during language interpretation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1919">
<title id=" W06-1517.xml">extended cross serial dependencies in tree adjoining grammars </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we are particularly interested in formalisms that induce only well-nested dependency structures.
</prevsent>
<prevsent>this interest is motivated by two observations:first, well-nestedness is interesting as generalization of projectivity (marcus, 1967)while more than 23% of the 73 088 dependency structures inthe prague dependency treebank of czech (ha jic?
</prevsent>
</prevsection>
<citsent citstr=" P06-2066 ">
et al, 2001) are non-projective, only 0.11% are not well-nested (kuhlmann and nivre, 2006).<papid> P06-2066 </papid>second, well-nestedness is interesting for process ing.</citsent>
<aftsection>
<nextsent>specifically, parsers for well-nested grammar formalisms are not confronted with the crossingconfigurations?
</nextsent>
<nextsent>that make the universal recognition problem of linear context-free rewriting systems np-complete (satta, 1992).<papid> P92-1012 </papid></nextsent>
<nextsent>in summary, it appears that well-nestedness can strike successful balance between empirical coverage and computational tractability.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1920">
<title id=" W06-1517.xml">extended cross serial dependencies in tree adjoining grammars </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>et al, 2001) are non-projective, only 0.11% are not well-nested (kuhlmann and nivre, 2006).<papid> P06-2066 </papid>second, well-nestedness is interesting for process ing.</prevsent>
<prevsent>specifically, parsers for well-nested grammar formalisms are not confronted with the crossingconfigurations?</prevsent>
</prevsection>
<citsent citstr=" P92-1012 ">
that make the universal recognition problem of linear context-free rewriting systems np-complete (satta, 1992).<papid> P92-1012 </papid></citsent>
<aftsection>
<nextsent>in summary, it appears that well-nestedness can strike successful balance between empirical coverage and computational tractability.
</nextsent>
<nextsent>if this is true, then formalism that has the well-nestedness constraint hard wired is preferable over one that has not.
</nextsent>
<nextsent>the results of this paper can be summarize das follows: derivations in lexicalized multi-com ponent tags (weir, 1988; kallmeyer, 2005), inwhich single adjunction adds set of elementary trees, either induce exactly the same dependency structures as tag, or induce all structures of bounded gap degree, even non-well-nested ones.
</nextsent>
<nextsent>this depends on the decision whether one takes lexicalized?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1921">
<title id=" W06-1517.xml">extended cross serial dependencies in tree adjoining grammars </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this depends on the decision whether one takes lexicalized?
</prevsent>
<prevsent>to mean one lexical anchor per tree?, or one lexical anchor per tree set?.
</prevsent>
</prevsection>
<citsent citstr=" C88-1001 ">
in contrast, multi-foot extensions of tag (abe, 1988; <papid> C88-1001 </papid>hotz and pitsch, 1996), where single elementary tree may have more than one foot node, only induce well-nested dependency structures of bounded gap degree.</citsent>
<aftsection>
<nextsent>thus, from the dependency point of view, they constitute the structurally more conservative extension of tag.
</nextsent>
<nextsent>121
</nextsent>
<nextsent>we start with presentation of the dependency view on tag that constitutes the basis for our work, and introduce the relevant terminology.
</nextsent>
<nextsent>the main objective of this section is to provide intuitions; for the formal details, see bodirsky et al (2005).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1923">
<title id=" W06-1517.xml">extended cross serial dependencies in tree adjoining grammars </title>
<section> summary.  </section>
<citcontext>
<prevsection>
<prevsent>in the former case, mctag has the same structural limits as standard tag; in the latter case, even non-well-nested dependency structures are induced. the multi-foot extension ccfg (and its equivalent rnrg) is restricted to well-nested dependency structures, but in contrast to tag, it can induce structures with any bounded gap degree.
</prevsent>
<prevsent>the rank of grammar is an upper bound on the gap degree of the dependency structures it induces.
</prevsent>
</prevsection>
<citsent citstr=" P01-1018 ">
since the extensions inherent to mctag and ccfg are orthogonal, it is possible to combine them: multi-component multi-foot tag (mmtag)as described by chiang (2001) <papid> P01-1018 </papid>allows to simultaneously adjoin sets of trees, where each tree mayhave multiple foot nodes.</citsent>
<aftsection>
<nextsent>the structural limitations of the dependency structures inducible by mctag and ccfg generalize to mmtag as one would expect.
</nextsent>
<nextsent>as in the case of mctag, there are two different understandings of how dependency structure is induced by an mmtag.
</nextsent>
<nextsent>under the one anchor per component?
</nextsent>
<nextsent>perspective, mm tag, just like ccfg, derives well-nested structures of bounded gap-degree.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1924">
<title id=" W06-3111.xml">partitioning parallel documents using binary segmentation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the exploitation of more bilingual sentences automatically and accurately as wellas the use of these data with the limited computational requirements become crucial problems.
</prevsent>
<prevsent>the conventional method for producing parallel sentences is to break the documents into sentence sand to align these sentences using dynamic programming.
</prevsent>
</prevsection>
<citsent citstr=" J93-1004 ">
previous investigations can be found in works such as (gale and church, 1993) <papid> J93-1004 </papid>and (ma, 2006).</citsent>
<aftsection>
<nextsent>a disadvantage is that only the monotone sentence alignments are allowed.
</nextsent>
<nextsent>another approach is the binary segmentation method described in (simard and langlais, 2003), (<papid> W03-0304 </papid>xu et al, 2005) and (deng et al, 2006), which separates long sentence pair into two sub-pairs recursively.</nextsent>
<nextsent>the binary reordering in alignment is allowed but the segmentation decision is only optimum in each recur sion step.hence, combination of both methods is expected to produce more satisfying result.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1925">
<title id=" W06-3111.xml">partitioning parallel documents using binary segmentation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>previous investigations can be found in works such as (gale and church, 1993) <papid> J93-1004 </papid>and (ma, 2006).</prevsent>
<prevsent>a disadvantage is that only the monotone sentence alignments are allowed.</prevsent>
</prevsection>
<citsent citstr=" W03-0304 ">
another approach is the binary segmentation method described in (simard and langlais, 2003), (<papid> W03-0304 </papid>xu et al, 2005) and (deng et al, 2006), which separates long sentence pair into two sub-pairs recursively.</citsent>
<aftsection>
<nextsent>the binary reordering in alignment is allowed but the segmentation decision is only optimum in each recur sion step.hence, combination of both methods is expected to produce more satisfying result.
</nextsent>
<nextsent>(deng et al, 2006) performs two-stage procedure.
</nextsent>
<nextsent>the documents are first aligned at level using dynamic programming, the initial alignments are then refined to produce shorter segments using binary segmentation.
</nextsent>
<nextsent>but on the chinese-english fbis training corpus, the alignment accuracy and recall are lower than with champollion (ma, 2006).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1926">
<title id=" W06-3111.xml">partitioning parallel documents using binary segmentation </title>
<section> review of the baseline statistical.  </section>
<citcontext>
<prevsection>
<prevsent>word alignment = aj1 , which describes mapping from source position to target position aj . 1the notational convention will be as follows: we use the symbol pr(?)
</prevsent>
<prevsent>to denote general probability distributions with (nearly) no specific assumptions.
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
in contrast, for model-based probability distributions, we use the generic symbol p(?).monotone non monotone target a positions d source positions figure 1: two types of alignment the ibm model 1 (ibm-1) (brown et al, 1993)<papid> J93-2003 </papid>assumes that all alignments have the same probability by using uniform distribution: p(fj1 |ei1) = 1 ij ? j? j=1 i?</citsent>
<aftsection>
<nextsent>i=1 p(fj |ei) (2) we use the ibm-1 to train the lexicon parameters p(f |e), the training software is giza++ (och and ney, 2003).<papid> J03-1002 </papid></nextsent>
<nextsent>to incorporate the context into the translation model, the phrase-based translation approach (zenset al, 2005) is applied.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1927">
<title id=" W06-3111.xml">partitioning parallel documents using binary segmentation </title>
<section> review of the baseline statistical.  </section>
<citcontext>
<prevsection>
<prevsent>to denote general probability distributions with (nearly) no specific assumptions.
</prevsent>
<prevsent>in contrast, for model-based probability distributions, we use the generic symbol p(?).monotone non monotone target a positions d source positions figure 1: two types of alignment the ibm model 1 (ibm-1) (brown et al, 1993)<papid> J93-2003 </papid>assumes that all alignments have the same probability by using uniform distribution: p(fj1 |ei1) = 1 ij ? j? j=1 i?</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
i=1 p(fj |ei) (2) we use the ibm-1 to train the lexicon parameters p(f |e), the training software is giza++ (och and ney, 2003).<papid> J03-1002 </papid></citsent>
<aftsection>
<nextsent>to incorporate the context into the translation model, the phrase-based translation approach (zenset al, 2005) is applied.
</nextsent>
<nextsent>pairs of source and target language phrases are extracted from the bilingual training corpus and beam search algorithm is implemented to generate the translation hypothesis with maximum probability.
</nextsent>
<nextsent>3.1 approach.
</nextsent>
<nextsent>here document or sentence pair (fj1 , ei1) 2 is represented as matrix.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1928">
<title id=" W06-3111.xml">partitioning parallel documents using binary segmentation </title>
<section> translation experiments.  </section>
<citcontext>
<prevsection>
<prevsent>( weight for champollion: 1?
</prevsent>
<prevsent>) bigrams, trigrams and four grams with penalty for too short sentences.
</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
(papineni et al, 2002).<papid> P02-1040 </papid></citsent>
<aftsection>
<nextsent>nist score: this score is similar to bleu, but it uses an arithmetic average of n-gram counts rather than geometric average, and it weights more heavily those n-grams that are more informative.
</nextsent>
<nextsent>(doddington, 2002).
</nextsent>
<nextsent>the bleu and nist scores measure accuracy, i.e. larger scores are better.
</nextsent>
<nextsent>in our evaluation the scores are measured as case insensitive and with respect to multiple references.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1929">
<title id=" W06-3111.xml">partitioning parallel documents using binary segmentation </title>
<section> discussion and future work.  </section>
<citcontext>
<prevsection>
<prevsent>in the binary segmentation method, an incor-.
</prevsent>
<prevsent>rect segmentation results in further mistake sin the segmentation decisions of all its sub segments.
</prevsent>
</prevsection>
<citsent citstr=" J97-3002 ">
an alternative method (wu, 1997)<papid> J97-3002 </papid>makes decisions at the end but has high computational requirement.</citsent>
<aftsection>
<nextsent>a restricted expansion of the search space might better balance segmentation accuracy and the efficiency.
</nextsent>
<nextsent>this work was supported by the european union under the integrated project tc-star (technology and corpora for speech to speech translation, ist-2002-fp6-506738, http://www.tc-star.org) and the defense advanced research projects agency (darpa) under contract no.
</nextsent>
<nextsent>hr0011-06-c-0023.
</nextsent>

</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1930">
<title id=" W06-3112.xml">contextual bitextderived paraphrases in automatic mt evaluation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>using target language paraphrases produced through word and phrase alignment number of alternative reference sentences are constructed automatically for each candidate translation.
</prevsent>
<prevsent>the method produces lexical and low level syntactic paraphrases that are relevant to the domain in hand, does not use external knowledge resources, and can be combined with variety of automatic mt evaluation system.
</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
since their appearance, bleu (papineni et al, 2002) <papid> P02-1040 </papid>and nist (doddington, 2002) have been the standard tools used for evaluating the quality of machine translation.</citsent>
<aftsection>
<nextsent>they both score candidate translations on the basis of the number of n-grams it shares with one or more reference translations provided.
</nextsent>
<nextsent>such automatic measures are indispensable in the development of machine translation systems, because they allow the developers to conduct frequent, cost-effective, and fast evaluations of their evolving models.
</nextsent>
<nextsent>these advantages come at price, though: an automatic comparison of n-grams measures only the string similarity of the candidate translation to one or more reference strings, and will penalize any divergence from them.
</nextsent>
<nextsent>in effect, candidate translation expressing the source meaning accurately and fluently will be given low score if the lexical choices and syntactic structure it contains, even though perfectly legitimate, are not present in at least one of the references.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1931">
<title id=" W06-3112.xml">contextual bitextderived paraphrases in automatic mt evaluation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the remainder of this paper is organized as fol lows: section 2 describes related work; section 3 describes our method and presents examples of derived paraphrases; section 4 presents the results of the comparison between the blue and nist scores for single-reference translation and the same translation using the paraphrases automatically generated from the bitext, as well as the correlations between the scores and human judgment; section 5 discusses ongoing work; section 6 concludes.
</prevsent>
<prevsent>2 2.1 related work word and phrase alignment several researchers noted that the word and phrase alignment used in training translation models in statistical mt can be used for other purposes as well.
</prevsent>
</prevsection>
<citsent citstr=" P02-1033 ">
(diab and resnik, 2002) <papid> P02-1033 </papid>use second language alignments to tag word senses.</citsent>
<aftsection>
<nextsent>working on an assumption that separate senses of l1 word 2.2 1 http://www.lec.com/ 2 http://wordnet.princeton.edu/ can be distinguished by its different translations in l2, they also note that set of possible l2 translations for l1 word may contain many synonyms.
</nextsent>
<nextsent>(bannard and callison-burch, 2005), on the other hand, conduct an experiment to show that paraphrases derived from such alignments can be semantically correct in more than 70% of the cases.
</nextsent>
<nextsent>automatic mt evaluation the insensitivity of bleu and nist to perfectly legitimate variation has been raised, among others, in (callison-burch et al, 2006), but the criticism is widespread.
</nextsent>
<nextsent>even the creators of bleu point out that it may not correlate particularly well with human judgment at the sentence level (pap ineni et al, 2002), <papid> P02-1040 </papid>problem also noted by (och et al., 2003) and (russo-lassner et al, 2005).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1933">
<title id=" W06-3112.xml">contextual bitextderived paraphrases in automatic mt evaluation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>recently number of attempts to remedy these shortcomings have led to the development of other automatic machine translation metrics.
</prevsent>
<prevsent>some of them concentrate mainly on the word reordering aspect, like maximum matching string (turian et al., 2003) or translation error rate (snover et al, 2005).
</prevsent>
</prevsection>
<citsent citstr=" E06-1031 ">
others try to accommodate both syntactic and lexical differences between the candidate translation and the reference, like cder (leusch et al, 2006), <papid> E06-1031 </papid>which employs version of edit distance for word substitution and reordering; meteor (banerjee and lavie, 2005), <papid> W05-0909 </papid>which uses stemming and wordnet synonymy; and linear regression model developed by (russo-lassner et al., 2005), which makes use of stemming, wordnet synonymy, verb class synonymy, matching noun phrase heads, and proper name matching.</citsent>
<aftsection>
<nextsent>a closer examination of these metrics suggests that the accommodation of lexical equivalence is as difficult as the appropriate treatment of syntactic variation, in that it requires considerable external knowledge resources like wordnet, verb class databases, and extensive text preparation: stemming, tagging, etc. the advantage of our method is that it produces relevant paraphrases with nothing more than the evaluation bitext and widely available word and phrase alignment software, and therefore can be used with any existing evaluation metric.
</nextsent>
<nextsent>87 3 contextual bitext-derived paraphrases.
</nextsent>
<nextsent>the method presented in this paper rests on combination of two simple ideas.
</nextsent>
<nextsent>first, the components necessary for automatic mt evaluation like bleu or nist, source text and reference text, constitute miniature parallel corpus, from which word and phrase alignments can be extracted automatically, much like during the training for statistical machine translation system.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1934">
<title id=" W06-3112.xml">contextual bitextderived paraphrases in automatic mt evaluation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>recently number of attempts to remedy these shortcomings have led to the development of other automatic machine translation metrics.
</prevsent>
<prevsent>some of them concentrate mainly on the word reordering aspect, like maximum matching string (turian et al., 2003) or translation error rate (snover et al, 2005).
</prevsent>
</prevsection>
<citsent citstr=" W05-0909 ">
others try to accommodate both syntactic and lexical differences between the candidate translation and the reference, like cder (leusch et al, 2006), <papid> E06-1031 </papid>which employs version of edit distance for word substitution and reordering; meteor (banerjee and lavie, 2005), <papid> W05-0909 </papid>which uses stemming and wordnet synonymy; and linear regression model developed by (russo-lassner et al., 2005), which makes use of stemming, wordnet synonymy, verb class synonymy, matching noun phrase heads, and proper name matching.</citsent>
<aftsection>
<nextsent>a closer examination of these metrics suggests that the accommodation of lexical equivalence is as difficult as the appropriate treatment of syntactic variation, in that it requires considerable external knowledge resources like wordnet, verb class databases, and extensive text preparation: stemming, tagging, etc. the advantage of our method is that it produces relevant paraphrases with nothing more than the evaluation bitext and widely available word and phrase alignment software, and therefore can be used with any existing evaluation metric.
</nextsent>
<nextsent>87 3 contextual bitext-derived paraphrases.
</nextsent>
<nextsent>the method presented in this paper rests on combination of two simple ideas.
</nextsent>
<nextsent>first, the components necessary for automatic mt evaluation like bleu or nist, source text and reference text, constitute miniature parallel corpus, from which word and phrase alignments can be extracted automatically, much like during the training for statistical machine translation system.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1935">
<title id=" W06-3112.xml">contextual bitextderived paraphrases in automatic mt evaluation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>each translation was evaluated by the bleu and nist metrics first with the single reference, then with the multiple references for each sentence using the paraphrases automatically generated from the source-reference mini corpus.
</prevsent>
<prevsent>a subset of 100 sentences was randomly extracted from each test set and evaluated by two independent human judges with respect to accuracy and fluency; the human scores were then compared to the bleu and nist scores for the single-reference and the automatically generated multiple-reference files.
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
word alignment and phrase extraction we used the giza++ word alignment soft ware3 to produce initial word alignments for our miniature bilingual corpus consisting of the source french file and the english reference file, and the refined word alignment strategy of (och and ney, 2003; <papid> J03-1002 </papid>koehn et al, 2003; <papid> N03-1017 </papid>tiedemann, 2004) to obtain improved word and phrase alignments.</citsent>
<aftsection>
<nextsent>for each source word or phrase fi that is aligned with more than one target words or phrases, its possible translations ei1, ..., ein were placed in list as equivalent expressions (i.e. synonyms, near-synonyms, or paraphrases of each other).
</nextsent>
<nextsent>a few examples are given in (1).
</nextsent>
<nextsent>(1) agreement - accordance adopted - implemented matter - lot - case funds - money arms - weapons area - aspect question ? issue ? matter we would expect - we certainly expect bear on - are centred around alignment divides target words and phrases into equivalence sets; each set corresponds to one source word/phrase that was originally aligned with the target elements.
</nextsent>
<nextsent>for example, for the french word cito yens three english words were deemed to be the most appropriate translations: people, public, and citizens; therefore these three words constitute an equivalence set.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1936">
<title id=" W06-3112.xml">contextual bitextderived paraphrases in automatic mt evaluation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>each translation was evaluated by the bleu and nist metrics first with the single reference, then with the multiple references for each sentence using the paraphrases automatically generated from the source-reference mini corpus.
</prevsent>
<prevsent>a subset of 100 sentences was randomly extracted from each test set and evaluated by two independent human judges with respect to accuracy and fluency; the human scores were then compared to the bleu and nist scores for the single-reference and the automatically generated multiple-reference files.
</prevsent>
</prevsection>
<citsent citstr=" N03-1017 ">
word alignment and phrase extraction we used the giza++ word alignment soft ware3 to produce initial word alignments for our miniature bilingual corpus consisting of the source french file and the english reference file, and the refined word alignment strategy of (och and ney, 2003; <papid> J03-1002 </papid>koehn et al, 2003; <papid> N03-1017 </papid>tiedemann, 2004) to obtain improved word and phrase alignments.</citsent>
<aftsection>
<nextsent>for each source word or phrase fi that is aligned with more than one target words or phrases, its possible translations ei1, ..., ein were placed in list as equivalent expressions (i.e. synonyms, near-synonyms, or paraphrases of each other).
</nextsent>
<nextsent>a few examples are given in (1).
</nextsent>
<nextsent>(1) agreement - accordance adopted - implemented matter - lot - case funds - money arms - weapons area - aspect question ? issue ? matter we would expect - we certainly expect bear on - are centred around alignment divides target words and phrases into equivalence sets; each set corresponds to one source word/phrase that was originally aligned with the target elements.
</nextsent>
<nextsent>for example, for the french word cito yens three english words were deemed to be the most appropriate translations: people, public, and citizens; therefore these three words constitute an equivalence set.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1937">
<title id=" W06-3112.xml">contextual bitextderived paraphrases in automatic mt evaluation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in our experiment we do not allow synonym transitivity; we only use the paraphrases from equivalence sets containing the word/phrase we want to replace.
</prevsent>
<prevsent>multiple simultaneous substitution note that at the moment the references we are producing do not contain multiple simultaneous substitutions of equivalent expressions; for example, in (2) we currently do not produce the following versions: (3) paraphrase 4: admire the reply mrs parly gave this morning however we have turned blind eye to that paraphrase 5: admire the answer mrs parly gave this morning however we have turned blind eye to it paraphrase 6: admire the reply mrs parly gave this morning but we have turned blind eye to it this can potentially prevent higher n-grams being successfully matched if two or more equivalent expressions find themselves within the range of grams being tested by bleu and nist.
</prevsent>
</prevsection>
<citsent citstr=" N03-1024 ">
to avoid combinatorial problems, implementing multiple simultaneous substitutions could be done using lattice, much like in (pang et al, 2003).<papid> N03-1024 </papid></citsent>
<aftsection>
<nextsent>4 results.
</nextsent>
<nextsent>as expected, the use of multiple references produced by our method raises both the bleu and nist scores for translations produced by pharaoh (test set ph) and logo media (test set lm).
</nextsent>
<nextsent>the results are presented in table 1.
</nextsent>
<nextsent>bleu nist ph single ref 0.2131 6.1625 ph multi ref 0.2407 7.0068 lm single ref 0.1782 5.5406 lm multi ref 0.2043 6.3834 table 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1938">
<title id=" W06-3603.xml">computational challenges in parsing by classification </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>discriminative machine learning methods have improved accuracy on many nlp tasks, including pos-tagging, shallow parsing, relation extraction,and machine translation.
</prevsent>
<prevsent>however, only limited advances have been made on full syntactic constituent parsing.
</prevsent>
</prevsection>
<citsent citstr=" P04-1015 ">
successful discriminative parsers have used generative models to reduce training time and raise accuracy above generative baselines (collins &amp; roark, 2004; <papid> P04-1015 </papid>henderson, 2004; <papid> P04-1013 </papid>taskar et al, 2004).<papid> W04-3201 </papid>however, relying upon information from generative model might limit the potential of these approaches to realize the accuracy gains achieved by discriminative methods on other nlp tasks.</citsent>
<aftsection>
<nextsent>another difficulty is that discriminative parsing approaches can be very task-specific and require quite bit of trial and error with different hyper-parameter values and types of features.
</nextsent>
<nextsent>in the present work, we make progress towards overcoming these obstacles.
</nextsent>
<nextsent>we propose flexible, well-integrated method for training discriminative parsers, demonstrating techniques that might also be useful for other structured learning problems.
</nextsent>
<nextsent>the learning algorithm projects the hand-provided atomic features into compound feature space and performs incremental feature selection from this large feature space.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1940">
<title id=" W06-3603.xml">computational challenges in parsing by classification </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>discriminative machine learning methods have improved accuracy on many nlp tasks, including pos-tagging, shallow parsing, relation extraction,and machine translation.
</prevsent>
<prevsent>however, only limited advances have been made on full syntactic constituent parsing.
</prevsent>
</prevsection>
<citsent citstr=" P04-1013 ">
successful discriminative parsers have used generative models to reduce training time and raise accuracy above generative baselines (collins &amp; roark, 2004; <papid> P04-1015 </papid>henderson, 2004; <papid> P04-1013 </papid>taskar et al, 2004).<papid> W04-3201 </papid>however, relying upon information from generative model might limit the potential of these approaches to realize the accuracy gains achieved by discriminative methods on other nlp tasks.</citsent>
<aftsection>
<nextsent>another difficulty is that discriminative parsing approaches can be very task-specific and require quite bit of trial and error with different hyper-parameter values and types of features.
</nextsent>
<nextsent>in the present work, we make progress towards overcoming these obstacles.
</nextsent>
<nextsent>we propose flexible, well-integrated method for training discriminative parsers, demonstrating techniques that might also be useful for other structured learning problems.
</nextsent>
<nextsent>the learning algorithm projects the hand-provided atomic features into compound feature space and performs incremental feature selection from this large feature space.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1941">
<title id=" W06-3603.xml">computational challenges in parsing by classification </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>discriminative machine learning methods have improved accuracy on many nlp tasks, including pos-tagging, shallow parsing, relation extraction,and machine translation.
</prevsent>
<prevsent>however, only limited advances have been made on full syntactic constituent parsing.
</prevsent>
</prevsection>
<citsent citstr=" W04-3201 ">
successful discriminative parsers have used generative models to reduce training time and raise accuracy above generative baselines (collins &amp; roark, 2004; <papid> P04-1015 </papid>henderson, 2004; <papid> P04-1013 </papid>taskar et al, 2004).<papid> W04-3201 </papid>however, relying upon information from generative model might limit the potential of these approaches to realize the accuracy gains achieved by discriminative methods on other nlp tasks.</citsent>
<aftsection>
<nextsent>another difficulty is that discriminative parsing approaches can be very task-specific and require quite bit of trial and error with different hyper-parameter values and types of features.
</nextsent>
<nextsent>in the present work, we make progress towards overcoming these obstacles.
</nextsent>
<nextsent>we propose flexible, well-integrated method for training discriminative parsers, demonstrating techniques that might also be useful for other structured learning problems.
</nextsent>
<nextsent>the learning algorithm projects the hand-provided atomic features into compound feature space and performs incremental feature selection from this large feature space.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1942">
<title id=" W06-3603.xml">computational challenges in parsing by classification </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>4 presents experiments with discriminative parsers built using these methods.
</prevsent>
<prevsent>5 dis 1similar memory limitations exist in other large-scale nlp tasks.
</prevsent>
</prevsection>
<citsent citstr=" P05-1044 ">
syntax-driven smt systems are typically trained on an order of magnitude more sentences than english parsers, and unsupervised estimation methods can generate an arbitrary number of negative examples (smith &amp; eisner, 2005).<papid> P05-1044 </papid></citsent>
<aftsection>
<nextsent>17 cusses possible issues in scaling to larger example sets.
</nextsent>
<nextsent>the following terms will help to explain our work.a span is range over contiguous words in the in put.
</nextsent>
<nextsent>spans cross if they overlap but neither contains the other.
</nextsent>
<nextsent>an item is (span, label) pair.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1943">
<title id=" W06-3603.xml">computational challenges in parsing by classification </title>
<section> parsing algorithm.  </section>
<citcontext>
<prevsection>
<prevsent>ip l(i) ? ????????
</prevsent>
<prevsent>(1) where p(s) are possible parses of the sentence, and the loss (or cost) of parse is summed over the inferences that lead to the parse.
</prevsent>
</prevsection>
<citsent citstr=" W96-0213 ">
to find p?, the parsing algorithm considers sequence of states.the initial state contains terminal items, whose labels are the pos tags given by ratnaparkhi (1996).<papid> W96-0213 </papid></citsent>
<aftsection>
<nextsent>the parser considers set of (bottom-up) inferences at each state.
</nextsent>
<nextsent>each inference results in successor state to be placed on the agenda.
</nextsent>
<nextsent>the loss function can consider arbitrary properties of the input and parse state,2 which precludes tractable dynamic programming solution to equation 1.
</nextsent>
<nextsent>therefore, we do standard agenda-based parsing, but instead of items our agenda stores entire states, as per more general best-first search over parsing hypergraphs (klein &amp; manning, 2001).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1944">
<title id=" W06-3603.xml">computational challenges in parsing by classification </title>
<section> training method.  </section>
<citcontext>
<prevsection>
<prevsent>we train until the objective cannot be further reduced for the current choice of ?.
</prevsent>
<prevsent>we then relax the regularization penalty by decreasing ? and continuing training.
</prevsent>
</prevsection>
<citsent citstr=" W05-1515 ">
we also de 3including the following learning algorithms: ? un regularized logistic regression ? logistic regression with an `2 penalty (i.e. gaussian prior) ? svms using most kernels ? multilayer neural nets trained by back propagation ? the perceptron algorithm4turian and melamed (2005) <papid> W05-1515 </papid>show that that decision trees applied to parsing have higher accuracy and training speed than decision stumps.</citsent>
<aftsection>
<nextsent>listing 1 training algorithm.
</nextsent>
<nextsent>1: procedure t????(i) 2: ensemble?
</nextsent>
<nextsent>3: h(i)?
</nextsent>
<nextsent>0 for all ? 4: for = 1 . . .?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1950">
<title id=" W06-3603.xml">computational challenges in parsing by classification </title>
<section> training method.  </section>
<citcontext>
<prevsection>
<prevsent>moreover, even if samples gain estimates are inaccurate and the feature selection step chooses irrelevant compound features, confidence updates are based upon the entire training set andthe regularization penalty will prevent irrelevant features from having their parameters move away from zero.
</prevsent>
<prevsent>3.4 the training set.
</prevsent>
</prevsection>
<citsent citstr=" W05-1513 ">
our training set contains all inferences considered in every state along the correct path for each gold standard parse tree (sagae &amp; lavie, 2005).<papid> W05-1513 </papid>7 this method of generating training examples does notre quire working parser and can be run prior to any training.</citsent>
<aftsection>
<nextsent>the downside of this approach is that it minimizes the error of the parser at correct states only.
</nextsent>
<nextsent>it does not account for compounded error or teach the parser to recover from mistakes gracefully.
</nextsent>
<nextsent>7since parsing is done deterministically right-to-left, there can be no more than one correct inference at each state.
</nextsent>
<nextsent>20turian and melamed (2005) <papid> W05-1515 </papid>observed that uniform example biases b(i) produced lower accuracy as training progressed, because the induced classifiers minimized the example-wise error.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1966">
<title id=" W06-3603.xml">computational challenges in parsing by classification </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>we used sections 0221 for training, section 22 for development, and section 23, for testing.
</prevsent>
<prevsent>we use the same preprocessing steps asturian and melamed (2005): <papid> W05-1515 </papid>during both training and testing, the parser is given text pos-tagged by the tagger of ratnaparkhi (1996), <papid> W96-0213 </papid>with capitalization stripped and outermost punctuation removed.</prevsent>
</prevsection>
<citsent citstr=" P06-1110 ">
for reasons given in turian and melamed (2006),<papid> P06-1110 </papid>items are inferred bottom-up right-to-left.</citsent>
<aftsection>
<nextsent>as mentioned in 2, the parser cannot infer any item that crosses an item already in the state.
</nextsent>
<nextsent>to ensure the parser does not enter an infinite loop, no two items in state can have both the same span and the same label.
</nextsent>
<nextsent>given these restrictions, there were roughly 40 million training examples.
</nextsent>
<nextsent>these were partitioned among the constituent label classifiers.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1967">
<title id=" W06-3603.xml">computational challenges in parsing by classification </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>these templates gave 1.1 million different atomic features.
</prevsent>
<prevsent>we experimented with smaller feature sets, but found that accuracy was lower.
</prevsent>
</prevsection>
<citsent citstr=" P05-1022 ">
charniak and johnson (2005) <papid> P05-1022 </papid>use linguistically more sophisticated features, and bod (2003) <papid> E03-1005 </papid>and kudo et al (2005) <papid> P05-1024 </papid>usesub-tree features, all of which we plan to try in future work.we evaluated our parser using the standard parseval measures (black et al, 1991): <papid> H91-1060 </papid>labelled precision, labelled recall, and labelled f-measure (prec., rec., and f1, respectively), which are based on the number of non-terminal items in the parsers output that match those in the gold-standard parse.</citsent>
<aftsection>
<nextsent>the solid curve figure 1 shows the accuracy ofthe parser over the development set as training progressed.
</nextsent>
<nextsent>the parser exceeded 89% f-measure after 2.5 days of training.
</nextsent>
<nextsent>the peak f-measure was 90.55%, achieved at 5.4 days using 6.3k active parameters.
</nextsent>
<nextsent>we omit details given by turian and melamed (2006) <papid> P06-1110 </papid>in favor of longer discussion in 4.2.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1968">
<title id=" W06-3603.xml">computational challenges in parsing by classification </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>these templates gave 1.1 million different atomic features.
</prevsent>
<prevsent>we experimented with smaller feature sets, but found that accuracy was lower.
</prevsent>
</prevsection>
<citsent citstr=" E03-1005 ">
charniak and johnson (2005) <papid> P05-1022 </papid>use linguistically more sophisticated features, and bod (2003) <papid> E03-1005 </papid>and kudo et al (2005) <papid> P05-1024 </papid>usesub-tree features, all of which we plan to try in future work.we evaluated our parser using the standard parseval measures (black et al, 1991): <papid> H91-1060 </papid>labelled precision, labelled recall, and labelled f-measure (prec., rec., and f1, respectively), which are based on the number of non-terminal items in the parsers output that match those in the gold-standard parse.</citsent>
<aftsection>
<nextsent>the solid curve figure 1 shows the accuracy ofthe parser over the development set as training progressed.
</nextsent>
<nextsent>the parser exceeded 89% f-measure after 2.5 days of training.
</nextsent>
<nextsent>the peak f-measure was 90.55%, achieved at 5.4 days using 6.3k active parameters.
</nextsent>
<nextsent>we omit details given by turian and melamed (2006) <papid> P06-1110 </papid>in favor of longer discussion in 4.2.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1969">
<title id=" W06-3603.xml">computational challenges in parsing by classification </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>these templates gave 1.1 million different atomic features.
</prevsent>
<prevsent>we experimented with smaller feature sets, but found that accuracy was lower.
</prevsent>
</prevsection>
<citsent citstr=" P05-1024 ">
charniak and johnson (2005) <papid> P05-1022 </papid>use linguistically more sophisticated features, and bod (2003) <papid> E03-1005 </papid>and kudo et al (2005) <papid> P05-1024 </papid>usesub-tree features, all of which we plan to try in future work.we evaluated our parser using the standard parseval measures (black et al, 1991): <papid> H91-1060 </papid>labelled precision, labelled recall, and labelled f-measure (prec., rec., and f1, respectively), which are based on the number of non-terminal items in the parsers output that match those in the gold-standard parse.</citsent>
<aftsection>
<nextsent>the solid curve figure 1 shows the accuracy ofthe parser over the development set as training progressed.
</nextsent>
<nextsent>the parser exceeded 89% f-measure after 2.5 days of training.
</nextsent>
<nextsent>the peak f-measure was 90.55%, achieved at 5.4 days using 6.3k active parameters.
</nextsent>
<nextsent>we omit details given by turian and melamed (2006) <papid> P06-1110 </papid>in favor of longer discussion in 4.2.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1970">
<title id=" W06-3603.xml">computational challenges in parsing by classification </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>these templates gave 1.1 million different atomic features.
</prevsent>
<prevsent>we experimented with smaller feature sets, but found that accuracy was lower.
</prevsent>
</prevsection>
<citsent citstr=" H91-1060 ">
charniak and johnson (2005) <papid> P05-1022 </papid>use linguistically more sophisticated features, and bod (2003) <papid> E03-1005 </papid>and kudo et al (2005) <papid> P05-1024 </papid>usesub-tree features, all of which we plan to try in future work.we evaluated our parser using the standard parseval measures (black et al, 1991): <papid> H91-1060 </papid>labelled precision, labelled recall, and labelled f-measure (prec., rec., and f1, respectively), which are based on the number of non-terminal items in the parsers output that match those in the gold-standard parse.</citsent>
<aftsection>
<nextsent>the solid curve figure 1 shows the accuracy ofthe parser over the development set as training progressed.
</nextsent>
<nextsent>the parser exceeded 89% f-measure after 2.5 days of training.
</nextsent>
<nextsent>the peak f-measure was 90.55%, achieved at 5.4 days using 6.3k active parameters.
</nextsent>
<nextsent>we omit details given by turian and melamed (2006) <papid> P06-1110 </papid>in favor of longer discussion in 4.2.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1983">
<title id=" W06-3603.xml">computational challenges in parsing by classification </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>f1 % rec.
</prevsent>
<prevsent>% prec.
</prevsent>
</prevsection>
<citsent citstr=" J04-4004 ">
% turian and melamed (2005) <papid> W05-1515 </papid>87.13 86.47 87.80 bikel (2004) <papid> J04-4004 </papid>88.30 87.85 88.75 taskar et al (2004)<papid> W04-3201 </papid>89.12 89.10 89.14 our parser 89.40 89.26 89.55 table 2 profile of an np training iteration, given in seconds, using an amd opteron 242 (64-bit, 1.6ghz).</citsent>
<aftsection>
<nextsent>steps refer to listing 1.
</nextsent>
<nextsent>step description mean stddev % 1.5 sample 1.5s 0.07s 0.7%.
</nextsent>
<nextsent>1.6 extraction 38.2s 0.13s 18.6%.
</nextsent>
<nextsent>1.7 build tree 127.6s 27.60s 62.3%.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1997">
<title id=" W06-3328.xml">bootstrapping and evaluating named entity recognition in the biomedical domain </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these shared tasks aimed at evaluating fully supervised trainable systems.
</prevsent>
<prevsent>however, the limited availability of annotated material in most domains, including the biomedical, restricts the application of such methods.
</prevsent>
</prevsection>
<citsent citstr=" P04-1075 ">
in order to circumvent this obstacle several approaches have been presented, among them active learning (shen et al,2004) <papid> P04-1075 </papid>and rule-based systems encoding domain specific knowledge (gaizauskas et al, 2003).</citsent>
<aftsection>
<nextsent>in this work we build on the idea of bootstrapping, which has been applied by collins &amp; singer (1999) <papid> W99-0613 </papid>in the newsire domain and by morgan et al (2004) in the biomedical domain.</nextsent>
<nextsent>this approach is based on creating training material automatically using existing domain resources, which in turn is used to train supervised named entity recognizer.the structure of this paper is the following.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1998">
<title id=" W06-3328.xml">bootstrapping and evaluating named entity recognition in the biomedical domain </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, the limited availability of annotated material in most domains, including the biomedical, restricts the application of such methods.
</prevsent>
<prevsent>in order to circumvent this obstacle several approaches have been presented, among them active learning (shen et al,2004) <papid> P04-1075 </papid>and rule-based systems encoding domain specific knowledge (gaizauskas et al, 2003).</prevsent>
</prevsection>
<citsent citstr=" W99-0613 ">
in this work we build on the idea of bootstrapping, which has been applied by collins &amp; singer (1999) <papid> W99-0613 </papid>in the newsire domain and by morgan et al (2004) in the biomedical domain.</citsent>
<aftsection>
<nextsent>this approach is based on creating training material automatically using existing domain resources, which in turn is used to train supervised named entity recognizer.the structure of this paper is the following.
</nextsent>
<nextsent>section 2 describes the construction of new test set to evaluate named entity recognition for drosophila fly genes.
</nextsent>
<nextsent>section 3 compares bootstrapping to theuse of manually annotated material for training supervised method.
</nextsent>
<nextsent>an extension to the evaluation of ner appear in section 4.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC1999">
<title id=" W06-3328.xml">bootstrapping and evaluating named entity recognition in the biomedical domain </title>
<section> building test set.  </section>
<citcontext>
<prevsection>
<prevsent>1www.flybase.net 2http://www.cogs.susx.ac.uk/lab/nlp/rasp/ 139 we estimated the inter-annotator agreement in twoways.
</prevsent>
<prevsent>first, we calculated the f-score achieved between them, which was 91%.
</prevsent>
</prevsection>
<citsent citstr=" J96-2004 ">
secondly, we used the kappa coefficient (carletta, 1996), <papid> J96-2004 </papid>which has be come the standard evaluation metric and the score obtained was 0.905.</citsent>
<aftsection>
<nextsent>this high agreement score can be attributed to the clarification of what gene name should capture through the introduction ofgene mention and other mention.
</nextsent>
<nextsent>it must be mentioned that in the experiments that follow in the restof the paper, only the gene names were used to evaluate the performance of bootstrapping.
</nextsent>
<nextsent>the identification and the classification of mentions is the subject of ongoing research.the annotation of mentions presented greater difficulty, because computational linguists do not have sufficient knowledge of biology in order to use the context of the mentions whilst biologists are not trained to identify noun phrases in text.
</nextsent>
<nextsent>in this effort, the boundaries of the mentions where defined by the computational linguist and the classification was performed by the curator.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2000">
<title id=" W06-3328.xml">bootstrapping and evaluating named entity recognition in the biomedical domain </title>
<section> evaluating ner.  </section>
<citcontext>
<prevsection>
<prevsent>the ability to generalize to unseen named entities is very significant be cause it is unlikely that training material can cover all possible names and moreover, in most domains, new names appear regularly.
</prevsent>
<prevsent>a common way to assess these two aspects is to measure the performance on seen and unseen data separately.
</prevsent>
</prevsection>
<citsent citstr=" E03-1071 ">
it is straightforward to apply this in tasks with token-based evaluation, such as part-of-speech tagging (curran and clark, 2003).<papid> E03-1071 </papid></citsent>
<aftsection>
<nextsent>however, in the case of ner, this is not entirely appropriate dueto the existence of multi-token entities.
</nextsent>
<nextsent>for example, consider the case of the gene-name head inhibition defective?, which consists of three common words that are very likely to occur independently ofeach other in training set.
</nextsent>
<nextsent>if this gene name appears in the test set but not in the training set, with token-based evaluation its identification (or not)would count towards the performance on seen tokens if the tokens appeared independently.
</nextsent>
<nextsent>more over, system would be rewarded or penalized for each of the tokens.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2001">
<title id=" W06-1629.xml">modeling impression in probabilistic transliteration into chinese </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in broad sense, the term transliteration?
</prevsent>
<prevsent>has been used to refer to two tasks.
</prevsent>
</prevsection>
<citsent citstr=" P98-2220 ">
the first task is transliteration in the strict sense, which creates new words in target language (haizhou et al, 2004; wan and verspoor, 1998).<papid> P98-2220 </papid></citsent>
<aftsection>
<nextsent>the second task is back-transliteration (fujii and ishikawa, 2001; jeong et al, 1999; knight and graehl, 1998; <papid> J98-4003 </papid>qu et al, 2003), which identifies the source word corresponding to an existing transliterated word.</nextsent>
<nextsent>back-transliteration is intended mainly for cross-lingual information retrieval and machine translation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2002">
<title id=" W06-1629.xml">modeling impression in probabilistic transliteration into chinese </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>has been used to refer to two tasks.
</prevsent>
<prevsent>the first task is transliteration in the strict sense, which creates new words in target language (haizhou et al, 2004; wan and verspoor, 1998).<papid> P98-2220 </papid></prevsent>
</prevsection>
<citsent citstr=" J98-4003 ">
the second task is back-transliteration (fujii and ishikawa, 2001; jeong et al, 1999; knight and graehl, 1998; <papid> J98-4003 </papid>qu et al, 2003), which identifies the source word corresponding to an existing transliterated word.</citsent>
<aftsection>
<nextsent>back-transliteration is intended mainly for cross-lingual information retrieval and machine translation.
</nextsent>
<nextsent>both transliteration tasks require methods that model pronunciation in the source and target languages.
</nextsent>
<nextsent>however, by definition, in back-transliteration,the word in question has already been transliterated and the meaning or impression of the source word does not have to be considered.
</nextsent>
<nextsent>thus, back transliteration is outside the scope of this paper.in the following, we use the term translitera tion?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2005">
<title id=" W06-3113.xml">how many bits are needed to store probabilities for phrase based translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in several natural language processing tasks, such as automatic speech recognition and machine translation, state-of-the-art systems relyon the statistical approach.
</prevsent>
<prevsent>statistical machine translation (smt) is basedon parametric models incorporating large number of observations and probabilities estimated from monolingual and parallel texts.
</prevsent>
</prevsection>
<citsent citstr=" J04-4002 ">
the current state of the art is represented by the so-called phrase-based translation approach (och and ney, 2004; <papid> J04-4002 </papid>koehn et al., 2003).<papid> N03-1017 </papid></citsent>
<aftsection>
<nextsent>its core components are translation model that contains probabilities of phrase-pairs, and language model that incorporates probabilities of word n-grams.
</nextsent>
<nextsent>due to the intrinsic data-sparseness of language corpora, the set of observations increases almost linearly with the size of the training data.
</nextsent>
<nextsent>hence, to efficiently store observations and probabilities in computer memory the following approaches can be tackled: designing compact data-structures, pruning rare or unreliable observations, and applying data compression.
</nextsent>
<nextsent>in this paper we only focus on the last approach.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2006">
<title id=" W06-3113.xml">how many bits are needed to store probabilities for phrase based translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in several natural language processing tasks, such as automatic speech recognition and machine translation, state-of-the-art systems relyon the statistical approach.
</prevsent>
<prevsent>statistical machine translation (smt) is basedon parametric models incorporating large number of observations and probabilities estimated from monolingual and parallel texts.
</prevsent>
</prevsection>
<citsent citstr=" N03-1017 ">
the current state of the art is represented by the so-called phrase-based translation approach (och and ney, 2004; <papid> J04-4002 </papid>koehn et al., 2003).<papid> N03-1017 </papid></citsent>
<aftsection>
<nextsent>its core components are translation model that contains probabilities of phrase-pairs, and language model that incorporates probabilities of word n-grams.
</nextsent>
<nextsent>due to the intrinsic data-sparseness of language corpora, the set of observations increases almost linearly with the size of the training data.
</nextsent>
<nextsent>hence, to efficiently store observations and probabilities in computer memory the following approaches can be tackled: designing compact data-structures, pruning rare or unreliable observations, and applying data compression.
</nextsent>
<nextsent>in this paper we only focus on the last approach.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2007">
<title id=" W06-3113.xml">how many bits are needed to store probabilities for phrase based translation </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>sections 5 reports and discusses experimental results addressing the questions in the introduction.
</prevsent>
<prevsent>finally, section 6 draws some conclusions.
</prevsent>
</prevsection>
<citsent citstr=" P02-1023 ">
most related work can be found in the area of speech recognition, where n-gram language models have been used for while.efforts targeting efficiency have been mainly focused on pruning techniques (seymore and rosenfeld, 1996; gao and zhang, 2002), <papid> P02-1023 </papid>which permit to significantly reduce the amount of n-grams to be stored at negligible cost in performance.</citsent>
<aftsection>
<nextsent>moreover, very compact data-structures for storing back off n-gram models have been recently proposed by raj and whittaker (2003).whittaker and raj (2001) discuss probability encoding as means to reduce memory requirements of an n-gram language model.
</nextsent>
<nextsent>quant ization of 3-gram back-off model was performed by applying the k-means lloyd-max algorithm at each n-gramlevel.
</nextsent>
<nextsent>experiments were performed on several large vocabulary speech recognition tasks by considering different levels of compression.
</nextsent>
<nextsent>by encoded probabilities in 4 bits, the increase in word-error-rate was only around 2% relative with respect to baseline using 32-bit floating point probabilities.similar work was carried out in the field of information retrieval, where memory efficiency is instead related to the indexing data structure, which contains information about frequencies of terms in all the individual documents.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2008">
<title id=" W07-0414.xml">comparing reordering constraints for smt using efficient bleu oracle computation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>reordering the words and phrases of foreign sentence to obtain the target word order is fundamental, and potentially the hardest, problem in machinetranslation.
</prevsent>
<prevsent>the search space for all possible permutations of sentence is facto rial in the number of words/phrases; therefore variety of models have been proposed that constrain the set of possible permutations by allowing certain reorderings while disallowing others.
</prevsent>
</prevsection>
<citsent citstr=" H05-1021 ">
some models (brown et al (1996), kumar and byrne (2005)) <papid> H05-1021 </papid>allow words to change place with their local neighbors, but disallow global reorderings.</citsent>
<aftsection>
<nextsent>other models (wu (1997), <papid> J97-3002 </papid>xiong et al (2006)) <papid> P06-1066 </papid>explicitly allow global reorderings, but do not allow all possible permutations, including some local permutations.we present novel technique to compare achievable translation accuracies under different reordering constraints.</nextsent>
<nextsent>while earlier work has trained and tested instantiations of different reordering models and then compared the translation results (zens and ney, 2003) <papid> P03-1019 </papid>we provide more general mechanism to evaluate the potential efficacy of reordering constraints, independent of specific training paradigms.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2009">
<title id=" W07-0414.xml">comparing reordering constraints for smt using efficient bleu oracle computation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the search space for all possible permutations of sentence is facto rial in the number of words/phrases; therefore variety of models have been proposed that constrain the set of possible permutations by allowing certain reorderings while disallowing others.
</prevsent>
<prevsent>some models (brown et al (1996), kumar and byrne (2005)) <papid> H05-1021 </papid>allow words to change place with their local neighbors, but disallow global reorderings.</prevsent>
</prevsection>
<citsent citstr=" J97-3002 ">
other models (wu (1997), <papid> J97-3002 </papid>xiong et al (2006)) <papid> P06-1066 </papid>explicitly allow global reorderings, but do not allow all possible permutations, including some local permutations.we present novel technique to compare achievable translation accuracies under different reordering constraints.</citsent>
<aftsection>
<nextsent>while earlier work has trained and tested instantiations of different reordering models and then compared the translation results (zens and ney, 2003) <papid> P03-1019 </papid>we provide more general mechanism to evaluate the potential efficacy of reordering constraints, independent of specific training paradigms.</nextsent>
<nextsent>our technique attempts to answer the question:what is the highest bleu score that given translation system could reach when using reordering constraints x?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2010">
<title id=" W07-0414.xml">comparing reordering constraints for smt using efficient bleu oracle computation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the search space for all possible permutations of sentence is facto rial in the number of words/phrases; therefore variety of models have been proposed that constrain the set of possible permutations by allowing certain reorderings while disallowing others.
</prevsent>
<prevsent>some models (brown et al (1996), kumar and byrne (2005)) <papid> H05-1021 </papid>allow words to change place with their local neighbors, but disallow global reorderings.</prevsent>
</prevsection>
<citsent citstr=" P06-1066 ">
other models (wu (1997), <papid> J97-3002 </papid>xiong et al (2006)) <papid> P06-1066 </papid>explicitly allow global reorderings, but do not allow all possible permutations, including some local permutations.we present novel technique to compare achievable translation accuracies under different reordering constraints.</citsent>
<aftsection>
<nextsent>while earlier work has trained and tested instantiations of different reordering models and then compared the translation results (zens and ney, 2003) <papid> P03-1019 </papid>we provide more general mechanism to evaluate the potential efficacy of reordering constraints, independent of specific training paradigms.</nextsent>
<nextsent>our technique attempts to answer the question:what is the highest bleu score that given translation system could reach when using reordering constraints x?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2011">
<title id=" W07-0414.xml">comparing reordering constraints for smt using efficient bleu oracle computation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>some models (brown et al (1996), kumar and byrne (2005)) <papid> H05-1021 </papid>allow words to change place with their local neighbors, but disallow global reorderings.</prevsent>
<prevsent>other models (wu (1997), <papid> J97-3002 </papid>xiong et al (2006)) <papid> P06-1066 </papid>explicitly allow global reorderings, but do not allow all possible permutations, including some local permutations.we present novel technique to compare achievable translation accuracies under different reordering constraints.</prevsent>
</prevsection>
<citsent citstr=" P03-1019 ">
while earlier work has trained and tested instantiations of different reordering models and then compared the translation results (zens and ney, 2003) <papid> P03-1019 </papid>we provide more general mechanism to evaluate the potential efficacy of reordering constraints, independent of specific training paradigms.</citsent>
<aftsection>
<nextsent>our technique attempts to answer the question:what is the highest bleu score that given translation system could reach when using reordering constraints x?
</nextsent>
<nextsent>using this oracle approach, we abstract away from issues that are not inherent in the reordering constraints, but may nevertheless influence the comparison results, such as model and feature design, feature selection, or parameter estimation.
</nextsent>
<nextsent>in fact, we compare several sets of reordering constraints empirically, but do not train them asmodels.
</nextsent>
<nextsent>we merely decode by efficiently searching over possible translations allowed by each mod eland choosing the reordering that achieves the highest bleu score.we start by introducing popular reordering constraints (section 2).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2016">
<title id=" W07-0414.xml">comparing reordering constraints for smt using efficient bleu oracle computation </title>
<section> factored bleu computation.  </section>
<citcontext>
<prevsection>
<prevsent>therefore we cannot discount locally attached word that has already been attached else where to an alternative path/constituent.
</prevsent>
<prevsent>however, clipping affects most heavily the unigram scores which are constant, like the length of the sentence.4 4since the sentence lengths are constant for all reorderings of given sentence we can in our experiments also ignore the brevity penalty which cancels out.
</prevsent>
</prevsection>
<citsent citstr=" P06-1091 ">
if the input consists of sev we also adopt the approximation that treats every sentence with its reference as separate corpus (till mann and zhang, 2006) <papid> P06-1091 </papid>so that ngram counts are not accumulated, and parallel processing of sentences becomes possible.</citsent>
<aftsection>
<nextsent>due to these two approximations,our method is not guaranteed to find the best reordering defined by the reordering constraints.
</nextsent>
<nextsent>however, we have found on our heldout data that an oracle that does not accumulate n-gram counts is only minimally worse than an oracle that does accumulate them (up to 0.25 bleu points).5 if, in addition,clipping is ignored, the resulting oracle stays virtually the same, at most 0.02 bleu points worse than the oracle found otherwise.
</nextsent>
<nextsent>all results in this paper are computed with the original bleu formula on the sentences found by the oracle algorithms.
</nextsent>
<nextsent>baseline to compare the reordering constraints under oracle conditions we first obtain un reordered candidate translations from simple baseline translation model.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2017">
<title id=" W07-0414.xml">comparing reordering constraints for smt using efficient bleu oracle computation </title>
<section> creating monotone translation.  </section>
<citcontext>
<prevsection>
<prevsent>for each reordering paradigm, we take the candidate translations, get the best oracle reorderings under the given reordering constraints and pick the best sentence according to the bleu score.
</prevsent>
<prevsent>the baseline translation system is created using probabilistic word-to-word and phrase-to-phrase ta eral sentences of different lengths (see fn.
</prevsent>
</prevsection>
<citsent citstr=" W05-0834 ">
1) then the brevity penalty can be built in by keeping track of length ratios of attached phrases.5the accumulating oracle algorithm makes greedy decision for every sentence given the ngram counts so far accumulated (zens and ney, 2005).<papid> W05-0834 </papid></citsent>
<aftsection>
<nextsent>the result of such greedy oracle method may depend on the order of the input sentences.we tried 100 shuffles of these and received 100 very similar results, with variance of under 0.006 bleu points.
</nextsent>
<nextsent>the non-accumulating oracles use an epsilon value (110) for zero counts.
</nextsent>
<nextsent>107 bles.
</nextsent>
<nextsent>using the translation probabilities, we create lattice that contains word and phrase translations for every substring of the source sentence.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2018">
<title id=" W07-0414.xml">comparing reordering constraints for smt using efficient bleu oracle computation </title>
<section> empirical comparison of reordering.  </section>
<citcontext>
<prevsection>
<prevsent>we takethe 10-best oracle candidates, according to the reference, and use bleu decoder to create the best permutation of each of them and pick the best one.
</prevsent>
<prevsent>using this procedure, we make sure that we get the highest-scoring un reordered candidates and choose the best one among their oracle reorderings.
</prevsent>
</prevsection>
<citsent citstr=" W05-1506 ">
table 2 6we use straightforward adaption of algorithm 3 in huang and chiang (2005) <papid> W05-1506 </papid>7we fill the initial phrase and word lattice with the 20 best candidates, using phrases of 3 or less words.and figure 6 show the resulting bleu scores for different sentence lengths.</citsent>
<aftsection>
<nextsent>table 3 shows results of theitg runs with different length bounds ?.
</nextsent>
<nextsent>the average phrase length in the candidate translations of the test set is 1.42 words.
</nextsent>
<nextsent>oracle deco dings under the itg and under ibm(4) constraints were up to 1000 times slower than under the other tested oracle reordering methods in our implementations.
</nextsent>
<nextsent>among the faster methods, decoding under mj-2 constraints was up to 40%faster than under ibm(2) constraints in our implementation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2019">
<title id=" W07-0414.xml">comparing reordering constraints for smt using efficient bleu oracle computation </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>we did not test ibm(3), but the values can be interpolated between ibm(2) and ibm(4).
</prevsent>
<prevsent>the itg constraints do not belong in this family of finite-state contraints; they allow reorderings that none of the other methods allow, and vice versa.
</prevsent>
</prevsection>
<citsent citstr=" C04-1030 ">
the fact that itg constraints can reach such high translation accuracies supports the findings in zens et al (2004) <papid> C04-1030 </papid>and is an empirical validation of the itg hypothesis.</citsent>
<aftsection>
<nextsent>the experiments with the constrained itg show the effect of reorderings spanning different lengths(see table 3).
</nextsent>
<nextsent>while most reorderings are short distance ( 5 phrases) lot of improvements can still be obtained when ? is increased from length 5 to 10 and even from 10 to 20 phrases.
</nextsent>
<nextsent>there exist related algorithms that search the spaceof reorderings and compute bleu oracle approxi len.
</nextsent>
<nextsent>?=0 ?=5 ?=10 ?=20 ?=30 ?=40 2630 18.31 24.07 26.40 26.79 26.85 26.85 3135 18.94 25.10 27.21 28.00 28.09 28.11 3640 18.22 24.46 26.66 27.53 27.64 27.65 2640 18.49 24.74 26.74 27.41 27.50 27.51table 3: bleu results of itgs that are constrained to reorderings not exceeding certain span length ?.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2027">
<title id=" W07-0414.xml">comparing reordering constraints for smt using efficient bleu oracle computation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>zens et al (2004) <papid> C04-1030 </papid>conduct study similar to zens and ney (2003) <papid> P03-1019 </papid>and note that the results for the itg reordering constraints were quite dependent on the very simple probability model used.</prevsent>
<prevsent>our study avoids this issue by using the 109 bleu oracle approach.</prevsent>
</prevsection>
<citsent citstr=" P06-1123 ">
in wellington et al (2006), <papid> P06-1123 </papid>hand-aligned data are used to compare the standard itg constraints to itgs that allow gaps.</citsent>
<aftsection>
<nextsent>we have presented training-independent method to compare different reordering constraints forma chine translation.
</nextsent>
<nextsent>given sentence in foreign word order, its reference translation(s) and reordering constraints, our dynamic-programming algorithms efficiently find the oracle reordering that has the approximately highest bleu score.
</nextsent>
<nextsent>this allows evaluating different reordering constraints experimentally, but abstracting away from specific features,the probability model or training methods of there ordering strategies.
</nextsent>
<nextsent>the presented method evaluates the theoretical capabilities of reordering constraints,as opposed to more arbitrary accuracies of specifically trained instances of reordering models.using our oracle method, we presented an empirical evaluation of different reordering constraints for german-english translation task.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2029">
<title id=" W06-2904.xml">improved large margin dependency parsing via local constraints and laplacian regularization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>to demonstrate the benefits of our approach,we consider the problem of parsing chinese treebank data using only lexical features, that is, without part-of-speech tags or grammatical categories.
</prevsent>
<prevsent>we achieve state of the art performance, improving upon current large margin approaches.
</prevsent>
</prevsection>
<citsent citstr=" P97-1003 ">
over the past decade, there has been tremendous progress on learning parsing models from treebank data (collins, 1997; <papid> P97-1003 </papid>charniak, 2000; <papid> A00-2018 </papid>wang et al, 2005; <papid> W05-1516 </papid>mcdonald et al, 2005).<papid> P05-1012 </papid></citsent>
<aftsection>
<nextsent>most of the early work in this area was based on postulating generative probability models of language that included parse structure (collins, 1997).<papid> P97-1003 </papid></nextsent>
<nextsent>learning in this context consisted of estimating the parameters of the model with simple likelihood based techniques, but incorporating various smoothing and back-off estimation tricks to cope with the sparse data problems (collins, 1997; <papid> P97-1003 </papid>bikel, 2004).<papid> J04-4004 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2032">
<title id=" W06-2904.xml">improved large margin dependency parsing via local constraints and laplacian regularization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>to demonstrate the benefits of our approach,we consider the problem of parsing chinese treebank data using only lexical features, that is, without part-of-speech tags or grammatical categories.
</prevsent>
<prevsent>we achieve state of the art performance, improving upon current large margin approaches.
</prevsent>
</prevsection>
<citsent citstr=" A00-2018 ">
over the past decade, there has been tremendous progress on learning parsing models from treebank data (collins, 1997; <papid> P97-1003 </papid>charniak, 2000; <papid> A00-2018 </papid>wang et al, 2005; <papid> W05-1516 </papid>mcdonald et al, 2005).<papid> P05-1012 </papid></citsent>
<aftsection>
<nextsent>most of the early work in this area was based on postulating generative probability models of language that included parse structure (collins, 1997).<papid> P97-1003 </papid></nextsent>
<nextsent>learning in this context consisted of estimating the parameters of the model with simple likelihood based techniques, but incorporating various smoothing and back-off estimation tricks to cope with the sparse data problems (collins, 1997; <papid> P97-1003 </papid>bikel, 2004).<papid> J04-4004 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2033">
<title id=" W06-2904.xml">improved large margin dependency parsing via local constraints and laplacian regularization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>to demonstrate the benefits of our approach,we consider the problem of parsing chinese treebank data using only lexical features, that is, without part-of-speech tags or grammatical categories.
</prevsent>
<prevsent>we achieve state of the art performance, improving upon current large margin approaches.
</prevsent>
</prevsection>
<citsent citstr=" W05-1516 ">
over the past decade, there has been tremendous progress on learning parsing models from treebank data (collins, 1997; <papid> P97-1003 </papid>charniak, 2000; <papid> A00-2018 </papid>wang et al, 2005; <papid> W05-1516 </papid>mcdonald et al, 2005).<papid> P05-1012 </papid></citsent>
<aftsection>
<nextsent>most of the early work in this area was based on postulating generative probability models of language that included parse structure (collins, 1997).<papid> P97-1003 </papid></nextsent>
<nextsent>learning in this context consisted of estimating the parameters of the model with simple likelihood based techniques, but incorporating various smoothing and back-off estimation tricks to cope with the sparse data problems (collins, 1997; <papid> P97-1003 </papid>bikel, 2004).<papid> J04-4004 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2036">
<title id=" W06-2904.xml">improved large margin dependency parsing via local constraints and laplacian regularization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>to demonstrate the benefits of our approach,we consider the problem of parsing chinese treebank data using only lexical features, that is, without part-of-speech tags or grammatical categories.
</prevsent>
<prevsent>we achieve state of the art performance, improving upon current large margin approaches.
</prevsent>
</prevsection>
<citsent citstr=" P05-1012 ">
over the past decade, there has been tremendous progress on learning parsing models from treebank data (collins, 1997; <papid> P97-1003 </papid>charniak, 2000; <papid> A00-2018 </papid>wang et al, 2005; <papid> W05-1516 </papid>mcdonald et al, 2005).<papid> P05-1012 </papid></citsent>
<aftsection>
<nextsent>most of the early work in this area was based on postulating generative probability models of language that included parse structure (collins, 1997).<papid> P97-1003 </papid></nextsent>
<nextsent>learning in this context consisted of estimating the parameters of the model with simple likelihood based techniques, but incorporating various smoothing and back-off estimation tricks to cope with the sparse data problems (collins, 1997; <papid> P97-1003 </papid>bikel, 2004).<papid> J04-4004 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2045">
<title id=" W06-2904.xml">improved large margin dependency parsing via local constraints and laplacian regularization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>over the past decade, there has been tremendous progress on learning parsing models from treebank data (collins, 1997; <papid> P97-1003 </papid>charniak, 2000; <papid> A00-2018 </papid>wang et al, 2005; <papid> W05-1516 </papid>mcdonald et al, 2005).<papid> P05-1012 </papid></prevsent>
<prevsent>most of the early work in this area was based on postulating generative probability models of language that included parse structure (collins, 1997).<papid> P97-1003 </papid></prevsent>
</prevsection>
<citsent citstr=" J04-4004 ">
learning in this context consisted of estimating the parameters of the model with simple likelihood based techniques, but incorporating various smoothing and back-off estimation tricks to cope with the sparse data problems (collins, 1997; <papid> P97-1003 </papid>bikel, 2004).<papid> J04-4004 </papid></citsent>
<aftsection>
<nextsent>subsequent research began to focus more on conditional models of parse structure given the input sentence, which allowed discriminative training techniques such as maximum conditional likelihood (i.e. maximum entropy?)
</nextsent>
<nextsent>to be applied (ratnaparkhi, 1999; charniak, 2000).<papid> A00-2018 </papid>in fact, recently, effective conditional parsing models have been learned using relatively straightforward plug-in?</nextsent>
<nextsent>estimates, augmented with similarity based smoothing (wang et al, 2005).<papid> W05-1516 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2050">
<title id=" W06-2904.xml">improved large margin dependency parsing via local constraints and laplacian regularization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>to be applied (ratnaparkhi, 1999; charniak, 2000).<papid> A00-2018 </papid>in fact, recently, effective conditional parsing models have been learned using relatively straightforward plug-in?</prevsent>
<prevsent>estimates, augmented with similarity based smoothing (wang et al, 2005).<papid> W05-1516 </papid></prevsent>
</prevsection>
<citsent citstr=" W04-3201 ">
currently, the work on conditional parsing models appears to have culminated in large margin training (taskar et al, 2003; taskar et al, 2004; <papid> W04-3201 </papid>tsochantaridis et al., 2004; mcdonald et al, 2005), <papid> P05-1012 </papid>which currently demonstrates the state of the art performance in english dependency parsing (mcdonald et al, 2005).<papid> P05-1012 </papid></citsent>
<aftsection>
<nextsent>despite the realization that maximum margin training is closely related to maximum conditional likelihood for conditional models (mcdonald et al., 2005)<papid> P05-1012 </papid>sufficiently unified view has not yet been achieved that permits the easy exchange of improvements between the probabilistic and non probabilistic approaches.</nextsent>
<nextsent>for example, smoothing methods have played central role in probabilistic approaches (collins, 1997; <papid> P97-1003 </papid>wang et al, 2005), <papid> W05-1516 </papid>and yet they are not being used in current large margin training algorithms.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2067">
<title id=" W06-2904.xml">improved large margin dependency parsing via local constraints and laplacian regularization </title>
<section> lexicalized dependency parsing.  </section>
<citcontext>
<prevsection>
<prevsent>that is, the dependency structure of sentence is directed tree where the nodes are the words in the sentence and links represent the direct dependency relationships between the words; see figure 1.
</prevsent>
<prevsent>there has been growing interest in dependency parsing in recent years.
</prevsent>
</prevsection>
<citsent citstr=" W02-1039 ">
(fox, 2002) <papid> W02-1039 </papid>found that the dependency structures of pair of translated sentences have greater degree of cohesion than phrase structures.</citsent>
<aftsection>
<nextsent>(cherry andlin, 2003) <papid> P03-1012 </papid>exploited such cohesion between the dependency structures to improve the quality of word alignment of parallel sentences.</nextsent>
<nextsent>dependency relations have also been found to be useful in information extraction (culotta and sorensen, 2004; <papid> P04-1054 </papid>yangarber et al, 2000).<papid> A00-1039 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2068">
<title id=" W06-2904.xml">improved large margin dependency parsing via local constraints and laplacian regularization </title>
<section> lexicalized dependency parsing.  </section>
<citcontext>
<prevsection>
<prevsent>there has been growing interest in dependency parsing in recent years.
</prevsent>
<prevsent>(fox, 2002) <papid> W02-1039 </papid>found that the dependency structures of pair of translated sentences have greater degree of cohesion than phrase structures.</prevsent>
</prevsection>
<citsent citstr=" P03-1012 ">
(cherry andlin, 2003) <papid> P03-1012 </papid>exploited such cohesion between the dependency structures to improve the quality of word alignment of parallel sentences.</citsent>
<aftsection>
<nextsent>dependency relations have also been found to be useful in information extraction (culotta and sorensen, 2004; <papid> P04-1054 </papid>yangarber et al, 2000).<papid> A00-1039 </papid></nextsent>
<nextsent>a key aspect of dependency tree is that it doesnot necessarily report parts-of-speech or phrase la bels.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2069">
<title id=" W06-2904.xml">improved large margin dependency parsing via local constraints and laplacian regularization </title>
<section> lexicalized dependency parsing.  </section>
<citcontext>
<prevsection>
<prevsent>(fox, 2002) <papid> W02-1039 </papid>found that the dependency structures of pair of translated sentences have greater degree of cohesion than phrase structures.</prevsent>
<prevsent>(cherry andlin, 2003) <papid> P03-1012 </papid>exploited such cohesion between the dependency structures to improve the quality of word alignment of parallel sentences.</prevsent>
</prevsection>
<citsent citstr=" P04-1054 ">
dependency relations have also been found to be useful in information extraction (culotta and sorensen, 2004; <papid> P04-1054 </papid>yangarber et al, 2000).<papid> A00-1039 </papid></citsent>
<aftsection>
<nextsent>a key aspect of dependency tree is that it doesnot necessarily report parts-of-speech or phrase labels.
</nextsent>
<nextsent>not requiring parts-of-speech is especially beneficial for languages such as chinese, whereparts-of-speech are not as clearly defined as english.
</nextsent>
<nextsent>in chinese, clear indicators of words partof-speech such as suffixes ?-ment?, ?-ous?
</nextsent>
<nextsent>or function words such as the?, are largely absent.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2070">
<title id=" W06-2904.xml">improved large margin dependency parsing via local constraints and laplacian regularization </title>
<section> lexicalized dependency parsing.  </section>
<citcontext>
<prevsection>
<prevsent>(fox, 2002) <papid> W02-1039 </papid>found that the dependency structures of pair of translated sentences have greater degree of cohesion than phrase structures.</prevsent>
<prevsent>(cherry andlin, 2003) <papid> P03-1012 </papid>exploited such cohesion between the dependency structures to improve the quality of word alignment of parallel sentences.</prevsent>
</prevsection>
<citsent citstr=" A00-1039 ">
dependency relations have also been found to be useful in information extraction (culotta and sorensen, 2004; <papid> P04-1054 </papid>yangarber et al, 2000).<papid> A00-1039 </papid></citsent>
<aftsection>
<nextsent>a key aspect of dependency tree is that it doesnot necessarily report parts-of-speech or phrase labels.
</nextsent>
<nextsent>not requiring parts-of-speech is especially beneficial for languages such as chinese, whereparts-of-speech are not as clearly defined as english.
</nextsent>
<nextsent>in chinese, clear indicators of words partof-speech such as suffixes ?-ment?, ?-ous?
</nextsent>
<nextsent>or function words such as the?, are largely absent.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2072">
<title id=" W06-2904.xml">improved large margin dependency parsing via local constraints and laplacian regularization </title>
<section> lexicalized dependency parsing.  </section>
<citcontext>
<prevsection>
<prevsent>parser made use of bi-lexical statistics only1.49% of the time.
</prevsent>
<prevsent>the parser has to compute back off probability using parts-of-speech in vast majority of the cases.
</prevsent>
</prevsection>
<citsent citstr=" W01-0521 ">
in fact, it was found in (gildea, 2001) <papid> W01-0521 </papid>that the removal of bi-lexical statistics from state of the art pcfg parser resulted in very little change in the output.</citsent>
<aftsection>
<nextsent>(klein and manning, 2003) <papid> P03-1054 </papid>presented an un lexicalized parser that eliminated all lexicalized parameters.</nextsent>
<nextsent>its performance was close to the state of the art lexicalized parsers.nevertheless, in this paper we follow there cent work of (wang et al, 2005) <papid> W05-1516 </papid>and consider acompletely lexicalized parser that uses no parts-of speech or grammatical categories of any kind.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2073">
<title id=" W06-2904.xml">improved large margin dependency parsing via local constraints and laplacian regularization </title>
<section> lexicalized dependency parsing.  </section>
<citcontext>
<prevsection>
<prevsent>the parser has to compute back off probability using parts-of-speech in vast majority of the cases.
</prevsent>
<prevsent>in fact, it was found in (gildea, 2001) <papid> W01-0521 </papid>that the removal of bi-lexical statistics from state of the art pcfg parser resulted in very little change in the output.</prevsent>
</prevsection>
<citsent citstr=" P03-1054 ">
(klein and manning, 2003) <papid> P03-1054 </papid>presented an un lexicalized parser that eliminated all lexicalized parameters.</citsent>
<aftsection>
<nextsent>its performance was close to the state of the art lexicalized parsers.nevertheless, in this paper we follow there cent work of (wang et al, 2005) <papid> W05-1516 </papid>and consider acompletely lexicalized parser that uses no parts-of speech or grammatical categories of any kind.</nextsent>
<nextsent>even though part-of-speech lexicon has always been considered to be necessary in any natural language parser, (wang et al, 2005) <papid> W05-1516 </papid>showed that distributional word similarities from large unannotated corpus can be used to supplant part-of-speech smoothing with word similarity smoothing, to still achieve state of the art dependency parsing accuracy for chinese.before discussing our modifications to large margin training for parsing in detail, we first present the dependency parsing model we use.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2078">
<title id=" W06-2904.xml">improved large margin dependency parsing via local constraints and laplacian regularization </title>
<section> dependency parsing model.  </section>
<citcontext>
<prevsection>
<prevsent>given sentence        we are interested in computing directed dependency tree, 22 , over  . in particular, we assume that directed dependency tree  consists of ordered pairs of words in  such that each word appears in at least one pair and each word has in-degree at most one.
</prevsent>
<prevsent>dependency trees are usually assumed to be projective (no crossing arcs), which means that if there is an arc fffifl , then  is an ancestor of all the words between ffi and  . let  !  denote the set of all the directed, projective trees that span  . given an input sentence  , we would like to be able to compute the best parse; that is, projective tree, $#  !  , that obtains the highest score?.
</prevsent>
</prevsection>
<citsent citstr=" C96-1058 ">
in particular, we follow (eisner, 1996; <papid> C96-1058 </papid>eisner and satta, 1999; <papid> P99-1059 </papid>mcdonald et al, 2005) <papid> P05-1012 </papid>and assume that the score of complete spanning tree  forgiven sentence, whether probabilistically motivated or not, can be decomposed as sum of local scores for each link (a word pair).</citsent>
<aftsection>
<nextsent>in which case, the parsing problem reduces to &amp;% ( *),+.-/ 0 132*46587:9 ; 5= ? @a cb,9d2fl1   e   (1) where the score fff g can depend on any measurable property of ffi and  within the tree  . this formulation is sufficiently general to capture most dependency parsing models, including probabilistic dependency models (wang et al, 2005; <papid> W05-1516 </papid>eisner, 1996) <papid> C96-1058 </papid>as well as non-probabilistic models (mc donald et al, 2005).<papid> P05-1012 </papid></nextsent>
<nextsent>for standard scoring functions, parsing requires an h:ikj  dynamic programming algorithm to compute projective tree that obtains the maximum score (eisner and satta, 1999; <papid> P99-1059 </papid>wang et al, 2005; <papid> W05-1516 </papid>mcdonald et al, 2005).<papid> P05-1012 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2079">
<title id=" W06-2904.xml">improved large margin dependency parsing via local constraints and laplacian regularization </title>
<section> dependency parsing model.  </section>
<citcontext>
<prevsection>
<prevsent>given sentence        we are interested in computing directed dependency tree, 22 , over  . in particular, we assume that directed dependency tree  consists of ordered pairs of words in  such that each word appears in at least one pair and each word has in-degree at most one.
</prevsent>
<prevsent>dependency trees are usually assumed to be projective (no crossing arcs), which means that if there is an arc fffifl , then  is an ancestor of all the words between ffi and  . let  !  denote the set of all the directed, projective trees that span  . given an input sentence  , we would like to be able to compute the best parse; that is, projective tree, $#  !  , that obtains the highest score?.
</prevsent>
</prevsection>
<citsent citstr=" P99-1059 ">
in particular, we follow (eisner, 1996; <papid> C96-1058 </papid>eisner and satta, 1999; <papid> P99-1059 </papid>mcdonald et al, 2005) <papid> P05-1012 </papid>and assume that the score of complete spanning tree  forgiven sentence, whether probabilistically motivated or not, can be decomposed as sum of local scores for each link (a word pair).</citsent>
<aftsection>
<nextsent>in which case, the parsing problem reduces to &amp;% ( *),+.-/ 0 132*46587:9 ; 5= ? @a cb,9d2fl1   e   (1) where the score fff g can depend on any measurable property of ffi and  within the tree  . this formulation is sufficiently general to capture most dependency parsing models, including probabilistic dependency models (wang et al, 2005; <papid> W05-1516 </papid>eisner, 1996) <papid> C96-1058 </papid>as well as non-probabilistic models (mc donald et al, 2005).<papid> P05-1012 </papid></nextsent>
<nextsent>for standard scoring functions, parsing requires an h:ikj  dynamic programming algorithm to compute projective tree that obtains the maximum score (eisner and satta, 1999; <papid> P99-1059 </papid>wang et al, 2005; <papid> W05-1516 </papid>mcdonald et al, 2005).<papid> P05-1012 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2114">
<title id=" W06-2904.xml">improved large margin dependency parsing via local constraints and laplacian regularization </title>
<section> distributional word similarity.  </section>
<citcontext>
<prevsection>
<prevsent>25 the basic intuition behind similarity smoothing is that words that tend to appear in the same contexts tend to have similar meanings.
</prevsent>
<prevsent>this is knownas the distributional hypothesis in linguistics (har ris, 1968).
</prevsent>
</prevsection>
<citsent citstr=" P90-1034 ">
for example, the words test and exam are similar because both of them can follow verbs such as administer, cancel, cheat on, conduct, etc. many methods have been proposed to compute distributional similarity between words, e.g., (hin dle, 1990; <papid> P90-1034 </papid>pereira et al, 1993; <papid> P93-1024 </papid>grefenstette, 1994; lin, 1998).<papid> P98-2127 </papid></citsent>
<aftsection>
<nextsent>almost all of the methods represent aword by feature vector where each feature corresponds to type of context in which the word appeared.
</nextsent>
<nextsent>they differ in how the feature vectors are constructed and how the similarity between two feature vectors is computed.
</nextsent>
<nextsent>in our approach below, we define the features of word  to be the set of words that occurred withina small window of  in large corpus.
</nextsent>
<nextsent>the context window of  consists of the closest non-stopword on each side of  and the stop-words in between.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2115">
<title id=" W06-2904.xml">improved large margin dependency parsing via local constraints and laplacian regularization </title>
<section> distributional word similarity.  </section>
<citcontext>
<prevsection>
<prevsent>25 the basic intuition behind similarity smoothing is that words that tend to appear in the same contexts tend to have similar meanings.
</prevsent>
<prevsent>this is knownas the distributional hypothesis in linguistics (har ris, 1968).
</prevsent>
</prevsection>
<citsent citstr=" P93-1024 ">
for example, the words test and exam are similar because both of them can follow verbs such as administer, cancel, cheat on, conduct, etc. many methods have been proposed to compute distributional similarity between words, e.g., (hin dle, 1990; <papid> P90-1034 </papid>pereira et al, 1993; <papid> P93-1024 </papid>grefenstette, 1994; lin, 1998).<papid> P98-2127 </papid></citsent>
<aftsection>
<nextsent>almost all of the methods represent aword by feature vector where each feature corresponds to type of context in which the word appeared.
</nextsent>
<nextsent>they differ in how the feature vectors are constructed and how the similarity between two feature vectors is computed.
</nextsent>
<nextsent>in our approach below, we define the features of word  to be the set of words that occurred withina small window of  in large corpus.
</nextsent>
<nextsent>the context window of  consists of the closest non-stopword on each side of  and the stop-words in between.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2116">
<title id=" W06-2904.xml">improved large margin dependency parsing via local constraints and laplacian regularization </title>
<section> distributional word similarity.  </section>
<citcontext>
<prevsection>
<prevsent>25 the basic intuition behind similarity smoothing is that words that tend to appear in the same contexts tend to have similar meanings.
</prevsent>
<prevsent>this is knownas the distributional hypothesis in linguistics (har ris, 1968).
</prevsent>
</prevsection>
<citsent citstr=" P98-2127 ">
for example, the words test and exam are similar because both of them can follow verbs such as administer, cancel, cheat on, conduct, etc. many methods have been proposed to compute distributional similarity between words, e.g., (hin dle, 1990; <papid> P90-1034 </papid>pereira et al, 1993; <papid> P93-1024 </papid>grefenstette, 1994; lin, 1998).<papid> P98-2127 </papid></citsent>
<aftsection>
<nextsent>almost all of the methods represent aword by feature vector where each feature corresponds to type of context in which the word appeared.
</nextsent>
<nextsent>they differ in how the feature vectors are constructed and how the similarity between two feature vectors is computed.
</nextsent>
<nextsent>in our approach below, we define the features of word  to be the set of words that occurred withina small window of  in large corpus.
</nextsent>
<nextsent>the context window of  consists of the closest non-stopword on each side of  and the stop-words in between.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2125">
<title id=" W06-2409.xml">modeling monolingual and bilingual collocation dictionaries in description logics </title>
<section> modeling in owl dl.  </section>
<citcontext>
<prevsection>
<prevsent>the possibility to infer explicit knowledge from implicit statements is core feature of owl dl and can be performed by usingdl reason ers (such as fact5, pellet6 or racer pro7).
</prevsent>
<prevsent>the most basic inference is achieved viathe subsumption relation among classes or properties in the respective hierarchy (see above), but also more sophisticated inferences are possible.among others, these may involve the formal attributes of properties just mentioned.
</prevsent>
</prevsection>
<citsent citstr=" J96-2002 ">
for example, 4as the emphasis in our work is on morphology, syntax and lexical combinatorics, we profit from the formal properties of dl without feeling the need for non-monotonicity as implemented, for example, in datr (evans and gazdar, 1996).<papid> J96-2002 </papid></citsent>
<aftsection>
<nextsent>5http://www.cs.man.ac.uk/horrocks/ fact/ 6http://www.mindswap.org/2003/pellet/ 7http://www.racer-systems.comstating that instance is linked to via symmetric property leads reasoner to infer that bis also linked to via p. in conjunction with tran sit ivity, relatively small set of explicit statements may suffice to interrelate several instances implicitly (i.e. all instances in particular equivalence class created by p).consistency.
</nextsent>
<nextsent>in addition to inferences, dl rea soners can further be used to check the consistency of an owl dl model.
</nextsent>
<nextsent>one of the primary objectives is to check whether the assertions made about classes and their instances (see above) are logically consistent or whether there are contradictions.
</nextsent>
<nextsent>this consistency checking is based on the open-world assumption, which states that what cannot be proven to be true is not believed to befalse?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2126">
<title id=" W06-1658.xml">entity annotation based on inverse index operations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we build an index forthe tokens in the document collection first.
</prevsent>
<prevsent>using set of operators on the index, we can generate new index entries for sequences of tokens that match any given regular expression.
</prevsent>
</prevsection>
<citsent citstr=" P02-1022 ">
since large class of annotators (e.g., gate (cunningham etal., 2002)) <papid> P02-1022 </papid>can be built using cascading regular expressions, this approach allows us to support annotation of the document collection purely from the index.</citsent>
<aftsection>
<nextsent>we show both theoretically and experimentally that this approach can lead to substantial reductions in computational complexity, since the orderof computation is dependent on the size of the indexes and not the number of tokens in the document collection.
</nextsent>
<nextsent>in most cases, the index sizes used for computing the annotations will be small fraction of the total number of tokens.in (cho and raja gopalan, 2002) the authors develop method for speeding up the evaluation of regular expression r? on large text corpus by use of an optimally constructed multi-gram indexto filter documents that will match r?.
</nextsent>
<nextsent>unfortunately, their method requires access to the document collection for the final match of r? to the filtered document set, which can be very time consuming.
</nextsent>
<nextsent>the other bodies of related prior work concern indexing annotated data (cooper et al,2001; li and moon, 2001) and methods for document level annotation (agichtein and gravano,2000; mccallum et al, 2000).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2128">
<title id=" W06-2705.xml">multidimensional annotation and alignment in an english german translation corpus </title>
<section> croco xml.  </section>
<citcontext>
<prevsection>
<prevsent>2.1 token isation and indexing.
</prevsent>
<prevsent>the first layer to be presented here is the tokeni sation layer.
</prevsent>
</prevsection>
<citsent citstr=" A00-1031 ">
token isation is performed in croco for both german and english by tnt (brants 2000), <papid> A00-1031 </papid>statistical part-of-speech tagger.</citsent>
<aftsection>
<nextsent>as shown in figure 2.1 each token annotated with the attribute strg has also an id attribute, which indicates the position of the word in the text.
</nextsent>
<nextsent>this id represents the anchor for all xpointers pointing to the token isation file by an id starting with t?.
</nextsent>
<nextsent>the file is identified by the name attribute.
</nextsent>
<nextsent>the xml:lang attribute indicates the language of the file, doctype provides information on whether the present file is an original or translation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2129">
<title id=" W06-2705.xml">multidimensional annotation and alignment in an english german translation corpus </title>
<section> croco xml.  </section>
<citcontext>
<prevsection>
<prevsent>by aligning words, grammatical functions, clauses and sentences, the connection between original and translated text is made visible.
</prevsent>
<prevsent>the use of this multi-layer alignment will become clearer from the discussion of sample query in section 3.
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
for the purpose of the croco project word alignment is realised with giza++ (och &amp; ney 2003), <papid> J03-1002 </papid>statistical alignment tool.</citsent>
<aftsection>
<nextsent>chunks and clauses are aligned manually with the help of mmax ii (mller &amp; strube 2003), tool allowing assignment of own categories and linking units.
</nextsent>
<nextsent>finally, sentences are aligned using win align, an alignment tool within the translators workbench by trados (heyn 1996).
</nextsent>
<nextsent>the alignment procedure produces four new layers.
</nextsent>
<nextsent>it follows the xces standard.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2130">
<title id=" W06-3202.xml">improving syllabification models with phonotactic knowledge </title>
<section> abstract </section>
<citcontext>
<prevsection>
<prevsent>we report on series of experiments with probabilistic context-free grammars predicting english and german syllable structure.
</prevsent>
<prevsent>the treebank-trained grammars are evaluated on syllabification task.
</prevsent>
</prevsection>
<citsent citstr=" W02-0608 ">
the grammar used by muller (2002) <papid> W02-0608 </papid>servesas point of comparison.</citsent>
<aftsection>
<nextsent>as she evaluates the grammar only for german, we re implement the grammar and experiment with additional phonotactic features.
</nextsent>
<nextsent>using bi-grams within the syllable, we can model the dependency from the previous consonant in the onset and coda.
</nextsent>
<nextsent>a 10 fold cross validation procedure shows thatsyllabification can be improved by incorporating this type of phonotactic knowledge.
</nextsent>
<nextsent>compared to the grammar of muller(2002), <papid> W02-0608 </papid>syllable boundary accuracy increases from 95.8% to 97.2% for english, and from 95.9% to 97.2% forger man. moreover, our experiments with different syllable structures point out that there are dependencies between the on set on the nucleus for german but not for english.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2133">
<title id=" W06-3202.xml">improving syllabification models with phonotactic knowledge </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, the lexicons are finite and every natural language has productive word formation processes.thus, tts system needs module which converts letters to sounds and second module whichsyllabifies these sound sequences.
</prevsent>
<prevsent>the syllabification information is important to assign the stress status of the syllable, to calculate the phone duration(van santen et al (1997)), and to apply phonological rules (kahn (1976), blevins (1995)).
</prevsent>
</prevsection>
<citsent citstr=" P01-1053 ">
many automatic syllabification methods have been suggested e.g., (daelemans and vanden bosch, 1992; vanden bosch, 1997; kiraz and mobius, 1998; vroomenet al, 1998; muller, 2001; <papid> P01-1053 </papid>marchand et al, to appear 2006).</citsent>
<aftsection>
<nextsent>muller (2001) <papid> P01-1053 </papid>shows that incorporating syllable structure improves the prediction of syllable boundaries.</nextsent>
<nextsent>the syllabification accuracy increases if the onset and coda is more fine-grained(muller, 2002).<papid> W02-0608 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2166">
<title id=" W06-1503.xml">the meta grammar goes multilingual a cross linguistic look at the v2phenomenon </title>
<section> an overview of meta grammars.  </section>
<citcontext>
<prevsection>
<prevsent>or wh-traces?
</prevsent>
<prevsent>are accepted by most if not all existing frameworks, even though the machinery of given framework maynot necessarily account explicitly for each invari ant.
</prevsent>
</prevsection>
<citsent citstr=" C00-1065 ">
for instance, tag does not have an explicit notion of syntactic function: although by convention node indices tend to reflect function, it is not enforced by the frameworks machinery.1 hyper tags based on such framework- and language-neutral syntactic properties, kinyon (2000) <papid> C00-1065 </papid>defined the notion of hyper tag (ht), combination of supertags (st) srinivas (1997) and of the mg.</citsent>
<aftsection>
<nextsent>a st is tag elementary tree, which provides richer information than standard pos tagging, but in framework-specific manner (tag), and also in grammar-specific manner since st tagset cant be ported from one tag to another tag.
</nextsent>
<nextsent>a ht is an abstraction of sts, where the main syntactic properties of any givenst is encoded in general readable feature structure (fs), by recording which mg classes st inherited from when it was generated.
</nextsent>
<nextsent>figure 1 illustrates the  st, ht
</nextsent>
<nextsent>pair for par qui sera accompagnee marie by whom will mary be accompa nied?.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2167">
<title id=" W06-1503.xml">the meta grammar goes multilingual a cross linguistic look at the v2phenomenon </title>
<section> an overview of meta grammars.  </section>
<citcontext>
<prevsection>
<prevsent>and dimension 3?, where each feature takes its value from the mg terminal classes used to generate given st.the xmg tool cand itos tool brought significant linguistic insight, therefore we essentially retain the above-mentioned syntactic invariants.
</prevsent>
<prevsent>however, more recent mg implementations have been developed since, each adding its significant contribution to the underlying meta grammatical hypothesis.in this paper, we use the extensible meta gram mar (xmg) tool which was developed by crabbe?
</prevsent>
</prevsection>
<citsent citstr=" W06-1520 ">
1but several attempts have been made to explicitly add functions to tag, e.g. by kameyama (1986) to retain the benefits of both tag and lfg, or by prolo (2006) <papid> W06-1520 </papid>to account for the coordination of constituents of different categories, yet sharing the same function.</citsent>
<aftsection>
<nextsent>s pp par nwh (qui) aux sera v accompagnee n  (marie)   dimension1 strict transitive dimension2 personalfullpassive dimension3 subject invertedsubjectcomplement whquestionedbycomplement figure 1:  supertag, hypertag
</nextsent>
<nextsent>pair for accompagnee (accompanied?)
</nextsent>
<nextsent>obtained with can ditos meta grammar compiler (2005).
</nextsent>
<nextsent>in xmg, an mg consists of set of classes similar to those in object-oriented programming, which are structured into multiple inheritance hierarchy.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2168">
<title id=" W07-0701.xml">using dependency order templates to improve generality in translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the treelet approach models reordering separately from lexical choice, using discriminatively trained order model, which allows tree lets to apply broadly, and has shown better generalization to new domains, but suffers factorially large search space.
</prevsent>
<prevsent>we introduce new reordering model based on dependency order templates, and show that it outperforms both phrasal and treelet systems on in-domain and out-of-domain text, while limiting the search space.
</prevsent>
</prevsection>
<citsent citstr=" N03-1017 ">
modern phrasal smt systems such as (koehn et al., 2003) <papid> N03-1017 </papid>derive much of their power from being able to memorize and use long phrases.</citsent>
<aftsection>
<nextsent>phrases allow for non-compositional translation, local reordering and contextual lexical choice.
</nextsent>
<nextsent>however the phrases are fully lexicalized, which means they generalize poorly to even slightly out of-domain text.
</nextsent>
<nextsent>in an open competition (koehn &amp; monz, 2006) <papid> W06-3114 </papid>systems trained on parliamentary proceedings were tested on text from  news commentary  web sites, very slightly different domain.</nextsent>
<nextsent>the 9 phrasal systems in the english to spanish track suffered an absolute drop in bleu score of between 4.4% and 6.34% (14% to 27% relative).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2169">
<title id=" W07-0701.xml">using dependency order templates to improve generality in translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>phrases allow for non-compositional translation, local reordering and contextual lexical choice.
</prevsent>
<prevsent>however the phrases are fully lexicalized, which means they generalize poorly to even slightly out of-domain text.
</prevsent>
</prevsection>
<citsent citstr=" W06-3114 ">
in an open competition (koehn &amp; monz, 2006) <papid> W06-3114 </papid>systems trained on parliamentary proceedings were tested on text from  news commentary  web sites, very slightly different domain.</citsent>
<aftsection>
<nextsent>the 9 phrasal systems in the english to spanish track suffered an absolute drop in bleu score of between 4.4% and 6.34% (14% to 27% relative).
</nextsent>
<nextsent>the treelet system of menezes et al (2006) <papid> W06-3124 </papid>fared somewhat better but still suffered an absolute drop of 3.61%.</nextsent>
<nextsent>clearly there is need for approaches with greater powers of generalization.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2170">
<title id=" W07-0701.xml">using dependency order templates to improve generality in translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in an open competition (koehn &amp; monz, 2006) <papid> W06-3114 </papid>systems trained on parliamentary proceedings were tested on text from  news commentary  web sites, very slightly different domain.</prevsent>
<prevsent>the 9 phrasal systems in the english to spanish track suffered an absolute drop in bleu score of between 4.4% and 6.34% (14% to 27% relative).</prevsent>
</prevsection>
<citsent citstr=" W06-3124 ">
the treelet system of menezes et al (2006) <papid> W06-3124 </papid>fared somewhat better but still suffered an absolute drop of 3.61%.</citsent>
<aftsection>
<nextsent>clearly there is need for approaches with greater powers of generalization.
</nextsent>
<nextsent>there are multiple facets to this issue, including handling of unknown words, new senses of known words etc. in this work, we will focus on the issue of reordering, i.e. can we learn how to transform the sentence structure of one language into the sentence structure of another, in way that is not tied to specific domain or sub-domains, or indeed, sequences of individual words.
</nextsent>
<nextsent>an early attempt at greater generality in purely phrasal setting was the alignment template approach (och &amp; ney 2004); <papid> J04-4002 </papid>newer approaches include formally syntactic (chiang 2005), <papid> P05-1033 </papid>and linguistically syntactic approaches (quirk et al 2005), (<papid> P05-1034 </papid>huang et al 2006).<papid> W06-3601 </papid></nextsent>
<nextsent>in the next section, we examine these representative approaches to the reordering problem.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2171">
<title id=" W07-0701.xml">using dependency order templates to improve generality in translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>clearly there is need for approaches with greater powers of generalization.
</prevsent>
<prevsent>there are multiple facets to this issue, including handling of unknown words, new senses of known words etc. in this work, we will focus on the issue of reordering, i.e. can we learn how to transform the sentence structure of one language into the sentence structure of another, in way that is not tied to specific domain or sub-domains, or indeed, sequences of individual words.
</prevsent>
</prevsection>
<citsent citstr=" J04-4002 ">
an early attempt at greater generality in purely phrasal setting was the alignment template approach (och &amp; ney 2004); <papid> J04-4002 </papid>newer approaches include formally syntactic (chiang 2005), <papid> P05-1033 </papid>and linguistically syntactic approaches (quirk et al 2005), (<papid> P05-1034 </papid>huang et al 2006).<papid> W06-3601 </papid></citsent>
<aftsection>
<nextsent>in the next section, we examine these representative approaches to the reordering problem.
</nextsent>
<nextsent>our discussion of related work will be grounded in the following tiny english to spanish example, where the training set includes: very old book un libro ms antiguo book very old1 the old man el hombre viejo the man old it is very important es muy import ante is very important 1 english gloss of spanish sentences in italics..
</nextsent>
<nextsent>1 and the test sentence and reference translation are very old man un hombre muy viejo man very old note that while the first training pair has the correct structure for the test sentence, most of the contextually correct lexical choices come from the other two pairs.
</nextsent>
<nextsent>2.1 phrasal translation, alignment templates.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2172">
<title id=" W07-0701.xml">using dependency order templates to improve generality in translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>clearly there is need for approaches with greater powers of generalization.
</prevsent>
<prevsent>there are multiple facets to this issue, including handling of unknown words, new senses of known words etc. in this work, we will focus on the issue of reordering, i.e. can we learn how to transform the sentence structure of one language into the sentence structure of another, in way that is not tied to specific domain or sub-domains, or indeed, sequences of individual words.
</prevsent>
</prevsection>
<citsent citstr=" P05-1033 ">
an early attempt at greater generality in purely phrasal setting was the alignment template approach (och &amp; ney 2004); <papid> J04-4002 </papid>newer approaches include formally syntactic (chiang 2005), <papid> P05-1033 </papid>and linguistically syntactic approaches (quirk et al 2005), (<papid> P05-1034 </papid>huang et al 2006).<papid> W06-3601 </papid></citsent>
<aftsection>
<nextsent>in the next section, we examine these representative approaches to the reordering problem.
</nextsent>
<nextsent>our discussion of related work will be grounded in the following tiny english to spanish example, where the training set includes: very old book un libro ms antiguo book very old1 the old man el hombre viejo the man old it is very important es muy import ante is very important 1 english gloss of spanish sentences in italics..
</nextsent>
<nextsent>1 and the test sentence and reference translation are very old man un hombre muy viejo man very old note that while the first training pair has the correct structure for the test sentence, most of the contextually correct lexical choices come from the other two pairs.
</nextsent>
<nextsent>2.1 phrasal translation, alignment templates.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2173">
<title id=" W07-0701.xml">using dependency order templates to improve generality in translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>clearly there is need for approaches with greater powers of generalization.
</prevsent>
<prevsent>there are multiple facets to this issue, including handling of unknown words, new senses of known words etc. in this work, we will focus on the issue of reordering, i.e. can we learn how to transform the sentence structure of one language into the sentence structure of another, in way that is not tied to specific domain or sub-domains, or indeed, sequences of individual words.
</prevsent>
</prevsection>
<citsent citstr=" P05-1034 ">
an early attempt at greater generality in purely phrasal setting was the alignment template approach (och &amp; ney 2004); <papid> J04-4002 </papid>newer approaches include formally syntactic (chiang 2005), <papid> P05-1033 </papid>and linguistically syntactic approaches (quirk et al 2005), (<papid> P05-1034 </papid>huang et al 2006).<papid> W06-3601 </papid></citsent>
<aftsection>
<nextsent>in the next section, we examine these representative approaches to the reordering problem.
</nextsent>
<nextsent>our discussion of related work will be grounded in the following tiny english to spanish example, where the training set includes: very old book un libro ms antiguo book very old1 the old man el hombre viejo the man old it is very important es muy import ante is very important 1 english gloss of spanish sentences in italics..
</nextsent>
<nextsent>1 and the test sentence and reference translation are very old man un hombre muy viejo man very old note that while the first training pair has the correct structure for the test sentence, most of the contextually correct lexical choices come from the other two pairs.
</nextsent>
<nextsent>2.1 phrasal translation, alignment templates.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2174">
<title id=" W07-0701.xml">using dependency order templates to improve generality in translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>clearly there is need for approaches with greater powers of generalization.
</prevsent>
<prevsent>there are multiple facets to this issue, including handling of unknown words, new senses of known words etc. in this work, we will focus on the issue of reordering, i.e. can we learn how to transform the sentence structure of one language into the sentence structure of another, in way that is not tied to specific domain or sub-domains, or indeed, sequences of individual words.
</prevsent>
</prevsection>
<citsent citstr=" W06-3601 ">
an early attempt at greater generality in purely phrasal setting was the alignment template approach (och &amp; ney 2004); <papid> J04-4002 </papid>newer approaches include formally syntactic (chiang 2005), <papid> P05-1033 </papid>and linguistically syntactic approaches (quirk et al 2005), (<papid> P05-1034 </papid>huang et al 2006).<papid> W06-3601 </papid></citsent>
<aftsection>
<nextsent>in the next section, we examine these representative approaches to the reordering problem.
</nextsent>
<nextsent>our discussion of related work will be grounded in the following tiny english to spanish example, where the training set includes: very old book un libro ms antiguo book very old1 the old man el hombre viejo the man old it is very important es muy import ante is very important 1 english gloss of spanish sentences in italics..
</nextsent>
<nextsent>1 and the test sentence and reference translation are very old man un hombre muy viejo man very old note that while the first training pair has the correct structure for the test sentence, most of the contextually correct lexical choices come from the other two pairs.
</nextsent>
<nextsent>2.1 phrasal translation, alignment templates.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2176">
<title id=" W07-0701.xml">using dependency order templates to improve generality in translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>a un very ms old antiguo very old ms antiguo old viejo man hombre old man hombre viejo very muy table 2.1: relevant extracted phrases looking at this as sparse data issue we might suspect that generalization could solve the problem.
</prevsent>
<prevsent>the alignment template approach (och &amp; ney, 2004) <papid> J04-4002 </papid>uses word classes rather than lexical items to model phrase translation.</prevsent>
</prevsection>
<citsent citstr=" N04-1033 ">
yet this approach loses the advantage of context-sensitive lexical selection: the word translation model depends only on the word classes to sub categorize for translations, which leads to less accurate lexical choice in practice (zens &amp; ney, 2004).<papid> N04-1033 </papid></citsent>
<aftsection>
<nextsent>2.2 hierarchical translation.
</nextsent>
<nextsent>hierarchical systems (chiang, 2005) <papid> P05-1033 </papid>induce context-free grammar with one non-terminal directly from the parallel corpus, with the advantage of not requiring any additional knowledge source or tools, such as treebank or parser.</nextsent>
<nextsent>however this can lead to an explosion of rules.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2178">
<title id=" W07-0701.xml">using dependency order templates to improve generality in translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>2.3 constituency tree transduction.
</prevsent>
<prevsent>an alternate approach is to use linguistic information from parser.
</prevsent>
</prevsection>
<citsent citstr=" N04-1014 ">
transduction rules between spanish strings and english trees can be learned from word-aligned parallel corpus with parse trees on one side (graehl &amp; knight, 2004).<papid> N04-1014 </papid></citsent>
<aftsection>
<nextsent>such rules can be used to translate from spanish to english by searching for the best english language tree forgiven spanish language string (marcu et al, 2006).<papid> W06-1606 </papid></nextsent>
<nextsent>alternately english trees produced by parser can be transduced to 2 spanish strings using the same rules (huang et al, 2006).<papid> W06-3601 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2179">
<title id=" W07-0701.xml">using dependency order templates to improve generality in translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>an alternate approach is to use linguistic information from parser.
</prevsent>
<prevsent>transduction rules between spanish strings and english trees can be learned from word-aligned parallel corpus with parse trees on one side (graehl &amp; knight, 2004).<papid> N04-1014 </papid></prevsent>
</prevsection>
<citsent citstr=" W06-1606 ">
such rules can be used to translate from spanish to english by searching for the best english language tree forgiven spanish language string (marcu et al, 2006).<papid> W06-1606 </papid></citsent>
<aftsection>
<nextsent>alternately english trees produced by parser can be transduced to 2 spanish strings using the same rules (huang et al, 2006).<papid> W06-3601 </papid></nextsent>
<nextsent>translation rules may reach beyond one level in the syntax tree; this extended domain of locality allows many phenomena including both lexicalized and un lexicalized rules.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2187">
<title id=" W07-0701.xml">using dependency order templates to improve generality in translation </title>
<section> decoding.  </section>
<citcontext>
<prevsection>
<prevsent>th entry in the beam ?????????.
</prevsent>
<prevsent>figure 4.2 describes the transduction process.
</prevsent>
</prevsection>
<citsent citstr=" P07-1019 ">
since we approach decoding as xr transduction, the process is identical to that of constituency based algorithms (e.g. huang and chiang, 2007).<papid> P07-1019 </papid></citsent>
<aftsection>
<nextsent>there are several free parameters to tune: ? beam size ? maximum number of candidates per input node (in this paper we use 100) ? beam threshold ? maximum range of scores between top and bottom scoring candidate (we use logprob difference of 30) ? maximum combinations considered ? to bound search time, we can stop after specified number of elements are popped off the priority queue (we use 5000)
</nextsent>
<nextsent>we use all of the treelet models we described in quirk et al (2005) <papid> P05-1034 </papid>namely: ? treelet table with translation probabilities estimated using maximum likelihood, with absolute discounting.</nextsent>
<nextsent>discriminative tree-based order model.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2189">
<title id=" W07-0701.xml">using dependency order templates to improve generality in translation </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>a feature function that counts the number of artificial source order templates (see below) used in candidate.
</prevsent>
<prevsent>the models are combined in log-linear framework, with weights trained using minimum error rate training to optimize the bleu score.
</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
we evaluated the translation quality of the system using the bleu metric (papineni et al, 2002).<papid> P02-1040 </papid></citsent>
<aftsection>
<nextsent>we compared our system to pharaoh, leading phrasal smt decoder (koehn et al, 2003), <papid> N03-1017 </papid>and our treelet system.</nextsent>
<nextsent>we report numbers for english to spanish.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2191">
<title id=" W07-0701.xml">using dependency order templates to improve generality in translation </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>from the aligned tree pairs we extracted treelet table and an order template table.
</prevsent>
<prevsent>the comparison treelet system was identical except that no order template model was used.
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
the comparison phrasal system was constructed using the same giza++ alignments and the heuristic combination described in (och &amp; ney, 2003).<papid> J03-1002 </papid></citsent>
<aftsection>
<nextsent>except for the order models (pharaoh uses penalty on the deviance from monotone), the same models were used.
</nextsent>
<nextsent>all systems used treelet or phrase size of 7 and trigram language model.
</nextsent>
<nextsent>model weights were trained separately for all 3 systems using minimum error rate training to maximize bleu (och, 2003) <papid> P03-1021 </papid>on the development set (dev).</nextsent>
<nextsent>some decoder pruning parameters were tuned on the development test (devtest).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2192">
<title id=" W07-0701.xml">using dependency order templates to improve generality in translation </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>except for the order models (pharaoh uses penalty on the deviance from monotone), the same models were used.
</prevsent>
<prevsent>all systems used treelet or phrase size of 7 and trigram language model.
</prevsent>
</prevsection>
<citsent citstr=" P03-1021 ">
model weights were trained separately for all 3 systems using minimum error rate training to maximize bleu (och, 2003) <papid> P03-1021 </papid>on the development set (dev).</citsent>
<aftsection>
<nextsent>some decoder pruning parameters were tuned on the development test (devtest).
</nextsent>
<nextsent>the test and nc-test datasets were not used until final tests.
</nextsent>
<nextsent>we present the results of our system comparisons in table 7.1 and figure 7.1 using three different test sets: the in-domain development test data (devtest), the in-domain blind test data (test) and the out-of-domain news commentary test data (nc-test).
</nextsent>
<nextsent>all differences (except phrasal vs. template on devtest), are statistically significant at the =0.99 level under the bootstrap re sampling test.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2193">
<title id=" W06-1323.xml">relationship between utterances and enthusiasm in nontaskoriented conversational dialogue </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our results indicate that affective and cooperative utterances are significant to enthusiastic dialogue.
</prevsent>
<prevsent>for non-task-oriented conversational dialogue system (e.g. home robots), we should strive for dialogue strategy that is both enjoyable and enthusiastic, as well as efficient.
</prevsent>
</prevsection>
<citsent citstr=" W00-0304 ">
many studies have been conducted on efficient dialogue strategies (walker et al, 1998; litman et al, 2000; <papid> W00-0304 </papid>komatani et al, 2002), <papid> C02-1152 </papid>but it is not clear how to accomplish more human-like enthusiasm?</citsent>
<aftsection>
<nextsent>for conversational dialogue.
</nextsent>
<nextsent>the goal of this paper is to show the types of utterances that contribute to enthusiasm in conversational dialogues.
</nextsent>
<nextsent>we created conversational corpus annotated with two types of tags: one type indicates particular aspects of the utterance itself, while the other indicates the degree of enthusiasm in the dialogue.
</nextsent>
<nextsent>this section describes our corpus and tagging scheme in detail.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2194">
<title id=" W06-1323.xml">relationship between utterances and enthusiasm in nontaskoriented conversational dialogue </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our results indicate that affective and cooperative utterances are significant to enthusiastic dialogue.
</prevsent>
<prevsent>for non-task-oriented conversational dialogue system (e.g. home robots), we should strive for dialogue strategy that is both enjoyable and enthusiastic, as well as efficient.
</prevsent>
</prevsection>
<citsent citstr=" C02-1152 ">
many studies have been conducted on efficient dialogue strategies (walker et al, 1998; litman et al, 2000; <papid> W00-0304 </papid>komatani et al, 2002), <papid> C02-1152 </papid>but it is not clear how to accomplish more human-like enthusiasm?</citsent>
<aftsection>
<nextsent>for conversational dialogue.
</nextsent>
<nextsent>the goal of this paper is to show the types of utterances that contribute to enthusiasm in conversational dialogues.
</nextsent>
<nextsent>we created conversational corpus annotated with two types of tags: one type indicates particular aspects of the utterance itself, while the other indicates the degree of enthusiasm in the dialogue.
</nextsent>
<nextsent>this section describes our corpus and tagging scheme in detail.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2195">
<title id=" W06-1323.xml">relationship between utterances and enthusiasm in nontaskoriented conversational dialogue </title>
<section> corpus annotation.  </section>
<citcontext>
<prevsection>
<prevsent>this section describes our corpus and tagging scheme in detail.
</prevsent>
<prevsent>2.1 corpus collection.
</prevsent>
</prevsection>
<citsent citstr=" W01-1623 ">
as result of previous works, several conversational dialogue corpora have been collected with various settings (graff and bird, 2000; tseng,2001).<papid> W01-1623 </papid></citsent>
<aftsection>
<nextsent>the largest conversational dialogue corpus is the switchboard corpus, which consists ofabout 2400 conversational english dialogues between two unfamiliar speakers over the telephone on one of 70 topics (e.g. pets, family life, education, gun control, etc.).our corpus was collected from face-to-face interaction between two unfamiliar speakers.
</nextsent>
<nextsent>the reasons were 1) face-to-face interaction increases the number of enthusiastic utterances, relative to limited conversational channel interaction such asover the telephone; 2) the interaction between unfamiliar speakers reduces the enthusiasm resulting from unobserved reasons during the recording; 3)the exchange in two party dialogue will be simpler than that of multiparty dialogue.we created corpus containing ten conversational dialogues that were spoken by an operator (thirties, female) and one of ten subjects (twenties to sixties, equal numbers of males and females).before beginning the recording session, the subject chose three cards from fifteen cards on the following topics: food, travel, sport, hobbies, movies, prizes, tv programs, family, books, school, music, pets, shopping, recent purchases, celebrities straying from the selected topic was permitted, because these topic cards were only ever intend edas prompt to start the dialogue.
</nextsent>
<nextsent>thus, we collected ten dialogues, each about 20 minutes long.for convenience, in this paper, we refer to the operator as speaker1, and the subject as speaker2.
</nextsent>
<nextsent>161 2.2 annotation of das and rrs.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2196">
<title id=" W06-3601.xml">a syntax directed translator with extended domain of locality </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for example, the s(vo) structure in english is translated into vso word-order in arabic, an instance of complex reordering not captured by any 1 scfg (fig.
</prevsent>
<prevsent>2).to alleviate the non-isomorphism problem, (syn chronous) grammars with richer expressive power have been proposed whose rules apply to larger fragments of the tree.
</prevsent>
</prevsection>
<citsent citstr=" C90-3045 ">
for example, shieber and schabes (1990) <papid> C90-3045 </papid>introduce synchronous tree-adjoininggrammar (stag) and eisner (2003) <papid> P03-2041 </papid>uses synchronous tree-substitution grammar (stsg), which is restricted version of stag with no adjunctions.</citsent>
<aftsection>
<nextsent>stsgs and stags generate more tree relations than scfgs, e.g. the non-isomorphic tree pair in fig.
</nextsent>
<nextsent>2.this extra expressive power lies in the extended do main of locality (edl) (joshi and schabes, 1997),i.e., elementary structures beyond the scope of one level context-free productions.
</nextsent>
<nextsent>besides being linguistically motivated, the need for edl is also supported by empirical findings in mt that one-level rules are often inadequate (fox, 2002; <papid> W02-1039 </papid>galley et al, 2004).<papid> N04-1035 </papid></nextsent>
<nextsent>similarly, in the tree-transducer terminology,graehl and knight (2004) <papid> N04-1014 </papid>define extended tree transducers that have multi-level trees on the source-side.since an sd translator separates the source language analysis from the recursive transformation,the domains of locality in these two modules are orthogonal to each other: in this work, we use cfg based treebank parser but focuses on the extended domain in the recursive converter.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2197">
<title id=" W06-3601.xml">a syntax directed translator with extended domain of locality </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for example, the s(vo) structure in english is translated into vso word-order in arabic, an instance of complex reordering not captured by any 1 scfg (fig.
</prevsent>
<prevsent>2).to alleviate the non-isomorphism problem, (syn chronous) grammars with richer expressive power have been proposed whose rules apply to larger fragments of the tree.
</prevsent>
</prevsection>
<citsent citstr=" P03-2041 ">
for example, shieber and schabes (1990) <papid> C90-3045 </papid>introduce synchronous tree-adjoininggrammar (stag) and eisner (2003) <papid> P03-2041 </papid>uses synchronous tree-substitution grammar (stsg), which is restricted version of stag with no adjunctions.</citsent>
<aftsection>
<nextsent>stsgs and stags generate more tree relations than scfgs, e.g. the non-isomorphic tree pair in fig.
</nextsent>
<nextsent>2.this extra expressive power lies in the extended do main of locality (edl) (joshi and schabes, 1997),i.e., elementary structures beyond the scope of one level context-free productions.
</nextsent>
<nextsent>besides being linguistically motivated, the need for edl is also supported by empirical findings in mt that one-level rules are often inadequate (fox, 2002; <papid> W02-1039 </papid>galley et al, 2004).<papid> N04-1035 </papid></nextsent>
<nextsent>similarly, in the tree-transducer terminology,graehl and knight (2004) <papid> N04-1014 </papid>define extended tree transducers that have multi-level trees on the source-side.since an sd translator separates the source language analysis from the recursive transformation,the domains of locality in these two modules are orthogonal to each other: in this work, we use cfg based treebank parser but focuses on the extended domain in the recursive converter.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2198">
<title id=" W06-3601.xml">a syntax directed translator with extended domain of locality </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>stsgs and stags generate more tree relations than scfgs, e.g. the non-isomorphic tree pair in fig.
</prevsent>
<prevsent>2.this extra expressive power lies in the extended do main of locality (edl) (joshi and schabes, 1997),i.e., elementary structures beyond the scope of one level context-free productions.
</prevsent>
</prevsection>
<citsent citstr=" W02-1039 ">
besides being linguistically motivated, the need for edl is also supported by empirical findings in mt that one-level rules are often inadequate (fox, 2002; <papid> W02-1039 </papid>galley et al, 2004).<papid> N04-1035 </papid></citsent>
<aftsection>
<nextsent>similarly, in the tree-transducer terminology,graehl and knight (2004) <papid> N04-1014 </papid>define extended tree transducers that have multi-level trees on the source-side.since an sd translator separates the source language analysis from the recursive transformation,the domains of locality in these two modules are orthogonal to each other: in this work, we use cfg based treebank parser but focuses on the extended domain in the recursive converter.</nextsent>
<nextsent>following galley et al (2004), <papid> N04-1035 </papid>we use special class of extendedtree-to-string transducer (xrs for short) with multilevel left-hand-side (lhs) trees.1 since the righthand-side (rhs) string can be viewed as flat one level tree with the same nonterminal root from lhs (fig.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2199">
<title id=" W06-3601.xml">a syntax directed translator with extended domain of locality </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>stsgs and stags generate more tree relations than scfgs, e.g. the non-isomorphic tree pair in fig.
</prevsent>
<prevsent>2.this extra expressive power lies in the extended do main of locality (edl) (joshi and schabes, 1997),i.e., elementary structures beyond the scope of one level context-free productions.
</prevsent>
</prevsection>
<citsent citstr=" N04-1035 ">
besides being linguistically motivated, the need for edl is also supported by empirical findings in mt that one-level rules are often inadequate (fox, 2002; <papid> W02-1039 </papid>galley et al, 2004).<papid> N04-1035 </papid></citsent>
<aftsection>
<nextsent>similarly, in the tree-transducer terminology,graehl and knight (2004) <papid> N04-1014 </papid>define extended tree transducers that have multi-level trees on the source-side.since an sd translator separates the source language analysis from the recursive transformation,the domains of locality in these two modules are orthogonal to each other: in this work, we use cfg based treebank parser but focuses on the extended domain in the recursive converter.</nextsent>
<nextsent>following galley et al (2004), <papid> N04-1035 </papid>we use special class of extendedtree-to-string transducer (xrs for short) with multilevel left-hand-side (lhs) trees.1 since the righthand-side (rhs) string can be viewed as flat one level tree with the same nonterminal root from lhs (fig.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2200">
<title id=" W06-3601.xml">a syntax directed translator with extended domain of locality </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>2.this extra expressive power lies in the extended do main of locality (edl) (joshi and schabes, 1997),i.e., elementary structures beyond the scope of one level context-free productions.
</prevsent>
<prevsent>besides being linguistically motivated, the need for edl is also supported by empirical findings in mt that one-level rules are often inadequate (fox, 2002; <papid> W02-1039 </papid>galley et al, 2004).<papid> N04-1035 </papid></prevsent>
</prevsection>
<citsent citstr=" N04-1014 ">
similarly, in the tree-transducer terminology,graehl and knight (2004) <papid> N04-1014 </papid>define extended tree transducers that have multi-level trees on the source-side.since an sd translator separates the source language analysis from the recursive transformation,the domains of locality in these two modules are orthogonal to each other: in this work, we use cfg based treebank parser but focuses on the extended domain in the recursive converter.</citsent>
<aftsection>
<nextsent>following galley et al (2004), <papid> N04-1035 </papid>we use special class of extendedtree-to-string transducer (xrs for short) with multilevel left-hand-side (lhs) trees.1 since the righthand-side (rhs) string can be viewed as flat one level tree with the same nonterminal root from lhs (fig.</nextsent>
<nextsent>2), this framework is closely related to stsgs: they both have extended domain of locality on the source-side, while our framework remains as cfg on the target-side.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2202">
<title id=" W06-3601.xml">a syntax directed translator with extended domain of locality </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>2 would be s(x1:np, vp(x2:vb, x3:np))?
</prevsent>
<prevsent>x2 x1 x3 while section 3 will define the model formally, we first proceed with an example translation from english to chinese (note in particular that the inverted phrases between source and target): 1throughout this paper, we will use lhs and source-side interchangeably (so are rhs and target-side).
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
in accordance with our experiments, we also use english and chinese as the source and target languages, opposite to the foreign-to-english convention of brown et al (1993).<papid> J93-2003 </papid></citsent>
<aftsection>
<nextsent>(a) the gunman was [killed]1 by [the police]2 . parser ?
</nextsent>
<nextsent>(b) np-c dt the nn gunman vp vbd was vp-c vbn killed pp in by np-c dt the nn police punc . r1, r2 ?
</nextsent>
<nextsent>(c) qiang shou vp vbd was vp-c vbn killed pp in by np-c dt the nn police ? r3 ?
</nextsent>
<nextsent>(d) qiang shou bei np-c dt the nn police vbn killed ? r5 ? r4 ?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2203">
<title id=" W06-3601.xml">a syntax directed translator with extended domain of locality </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>are always inverted between english and chinese in passivevoice.
</prevsent>
<prevsent>finally, we apply rules r4 and r5 which perform phrasal translations for the two remaining subtrees in (d), respectively, and get the completed chinese string in (e).
</prevsent>
</prevsection>
<citsent citstr=" N03-1017 ">
it is helpful to compare this approach with recent efforts in statistical mt. phrase-based models (koehnet al, 2003; <papid> N03-1017 </papid>och and ney, 2004) <papid> J04-4002 </papid>are good at learning local translations that are pairs of (consecutive)sub-strings, but often insufficient in modeling there orderings of phrases themselves, especially between language pairs with very different word-order.</citsent>
<aftsection>
<nextsent>this is because the generative capacity of these models lies within the realm of finite-state machinery (ku mar and byrne, 2003), <papid> N03-1019 </papid>which is unable to process nested structures and long-distance dependencies in natural languages.syntax-based models aim to alleviate this problem by exploiting the power of synchronous rewriting systems.</nextsent>
<nextsent>both yamada and knight (2001) <papid> P01-1067 </papid>and chiang (2005) <papid> P05-1033 </papid>use scfgs as the underlying model, so their translation schemata are syntax-directed as in fig.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2204">
<title id=" W06-3601.xml">a syntax directed translator with extended domain of locality </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>are always inverted between english and chinese in passivevoice.
</prevsent>
<prevsent>finally, we apply rules r4 and r5 which perform phrasal translations for the two remaining subtrees in (d), respectively, and get the completed chinese string in (e).
</prevsent>
</prevsection>
<citsent citstr=" J04-4002 ">
it is helpful to compare this approach with recent efforts in statistical mt. phrase-based models (koehnet al, 2003; <papid> N03-1017 </papid>och and ney, 2004) <papid> J04-4002 </papid>are good at learning local translations that are pairs of (consecutive)sub-strings, but often insufficient in modeling there orderings of phrases themselves, especially between language pairs with very different word-order.</citsent>
<aftsection>
<nextsent>this is because the generative capacity of these models lies within the realm of finite-state machinery (ku mar and byrne, 2003), <papid> N03-1019 </papid>which is unable to process nested structures and long-distance dependencies in natural languages.syntax-based models aim to alleviate this problem by exploiting the power of synchronous rewriting systems.</nextsent>
<nextsent>both yamada and knight (2001) <papid> P01-1067 </papid>and chiang (2005) <papid> P05-1033 </papid>use scfgs as the underlying model, so their translation schemata are syntax-directed as in fig.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2205">
<title id=" W06-3601.xml">a syntax directed translator with extended domain of locality </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>finally, we apply rules r4 and r5 which perform phrasal translations for the two remaining subtrees in (d), respectively, and get the completed chinese string in (e).
</prevsent>
<prevsent>it is helpful to compare this approach with recent efforts in statistical mt. phrase-based models (koehnet al, 2003; <papid> N03-1017 </papid>och and ney, 2004) <papid> J04-4002 </papid>are good at learning local translations that are pairs of (consecutive)sub-strings, but often insufficient in modeling there orderings of phrases themselves, especially between language pairs with very different word-order.</prevsent>
</prevsection>
<citsent citstr=" N03-1019 ">
this is because the generative capacity of these models lies within the realm of finite-state machinery (ku mar and byrne, 2003), <papid> N03-1019 </papid>which is unable to process nested structures and long-distance dependencies in natural languages.syntax-based models aim to alleviate this problem by exploiting the power of synchronous rewriting systems.</citsent>
<aftsection>
<nextsent>both yamada and knight (2001) <papid> P01-1067 </papid>and chiang (2005) <papid> P05-1033 </papid>use scfgs as the underlying model, so their translation schemata are syntax-directed as in fig.</nextsent>
<nextsent>1, but their translators are not: both systems do parsing and transformation in joint search, essentially over packed forest of parse-trees.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2206">
<title id=" W06-3601.xml">a syntax directed translator with extended domain of locality </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>it is helpful to compare this approach with recent efforts in statistical mt. phrase-based models (koehnet al, 2003; <papid> N03-1017 </papid>och and ney, 2004) <papid> J04-4002 </papid>are good at learning local translations that are pairs of (consecutive)sub-strings, but often insufficient in modeling there orderings of phrases themselves, especially between language pairs with very different word-order.</prevsent>
<prevsent>this is because the generative capacity of these models lies within the realm of finite-state machinery (ku mar and byrne, 2003), <papid> N03-1019 </papid>which is unable to process nested structures and long-distance dependencies in natural languages.syntax-based models aim to alleviate this problem by exploiting the power of synchronous rewriting systems.</prevsent>
</prevsection>
<citsent citstr=" P01-1067 ">
both yamada and knight (2001) <papid> P01-1067 </papid>and chiang (2005) <papid> P05-1033 </papid>use scfgs as the underlying model, so their translation schemata are syntax-directed as in fig.</citsent>
<aftsection>
<nextsent>1, but their translators are not: both systems do parsing and transformation in joint search, essentially over packed forest of parse-trees.
</nextsent>
<nextsent>to thisend, their translators are not directed by syntactic tree.
</nextsent>
<nextsent>although their method potentially considers more than one single parse-tree as in our case, the packed representation of the forest restricts the scope of each transfer step to one-level context free rule, while our approach decouples the source language analyzer and the recursive converter, sothat the latter can have an extended domain of locality.
</nextsent>
<nextsent>in addition, our translator also enjoys speed up by this decoupling, with each of the two stages having smaller search space.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2207">
<title id=" W06-3601.xml">a syntax directed translator with extended domain of locality </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>it is helpful to compare this approach with recent efforts in statistical mt. phrase-based models (koehnet al, 2003; <papid> N03-1017 </papid>och and ney, 2004) <papid> J04-4002 </papid>are good at learning local translations that are pairs of (consecutive)sub-strings, but often insufficient in modeling there orderings of phrases themselves, especially between language pairs with very different word-order.</prevsent>
<prevsent>this is because the generative capacity of these models lies within the realm of finite-state machinery (ku mar and byrne, 2003), <papid> N03-1019 </papid>which is unable to process nested structures and long-distance dependencies in natural languages.syntax-based models aim to alleviate this problem by exploiting the power of synchronous rewriting systems.</prevsent>
</prevsection>
<citsent citstr=" P05-1033 ">
both yamada and knight (2001) <papid> P01-1067 </papid>and chiang (2005) <papid> P05-1033 </papid>use scfgs as the underlying model, so their translation schemata are syntax-directed as in fig.</citsent>
<aftsection>
<nextsent>1, but their translators are not: both systems do parsing and transformation in joint search, essentially over packed forest of parse-trees.
</nextsent>
<nextsent>to thisend, their translators are not directed by syntactic tree.
</nextsent>
<nextsent>although their method potentially considers more than one single parse-tree as in our case, the packed representation of the forest restricts the scope of each transfer step to one-level context free rule, while our approach decouples the source language analyzer and the recursive converter, sothat the latter can have an extended domain of locality.
</nextsent>
<nextsent>in addition, our translator also enjoys speed up by this decoupling, with each of the two stages having smaller search space.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2211">
<title id=" W06-3601.xml">a syntax directed translator with extended domain of locality </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>although their method potentially considers more than one single parse-tree as in our case, the packed representation of the forest restricts the scope of each transfer step to one-level context free rule, while our approach decouples the source language analyzer and the recursive converter, sothat the latter can have an extended domain of locality.
</prevsent>
<prevsent>in addition, our translator also enjoys speed up by this decoupling, with each of the two stages having smaller search space.
</prevsent>
</prevsection>
<citsent citstr=" A00-2018 ">
in fact, the recursive transfer step can be done by a linear-time algorithm (see section 5), and the parsing step is also fast with the modern treebank parsers, for instance (collins, 1999; charniak, 2000).<papid> A00-2018 </papid></citsent>
<aftsection>
<nextsent>in contrast, theirdecodings are reported to be computationally expensive and chiang (2005) <papid> P05-1033 </papid>uses aggressive pruning to make it tractable.</nextsent>
<nextsent>there also exists compromise between these two approaches, which uses k-bestlist of parse trees (for relatively small k) to approximate the full forest (see future work).besides, our model, as being linguistically motivated, is also more expressive than the formally syntax-based models of chiang (2005) <papid> P05-1033 </papid>and wu (1997).<papid> J97-3002 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2220">
<title id=" W06-3601.xml">a syntax directed translator with extended domain of locality </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>in fact, the recursive transfer step can be done by a linear-time algorithm (see section 5), and the parsing step is also fast with the modern treebank parsers, for instance (collins, 1999; charniak, 2000).<papid> A00-2018 </papid></prevsent>
<prevsent>in contrast, theirdecodings are reported to be computationally expensive and chiang (2005) <papid> P05-1033 </papid>uses aggressive pruning to make it tractable.</prevsent>
</prevsection>
<citsent citstr=" J97-3002 ">
there also exists compromise between these two approaches, which uses k-bestlist of parse trees (for relatively small k) to approximate the full forest (see future work).besides, our model, as being linguistically motivated, is also more expressive than the formally syntax-based models of chiang (2005) <papid> P05-1033 </papid>and wu (1997).<papid> J97-3002 </papid></citsent>
<aftsection>
<nextsent>consider, again, the passive example in rule r3.
</nextsent>
<nextsent>in chiangs scfg, there is only one nonterminal x, so corresponding rule would be ? was x(1) by x(2), bei x(2) x(1) ? which can also pattern-match the english sentence: was [asleep]1 by [sunset]2 . and translate it into chinese as passive voice.
</nextsent>
<nextsent>this produces very odd chinese translation, because herewas by b?
</nextsent>
<nextsent>in the english sentence is not passive construction.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2222">
<title id=" W06-3601.xml">a syntax directed translator with extended domain of locality </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>by contrast, our model applies rule r3 only if is past participle (vbn) and is noun phrase (np-c).
</prevsent>
<prevsent>this example also shows that, one-level scfg rule, even if informed by the treebank as in (yamada and knight, 2001), <papid> P01-1067 </papid>is not enough to capture common construction like this which is five levels deep (from vp to by?).</prevsent>
</prevsection>
<citsent citstr=" C04-1090 ">
there are also some variations of syntax-directed translators where dependency structures are used in place of constituent trees (lin, 2004; <papid> C04-1090 </papid>ding and palmer, 2005; <papid> P05-1067 </papid>quirk et al, 2005).<papid> P05-1034 </papid></citsent>
<aftsection>
<nextsent>although they share with this work the basic motivations and similar speed-up, it is difficult to specify re-ordering information within dependency elementary structures,so they either resort to heuristics (lin) or separate ordering model for linearization (the other two 3works).2 our approach, in contrast, explicitly models the re-ordering of sub-trees within individual transfer rules.
</nextsent>
<nextsent>in this section, we define the formal machinery of our recursive transformation model as special case of xrs transducers (graehl and knight, 2004) <papid> N04-1014 </papid>that has only one state, and each rule is linear (l) and non-deleting (n) with regarding to variables in the source and target sides (henth the name 1-xrlns).</nextsent>
<nextsent>definition 1.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2224">
<title id=" W06-3601.xml">a syntax directed translator with extended domain of locality </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>by contrast, our model applies rule r3 only if is past participle (vbn) and is noun phrase (np-c).
</prevsent>
<prevsent>this example also shows that, one-level scfg rule, even if informed by the treebank as in (yamada and knight, 2001), <papid> P01-1067 </papid>is not enough to capture common construction like this which is five levels deep (from vp to by?).</prevsent>
</prevsection>
<citsent citstr=" P05-1067 ">
there are also some variations of syntax-directed translators where dependency structures are used in place of constituent trees (lin, 2004; <papid> C04-1090 </papid>ding and palmer, 2005; <papid> P05-1067 </papid>quirk et al, 2005).<papid> P05-1034 </papid></citsent>
<aftsection>
<nextsent>although they share with this work the basic motivations and similar speed-up, it is difficult to specify re-ordering information within dependency elementary structures,so they either resort to heuristics (lin) or separate ordering model for linearization (the other two 3works).2 our approach, in contrast, explicitly models the re-ordering of sub-trees within individual transfer rules.
</nextsent>
<nextsent>in this section, we define the formal machinery of our recursive transformation model as special case of xrs transducers (graehl and knight, 2004) <papid> N04-1014 </papid>that has only one state, and each rule is linear (l) and non-deleting (n) with regarding to variables in the source and target sides (henth the name 1-xrlns).</nextsent>
<nextsent>definition 1.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2225">
<title id=" W06-3601.xml">a syntax directed translator with extended domain of locality </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>by contrast, our model applies rule r3 only if is past participle (vbn) and is noun phrase (np-c).
</prevsent>
<prevsent>this example also shows that, one-level scfg rule, even if informed by the treebank as in (yamada and knight, 2001), <papid> P01-1067 </papid>is not enough to capture common construction like this which is five levels deep (from vp to by?).</prevsent>
</prevsection>
<citsent citstr=" P05-1034 ">
there are also some variations of syntax-directed translators where dependency structures are used in place of constituent trees (lin, 2004; <papid> C04-1090 </papid>ding and palmer, 2005; <papid> P05-1067 </papid>quirk et al, 2005).<papid> P05-1034 </papid></citsent>
<aftsection>
<nextsent>although they share with this work the basic motivations and similar speed-up, it is difficult to specify re-ordering information within dependency elementary structures,so they either resort to heuristics (lin) or separate ordering model for linearization (the other two 3works).2 our approach, in contrast, explicitly models the re-ordering of sub-trees within individual transfer rules.
</nextsent>
<nextsent>in this section, we define the formal machinery of our recursive transformation model as special case of xrs transducers (graehl and knight, 2004) <papid> N04-1014 </papid>that has only one state, and each rule is linear (l) and non-deleting (n) with regarding to variables in the source and target sides (henth the name 1-xrlns).</nextsent>
<nextsent>definition 1.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2229">
<title id=" W06-3601.xml">a syntax directed translator with extended domain of locality </title>
<section> extended tree-to-string tranducers.  </section>
<citcontext>
<prevsection>
<prevsent>if rule has no variable, i.e., it is of rank zero, then it is called apurely lexical rule, which performs phrasal translation as in phrase-based models.
</prevsent>
<prevsent>rule r2, for instance, can be thought of as phrase pair the gun man, qiangshou?.
</prevsent>
</prevsection>
<citsent citstr=" J00-1004 ">
informally speaking, derivation in transducer is sequence of steps converting source-language2although hybrid approaches, such as dependency grammars augmented with phrase-structure information (alshawi et al., 2000), <papid> J00-1004 </papid>can do re-ordering easily.</citsent>
<aftsection>
<nextsent>r1 r2 r3 r4 r5 r1 r2 r6 r4 r7 r5 (a) (b) figure 4: (a) the derivation in figure 3; (b) another derviation producing the same output by replacing r3 with r6 and r7, which provides another way of translating the passive construction: (r6) vp ( vbd (was) vp-c (x1:vbn x2:pp ) )?
</nextsent>
<nextsent>x2 x1 (r7) pp ( in (by) x1:np-c )?
</nextsent>
<nextsent>bei x1tree into target-language string, with each step applying one tranduction rule.
</nextsent>
<nextsent>however, it can also be formalized as tree, following the notion of derivation-tree in tag (joshi and schabes, 1997): definition 2.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2233">
<title id=" W06-3601.xml">a syntax directed translator with extended domain of locality </title>
<section> probability models.  </section>
<citcontext>
<prevsection>
<prevsent>(8) where c(r) is the count (or frequency) of rule in the training data.
</prevsent>
<prevsent>4.2 log-linear model.
</prevsent>
</prevsection>
<citsent citstr=" P02-1038 ">
following och and ney (2002), <papid> P02-1038 </papid>we extend the direct model into general log-linear framework in order to incorporate other features: c?</citsent>
<aftsection>
<nextsent>= argmax pr(c | e)?
</nextsent>
<nextsent>pr(c)?
</nextsent>
<nextsent>e??|c| (9) where pr(c) is the language model and e??|c| is the length penalty term based on |c|, the length of the translation.
</nextsent>
<nextsent>parameters ?, ?, and ? are the weights of relevant features.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2238">
<title id=" W06-3601.xml">a syntax directed translator with extended domain of locality </title>
<section> search algorithms.  </section>
<citcontext>
<prevsection>
<prevsent>as standard alternative, rather than aiming at the exact best derivation, we search for top-k derivations under the direct model using algorithm 1, and then rerank the k-best list with the language model and length penalty.
</prevsent>
<prevsent>like other instances of dynamic programming, algorithm 1 can be viewed as hypergraph searchproblem.
</prevsent>
</prevsection>
<citsent citstr=" W05-1506 ">
to this end, we use an efficient algorithm by huang and chiang (2005, <papid> W05-1506 </papid>algorithm 3) that solves the general k-best derivations problem in monotonic hypergraphs.</citsent>
<aftsection>
<nextsent>it consists of normal forward phase for the 1-best derivation and recursive backward phase for the 2nd, 3rd, . . .
</nextsent>
<nextsent>, kth derivations.
</nextsent>
<nextsent>unfortunately, different derivations may have the same yield (a problem called spurious ambiguity), due to multi-level lhs of our rules.
</nextsent>
<nextsent>in practice, this results in very small ratio of unique strings amongtop-k derivations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2240">
<title id=" W06-3601.xml">a syntax directed translator with extended domain of locality </title>
<section> search algorithms.  </section>
<citcontext>
<prevsection>
<prevsent>unfortunately, different derivations may have the same yield (a problem called spurious ambiguity), due to multi-level lhs of our rules.
</prevsent>
<prevsent>in practice, this results in very small ratio of unique strings amongtop-k derivations.
</prevsent>
</prevsection>
<citsent citstr=" N06-1045 ">
to alleviate this problem, deter min ization techniques have been proposed by mohriand riley (2002) for finite-state automata and extended to tree automata by may and knight (2006).<papid> N06-1045 </papid>these methods eliminate spurious ambiguity by effectively transforming the grammar into an equivalent deterministic form.</citsent>
<aftsection>
<nextsent>however, this transformation often leads to blow-up in forest size, which is exponential to the original size in the worst-case.
</nextsent>
<nextsent>so instead of determinization, here we present simple-yet-effective extension to the algorithm 3 of huang and chiang (2005) <papid> W05-1506 </papid>that guarantees to output unique translated strings: ? keep hash-table of unique strings at each vertex in the hypergraph?</nextsent>
<nextsent>when asking for the next-best derivation of vertex, keep asking until we get new string, and then add it into the hash-table this method should work in general for any equivalence relation (say, same derived tree) that can be defined on derivations.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2247">
<title id=" W06-3601.xml">a syntax directed translator with extended domain of locality </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>since the target language is chinese, we report character-based bleu score instead of word-based to ensure our results are independent of chinese tokenizations (although our language models are word-based).
</prevsent>
<prevsent>the bleu scores are based on single reference and up to 4-gram pre cisions (r1n4).
</prevsent>
</prevsection>
<citsent citstr=" P03-1021 ">
feature weights of both systems are tuned on the same data set.3 for pharaoh, we use the standard minimum error-rate training (och, 2003);<papid> P03-1021 </papid>and for our system, since there are only two independent features (as we always fix ? = 1), we use simple grid-based line-optimization along thelanguage-model weight axis.</citsent>
<aftsection>
<nextsent>forgiven language model weight ?, we use binary search to find the best length penalty ? that leads to length-ratio closest 3in this sense, we are only reporting performances on the development set at this point.
</nextsent>
<nextsent>we will report results tuned and tested on separate datasets in the final version of this paper.
</nextsent>
<nextsent>table 1: bleu (r1n4) score results system bleu pharaoh 25.5 direct model (1-best) 20.3 log-linear model (rescored 5000-best) 23.8to 1 against the reference.
</nextsent>
<nextsent>the results are summarized in table 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2248">
<title id=" W06-3209.xml">learning probabilistic paradigms for morphology in a latent class model </title>
<section> abstract </section>
<citcontext>
<prevsection>
<prevsent>we describe an algorithm that recursively applies latent dirichlet allocation with an orthogonality constraint to discover morphological paradigms as the latent classes within suffix-stem matrix.
</prevsent>
<prevsent>we apply the algorithm to data preprocessed in several different ways, and show that when suffixes are distinguished for part of speech and allomorphs or gender/conjugational variants are merged, the model is able to correctly learn morphological paradigms for english and spanish.
</prevsent>
</prevsection>
<citsent citstr=" J01-2001 ">
we compare our system with linguist ica (goldsmith 2001), <papid> J01-2001 </papid>and discuss the advantages of the probabilistic paradigm over linguist icas signature representation.</citsent>
<aftsection>
<nextsent>in recent years researchers have addressed the task of unsupervised learning of declarative representations of morphological structure.
</nextsent>
<nextsent>these models include the signature of (goldsmith 2001), <papid> J01-2001 </papid>the conflation set of (schone and jurafsky 2001), <papid> N01-1024 </papid>the paradigm of (brent et. al. 2002), and the inflectional class of (monson 2004).</nextsent>
<nextsent>while these representations group morphologically related words in systematic ways, they are rather different from the paradigm, the representation of morphology in traditional grammars.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2254">
<title id=" W06-3209.xml">learning probabilistic paradigms for morphology in a latent class model </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we compare our system with linguist ica (goldsmith 2001), <papid> J01-2001 </papid>and discuss the advantages of the probabilistic paradigm over linguist icas signature representation.</prevsent>
<prevsent>in recent years researchers have addressed the task of unsupervised learning of declarative representations of morphological structure.</prevsent>
</prevsection>
<citsent citstr=" N01-1024 ">
these models include the signature of (goldsmith 2001), <papid> J01-2001 </papid>the conflation set of (schone and jurafsky 2001), <papid> N01-1024 </papid>the paradigm of (brent et. al. 2002), and the inflectional class of (monson 2004).</citsent>
<aftsection>
<nextsent>while these representations group morphologically related words in systematic ways, they are rather different from the paradigm, the representation of morphology in traditional grammars.
</nextsent>
<nextsent>a paradigm lists the prototypical morphological properties of lexemes belonging to particular part of speech (pos) category; for example, paradigm for regular english verbs would include the suffixes {$,ed$,ing$,s$}1.
</nextsent>
<nextsent>hand-built computational implementations of paradigms as inheritance hierarchies include datr (evans and gazdar 1996) <papid> J96-2002 </papid>and functional morphology (forsberg and ranta 2004).</nextsent>
<nextsent>the two principal ways in which learned models have differed from paradigms are that: 1) they do not have pos types, and 2) they are not abstractions that generalize beyond the words of the input corpus.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2255">
<title id=" W06-3209.xml">learning probabilistic paradigms for morphology in a latent class model </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>while these representations group morphologically related words in systematic ways, they are rather different from the paradigm, the representation of morphology in traditional grammars.
</prevsent>
<prevsent>a paradigm lists the prototypical morphological properties of lexemes belonging to particular part of speech (pos) category; for example, paradigm for regular english verbs would include the suffixes {$,ed$,ing$,s$}1.
</prevsent>
</prevsection>
<citsent citstr=" J96-2002 ">
hand-built computational implementations of paradigms as inheritance hierarchies include datr (evans and gazdar 1996) <papid> J96-2002 </papid>and functional morphology (forsberg and ranta 2004).</citsent>
<aftsection>
<nextsent>the two principal ways in which learned models have differed from paradigms are that: 1) they do not have pos types, and 2) they are not abstractions that generalize beyond the words of the input corpus.
</nextsent>
<nextsent>there are important reasons for learning pos-associated, paradigmatic representation of morphology.
</nextsent>
<nextsent>currently, the dominant technology for morphological analysis involves mapping between inflected and base of forms of words with finite-state transducers (fsts), procedural model of morphological relations.
</nextsent>
<nextsent>rewrite rules are handcrafted and compiled into fsts, and it would be beneficial if these rules could be learned automatically.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2256">
<title id=" W06-3209.xml">learning probabilistic paradigms for morphology in a latent class model </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>currently, the dominant technology for morphological analysis involves mapping between inflected and base of forms of words with finite-state transducers (fsts), procedural model of morphological relations.
</prevsent>
<prevsent>rewrite rules are handcrafted and compiled into fsts, and it would be beneficial if these rules could be learned automatically.
</prevsent>
</prevsection>
<citsent citstr=" J01-1003 ">
one line of research in computational morphology has been directed towards learning finite state mapping rules from some sort of paradig matic structure, where all morphological forms and pos types are presumed known for set of lex emes (clark 2001, kazakov and manandhar 2001, oflazer et. al. 2001, <papid> J01-1003 </papid>zajac 2001, <papid> W01-0711 </papid>albright 2002).</citsent>
<aftsection>
<nextsent>this can be accomplished by first deciding on base form, then learning rules to convert other forms of the paradigm into this base form.
</nextsent>
<nextsent>if one could develop an unsupervised algorithm for learning paradigms, it could serve as the input to rule learning procedures, effectively leading to an entirely unsupervised system for learning fsts from raw data.
</nextsent>
<nextsent>this is our long-term goal.
</nextsent>
<nextsent>1 $ is the null suffix.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2257">
<title id=" W06-3209.xml">learning probabilistic paradigms for morphology in a latent class model </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>currently, the dominant technology for morphological analysis involves mapping between inflected and base of forms of words with finite-state transducers (fsts), procedural model of morphological relations.
</prevsent>
<prevsent>rewrite rules are handcrafted and compiled into fsts, and it would be beneficial if these rules could be learned automatically.
</prevsent>
</prevsection>
<citsent citstr=" W01-0711 ">
one line of research in computational morphology has been directed towards learning finite state mapping rules from some sort of paradig matic structure, where all morphological forms and pos types are presumed known for set of lex emes (clark 2001, kazakov and manandhar 2001, oflazer et. al. 2001, <papid> J01-1003 </papid>zajac 2001, <papid> W01-0711 </papid>albright 2002).</citsent>
<aftsection>
<nextsent>this can be accomplished by first deciding on base form, then learning rules to convert other forms of the paradigm into this base form.
</nextsent>
<nextsent>if one could develop an unsupervised algorithm for learning paradigms, it could serve as the input to rule learning procedures, effectively leading to an entirely unsupervised system for learning fsts from raw data.
</nextsent>
<nextsent>this is our long-term goal.
</nextsent>
<nextsent>1 $ is the null suffix.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2258">
<title id=" W06-3209.xml">learning probabilistic paradigms for morphology in a latent class model </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>1 $ is the null suffix.
</prevsent>
<prevsent>69 an alternative approach is to skip the paradigm formulation step and construct procedural model directly from raw data.
</prevsent>
</prevsection>
<citsent citstr=" P00-1027 ">
(yarowsky and wicentowski 2000) <papid> P00-1027 </papid>bootstrap inflected and base forms directly from raw data and learn mappings between them.</citsent>
<aftsection>
<nextsent>their results are quite successful, but the morphological information they learn is not structured as clearly as paradigmatic model.
</nextsent>
<nextsent>(freitag 2005) <papid> W05-0617 </papid>constructs morphological automaton, where nodes are clustered word types and arcs are suffix ation rules.</nextsent>
<nextsent>this paper addresses the problem of finding an organization of stems and suffixes as probabilistic paradigms (section 2), model of morphology closer to linguistic notion of paradigm than previously proposed models.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2259">
<title id=" W06-3209.xml">learning probabilistic paradigms for morphology in a latent class model </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(yarowsky and wicentowski 2000) <papid> P00-1027 </papid>bootstrap inflected and base forms directly from raw data and learn mappings between them.</prevsent>
<prevsent>their results are quite successful, but the morphological information they learn is not structured as clearly as paradigmatic model.</prevsent>
</prevsection>
<citsent citstr=" W05-0617 ">
(freitag 2005) <papid> W05-0617 </papid>constructs morphological automaton, where nodes are clustered word types and arcs are suffix ation rules.</citsent>
<aftsection>
<nextsent>this paper addresses the problem of finding an organization of stems and suffixes as probabilistic paradigms (section 2), model of morphology closer to linguistic notion of paradigm than previously proposed models.
</nextsent>
<nextsent>we encode the morphological structure of language in matrix containing frequencies of words, and formulate the problem of learning paradigms as one of finding latent classes within the matrix.
</nextsent>
<nextsent>we present recursive lda, learning algorithm based on latent dirichlet allocation (section 3), and show that under certain conditions (section 5), it can correctly learn morphological paradigms for english and spanish.
</nextsent>
<nextsent>in section 6, we compare the probabilistic paradigm to the signature model of (goldsmith 2001).<papid> J01-2001 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2261">
<title id=" W06-3209.xml">learning probabilistic paradigms for morphology in a latent class model </title>
<section> the probabilistic paradigm.  </section>
<citcontext>
<prevsection>
<prevsent>4 data.
</prevsent>
<prevsent>we conducted experiments on english and spanish.
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
for english, we chose the penn treebank (marcus et. al. 1993), <papid> J93-2004 </papid>which is already pos tagged; for spanish, we chose an equivalent-sized portion of newswire (graff and galegos 1999), pos-tagged by the free ling morphological analyzer (carreras et. al. 2004).</citsent>
<aftsection>
<nextsent>we restricted our data to nouns, verbs, adjectives, and adverbs.
</nextsent>
<nextsent>words that did not follow canonical suffix ation patterns for their pos category (irregulars, foreign words, incorrectly tagged words, etc.) were excluded.
</nextsent>
<nextsent>we segmented each word into stem and suffix for specified set of suffixes.
</nextsent>
<nextsent>rare suffixes were excluded, such as many english adjective-forming suffixes and spanish 2nd person plural forms.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2265">
<title id=" W06-3209.xml">learning probabilistic paradigms for morphology in a latent class model </title>
<section> the probabilistic paradigm.  </section>
<citcontext>
<prevsection>
<prevsent>another way to view the signature is as special case of the probabilistic paradigm where all probabilities are restricted to being 0 or 1, for if this were so, the only way to fit the data would be to let there be canonical paradigm for every different subset of suffixes that some stem appears with.
</prevsent>
<prevsent>in theory, it is possible for the number of signatures to be exponential in the number of suffixes; in practice, linguist ica finds hundreds of signatures for english and spanish.
</prevsent>
</prevsection>
<citsent citstr=" W04-0105 ">
although there has been work on reducing the number of signatures (goldwater and johnson 2004; <papid> W04-0105 </papid>hu et. al. 2005, <papid> W05-0503 </papid>who report reduction of up to 30%), the number of remaining signatures is still two orders of magnitude greater than the number of canonical paradigms we find.</citsent>
<aftsection>
<nextsent>the simplest explanation for this is that suffix can be listed many times in the different signatures, but only has one entry in the matrix of the probabilistic paradigm.
</nextsent>
<nextsent>it is important for natural language system to handle out-of-vocabulary words.
</nextsent>
<nextsent>a signature does not predict the forms of potential but unseen forms of stems.
</nextsent>
<nextsent>to some extent linguist ica could accommodate this, as it identifies when one signature suffixes are proper subset of another s, but it does not handle cases where suffixes are partially overlapping.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2266">
<title id=" W06-3209.xml">learning probabilistic paradigms for morphology in a latent class model </title>
<section> the probabilistic paradigm.  </section>
<citcontext>
<prevsection>
<prevsent>another way to view the signature is as special case of the probabilistic paradigm where all probabilities are restricted to being 0 or 1, for if this were so, the only way to fit the data would be to let there be canonical paradigm for every different subset of suffixes that some stem appears with.
</prevsent>
<prevsent>in theory, it is possible for the number of signatures to be exponential in the number of suffixes; in practice, linguist ica finds hundreds of signatures for english and spanish.
</prevsent>
</prevsection>
<citsent citstr=" W05-0503 ">
although there has been work on reducing the number of signatures (goldwater and johnson 2004; <papid> W04-0105 </papid>hu et. al. 2005, <papid> W05-0503 </papid>who report reduction of up to 30%), the number of remaining signatures is still two orders of magnitude greater than the number of canonical paradigms we find.</citsent>
<aftsection>
<nextsent>the simplest explanation for this is that suffix can be listed many times in the different signatures, but only has one entry in the matrix of the probabilistic paradigm.
</nextsent>
<nextsent>it is important for natural language system to handle out-of-vocabulary words.
</nextsent>
<nextsent>a signature does not predict the forms of potential but unseen forms of stems.
</nextsent>
<nextsent>to some extent linguist ica could accommodate this, as it identifies when one signature suffixes are proper subset of another s, but it does not handle cases where suffixes are partially overlapping.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2268">
<title id=" W06-3510.xml">a very brief introduction to fluid construction grammar </title>
<section> syntactic and semantic structures.  </section>
<citcontext>
<prevsection>
<prevsent>right: related syntactic structure.
</prevsent>
<prevsent>in reality both structures contain lot more information.
</prevsent>
</prevsection>
<citsent citstr=" P84-1018 ">
as mentioned, fcg organises the information about an utterance in feature structures, similar to otherfeature-structure based formalisms (as first introduced by kay (kay, 1984)) <papid> P84-1018 </papid>but with some important differences.</citsent>
<aftsection>
<nextsent>an fcg feature structure contains units which correspond (roughly) to words (more precisely morphemes) and constituents.
</nextsent>
<nextsent>a unit has name and set of features.
</nextsent>
<nextsent>hierarchical structure is not implicitly represented by embedding one unit in another one, but explicitly by the features syn-subunits (for the syntactic structure) and sem-subunits (for the semantic structure).
</nextsent>
<nextsent>there is astrong correspondence between the syntactic and semantic structure built up for the same utterance (seefigure 2) although there can be units which only appear in the syntactic structure (for example for grammatical function words) and vice versa.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2269">
<title id=" W06-3510.xml">a very brief introduction to fluid construction grammar </title>
<section> fluidity, conventional isation and.  </section>
<citcontext>
<prevsection>
<prevsent>again application may be constrained when semantic constraints in the construction prevent it.
</prevsent>
<prevsent>meta-grammars although fcg must become adequate for dealing with the typical phenomena that we find in human natural languages, our main target is to make scientific models of the processes that underly the origins of language, in other words of the creative process by which language users adaptor invent new forms to express new meanings that unavoidably arise in an open world and negotiate tacitly the conventions that they adopt as group.
</prevsent>
</prevsection>
<citsent citstr=" P04-1002 ">
we have already carried out number of experiments in this direction and here only brief summary can be given (for more discussion see: (steels, 2004; <papid> P04-1002 </papid>debeulebergen, 2006; steelsloetzsch, 2006)).in our experiments, speaker and hearer are chosen randomly from population to play language game as part of situated embodied interaction that involves perception, joint attention and feedback.</citsent>
<aftsection>
<nextsent>when the speaker conceptualizes the scene, he may construct new semantic objects (for example new categories) or recruit new constraint networks in order to achieve the communicative goal imposed by the game.
</nextsent>
<nextsent>also when the speaker is trying to verbal ise the constraint network that constitutes the meaning of an utterance, there may be lexical items missing or new constructions may have to be built.
</nextsent>
<nextsent>we use meta-level architecture with reflection to organise this process.
</nextsent>
<nextsent>the speaker goes through the normal processing steps, using whatever inventory is available.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2270">
<title id=" W06-1610.xml">reevaluating machine translation results with paraphrase support </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>paraeval addresses three important issues: support for paraphrase/synonym matching, recall measurement, and correlation with human judgments.
</prevsent>
<prevsent>we show that paraeval correlates significantly better than bleu with human assessment in measurements for both fluency and adequacy.
</prevsent>
</prevsection>
<citsent citstr=" N03-1020 ">
the introduction of automated evaluation procedures, such as bleu (papineni et al, 2001) for machine translation (mt) and rouge (lin and hovy, 2003) <papid> N03-1020 </papid>for summarization, have prompted much progress and development in both of these areas of research in natural language processing (nlp).</citsent>
<aftsection>
<nextsent>both evaluation tasks employ comparison strategy for comparing textual units from machine-generated and gold-standard texts.
</nextsent>
<nextsent>ideally, this comparison process would be performed manually, because of humans?
</nextsent>
<nextsent>abilities to infer, paraphrase, and use world knowledge to relate differently worded pieces of equivalent information.
</nextsent>
<nextsent>however, manual evaluations are time consuming and expensive, thus making them bottleneck in system development cycles.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2271">
<title id=" W06-1610.xml">reevaluating machine translation results with paraphrase support </title>
<section> paraeval for mt evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>grey areas are matched by using bleu.
</prevsent>
<prevsent>79foreign language.
</prevsent>
</prevsection>
<citsent citstr=" J04-4002 ">
phrase-based statistical machine translation (smt) systems analyze large quantities of bilingual parallel texts in order to learn translational alignments between pairs of words and phrases in two languages (och and ney, 2004).<papid> J04-4002 </papid></citsent>
<aftsection>
<nextsent>the sentence-based translation model makes word/phrase alignment decisions probabilistically by computing the optimal model parameters with application of the statistical estimation theory.
</nextsent>
<nextsent>this alignment process results in corpus of word/phrase-aligned parallel sentences from which we can extract phrase pairs that are translations of each other.
</nextsent>
<nextsent>we ran the alignment algorithm from (och and ney, 2003) <papid> J03-1002 </papid>on chinese-english parallel corpus of 218 million english words, available from the linguistic data consortium (ldc).</nextsent>
<nextsent>phrase pairs are extracted by following the method described in (och and ney, 2004) <papid> J04-4002 </papid>where all contiguous phrase pairs having consistent alignments are extraction candidates.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2272">
<title id=" W06-1610.xml">reevaluating machine translation results with paraphrase support </title>
<section> paraeval for mt evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>the sentence-based translation model makes word/phrase alignment decisions probabilistically by computing the optimal model parameters with application of the statistical estimation theory.
</prevsent>
<prevsent>this alignment process results in corpus of word/phrase-aligned parallel sentences from which we can extract phrase pairs that are translations of each other.
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
we ran the alignment algorithm from (och and ney, 2003) <papid> J03-1002 </papid>on chinese-english parallel corpus of 218 million english words, available from the linguistic data consortium (ldc).</citsent>
<aftsection>
<nextsent>phrase pairs are extracted by following the method described in (och and ney, 2004) <papid> J04-4002 </papid>where all contiguous phrase pairs having consistent alignments are extraction candidates.</nextsent>
<nextsent>using these pairs we build paraphrase sets by joining together all english phrases that have the same chinese translation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2274">
<title id=" W06-1610.xml">reevaluating machine translation results with paraphrase support </title>
<section> evaluating paraeval.  </section>
<citcontext>
<prevsection>
<prevsent>the paraeval scores used were precision scores.
</prevsent>
<prevsent>in addition to distinguishing the quality of mt systems, reliable evaluation procedure must be able to distinguish system translations from hu mans?
</prevsent>
</prevsection>
<citsent citstr=" P04-1077 ">
(lin and och, 2004).<papid> P04-1077 </papid></citsent>
<aftsection>
<nextsent>figure 5 shows the overall system and human ranking.
</nextsent>
<nextsent>in the upper left corner, human translators are grouped together, significantly separated from the automatic mt systems clustered into the lower right corner.
</nextsent>
<nextsent>4 .2 implications to word-alignment we experimented with restricting the paraphrases being matched to various lengths.
</nextsent>
<nextsent>when allowing only paraphrases of three or more words to match, the correlation figures become stabilized and paraeval achieves even higher correlation with fluency measurement to 0.7619 on the spearman ranking coefficient.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2275">
<title id=" W06-2927.xml">multilingual dependency parsing at naist </title>
<section> abstract </section>
<citcontext>
<prevsection>

<prevsent>in this paper, we present framework for multi-lingual dependency parsing.
</prevsent>
</prevsection>
<citsent citstr=" W04-0308 ">
our bottom-up deterministic parser adopts nivres algorithm (nivre, 2004) <papid> W04-0308 </papid>with preprocessor.</citsent>
<aftsection>
<nextsent>support vector machines (svms) are utilized to determine the word dependency attachments.
</nextsent>
<nextsent>then, maximum entropy method (maxent) is used for determining the label of the dependency relation.
</nextsent>
<nextsent>to improve the performance of the parser, we construct tagger based on svms to find neighboring attachment as preprocessor.
</nextsent>
<nextsent>experimental evaluation shows that the proposed extension improves the parsing accuracy of our base parser in 9 languages.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2278">
<title id=" W06-2404.xml">chunking japanese compound functional expressions by machine learning </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this paper, we apply machine learning technique to the task of identifying japanese compound functional expressions in text.
</prevsent>
<prevsent>we formalize this identification task as chunking problem.
</prevsent>
</prevsection>
<citsent citstr=" N01-1025 ">
we employ the technique of support vector machines (svms) (vapnik, 1998) as the machine learning technique, which has been successfully applied to various natural language processing tasks including chunking tasks such as phrase chunking (kudo and matsumoto, 2001) <papid> N01-1025 </papid>and named entity chunking (mayfield et al, 2003).<papid> W03-0429 </papid>in the preliminary experimental evaluation, we focus on 52 expressions that have balanced distribution of their usages in the newspaper text corpus and are among the most difficult ones in terms oftheir identification in text.</citsent>
<aftsection>
<nextsent>we show that the proposed method significantly outperforms existing japanese text processing tools as well as another tool based on hand-crafted rules.
</nextsent>
<nextsent>we further show that, in the proposed svms based framework, it is sufficient to collect and manually annotate about 50 training examples per expression.
</nextsent>
<nextsent>expressions and their example database 2.1 japanese compound functional.
</nextsent>
<nextsent>expressions there exist several collections which list japanese functional expressions and examine their usages.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2279">
<title id=" W06-2404.xml">chunking japanese compound functional expressions by machine learning </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this paper, we apply machine learning technique to the task of identifying japanese compound functional expressions in text.
</prevsent>
<prevsent>we formalize this identification task as chunking problem.
</prevsent>
</prevsection>
<citsent citstr=" W03-0429 ">
we employ the technique of support vector machines (svms) (vapnik, 1998) as the machine learning technique, which has been successfully applied to various natural language processing tasks including chunking tasks such as phrase chunking (kudo and matsumoto, 2001) <papid> N01-1025 </papid>and named entity chunking (mayfield et al, 2003).<papid> W03-0429 </papid>in the preliminary experimental evaluation, we focus on 52 expressions that have balanced distribution of their usages in the newspaper text corpus and are among the most difficult ones in terms oftheir identification in text.</citsent>
<aftsection>
<nextsent>we show that the proposed method significantly outperforms existing japanese text processing tools as well as another tool based on hand-crafted rules.
</nextsent>
<nextsent>we further show that, in the proposed svms based framework, it is sufficient to collect and manually annotate about 50 training examples per expression.
</nextsent>
<nextsent>expressions and their example database 2.1 japanese compound functional.
</nextsent>
<nextsent>expressions there exist several collections which list japanese functional expressions and examine their usages.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2280">
<title id=" W06-2404.xml">chunking japanese compound functional expressions by machine learning </title>
<section> japanese compound functional.  </section>
<citcontext>
<prevsection>
<prevsent>out of those 187 expressions with more than50 sentences, 52 are those with balanced distribution of the functional/content usages in the newspaper text corpus.
</prevsent>
<prevsent>those 52 expressions can be regarded as among the most difficult ones in the task of identifying and classifying functional/content 6for those expressions whose constituent has conjugation and the conjugated form also has the same usage as the expression with the original form, the morpheme sequence is expanded so that the expanded morpheme sequences include those with conjugated forms.
</prevsent>
</prevsection>
<citsent citstr=" J96-2004 ">
7for the most frequent 184 expressions, on the average, the agreement rate between two human annotators is 0.93 andthe kappa value is 0.73, which means allowing tentative conclusions to be drawn (carletta, 1996; <papid> J96-2004 </papid>ng et al, 1999).<papid> W99-0502 </papid></citsent>
<aftsection>
<nextsent>for 65% of the 184 expressions, the kappa value is above 0.8, which means good reliability.usages.
</nextsent>
<nextsent>thus, this paper focuses on those 52 expressions in the training/testing of chunking compound functional expressions.
</nextsent>
<nextsent>we extract 2,600 sentences (= 52 expressions ? 50 sentences) from the whole example database and use them for training/testing the chunker.
</nextsent>
<nextsent>the number of the morphemes for the 2,600 sentences is 92,899.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2281">
<title id=" W06-2404.xml">chunking japanese compound functional expressions by machine learning </title>
<section> japanese compound functional.  </section>
<citcontext>
<prevsection>
<prevsent>out of those 187 expressions with more than50 sentences, 52 are those with balanced distribution of the functional/content usages in the newspaper text corpus.
</prevsent>
<prevsent>those 52 expressions can be regarded as among the most difficult ones in the task of identifying and classifying functional/content 6for those expressions whose constituent has conjugation and the conjugated form also has the same usage as the expression with the original form, the morpheme sequence is expanded so that the expanded morpheme sequences include those with conjugated forms.
</prevsent>
</prevsection>
<citsent citstr=" W99-0502 ">
7for the most frequent 184 expressions, on the average, the agreement rate between two human annotators is 0.93 andthe kappa value is 0.73, which means allowing tentative conclusions to be drawn (carletta, 1996; <papid> J96-2004 </papid>ng et al, 1999).<papid> W99-0502 </papid></citsent>
<aftsection>
<nextsent>for 65% of the 184 expressions, the kappa value is above 0.8, which means good reliability.usages.
</nextsent>
<nextsent>thus, this paper focuses on those 52 expressions in the training/testing of chunking compound functional expressions.
</nextsent>
<nextsent>we extract 2,600 sentences (= 52 expressions ? 50 sentences) from the whole example database and use them for training/testing the chunker.
</nextsent>
<nextsent>the number of the morphemes for the 2,600 sentences is 92,899.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2286">
<title id=" W06-2404.xml">chunking japanese compound functional expressions by machine learning </title>
<section> concluding remarks.  </section>
<citcontext>
<prevsection>
<prevsent>we showed thatthe proposed method significantly outperforms existing japanese text processing tools.
</prevsent>
<prevsent>the pro context feature.
</prevsent>
</prevsection>
<citsent citstr=" W04-0405 ">
posed framework has advantages over an approach based on manually created rules such as the one in (shudo et al, 2004), <papid> W04-0405 </papid>in that it requires human cost to manually create and maintain those rules.</citsent>
<aftsection>
<nextsent>onthe other hand, in our framework based on thema chine learning technique, it is sufficient to collect and manually annotate about 50 training examples per expression.
</nextsent>



</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2287">
<title id=" W07-0409.xml">combining morphosyntactic enriched representation with nbest reranking in statistical translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>using the morphosyntactic language model alone does not results in any improvement in performance.
</prevsent>
<prevsent>however, combining morphosyntactic word disambiguation with word based 4-gram language model results in relative improvement in the bleu score of 2.3% on the development set and 1.9% on the test set.
</prevsent>
</prevsection>
<citsent citstr=" P00-1056 ">
recent works in statistical machine translation (smt) shows how phrase-based modeling (och andney, 2000<papid> P00-1056 </papid>a; koehn et al, 2003) <papid> N03-1017 </papid>significantly out perform the historical word-based modeling (brown et al, 1993).<papid> J93-2003 </papid></citsent>
<aftsection>
<nextsent>using phrases, i.e. sequences ofwords, as translation units allows the system to preserve local word order constraints and to improve the consistency of phrases during the translation process.
</nextsent>
<nextsent>phrase-based models provide some sort of context information as opposed to word-based models.
</nextsent>
<nextsent>training phrase-based model typically requires aligning parallel corpus, extracting phrases and scoring them using word and phrase counts.
</nextsent>
<nextsent>the derived statistics capture the structure of natural language to some extent, including implicit syntactic and semantic relations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2288">
<title id=" W07-0409.xml">combining morphosyntactic enriched representation with nbest reranking in statistical translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>using the morphosyntactic language model alone does not results in any improvement in performance.
</prevsent>
<prevsent>however, combining morphosyntactic word disambiguation with word based 4-gram language model results in relative improvement in the bleu score of 2.3% on the development set and 1.9% on the test set.
</prevsent>
</prevsection>
<citsent citstr=" N03-1017 ">
recent works in statistical machine translation (smt) shows how phrase-based modeling (och andney, 2000<papid> P00-1056 </papid>a; koehn et al, 2003) <papid> N03-1017 </papid>significantly out perform the historical word-based modeling (brown et al, 1993).<papid> J93-2003 </papid></citsent>
<aftsection>
<nextsent>using phrases, i.e. sequences ofwords, as translation units allows the system to preserve local word order constraints and to improve the consistency of phrases during the translation process.
</nextsent>
<nextsent>phrase-based models provide some sort of context information as opposed to word-based models.
</nextsent>
<nextsent>training phrase-based model typically requires aligning parallel corpus, extracting phrases and scoring them using word and phrase counts.
</nextsent>
<nextsent>the derived statistics capture the structure of natural language to some extent, including implicit syntactic and semantic relations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2289">
<title id=" W07-0409.xml">combining morphosyntactic enriched representation with nbest reranking in statistical translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>using the morphosyntactic language model alone does not results in any improvement in performance.
</prevsent>
<prevsent>however, combining morphosyntactic word disambiguation with word based 4-gram language model results in relative improvement in the bleu score of 2.3% on the development set and 1.9% on the test set.
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
recent works in statistical machine translation (smt) shows how phrase-based modeling (och andney, 2000<papid> P00-1056 </papid>a; koehn et al, 2003) <papid> N03-1017 </papid>significantly out perform the historical word-based modeling (brown et al, 1993).<papid> J93-2003 </papid></citsent>
<aftsection>
<nextsent>using phrases, i.e. sequences ofwords, as translation units allows the system to preserve local word order constraints and to improve the consistency of phrases during the translation process.
</nextsent>
<nextsent>phrase-based models provide some sort of context information as opposed to word-based models.
</nextsent>
<nextsent>training phrase-based model typically requires aligning parallel corpus, extracting phrases and scoring them using word and phrase counts.
</nextsent>
<nextsent>the derived statistics capture the structure of natural language to some extent, including implicit syntactic and semantic relations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2290">
<title id=" W07-0409.xml">combining morphosyntactic enriched representation with nbest reranking in statistical translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>therefore, explicit introduction of structure in the language models becomes major and promising focus of atten tion.however, as of today, it seems difficult to outperform 4-gram word language model.
</prevsent>
<prevsent>several studies have attempted to use morphosyntactic information (also known as part-of-speech or pos informa tion) to improve translation.
</prevsent>
</prevsection>
<citsent citstr=" N04-1021 ">
(och et al, 2004) <papid> N04-1021 </papid>have explored many different feature functions.</citsent>
<aftsection>
<nextsent>reranking n-best lists using pos has also been explored by (hasan et al, 2006).<papid> W06-2606 </papid></nextsent>
<nextsent>in (kirchhoff and yang, 2005), <papid> W05-0821 </papid>factored language model using pos information showed similar performance to 4-gram word language model.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2291">
<title id=" W07-0409.xml">combining morphosyntactic enriched representation with nbest reranking in statistical translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>several studies have attempted to use morphosyntactic information (also known as part-of-speech or pos informa tion) to improve translation.
</prevsent>
<prevsent>(och et al, 2004) <papid> N04-1021 </papid>have explored many different feature functions.</prevsent>
</prevsection>
<citsent citstr=" W06-2606 ">
reranking n-best lists using pos has also been explored by (hasan et al, 2006).<papid> W06-2606 </papid></citsent>
<aftsection>
<nextsent>in (kirchhoff and yang, 2005), <papid> W05-0821 </papid>factored language model using pos information showed similar performance to 4-gram word language model.</nextsent>
<nextsent>syntax-based language models have also been investigated in (charniak et al, 2003).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2292">
<title id=" W07-0409.xml">combining morphosyntactic enriched representation with nbest reranking in statistical translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(och et al, 2004) <papid> N04-1021 </papid>have explored many different feature functions.</prevsent>
<prevsent>reranking n-best lists using pos has also been explored by (hasan et al, 2006).<papid> W06-2606 </papid></prevsent>
</prevsection>
<citsent citstr=" W05-0821 ">
in (kirchhoff and yang, 2005), <papid> W05-0821 </papid>factored language model using pos information showed similar performance to 4-gram word language model.</citsent>
<aftsection>
<nextsent>syntax-based language models have also been investigated in (charniak et al, 2003).
</nextsent>
<nextsent>all these studies use word phrases as translation units and pos information in just post-processing step.this paper explores the integration of morphosyntactic information into the translation model itself by enriching words with their morphosyntactic cat 65 egories.
</nextsent>
<nextsent>the same idea has already been applied in (hwang et al, 2007) to the basic travel expression corpus (btec).
</nextsent>
<nextsent>to our knowledge, this approach has not been evaluated on large real word translation problem.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2294">
<title id=" W07-0409.xml">combining morphosyntactic enriched representation with nbest reranking in statistical translation </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>the baseline system uses 8 feature functions hi,namely phrase translation probabilities in both directions, lexical translation probabilities in both directions, distortion feature, word and phrase 1http://www.statmt.org/moses/penalty and trigram target language model.
</prevsent>
<prevsent>additional features can be added, as described in the following sections.
</prevsent>
</prevsection>
<citsent citstr=" P02-1038 ">
the weights are typically optimized so as to maximize scoring function on development set (och and ney, 2002).<papid> P02-1038 </papid>the moses decoder can output n-best lists, producing either distinct target sentences or not (asdifferent segment ations may lead to the same sen tence).</citsent>
<aftsection>
<nextsent>in this work, distinct sentences were always used.
</nextsent>
<nextsent>these n-best lists can be rescored using higher order language models (word- or syntactic-based).
</nextsent>
<nextsent>there are two ways to carry out the rescoring: one, by replacing the language model score or by addinga new feature function; two, by performing loglinear interpolation of the language model used for decoding and the new language model.
</nextsent>
<nextsent>this latter approach was used in all the experiments described in this paper.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2296">
<title id=" W07-0409.xml">combining morphosyntactic enriched representation with nbest reranking in statistical translation </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>the set of weights is systematically re-optimized using the algorithm presented below.
</prevsent>
<prevsent>2.2 weight optimization.
</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
a common criterion to optimize the coefficients of the log-linear combination of feature functions is to maximize the bleu score (papineni et al, 2002) <papid> P02-1040 </papid>on development set (och and ney, 2002).<papid> P02-1038 </papid></citsent>
<aftsection>
<nextsent>for this purpose, the public numerical optimization tool condor (berghen and bersini, 2005) is integrated in the following iterative algorithm: 0.
</nextsent>
<nextsent>using good general purpose weights, the.
</nextsent>
<nextsent>moses decoder is used to generate 1000-best lists.
</nextsent>
<nextsent>1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2301">
<title id=" W06-2408.xml">multiword verbs in a flec tive language the case of estonian </title>
<section> the database and corpus.  </section>
<citcontext>
<prevsection>
<prevsent>describing the automatic treatment of multiword expressions in basque, alegria et.al.
</prevsent>
<prevsent>(2004) show that the support verb constructions in basque can have significant morphosyntactic variability, including modification of the noun and case alternation.
</prevsent>
</prevsection>
<citsent citstr=" W04-0409 ">
similar phenomenon (number and case alternation) in turkish is described in (oflazer et. al. 2004).<papid> W04-0409 </papid></citsent>
<aftsection>
<nextsent>in the following subsections we will briefly describe the phenomenon of the case alternation of the object in estonian and then discuss the variation of the nominal component of idioms and support verb constructions.
</nextsent>
<nextsent>then we will describe the number alternations of the nominal components.
</nextsent>
<nextsent>estonian vmwe often consists of verb and noun phrase that is its object syntactically.
</nextsent>
<nextsent>a few words should be said about the case alternation of the object in estonian in general (cf also erelt 2003: 96-97).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2302">
<title id=" W06-3504.xml">increasing the coverage of a domain independent dialogue lexicon with verbnet </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we report results on the efficiency of the method, discussing in particular precision versus coverage issues and implications for mapping to other lexical databases.
</prevsent>
<prevsent>this paper explores how different lexicons can be integrated with the goal of extending coverage of deep parser and semantic interpreter.
</prevsent>
</prevsection>
<citsent citstr=" A00-2008 ">
lexical semantic databases (kipper et al, 2000; johnson and fillmore, 2000; <papid> A00-2008 </papid>dorr, 1997) use frame-based model of lexical semantics.</citsent>
<aftsection>
<nextsent>each database groups words in classes where predicative words and their arguments are described.
</nextsent>
<nextsent>the classes are generally organised in an inheritance structure.
</nextsent>
<nextsent>each such database can be used, among other things, to perform semantic interpretation.
</nextsent>
<nextsent>however, their actual structures are quite different, reflecting different underlying methodological approaches to lexical description, and this results in representation that are not directly compatible.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2303">
<title id=" W06-3504.xml">increasing the coverage of a domain independent dialogue lexicon with verbnet </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>it outputs highly detailed semantic representations suitable for complex dialogue tasks such as problem-solving and tutoring dialogue, inter alia.
</prevsent>
<prevsent>an essential feature of tripsis the integration of detailed lexical semantic representation, semantic classes and theta role assignments in the parsing process.
</prevsent>
</prevsection>
<citsent citstr=" C04-1100 ">
semantic types and role labelling are helpful inboth deep (tetreault, 2005) and shallow interpretation tasks (narayanan and harabagiu, 2004).<papid> C04-1100 </papid></citsent>
<aftsection>
<nextsent>trips provides convenient test case because its grammar is already equipped with the formal devices required to build up frame-based semantic representation including this information.11while wide coverage grammars such as the english re 25 we chose verbnet to extend the trips lexicon because it includes detailed syntax-semantic mappings, thus providing more convenient interface tothe syntactic component of the grammar than lexicons where this connection is left unclear, such as framenet.
</nextsent>
<nextsent>however the methods described hereare designed to be reusable for merging other lexical databases, in particular we intend to experiment with framenet in the near future.the plan of the paper is as follows: we first describe the target lexicon (section 2) and the source lexicon (section 3) for our experiment before describing the methodology for integration (section 4).
</nextsent>
<nextsent>we finally present an evaluation of the techniques in section 5.
</nextsent>
<nextsent>the trips lexicon (dzikovska, 2004) is the target of the mapping procedure we describe in section 4.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2304">
<title id=" W06-3504.xml">increasing the coverage of a domain independent dialogue lexicon with verbnet </title>
<section> the trips lexicon.  </section>
<citcontext>
<prevsection>
<prevsent>the lf ontology was originally based on simplified version of framenet source grammar (copestake and flickinger, 2000) build deep semantic representations which account for scoping and temporal structure, their lexicons do not provide information related to word senses and role labels, in part due to the additional difficulty involved building wide coverage lexicon with the necessary lexical semantic information.
</prevsent>
<prevsent>the tourists admired the paintings lsubj lobj lf::experiencer-emotion lf::experiencer lf::themefigure 1: information in the trips word sense definition for mapping between syntactic and semantic roles.
</prevsent>
</prevsection>
<citsent citstr=" P98-1013 ">
(baker et al, 1998; <papid> P98-1013 </papid>dzikovska et al, 2004), with each lf type describing particular situation, objector event and its participants.</citsent>
<aftsection>
<nextsent>syntax-semantics templates (or templates) capture the linking between the syntax and semantics (lf type and semantic roles) of word.
</nextsent>
<nextsent>the semantic properties of an argument are described by means of semantic role assigned to it and selectional restrictions.2the trips grammar contains set of independently described lexical rules, such as the passive orda tive shift rules, which are designed to create non canonical lexical entries automatically, while preserving the linking properties defined in the canonical entry.in this context adding an entry to the lexicon requires determining both the list of lf types and the list of templates for canonical contexts, that is, the list of mappings between logical frame and canonical subcategorization frame.
</nextsent>
<nextsent>verbnet (kipper et al, 2000) provides an actual implementation of the descriptive work carried out by levin (1993), which has been extended to cover prepositional constructions and corpus-based subcategorization frames (kipper et al, 2004; <papid> W04-2604 </papid>kipper et al, 2006).</nextsent>
<nextsent>verbnet is hierarchical verb lexicon in which verbs are organised in classes.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2305">
<title id=" W06-3504.xml">increasing the coverage of a domain independent dialogue lexicon with verbnet </title>
<section> verbnet.  </section>
<citcontext>
<prevsection>
<prevsent>syntax-semantics templates (or templates) capture the linking between the syntax and semantics (lf type and semantic roles) of word.
</prevsent>
<prevsent>the semantic properties of an argument are described by means of semantic role assigned to it and selectional restrictions.2the trips grammar contains set of independently described lexical rules, such as the passive orda tive shift rules, which are designed to create non canonical lexical entries automatically, while preserving the linking properties defined in the canonical entry.in this context adding an entry to the lexicon requires determining both the list of lf types and the list of templates for canonical contexts, that is, the list of mappings between logical frame and canonical subcategorization frame.
</prevsent>
</prevsection>
<citsent citstr=" W04-2604 ">
verbnet (kipper et al, 2000) provides an actual implementation of the descriptive work carried out by levin (1993), which has been extended to cover prepositional constructions and corpus-based subcategorization frames (kipper et al, 2004; <papid> W04-2604 </papid>kipper et al, 2006).</citsent>
<aftsection>
<nextsent>verbnet is hierarchical verb lexicon in which verbs are organised in classes.
</nextsent>
<nextsent>the fundamental assumption underlying the classification is that the members of given class share similar syntactic 2the selectional restrictions are domain independent and specified using features derived from euro wordnet (vossen, 1997; dzikovska et al, to appear).
</nextsent>
<nextsent>26behaviour, that is, they pattern in the same set of alternations, and are further assumed to share common semantic properties.3 verbnet classes are organised in an inheritance hierarchy.
</nextsent>
<nextsent>each class includes set of members (verbs), set of (subcategorization) frames and set of semantic descriptions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2306">
<title id=" W06-1405.xml">individuality and alignment in generated dialogues </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>argamon et al (2005) attempted to classify authors as high or low extra vert and high or low neurotic, using penne baker and kings (1999) data.
</prevsent>
<prevsent>they report classification accuracies of around 58% (with 50% baseline).
</prevsent>
</prevsection>
<citsent citstr=" P06-2081 ">
oberlander and nowson (2006) <papid> P06-2081 </papid>undertake comparable task,using weblog data.</citsent>
<aftsection>
<nextsent>they report classification accuracies of roughly 85% (neuroticism) and 94%(extraversion), and comparable figures for agree able ness and conscientiousness.
</nextsent>
<nextsent>such studies can provide ordered lists of linguistic features which are useful for distinguishing language producers, and we will return to this, below.
</nextsent>
<nextsent>2.2 alignment and language.
</nextsent>
<nextsent>people converge with their interlocutors in linguistic choices at number of levels (pickering and garrod, 2004).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2307">
<title id=" W06-1405.xml">individuality and alignment in generated dialogues </title>
<section> dialogue and utterance specifications.  </section>
<citcontext>
<prevsection>
<prevsent>an example using the topic music and polarity negative is shown in figure 1.at this point the system also decides which discourse connectives may be appropriate, based on the previous topic and polarity.
</prevsent>
<prevsent> utterance   utt topic= music  polarity= dislike  opp-polarity= like  so= no  right= no  also= no  well= yes  and= no  but= no    pred adj= bad /   opp-pred adj= good /   /utt   /utterance  figure 1: simple utterance specification 6.3 openccg logical forms.
</prevsent>
</prevsection>
<citsent citstr=" W04-0601 ">
following the method described in foster and white (2004), <papid> W04-0601 </papid>the basic utterance specification is transformed, using style sheets written in the xsltransformation language, into an openccg logical form.</citsent>
<aftsection>
<nextsent>we make use of the facility for defining optional and alternative inputs and underspecified semantics to massively over-generate candidate utterances.
</nextsent>
<nextsent>a fragment of the logical form which results from the transformation of figure 1is shown in figure 2.
</nextsent>
<nextsent>we also include some fragments of canned text from the crag corpus in our openccg lexicon.
</nextsent>
<nextsent>we also add optional interjections (i mean, you know, sort of ) and conversational markers (right,but, and, well) where appropriate given the discourse history.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2308">
<title id=" W06-1405.xml">individuality and alignment in generated dialogues </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>related work in nlg involves either personality or alignment.
</prevsent>
<prevsent>so far as we can tell, there is little work on the latter.
</prevsent>
</prevsection>
<citsent citstr=" W05-1627 ">
varges (2005) <papid> W05-1627 </papid>suggests that aword similarity-based ranker could align the generation output (i.e. the highest-ranked candidate) with previous utterances in the discourse context?, but there is no report yet on an implementation ofthis proposal.</citsent>
<aftsection>
<nextsent>a rather different approach is suggested by bateman and paris (2005), who discuss initial work on alignment, mediated by process of register-recognition.
</nextsent>
<nextsent>regarding generation with personality, the most influential work is probablyhovys pauline system, which varies both content selection and realisation according to an individual speakers goals and attitudes (hovy, 1990).
</nextsent>
<nextsent>in her extremely useful survey of work on affective(particularly, emotional) natural language generation, belz (2003) notes that the complexity of pau lines rule system means that numerous rule interactions can lead to unpredictable side effects.
</nextsent>
<nextsent>in response, paiva and evans (2004) take more empirical line on style generation, which is closer to that pursued here.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2309">
<title id=" W07-0704.xml">exploring different representational units in englishtoturkish statistical machine translation </title>
<section> turkish and smt.  </section>
<citcontext>
<prevsection>
<prevsent>look quite different, the lexical morphemes except for the root are the same: ev+sh+nda vs. masa+sh+nda.we should however note that although employing morpheme based representations dramatically reduces the vocabulary size on the turkish side, italso runs the risk of overloading distortion mechanisms to account for both word-internal morpheme sequencing and sentence level word ordering.
</prevsent>
<prevsent>the segmentation of word in general is notunique.
</prevsent>
</prevsection>
<citsent citstr=" P06-1122 ">
we first generate representation that contains both the lexical segments and the morphological features encoded for all possible segmenta 1this is in sense very similar to the more general problem of lexical redundancy addressed by talbot and osborne (2006)<papid> P06-1122 </papid>but our approach does not require the more sophisticated solution there.</citsent>
<aftsection>
<nextsent>tions and interpretations of the word.
</nextsent>
<nextsent>for the wordemeli for instance, our morphological analyzer generates the following with lexical morphemes bracketed with (..): (em)em+verb+pos(+yalh)db+adverb+since since (someone) sucked (something) (emel)emel+noun+a3sg(+sh)+p3sg+nom his/her ambition (emel)emel+noun+a3sg+pnon(+yh)+acc ambition (as object of transitive verb)these analyses are then disambiguated with statistical disambiguator (yuret and ture, 2006) <papid> N06-1042 </papid>which operates on the morphological features.2 finally, the morphological features are removed from each parse leaving the lexical morphemes.using morphology in smt has been recently addressed by researchers translation from or into morphologically rich(er) languages.</nextsent>
<nextsent>niessen and ney (2004) <papid> J04-2003 </papid>have used morphological decomposition to improve alignment quality.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2310">
<title id=" W07-0704.xml">exploring different representational units in englishtoturkish statistical machine translation </title>
<section> turkish and smt.  </section>
<citcontext>
<prevsection>
<prevsent>we first generate representation that contains both the lexical segments and the morphological features encoded for all possible segmenta 1this is in sense very similar to the more general problem of lexical redundancy addressed by talbot and osborne (2006)<papid> P06-1122 </papid>but our approach does not require the more sophisticated solution there.</prevsent>
<prevsent>tions and interpretations of the word.</prevsent>
</prevsection>
<citsent citstr=" N06-1042 ">
for the wordemeli for instance, our morphological analyzer generates the following with lexical morphemes bracketed with (..): (em)em+verb+pos(+yalh)db+adverb+since since (someone) sucked (something) (emel)emel+noun+a3sg(+sh)+p3sg+nom his/her ambition (emel)emel+noun+a3sg+pnon(+yh)+acc ambition (as object of transitive verb)these analyses are then disambiguated with statistical disambiguator (yuret and ture, 2006) <papid> N06-1042 </papid>which operates on the morphological features.2 finally, the morphological features are removed from each parse leaving the lexical morphemes.using morphology in smt has been recently addressed by researchers translation from or into morphologically rich(er) languages.</citsent>
<aftsection>
<nextsent>niessen and ney (2004) <papid> J04-2003 </papid>have used morphological decomposition to improve alignment quality.</nextsent>
<nextsent>yang and kirchhoff (2006) use phrase-based backoff models to translate words that are unknown to the decoder, by morphologically decomposing the unknown source word.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2311">
<title id=" W07-0704.xml">exploring different representational units in englishtoturkish statistical machine translation </title>
<section> turkish and smt.  </section>
<citcontext>
<prevsection>
<prevsent>tions and interpretations of the word.
</prevsent>
<prevsent>for the wordemeli for instance, our morphological analyzer generates the following with lexical morphemes bracketed with (..): (em)em+verb+pos(+yalh)db+adverb+since since (someone) sucked (something) (emel)emel+noun+a3sg(+sh)+p3sg+nom his/her ambition (emel)emel+noun+a3sg+pnon(+yh)+acc ambition (as object of transitive verb)these analyses are then disambiguated with statistical disambiguator (yuret and ture, 2006) <papid> N06-1042 </papid>which operates on the morphological features.2 finally, the morphological features are removed from each parse leaving the lexical morphemes.using morphology in smt has been recently addressed by researchers translation from or into morphologically rich(er) languages.</prevsent>
</prevsection>
<citsent citstr=" J04-2003 ">
niessen and ney (2004) <papid> J04-2003 </papid>have used morphological decomposition to improve alignment quality.</citsent>
<aftsection>
<nextsent>yang and kirchhoff (2006) use phrase-based backoff models to translate words that are unknown to the decoder, by morphologically decomposing the unknown source word.
</nextsent>
<nextsent>they particularly apply their method to translating from finnish ? another language with very similar structural characteristics to turkish.
</nextsent>
<nextsent>corston-oliverand gamon (2004) normalize inflectional morphology by stemming the word for german-english word alignment.
</nextsent>
<nextsent>lee (2004) <papid> N04-4015 </papid>uses morphologically analyzed and tagged parallel corpus for arabic english smt.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2312">
<title id=" W07-0704.xml">exploring different representational units in englishtoturkish statistical machine translation </title>
<section> turkish and smt.  </section>
<citcontext>
<prevsection>
<prevsent>they particularly apply their method to translating from finnish ? another language with very similar structural characteristics to turkish.
</prevsent>
<prevsent>corston-oliverand gamon (2004) normalize inflectional morphology by stemming the word for german-english word alignment.
</prevsent>
</prevsection>
<citsent citstr=" N04-4015 ">
lee (2004) <papid> N04-4015 </papid>uses morphologically analyzed and tagged parallel corpus for arabic english smt.</citsent>
<aftsection>
<nextsent>zolmann et al (2006) also exploit morphology in arabic-english smt.
</nextsent>
<nextsent>popovic andney (2004) investigate improving translation qual 2this disambiguator has about 94% accuracy.
</nextsent>
<nextsent>26 ity from inflected languages by using stems, suffixes and part-of-speech tags.
</nextsent>
<nextsent>goldwater and mcclosky (2005) <papid> H05-1085 </papid>use morphological analysis on czech text toget improvements in czech to english smt.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2313">
<title id=" W07-0704.xml">exploring different representational units in englishtoturkish statistical machine translation </title>
<section> turkish and smt.  </section>
<citcontext>
<prevsection>
<prevsent>popovic andney (2004) investigate improving translation qual 2this disambiguator has about 94% accuracy.
</prevsent>
<prevsent>26 ity from inflected languages by using stems, suffixes and part-of-speech tags.
</prevsent>
</prevsection>
<citsent citstr=" H05-1085 ">
goldwater and mcclosky (2005) <papid> H05-1085 </papid>use morphological analysis on czech text toget improvements in czech to english smt.</citsent>
<aftsection>
<nextsent>recently, minkov et al (2007) <papid> P07-1017 </papid>have used morphological postprocessing on the output side using structural information and information from the source side, to improve smt quality.</nextsent>
<nextsent>our parallel data consists mainly of documents in international relations and legal documents from sources such as the turkish ministry of foreign affairs, eu, etc. we process these as follows: (i) we segment the words in our turkish corpus into lexical morphemes whereby differences in the surface representations of morphemes due to word-internal phenomena are abstracted out to improve statistics during alignment.3 (ii) we tag the english side using tree tagger (schmid, 1994), which provides lemma and part-of-speech for each word.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2314">
<title id=" W07-0704.xml">exploring different representational units in englishtoturkish statistical machine translation </title>
<section> turkish and smt.  </section>
<citcontext>
<prevsection>
<prevsent>26 ity from inflected languages by using stems, suffixes and part-of-speech tags.
</prevsent>
<prevsent>goldwater and mcclosky (2005) <papid> H05-1085 </papid>use morphological analysis on czech text toget improvements in czech to english smt.</prevsent>
</prevsection>
<citsent citstr=" P07-1017 ">
recently, minkov et al (2007) <papid> P07-1017 </papid>have used morphological postprocessing on the output side using structural information and information from the source side, to improve smt quality.</citsent>
<aftsection>
<nextsent>our parallel data consists mainly of documents in international relations and legal documents from sources such as the turkish ministry of foreign affairs, eu, etc. we process these as follows: (i) we segment the words in our turkish corpus into lexical morphemes whereby differences in the surface representations of morphemes due to word-internal phenomena are abstracted out to improve statistics during alignment.3 (ii) we tag the english side using tree tagger (schmid, 1994), which provides lemma and part-of-speech for each word.
</nextsent>
<nextsent>we then remove any tags which do not imply an explicit morpheme or an exceptional form.
</nextsent>
<nextsent>so for instance, if the word book gets tagged as +nn, we keep book in the text, but remove +nn.
</nextsent>
<nextsent>for books tagged as +nns or booking tagged as +vvg, we keep book and +nns, and book and +vvg.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2315">
<title id=" W07-0704.xml">exploring different representational units in englishtoturkish statistical machine translation </title>
<section> exploiting morphology.  </section>
<citcontext>
<prevsection>
<prevsent>e: the implementation3 of the acces sion1 partnership2 will be monitor7 +vvn in the framework6 of the association4 agreement5 . note that when the morphemes/tags (starting with +) are concatenated, we get the word-basedversion of the corpus, since surface words are directly recoverable from the concatenated representation.
</prevsent>
<prevsent>we use this word-based representation also for word-based language models used for rescoring.
</prevsent>
</prevsection>
<citsent citstr=" N03-1017 ">
we employ the phrase-based smt framework (koehn et al, 2003), <papid> N03-1017 </papid>and use the moses toolkit(koehn et al, 2007), <papid> P07-2045 </papid>and the srilm language modelling toolkit (stolcke, 2002), and evaluate our decoded translations using the bleu measure (papineni et al, 2002), <papid> P02-1040 </papid>using single reference transla tion.5the training set in the first row of 1 was limited to sentences on the turkish side which had at most 90 tokens (rootsand bound morphemes) in total in order to comply with requirements of the giza++ alignment tool.</citsent>
<aftsection>
<nextsent>however when only the content words are included, we have more sentences to include since much less number of sentences violate the length restriction when morphemes/function word are removed.
</nextsent>
<nextsent>27 moses dec. parms.
</nextsent>
<nextsent>bleu bleu-c default 16.29 16.13 dl = -1, -weight-d = 0.1 20.16 19.77 table 2: bleu results for baseline experiments.
</nextsent>
<nextsent>bleu is for the model trained on the training set bleu-c is for the model trained on training set augmented with the content words.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2316">
<title id=" W07-0704.xml">exploring different representational units in englishtoturkish statistical machine translation </title>
<section> exploiting morphology.  </section>
<citcontext>
<prevsection>
<prevsent>e: the implementation3 of the acces sion1 partnership2 will be monitor7 +vvn in the framework6 of the association4 agreement5 . note that when the morphemes/tags (starting with +) are concatenated, we get the word-basedversion of the corpus, since surface words are directly recoverable from the concatenated representation.
</prevsent>
<prevsent>we use this word-based representation also for word-based language models used for rescoring.
</prevsent>
</prevsection>
<citsent citstr=" P07-2045 ">
we employ the phrase-based smt framework (koehn et al, 2003), <papid> N03-1017 </papid>and use the moses toolkit(koehn et al, 2007), <papid> P07-2045 </papid>and the srilm language modelling toolkit (stolcke, 2002), and evaluate our decoded translations using the bleu measure (papineni et al, 2002), <papid> P02-1040 </papid>using single reference transla tion.5the training set in the first row of 1 was limited to sentences on the turkish side which had at most 90 tokens (rootsand bound morphemes) in total in order to comply with requirements of the giza++ alignment tool.</citsent>
<aftsection>
<nextsent>however when only the content words are included, we have more sentences to include since much less number of sentences violate the length restriction when morphemes/function word are removed.
</nextsent>
<nextsent>27 moses dec. parms.
</nextsent>
<nextsent>bleu bleu-c default 16.29 16.13 dl = -1, -weight-d = 0.1 20.16 19.77 table 2: bleu results for baseline experiments.
</nextsent>
<nextsent>bleu is for the model trained on the training set bleu-c is for the model trained on training set augmented with the content words.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2317">
<title id=" W07-0704.xml">exploring different representational units in englishtoturkish statistical machine translation </title>
<section> exploiting morphology.  </section>
<citcontext>
<prevsection>
<prevsent>e: the implementation3 of the acces sion1 partnership2 will be monitor7 +vvn in the framework6 of the association4 agreement5 . note that when the morphemes/tags (starting with +) are concatenated, we get the word-basedversion of the corpus, since surface words are directly recoverable from the concatenated representation.
</prevsent>
<prevsent>we use this word-based representation also for word-based language models used for rescoring.
</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
we employ the phrase-based smt framework (koehn et al, 2003), <papid> N03-1017 </papid>and use the moses toolkit(koehn et al, 2007), <papid> P07-2045 </papid>and the srilm language modelling toolkit (stolcke, 2002), and evaluate our decoded translations using the bleu measure (papineni et al, 2002), <papid> P02-1040 </papid>using single reference transla tion.5the training set in the first row of 1 was limited to sentences on the turkish side which had at most 90 tokens (rootsand bound morphemes) in total in order to comply with requirements of the giza++ alignment tool.</citsent>
<aftsection>
<nextsent>however when only the content words are included, we have more sentences to include since much less number of sentences violate the length restriction when morphemes/function word are removed.
</nextsent>
<nextsent>27 moses dec. parms.
</nextsent>
<nextsent>bleu bleu-c default 16.29 16.13 dl = -1, -weight-d = 0.1 20.16 19.77 table 2: bleu results for baseline experiments.
</nextsent>
<nextsent>bleu is for the model trained on the training set bleu-c is for the model trained on training set augmented with the content words.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2319">
<title id=" W07-0704.xml">exploring different representational units in englishtoturkish statistical machine translation </title>
<section> model iteration.  </section>
<citcontext>
<prevsection>
<prevsent>the first two are quite accurate and acceptable translations while the third clearly has missing and incorrect parts.
</prevsent>
<prevsent>we have also experimented with an iterative approach to use multiple models to see if further improvements are possible.
</prevsent>
</prevsection>
<citsent citstr=" N07-1064 ">
this is akin to post-editing(though definitely not akin to the much more sophisticated approach in described in simard et al (2007)).<papid> N07-1064 </papid></citsent>
<aftsection>
<nextsent>we proceeded as follows: we used the selective segmentation based model above and decoded our english training data etrain and english test data etest to obtain t1train and t1test re step bleu from table 4 24.61 iter.
</nextsent>
<nextsent>1 24.77 iter.
</nextsent>
<nextsent>2 25.08 table 6: bleu results for two model iterations spectively.
</nextsent>
<nextsent>we then trained the next model using t1train and ttrain, to build model that hopefully will improve upon the output of the previous model, t1test, to bring it closer to ttest.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2320">
<title id=" W07-0704.xml">exploring different representational units in englishtoturkish statistical machine translation </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>rors, but nevertheless they are very close to the root word bleu scores above.
</prevsent>
<prevsent>another path to pursue in repairing words is to identify morphologically correct words which are either oovs in the language model or for which the language model has low confidence.
</prevsent>
</prevsection>
<citsent citstr=" W06-3110 ">
one can perhaps identify these using posterior probabilities (e.g., using techniques in zens and ney (2006)) <papid> W06-3110 </papid>and generate additional morphologically valid words that are close?</citsent>
<aftsection>
<nextsent>and construct lattice that can be rescored.
</nextsent>
<nextsent>6.2 some thoughts on bleu.
</nextsent>
<nextsent>bleu is particularly harsh for turkish and the morpheme based-approach, because of the all-or-none nature of token comparison, as discussed above.
</nextsent>
<nextsent>there are also cases where words with different morphemes have very close morpho semantics, convey the relevant meaning and are almost inter changeable: ? gel+hyor (geliyor - he is coming) vs. gel+makta (gelmekte - he is (in state of) coming) are essentially the same.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2321">
<title id=" W07-0704.xml">exploring different representational units in englishtoturkish statistical machine translation </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>- he came (hearsay past tense)).
</prevsent>
<prevsent>these essentially mark past tense but differ in how the speaker relates to the event and could berated at perhaps 0.70 similarity.
</prevsent>
</prevsection>
<citsent citstr=" W05-0909 ">
note that using stems and their synonyms as used in meteor (banerjee and lavie, 2005) <papid> W05-0909 </papid>could also be considered for word similarity.again using the bleu+ tool and slightly different formulation of token similarity in bleu computation, we find that using morphological similarity our best score above, 25.08 bleu increases to 25.14 bleu, while using only root word synonymy and very close hypernymy from wordnet, gives us 25.45 bleu.</citsent>
<aftsection>
<nextsent>the combination of rules and wordnet match gives 25.46 bleu.
</nextsent>
<nextsent>note that these increases are much less than what can (potentially) be gained from solving the word-repair problem above.
</nextsent>
<nextsent>we have presented results from our investigation into using different granularity of sub-lexical representations for english to turkish smt.
</nextsent>
<nextsent>we have found that employing language-pair specific representation somewhere in between using full word forms and fully morphologically segmented representations and using content words as additional 31 data provide significant boost in bleu scores, in addition to contributions of word-level rescoring of 1000-best outputs and model iteration, to give ableu score of 25.08 points with very modest parallel text resources.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2322">
<title id=" W06-2918.xml">using gazette ers in discriminative information extraction </title>
<section> abstract </section>
<citcontext>
<prevsection>
<prevsent>approaches to such tasks often involve the use of maximum entropy-style models, where gazette ers usually appear as highly informative features in the model.
</prevsent>
<prevsent>although such features can improve model accuracy, they can also introduce hidden negative effects.
</prevsent>
</prevsection>
<citsent citstr=" P05-1003 ">
in this paper we describe and analyse these effects and suggest ways in which they may be overcome.in particular, we show that by quarantining gazetteer features and training them in separate model, then decoding using logarithmic opinion pool (smith et al,2005), <papid> P05-1003 </papid>we may achieve much higher accu racy.</citsent>
<aftsection>
<nextsent>finally, we suggest ways in which other features with gazetteer feature-like behaviour may be identified.
</nextsent>
<nextsent>in recent years discriminative probabilistic model shave been successfully applied to number of information extraction tasks in natural language processing (nlp), such as named entity recognition (ner) (mccallum and li, 2003), <papid> W03-0430 </papid>noun phrase chunking (sha and pereira, 2003) <papid> N03-1028 </papid>and information extraction from research papers (peng and mccallum, 2004).<papid> N04-1042 </papid></nextsent>
<nextsent>discriminative models offer significant advantage over their generative counterparts by allowing the specification of powerful, possibly non-independent features which would be difficult to tract ably encode in generative model.in task such asner, one sometimes encounters an entity which is difficult to identify using local contextual cues alone because the entity has notbe seen before.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2323">
<title id=" W06-2918.xml">using gazette ers in discriminative information extraction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this paper we describe and analyse these effects and suggest ways in which they may be overcome.in particular, we show that by quarantining gazetteer features and training them in separate model, then decoding using logarithmic opinion pool (smith et al,2005), <papid> P05-1003 </papid>we may achieve much higher accu racy.</prevsent>
<prevsent>finally, we suggest ways in which other features with gazetteer feature-like behaviour may be identified.</prevsent>
</prevsection>
<citsent citstr=" W03-0430 ">
in recent years discriminative probabilistic model shave been successfully applied to number of information extraction tasks in natural language processing (nlp), such as named entity recognition (ner) (mccallum and li, 2003), <papid> W03-0430 </papid>noun phrase chunking (sha and pereira, 2003) <papid> N03-1028 </papid>and information extraction from research papers (peng and mccallum, 2004).<papid> N04-1042 </papid></citsent>
<aftsection>
<nextsent>discriminative models offer significant advantage over their generative counterparts by allowing the specification of powerful, possibly non-independent features which would be difficult to tract ably encode in generative model.in task such asner, one sometimes encounters an entity which is difficult to identify using local contextual cues alone because the entity has notbe seen before.
</nextsent>
<nextsent>in these cases, gazetteer or dictionary of possible entity identifiers is often useful.
</nextsent>
<nextsent>such identifiers could be names of people, places, companies or other organisations.
</nextsent>
<nextsent>using gazette ers one may define additional features in the model that represent the dependencies between words ner label and its presence in particular gazetteer.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2324">
<title id=" W06-2918.xml">using gazette ers in discriminative information extraction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this paper we describe and analyse these effects and suggest ways in which they may be overcome.in particular, we show that by quarantining gazetteer features and training them in separate model, then decoding using logarithmic opinion pool (smith et al,2005), <papid> P05-1003 </papid>we may achieve much higher accu racy.</prevsent>
<prevsent>finally, we suggest ways in which other features with gazetteer feature-like behaviour may be identified.</prevsent>
</prevsection>
<citsent citstr=" N03-1028 ">
in recent years discriminative probabilistic model shave been successfully applied to number of information extraction tasks in natural language processing (nlp), such as named entity recognition (ner) (mccallum and li, 2003), <papid> W03-0430 </papid>noun phrase chunking (sha and pereira, 2003) <papid> N03-1028 </papid>and information extraction from research papers (peng and mccallum, 2004).<papid> N04-1042 </papid></citsent>
<aftsection>
<nextsent>discriminative models offer significant advantage over their generative counterparts by allowing the specification of powerful, possibly non-independent features which would be difficult to tract ably encode in generative model.in task such asner, one sometimes encounters an entity which is difficult to identify using local contextual cues alone because the entity has notbe seen before.
</nextsent>
<nextsent>in these cases, gazetteer or dictionary of possible entity identifiers is often useful.
</nextsent>
<nextsent>such identifiers could be names of people, places, companies or other organisations.
</nextsent>
<nextsent>using gazette ers one may define additional features in the model that represent the dependencies between words ner label and its presence in particular gazetteer.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2326">
<title id=" W06-2918.xml">using gazette ers in discriminative information extraction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this paper we describe and analyse these effects and suggest ways in which they may be overcome.in particular, we show that by quarantining gazetteer features and training them in separate model, then decoding using logarithmic opinion pool (smith et al,2005), <papid> P05-1003 </papid>we may achieve much higher accu racy.</prevsent>
<prevsent>finally, we suggest ways in which other features with gazetteer feature-like behaviour may be identified.</prevsent>
</prevsection>
<citsent citstr=" N04-1042 ">
in recent years discriminative probabilistic model shave been successfully applied to number of information extraction tasks in natural language processing (nlp), such as named entity recognition (ner) (mccallum and li, 2003), <papid> W03-0430 </papid>noun phrase chunking (sha and pereira, 2003) <papid> N03-1028 </papid>and information extraction from research papers (peng and mccallum, 2004).<papid> N04-1042 </papid></citsent>
<aftsection>
<nextsent>discriminative models offer significant advantage over their generative counterparts by allowing the specification of powerful, possibly non-independent features which would be difficult to tract ably encode in generative model.in task such asner, one sometimes encounters an entity which is difficult to identify using local contextual cues alone because the entity has notbe seen before.
</nextsent>
<nextsent>in these cases, gazetteer or dictionary of possible entity identifiers is often useful.
</nextsent>
<nextsent>such identifiers could be names of people, places, companies or other organisations.
</nextsent>
<nextsent>using gazette ers one may define additional features in the model that represent the dependencies between words ner label and its presence in particular gazetteer.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2330">
<title id=" W06-2918.xml">using gazette ers in discriminative information extraction </title>
<section> previous use of gazette ers.  </section>
<citcontext>
<prevsection>
<prevsent>gazette ers have been widely used in variety of information extraction systems, including both rule based systems and statistical models.
</prevsent>
<prevsent>in addition to lists of people names, locations, etc., recent work in the biomedical domain has utilised gazette ers of biological and genetic entities such as gene names (finkel et al, 2005; mcdonald and pereira, 2005).
</prevsent>
</prevsection>
<citsent citstr=" M98-1015 ">
in general gazette ers are thought to provide useful source of external knowledge that is helpful whenan entity cannot be identified from knowledge contained solely within the dataset used for training.however, some research has questioned the usefulness of gazette ers (krupka and hausman, 1998).<papid> M98-1015 </papid></citsent>
<aftsection>
<nextsent>other work has supported the use of gazette ers in general but has found that lists of only moderate size are sufficient to provide most of the benefit(mikheev et al, 1999).<papid> E99-1001 </papid></nextsent>
<nextsent>therefore, to date the effective use of gazette ers for information extraction has in general been regarded as black art?.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2331">
<title id=" W06-2918.xml">using gazette ers in discriminative information extraction </title>
<section> previous use of gazette ers.  </section>
<citcontext>
<prevsection>
<prevsent>in addition to lists of people names, locations, etc., recent work in the biomedical domain has utilised gazette ers of biological and genetic entities such as gene names (finkel et al, 2005; mcdonald and pereira, 2005).
</prevsent>
<prevsent>in general gazette ers are thought to provide useful source of external knowledge that is helpful whenan entity cannot be identified from knowledge contained solely within the dataset used for training.however, some research has questioned the usefulness of gazette ers (krupka and hausman, 1998).<papid> M98-1015 </papid></prevsent>
</prevsection>
<citsent citstr=" E99-1001 ">
other work has supported the use of gazette ers in general but has found that lists of only moderate size are sufficient to provide most of the benefit(mikheev et al, 1999).<papid> E99-1001 </papid></citsent>
<aftsection>
<nextsent>therefore, to date the effective use of gazette ers for information extraction has in general been regarded as black art?.
</nextsent>
<nextsent>in this paper we explain some of the likely reasons for these findings, and propose ways to more effectively handle gazette ers when they are used by maxent-style models.
</nextsent>
<nextsent>in work developed independently and in parallel to the work presented here, sutton et al (2006) <papid> N06-1012 </papid>identify general problems with gazetteer features and propose solution similar to ours.</nextsent>
<nextsent>they present results on np-chunking in addition toner, and provide slightly more general approach.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2332">
<title id=" W06-2918.xml">using gazette ers in discriminative information extraction </title>
<section> previous use of gazette ers.  </section>
<citcontext>
<prevsection>
<prevsent>therefore, to date the effective use of gazette ers for information extraction has in general been regarded as black art?.
</prevsent>
<prevsent>in this paper we explain some of the likely reasons for these findings, and propose ways to more effectively handle gazette ers when they are used by maxent-style models.
</prevsent>
</prevsection>
<citsent citstr=" N06-1012 ">
in work developed independently and in parallel to the work presented here, sutton et al (2006) <papid> N06-1012 </papid>identify general problems with gazetteer features and propose solution similar to ours.</citsent>
<aftsection>
<nextsent>they present results on np-chunking in addition toner, and provide slightly more general approach.
</nextsent>
<nextsent>by contrast, we motivate the problem more thoroughly through analysis of the actual errors observed and through consideration of the success of other candidate solutions, such as traditional regular isation over feature subsets.
</nextsent>
<nextsent>in this section we describe our experimental setup, and provide results for the baseline models.
</nextsent>
<nextsent>4.1 task and dataset.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2333">
<title id=" W06-2918.xml">using gazette ers in discriminative information extraction </title>
<section> our experiments.  </section>
<citcontext>
<prevsection>
<prevsent>the model without gazetteer features, which we call standard, comprises features defined in window of five words around the current word.
</prevsent>
<prevsent>these include features encoding n-grams of words and pos tags, and features encoding orthographic properties of the current word.
</prevsent>
</prevsection>
<citsent citstr=" W03-0424 ">
the orthographic features are based on those found in (curran and clark, 2003).<papid> W03-0424 </papid>examples include whether the current word is capitalised, is an initial, contains digit, contains punctuation, etc. in total there are 450 345 features in the standard model.</citsent>
<aftsection>
<nextsent>we call the second model, with gazetteer features, standard+g. this includes all the features contained in the standard model as well as 8 329 gazetteer features.
</nextsent>
<nextsent>our gazetteer features are typical way to represent gazetteer information in maxent-stylemodels.
</nextsent>
<nextsent>they are divided into two categories: un lexicalised and lexicalised.
</nextsent>
<nextsent>the un lexicalised features model the dependency between words presence in gazetteer and its ner label, irrespective of the words identity.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2335">
<title id=" W06-1613.xml">automatic classification of citation function </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>can fulfil the relation search need: it is always centered around the physical location of the citations, but the context is often not informative enough for the searcher to infer the relation.
</prevsent>
<prevsent>in fact, studies from our annotated corpus (teufel,1999) show that 69% of the 600 sentences stating contrast with other work and 21% of the 246 sentences stating research continuation with other work do not contain the corresponding cita tion; the citation is found in preceding sentences (which means that the sentence expressing the contrast or continuation is outside the cite seer snippet).
</prevsent>
</prevsection>
<citsent citstr=" W06-0804 ">
a more sophisticated, discourse-aware citation indexer which finds these sentences and associates them with the citation would add considerable value to the researchers bibliographic search (ritchie et al , 2006<papid> W06-0804 </papid>b).</citsent>
<aftsection>
<nextsent>our annotation scheme for citations is based on empirical work in content citation analysis.
</nextsent>
<nextsent>it is designed for information retrieval applications such as improved citation indexing and better bibliometric measures (teufel et al , 2006).<papid> W06-1312 </papid></nextsent>
<nextsent>its 12 categories mark relationships with other works.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2339">
<title id=" W06-1613.xml">automatic classification of citation function </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a more sophisticated, discourse-aware citation indexer which finds these sentences and associates them with the citation would add considerable value to the researchers bibliographic search (ritchie et al , 2006<papid> W06-0804 </papid>b).</prevsent>
<prevsent>our annotation scheme for citations is based on empirical work in content citation analysis.</prevsent>
</prevsection>
<citsent citstr=" W06-1312 ">
it is designed for information retrieval applications such as improved citation indexing and better bibliometric measures (teufel et al , 2006).<papid> W06-1312 </papid></citsent>
<aftsection>
<nextsent>its 12 categories mark relationships with other works.
</nextsent>
<nextsent>each citation is labelled with exactly one category.
</nextsent>
<nextsent>the following top-level four-way distinction applies: ? explicit statement of weakness ? contrast or comparison with other work (4 categories) ? agreement/usage/compatibility with other work (6 categories), and ? neutral category.
</nextsent>
<nextsent>in this paper, we show that the scheme can be reliably annotated by independent coders.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2340">
<title id=" W06-1613.xml">automatic classification of citation function </title>
<section> an annotation scheme for citations.  </section>
<citcontext>
<prevsection>
<prevsent>annotators are thus asked to skim-read the paper before annotation.
</prevsent>
<prevsent>one possible view on this annotation scheme could consider the first two sets of categories asnegative?
</prevsent>
</prevsection>
<citsent citstr=" W02-1011 ">
and the third set of categories posi tive?, in the sense of pang et al  (2002) <papid> W02-1011 </papid>and turney (2002).<papid> P02-1053 </papid></citsent>
<aftsection>
<nextsent>authors need to make point (namely, 2our citation processor can recognise these after parsing the citation list.that they have contributed something which is better or at least new (myers, 1992)), and they thus have stance towards their citations.
</nextsent>
<nextsent>but although there is sentiment aspect to the interpretation of citations, this is not the whole story.
</nextsent>
<nextsent>many of ourpositive?
</nextsent>
<nextsent>categories are more concerned with different ways in which the cited work is useful to the current work (which aspect of it is used, e.g., just definition or the entire solution?), and many of the contrastive statements have no negative connotation at all and simply state (value-free) difference between approaches.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2341">
<title id=" W06-1613.xml">automatic classification of citation function </title>
<section> an annotation scheme for citations.  </section>
<citcontext>
<prevsection>
<prevsent>annotators are thus asked to skim-read the paper before annotation.
</prevsent>
<prevsent>one possible view on this annotation scheme could consider the first two sets of categories asnegative?
</prevsent>
</prevsection>
<citsent citstr=" P02-1053 ">
and the third set of categories posi tive?, in the sense of pang et al  (2002) <papid> W02-1011 </papid>and turney (2002).<papid> P02-1053 </papid></citsent>
<aftsection>
<nextsent>authors need to make point (namely, 2our citation processor can recognise these after parsing the citation list.that they have contributed something which is better or at least new (myers, 1992)), and they thus have stance towards their citations.
</nextsent>
<nextsent>but although there is sentiment aspect to the interpretation of citations, this is not the whole story.
</nextsent>
<nextsent>many of ourpositive?
</nextsent>
<nextsent>categories are more concerned with different ways in which the cited work is useful to the current work (which aspect of it is used, e.g., just definition or the entire solution?), and many of the contrastive statements have no negative connotation at all and simply state (value-free) difference between approaches.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2344">
<title id=" W06-1613.xml">automatic classification of citation function </title>
<section> an annotation scheme for citations.  </section>
<citcontext>
<prevsection>
<prevsent>the guidelines were developed on different set of articles from the ones used for annotation.
</prevsent>
<prevsent>inter-annotator agreement was kappa=.72(n=12;n=548;k=3)4 . this is quite high, considering the number of categories and the difficulties 3as opposed to reference list items, which are fewer.
</prevsent>
</prevsection>
<citsent citstr=" J96-2004 ">
4following carletta (1996), <papid> J96-2004 </papid>we measure agreement in kappa, which follows the formula = (a)p (e)1p (e) wherep(a) is observed, and p(e) expected agreement.</citsent>
<aftsection>
<nextsent>kappa ranges between -1 and 1.
</nextsent>
<nextsent>k=0 means agreement is only as expected by chance.
</nextsent>
<nextsent>generally, kappas of 0.8 are considered stable, and kappas of .69 as marginally stable, according to the strictest scheme applied in the field.
</nextsent>
<nextsent>106 (e.g., non-local dependencies) of the task.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2345">
<title id=" W06-1613.xml">automatic classification of citation function </title>
<section> features for automatic recognition of.  </section>
<citcontext>
<prevsection>
<prevsent>this is in concordance with earlier annotation experiments (moravcsik and murugesan, 1975; spiegel-rusing, 1977).
</prevsent>
<prevsent>citation function this section summarises the features we use for machine learning citation function.
</prevsent>
</prevsection>
<citsent citstr=" J02-4002 ">
some of these features were previously found useful for different application, namely argumentative zoning (teufel, 1999; teufel and moens, 2002), <papid> J02-4002 </papid>some are specific to citation classification.</citsent>
<aftsection>
<nextsent>3.1 cue phrases.
</nextsent>
<nextsent>myers (1992) calls meta-discourse the set of expressions that talk about the act of presenting research in paper, rather than the research itself(which is called object-level discourse).
</nextsent>
<nextsent>for instance, swales (1990) names phrases such as to our knowledge, no.
</nextsent>
<nextsent>or as far as we aware?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2346">
<title id=" W06-1613.xml">automatic classification of citation function </title>
<section> features for automatic recognition of.  </section>
<citcontext>
<prevsection>
<prevsent>we use 20 manually acquired verb clusters.
</prevsent>
<prevsent>negation is recognised, but too rare to define its own clusters: out of the 20 ? 2 = 40 theoretically possible verb clusters, only 27 were observed inour development corpus.
</prevsent>
</prevsection>
<citsent citstr=" P06-1116 ">
we have recently automated the process of verb object pair acquisition from corpora for two types of cue phrases (abdalla and teufel, 2006) <papid> P06-1116 </papid>and are planning on expanding this work to other cue phrases.</citsent>
<aftsection>
<nextsent>3.2 cues identified by annotators.
</nextsent>
<nextsent>during the annotator training phase, the annotators were encouraged to type in the meta description cue phrases that justify their choice of category.
</nextsent>
<nextsent>we went through this list by hand and extracted 892 cue phrases (around 75 per cate gory).
</nextsent>
<nextsent>the files these cues came from were not part of the test corpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2352">
<title id=" W06-3301.xml">the semantics of a definiendum constrains both the lexical semantics and the lexico syntactic patterns in the definiens </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>definitional questions (e.g., what is x??)
</prevsent>
<prevsent>constitute an important question type and have been part of the evaluation at the text retrieval conference (trec) question answering track since 2003.
</prevsent>
</prevsection>
<citsent citstr=" N04-1007 ">
most systems apply one-size-fits-all lexico syntactic patterns to identify definitions (liang et al. 2001; blair-goldensohn et al 2004; hildebrandt et al 2004; <papid> N04-1007 </papid>cui et al 2005).</citsent>
<aftsection>
<nextsent>forex ample, the pattern np, (such as|like|including) query term?
</nextsent>
<nextsent>can be used to identify the definition new research in mice suggests that drugs such as ritalin quiet hyperactivity?
</nextsent>
<nextsent>(liang et al 2001).
</nextsent>
<nextsent>few existing systems, however, have explored the relations between the semantic type (denoted as sdt) of definiendum (i.e., defined term (dt)) and the semantic types (denoted as sdef) of terms in its definiens (i.e., definition).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2353">
<title id=" W06-3301.xml">the semantics of a definiendum constrains both the lexical semantics and the lexico syntactic patterns in the definiens </title>
<section> unified medical language system.  </section>
<citcontext>
<prevsection>
<prevsent>the unified medical language system (umls) is the largest biomedical knowledge source maintained by the national library of medicine.
</prevsent>
<prevsent>it provides standardized biomedical concept relations and synonyms (humphreys et al 1998).
</prevsent>
</prevsection>
<citsent citstr=" W04-2611 ">
the umls has been widely used in many natural language processing tasks, including information retrieval (eichmann et al 1998), extraction (rindflesch et al 2000), and text summarization (elhadad et al 2004; fiszman et al 2004).<papid> W04-2611 </papid></citsent>
<aftsection>
<nextsent>the umls includes the meta thesaurus (mt), which contains over one million biomedical concepts and the semantic network (sn), which represents high-level abstraction from the umls metathesaurus.
</nextsent>
<nextsent>the sn consists of 134 semantic types with 54 types of semantic relations (e.g., is-a or part-of) that relate the semantic types to each other.
</nextsent>
<nextsent>the umls semantic network provides broad and general world knowledge that is related to human health.
</nextsent>
<nextsent>each umls concept is assigned one or more semantic types.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2354">
<title id=" W06-3301.xml">the semantics of a definiendum constrains both the lexical semantics and the lexico syntactic patterns in the definiens </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>systems have used named entities (e.g., people?
</prevsent>
<prevsent>and location?)
</prevsent>
</prevsection>
<citsent citstr=" H05-1015 ">
to assist in information extraction (agichtein and gravano 2000) and question answering (moldovan et al 2002; filatova and prager 2005).<papid> H05-1015 </papid></citsent>
<aftsection>
<nextsent>semantic constraints were first explored by (bodenreider and burgun 2002; rindflesch and fiszman 2003) who observed that the principle nouns in definientia are frequently semantically related (e.g., hyponyms, hy pernyms, siblings, and synonyms) to definiena.
</nextsent>
<nextsent>semantic constraints have been introduced to definitional question answering (prager et al 2000; liang et al 2001).
</nextsent>
<nextsent>for example, an artists work must be completed between his birth and death (prager et al 2000); and the hyponyms of defined terms might be incorporated in the definitions (liang et al 2001).
</nextsent>
<nextsent>semantic correlations have been explored in other areas of nlp.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2355">
<title id=" W06-3301.xml">the semantics of a definiendum constrains both the lexical semantics and the lexico syntactic patterns in the definiens </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>for example, an artists work must be completed between his birth and death (prager et al 2000); and the hyponyms of defined terms might be incorporated in the definitions (liang et al 2001).
</prevsent>
<prevsent>semantic correlations have been explored in other areas of nlp.
</prevsent>
</prevsection>
<citsent citstr=" P02-1053 ">
for example, researchers (turney 2002; <papid> P02-1053 </papid>yu and hatzivassiloglou 2003) <papid> W03-1017 </papid>have identified semantic correlation between words and views: positive words tend to appear more frequently in positive movie and product reviews and newswire article sentences that have positive semantic orientation and vice versa for negative reviews or sentences with negative semantic orientation.</citsent>
<aftsection>
<nextsent>this is the first study in definitional question answering that concludes that the semantics of de finiendum constrain both the lexical semantics and the lexico syntactic patterns in the definition.
</nextsent>
<nextsent>our discoveries may be useful for the building of biomedical definitional question answering system.
</nextsent>
<nextsent>although our discoveries (i.e., that the semantic types of the definitional terms determine both the lexico syntactic patterns and the semantic types in the definitions) were evaluated with the knowledge framework from the biomedical, domain-specific knowledge resource the umls, the principles may be generalizable to any type of semantic classification of definitions.
</nextsent>
<nextsent>the semantic constraints may enhance both recall and precision of one-size-fits all question answering systems, which may be evaluated in future work.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2356">
<title id=" W06-3301.xml">the semantics of a definiendum constrains both the lexical semantics and the lexico syntactic patterns in the definiens </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>for example, an artists work must be completed between his birth and death (prager et al 2000); and the hyponyms of defined terms might be incorporated in the definitions (liang et al 2001).
</prevsent>
<prevsent>semantic correlations have been explored in other areas of nlp.
</prevsent>
</prevsection>
<citsent citstr=" W03-1017 ">
for example, researchers (turney 2002; <papid> P02-1053 </papid>yu and hatzivassiloglou 2003) <papid> W03-1017 </papid>have identified semantic correlation between words and views: positive words tend to appear more frequently in positive movie and product reviews and newswire article sentences that have positive semantic orientation and vice versa for negative reviews or sentences with negative semantic orientation.</citsent>
<aftsection>
<nextsent>this is the first study in definitional question answering that concludes that the semantics of de finiendum constrain both the lexical semantics and the lexico syntactic patterns in the definition.
</nextsent>
<nextsent>our discoveries may be useful for the building of biomedical definitional question answering system.
</nextsent>
<nextsent>although our discoveries (i.e., that the semantic types of the definitional terms determine both the lexico syntactic patterns and the semantic types in the definitions) were evaluated with the knowledge framework from the biomedical, domain-specific knowledge resource the umls, the principles may be generalizable to any type of semantic classification of definitions.
</nextsent>
<nextsent>the semantic constraints may enhance both recall and precision of one-size-fits all question answering systems, which may be evaluated in future work.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2357">
<title id=" W07-0303.xml">the multimodal presentation dashboard </title>
<section> multimodal architecture.  </section>
<citcontext>
<prevsection>
<prevsent>a common use of deck sets is to combine main presentation with series of other slide decks that provide background information and detail for answering questions and expanding points, so the presenter can adapt to the interests of the audience.
</prevsent>
<prevsent>figure 4 the loader panel
</prevsent>
</prevsection>
<citsent citstr=" P02-1048 ">
the multimodal presentation dashboard uses an underlying multimodal architecture that inherits core components from the match architecture (johnston et al 2002).<papid> P02-1048 </papid></citsent>
<aftsection>
<nextsent>the components communicate through central messaging facilitator and include speech recognition client, speech recognition server (goffin et al 2005), natural language understanding component (johnston &amp; bangalore 2005), an information retrieval engine, 20 and graphical user interface client.
</nextsent>
<nextsent>the graphical ui runs in web browser and controls power point via its com interface.
</nextsent>
<nextsent>we first describe the compilation architecture, which builds models and performs indexing when the user selects series of decks to activate.
</nextsent>
<nextsent>we then describe the runtime architecture that operates when the user gives presentation using the system.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2358">
<title id=" W06-1906.xml">bruja question classification for spanish using machine translation and an english classifier </title>
<section> question classification.  </section>
<citcontext>
<prevsection>
<prevsent>(zhang and lee, 2003)propose qc system that uses support vector machine (svm) as the best machine learning algorithm.
</prevsent>
<prevsent>they compare the obtained results with other algorithms, such as nearest neighbors, naive bayes, decision tree or sparse network of winnows (snow).
</prevsent>
</prevsection>
<citsent citstr=" C02-1150 ">
(li and roth, 2002) <papid> C02-1150 </papid>propose system based on snow.</citsent>
<aftsection>
<nextsent>they used five main classes and fifty finedclasses.
</nextsent>
<nextsent>other systems have used svm and modified kernels.
</nextsent>
<nextsent>qc systems have some restrictions (hacioglu and ward, 2003), <papid> N03-2010 </papid>such as: ? traditional question classification uses set of rules, for instance questions that start with who ask about person?.</nextsent>
<nextsent>these are manual rules that have to be revised to im prove the results.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2360">
<title id=" W06-1906.xml">bruja question classification for spanish using machine translation and an english classifier </title>
<section> question classification.  </section>
<citcontext>
<prevsection>
<prevsent>they used five main classes and fifty finedclasses.
</prevsent>
<prevsent>other systems have used svm and modified kernels.
</prevsent>
</prevsection>
<citsent citstr=" N03-2010 ">
qc systems have some restrictions (hacioglu and ward, 2003), <papid> N03-2010 </papid>such as: ? traditional question classification uses set of rules, for instance questions that start with who ask about person?.</citsent>
<aftsection>
<nextsent>these are manual rules that have to be revised to im prove the results.
</nextsent>
<nextsent>these rules are very weak, because when new questions arise, the system has to be updated to classify them.
</nextsent>
<nextsent>most of the qc systems use english as the main language, and some of the best and standard resources are developed for english.it would be possible to build question classifier for every language based on machine learning, using good training corpus for each language, but is something expensive to produce.
</nextsent>
<nextsent>for this reason we have used machine translation sys tems.machine translation (mt) systems are very appreciated in clir (mcnamee et al, 2000).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2365">
<title id=" W06-1614.xml">is it really that difficult to parse german </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>parser performance for the models trained on tuba-d/z are comparable to parsing results for english with the stanford parser, when trained on the penntreebank.
</prevsent>
<prevsent>this comparison at least suggests that german is not harder to parse than its west-germanic neighbor language english.
</prevsent>
</prevsection>
<citsent citstr=" P05-1039 ">
there have been number of recent studies on probabilistic treebank parsing of german (dubey, 2005; <papid> P05-1039 </papid>dubey and keller, 2003; <papid> P03-1013 </papid>schiehlen, 2004;<papid> C04-1056 </papid>schulte im walde, 2003), using the negra tree bank (skut et al, 1997) <papid> A97-1014 </papid>as their underlying data source.</citsent>
<aftsection>
<nextsent>a common theme that has emerged from this research is the claim that lexicalization of pcfgs, which has been proven highly beneficial for other languages1 , is detrimental for parsing accuracy of german.
</nextsent>
<nextsent>in fact, this assumption is by now so widely held that schiehlen (2004) <papid> C04-1056 </papid>does not even consider lexicalization as possible 1for english, see collins (1999).parameter and concentrates instead only on treebank transformations of various sorts in his exper iments.another striking feature of all studies mentioned above are the relatively low parsing scores achieved for german by comparison to the scores reported for english, its west-germanic neighbor, using similar parsers.</nextsent>
<nextsent>this naturally raises the question whether german is just harder to parse or whether it is just hard to parse the negra treebank.2the purpose of this paper is to address precisely this question by training the stanford parser (klein and manning, 2003<papid> P03-1054 </papid>b) and the lopar parser (schmid, 2000) on the two major treebanks available for german, negra and tuba-d/z, the tubingen treebank of written german (telljohann et al, 2005).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2366">
<title id=" W06-1614.xml">is it really that difficult to parse german </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>parser performance for the models trained on tuba-d/z are comparable to parsing results for english with the stanford parser, when trained on the penntreebank.
</prevsent>
<prevsent>this comparison at least suggests that german is not harder to parse than its west-germanic neighbor language english.
</prevsent>
</prevsection>
<citsent citstr=" P03-1013 ">
there have been number of recent studies on probabilistic treebank parsing of german (dubey, 2005; <papid> P05-1039 </papid>dubey and keller, 2003; <papid> P03-1013 </papid>schiehlen, 2004;<papid> C04-1056 </papid>schulte im walde, 2003), using the negra tree bank (skut et al, 1997) <papid> A97-1014 </papid>as their underlying data source.</citsent>
<aftsection>
<nextsent>a common theme that has emerged from this research is the claim that lexicalization of pcfgs, which has been proven highly beneficial for other languages1 , is detrimental for parsing accuracy of german.
</nextsent>
<nextsent>in fact, this assumption is by now so widely held that schiehlen (2004) <papid> C04-1056 </papid>does not even consider lexicalization as possible 1for english, see collins (1999).parameter and concentrates instead only on treebank transformations of various sorts in his exper iments.another striking feature of all studies mentioned above are the relatively low parsing scores achieved for german by comparison to the scores reported for english, its west-germanic neighbor, using similar parsers.</nextsent>
<nextsent>this naturally raises the question whether german is just harder to parse or whether it is just hard to parse the negra treebank.2the purpose of this paper is to address precisely this question by training the stanford parser (klein and manning, 2003<papid> P03-1054 </papid>b) and the lopar parser (schmid, 2000) on the two major treebanks available for german, negra and tuba-d/z, the tubingen treebank of written german (telljohann et al, 2005).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2367">
<title id=" W06-1614.xml">is it really that difficult to parse german </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>parser performance for the models trained on tuba-d/z are comparable to parsing results for english with the stanford parser, when trained on the penntreebank.
</prevsent>
<prevsent>this comparison at least suggests that german is not harder to parse than its west-germanic neighbor language english.
</prevsent>
</prevsection>
<citsent citstr=" C04-1056 ">
there have been number of recent studies on probabilistic treebank parsing of german (dubey, 2005; <papid> P05-1039 </papid>dubey and keller, 2003; <papid> P03-1013 </papid>schiehlen, 2004;<papid> C04-1056 </papid>schulte im walde, 2003), using the negra tree bank (skut et al, 1997) <papid> A97-1014 </papid>as their underlying data source.</citsent>
<aftsection>
<nextsent>a common theme that has emerged from this research is the claim that lexicalization of pcfgs, which has been proven highly beneficial for other languages1 , is detrimental for parsing accuracy of german.
</nextsent>
<nextsent>in fact, this assumption is by now so widely held that schiehlen (2004) <papid> C04-1056 </papid>does not even consider lexicalization as possible 1for english, see collins (1999).parameter and concentrates instead only on treebank transformations of various sorts in his exper iments.another striking feature of all studies mentioned above are the relatively low parsing scores achieved for german by comparison to the scores reported for english, its west-germanic neighbor, using similar parsers.</nextsent>
<nextsent>this naturally raises the question whether german is just harder to parse or whether it is just hard to parse the negra treebank.2the purpose of this paper is to address precisely this question by training the stanford parser (klein and manning, 2003<papid> P03-1054 </papid>b) and the lopar parser (schmid, 2000) on the two major treebanks available for german, negra and tuba-d/z, the tubingen treebank of written german (telljohann et al, 2005).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2368">
<title id=" W06-1614.xml">is it really that difficult to parse german </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>parser performance for the models trained on tuba-d/z are comparable to parsing results for english with the stanford parser, when trained on the penntreebank.
</prevsent>
<prevsent>this comparison at least suggests that german is not harder to parse than its west-germanic neighbor language english.
</prevsent>
</prevsection>
<citsent citstr=" A97-1014 ">
there have been number of recent studies on probabilistic treebank parsing of german (dubey, 2005; <papid> P05-1039 </papid>dubey and keller, 2003; <papid> P03-1013 </papid>schiehlen, 2004;<papid> C04-1056 </papid>schulte im walde, 2003), using the negra tree bank (skut et al, 1997) <papid> A97-1014 </papid>as their underlying data source.</citsent>
<aftsection>
<nextsent>a common theme that has emerged from this research is the claim that lexicalization of pcfgs, which has been proven highly beneficial for other languages1 , is detrimental for parsing accuracy of german.
</nextsent>
<nextsent>in fact, this assumption is by now so widely held that schiehlen (2004) <papid> C04-1056 </papid>does not even consider lexicalization as possible 1for english, see collins (1999).parameter and concentrates instead only on treebank transformations of various sorts in his exper iments.another striking feature of all studies mentioned above are the relatively low parsing scores achieved for german by comparison to the scores reported for english, its west-germanic neighbor, using similar parsers.</nextsent>
<nextsent>this naturally raises the question whether german is just harder to parse or whether it is just hard to parse the negra treebank.2the purpose of this paper is to address precisely this question by training the stanford parser (klein and manning, 2003<papid> P03-1054 </papid>b) and the lopar parser (schmid, 2000) on the two major treebanks available for german, negra and tuba-d/z, the tubingen treebank of written german (telljohann et al, 2005).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2371">
<title id=" W06-1614.xml">is it really that difficult to parse german </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a common theme that has emerged from this research is the claim that lexicalization of pcfgs, which has been proven highly beneficial for other languages1 , is detrimental for parsing accuracy of german.
</prevsent>
<prevsent>in fact, this assumption is by now so widely held that schiehlen (2004) <papid> C04-1056 </papid>does not even consider lexicalization as possible 1for english, see collins (1999).parameter and concentrates instead only on treebank transformations of various sorts in his exper iments.another striking feature of all studies mentioned above are the relatively low parsing scores achieved for german by comparison to the scores reported for english, its west-germanic neighbor, using similar parsers.</prevsent>
</prevsection>
<citsent citstr=" P03-1054 ">
this naturally raises the question whether german is just harder to parse or whether it is just hard to parse the negra treebank.2the purpose of this paper is to address precisely this question by training the stanford parser (klein and manning, 2003<papid> P03-1054 </papid>b) and the lopar parser (schmid, 2000) on the two major treebanks available for german, negra and tuba-d/z, the tubingen treebank of written german (telljohann et al, 2005).</citsent>
<aftsection>
<nextsent>a series of comparative parsing experiments that utilize different parameter settings of the parsers is conducted, including lexicalization and markovization.
</nextsent>
<nextsent>these experiments show striking differences in performance between the two treebanks.
</nextsent>
<nextsent>what makes this comparison interesting is that the treebanks are of comparable size and are both based on newspaper corpus.
</nextsent>
<nextsent>however, both treebanks differ significantly in their syntactic annotation scheme.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2373">
<title id=" W06-1614.xml">is it really that difficult to parse german </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>note, however, that our experiments concentrate on the original (context-free) annotations of the treebank.the structure of this paper is as follows: section 2 discusses three characteristic grammatical features of german that need to be taken into account in syntactic annotation and in choosing an appropriate parsing model for german.
</prevsent>
<prevsent>section 3 introduces the negra and tuba-d/z treebanks and 2german is not the first language for which this question has been raised.
</prevsent>
</prevsection>
<citsent citstr=" P03-1056 ">
see levy and manning (2003) <papid> P03-1056 </papid>for similar discussion of chinese and the penn chinese treebank.</citsent>
<aftsection>
<nextsent>111discusses the main differences between their annotation schemes.
</nextsent>
<nextsent>section 4 explains the experimental setup, sections 5-7 the experiments, and section 8 discusses the results.
</nextsent>
<nextsent>there are three distinctive grammatical features that make syntactic annotation and parsing of german particularly challenging: its placement of the finite verb, its flexible phrasal ordering, and the presence of discontinuous constituents.
</nextsent>
<nextsent>these features will be discussed in the following subsections.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2386">
<title id=" W06-1614.xml">is it really that difficult to parse german </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>section 3).
</prevsent>
<prevsent>one possible explanation for the better performance of tuba-d/z might be that it has more information about the correct attachment site of extra posed constituents, which is completely lacking in the context-free version of negra.
</prevsent>
</prevsection>
<citsent citstr=" P06-3004 ">
for this reason, kubler (2005) and maier (2006) <papid> P06-3004 </papid>tested version of negra which contained information of the original attachment siteof these discontinuous constituents.</citsent>
<aftsection>
<nextsent>in this version of negra, the grammatical function oa in figure 2 would be changed to oa vp to show that it was originally attached to the vp.
</nextsent>
<nextsent>experiments with this version showed decrease in score from 52.30 to 49.75.
</nextsent>
<nextsent>consequently, adding this information in similar way to the encoding of discontinuous constituents in tuba-d/z harms performance.
</nextsent>
<nextsent>by contrast, tuba-d/z uses topological fields as the primary structuring principle, which leads to purely context-free annotation of discontinuousstructures.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2387">
<title id=" W06-1614.xml">is it really that difficult to parse german </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>consequently, adding this information in similar way to the encoding of discontinuous constituents in tuba-d/z harms performance.
</prevsent>
<prevsent>by contrast, tuba-d/z uses topological fields as the primary structuring principle, which leads to purely context-free annotation of discontinuousstructures.
</prevsent>
</prevsection>
<citsent citstr=" P03-1014 ">
there is evidence that the use of topological fields is advantageous also for other parsing approaches (frank et al, 2003; <papid> P03-1014 </papid>kubler, 2005; maier, 2006).<papid> P06-3004 </papid></citsent>
<aftsection>
<nextsent>another difference in the annotation schemes concerns the treatment of phrases.
</nextsent>
<nextsent>negra phrases are flat, and unary projections are not annotated.
</nextsent>
<nextsent>tuba-d/z always projects to the phrasal category and annotates more phrase-internal structure.
</nextsent>
<nextsent>the deeper structures in tuba-d/z lead to fewer rules for phrasal categories, which allows the parser amore consistent treatment of such phrases.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2392">
<title id=" W06-1318.xml">measuring annotator agreement in a complex hierarchical dialogue act annotation scheme </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>as it is often thought that more elaborate and fine-grained annotation schemes are difficult for annotators to apply consistently, we decided to address this issue in an annotation experiment on which we report in thispaper.
</prevsent>
<prevsent>a frequently used way of evaluating human dialogue act classification is inter-annotator agreement.
</prevsent>
</prevsection>
<citsent citstr=" J96-2004 ">
agreement is sometimes measured as percentage of the cases on which the annotators agree, but more often expected agreement is taken into account in using the kappa statistic (cohen, 1960; carletta, 1996), <papid> J96-2004 </papid>which is given by: ? = po ? pe1 ? pe (1) where po is the observed proportion of agreement and pe is the proportion of agreement expected bychance.</citsent>
<aftsection>
<nextsent>ever since its introduction in general (cohen, 1960) and in computational linguistics (car letta, 1996), <papid> J96-2004 </papid>many researchers have pointed out that there are quite some problems in using ?</nextsent>
<nextsent>(e.g.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2395">
<title id=" W06-1318.xml">measuring annotator agreement in a complex hierarchical dialogue act annotation scheme </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>ever since its introduction in general (cohen, 1960) and in computational linguistics (car letta, 1996), <papid> J96-2004 </papid>many researchers have pointed out that there are quite some problems in using ?</prevsent>
<prevsent>(e.g.</prevsent>
</prevsection>
<citsent citstr=" J04-1005 ">
(di eugenio and glass, 2004)), <papid> J04-1005 </papid>one of which is the discrepancy between p0 and ? for skewed class distribution.</citsent>
<aftsection>
<nextsent>another is that the degree of disagreement is not taken into account, which is relevant for any non-nominal scale.
</nextsent>
<nextsent>to address this problem, weighted ? has been proposed (cohen, 1968) that penalizes disagreement according to their degree rather than treating all disagreements equally.
</nextsent>
<nextsent>itwould be arguable that in similar way, characteristics of dialogue acts in particular taxonomy and possible pragmatic similarity between them should be taken into account to express annotator agreement.
</nextsent>
<nextsent>for dialogue act taxonomies which are structured in meaningful way, such as those that 126 express hierarchical relations between concepts inthe taxonomy, the taxonomic structure can be exploited to express how much annotators disagree when they choose different concepts that are directly or indirectly related.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2398">
<title id=" W06-1318.xml">measuring annotator agreement in a complex hierarchical dialogue act annotation scheme </title>
<section> agreement using ?.  </section>
<citcontext>
<prevsection>
<prevsent>another more recent analysis was performed for 8 dialogues of the monroe corpus (stent, 2000), counting 2897 utterances in total, processed by two annotators for 13 damsl dimensions.
</prevsent>
<prevsent>other analyses apply damsl derived schemes (such as switchboard-damsl) to various corpora (e.g.
</prevsent>
</prevsection>
<citsent citstr=" P98-1052 ">
(di eugenio et al, 1998; <papid> P98-1052 </papid>shriberg et al, 2004) ).<papid> W04-2319 </papid></citsent>
<aftsection>
<nextsent>for the comprehensivedit++ taxonomy, the work reported here represents the first investigation of annotator agreement.
</nextsent>
<nextsent>3.2 experiment outline.
</nextsent>
<nextsent>as noted, existing work on annotator agreement analysis has mostly involved only two annotators.
</nextsent>
<nextsent>it may be argued that especially for annotation of concepts that are rather complex, an odd number of annotators is desirable.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2399">
<title id=" W06-1318.xml">measuring annotator agreement in a complex hierarchical dialogue act annotation scheme </title>
<section> agreement using ?.  </section>
<citcontext>
<prevsection>
<prevsent>another more recent analysis was performed for 8 dialogues of the monroe corpus (stent, 2000), counting 2897 utterances in total, processed by two annotators for 13 damsl dimensions.
</prevsent>
<prevsent>other analyses apply damsl derived schemes (such as switchboard-damsl) to various corpora (e.g.
</prevsent>
</prevsection>
<citsent citstr=" W04-2319 ">
(di eugenio et al, 1998; <papid> P98-1052 </papid>shriberg et al, 2004) ).<papid> W04-2319 </papid></citsent>
<aftsection>
<nextsent>for the comprehensivedit++ taxonomy, the work reported here represents the first investigation of annotator agreement.
</nextsent>
<nextsent>3.2 experiment outline.
</nextsent>
<nextsent>as noted, existing work on annotator agreement analysis has mostly involved only two annotators.
</nextsent>
<nextsent>it may be argued that especially for annotation of concepts that are rather complex, an odd number of annotators is desirable.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2400">
<title id=" W06-2710.xml">the nite xml toolkit demonstration from five corpora </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>academic software is often inadequately documented, and therefore only usable with the help of its developers.
</prevsent>
<prevsent>it is inevitable given the size of the target user community that most of them are at least friends of friends?.
</prevsent>
</prevsection>
<citsent citstr=" W06-2704 ">
enough users have worked independently of the developers that we are confident that all but the newest parts of nxt are understandable from the documentation.although nxt is mature enough for use, several projects are investing in further development.the largest current efforts are to create an annotation and analysis tool with time-aligned tiered data display and query processor with better performance (mayo et al, 2006).<papid> W06-2704 </papid></citsent>
<aftsection>
<nextsent>another priority is better packaging, particularly of the configurable interfaces and of the existing translations from the formats used in transcription and other annotation tools.
</nextsent>
<nextsent>finally, contributing projects plan work that will improve interoperability between nxt and other tools, including eye trackers, nlp applications such as part-of-speech taggers, and machine learning software.
</nextsent>
<nextsent>acknowledgmentsnxt prototyping and development has been supported primarily by the european commission (mate, le4-8370; nite, ist-2000-2609; and ami, fp6-506811) and scottish enterprise via the edinburgh-stanford link.
</nextsent>
<nextsent>the examples shown are by kind permission of scottish enterprise, the ami project, and dr. matthew anstey of charles sturt university.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2401">
<title id=" W06-2002.xml">a framework for incorporating alignment information in parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this work, we discuss preliminary work in developing new probabilistic parsing model that allows us to easily incorporate many different typesof features, including cross lingual information.
</prevsent>
<prevsent>we show how this model can be used to build successful parser for small handmade gold-standard corpus of 188 sentences (in 3 languages) from the europarl corpus.
</prevsent>
</prevsection>
<citsent citstr=" A00-2018 ">
much of the current research into probabilistic parsing is founded on probabilistic context free grammars (pcfgs) (collins, 1999; charniak, 2000; <papid> A00-2018 </papid>charniak, 2001).<papid> P01-1017 </papid></citsent>
<aftsection>
<nextsent>for instance, consider the parse tree in figure 1.
</nextsent>
<nextsent>one way to decompose this parse tree is to view it as sequence of applications of cfg rules.
</nextsent>
<nextsent>for this particular tree, we could view it as the application of rule np?
</nextsent>
<nextsent>np pp,?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2402">
<title id=" W06-2002.xml">a framework for incorporating alignment information in parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this work, we discuss preliminary work in developing new probabilistic parsing model that allows us to easily incorporate many different typesof features, including cross lingual information.
</prevsent>
<prevsent>we show how this model can be used to build successful parser for small handmade gold-standard corpus of 188 sentences (in 3 languages) from the europarl corpus.
</prevsent>
</prevsection>
<citsent citstr=" P01-1017 ">
much of the current research into probabilistic parsing is founded on probabilistic context free grammars (pcfgs) (collins, 1999; charniak, 2000; <papid> A00-2018 </papid>charniak, 2001).<papid> P01-1017 </papid></citsent>
<aftsection>
<nextsent>for instance, consider the parse tree in figure 1.
</nextsent>
<nextsent>one way to decompose this parse tree is to view it as sequence of applications of cfg rules.
</nextsent>
<nextsent>for this particular tree, we could view it as the application of rule np?
</nextsent>
<nextsent>np pp,?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2403">
<title id=" W06-2002.xml">a framework for incorporating alignment information in parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>then using these learned probabilities, we can find the most likely parse of given sentence using the aforementioned cubic algorithms.the problem, of course, with this simplification is that although it is computationally attractive, it is usually too strong of an independenceassumption.
</prevsent>
<prevsent>to mitigate this loss of context, with out sacrificing algorithmic tract ability, typically researchers annotate the nodes of the parse tree with contextual information.
</prevsent>
</prevsection>
<citsent citstr=" J98-4004 ">
for instance, it has been found to be useful to annotate nodes with their parent labels (johnson, 1998), <papid> J98-4004 </papid>as shown in figure 2.</citsent>
<aftsection>
<nextsent>in this case, we would be learning probabilities like: p(pp-np?
</nextsent>
<nextsent>in-pp np-pp).
</nextsent>
<nextsent>the choice of which annotations to use is one of the main features that distinguish parsers based on this approach.
</nextsent>
<nextsent>generally, this approach has proven quite effective in producing english phrase-structure grammar parsers that perform well on the penn treebank.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2404">
<title id=" W06-2002.xml">a framework for incorporating alignment information in parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the closest relative 1 2 3 4 5 1 true true false false true 2 - true false false false 3 - - true false true 4 - - - true true 5 - - - - true figure 3: span chart for example parse tree.
</prevsent>
<prevsent>chart entry (i, j) = true iff span (i, j) is constituent in the tree.
</prevsent>
</prevsection>
<citsent citstr=" W97-0301 ">
of our framework is the maximum-entropy parser of ratnaparkhi(ratnaparkhi, 1997).<papid> W97-0301 </papid></citsent>
<aftsection>
<nextsent>both frameworks are bottom-up, but while ratnaparkhis views parse trees as the sequence of applications of four different types of tree construction rules, our framework strives to be somewhat simpler and more general.
</nextsent>
<nextsent>the example parse tree in figure 1 can also be decomposed in the following manner.
</nextsent>
<nextsent>first, we can represent the unlabeled tree with boolean-valuedchart (which we will call the span chart) that assigns the value of true to span if it is constituent in the tree, and false otherwise.
</nextsent>
<nextsent>the span chart for figure 1 is shown in figure 3.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2405">
<title id=" W06-2002.xml">a framework for incorporating alignment information in parsing </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>the test data were 50 sentences for each language, picked arbitrarily with the same length restrictions.
</prevsent>
<prevsent>the training and test data were manually aligned following the guidelines.1for the word alignments used as learning features, we used giza++, relying on the default parameters.
</prevsent>
</prevsection>
<citsent citstr=" J04-4004 ">
we trained the alignments on the fulleuroparl corpus for both directions of each language pair.as baseline system we trained bikels reim plementation (bikel, 2004) <papid> J04-4004 </papid>of collins?</citsent>
<aftsection>
<nextsent>parser(collins, 1999) on the gold standard (en 1a subset of 39 sentences was annotated by two people independently, leading to an f-score in bracketing agreement between 84 and 90 for the three languages.
</nextsent>
<nextsent>since finding an annotation scheme that works well in the bootstrapping set up is an issue on our research agenda, we postpone more detailed analysis of the annotation process until it becomes clear that particular scheme is indeed useful.
</nextsent>
<nextsent>glish) training data, applying simple additional smoothing procedure for the modifier events in order to counteract some obvious data sparseness is sues.2 since we were attempting to learn unlabeled trees, in this experiment we only needed to learn the probabilistic model of section 3 with no labeling schemes.
</nextsent>
<nextsent>hence we need only to learn the probability distribution: ps(sij|fs)in other words, we need to learn the probability that given span is tree constituent, given some set of features of the words and pre terminal tags of the sentences, as well as the previous span decisions we have made.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2406">
<title id=" W06-1625.xml">humor prosody analysis and automatic recognition for friends </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>humor recognition was carried out using standard supervised learning classifiers, and shows promising results significantly above the baseline.
</prevsent>
<prevsent>as conversational systems are becoming prevalent in our lives, we notice an increasing need for adding social intelligence in computers.
</prevsent>
</prevsection>
<citsent citstr=" H05-1073 ">
there hasbeen considerable amount of research on incorporating affect (litman and forbes-riley, 2004) (alm et al, 2005) (<papid> H05-1073 </papid>dmello et al, 2005) (shroderand cowie, 2005) (klein et al, 2002) and personality (gebhard et al, 2004) in computer interfaces,so that, for instance, user frustrations can be recognized and addressed in graceful manner.</citsent>
<aftsection>
<nextsent>as (binsted, 1995) correctly pointed out, one way to alleviate user frustrations, and to make human computer interaction more natural, personal and interesting for the users, is to model humor.
</nextsent>
<nextsent>research in computational humor is still invery early stages, partially because humorous language often uses complex, ambiguous and incongruous syntactic and semantic expressions (attardo, 1994) (mulder and nijholt, 2002) which require deep semantic interpretation.
</nextsent>
<nextsent>nonetheless,recent studies have shown feasibility of automatically recognizing (mihalcea and strapparava, 2005) <papid> H05-1067 </papid>taylor and mazlack, 2004) and generating(binsted and ritchie, 1997) (stock and strapparava, 2005) <papid> P05-3029 </papid>humor in computer systems.</nextsent>
<nextsent>the stateof the art research in computational humor (bin sted et al, 2006) is, however, limited to text (such as humorous one-liners, acronyms or wordplays), and to our knowledge, there has been no work to date on automatic humor recognition in spoken conversations.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2407">
<title id=" W06-1625.xml">humor prosody analysis and automatic recognition for friends </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>as (binsted, 1995) correctly pointed out, one way to alleviate user frustrations, and to make human computer interaction more natural, personal and interesting for the users, is to model humor.
</prevsent>
<prevsent>research in computational humor is still invery early stages, partially because humorous language often uses complex, ambiguous and incongruous syntactic and semantic expressions (attardo, 1994) (mulder and nijholt, 2002) which require deep semantic interpretation.
</prevsent>
</prevsection>
<citsent citstr=" H05-1067 ">
nonetheless,recent studies have shown feasibility of automatically recognizing (mihalcea and strapparava, 2005) <papid> H05-1067 </papid>taylor and mazlack, 2004) and generating(binsted and ritchie, 1997) (stock and strapparava, 2005) <papid> P05-3029 </papid>humor in computer systems.</citsent>
<aftsection>
<nextsent>the stateof the art research in computational humor (bin sted et al, 2006) is, however, limited to text (such as humorous one-liners, acronyms or wordplays), and to our knowledge, there has been no work to date on automatic humor recognition in spoken conversations.
</nextsent>
<nextsent>before we can model humor in real application systems, we must first analyze features that characterize humor.
</nextsent>
<nextsent>computational approaches to humor recognition so far primarily relyon lexical and stylistic cues such as alliteration, antonyms, adult slang (mihalcea and strapparava, 2005).<papid> H05-1067 </papid></nextsent>
<nextsent>the focus of our study is, on the other hand, on analyzing acoustic-prosodic cues (such as pitch, intensity, tempo etc.) in humorous conversation sand testing if these cues can help us to automatically distinguish between humorous and non humorous (normal) utterances in speech.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2409">
<title id=" W06-1625.xml">humor prosody analysis and automatic recognition for friends </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>as (binsted, 1995) correctly pointed out, one way to alleviate user frustrations, and to make human computer interaction more natural, personal and interesting for the users, is to model humor.
</prevsent>
<prevsent>research in computational humor is still invery early stages, partially because humorous language often uses complex, ambiguous and incongruous syntactic and semantic expressions (attardo, 1994) (mulder and nijholt, 2002) which require deep semantic interpretation.
</prevsent>
</prevsection>
<citsent citstr=" P05-3029 ">
nonetheless,recent studies have shown feasibility of automatically recognizing (mihalcea and strapparava, 2005) <papid> H05-1067 </papid>taylor and mazlack, 2004) and generating(binsted and ritchie, 1997) (stock and strapparava, 2005) <papid> P05-3029 </papid>humor in computer systems.</citsent>
<aftsection>
<nextsent>the stateof the art research in computational humor (bin sted et al, 2006) is, however, limited to text (such as humorous one-liners, acronyms or wordplays), and to our knowledge, there has been no work to date on automatic humor recognition in spoken conversations.
</nextsent>
<nextsent>before we can model humor in real application systems, we must first analyze features that characterize humor.
</nextsent>
<nextsent>computational approaches to humor recognition so far primarily relyon lexical and stylistic cues such as alliteration, antonyms, adult slang (mihalcea and strapparava, 2005).<papid> H05-1067 </papid></nextsent>
<nextsent>the focus of our study is, on the other hand, on analyzing acoustic-prosodic cues (such as pitch, intensity, tempo etc.) in humorous conversation sand testing if these cues can help us to automatically distinguish between humorous and non humorous (normal) utterances in speech.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2415">
<title id=" W06-2925.xml">projective dependency parsing with perceptron </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in experiments, the treatment of multilingual information was totally blind.
</prevsent>
<prevsent>we describe learning system for the conll-x shared task on multilingual dependency parsing (buchholz et al, 2006), for 13 different languages.
</prevsent>
</prevsection>
<citsent citstr=" C96-1058 ">
our system is bottom-up projective dependency parser, based on the cubic-time algorithm by eisner (1996), <papid> C96-1058 </papid>algorithm by eisner (2000).</citsent>
<aftsection>
<nextsent>the parser uses learning function that scores all possible labeled dependencies.
</nextsent>
<nextsent>this function is trained globally with online perceptron,by parsing training sentences and correcting its parameters based on the parsing mistakes.
</nextsent>
<nextsent>the features used to score, while based on the previous work in dependency parsing (mcdonald et al, 2005), <papid> P05-1012 </papid>introduce some novel concepts such as better codification of context and surface distances, and runtime information from dependencies previously parsed.regarding experimentation, the treatment of multilingual data has been totally blind, with no special processing or features that depend on the lan guage.</nextsent>
<nextsent>considering its simplicity, our system achieves moderate but encouraging results, with an overall labeled attachment accuracy of 74.72% on the conll-x test set.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2417">
<title id=" W06-2925.xml">projective dependency parsing with perceptron </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the parser uses learning function that scores all possible labeled dependencies.
</prevsent>
<prevsent>this function is trained globally with online perceptron,by parsing training sentences and correcting its parameters based on the parsing mistakes.
</prevsent>
</prevsection>
<citsent citstr=" P05-1012 ">
the features used to score, while based on the previous work in dependency parsing (mcdonald et al, 2005), <papid> P05-1012 </papid>introduce some novel concepts such as better codification of context and surface distances, and runtime information from dependencies previously parsed.regarding experimentation, the treatment of multilingual data has been totally blind, with no special processing or features that depend on the lan guage.</citsent>
<aftsection>
<nextsent>considering its simplicity, our system achieves moderate but encouraging results, with an overall labeled attachment accuracy of 74.72% on the conll-x test set.
</nextsent>
<nextsent>this section describes the three main components ofthe dependency parsing: the parsing model, the parsing algorithm, and the learning algorithm.
</nextsent>
<nextsent>2.1 model.
</nextsent>
<nextsent>let 1, . . .
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2420">
<title id=" W06-2925.xml">projective dependency parsing with perceptron </title>
<section> parsing and learning algorithms.  </section>
<citcontext>
<prevsection>
<prevsent>thus, by conditioning on features extracted from we are making the search approximative.
</prevsent>
<prevsent>2.3 perceptron learning.
</prevsent>
</prevsection>
<citsent citstr=" W02-1001 ">
as learning algorithm, we use perceptron tailored for structured scenarios, proposed by collins (2002).<papid> W02-1001 </papid>in recent years, perceptron has been used in number of natural language learning works, such as in = 0 for = 1 to foreach training example (x, y) do y?</citsent>
<aftsection>
<nextsent>= dp(x,w) foreach [h,m, l] ? y\y? do wl = wl + ?(h,m, x, y?)
</nextsent>
<nextsent>foreach [h,m, l] ? y?\y do wl = wl ? ?(h,m, x, y?)
</nextsent>
<nextsent>return figure 1: pseudo code of the perceptron algorithm.
</nextsent>
<nextsent>t is parameter that indicates the number of epochs that the algorithm cycles the training set.partial parsing (carreras et al, 2005) or even dependency parsing (mcdonald et al, 2005).<papid> P05-1012 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2423">
<title id=" W06-3406.xml">improving email speech acts analysis via ngram selection </title>
<section> abstract </section>
<citcontext>
<prevsection>
<prevsent>or commit to task?.we demonstrate that exploiting the contextual information in the messages can noticeably improve email-act classification.
</prevsent>
<prevsent>more specifically, we describe combination of n-gram sequence features with careful message preprocessing that is highly effective for this task.
</prevsent>
</prevsection>
<citsent citstr=" W04-3240 ">
compared to previous study (cohen et al, 2004),<papid> W04-3240 </papid>this representation reduces the classification error rates by 26.4% on average.</citsent>
<aftsection>
<nextsent>finally, we introduce ciranda: new open source toolkit for email speech act prediction.
</nextsent>
<nextsent>one important use of work-related email is negotiating and delegating shared tasks and subtasks.
</nextsent>
<nextsent>to provide intelligent email automated assistance, it is desirable to be able to automatically detect the intent of an email message for example, to determine if the email contains request, commitment by the sender to perform some task, or an amendment to an earlier proposal.
</nextsent>
<nextsent>successfully adding such semantic layer to email communication is still challenge to current email clients.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2438">
<title id=" W06-3406.xml">improving email speech acts analysis via ngram selection </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>it was also shown that there is an acceptable level of human agreement over the categories.
</prevsent>
<prevsent>in experiments using different human annotators, kappa values between 0.72 and 0.85 were obtained.
</prevsent>
</prevsection>
<citsent citstr=" J96-2004 ">
the kappa statistic (carletta,1996) <papid> J96-2004 </papid>is typically used to measure the human interrater agreement.</citsent>
<aftsection>
<nextsent>its values ranges from -1 (com plete disagreement) to +1 (perfect agreement) andit is defined as (a-r)/(1-r), where is the empirical probability of agreement on category, and is the probability of agreement for two annotators that 36label documents at random (with the empirically observed frequency of each label).
</nextsent>
<nextsent>3 the corpus.
</nextsent>
<nextsent>the cspace email corpus used in this paper contains approximately 15,000 email messages collected from management course at carnegie mellon university.
</nextsent>
<nextsent>this corpus originated from working groups who signed agreements to make certain parts of their email accessible to researchers.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2448">
<title id=" W06-2106.xml">coverage and inheritance in the preposition project </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>next, we examine issues of coverage in relation to the range of preposition meaning contained in quirk et al (1985), alongside the ranges in other resources such as the penn treebank, framenet, and lexical conceptual structures.
</prevsent>
<prevsent>this analysis also considers accounts of semantic relations that have been presented in literature that has used these other resources.
</prevsent>
</prevsection>
<citsent citstr=" W02-0802 ">
next, we critically examine claims of the inheritance of preposition meaning as described in litkowski (2002), <papid> W02-0802 </papid>including consideration of inheritance mechanisms in framenet.</citsent>
<aftsection>
<nextsent>this analysis suggests some mechanisms for data-driven or corpus-based approach to the identification of semantic relation inventory.
</nextsent>
<nextsent>finally, based on these analyses of coverage and inheritance, we identify some next steps tpp needs to take.
</nextsent>
<nextsent>the primary objective of tpp is to characterize each of 847 preposition senses for 373 prepositions (including 220 phrasal prepositions with 309 senses) with semantic role name and the syntactic and semantic properties of its complement and attachment point.
</nextsent>
<nextsent>the preposition sense inventory is taken from the oxford dictionary of english 1 http://www.clres.com/prepositions.html.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2454">
<title id=" W06-2106.xml">coverage and inheritance in the preposition project </title>
<section> semantic coverage of tpp.  </section>
<citcontext>
<prevsection>
<prevsent>space to assess the coverage, the first question is what inventory should be used.
</prevsent>
<prevsent>the linguistics and computational linguistics literatures are replete with introspective lists of semantic roles.
</prevsent>
</prevsection>
<citsent citstr=" J02-3001 ">
gildea &amp; jurafsky (2002) <papid> J02-3001 </papid>present list of 18 that may be viewed as reasonably well-accepted.</citsent>
<aftsection>
<nextsent>ohara (2005) provides several compilations based on penn treebank annotations, framenet, opencyc, and factotum.
</nextsent>
<nextsent>boonthum et al (2006) includes an assessment of semantic roles in jackendoff, dorrs lexical conceptual structures preposition database, and barkers analysis of preposition meaning; she posits list of 7 overarching semantic roles (although specifically intended for use in paraphrase analysis).
</nextsent>
<nextsent>without going into detailed analysis of each of these lists, all of which are relatively small in number, the semantic relations included in tpp clearly cover each of the lists.
</nextsent>
<nextsent>however, since the semantic relations in these lists are relatively coarse-grained, this assessment is not sufficient.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2477">
<title id=" W07-0601.xml">a linguistic investigation into unsupervised dop </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>dop produces and analyzes new sentences out of largest and most probable subtrees from previously analyzed sentences.
</prevsent>
<prevsent>dop maximizes what has been called the structural analogy?
</prevsent>
</prevsection>
<citsent citstr=" P06-1109 ">
between sentence and corpus of previous sentence-structures (bod 2006<papid> P06-1109 </papid>b).</citsent>
<aftsection>
<nextsent>while dop has been successful in some areas, e.g. in ambiguity resolution, there is also serious shortcoming to the approach: it does not account for the acquisition of initial structures.
</nextsent>
<nextsent>that is, dop assumes that the structures of previous linguistic experiences are already given and stored in corpus.
</nextsent>
<nextsent>as such, dop can at best account for adult language use and has nothing to say about language acquisition.
</nextsent>
<nextsent>in bod (2005), in bod (2006<papid> P06-1109 </papid>a), dop was extended to unsupervised parsing in rather straightforward way.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2482">
<title id=" W07-0601.xml">a linguistic investigation into unsupervised dop </title>
<section> learning movement? by u-dop.  </section>
<citcontext>
<prevsection>
<prevsent>it can also be shown that u-dop still learns the correct agreement if sentences with incorrect agreement, like *swimming in rivers are dangerous, are heard as long as the correct agreement has higher frequency than the incorrect agreement during the learning process.
</prevsent>
<prevsent>we now come to what is often assumed to be the greatest challenge for models of language learning, and what crain (1991) calls the parade case of an innate constraint?: the problem of auxiliary movement, also known as auxiliary fronting or inversion.
</prevsent>
</prevsection>
<citsent citstr=" W06-2917 ">
lets start with the typical examples, which are similar to those used in crain (1991), macwhinney (2005), clark and eyraud (2006) <papid> W06-2917 </papid>and many others: (8) the man is hungry if we turn sentence (8) into (polar) interrogative, the auxiliary is is fronted, resulting in sentence (9).</citsent>
<aftsection>
<nextsent>(9) is the man hungry?
</nextsent>
<nextsent>a language learner might derive from these two sentences that the first occurring auxiliary is fronted.
</nextsent>
<nextsent>however, when the sentence also contains relative clause with an auxiliary is, it should not be the first occurrence of is that is fronted but the one in the main clause, as shown in sentences (11) and (12).
</nextsent>
<nextsent>(11) the man who is eating is hungry (12) is the man who is eating hungry?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2486">
<title id=" W07-0601.xml">a linguistic investigation into unsupervised dop </title>
<section> learning movement? by u-dop.  </section>
<citcontext>
<prevsection>
<prevsent>however, different from clark and eyraud, we have shown that u-dop can also learn complex, dis contiguous constructions.
</prevsent>
<prevsent>in order to learn both rule-based phenomena like auxiliary inversion and exemplar-based phenomena like idiosyncratic constructions, we believe we need 1 we are implicitly assuming here an extension of dop which computes the most probable shortest derivation given certain meaning to be conveyed.
</prevsent>
</prevsection>
<citsent citstr=" P97-1021 ">
this semantic dop model was worked out in bonnema et al (1997) <papid> P97-1021 </papid>where the meaning of sentence was represented by its logical form.</citsent>
<aftsection>
<nextsent>7 the richness of probabilistic tree grammar rather than probabilistic context-free grammar.
</nextsent>
<nextsent>we have shown that various syntactic phenomena can be learned by model that only assumes (1) the notion of recursive tree structure, and (2) an ana logical matching algorithm which reconstructs new sentence out of largest and most frequent fragments from previous sentences.
</nextsent>
<nextsent>the major difference between our model and other computational learning models (such as klein and manning 2005 or clark and eyraud 2006) <papid> W06-2917 </papid>is that we start with trees.</nextsent>
<nextsent>but since we do not know which trees are correct, we initially allow for all of them and let analogy decide.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2488">
<title id=" W06-1705.xml">annotated web as corpus </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the motivation for increasingly large datasets remains the same.
</prevsent>
<prevsent>due to the zipfian nature of word frequencies, around half the word types in corpus occur only once, so tremendous increases in corpus size are required both to ensure inclusion of essential word and phrase types and to increase the chances of multiple occurrences of given type.
</prevsent>
</prevsection>
<citsent citstr=" J03-3001 ">
in corpus linguistics building such mega corpora is beyond the scope of individual researchers, and they are not easily accessible (kennedy, 1998: 56) unless the web is used as corpus (kilgarriff and grefenstette, 2003).<papid> J03-3001 </papid></citsent>
<aftsection>
<nextsent>increasingly, corpus researchers are tapping the web to overcome the sparse data problem (keller et al, 2002).<papid> W02-1030 </papid></nextsent>
<nextsent>this topic generated intense interest at workshops held at the university of heidelberg (october 2004), university of bologna (january 2005), university of birmingham (july 2005) and now in trento in april 2006.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2490">
<title id=" W06-1705.xml">annotated web as corpus </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>due to the zipfian nature of word frequencies, around half the word types in corpus occur only once, so tremendous increases in corpus size are required both to ensure inclusion of essential word and phrase types and to increase the chances of multiple occurrences of given type.
</prevsent>
<prevsent>in corpus linguistics building such mega corpora is beyond the scope of individual researchers, and they are not easily accessible (kennedy, 1998: 56) unless the web is used as corpus (kilgarriff and grefenstette, 2003).<papid> J03-3001 </papid></prevsent>
</prevsection>
<citsent citstr=" W02-1030 ">
increasingly, corpus researchers are tapping the web to overcome the sparse data problem (keller et al, 2002).<papid> W02-1030 </papid></citsent>
<aftsection>
<nextsent>this topic generated intense interest at workshops held at the university of heidelberg (october 2004), university of bologna (january 2005), university of birmingham (july 2005) and now in trento in april 2006.
</nextsent>
<nextsent>in addition, the advantages of using linguistically annotated data over raw data are well documented (mair, 2005; granger and rayson, 1998).
</nextsent>
<nextsent>as the size of corpus increases, near linear increase in computing power is required to annotate the text.
</nextsent>
<nextsent>although processing power is steadily growing, it has already become impractical for single computer to annotate mega-corpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2493">
<title id=" W06-1705.xml">annotated web as corpus </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>more recently, the sketch engine was used to develop the new edition of the oxford thesaurus of english (2004, edited by maurice waite).
</prevsent>
<prevsent>parallel ising or distributing processing has been suggested before.
</prevsent>
</prevsection>
<citsent citstr=" P04-1014 ">
clark and currans (2004) <papid> P04-1014 </papid>work is in parallel ising an implementation of log-linear parsing on the wall street journal corpus, whereas we focus on part-of-speech tagging of far larger and more varied web corpus, technique more widely considered prerequisite for corpus linguistics research.</citsent>
<aftsection>
<nextsent>curran (2003) <papid> W03-0806 </papid>9 http://pie.usna.edu/ suggested distributed processing in terms of web services but only to allow components developed by different researchers in different locations to be composed to build larger systems?</nextsent>
<nextsent>and not for parallel processing.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2494">
<title id=" W06-1705.xml">annotated web as corpus </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>parallel ising or distributing processing has been suggested before.
</prevsent>
<prevsent>clark and currans (2004) <papid> P04-1014 </papid>work is in parallel ising an implementation of log-linear parsing on the wall street journal corpus, whereas we focus on part-of-speech tagging of far larger and more varied web corpus, technique more widely considered prerequisite for corpus linguistics research.</prevsent>
</prevsection>
<citsent citstr=" W03-0806 ">
curran (2003) <papid> W03-0806 </papid>9 http://pie.usna.edu/ suggested distributed processing in terms of web services but only to allow components developed by different researchers in different locations to be composed to build larger systems?</citsent>
<aftsection>
<nextsent>and not for parallel processing.
</nextsent>
<nextsent>most significantly, previous investigations have not examined three essential questions: how to apply distributed techniques to vast quantities of corpus data derived from the web, how to ensure that web-derived corpora are representative, and how to provide verifiability and replicability.
</nextsent>
<nextsent>these core foci of our work represent crucial innovations lacking in prior research.
</nextsent>
<nextsent>in particular, representativeness and replicability are key research concerns to enhance the reliability of web data for corpora.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2495">
<title id=" W06-3110.xml">ngram posterior probabilities for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the absolute improvements of the bleu score is between 1.1% and 1.6%.
</prevsent>
<prevsent>the use of word posterior probabilities is common approach for confidence estimation in automatic speech recognition, e.g. see (wessel, 2002).
</prevsent>
</prevsection>
<citsent citstr=" C04-1046 ">
this idea has been adopted to estimate confidences for machine translation, e.g. see (blatz et al, 2003;ueffing et al, 2003; blatz et al, 2004).<papid> C04-1046 </papid></citsent>
<aftsection>
<nextsent>these confidence measures were used in the computer assisted translation (cat) framework, e.g.
</nextsent>
<nextsent>(gandrabur and foster, 2003).<papid> W03-0413 </papid></nextsent>
<nextsent>the (simplified) idea is that the confidence measure is used to decide if the machine generated prediction should be suggested to the human translator or not.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2496">
<title id=" W06-3110.xml">ngram posterior probabilities for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this idea has been adopted to estimate confidences for machine translation, e.g. see (blatz et al, 2003;ueffing et al, 2003; blatz et al, 2004).<papid> C04-1046 </papid></prevsent>
<prevsent>these confidence measures were used in the computer assisted translation (cat) framework, e.g.</prevsent>
</prevsection>
<citsent citstr=" W03-0413 ">
(gandrabur and foster, 2003).<papid> W03-0413 </papid></citsent>
<aftsection>
<nextsent>the (simplified) idea is that the confidence measure is used to decide if the machine generated prediction should be suggested to the human translator or not.
</nextsent>
<nextsent>there is only few work on how to improve machine translation performance using confidence measures.
</nextsent>
<nextsent>the only work, we are aware of, is(blatz et al, 2003).
</nextsent>
<nextsent>the outcome was that the confidence measures did not result in improvements of the translation quality measured with the bleu and nist scores.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2497">
<title id=" W06-3110.xml">ngram posterior probabilities for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>so far, always word-level posterior probabilities were used.
</prevsent>
<prevsent>here, we will generalize this idea to grams.in addition to the n-gram posterior probabilities, we introduce sentence-length model basedon posterior probabilities.
</prevsent>
</prevsection>
<citsent citstr=" W99-0604 ">
the common phrase based translation systems, such as (och et al, 1999; <papid> W99-0604 </papid>koehn, 2004), do not use an explicit sentence length model.</citsent>
<aftsection>
<nextsent>only the simple word penalty goes into that direction.
</nextsent>
<nextsent>it can be adjusted to prefer longer or shorter translations.
</nextsent>
<nextsent>here, we will explicitly model the sentence length.the novel contributions of this work are to introduce n-gram posterior probabilities and sentence length posterior probabilities.
</nextsent>
<nextsent>using these methods, we achieve significant improvements of translation quality.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2499">
<title id=" W06-3110.xml">ngram posterior probabilities for statistical machine translation </title>
<section> baseline system.  </section>
<citcontext>
<prevsection>
<prevsent>fj ,which is to be translated into target language sentence ei1 = e1 . . .
</prevsent>
<prevsent>ei . . .
</prevsent>
</prevsection>
<citsent citstr=" P02-1038 ">
ei . among all possible target language sentences, we will choose the sentence with the highest probability: ei1 = argmax i,ei1 { pr(ei1|fj1 ) } (1)the posterior probability pr(ei1|fj1 ) is modeled directly using log-linear combination of several models (och and ney, 2002): <papid> P02-1038 </papid>pr(ei1|fj1 ) = exp ( m=1 mhm(ei1, fj1 ) ) ? i?,ei1 exp ( m=1 mhm(ei ? 1 , fj1 ) ) (2)the denominator is normalization factor that depends only on the source sentence fj1 . therefore,we can omit it during the search process.</citsent>
<aftsection>
<nextsent>as decision rule, we obtain: ei1 = argmax i,ei1 { ? m=1 mhm(ei1, fj1 ) } (3)this approach is generalization of the source channel approach (brown et al, 1990).<papid> J90-2002 </papid></nextsent>
<nextsent>it has the advantage that additional models h(?)</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2500">
<title id=" W06-3110.xml">ngram posterior probabilities for statistical machine translation </title>
<section> baseline system.  </section>
<citcontext>
<prevsection>
<prevsent>ei . . .
</prevsent>
<prevsent>ei . among all possible target language sentences, we will choose the sentence with the highest probability: ei1 = argmax i,ei1 { pr(ei1|fj1 ) } (1)the posterior probability pr(ei1|fj1 ) is modeled directly using log-linear combination of several models (och and ney, 2002): <papid> P02-1038 </papid>pr(ei1|fj1 ) = exp ( m=1 mhm(ei1, fj1 ) ) ? i?,ei1 exp ( m=1 mhm(ei ? 1 , fj1 ) ) (2)the denominator is normalization factor that depends only on the source sentence fj1 . therefore,we can omit it during the search process.</prevsent>
</prevsection>
<citsent citstr=" J90-2002 ">
as decision rule, we obtain: ei1 = argmax i,ei1 { ? m=1 mhm(ei1, fj1 ) } (3)this approach is generalization of the source channel approach (brown et al, 1990).<papid> J90-2002 </papid></citsent>
<aftsection>
<nextsent>it has the advantage that additional models h(?)
</nextsent>
<nextsent>can be easily integrated into the overall system.
</nextsent>
<nextsent>the model scaling factors m1 are trained with respect to the final translation quality measured by an error criterion (och, 2003).<papid> P03-1021 </papid></nextsent>
<nextsent>we use state-of-the-art phrase-based translation system as described in (zens and ney, 2004; <papid> N04-1033 </papid>zenset al, 2005).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2501">
<title id=" W06-3110.xml">ngram posterior probabilities for statistical machine translation </title>
<section> baseline system.  </section>
<citcontext>
<prevsection>
<prevsent>it has the advantage that additional models h(?)
</prevsent>
<prevsent>can be easily integrated into the overall system.
</prevsent>
</prevsection>
<citsent citstr=" P03-1021 ">
the model scaling factors m1 are trained with respect to the final translation quality measured by an error criterion (och, 2003).<papid> P03-1021 </papid></citsent>
<aftsection>
<nextsent>we use state-of-the-art phrase-based translation system as described in (zens and ney, 2004; <papid> N04-1033 </papid>zenset al, 2005).</nextsent>
<nextsent>the baseline system includes the following models: an n-gram language model, phrase translation model and word-based lexicon model.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2502">
<title id=" W06-3110.xml">ngram posterior probabilities for statistical machine translation </title>
<section> baseline system.  </section>
<citcontext>
<prevsection>
<prevsent>can be easily integrated into the overall system.
</prevsent>
<prevsent>the model scaling factors m1 are trained with respect to the final translation quality measured by an error criterion (och, 2003).<papid> P03-1021 </papid></prevsent>
</prevsection>
<citsent citstr=" N04-1033 ">
we use state-of-the-art phrase-based translation system as described in (zens and ney, 2004; <papid> N04-1033 </papid>zenset al, 2005).</citsent>
<aftsection>
<nextsent>the baseline system includes the following models: an n-gram language model, phrase translation model and word-based lexicon model.
</nextsent>
<nextsent>the latter two models are used for both directions: p(f |e) and p(e|f).
</nextsent>
<nextsent>additionally, we use word penalty and phrase penalty.
</nextsent>
<nextsent>the idea is similar to the word posterior probabili ties: we sum the sentence posterior probabilities for each occurrence of an n-gram.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2505">
<title id=" W06-3110.xml">ngram posterior probabilities for statistical machine translation </title>
<section> rescoring/reranking.  </section>
<citcontext>
<prevsection>
<prevsent>it can be adjusted to prefer longer or shorter translations.
</prevsent>
<prevsent>here, we will use the posterior probability of specific target sentence length as length model: p(i|fj1 ) = ? ei1 p(ei1|fj1 ) (6)note that the sum is carried out only over target sentences ei1 with the specific length . again, the candidate target language sentences are limited to an -best list.
</prevsent>
</prevsection>
<citsent citstr=" N04-1021 ">
a straightforward application of the posterior probabilities is to use them as additional features in rescoring/reranking approach (och et al, 2004).<papid> N04-1021 </papid></citsent>
<aftsection>
<nextsent>the use of -best lists in machine translation has several advantages.
</nextsent>
<nextsent>it alleviates the effects of the huge search space which is represented in word 73graphs by using compact excerpt of the best hypotheses generated by the system.
</nextsent>
<nextsent>n -best lists are suitable for easily applying several rescoring techniques since the hypotheses are already fully generated.
</nextsent>
<nextsent>in comparison, word graph rescoring techniques need specialized tools which can traverse the graph accordingly.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2506">
<title id=" W06-3110.xml">ngram posterior probabilities for statistical machine translation </title>
<section> experimental results.  </section>
<citcontext>
<prevsection>
<prevsent>the total amount of language model training data was about 600m running words.
</prevsent>
<prevsent>we use four gram language model with modified kneser-ney smoothing as implemented in the srilm toolkit (stolcke, 2002).
</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
to measure the translation quality, we use the bleu score (papineni et al, 2002) <papid> P02-1040 </papid>and the nist score (doddington, 2002).</citsent>
<aftsection>
<nextsent>the bleu score is the geometric mean of the n-gram precision in combination with brevity penalty for too short sentences.
</nextsent>
<nextsent>the nist score is the arithmetic mean of weighted n-gram precision in combination with brevity penalty for too short sentences.
</nextsent>
<nextsent>both scores are computed case-sensitive with respect to four reference translations using the mteval-v11b tool1.
</nextsent>
<nextsent>as the bleu and nist scores measure accuracy higher scores are better.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2508">
<title id=" W06-2703.xml">tools to address the interdependence between token isation and standoff annotation </title>
<section> an xml-based standoff annotation.  </section>
<citcontext>
<prevsection>
<prevsent>as improved token isation procedures become available we can retokenise both the annotated material and the remaining unannotated data using program which we have developed forthis task.
</prevsent>
<prevsent>we describe the extension to the annotation tool, the xml representation of conflict and the retokenisation program in section 4.
</prevsent>
</prevsection>
<citsent citstr=" W05-0619 ">
tool in number of recent projects we have explored the use of machine learning techniques for named entity recognition (ner) and have worked with data from number of different domains, including data from biomedicine (finkel et al, in press; dingare et al, 2004), law reports (grover et al, 2004), social science (nissim et al, 2004), and astronomy and astrophysics (becker et al, 2005;hachey et al, 2005).<papid> W05-0619 </papid></citsent>
<aftsection>
<nextsent>we have worked with number of xml-based annotation tools, including thethe nxt annotation tool (carletta et al, 2003; carletta et al, in press).
</nextsent>
<nextsent>since we are interested onlyin written text and focus on annotation for information extraction (ie), much of the complexity offered by the nxt tool is not required and wehave therefore recently implemented our own ie specific tool.
</nextsent>
<nextsent>this has much in common with nxt, in particular annotations are encoded as standoff with pointers to the indices of the word tokens.
</nextsent>
<nextsent>a screen shot of the tool being used for ne annotation of biomedical text is shown in figure 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2509">
<title id=" W06-2703.xml">tools to address the interdependence between token isation and standoff annotation </title>
<section> an xml-based standoff annotation.  </section>
<citcontext>
<prevsection>
<prevsent>our annotation tool and the format for storing annotations that we have chosen are just one instance of wide range of possible tools and formats for the ne annotation task.
</prevsent>
<prevsent>there are number of decision points involved in the development of such tools, some of which come down to matter of preference and some of which are consequences of other choices.
</prevsent>
</prevsection>
<citsent citstr=" P02-1022 ">
examples of annotation methods which are not primarily based on xmlare gate (cunningham et al, 2002) <papid> P02-1022 </papid>and the annotation graph model of bird and liberman (2001).</citsent>
<aftsection>
<nextsent>the gate system organises annotations in graphs where the start and end nodes have pointers into the source document character offsets.
</nextsent>
<nextsent>this is an adaptation of the tipster architecture (grishman, 1997).
</nextsent>
<nextsent>(the uima system from ibm (ferrucci andlally, 2004) also stores annotations in tipster like format.)
</nextsent>
<nextsent>the annotation graph model en 1soon to be available under gpl as lt-xml2 and lt-ttt2 from http://www.ltg.ed.ac.uk/ codes annotations as directed graph with fielded records on the arcs and optional time references on the nodes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2510">
<title id=" W06-2703.xml">tools to address the interdependence between token isation and standoff annotation </title>
<section> token isation for multiple components.  </section>
<citcontext>
<prevsection>
<prevsent>to demonstrate the effect that finer-grained token isation can have on pos and ner tagging, we performed series of experiments on the ner annotated data provided for the coling bionlp evaluation (kim et al, 2004), which was derived from the genia corpus (kim et al, 2003).
</prevsent>
<prevsent>(thebionlp data is annotated with five entities, protein, dna, rna, cell type and cell line.)
</prevsent>
</prevsection>
<citsent citstr=" W03-0424 ">
we trained the c&c; maximum entropy tagger (curran and clark, 2003) <papid> W03-0424 </papid>using default settings to obtain 24 orig tok1 tok2 training # sentences 18,546 eval # sentences 3,856 training # tokens 492,465 540,046 578,661 eval # tokens 101,028 110, 352 117, 950 precision 65.14% 62.36% 61.39% recall 67.35% 64.24% 63.24% f1 66.23% 63.27% 62.32% table 1: ner results for different tokenisations of the bionlp corpus ner models for the original token isation (orig), retokenisation using the first txm tokeniser (tok1) and retokenisation using the finer-grained secondtxm tokeniser (tok2) (see section 4).</citsent>
<aftsection>
<nextsent>in all experiments we discarded the original pos tags and performed pos tagging using the c&c; tagger trained on the medpost data (smith et al, 2004).
</nextsent>
<nextsent>table 1 shows precision, recall and f-score for the ner tagger trained and tested on these three token isa tions and it can be seen that performance drops as token isation becomes more fine-grained.
</nextsent>
<nextsent>the results of these experiments indicate that care needs to be taken to achieve sensible balance between the needs of the annotation and the needs of nlp modules.
</nextsent>
<nextsent>we do not believe, how ever, that the results demonstrate that the less fine-grained original token isation is necessarily the best.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2511">
<title id=" W06-3804.xml">measuring about ness of an entity in a text </title>
<section> methods.  </section>
<citcontext>
<prevsection>
<prevsent>it is natural to view co referencing as partitioning or clustering of the set of entities.
</prevsent>
<prevsent>the idea is to group co referents into the same cluster, which is accomplished in two steps: 1) detection of the entities and extraction of their features set; 2) clustering of the entities.
</prevsent>
</prevsection>
<citsent citstr=" W99-0611 ">
for the first subtask we use the same set of features as in cardie and wagstaff (1999).<papid> W99-0611 </papid></citsent>
<aftsection>
<nextsent>for the second step we used the progressive fuzzy clustering algorithm described in angheluta et al (2004).
</nextsent>
<nextsent>2.2 learning biographical terms.
</nextsent>
<nextsent>we learn terms biographical value as the correlation of the term with texts of biographical nature.
</nextsent>
<nextsent>there are different ways of learning associations present in corpora (e.g., use of the mutual information statistic, use of the chi-square statistic).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2512">
<title id=" W06-3804.xml">measuring about ness of an entity in a text </title>
<section> methods.  </section>
<citcontext>
<prevsection>
<prevsent>we learn terms biographical value as the correlation of the term with texts of biographical nature.
</prevsent>
<prevsent>there are different ways of learning associations present in corpora (e.g., use of the mutual information statistic, use of the chi-square statistic).
</prevsent>
</prevsection>
<citsent citstr=" J93-1003 ">
we use the likelihood ratio for binomial distribution (dunning 1993), <papid> J93-1003 </papid>which tests the hypothesis whether the term occurs independently in texts of biographical nature given large corpus of biographical and non-biographical texts.</citsent>
<aftsection>
<nextsent>for considering term as biography-related, we set likelihood ratio threshold such that the hypothesis can be rejected with certain significance level.
</nextsent>
<nextsent>2.3 reference detection between entities.
</nextsent>
<nextsent>we assume that the syntactic relationships between entities (proper or common nouns) in text give us information on their semantic reference status.
</nextsent>
<nextsent>in our simple experiment, we consider reference relationships found within single sentence, and more specifically we take into account relationships between two noun phrase entities.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2513">
<title id=" W06-3804.xml">measuring about ness of an entity in a text </title>
<section> discussion of the results and related.  </section>
<citcontext>
<prevsection>
<prevsent>ana logically, zha (2002) detects terms and sentences with high salience in text and uses these for summarization.
</prevsent>
<prevsent>the graph here is made of linked term and sentence nodes.
</prevsent>
</prevsection>
<citsent citstr=" W04-3252 ">
other work on text summarization computes centrality on graphs (erkan and radev 2004; mihalcea and tarau 2004).<papid> W04-3252 </papid></citsent>
<aftsection>
<nextsent>we use linguistic motivation for linking terms in texts founded in reference relationships such as coreference and reference by biographical terms in certain syntactical constructs.
</nextsent>
<nextsent>intuitively, an important entity is linked to many referents; the more important the referents are, the more important the entity is. latent semantic indexing (lsi) is also used to detect main topics in set of docu ments/sentences, it will not explicitly model the weights of the edges between entities.
</nextsent>
<nextsent>our implementation aims at measuring the about ness of an entity from biographical view point.
</nextsent>
<nextsent>one can easily focus upon other viewpoints when determining the terms that enter into reference relationship with the input entity (e.g., computing the about ness of an input animal name with regard to its reproductive activities).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2514">
<title id=" W06-2401.xml">named entities translation based on comparable corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this case, we can use the web as multilingual corpus, in order to search it for any possible entity translation.
</prevsent>
<prevsent>we have comparable dataset available for basque and spanish.
</prevsent>
</prevsection>
<citsent citstr=" E03-1035 ">
but besides using that data source, we decided also to resort to the web as complementary dataset too, as in (moore, 2003).<papid> E03-1035 </papid></citsent>
<aftsection>
<nextsent>apart from these two datasets, we have also used some other information sources to develop the basque-spanish bilingual ne translation system.
</nextsent>
<nextsent>we have carried out two main different experiments: one using language-dependent grammar, implementing transliteration transformations (al-onaizan et al, 2002b) and rules related to elements?
</nextsent>
<nextsent>order; and another one based on the edition distance (kukich, 1992) grammar, simulating simple cognates and transliteration transformations, but in language-independent way.in both experiments, we have used basque spanish bilingual dictionary for the words in which transliteration transformations were not enough to obtain the correct translated form.
</nextsent>
<nextsent>furthermore, we have always worked using 1 basque as source language, and spanish as target language.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2516">
<title id=" W06-2401.xml">named entities translation based on comparable corpora </title>
<section> related works.  </section>
<citcontext>
<prevsection>
<prevsent>although it has been less experimented with comparable corpora there are some known systems designed to work with them as well.
</prevsent>
<prevsent>mostof them deal with language pairs that have different kinds of alphabets.
</prevsent>
</prevsection>
<citsent citstr=" W03-1501 ">
for instance, the chinese-english translation tool presented in acl 2003 (chen et al, 2003), <papid> W03-1501 </papid>or the one published in the acl 2002 edition for translating entity names from arabic to english (al-onaizan et al., 2002a).</citsent>
<aftsection>
<nextsent>the main goal of both systems is to obtain the corresponding form for english, taking chinese and arabic respectively as source languages.
</nextsent>
<nextsent>two kinds of translations can be distinguished in both systems: direct/simple translations and transliterations (al-onaizan et al, 2002b).
</nextsent>
<nextsent>however, the techniques used by each tool for both kinds of translations are different.
</nextsent>
<nextsent>frequency based methods are used in chinese-english translations, while in the arabic-english language pair, more complex process is applied, which involves the combination of different kinds of techniques.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2517">
<title id=" W06-1701.xml">web based frequency dictionaries for medium density languages </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>since the principal source of variability in word (n-gram) frequencies is the choice of topic, we can subsume overall considerations of genre under the selection of topics, especially asthe former typically dictates the latter ? for example, we rarely see literary prose or poetry dealing with undersea sedimentation rates.
</prevsent>
<prevsent>we assume fixed inventory of topics t1, t2, . . .
</prevsent>
</prevsection>
<citsent citstr=" E03-1056 ">
, tk, with on the order 104, similar in granularity to the northern light topic hierarchy (kornai et al 2003)<papid> E03-1056 </papid>and reserve t0 to topic less texts or general lan guage?.</citsent>
<aftsection>
<nextsent>assuming that these topics appear in the language with frequency q1, q2, . . .
</nextsent>
<nextsent>, qk, summing to 1 ? q0 ? 1, the average?
</nextsent>
<nextsent>topic is expected to have frequency about 1/k (and clearly, q0 is on thesame order, as it is very hard to find entirely topi cless texts).
</nextsent>
<nextsent>as is well known, the salience of different nouns and noun phrases appearing in the same structural position is greatly impacted not just by frequency (generally, less frequent words are morememorable) but also by stylistic value.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2518">
<title id=" W06-1701.xml">web based frequency dictionaries for medium density languages </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>note that just measures the range, it does not measure how representative corpus is for some language community.
</prevsent>
<prevsent>here we discuss results concerning all three ranges.
</prevsent>
</prevsection>
<citsent citstr=" P98-1050 ">
for small range, we use the hungarian translation of orwells 1984 ? 98k words including punctuation tokens, (dimitrova etal., 1998).<papid> P98-1050 </papid></citsent>
<aftsection>
<nextsent>for mid-range, we consider four topically segregated sub corpora of the hungarian side of our hungarian-english parallel corpus ? 34m words, (varga et al , 2005).
</nextsent>
<nextsent>for large-range we use our web corpus ? 700m words, (halacsy et al , 2004).
</nextsent>
<nextsent>1 collecting and presenting the data.
</nextsent>
<nextsent>hungarian lags behind high density?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2520">
<title id=" W06-1701.xml">web based frequency dictionaries for medium density languages </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this enables much finer control of word selection in psycho linguistic experiments than was hitherto possible.
</prevsent>
<prevsent>1.1 how to present the data?.
</prevsent>
</prevsection>
<citsent citstr=" J03-3001 ">
for hungarian, the highest quality (4% thresh old) stratum of the corpus contains 1.22m unique pages for total of 699m tokens, already exceeding the 500m predicted in (kilgarriff and grefenstette, 2003).<papid> J03-3001 </papid></citsent>
<aftsection>
<nextsent>since the web has grown considerably since the crawl (which took place in 2003), their estimate was clearly on the conservative side.
</nextsent>
<nextsent>of the 699m tokens some 4.95m were outside the vocabulary of thema (7% oov in this mode, but less than 3% if numerals are excluded and the analysis of compounds is turned on).
</nextsent>
<nextsent>the remaining 649.7m tokens fall in 195k lemmas with an average 54 form types per lemma.
</nextsent>
<nextsent>if all stems are considered, the ratio is considerably lower, 33.6,but the average entropy of the inflectional distributions goes down only from 1.70 to 1.58 bits.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2521">
<title id=" W06-1701.xml">web based frequency dictionaries for medium density languages </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>as far as the summary frequency list (which is less than megabyte compressed) is concerned,this can be published trivially.
</prevsent>
<prevsent>clearly, the availability of large-range gigaword corpora is in the best interest of all workers in language technology,and equally clearly, only open (freely downloadable) materials allow for replicability of experiments.
</prevsent>
</prevsection>
<citsent citstr=" N04-1016 ">
while it is possible to exploit search engine queries for various nlp tasks (lapata and keller,2004), <papid> N04-1016 </papid>for applications which use corpora as unsupervised training material downloadable base data is essential.therefore, compiled web corpus should contain actual texts.</citsent>
<aftsection>
<nextsent>we believe all cover your be hind?
</nextsent>
<nextsent>efforts such as publishing only urls to be fundamentally misguided.
</nextsent>
<nextsent>first, urls age veryrapidly: in any given year more than 10% be come stale (cho and garcia-molina, 2000), which makes any experiment conducted on such basis effectively irreproducible.
</nextsent>
<nextsent>second, by presenting quality-filtered and characterset-normalized corpus the collectors actually perform service tothose who are less interested in such mundane issues.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2522">
<title id=" W06-1701.xml">web based frequency dictionaries for medium density languages </title>
<section> the disambiguation of morphological.  </section>
<citcontext>
<prevsection>
<prevsent>3 of speech (n, v, a, etc).
</prevsent>
<prevsent>a full tag contains both pos information and morphological annotation: in highly inflecting languages the latter can lead to tagsets of high cardinality (tufis?
</prevsent>
</prevsection>
<citsent citstr=" W04-1903 ">
et al , 2000).hungarian is particularly challenging in this regard, both because the number of ambiguous tokens is high (reaching 50% in the szeged corpus according to (csendes et al , 2004) <papid> W04-1903 </papid>who use different ma), and because the ratio of tokens that are not seen during training (unseen) can be as much as four times higher than incomparable size english corpora.</citsent>
<aftsection>
<nextsent>but if larger training corpora are available, significant disambiguation is possi ble: with 1 word training corpus (csendes et al ., 2004) <papid> W04-1903 </papid>the tnt (brants, 2000) <papid> A00-1031 </papid>architecture can achieve 97.42% overall precision.the ratio of ambiguous tokens is usually calculated based on alternatives offered by morphological lexicon (either built during the training process or furnished by an external application;see below).</nextsent>
<nextsent>if the lexicon offers alternative analyses, the token is taken as ambiguous irrespective of the probability of the alternatives.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2524">
<title id=" W06-1701.xml">web based frequency dictionaries for medium density languages </title>
<section> the disambiguation of morphological.  </section>
<citcontext>
<prevsection>
<prevsent>a full tag contains both pos information and morphological annotation: in highly inflecting languages the latter can lead to tagsets of high cardinality (tufis?
</prevsent>
<prevsent>et al , 2000).hungarian is particularly challenging in this regard, both because the number of ambiguous tokens is high (reaching 50% in the szeged corpus according to (csendes et al , 2004) <papid> W04-1903 </papid>who use different ma), and because the ratio of tokens that are not seen during training (unseen) can be as much as four times higher than incomparable size english corpora.</prevsent>
</prevsection>
<citsent citstr=" A00-1031 ">
but if larger training corpora are available, significant disambiguation is possi ble: with 1 word training corpus (csendes et al ., 2004) <papid> W04-1903 </papid>the tnt (brants, 2000) <papid> A00-1031 </papid>architecture can achieve 97.42% overall precision.the ratio of ambiguous tokens is usually calculated based on alternatives offered by morphological lexicon (either built during the training process or furnished by an external application;see below).</citsent>
<aftsection>
<nextsent>if the lexicon offers alternative analyses, the token is taken as ambiguous irrespective of the probability of the alternatives.
</nextsent>
<nextsent>if an external resource is used in the form of morphological analyzer (ma), this will almost always overgener ate, yielding false ambiguity.
</nextsent>
<nextsent>but even if thema is tight, considerable proportion of ambiguous tokens will come from legitimate but rare analyses of frequent types (church, 1988).<papid> A88-1019 </papid></nextsent>
<nextsent>for example the word nem, can mean both not?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2525">
<title id=" W06-1701.xml">web based frequency dictionaries for medium density languages </title>
<section> the disambiguation of morphological.  </section>
<citcontext>
<prevsection>
<prevsent>if the lexicon offers alternative analyses, the token is taken as ambiguous irrespective of the probability of the alternatives.
</prevsent>
<prevsent>if an external resource is used in the form of morphological analyzer (ma), this will almost always overgener ate, yielding false ambiguity.
</prevsent>
</prevsection>
<citsent citstr=" A88-1019 ">
but even if thema is tight, considerable proportion of ambiguous tokens will come from legitimate but rare analyses of frequent types (church, 1988).<papid> A88-1019 </papid></citsent>
<aftsection>
<nextsent>for example the word nem, can mean both not?
</nextsent>
<nextsent>and gender?, soboth adv and noun are valid analyses, but the adverbial reading is about five orders of magnitude more frequent than the noun reading, (12596 vs. 4 tokens in the 1 word manually annotated szeged korpusz (csendes et al , 2004)).<papid> W04-1903 </papid>thus the difficulty of the task is better measured by the average information required for dis ambiguating token.</nextsent>
<nextsent>if word is assigned the label ti with probability (ti|w) (estimated as c(ti, w)/c(w) from labeled corpus) then the label entropy for word can be calculated as h(w) = ? ?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2527">
<title id=" W06-1701.xml">web based frequency dictionaries for medium density languages </title>
<section> the disambiguation of morphological.  </section>
<citcontext>
<prevsection>
<prevsent>using the 1.2 words of szeged corpus for training, in the 699 word web corpus over 4% of the non-numeric tokens will be unseen.
</prevsent>
<prevsent>given that tnt performs rather dismally on unseen items (oravecz and dienes, 2002) it was clear from the outset that for lemmatizing the we bcorpus we needed something more elaborate.the standard solution to constrain the probabilistic tagging model for some of the unseen items is the application of ma (hakkani-tur et al , 2000; hajic?
</prevsent>
</prevsection>
<citsent citstr=" H05-1060 ">
et al , 2001; smith et al , 2005).<papid> H05-1060 </papid></citsent>
<aftsection>
<nextsent>here distinction must be made between those items that are not found in the training corpus (these we have called unseen tokens) and those that are not known to thema ? we call these out of vocabulary (oov).
</nextsent>
<nextsent>as we shall see shortly, the key to the best tagging architecture we found was to follow different strategies in the lemmatization and morphological disambiguation of oov and known (in vocabulary) tokens.
</nextsent>
<nextsent>the first step in tagging is the annotation of inflectional features, with lemmatization being postponed to later processing as in (erjavec and dzeroski, 2004).
</nextsent>
<nextsent>this differs from the method of (hakkani-tur et al , 2000), where all syntactically relevant features (including the stem or lemma) ofword forms are determined in one pass.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2528">
<title id=" W06-1701.xml">web based frequency dictionaries for medium density languages </title>
<section> the disambiguation of morphological.  </section>
<citcontext>
<prevsection>
<prevsent>the first step in tagging is the annotation of inflectional features, with lemmatization being postponed to later processing as in (erjavec and dzeroski, 2004).
</prevsent>
<prevsent>this differs from the method of (hakkani-tur et al , 2000), where all syntactically relevant features (including the stem or lemma) ofword forms are determined in one pass.
</prevsent>
</prevsection>
<citsent citstr=" W96-0213 ">
in our experience, the choice of stem depends so heavily on the type of linguistic information that later processing will need that it cannot be resolved in full generality at the morphosyntactic level.our first model (ma-me) is based on disam biguating thema output in the maximum entropy(me) framework (ratnaparkhi, 1996).<papid> W96-0213 </papid></citsent>
<aftsection>
<nextsent>in addition to thema output, we use me features coding the surface form of the preceding/following word,capitalization information, and different character length suffix strings of the current word.
</nextsent>
<nextsent>thema used is the open-source hunmorph analyzer (tron et al , 2005) with the morphdb.hu hungarian morphological resource, the me is the opennlp package (baldridge et al , 2001).
</nextsent>
<nextsent>the 4ma-me model achieves 97.72% correct pos tagging and morphological analysis on the test corpus (not used in training).
</nextsent>
<nextsent>maximum entropy or other discriminative markov models (mccallum et al , 2000) suffer from the label bias problem (lafferty et al , 2001), while generative models (most notably hmms) need strict independence assumptions to make thetask of sequential data labeling tractable.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2529">
<title id=" W06-2919.xml">a context pattern induction method for named entity extraction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>new entity instances of the same category canbe extracted from unlabeled data with the induced patterns to create high-precision extensions of the seed lists.
</prevsent>
<prevsent>iii.
</prevsent>
</prevsection>
<citsent citstr=" W02-1028 ">
features derived from token membership in the extended lists improve the accuracy of learned named-entity taggers.previous approaches to context pattern induction were described by riloff and jones (1999), agichtein and gravano (2000), thelen and riloff (2002), <papid> W02-1028 </papid>lin et al (2003), and etzioni et al (2005), among others.</citsent>
<aftsection>
<nextsent>the main advance in the present method is the combination of grammatical induction and statistical techniques to create high-precision patterns.the paper is organized as follows.
</nextsent>
<nextsent>section 2 describes our pattern induction algorithm.
</nextsent>
<nextsent>section 3 shows how to extend seed sets with entities extracted by the patterns from unlabeled data.
</nextsent>
<nextsent>section 4 gives experimental results, and section 5 compares our method with previous work.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2530">
<title id=" W06-2919.xml">a context pattern induction method for named entity extraction </title>
<section> experimental results.  </section>
<citcontext>
<prevsection>
<prevsent>on the other hand, massive amounts of unlabeled data are readily available.
</prevsent>
<prevsent>the goal of semi-supervised learning to combine the best of both worlds.
</prevsent>
</prevsection>
<citsent citstr=" N04-1043 ">
recent research have shown that improvements in supervised taggers are possible by including features derived from unlabeled data (miller et al, 2004; <papid> N04-1043 </papid>liang,2005; ando and zhang, 2005).<papid> P05-1001 </papid></citsent>
<aftsection>
<nextsent>similarly, automatically generated entity lists can be used as additional features in supervised tagger.
</nextsent>
<nextsent>system f1 (precision, recall) florian et al (2003), <papid> W03-0425 </papid>best single, no list 89.94 (91.37, 88.56) zhang and johnson (2003), <papid> W03-0434 </papid>no list 90.26 (91.00, 89.53) crf baseline, no list 89.52 (90.39, 88.66) table 6: baseline comparison on 4 categories (loc, org, per, misc) on test-a dataset.</nextsent>
<nextsent>for this experiment, we started with conditional random field (crf) (lafferty et al, 2001) tagger with competitive baseline (table 6).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2532">
<title id=" W06-2919.xml">a context pattern induction method for named entity extraction </title>
<section> experimental results.  </section>
<citcontext>
<prevsection>
<prevsent>on the other hand, massive amounts of unlabeled data are readily available.
</prevsent>
<prevsent>the goal of semi-supervised learning to combine the best of both worlds.
</prevsent>
</prevsection>
<citsent citstr=" P05-1001 ">
recent research have shown that improvements in supervised taggers are possible by including features derived from unlabeled data (miller et al, 2004; <papid> N04-1043 </papid>liang,2005; ando and zhang, 2005).<papid> P05-1001 </papid></citsent>
<aftsection>
<nextsent>similarly, automatically generated entity lists can be used as additional features in supervised tagger.
</nextsent>
<nextsent>system f1 (precision, recall) florian et al (2003), <papid> W03-0425 </papid>best single, no list 89.94 (91.37, 88.56) zhang and johnson (2003), <papid> W03-0434 </papid>no list 90.26 (91.00, 89.53) crf baseline, no list 89.52 (90.39, 88.66) table 6: baseline comparison on 4 categories (loc, org, per, misc) on test-a dataset.</nextsent>
<nextsent>for this experiment, we started with conditional random field (crf) (lafferty et al, 2001) tagger with competitive baseline (table 6).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2533">
<title id=" W06-2919.xml">a context pattern induction method for named entity extraction </title>
<section> experimental results.  </section>
<citcontext>
<prevsection>
<prevsent>recent research have shown that improvements in supervised taggers are possible by including features derived from unlabeled data (miller et al, 2004; <papid> N04-1043 </papid>liang,2005; ando and zhang, 2005).<papid> P05-1001 </papid></prevsent>
<prevsent>similarly, automatically generated entity lists can be used as additional features in supervised tagger.</prevsent>
</prevsection>
<citsent citstr=" W03-0425 ">
system f1 (precision, recall) florian et al (2003), <papid> W03-0425 </papid>best single, no list 89.94 (91.37, 88.56) zhang and johnson (2003), <papid> W03-0434 </papid>no list 90.26 (91.00, 89.53) crf baseline, no list 89.52 (90.39, 88.66) table 6: baseline comparison on 4 categories (loc, org, per, misc) on test-a dataset.</citsent>
<aftsection>
<nextsent>for this experiment, we started with conditional random field (crf) (lafferty et al, 2001) tagger with competitive baseline (table 6).
</nextsent>
<nextsent>the base line tagger was trained5 on the full conll-2003 shared task data.
</nextsent>
<nextsent>we experimented with the loc,org and per lists that were automatically generated in section 4.1.
</nextsent>
<nextsent>in table 7, we show the accuracy of the tagger for the entity types for which we had induced lists.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2534">
<title id=" W06-2919.xml">a context pattern induction method for named entity extraction </title>
<section> experimental results.  </section>
<citcontext>
<prevsection>
<prevsent>recent research have shown that improvements in supervised taggers are possible by including features derived from unlabeled data (miller et al, 2004; <papid> N04-1043 </papid>liang,2005; ando and zhang, 2005).<papid> P05-1001 </papid></prevsent>
<prevsent>similarly, automatically generated entity lists can be used as additional features in supervised tagger.</prevsent>
</prevsection>
<citsent citstr=" W03-0434 ">
system f1 (precision, recall) florian et al (2003), <papid> W03-0425 </papid>best single, no list 89.94 (91.37, 88.56) zhang and johnson (2003), <papid> W03-0434 </papid>no list 90.26 (91.00, 89.53) crf baseline, no list 89.52 (90.39, 88.66) table 6: baseline comparison on 4 categories (loc, org, per, misc) on test-a dataset.</citsent>
<aftsection>
<nextsent>for this experiment, we started with conditional random field (crf) (lafferty et al, 2001) tagger with competitive baseline (table 6).
</nextsent>
<nextsent>the base line tagger was trained5 on the full conll-2003 shared task data.
</nextsent>
<nextsent>we experimented with the loc,org and per lists that were automatically generated in section 4.1.
</nextsent>
<nextsent>in table 7, we show the accuracy of the tagger for the entity types for which we had induced lists.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2538">
<title id=" W06-3002.xml">woz simulation of interactive question answering </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the user may be young reporter, trained analyst, or common man without special training.
</prevsent>
<prevsent>questions can be answered by simple names and facts, such as those handled in early trec conferences (chai et al 2004), or by short passages retrieved like some systems developed in the aquaint program do (small et al 2003).
</prevsent>
</prevsection>
<citsent citstr=" W04-2509 ">
the situation in which qa systems are supposed to be used is an important factor of the system design and the evaluation must take such factor into account.qaciad (question answering challenge for information access dialogue) is an objective and quantitative evaluation framework to measure the abilities of qa systems used interactively to participate in dialogues for accessing information (kato et al .2004<papid> W04-2509 </papid>a)(kato et al 2006).</citsent>
<aftsection>
<nextsent>it assumes the situation in which users interactively collect information using qa system for writing report on given topic and evaluates, among other things, the capabilities needed under such circumstances, i.e. proper interpretation of questions under given dialogue con text; in other words, context processing capabilities such as anaphora resolution and ellipses handling.
</nextsent>
<nextsent>we are interested in examining the assumptions made by qaciad, and conducted an experiment, in which the dialogues under the situation qaciad assumes were simulated using the woz (wizard of oz) technique (fraser et al 1991) and analyzed.
</nextsent>
<nextsent>inwoz simulation, which is frequently used for collecting dialogue data for designing speech dialogue systems, dialogues that become possible when system has been developed are simulated by human, woz, who plays the role of the system, as well as asubject who is not informed that human is behaving as the system and plays the role of its user.
</nextsent>
<nextsent>analyzing the characteristics of language expressions and pragmatic devices used by users, we confirm whether qaciad is proper framework for evaluating qa systems used in the situation it assumes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2548">
<title id=" W06-1417.xml">generation of biomedical arguments for lay readers </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>one difference in argument generation is that our systems argument strategies are based on analysis of the corpus.
</prevsent>
<prevsent>also, our system creates an inten tional-level representation of an argument.
</prevsent>
</prevsection>
<citsent citstr=" J02-4002 ">
teufel and moens (2002) <papid> J02-4002 </papid>present coding scheme for scientific argumentation in research articles that is designed for automatic summarization of human-authored text.</citsent>
<aftsection>
<nextsent>thus, it would not be sufficient for generation from non-linguistic knowledge base.
</nextsent>
<nextsent>also, it does not make the finer grained distinctions of the toulmin model.
</nextsent>
<nextsent>branting et al (1999) present the architecture of legal document drafting system.
</nextsent>
<nextsent>in it, discourse grammar applies genre-specific knowledge, while legal reasoning module creates the illocutionary structure of legal arguments.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2549">
<title id=" W06-1417.xml">generation of biomedical arguments for lay readers </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>as for work on ordering and explicitness, reed and long (1997) propose ordering heuristics for 119 arguments of classical deductive logic.
</prevsent>
<prevsent>fiedler and horacek (2001) present model for deciding what can be omitted from explanations of mathematical proofs.
</prevsent>
</prevsection>
<citsent citstr=" P00-1020 ">
carenini and moore (2000) <papid> P00-1020 </papid>present an experiment to determine how much evidence is optimal in an evaluative argument.</citsent>
<aftsection>
<nextsent>this paper presents the design of discourse generator that plans the content and organization of genetic counseling letters containing arguments.
</nextsent>
<nextsent>a preliminary evaluation of the arguments was promising.
</nextsent>
<nextsent>the most important contribution of this work is the design of non-domain-specific normative argument generator that creates an intentional-level representation of an argument.
</nextsent>
<nextsent>from the corpus, we formulated argument strategies that map formal properties of qualitative causal probabilistic models to components of toulmins model.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2550">
<title id=" W06-2917.xml">learning auxiliary fronting with grammatical inference </title>
<section> algorithm.  </section>
<citcontext>
<prevsection>
<prevsent>thus the issue as to how frequently an infant child will see them is moot point.
</prevsent>
<prevsent>context-free grammatical inference algorithms are explored in two different communities: ingram matical inference and in nlp.
</prevsent>
</prevsection>
<citsent citstr=" P05-1044 ">
the task in nlp is normally taken to be one of recovering appropriate annotations (smith and eisner, 2005) <papid> P05-1044 </papid>that normally represent constituent structure (strong learn ing), while in grammatical inference, researchers 126are more interested in merely identifying the language (weak learning).</citsent>
<aftsection>
<nextsent>in both communities, thebest performing algorithms that learn from raw positive data only 1, generally relyon some combination of three heuristics: frequency, information theoretic measures of constituency, and finally substitutabil ity.
</nextsent>
<nextsent>2 the first rests on the observation that strings of words generated by constituents are likely to occur more frequently than by chance.
</nextsent>
<nextsent>the second heuristic looks for information theoretic measures that may predict boundaries, such as drops in conditional entropy.
</nextsent>
<nextsent>the third method which is the foundation of the algorithm we use, is based on the distributional analysis of harris (harris, 1954).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2551">
<title id=" W06-2917.xml">learning auxiliary fronting with grammatical inference </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>it in no way refers to anything specific to the field of language, still less specific to human language ? no references to parts of speech, or phrases, or even hierarchical phrase structure.
</prevsent>
<prevsent>it is now widely recognised that this sort of recursive structure is domain-general (jackendoff and pinker, 2005).we have selected for this demonstration an algorithm from grammatical inference.
</prevsent>
</prevsection>
<citsent citstr=" P02-1017 ">
a number of statistical models have been proposed over the last few years by researchers such as (klein and manning, 2002; <papid> P02-1017 </papid>klein and manning, 2004) <papid> P04-1061 </papid>and (solan et al,2005).</citsent>
<aftsection>
<nextsent>these models impressively manage to extract significant structure from raw data.
</nextsent>
<nextsent>however, for our purposes, neither of these models is suitable.klein and mannings model uses variety of different cues, which combine with some specific initial isation and smoothing, and an explicit constraint to produce binary branching trees.
</nextsent>
<nextsent>though very impressive, the model is replete with domain-specific biases and assumptions.
</nextsent>
<nextsent>moreover, it does not learn language in the strict sense (a subset of the set of all strings), though it would be simple modification to make it perform such task.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2552">
<title id=" W06-2917.xml">learning auxiliary fronting with grammatical inference </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>it in no way refers to anything specific to the field of language, still less specific to human language ? no references to parts of speech, or phrases, or even hierarchical phrase structure.
</prevsent>
<prevsent>it is now widely recognised that this sort of recursive structure is domain-general (jackendoff and pinker, 2005).we have selected for this demonstration an algorithm from grammatical inference.
</prevsent>
</prevsection>
<citsent citstr=" P04-1061 ">
a number of statistical models have been proposed over the last few years by researchers such as (klein and manning, 2002; <papid> P02-1017 </papid>klein and manning, 2004) <papid> P04-1061 </papid>and (solan et al,2005).</citsent>
<aftsection>
<nextsent>these models impressively manage to extract significant structure from raw data.
</nextsent>
<nextsent>however, for our purposes, neither of these models is suitable.klein and mannings model uses variety of different cues, which combine with some specific initial isation and smoothing, and an explicit constraint to produce binary branching trees.
</nextsent>
<nextsent>though very impressive, the model is replete with domain-specific biases and assumptions.
</nextsent>
<nextsent>moreover, it does not learn language in the strict sense (a subset of the set of all strings), though it would be simple modification to make it perform such task.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2553">
<title id=" W06-2908.xml">semantic role recognition using kernels on weighted marked ordered labeled trees </title>
<section> abstract </section>
<citcontext>
<prevsection>

<prevsent>we present method for recognizing semantic role arguments using kernel on weighted marked ordered labeled trees (the wmolt kernel).
</prevsent>
</prevsection>
<citsent citstr=" H05-1018 ">
we extend the kernels on marked ordered labeled trees (kazama and torisawa, 2005) <papid> H05-1018 </papid>so that themark can be weighted according to its im portance.</citsent>
<aftsection>
<nextsent>we improve the accuracy by giving more weights on subtrees that contain the predicate and the argument nodes with this ability.
</nextsent>
<nextsent>although kazama and torisawa (2005)<papid> H05-1018 </papid>presented fast training with tree kernels, the slow classification during runtime remained to be solved.</nextsent>
<nextsent>in this paper, we give solution that uses an efficient dp updating procedure applicable in argument recognition.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2557">
<title id=" W06-2908.xml">semantic role recognition using kernels on weighted marked ordered labeled trees </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we demonstrate that the wmolt kernel improves the accuracy, and our speed-up method makes the recognition more than 40 times faster than the naive classification.
</prevsent>
<prevsent>semantic role labeling (srl) is task that recognizes the arguments of predicate (verb) in sentence and assigns the correct role to each argument.
</prevsent>
</prevsection>
<citsent citstr=" J02-3001 ">
as this task is recognized as an important step after(or the last step of) syntactic analysis, many studies have been conducted to achieve accurate semantic role labeling (gildea and jurafsky, 2002; <papid> J02-3001 </papid>moschitti, 2004; <papid> P04-1043 </papid>hacioglu et al, 2004; <papid> W04-2416 </papid>punyakanok et al., 2004; <papid> C04-1197 </papid>pradhan et al, 2005<papid> P05-1072 </papid>a; pradhan et al, 2005<papid> P05-1072 </papid>b; toutanova et al, 2005).<papid> P05-1073 </papid></citsent>
<aftsection>
<nextsent>most of the studies have focused on machine learning because of the availability of standard datasets, such as propbank (kingsbury and palmer, 2002).
</nextsent>
<nextsent>naturally, the usefulness of parse trees in this task can be anticipated.
</nextsent>
<nextsent>for example, the recent conll 2005 shared task (carreras and ma`rquez,2005) provided parse trees for use and their usefulness was ensured.
</nextsent>
<nextsent>most of the methods heuristic ally extract features from parse trees, and from other sources, and use them in machine learning methods based on feature vector representation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2558">
<title id=" W06-2908.xml">semantic role recognition using kernels on weighted marked ordered labeled trees </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we demonstrate that the wmolt kernel improves the accuracy, and our speed-up method makes the recognition more than 40 times faster than the naive classification.
</prevsent>
<prevsent>semantic role labeling (srl) is task that recognizes the arguments of predicate (verb) in sentence and assigns the correct role to each argument.
</prevsent>
</prevsection>
<citsent citstr=" P04-1043 ">
as this task is recognized as an important step after(or the last step of) syntactic analysis, many studies have been conducted to achieve accurate semantic role labeling (gildea and jurafsky, 2002; <papid> J02-3001 </papid>moschitti, 2004; <papid> P04-1043 </papid>hacioglu et al, 2004; <papid> W04-2416 </papid>punyakanok et al., 2004; <papid> C04-1197 </papid>pradhan et al, 2005<papid> P05-1072 </papid>a; pradhan et al, 2005<papid> P05-1072 </papid>b; toutanova et al, 2005).<papid> P05-1073 </papid></citsent>
<aftsection>
<nextsent>most of the studies have focused on machine learning because of the availability of standard datasets, such as propbank (kingsbury and palmer, 2002).
</nextsent>
<nextsent>naturally, the usefulness of parse trees in this task can be anticipated.
</nextsent>
<nextsent>for example, the recent conll 2005 shared task (carreras and ma`rquez,2005) provided parse trees for use and their usefulness was ensured.
</nextsent>
<nextsent>most of the methods heuristic ally extract features from parse trees, and from other sources, and use them in machine learning methods based on feature vector representation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2559">
<title id=" W06-2908.xml">semantic role recognition using kernels on weighted marked ordered labeled trees </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we demonstrate that the wmolt kernel improves the accuracy, and our speed-up method makes the recognition more than 40 times faster than the naive classification.
</prevsent>
<prevsent>semantic role labeling (srl) is task that recognizes the arguments of predicate (verb) in sentence and assigns the correct role to each argument.
</prevsent>
</prevsection>
<citsent citstr=" W04-2416 ">
as this task is recognized as an important step after(or the last step of) syntactic analysis, many studies have been conducted to achieve accurate semantic role labeling (gildea and jurafsky, 2002; <papid> J02-3001 </papid>moschitti, 2004; <papid> P04-1043 </papid>hacioglu et al, 2004; <papid> W04-2416 </papid>punyakanok et al., 2004; <papid> C04-1197 </papid>pradhan et al, 2005<papid> P05-1072 </papid>a; pradhan et al, 2005<papid> P05-1072 </papid>b; toutanova et al, 2005).<papid> P05-1073 </papid></citsent>
<aftsection>
<nextsent>most of the studies have focused on machine learning because of the availability of standard datasets, such as propbank (kingsbury and palmer, 2002).
</nextsent>
<nextsent>naturally, the usefulness of parse trees in this task can be anticipated.
</nextsent>
<nextsent>for example, the recent conll 2005 shared task (carreras and ma`rquez,2005) provided parse trees for use and their usefulness was ensured.
</nextsent>
<nextsent>most of the methods heuristic ally extract features from parse trees, and from other sources, and use them in machine learning methods based on feature vector representation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2560">
<title id=" W06-2908.xml">semantic role recognition using kernels on weighted marked ordered labeled trees </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we demonstrate that the wmolt kernel improves the accuracy, and our speed-up method makes the recognition more than 40 times faster than the naive classification.
</prevsent>
<prevsent>semantic role labeling (srl) is task that recognizes the arguments of predicate (verb) in sentence and assigns the correct role to each argument.
</prevsent>
</prevsection>
<citsent citstr=" C04-1197 ">
as this task is recognized as an important step after(or the last step of) syntactic analysis, many studies have been conducted to achieve accurate semantic role labeling (gildea and jurafsky, 2002; <papid> J02-3001 </papid>moschitti, 2004; <papid> P04-1043 </papid>hacioglu et al, 2004; <papid> W04-2416 </papid>punyakanok et al., 2004; <papid> C04-1197 </papid>pradhan et al, 2005<papid> P05-1072 </papid>a; pradhan et al, 2005<papid> P05-1072 </papid>b; toutanova et al, 2005).<papid> P05-1073 </papid></citsent>
<aftsection>
<nextsent>most of the studies have focused on machine learning because of the availability of standard datasets, such as propbank (kingsbury and palmer, 2002).
</nextsent>
<nextsent>naturally, the usefulness of parse trees in this task can be anticipated.
</nextsent>
<nextsent>for example, the recent conll 2005 shared task (carreras and ma`rquez,2005) provided parse trees for use and their usefulness was ensured.
</nextsent>
<nextsent>most of the methods heuristic ally extract features from parse trees, and from other sources, and use them in machine learning methods based on feature vector representation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2561">
<title id=" W06-2908.xml">semantic role recognition using kernels on weighted marked ordered labeled trees </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we demonstrate that the wmolt kernel improves the accuracy, and our speed-up method makes the recognition more than 40 times faster than the naive classification.
</prevsent>
<prevsent>semantic role labeling (srl) is task that recognizes the arguments of predicate (verb) in sentence and assigns the correct role to each argument.
</prevsent>
</prevsection>
<citsent citstr=" P05-1072 ">
as this task is recognized as an important step after(or the last step of) syntactic analysis, many studies have been conducted to achieve accurate semantic role labeling (gildea and jurafsky, 2002; <papid> J02-3001 </papid>moschitti, 2004; <papid> P04-1043 </papid>hacioglu et al, 2004; <papid> W04-2416 </papid>punyakanok et al., 2004; <papid> C04-1197 </papid>pradhan et al, 2005<papid> P05-1072 </papid>a; pradhan et al, 2005<papid> P05-1072 </papid>b; toutanova et al, 2005).<papid> P05-1073 </papid></citsent>
<aftsection>
<nextsent>most of the studies have focused on machine learning because of the availability of standard datasets, such as propbank (kingsbury and palmer, 2002).
</nextsent>
<nextsent>naturally, the usefulness of parse trees in this task can be anticipated.
</nextsent>
<nextsent>for example, the recent conll 2005 shared task (carreras and ma`rquez,2005) provided parse trees for use and their usefulness was ensured.
</nextsent>
<nextsent>most of the methods heuristic ally extract features from parse trees, and from other sources, and use them in machine learning methods based on feature vector representation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2567">
<title id=" W06-2908.xml">semantic role recognition using kernels on weighted marked ordered labeled trees </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we demonstrate that the wmolt kernel improves the accuracy, and our speed-up method makes the recognition more than 40 times faster than the naive classification.
</prevsent>
<prevsent>semantic role labeling (srl) is task that recognizes the arguments of predicate (verb) in sentence and assigns the correct role to each argument.
</prevsent>
</prevsection>
<citsent citstr=" P05-1073 ">
as this task is recognized as an important step after(or the last step of) syntactic analysis, many studies have been conducted to achieve accurate semantic role labeling (gildea and jurafsky, 2002; <papid> J02-3001 </papid>moschitti, 2004; <papid> P04-1043 </papid>hacioglu et al, 2004; <papid> W04-2416 </papid>punyakanok et al., 2004; <papid> C04-1197 </papid>pradhan et al, 2005<papid> P05-1072 </papid>a; pradhan et al, 2005<papid> P05-1072 </papid>b; toutanova et al, 2005).<papid> P05-1073 </papid></citsent>
<aftsection>
<nextsent>most of the studies have focused on machine learning because of the availability of standard datasets, such as propbank (kingsbury and palmer, 2002).
</nextsent>
<nextsent>naturally, the usefulness of parse trees in this task can be anticipated.
</nextsent>
<nextsent>for example, the recent conll 2005 shared task (carreras and ma`rquez,2005) provided parse trees for use and their usefulness was ensured.
</nextsent>
<nextsent>most of the methods heuristic ally extract features from parse trees, and from other sources, and use them in machine learning methods based on feature vector representation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2568">
<title id=" W06-2908.xml">semantic role recognition using kernels on weighted marked ordered labeled trees </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>tree kernels (collins and duffy, 2001; kashima and koyanagi, 2002) have been proposed to directly handle trees in kernel-based methods, such as svms(vapnik, 1995).
</prevsent>
<prevsent>tree kernels calculate the similarity between trees, taking into consideration all of the subtrees, and, therefore there is no need for such feature engineering.
</prevsent>
</prevsection>
<citsent citstr=" W04-2403 ">
moschitti and bejan (2004) <papid> W04-2403 </papid>extensively studied tree kernels for semantic role labeling.</citsent>
<aftsection>
<nextsent>however, they reported that they could not successfully build an accurate argument recognizer, although the role assignment was improved.
</nextsent>
<nextsent>although moschitti et al (2005) <papid> W05-0407 </papid>reported on argument recognition using tree kernels, it was preliminary evaluation because they used oracle parse trees.</nextsent>
<nextsent>kazama and torisawa (2005)<papid> H05-1018 </papid>proposed new tree kernel for node relation labeling, as which srl canbe cast.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2570">
<title id=" W06-2908.xml">semantic role recognition using kernels on weighted marked ordered labeled trees </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>moschitti and bejan (2004) <papid> W04-2403 </papid>extensively studied tree kernels for semantic role labeling.</prevsent>
<prevsent>however, they reported that they could not successfully build an accurate argument recognizer, although the role assignment was improved.</prevsent>
</prevsection>
<citsent citstr=" W05-0407 ">
although moschitti et al (2005) <papid> W05-0407 </papid>reported on argument recognition using tree kernels, it was preliminary evaluation because they used oracle parse trees.</citsent>
<aftsection>
<nextsent>kazama and torisawa (2005)<papid> H05-1018 </papid>proposed new tree kernel for node relation labeling, as which srl canbe cast.</nextsent>
<nextsent>this kernel is defined on marked ordered labeled trees, where node can have mark to indicate the existence of relation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2607">
<title id=" W06-2908.xml">semantic role recognition using kernels on weighted marked ordered labeled trees </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>in this study, we could train the svm with 2,000 sentences (this took more than 30 hours including the conversion of trees), but this is very small fraction of the entire training set.
</prevsent>
<prevsent>we need to explore the methods for incorporating large training set within reasonable training time.
</prevsent>
</prevsection>
<citsent citstr=" W03-1012 ">
for example, the combination of small svms (shen et al., 2003) <papid> W03-1012 </papid>is possible direction.</citsent>
<aftsection>
<nextsent>the contribution of this study is not the accuracy achieved.
</nextsent>
<nextsent>the first contribution is the demonstration of the drastic effect of the mark weighting.
</nextsent>
<nextsent>we will explore more accurate kernels based on thewmoltkernel.
</nextsent>
<nextsent>for example, we are planning to use different weights depending on the marks.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2610">
<title id=" W06-3118.xml">portage with smoothed phrase tables and segment choice models </title>
<section> portage.  </section>
<citcontext>
<prevsection>
<prevsent>in section 4 the results are summarized in tabular form.
</prevsent>
<prevsent>following these, there is conclusions section that highlights what can be gleaned of value from these results.
</prevsent>
</prevsection>
<citsent citstr=" W05-0822 ">
because this is the second participation of portage in such shared task, description of the base system can be found elsewhere (sadat et al 2005).<papid> W05-0822 </papid></citsent>
<aftsection>
<nextsent>briefly,portage is research vehicle and development prototype system exploiting the state-of-the-art in statistical machine translation (smt).
</nextsent>
<nextsent>it uses custom built decoder followed by rescoring module that adjusts weights based on number of features defined on the source sentence.
</nextsent>
<nextsent>we will devote space to discussing changes made since the 2005 shared task.
</nextsent>
<nextsent>2.1 phrase-table smoothing.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2611">
<title id=" W06-3118.xml">portage with smoothed phrase tables and segment choice models </title>
<section> portage.  </section>
<citcontext>
<prevsection>
<prevsent>the estimates for pg(t|s) are analogous.
</prevsent>
<prevsent>the second strategy is kneser-ney smoothing(kneser and ney, 1995), using the interpolated variant described in (chen and goodman., 1998):1 pk(s|t) = c(s, t) ? + n1+(?, t) pk(s) ? c(s, t)where = n1/(n1 + 2n2), n1+(?, t) is the number of distinct phrases with which co-occurs, and pk(s) = n1+(s, ?)/ ? n1+(s, ?), with n1+(s, ?) analogous to n1+(?, t).
</prevsent>
</prevsection>
<citsent citstr=" N04-1033 ">
our approach to phrase-table smoothing contrasts to previous work (zens and ney, 2004) <papid> N04-1033 </papid>in which smoothed phrase probabilities are constructed from word-pair probabilities and combined in log-linear model with an un smoothed phrase-table.</citsent>
<aftsection>
<nextsent>we believe the two approaches are complementary, so combination of both would be worth exploring in future work.
</nextsent>
<nextsent>2.2 feature-rich dt-based distortion.
</nextsent>
<nextsent>in recent paper (kuhn et al 2006), <papid> N06-1004 </papid>we presented new class of probabilistic segment choicemodels?</nextsent>
<nextsent>(scms) for distortion in phrase-based systems.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2612">
<title id=" W06-3118.xml">portage with smoothed phrase tables and segment choice models </title>
<section> portage.  </section>
<citcontext>
<prevsection>
<prevsent>we believe the two approaches are complementary, so combination of both would be worth exploring in future work.
</prevsent>
<prevsent>2.2 feature-rich dt-based distortion.
</prevsent>
</prevsection>
<citsent citstr=" N06-1004 ">
in recent paper (kuhn et al 2006), <papid> N06-1004 </papid>we presented new class of probabilistic segment choicemodels?</citsent>
<aftsection>
<nextsent>(scms) for distortion in phrase-based systems.
</nextsent>
<nextsent>in some situations, scms will assign better distortion score to drastic reordering of the source sentence than to no reordering; in this, scms differ from the conventional penalty-based distortion, which always favours less rather than more distortion.
</nextsent>
<nextsent>we developed particular kind of scm based on decision trees (dts) containing both questions of positional type (e.g., questions about the distance of given phrase from the beginning of the source sentence or from the previously translated phrase) and word-based questions (e.g., questions about the presence or absence of given words in specified phrase).
</nextsent>
<nextsent>the dts are grown on corpus consisting of segment-aligned bilingual sentence pairs.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2614">
<title id=" W06-1515.xml">constraint based computational semantics a comparison between ltag and lrs </title>
<section> negative concord.  </section>
<citcontext>
<prevsection>
<prevsent>therefore, in contrast to ltag, two descriptions can denote thesame formula.
</prevsent>
<prevsent>here, ltag is more limited compared to lrs.
</prevsent>
</prevsection>
<citsent citstr=" W06-1511 ">
on the other hand, the way semantic representations are defined in ltag guarantees 3see (lichte and kallmeyer, 2006) <papid> W06-1511 </papid>for discussion of neg and n-scope in the context of npi-licensing.</citsent>
<aftsection>
<nextsent>113 that they almost correspond to normal dominance constraints, which are known to be polynomiallyparsable.
</nextsent>
<nextsent>the difference in the use of under specification techniques reflects the more general difference between generative rewriting system such as ltag, in which the elements of the grammar are objects, and purely description-based formalism such as hpsg, in which token identities between different components of linguistic structures are natural and frequently employed.
</nextsent>
<nextsent>ltag and lrs have several common characteristics: they both 1.
</nextsent>
<nextsent>use ty2 language for semantics; 2.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2615">
<title id=" W06-3121.xml">phramer  an open source statistical phrase based translator </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our goal wasto improve the translation quality by enhancing the translation table and by preprocessing the source language text
</prevsent>
<prevsent>despite the fact that the research in statistical machine translation (smt) is very active, there isnt an abundance of open-source tools available to the community.
</prevsent>
</prevsection>
<citsent citstr=" P03-1021 ">
in this paper, we present phramer, an open-source system that embeds aphrase-based decoder, minimum error rate training (och, 2003) <papid> P03-1021 </papid>module and various tools related to machine translation (mt).</citsent>
<aftsection>
<nextsent>the software is released under bsd license and it is available at http://www.phramer.org/.
</nextsent>
<nextsent>we also describe our phramer-based system that we build for the wmt06 shared task.
</nextsent>
<nextsent>phramer is phrase-based smt system written in java.
</nextsent>
<nextsent>it includes: ? decoder that is compatible with pharaoh (koehn, 2004),?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2617">
<title id=" W06-3121.xml">phramer  an open source statistical phrase based translator </title>
<section> phramer.  </section>
<citcontext>
<prevsection>
<prevsent>thus no external tools are required.
</prevsent>
<prevsent>the mert module is highly modular, efficient and customizable implementation of the algorithm described in (och, 2003).<papid> P03-1021 </papid></prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
the release has implementations for bleu (papineni et al , 2002), <papid> P02-1040 </papid>wer and per error criteria and it has decoding interfaces for phramer and pharaoh.</citsent>
<aftsection>
<nextsent>it can be used to search parameters over more than one million variables.
</nextsent>
<nextsent>it offers features as resume search, reuse hypotheses from previous runs and various strategies to search for optimal ? weight vectors.
</nextsent>
<nextsent>1http://www.sqlite.org/ 146 the package contains set of tools that include: ? distributed decoding (compatible with both phramer and pharaoh) ? it automatically splits decoding jobs and distributes them to workers and assembles the results.
</nextsent>
<nextsent>it is compatible with lattice generation, therefore it can also be used during weights search (using mert).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2618">
<title id=" W06-3121.xml">phramer  an open source statistical phrase based translator </title>
<section> wmt06 shared task.  </section>
<citcontext>
<prevsection>
<prevsent>3.1 baseline system.
</prevsent>
<prevsent>3.1.1 translation table generation to generate translation table for each pair of languages starting from sentence-aligned parallel corpus, we used modified version of the pharaoh training software 2.
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
the software also required giza++ word alignment tool(och and ney, 2003).<papid> J03-1002 </papid>we generated for each phrase pair in the translation table 5 features: phrase translation probability (both directions), lexical weighting (koehn et al , 2003) (<papid> N03-1017 </papid>both directions) and phrase penalty (constant value).</citsent>
<aftsection>
<nextsent>3.1.2 decoder the phramer decoder was used to translate the devtest2006 and test2006 files.
</nextsent>
<nextsent>we accelerated the decoding process by using the distributed decoding tool.
</nextsent>
<nextsent>3.1.3 minimum error rate training we determined the weights to combine the models using the mert component in phramer.
</nextsent>
<nextsent>because of the time constrains for the shared task submission3, we used pharaoh + carmel4 as the de 2http://www.iccs.inf.ed.ac.uk/pkoehn/training.tgz 3after the shared task submission, we optimized lot our decoder.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2619">
<title id=" W06-3121.xml">phramer  an open source statistical phrase based translator </title>
<section> wmt06 shared task.  </section>
<citcontext>
<prevsection>
<prevsent>3.1 baseline system.
</prevsent>
<prevsent>3.1.1 translation table generation to generate translation table for each pair of languages starting from sentence-aligned parallel corpus, we used modified version of the pharaoh training software 2.
</prevsent>
</prevsection>
<citsent citstr=" N03-1017 ">
the software also required giza++ word alignment tool(och and ney, 2003).<papid> J03-1002 </papid>we generated for each phrase pair in the translation table 5 features: phrase translation probability (both directions), lexical weighting (koehn et al , 2003) (<papid> N03-1017 </papid>both directions) and phrase penalty (constant value).</citsent>
<aftsection>
<nextsent>3.1.2 decoder the phramer decoder was used to translate the devtest2006 and test2006 files.
</nextsent>
<nextsent>we accelerated the decoding process by using the distributed decoding tool.
</nextsent>
<nextsent>3.1.3 minimum error rate training we determined the weights to combine the models using the mert component in phramer.
</nextsent>
<nextsent>because of the time constrains for the shared task submission3, we used pharaoh + carmel4 as the de 2http://www.iccs.inf.ed.ac.uk/pkoehn/training.tgz 3after the shared task submission, we optimized lot our decoder.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2620">
<title id=" W06-3121.xml">phramer  an open source statistical phrase based translator </title>
<section> wmt06 shared task.  </section>
<citcontext>
<prevsection>
<prevsent>to simplify the process, we limited the replacement of tokens to one-to-one (one real token to one special token), so that the word alignment file can be directly used together with the original parallel corpus to extract phrases required for the generation of the translation table.
</prevsent>
<prevsent>table 2 shows an example of the output.
</prevsent>
</prevsection>
<citsent citstr=" P05-1066 ">
the second enhancement tries to improve the quality of the translation by rearranging the words in the source sentence to better match the correct word order in the target language (collins et al , 2005).<papid> P05-1066 </papid></citsent>
<aftsection>
<nextsent>we focused on very specific pattern ? based on the part-of-speech tags, changing the order of nn-adj phrases in the non-english sentences.
</nextsent>
<nextsent>this process was also applied to the input dev/test files, when the target language was english.
</nextsent>
<nextsent>figure 1 shows there ordering process and its effect on the alignment.
</nextsent>
<nextsent>the expected benefits are: ? better word alignment due to an alignment closer to the expected alignment (monotone).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2621">
<title id=" W06-3121.xml">phramer  an open source statistical phrase based translator </title>
<section> wmt06 shared task.  </section>
<citcontext>
<prevsection>
<prevsent>the lm is the only model that opposes the tendency of the distortion model towards monotone phrase order.
</prevsent>
<prevsent>phramer delivers very good baseline system.
</prevsent>
</prevsection>
<citsent citstr=" W05-0820 ">
using only the baseline system, we obtain +0.68 on deen, +0.43 on fren and -0.18 on esen difference in bleu score compared to wpt05s best system (koehn and monz, 2005).<papid> W05-0820 </papid></citsent>
<aftsection>
<nextsent>this fact is caused by the mert module.
</nextsent>
<nextsent>this module is capable of estimating parameters over large development corpus in reasonable time, thus it is able to generate highly relevant parameters.
</nextsent>


</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2622">
<title id=" W07-0705.xml">can we translate letters </title>
<section> from words to letters.  </section>
<citcontext>
<prevsection>
<prevsent>ei in target language.
</prevsent>
<prevsent>bayes decision rule states that we should choose the sentence which maximizes the posterior probability ei1 = argmax ei1 p(ei1|fj1 ) , (1)where the argmax operator denotes the search process.
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
in the original work (brown et al, 1993) <papid> J93-2003 </papid>the posterior probability p(ei1|fj1 ) is decomposed following noisy-channel approach, but current stateof-the-art systems model the translation probability directly using log-linear model(och and ney, 2002): <papid> P02-1038 </papid>p(ei1|fj1 ) = exp ( m=1 mhm(ei1, fj1 ) ) ? ei1 exp ( m=1 mhm(ei1, fj1 ) ) , (2) with hm different models, scaling factors and the denominator normalization factor that can be ignored in the maximization process.</citsent>
<aftsection>
<nextsent>the are usually chosen by optimizing performance measure overdevelopment corpus using numerical optimization algorithm like the downhill simplex algorithm (press et al, 2002).the most widely used models in the loglinear combination are phrase-based models in source to-target and target-to-source directions, ibm1-likescores computed at phrase level, also in source-totarget and target-to-source directions, target language model and different penalties, like phrase penalty and word penalty.
</nextsent>
<nextsent>this same approach can be directly adapted to the letter-based translation framework.
</nextsent>
<nextsent>in this case we are given sequence of letters fj1 corresponding to source (word) string fj1 , which is to be translated into sequence of letters ei1 corresponding to string ei1 in target language.
</nextsent>
<nextsent>note that in this case white spaces are also part of the vocabulary and haveto be generated as any other letter.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2623">
<title id=" W07-0705.xml">can we translate letters </title>
<section> from words to letters.  </section>
<citcontext>
<prevsection>
<prevsent>ei in target language.
</prevsent>
<prevsent>bayes decision rule states that we should choose the sentence which maximizes the posterior probability ei1 = argmax ei1 p(ei1|fj1 ) , (1)where the argmax operator denotes the search process.
</prevsent>
</prevsection>
<citsent citstr=" P02-1038 ">
in the original work (brown et al, 1993) <papid> J93-2003 </papid>the posterior probability p(ei1|fj1 ) is decomposed following noisy-channel approach, but current stateof-the-art systems model the translation probability directly using log-linear model(och and ney, 2002): <papid> P02-1038 </papid>p(ei1|fj1 ) = exp ( m=1 mhm(ei1, fj1 ) ) ? ei1 exp ( m=1 mhm(ei1, fj1 ) ) , (2) with hm different models, scaling factors and the denominator normalization factor that can be ignored in the maximization process.</citsent>
<aftsection>
<nextsent>the are usually chosen by optimizing performance measure overdevelopment corpus using numerical optimization algorithm like the downhill simplex algorithm (press et al, 2002).the most widely used models in the loglinear combination are phrase-based models in source to-target and target-to-source directions, ibm1-likescores computed at phrase level, also in source-totarget and target-to-source directions, target language model and different penalties, like phrase penalty and word penalty.
</nextsent>
<nextsent>this same approach can be directly adapted to the letter-based translation framework.
</nextsent>
<nextsent>in this case we are given sequence of letters fj1 corresponding to source (word) string fj1 , which is to be translated into sequence of letters ei1 corresponding to string ei1 in target language.
</nextsent>
<nextsent>note that in this case white spaces are also part of the vocabulary and haveto be generated as any other letter.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2624">
<title id=" W07-0705.xml">can we translate letters </title>
<section> from words to letters.  </section>
<citcontext>
<prevsection>
<prevsent>this has negative impact in the running time of the actual implementation of the algorithms, specially for the alignment process.
</prevsent>
<prevsent>in order to alleviate this, the alignment process was split into two passes.
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
in the first part, word alignment was computed (using the giza++ toolkit (och and ney, 2003)).<papid> J03-1002 </papid></citsent>
<aftsection>
<nextsent>then the training sentences were split according to this alignment (in similar way to the standard phrase extraction algorithm), so that the length of the source and target part is around thirty letters.
</nextsent>
<nextsent>then, letter-based alignment is computed.
</nextsent>
<nextsent>2.2 efficiency issues.
</nextsent>
<nextsent>somewhat counter-intuitively, the reduced vocabulary size does not necessarily imply reduced mem 1for the word-based system this is also the case.ory footprint, at least not without dedicated program optimization.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2625">
<title id=" W06-1317.xml">classification of discourse coherence relations an exploratory study using multiple knowledge sources </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>this type of detailed feature analysis can serve to inform or augment more structured, compositional approaches to discourse such as those based on segmented discourse representation theory (sdrt) (asher and lascarides, 2003) or the approach taken with the d-ltag system (forbes et al , 2001).using comprehensive set of linguistic features as input to maximum entropy classifier,we achieve 81% accuracy on classifying the correct type of discourse coherence relation between two segments.
</prevsent>
<prevsent>in the past few years, the tasks of discourse segmentation and parsing have been tackled from different perspectives and within different frameworks.
</prevsent>
</prevsection>
<citsent citstr=" N03-1030 ">
within rhetorical structure theory (rst), soricut and marcu (2003) <papid> N03-1030 </papid>have developed two 117probabilistic models for identifying clausal elementary discourse units and generating discourse trees at the sentence level.</citsent>
<aftsection>
<nextsent>these are built using lexical and syntactic information obtained from mapping the discourse-annotated sentences in therst corpus (carlson et al , 2003) to their corresponding syntactic trees in the penn treebank.
</nextsent>
<nextsent>within sdrt, baldridge and lascarides (2005<papid> W05-0613 </papid>b) also take data-driven approach to the tasks of segmentation and identification of discourse relations.</nextsent>
<nextsent>they create probabilistic discourse parser based on dialogues from the redwoods treebank, annotated with sdrt rhetorical relations (baldridge and lascarides, 2005<papid> W05-0613 </papid>a).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2626">
<title id=" W06-1317.xml">classification of discourse coherence relations an exploratory study using multiple knowledge sources </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>within rhetorical structure theory (rst), soricut and marcu (2003) <papid> N03-1030 </papid>have developed two 117probabilistic models for identifying clausal elementary discourse units and generating discourse trees at the sentence level.</prevsent>
<prevsent>these are built using lexical and syntactic information obtained from mapping the discourse-annotated sentences in therst corpus (carlson et al , 2003) to their corresponding syntactic trees in the penn treebank.</prevsent>
</prevsection>
<citsent citstr=" W05-0613 ">
within sdrt, baldridge and lascarides (2005<papid> W05-0613 </papid>b) also take data-driven approach to the tasks of segmentation and identification of discourse relations.</citsent>
<aftsection>
<nextsent>they create probabilistic discourse parser based on dialogues from the redwoods treebank, annotated with sdrt rhetorical relations (baldridge and lascarides, 2005<papid> W05-0613 </papid>a).</nextsent>
<nextsent>the parser is grounded on headed tree representations and dialogue-based features, such as turn-taking and domain specific goals.in the penn discourse treebank (pdtb) (web beret al , 2005), the identification of discourse structure is approached independently of any linguistic theory by using discourse connectives rather than abstract rhetorical relations.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2628">
<title id=" W06-1317.xml">classification of discourse coherence relations an exploratory study using multiple knowledge sources </title>
<section> attribution (attr): reporting and evidential.  </section>
<citcontext>
<prevsection>
<prevsent>of event swe performed both modal parsing and temporal parsing over events.
</prevsent>
<prevsent>identification of events was performed using evita (saur??
</prevsent>
</prevsection>
<citsent citstr=" P05-3021 ">
et al , 2006), an open-domain event tagger developed under the tarsqi research framework (verhagen et al , 2005).<papid> P05-3021 </papid></citsent>
<aftsection>
<nextsent>evita locates and tags all event-referringexpressions in the input text that can be temporally ordered.
</nextsent>
<nextsent>in addition, it identifies those grammatical features implicated in temporal and modal information of events; namely, tense, aspect, polarity, modality, as well as the event class.
</nextsent>
<nextsent>event annotation follows version 1.2.1 of the timeml specifications.4modal parsing in the form of identifying subordinating verb relations and their type was performed using slinket (saur??
</nextsent>
<nextsent>et al , 2006), another component of the tarsqi framework.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2629">
<title id=" W06-1317.xml">classification of discourse coherence relations an exploratory study using multiple knowledge sources </title>
<section> attribution (attr): reporting and evidential.  </section>
<citcontext>
<prevsection>
<prevsent>in example (3) above, determining (potential) cause-effect relationship between crash and injury is necessary to identify the discourse relation.
</prevsent>
<prevsent>4.3.1 corpus-based lexical similarity lexical similarity was computed using the word sketch engine (wse) (killgarrif et al ,2004) similarity metric applied over british national corpus.
</prevsent>
</prevsection>
<citsent citstr=" P98-2127 ">
the wse similarity metric implements the word similarity measure based on grammatical relations as defined in (lin, 1998) <papid> P98-2127 </papid>with minor modifications.</citsent>
<aftsection>
<nextsent>4.3.2 the brandeis semantic ontology as second source of lexical coherence, we used the brandeis semantic ontology or bso(pustejovsky et al , 2006).
</nextsent>
<nextsent>the bso is lexicallybased ontology in the generative lexicon tradition (pustejovsky, 2001; pustejovsky, 1995).
</nextsent>
<nextsent>it focuses on contextualizing the meanings of words and does this by rich system of types and qualia structures.
</nextsent>
<nextsent>for example, if one were to look up the phrase red wine in the bso, one would find its type is wine and its types type is alcoholic beverage.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2630">
<title id=" W07-0311.xml">enhancing commercial grammar based applications using robust approaches to speech understanding </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>all experiments herein are run on data collected from deployed directed dialog applications and show that slmbased techniques outperform grammar based ones without requiring any change in the application logic.
</prevsent>
<prevsent>the bulk of the literature on spoken dialog systems is based on the simple architecture in which the input speech is processed by statistical language model-based recognizer (slm-based recognizer) to produce word string.
</prevsent>
</prevsection>
<citsent citstr=" H90-1027 ">
this word string is further processed by robust parser (ward, 1990) <papid> H90-1027 </papid>or call router (gorin et al 1997) to be converted in semantic interpretation.</citsent>
<aftsection>
<nextsent>however, it is striking to seethat large portion of deployed commercial applications do not follow this architecture and approach the recognition/interpretation problem by relying on hand-crafted rules (context-free grammars - cfgs).
</nextsent>
<nextsent>the apparent reasons for this are the up-front cost and additional delays of collecting domain-specificutterances to properly train the slm (not to mention semantic tagging needed to train the call router) (hemphill et al 1990; <papid> H90-1021 </papid>knight et al 2001; gorin etal, 1997).</nextsent>
<nextsent>choosing to use grammar-based approach also makes the application predictable and relatively easy to design.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2631">
<title id=" W07-0311.xml">enhancing commercial grammar based applications using robust approaches to speech understanding </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this word string is further processed by robust parser (ward, 1990) <papid> H90-1027 </papid>or call router (gorin et al 1997) to be converted in semantic interpretation.</prevsent>
<prevsent>however, it is striking to seethat large portion of deployed commercial applications do not follow this architecture and approach the recognition/interpretation problem by relying on hand-crafted rules (context-free grammars - cfgs).</prevsent>
</prevsection>
<citsent citstr=" H90-1021 ">
the apparent reasons for this are the up-front cost and additional delays of collecting domain-specificutterances to properly train the slm (not to mention semantic tagging needed to train the call router) (hemphill et al 1990; <papid> H90-1021 </papid>knight et al 2001; gorin etal, 1997).</citsent>
<aftsection>
<nextsent>choosing to use grammar-based approach also makes the application predictable and relatively easy to design.
</nextsent>
<nextsent>on the other hand, these applications are usually very rigid: the users areal lowed only finite set of ways to input their request sand, by way of consequences, these applications suffer from high out-of-grammar (oog) rates or out of-coverage rates.a few studies have been published comparing grammar-based and slm-based approaches to speech understanding.
</nextsent>
<nextsent>in (knight et al 2001),a comparison of grammar-based and robust approaches is presented for user-initiative home automation application.
</nextsent>
<nextsent>the authors concluded that it was relatively easy to use the corpus collected during the course of the application development totrain slm which would perform better on outof-coverage utterances, while degrading the accuracy on in-coverage utterances.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2634">
<title id=" W06-2112.xml">how bad is the problem of ppattachment a comparison of english german and swedish </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>one of the most frequent ambiguities arises from the attachment of prepositional phrases (pps).
</prevsent>
<prevsent>simply stated, pp that follows noun (in english, german or swedish) can be attached to the noun or to the verb.in the last decade various methods for theres olution of pp attachment ambiguities have been proposed.
</prevsent>
</prevsection>
<citsent citstr=" J93-1005 ">
the seminal paper by (hindle and rooth, 1993) <papid> J93-1005 </papid>started sequence of studies forenglish.</citsent>
<aftsection>
<nextsent>we investigated similar methods forger man (volk, 2001; volk, 2002).<papid> C02-1004 </papid></nextsent>
<nextsent>recently other languages (such as dutch (vandeghinste, 2002) or swedish (aasa, 2004)) have followed.in the pp attachment research for other languages there is often comparison of the disambiguation accuracy with the english results.but are the results really comparable across languages?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2639">
<title id=" W06-2112.xml">how bad is the problem of ppattachment a comparison of english german and swedish </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>simply stated, pp that follows noun (in english, german or swedish) can be attached to the noun or to the verb.in the last decade various methods for theres olution of pp attachment ambiguities have been proposed.
</prevsent>
<prevsent>the seminal paper by (hindle and rooth, 1993) <papid> J93-1005 </papid>started sequence of studies forenglish.</prevsent>
</prevsection>
<citsent citstr=" C02-1004 ">
we investigated similar methods forger man (volk, 2001; volk, 2002).<papid> C02-1004 </papid></citsent>
<aftsection>
<nextsent>recently other languages (such as dutch (vandeghinste, 2002) or swedish (aasa, 2004)) have followed.in the pp attachment research for other languages there is often comparison of the disambiguation accuracy with the english results.but are the results really comparable across languages?
</nextsent>
<nextsent>are we starting from the same baseline when working on pp attachment in structurally similar languages like english, german and swedish?
</nextsent>
<nextsent>is the problem of pp attachment equally bad (equally frequent and of equal balance) for these three languages?
</nextsent>
<nextsent>these are the questions we will discuss in this paper.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2650">
<title id=" W06-2112.xml">how bad is the problem of ppattachment a comparison of english german and swedish </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>interestingly, 586 ofthese triples (67%) were judged as noun attachments and only 33% as verb attachments.
</prevsent>
<prevsent>and(hindle and rooth, 1993) <papid> J93-1005 </papid>reported on 80% attachment accuracy, an improvement of 13% over the baseline (i.e. guessing noun attachment in all 81 cases).</prevsent>
</prevsection>
<citsent citstr=" H94-1048 ">
a year later (ratnaparkhi et al, 1994) <papid> H94-1048 </papid>publish eda supervised approach to the pp attachment prob lem.</citsent>
<aftsection>
<nextsent>they had extracted quadruples v-n-p-n1 (plus the accompanying attachment decision) from both an ibm computer manuals treebank (about 9000 tuples) and from the wall street journal (wsj) section of the penn treebank (about 24,000 tuples).
</nextsent>
<nextsent>the latter tuple set has been reused by subsequent research, so let us focus on this one.2 (ratnaparkhi et al, 1994) <papid> H94-1048 </papid>used 20,801 tuples for training and 3097 tuples for evaluation.</nextsent>
<nextsent>theyre ported on 81.6% correct attachments.but have they solved the same problem as (hin dle and rooth, 1993)?<papid> J93-1005 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2672">
<title id=" W06-2112.xml">how bad is the problem of ppattachment a comparison of english german and swedish </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>if, for instance, the baseline is 60% and the disambiguation result is 80% correct attachments, then we will claim that our disambiguation procedure is useful.
</prevsent>
<prevsent>whereas 1the v-n-p-n quadruples also contain the head noun of the np within the pp.2the ratnaparkhi training and test sets were later distributed together with development set of 4039 v-n-p-n tuples.
</prevsent>
</prevsection>
<citsent citstr=" W95-0103 ">
3it should be noted that important subsequent research, e.g. by (collins and brooks, 1995; <papid> W95-0103 </papid>stetina and nagao, 1997), <papid> W97-0109 </papid>used the ratnaparkhi datasets and thus allowed for good comparability.if we have baseline of 80% and the disambiguation result is 75%, then the procedure can be dis carded.so what are the baselines reported for other languages?</citsent>
<aftsection>
<nextsent>and is it possible to use the same extraction mechanisms for v-n-p-n tuples in order to come to comparable baseline swe did an in-depth study on german pp attachment (volk, 2001).
</nextsent>
<nextsent>we compiled our own treebank by annotating 3000 sentences from the weekly computer journal computerzeitung.
</nextsent>
<nextsent>we had first annotated larger number of subsequent sentences with part-of-speech tags, and based on these pos tags, we selected 3000 sentences that contained at least one full verb plus the sequence of noun followed by preposition.
</nextsent>
<nextsent>after annotating the 3000 sentences with complete syntax treeswe used prolog program to extract v-n-p-n tu ples with the accompanying attachment decisions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2673">
<title id=" W06-2112.xml">how bad is the problem of ppattachment a comparison of english german and swedish </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>if, for instance, the baseline is 60% and the disambiguation result is 80% correct attachments, then we will claim that our disambiguation procedure is useful.
</prevsent>
<prevsent>whereas 1the v-n-p-n quadruples also contain the head noun of the np within the pp.2the ratnaparkhi training and test sets were later distributed together with development set of 4039 v-n-p-n tuples.
</prevsent>
</prevsection>
<citsent citstr=" W97-0109 ">
3it should be noted that important subsequent research, e.g. by (collins and brooks, 1995; <papid> W95-0103 </papid>stetina and nagao, 1997), <papid> W97-0109 </papid>used the ratnaparkhi datasets and thus allowed for good comparability.if we have baseline of 80% and the disambiguation result is 75%, then the procedure can be dis carded.so what are the baselines reported for other languages?</citsent>
<aftsection>
<nextsent>and is it possible to use the same extraction mechanisms for v-n-p-n tuples in order to come to comparable baseline swe did an in-depth study on german pp attachment (volk, 2001).
</nextsent>
<nextsent>we compiled our own treebank by annotating 3000 sentences from the weekly computer journal computerzeitung.
</nextsent>
<nextsent>we had first annotated larger number of subsequent sentences with part-of-speech tags, and based on these pos tags, we selected 3000 sentences that contained at least one full verb plus the sequence of noun followed by preposition.
</nextsent>
<nextsent>after annotating the 3000 sentences with complete syntax treeswe used prolog program to extract v-n-p-n tu ples with the accompanying attachment decisions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2674">
<title id=" W06-2112.xml">how bad is the problem of ppattachment a comparison of english german and swedish </title>
<section> querying treebanks with.  </section>
<citcontext>
<prevsection>
<prevsent>we now present the results for each language in turn.
</prevsent>
<prevsent>3.1 results for english.
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
we used sections 0 to 12 of the wsj part of the penn treebank (marcus et al, 1993) <papid> J93-2004 </papid>with total of 24,618 sentences for our experiments.</citsent>
<aftsection>
<nextsent>our start query reveals that an ambiguously located pp (i.e. noun+pp sequence) occurs in 13,191 (54%) of these sentences, and it occurs total of 20,858 times (a rate of 0.84 occurrences per sentences with respect to all sentences in the treebank).
</nextsent>
<nextsent>searching for noun attachments with the second query described in section 3 we learn that 15,273noun+pp sequences are annotated as noun attachments.
</nextsent>
<nextsent>and we catch another 547 noun attachments if we query for noun phrases that contain two pps in sequence.5 in these cases the second pp is also attached to noun, although not 4there are few occurrences of this latter structure in the penn treebank which should probably count as annotation errors.
</nextsent>
<nextsent>5see (merlo et al, 1997) <papid> W97-0317 </papid>for discussion of these cases and an approach in automatically disambiguating them.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2675">
<title id=" W06-2112.xml">how bad is the problem of ppattachment a comparison of english german and swedish </title>
<section> querying treebanks with.  </section>
<citcontext>
<prevsection>
<prevsent>searching for noun attachments with the second query described in section 3 we learn that 15,273noun+pp sequences are annotated as noun attachments.
</prevsent>
<prevsent>and we catch another 547 noun attachments if we query for noun phrases that contain two pps in sequence.5 in these cases the second pp is also attached to noun, although not 4there are few occurrences of this latter structure in the penn treebank which should probably count as annotation errors.
</prevsent>
</prevsection>
<citsent citstr=" W97-0317 ">
5see (merlo et al, 1997) <papid> W97-0317 </papid>for discussion of these cases and an approach in automatically disambiguating them.</citsent>
<aftsection>
<nextsent>83 figure 1: noun phrase tree from the penn treebank to the noun immediately preceding it (as for example in the tree in figure 1).
</nextsent>
<nextsent>with some similar queries we located another 110 cases of noun attachments (most of which are probably annotation errors if the annotation guidelines are applied strictly).
</nextsent>
<nextsent>this means that we found totalof 15,930 cases of noun attachment which corresponds to noun attachment rate of 76.4% (by comparison to the 20,858 occurrences).
</nextsent>
<nextsent>this is surprisingly high number.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2682">
<title id=" W06-2112.xml">how bad is the problem of ppattachment a comparison of english german and swedish </title>
<section> we regarded pronominal adverbs (darin,.  </section>
<citcontext>
<prevsection>
<prevsent>are based on of-pps.
</prevsent>
<prevsent>the dominance becomes even more obvious if we consider that the following 6this is actually what has been done in some research on english pp attachment disambiguation.
</prevsent>
</prevsection>
<citsent citstr=" P98-2177 ">
(ratnaparkhi, 1998) <papid> P98-2177 </papid>first assumes noun attachment for all of-pps and then applies his disambiguation methods to all remaining pps.</citsent>
<aftsection>
<nextsent>86 prepositions on the frequency ranks are in (with only 485 occurrences) and for (246 occurrences).
</nextsent>
<nextsent>the dominance of the preposition of is so strong in english that we will get totally different picture of attachment preferences if we omit of-pps.
</nextsent>
<nextsent>the ratnaparkhi training set without of-tuples is left with nar of 35% (!)
</nextsent>
<nextsent>and the test set has nar of 42%.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2683">
<title id=" W06-2113.xml">handling of prepositions in english to bengali machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a practical study of the usage of prepositions was carried out for the purpose of teaching english as second language (wahlen, 1995; lind stromberg, 1997; yates, 1999).
</prevsent>
<prevsent>the deictic properties of spatial prepositions have been studied by hill (1982), while the geographical information provided by them was an interest of computational research (xu and badler, 2000; tezuka et al, 2001).
</prevsent>
</prevsection>
<citsent citstr=" P98-2201 ">
in the fields of natural language processing, the problem of pp attachment has been topic for research for quite long time, and in recent years, the problem was explored with neural network-based approach (sopena et al, 1998) <papid> P98-2201 </papid>and with syntax-based trainable approach (yeh and vilain, 1998).</citsent>
<aftsection>
<nextsent>although past research has revealed various aspects of prepositions, there is not much semantic research of prepositions available for computational use, which requires vigorous formalization of representing these mantics.
</nextsent>
<nextsent>a recent semantic study of prepositions for computational use is found in (voss, 2002), with focus on spatial prepositions.
</nextsent>
<nextsent>spatial prepositions are divided into three categories according to which one of the two thematic meanings between place and path they acquire when they are in argument, adjunct and non subcategorized positions of particular types of 89 verbs.
</nextsent>
<nextsent>the semantics of spatial prepositions dealt with in (voss, 2002) is not lexical but thematic.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2684">
<title id=" W06-1646.xml">corrective models for speech recognition of inflected languages </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>n -gram models have long been the stronghold of statistical language modeling approaches.
</prevsent>
<prevsent>within the n-gram paradigm, straightforward approaches for increasing accuracy include using larger training sets and augmenting the contextual information within the n-gram window.
</prevsent>
</prevsection>
<citsent citstr=" P05-1063 ">
incorporating syntactic features into the context has been at the forefront of recent research (collins et al, 2005; <papid> P05-1063 </papid>rosenfeld et al, 2001; chelba and jelinek, 2000; hall and johnson, 2004).<papid> P04-1006 </papid></citsent>
<aftsection>
<nextsent>however, much of the previous work has focused on english language syntax.
</nextsent>
<nextsent>this paper addresses syntax as captured by the inflectional morphology of highly inflected language.high inflection in language is generally correlated with some level of word-order flexibility.
</nextsent>
<nextsent>morphological features either directly identify or help disambiguate the syntactic participants of sentence.
</nextsent>
<nextsent>inflectional morphology works as aproxy for structured syntax in language.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2686">
<title id=" W06-1646.xml">corrective models for speech recognition of inflected languages </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>n -gram models have long been the stronghold of statistical language modeling approaches.
</prevsent>
<prevsent>within the n-gram paradigm, straightforward approaches for increasing accuracy include using larger training sets and augmenting the contextual information within the n-gram window.
</prevsent>
</prevsection>
<citsent citstr=" P04-1006 ">
incorporating syntactic features into the context has been at the forefront of recent research (collins et al, 2005; <papid> P05-1063 </papid>rosenfeld et al, 2001; chelba and jelinek, 2000; hall and johnson, 2004).<papid> P04-1006 </papid></citsent>
<aftsection>
<nextsent>however, much of the previous work has focused on english language syntax.
</nextsent>
<nextsent>this paper addresses syntax as captured by the inflectional morphology of highly inflected language.high inflection in language is generally correlated with some level of word-order flexibility.
</nextsent>
<nextsent>morphological features either directly identify or help disambiguate the syntactic participants of sentence.
</nextsent>
<nextsent>inflectional morphology works as aproxy for structured syntax in language.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2687">
<title id=" W06-1646.xml">corrective models for speech recognition of inflected languages </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>it has long been assumed that incorporating morphological features into language models should help improve the performance of speech recognition systems.
</prevsent>
<prevsent>early models for german showed little improvements over bigram language models and almost no improvement over trigram models (geutner, 1995).
</prevsent>
</prevsection>
<citsent citstr=" P05-2021 ">
more recently, morphology based models have been shown to help reduce error rate for out-of-vocabulary words (carki et al, 2000; podvesky and machek, 2005).<papid> P05-2021 </papid>much of the early work on morphological language modeling was focused on utilizing composite morphological tags, largely due to the difficulty in teasing apart the intricate interdependencies ofthe morphological features.</citsent>
<aftsection>
<nextsent>apart from few exceptions, there has been little work done in exploring the morphological systems of highly inflected languages.
</nextsent>
<nextsent>kirchhoff and colleagues (2004) successfully incorporated morphological features for arabic using factored language model.
</nextsent>
<nextsent>in their approach, morphological inflections are modeled in generative framework, and the space of factored morphological tags is explored using genetic algorithm.
</nextsent>
<nextsent>adopting different tactic, choueiter and colleagues (2006) exploited morphological constraints to prune illegal morpheme sequences from asr output.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2691">
<title id=" W06-1646.xml">corrective models for speech recognition of inflected languages </title>
<section> extract appropriately factored n-gram fea-.  </section>
<citcontext>
<prevsection>
<prevsent>bigram features extracted from an example sentence are illustrated in table 4.the following section describes how the fea 393tures extracted above are modeled in discriminative framework to reduce word error rate.
</prevsent>
<prevsent>4 corrective model and estimation.
</prevsent>
</prevsection>
<citsent citstr=" P05-1022 ">
in this work, we adopt the reranking framework of charniak and johnson (2005) <papid> P05-1022 </papid>for incorporating morphological features.</citsent>
<aftsection>
<nextsent>the model scores each test hypothesis using linear function, v?(y), of features extracted from the hypothesis fj(y) and model parameters , i.e., v?(y) = ? jfj(y).the hypothesis with the highest score is then chosen as the output.
</nextsent>
<nextsent>the model parameters, ?, are learned from training set by maximum entropy estimation of the following conditional model: ? ? yiys:g(yi)=maxjg(yj) p?(yi|ys) here, ys = {yj} is the set of hypotheses for each training utterance and the function returns an extrinsic evaluation score, which in our case is the wer of the hypothesis.
</nextsent>
<nextsent>p?(yi|ys) is modeled by maximum entropy distribution of the form, p?(yi|ys) = exp v?(yi)/ ? exp v?(yj).
</nextsent>
<nextsent>this choice simplifies the numerical estimation procedure since the gradient of the log-likelihood with respect to parameter, say , reduces to difference in expected counts of the associated feature, e?[fj |ys]e?[fj |yi ? ys : g(yi) = maxjg(yj)].to allow good generalization properties, gaussian regularization term is also included in the cost function.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2694">
<title id=" W06-3402.xml">off topic detection in conversational telephone speech </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the two speakers are identified as 1?
</prevsent>
<prevsent>and 2?.third, off-topic detection can be viewed as segmentation of conversation into relevant and irrelevant parts.
</prevsent>
</prevsection>
<citsent citstr=" J97-1003 ">
thus our work has many similarities to topic segmentation systems, which incorporate cue words that indicate an abrupt change in topic (e.g. so anyway...?), as well as long term variations in word occurrence statistics (hearst, 1997; <papid> J97-1003 </papid>reynar, 1999; <papid> P99-1046 </papid>beeferman et al, 1999, e.g.).</citsent>
<aftsection>
<nextsent>our approach uses previous and subsequent sentences to approximate these ideas, but might benefit from more explicitly segmentation-based strategy.
</nextsent>
<nextsent>in our work we use human-transcribed conversations from the fisher data (ldc, 2004).
</nextsent>
<nextsent>in each conversation, participants have been given topic to discuss for ten minutes.
</nextsent>
<nextsent>despite this, participants often talk about subjects that are not at all related to the assigned topic.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2695">
<title id=" W06-3402.xml">off topic detection in conversational telephone speech </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the two speakers are identified as 1?
</prevsent>
<prevsent>and 2?.third, off-topic detection can be viewed as segmentation of conversation into relevant and irrelevant parts.
</prevsent>
</prevsection>
<citsent citstr=" P99-1046 ">
thus our work has many similarities to topic segmentation systems, which incorporate cue words that indicate an abrupt change in topic (e.g. so anyway...?), as well as long term variations in word occurrence statistics (hearst, 1997; <papid> J97-1003 </papid>reynar, 1999; <papid> P99-1046 </papid>beeferman et al, 1999, e.g.).</citsent>
<aftsection>
<nextsent>our approach uses previous and subsequent sentences to approximate these ideas, but might benefit from more explicitly segmentation-based strategy.
</nextsent>
<nextsent>in our work we use human-transcribed conversations from the fisher data (ldc, 2004).
</nextsent>
<nextsent>in each conversation, participants have been given topic to discuss for ten minutes.
</nextsent>
<nextsent>despite this, participants often talk about subjects that are not at all related to the assigned topic.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2696">
<title id=" W06-3402.xml">off topic detection in conversational telephone speech </title>
<section> experimental setup.  </section>
<citcontext>
<prevsection>
<prevsent>to model the effect of proximity to the beginning of the conversation, we attach to each utterance feature that describes its approximate position in theconversation.
</prevsent>
<prevsent>we do not include feature for proximity to the end of the conversation because our transcriptions include only the first ten minutes of each recorded conversation.
</prevsent>
</prevsection>
<citsent citstr=" A92-1021 ">
in order to include features describing verb tense, we use brills part-of-speech tagger (brill, 1992) . <papid> A92-1021 </papid>each part of speech (pos) is taken to be feature, whose value is count of the number of occurrences in the given utterance.</citsent>
<aftsection>
<nextsent>to account for the words, we use bag of words model with counts for each word.
</nextsent>
<nextsent>we normalize words from the human transcripts by converting everything to lowercase and tokenizing contractions 10 features values word tokens for each word, # occurrences standard pos tags as in penn treebank for each tag, # occurrences line number in conversation 0-4, 5-9, 10-19, 20-49,  49 utterance type statement, question, fragment utterance length (number of words) 1, 2, ..., 20,  20 number of laughs laugh count word tokens in previous 5 utterances for each word, total # occurrences in 5 previous tags from pos tagger, previous 5 for each tag, total # occurrences in 5 previous number of words, previous 5 total from 5 previous number of laughs, previous 5 total from 5 previous word tokens, subsequent 5 utterances for each word, total # occ in 5 subsequent tags from pos tagger, subsequent 5 for each tag, total # occurrences in 5 subsequent number of words, subsequent 5 total from 5 subsequent number of laughs, subsequent 5 total from 5 subsequent table 3: summary of features that describe each utterance.and punctuation.
</nextsent>
<nextsent>we rank the utility of words according to the feature quality measure presented in (lewis and gale, 1994) because it was devised for the task of classifying similarly short fragments of text (news headlines), rather than long documents.we then consider the top tokens as features, varying the number in different experiments.
</nextsent>
<nextsent>table 2shows the most useful tokens for distinguishing between the three categories according to this metric.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2697">
<title id=" W06-2201.xml">learning effective surface text patterns for information extraction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the learned surface text patterns are applied in an ontology population algorithm, which not only learns new instances of classes but also new instance pairs of relations.
</prevsent>
<prevsent>we present some rst experiments with these methods.
</prevsent>
</prevsection>
<citsent citstr=" P02-1006 ">
ravichandran and hovy (2002) <papid> P02-1006 </papid>present method to automatically learn surface text patterns expressing relations between instances of classes using search engine.</citsent>
<aftsection>
<nextsent>their method, based on training set, identies natural language surface text patterns that express some relation between two instances.
</nextsent>
<nextsent>for example, was born in?
</nextsent>
<nextsent>proved to be precise pattern expressing the relation between instances mozart (of class person?)
</nextsent>
<nextsent>and 1756 (of class year?).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2698">
<title id=" W06-2201.xml">learning effective surface text patterns for information extraction </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in (agichtein and gravano, 2000), such system is combined with named-entity recognizer.in (craven et al, 2000) an ontology is populated by crawling website.
</prevsent>
<prevsent>based on tagged web pages from other sites, rules are learned to extract information from the website.research on named-entity recognition was addressed in the nineties at the message understanding conferences (chinchor, 1998) and is continued for example in (zhou and su, 2002).
</prevsent>
</prevsection>
<citsent citstr=" A92-1021 ">
automated part of speech tagging (brill, 1992) <papid> A92-1021 </papid>is useful technique in term extraction (frantziet al, 2000), domain closely related to named entity recognition.</citsent>
<aftsection>
<nextsent>here, terms are extracted with predened part-of-speech structure, e.g. an adjective-noun combination.
</nextsent>
<nextsent>in (nenadic?
</nextsent>
<nextsent>et al,2002), methods are discussed to extract information from natural language texts with the use of both part of speech tags and hyponym patterns.
</nextsent>
<nextsent>as referred to in the introduction, ravichandran and hovy (2002) <papid> P02-1006 </papid>present method to identify surface text patterns using web search engine.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2700">
<title id=" W06-2201.xml">learning effective surface text patterns for information extraction </title>
<section> the algorithm.  </section>
<citcontext>
<prevsection>
<prevsent>the instance class relation can be viewed of as hyponym relation.
</prevsent>
<prevsent>we therefore verify the hypothesis of being an instance of ca by go ogling hyponym relation patterns.
</prevsent>
</prevsection>
<citsent citstr=" C92-2082 ">
we use xed set of common patterns expressing the hyponym relation (hearst, 1992; <papid> C92-2082 </papid>cimiano and staab, 2004), see table 1.</citsent>
<aftsection>
<nextsent>for the class names, we use plurals.we use these patterns in the following acceptance function acceptcq(t) := ( ? ph h(p, cq, t) ? n), 4  cq including and   cq for example and   cq like and   cq such as and  table 1: hearst patterns for instance-class relation.
</nextsent>
<nextsent>where h(p, cq, t) is the number of google hits for query with pattern combined with term and the plural form of the class name cq.
</nextsent>
<nextsent>the threshold has to be chosen beforehand.
</nextsent>
<nextsent>we can do so, by calculating the sum of google hits for queries with known instances of the class.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2702">
<title id=" W07-0708.xml">speech input multi target machine translation </title>
<section> multi-target stochastic finite-state.  </section>
<citcontext>
<prevsection>
<prevsent>, pm, q?)
</prevsent>
<prevsent>= 1 2.2 training the multilingual translation model.
</prevsent>
</prevsection>
<citsent citstr=" J04-2004 ">
both topology and parameters of an sfst canbe learned fully automatically from bilingual examples making use of underlying alignment models (casacuberta and vidal, 2004).<papid> J04-2004 </papid></citsent>
<aftsection>
<nextsent>furthermore,a multi-target sfst can be inferred from multilingual set of samples (gonzalez and casacuberta,2006).
</nextsent>
<nextsent>even though in realistic situations multilingual corpora are too scarce, recent works (popovic?
</nextsent>
<nextsent>et al, 2005) show that bilingual corpora covering thesame domain are sufficient to obtain generalized corpora based on which one can subsequently create the required collections of aligned tuples.
</nextsent>
<nextsent>the inference algorithm, giamti (grammatical inference and alignments for multi-target transducer inference), requires multilingual corpus, that is, finite set of multilingual samples (s, t1, . . .
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2703">
<title id=" W07-0708.xml">speech input multi target machine translation </title>
<section> multi-target stochastic finite-state.  </section>
<citcontext>
<prevsection>
<prevsent>m) using labeling function (lm).
</prevsent>
<prevsent>this transformation searches an adequate monotonic segmentation for each of the source-target language pairs on the basis of bilingual alignments such as those given bygiza++ (och, 2000).
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
a monotonic segmentation copes with monotonic alignments, that is,   ? aj   ak following the notation of (brown et al, 1993).<papid> J93-2003 </papid></citsent>
<aftsection>
<nextsent>each source token, which can be either word or phrase (perez et al, 2007), is then joined with target phrase of each language as the corresponding segmentation suggests.
</nextsent>
<nextsent>each extended symbol consists of token from the source language plus zero 57 alignment #0 0:tenperatura 1:minimoa 2:jeitsiko 3:da 0: te mp er at ur as 1: mi ni ma 2: en 3: de sc en so (a) spanish-basque alignment #0 0:low 1:temperatures 2:falling 0: te mp er at ur as 1: mi ni ma 2: en 3: de sc en so (b) spanish-english 0 1temperaturas | tempera tura | nil 2maximas | maxim oak | high temperaturesminimas | minimoak | low temperatures 3en | nil | nil 5descenso | jaitsiko da | fallingascenso | igoko da | rising (c) multi-target sfst from spanish into english and basque.figure 1: example of trilingual alignment over trilingual sentence extracted from the task under consid eration;the related multi-target sfst (with spanish as input, and english and basque as output).
</nextsent>
<nextsent>or more words from each target language in their turn.
</nextsent>
<nextsent>2.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2704">
<title id=" W07-0708.xml">speech input multi target machine translation </title>
<section> experimental results.  </section>
<citcontext>
<prevsection>
<prevsent>in the basque country (located in the north of spain) it has an official status along withspanish.
</prevsent>
<prevsent>however, despite having coexisted for centuries in the same area, they differ greatly both in syntax and in semantics.
</prevsent>
</prevsection>
<citsent citstr=" W04-0407 ">
hence, efforts are being devoted nowadays to machine translation tools involving these two languages (alegria et al, 2004),<papid> W04-0407 </papid>although they are still scarce.</citsent>
<aftsection>
<nextsent>with regard to the order of the phrases within sentence, the most common one in basque is subject plus objects plus verb(even though some alternative structures are also accepted), whereas in spanish and english other constructions such as subject plus verb plus objects are more frequent (see figures 1(a) and 1(b)).
</nextsent>
<nextsent>another difference between basque and spanish or english is that basque is an extremely inflected language.in this experiment we intend to translate spanish speech simultaneously into both basque and english.
</nextsent>
<nextsent>just by having look at the main features of the corpus in table 1, we can realize that there are substantial differences among these three languages, in terms both of the size of the vocabulary and of the amount of running words.
</nextsent>
<nextsent>these figures reveal theagglutinant nature of the basque language in comparison with english or spanish.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2705">
<title id=" W07-0708.xml">speech input multi target machine translation </title>
<section> experimental results.  </section>
<citcontext>
<prevsection>
<prevsent>the underlying reason might be due to the fact that sfst models do not capture properly the rich morphology of the basque as they have to face long-distance reordering issues.
</prevsent>
<prevsent>these differences in the performance of the system when translating into english or into basque have been previously detected in other works (or tiz et al, 2003).
</prevsent>
</prevsection>
<citsent citstr=" W05-0831 ">
in our case, manual review of the models and the obtained translations encourage us to make use of reordering models in future work, since they have proved to report good results in similar framework (kanthak et al, 2005).<papid> W05-0831 </papid></citsent>
<aftsection>
<nextsent>the main contribution of this paper is the proposal of fully embedded architecture for multiple speech translation.
</nextsent>
<nextsent>thus, acoustic models are integrated on the fly into multi-target translation model.
</nextsent>
<nextsent>themost significant feature of this approach is its ability to carry out both the recognition and the translation into multiple languages integrated in unique model.
</nextsent>
<nextsent>due to the finite-state nature of this model,the speech translation engine is based on viterbi like algorithm.in contrast to the mono-target systems, multi target sfsts enable the translation from one source language simultaneously into several target languages with lower computational costs (in termsof space and time) and comparable qualitative results.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2706">
<title id=" W06-3312.xml">post nominal prepositional phrase attachment in proteomics </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>nominal ized relations are particularly frequent, with arguments and adjuncts mentioned in attached pps.
</prevsent>
<prevsent>thus, the tasks of automated search, retrieval, and extraction in this domain stand to benefit significantly from efforts in semantic interpretation of nps and pps.there are currently no publicly available biomedical corpora suitable for this task.
</prevsent>
</prevsection>
<citsent citstr=" W05-1306 ">
(see (cohen et al,2005) <papid> W05-1306 </papid>for an overview of currently available biomedical corpora.)</citsent>
<aftsection>
<nextsent>therefore, statistical approaches that relyon extensive training data are essentially notfeasible.
</nextsent>
<nextsent>instead, we approach the task through careful analysis of the data and development of heuristics.
</nextsent>
<nextsent>in this paper, we report on rule-based post nominal pp attachment system developed as first step toward more general np semantics for pro teomics.
</nextsent>
<nextsent>leroy et al (2002), leroy et al (2003) note the importance ofnoun phrases and prepositions in the capture of relational information in biomedical texts, citing the particular significance of the prepositions by, of, and in.their parser can extract many different relations using few rules by relying on closed-class words (e.g.prepositions) instead of restricting patterns with specific predefined verbs and entities.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2707">
<title id=" W06-3312.xml">post nominal prepositional phrase attachment in proteomics </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>the coverage of pps in our development and test data, comprised of varied texts in proteomics, is even higher with 26% of the text occurring in post nominal pps alone.little research in the biomedical domain addresses the problem of pp attachment proper.
</prevsent>
<prevsent>thisis partly due to the number of systems that process text using named-entity-based templates, disregarding pps.
</prevsent>
</prevsection>
<citsent citstr=" W02-0312 ">
in fact, the only recent bionlp system found in the literature that makes any mention of pp attachment is meds tract (pustejovsky et al, 2002), <papid> W02-0312 </papid>an automated information extraction system for medline abstracts.</citsent>
<aftsection>
<nextsent>the shallow parsing module used in meds tract performs limited?
</nextsent>
<nextsent>prepositional attachment only of prepositions are attached.there are, of course, several pp attachment systems for other domains.
</nextsent>
<nextsent>volk (2001) addresses pp attachment using the frequency of co-occurrence ofa pps preposition, object np, and possible attachment points, calculated from query results of web based search engine.
</nextsent>
<nextsent>this system was evaluated on sentences from weekly computer magazine,scoring 74% accuracy for both vp and np attachment.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2708">
<title id=" W06-3312.xml">post nominal prepositional phrase attachment in proteomics </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>volk (2001) addresses pp attachment using the frequency of co-occurrence ofa pps preposition, object np, and possible attachment points, calculated from query results of web based search engine.
</prevsent>
<prevsent>this system was evaluated on sentences from weekly computer magazine,scoring 74% accuracy for both vp and np attachment.
</prevsent>
</prevsection>
<citsent citstr=" C94-2195 ">
brill &amp; resnik (1994) <papid> C94-2195 </papid>put transformation based learning with added word-class information from wordnet to the task of pp attachment.</citsent>
<aftsection>
<nextsent>their system achieves 81.8% accuracy on sentences from the penn treebank wall street journal corpus.
</nextsent>
<nextsent>the main concerns of both these systems differ from the requirements for successful pp attachment in proteomics.
</nextsent>
<nextsent>the main attachment ambiguity inthese general texts is between vp and np attachment, where there are few nps to choose from forgiven pp.
</nextsent>
<nextsent>in contrast, proteomics texts, where nps are the main information carriers, contain many npswith long sequences of post nominal pps.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2709">
<title id=" W06-3312.xml">post nominal prepositional phrase attachment in proteomics </title>
<section> results &amp; analysis.  </section>
<citcontext>
<prevsection>
<prevsent>for both nominal ization affinity heuristics, the umls specialist lexicon4 is used to determine whether the head noun of each possible attachment point is nominalization.
</prevsent>
<prevsent>the development corpus was compiled from five articles retrieved from pubmed central5 (pmc).
</prevsent>
</prevsection>
<citsent citstr=" W06-3314 ">
the articles were the top-ranked results returned from five separate queries6 using bioki:enzymes, literature navigation tool (bergler et al, 2006).<papid> W06-3314 </papid></citsent>
<aftsection>
<nextsent>sentences containing enzymes were extracted and the remaining sentences were discarded.
</nextsent>
<nextsent>in total, 476sentences yielding 830 post nominal pps were manually annotated as the development corpus.
</nextsent>
<nextsent>attachment accuracy on the development corpus is 88%.
</nextsent>
<nextsent>the accuracy and coverage of each rule is summarized in table 2 and discussed in the following sections.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2710">
<title id=" W06-3814.xml">evaluating and optimizing the parameters of an unsupervised graph based wsd algorithm </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>word sense disambiguation (wsd) is key enabling technology.
</prevsent>
<prevsent>supervised wsd techniques are the best performing in public evaluations, but need large amounts of hand-tagging data.
</prevsent>
</prevsection>
<citsent citstr=" H93-1061 ">
existing hand annotated corpora like semcor (miller et al, 1993), <papid> H93-1061 </papid>which is annotated with wordnet senses (fellbaum, 1998) allow for small improvement over the simple most frequent sense heuristic, as attested in the all words track of the last senseval competition (sny der and palmer, 2004).<papid> W04-0811 </papid></citsent>
<aftsection>
<nextsent>in theory, larger amounts of training data (semcor has approx.
</nextsent>
<nextsent>500m words) would improve the performance of supervised wsd,but no current project exists to provide such an expensive resource.
</nextsent>
<nextsent>supervised wsd is based on the fixed-list of senses?
</nextsent>
<nextsent>paradigm, where the senses for target wordare closed list coming from dictionary or lexicon.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2711">
<title id=" W06-3814.xml">evaluating and optimizing the parameters of an unsupervised graph based wsd algorithm </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>word sense disambiguation (wsd) is key enabling technology.
</prevsent>
<prevsent>supervised wsd techniques are the best performing in public evaluations, but need large amounts of hand-tagging data.
</prevsent>
</prevsection>
<citsent citstr=" W04-0811 ">
existing hand annotated corpora like semcor (miller et al, 1993), <papid> H93-1061 </papid>which is annotated with wordnet senses (fellbaum, 1998) allow for small improvement over the simple most frequent sense heuristic, as attested in the all words track of the last senseval competition (sny der and palmer, 2004).<papid> W04-0811 </papid></citsent>
<aftsection>
<nextsent>in theory, larger amounts of training data (semcor has approx.
</nextsent>
<nextsent>500m words) would improve the performance of supervised wsd,but no current project exists to provide such an expensive resource.
</nextsent>
<nextsent>supervised wsd is based on the fixed-list of senses?
</nextsent>
<nextsent>paradigm, where the senses for target wordare closed list coming from dictionary or lexicon.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2712">
<title id=" W06-3814.xml">evaluating and optimizing the parameters of an unsupervised graph based wsd algorithm </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>typical unsupervised wsd systems involve clustering techniques, which group together similar examples.
</prevsent>
<prevsent>given set of induced clusters (which represent word uses or senses1),each new occurrence of the target word will be compared to the clusters and the most similar cluster will be selected as its sense.
</prevsent>
</prevsection>
<citsent citstr=" W04-2406 ">
most of the unsupervised wsd work has been based on the vector space model (schutze, 1998; pantel and lin, 2002; purandare and pedersen,2004), <papid> W04-2406 </papid>where each example is represented by vector of features (e.g. the words occurring in the context).</citsent>
<aftsection>
<nextsent>recently, veronis (veronis, 2004) has 1unsupervised wsd approaches prefer the term word uses?
</nextsent>
<nextsent>to word senses?.
</nextsent>
<nextsent>in this paper we use them interchangeably to refer to both the induced clusters, and to the word senses from some reference lexicon.
</nextsent>
<nextsent>89proposed hyper lex, an application of graph models to wsd based on the small-world properties of cooccurrence graphs.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2713">
<title id=" W06-3814.xml">evaluating and optimizing the parameters of an unsupervised graph based wsd algorithm </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>besides, hand inspection of the disambiguated occurrences yielded precis ions over 95% (compared to most frequent baseline of 73%) which is an outstanding figure for wsd systems.we noticed that hyper lex had some free parameters and had not been evaluated against public gold standard.
</prevsent>
<prevsent>besides, we were struck by the few works where supervised and unsupervised systems were evaluated on the same test data.
</prevsent>
</prevsection>
<citsent citstr=" W04-0807 ">
in this paper we use an automatic method to map the induced senses to wordnet using hand-tagged corpora, enabling the automatic evaluation against available gold standards (senseval 3 english lexical sample s3ls (mihalcea et al, 2004)) <papid> W04-0807 </papid>and the automatic optimization of the free parameters of the method.</citsent>
<aftsection>
<nextsent>the use of hand-tagged corpora for tagging makes this algorithm mixture of unsupervised and super vised: the method to induce senses incompletely unsupervised, but the mapping is supervised (albeit very straightforward).
</nextsent>
<nextsent>this paper is structured as follows.
</nextsent>
<nextsent>we first present the graph-based algorithm as proposed by veronis, reviewing briefly the features of small world graphs.
</nextsent>
<nextsent>section 3 presents our framework for mapping and evaluating the induced hubs.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2714">
<title id=" W06-3814.xml">evaluating and optimizing the parameters of an unsupervised graph based wsd algorithm </title>
<section> evaluating unsupervised wsd systems.  </section>
<citcontext>
<prevsection>
<prevsent>a third alternative would be to devise method to map the hubs (clusters) returned by the system to the senses in lexicon.
</prevsent>
<prevsent>pantel and lin (2002) automatically map the senses to wordnet, and then measure the quality of the mapping.
</prevsent>
</prevsection>
<citsent citstr=" W05-0605 ">
more recently, the mapping has been used to test the system on publicly available benchmarks (purandare and ped 91 default p180 p1800 p6700 value range best range best range best p1 5 2-3 2 1-3 2 1-3 1 p2 10 3-4 3 2-4 3 2-4 3 p3 0.9 0.7-0.9 0.7 0.5-0.7 0.5 0.3-0.7 0.4 p4 4 4 4 4 4 4 4 p5 6 6-7 6 3-7 3 1-7 1 p6 0.8 0.5-0.8 0.6 0.4-0.8 0.7 0.6-0.95 0.95 p7 0.001 0.0005-0.001 0.0009 0.0005-0.001 0.0009 0.0009-0.003 0.001 table 1: parameters of the hyper lex algorithm ersen, 2004; niu et al, 2005).<papid> W05-0605 </papid></citsent>
<aftsection>
<nextsent>see section 6 for more details on these systems.
</nextsent>
<nextsent>yet another possibility is to evaluate the induced senses against gold standard as clustering task.
</nextsent>
<nextsent>induced senses are clusters, gold standard senses are classes, and measures from the clustering literature like entropy or purity can be used.
</nextsent>
<nextsent>as we wanted to focus on the comparison against standard data-set, we decided to leave aside this otherwise interesting option.in this section we present framework for automatically evaluating unsupervised wsd systems against publicly available hand-tagged corpora.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2723">
<title id=" W06-1652.xml">feature subsumption for opinion analysis </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>approach that throws together variety of features.
</prevsent>
<prevsent>but in many cases adding new types of features does not improve performance.
</prevsent>
</prevsection>
<citsent citstr=" W02-1011 ">
for example, pang et al (2002) <papid> W02-1011 </papid>found that unigrams outperformed bigrams, and unigrams outperformed the combination of unigrams plusbigrams.</citsent>
<aftsection>
<nextsent>our second goal is to automatically identify features that are unnecessary because similar features provide equal or better coverage and discriminatory value.
</nextsent>
<nextsent>our hypothesis is that reduced feature set, which selectively combines unigrams with only the most valuable complex features, will perform better than larger feature set that includes the entire kitchen sink?
</nextsent>
<nextsent>of features.in this paper, we explore the use of subsumption hierarchy to formally define the subsumption relationships between different types of textual features.
</nextsent>
<nextsent>we use the subsumption hierarchy in two ways.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2724">
<title id=" W06-1652.xml">feature subsumption for opinion analysis </title>
<section> the subsumption hierarchy.  </section>
<citcontext>
<prevsection>
<prevsent>extracts the subject of passive-voice instances of recommended?
</prevsent>
<prevsent>as the object being recommended.
</prevsent>
</prevsection>
<citsent citstr=" W03-1014 ">
(riloff and wiebe, 2003) <papid> W03-1014 </papid>explored the idea of using extraction patterns to represent more complex subjective expressions that have non compositional meanings.</citsent>
<aftsection>
<nextsent>for example, the expression drive (someone) up the wall?
</nextsent>
<nextsent>expresses the feeling of being annoyed, but the meanings of the words drive?, up?, and wall?
</nextsent>
<nextsent>have no emotional connotations individually.
</nextsent>
<nextsent>furthermore, this expression is not fixed word sequence that can be adequately modeled by ngrams.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2729">
<title id=" W06-1652.xml">feature subsumption for opinion analysis </title>
<section> datasets.  </section>
<citcontext>
<prevsection>
<prevsent>note that based on the subsumption hierarchy shown in figure 2, all 1grams will always survive the subsumption process because they cannot be subsumed by any other types of features.
</prevsent>
<prevsent>our goal is to identify complex features that are worth adding to set of unigram features.
</prevsent>
</prevsection>
<citsent citstr=" J04-3002 ">
we used three opinion-related datasets for our analyses and experiments: the op dataset created by (wiebe et al, 2004), <papid> J04-3002 </papid>the polarity data set5 created by (pang and lee, 2004), <papid> P04-1035 </papid>and the mpqa dataset created by (wiebe et al, 2005).6 the op and polarity datasets involve document-level opinion classification, while the mpqa dataset involves 5version v2.0, which is available at: http://www.cs.cornell.edu/people/pabo/movie-review-data/ 6available at http://www.cs.pitt.edu/mpqa/databaserelease/ sentence-level classification.</citsent>
<aftsection>
<nextsent>the op data consists of 2,452 documents from the penn treebank (marcus et al, 1993).<papid> J93-2004 </papid></nextsent>
<nextsent>meta data tags assigned by the wall street journal define theopinion/non-opinion classes: the class of any document labeled editorial, letter to the editor, arts &amp; leisure review, or viewpoint by the wall street journal is opinion, and the class of documents in all other categories (such as business and news) is non-opinion.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2730">
<title id=" W06-1652.xml">feature subsumption for opinion analysis </title>
<section> datasets.  </section>
<citcontext>
<prevsection>
<prevsent>note that based on the subsumption hierarchy shown in figure 2, all 1grams will always survive the subsumption process because they cannot be subsumed by any other types of features.
</prevsent>
<prevsent>our goal is to identify complex features that are worth adding to set of unigram features.
</prevsent>
</prevsection>
<citsent citstr=" P04-1035 ">
we used three opinion-related datasets for our analyses and experiments: the op dataset created by (wiebe et al, 2004), <papid> J04-3002 </papid>the polarity data set5 created by (pang and lee, 2004), <papid> P04-1035 </papid>and the mpqa dataset created by (wiebe et al, 2005).6 the op and polarity datasets involve document-level opinion classification, while the mpqa dataset involves 5version v2.0, which is available at: http://www.cs.cornell.edu/people/pabo/movie-review-data/ 6available at http://www.cs.pitt.edu/mpqa/databaserelease/ sentence-level classification.</citsent>
<aftsection>
<nextsent>the op data consists of 2,452 documents from the penn treebank (marcus et al, 1993).<papid> J93-2004 </papid></nextsent>
<nextsent>meta data tags assigned by the wall street journal define theopinion/non-opinion classes: the class of any document labeled editorial, letter to the editor, arts &amp; leisure review, or viewpoint by the wall street journal is opinion, and the class of documents in all other categories (such as business and news) is non-opinion.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2731">
<title id=" W06-1652.xml">feature subsumption for opinion analysis </title>
<section> datasets.  </section>
<citcontext>
<prevsection>
<prevsent>our goal is to identify complex features that are worth adding to set of unigram features.
</prevsent>
<prevsent>we used three opinion-related datasets for our analyses and experiments: the op dataset created by (wiebe et al, 2004), <papid> J04-3002 </papid>the polarity data set5 created by (pang and lee, 2004), <papid> P04-1035 </papid>and the mpqa dataset created by (wiebe et al, 2005).6 the op and polarity datasets involve document-level opinion classification, while the mpqa dataset involves 5version v2.0, which is available at: http://www.cs.cornell.edu/people/pabo/movie-review-data/ 6available at http://www.cs.pitt.edu/mpqa/databaserelease/ sentence-level classification.</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
the op data consists of 2,452 documents from the penn treebank (marcus et al, 1993).<papid> J93-2004 </papid></citsent>
<aftsection>
<nextsent>meta data tags assigned by the wall street journal define theopinion/non-opinion classes: the class of any document labeled editorial, letter to the editor, arts &amp; leisure review, or viewpoint by the wall street journal is opinion, and the class of documents in all other categories (such as business and news) is non-opinion.
</nextsent>
<nextsent>this dataset is highly skewed, with only 9% of the documents belonging to the opinion class.
</nextsent>
<nextsent>consequently, trivial (but useless)opinion classifier that labels all documents as non opinion articles would achieve 91% accuracy.
</nextsent>
<nextsent>the polarity data consists of 700 positive and 700 negative reviews from the internet movie database (imdb) archive.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2732">
<title id=" W06-1652.xml">feature subsumption for opinion analysis </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>for the mpqa data, the improvement over baseline is 447 statistically significant at the   0.10 level.
</prevsent>
<prevsent>many features and classification algorithms have been explored in sentiment analysis and opinionrecognition.
</prevsent>
</prevsection>
<citsent citstr=" W04-3253 ">
lexical cues of differing complexities have been used, including single words and ngrams (e.g., (mullen and collier, 2004; <papid> W04-3253 </papid>pang et al., 2002; <papid> W02-1011 </papid>turney, 2002; <papid> P02-1053 </papid>yu and hatzivassiloglou, 2003; <papid> W03-1017 </papid>wiebe et al, 2004)), <papid> J04-3002 </papid>as well as phrases and lexico-syntactic patterns (e.g, (kim and hovy, 2004; <papid> C04-1200 </papid>hu and liu, 2004; popescu and etzioni, 2005; <papid> H05-1043 </papid>riloff and wiebe, 2003; <papid> W03-1014 </papid>whitelaw et al, 2005)).</citsent>
<aftsection>
<nextsent>while many of these studies investigate combinations of features and feature selection,this is the first work that uses the notion of sub sump tion to compare ngrams and lexico-syntacticpatterns to identify complex features that outperform simpler counterparts and to reduce combined feature set to improve opinion classification.
</nextsent>
<nextsent>this paper uses subsumption hierarchy of feature representations as (1) an analytic tool to compare features of different complexities,and (2) an automatic tool to remove unnecessary features to improve opinion classification performance.
</nextsent>
<nextsent>experiments with three opinion datasets showed that subsumption can improve classification accuracy, especially when combined with feature selection.
</nextsent>
<nextsent>acknowledgments this research was supported by nsf grants iis 0208798 and iis-0208985, the arda aquaintprogram, and the institute for scientific computing research and the center for applied scientific computing within lawrence livermore national laboratory.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2734">
<title id=" W06-1652.xml">feature subsumption for opinion analysis </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>for the mpqa data, the improvement over baseline is 447 statistically significant at the   0.10 level.
</prevsent>
<prevsent>many features and classification algorithms have been explored in sentiment analysis and opinionrecognition.
</prevsent>
</prevsection>
<citsent citstr=" P02-1053 ">
lexical cues of differing complexities have been used, including single words and ngrams (e.g., (mullen and collier, 2004; <papid> W04-3253 </papid>pang et al., 2002; <papid> W02-1011 </papid>turney, 2002; <papid> P02-1053 </papid>yu and hatzivassiloglou, 2003; <papid> W03-1017 </papid>wiebe et al, 2004)), <papid> J04-3002 </papid>as well as phrases and lexico-syntactic patterns (e.g, (kim and hovy, 2004; <papid> C04-1200 </papid>hu and liu, 2004; popescu and etzioni, 2005; <papid> H05-1043 </papid>riloff and wiebe, 2003; <papid> W03-1014 </papid>whitelaw et al, 2005)).</citsent>
<aftsection>
<nextsent>while many of these studies investigate combinations of features and feature selection,this is the first work that uses the notion of sub sump tion to compare ngrams and lexico-syntacticpatterns to identify complex features that outperform simpler counterparts and to reduce combined feature set to improve opinion classification.
</nextsent>
<nextsent>this paper uses subsumption hierarchy of feature representations as (1) an analytic tool to compare features of different complexities,and (2) an automatic tool to remove unnecessary features to improve opinion classification performance.
</nextsent>
<nextsent>experiments with three opinion datasets showed that subsumption can improve classification accuracy, especially when combined with feature selection.
</nextsent>
<nextsent>acknowledgments this research was supported by nsf grants iis 0208798 and iis-0208985, the arda aquaintprogram, and the institute for scientific computing research and the center for applied scientific computing within lawrence livermore national laboratory.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2735">
<title id=" W06-1652.xml">feature subsumption for opinion analysis </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>for the mpqa data, the improvement over baseline is 447 statistically significant at the   0.10 level.
</prevsent>
<prevsent>many features and classification algorithms have been explored in sentiment analysis and opinionrecognition.
</prevsent>
</prevsection>
<citsent citstr=" W03-1017 ">
lexical cues of differing complexities have been used, including single words and ngrams (e.g., (mullen and collier, 2004; <papid> W04-3253 </papid>pang et al., 2002; <papid> W02-1011 </papid>turney, 2002; <papid> P02-1053 </papid>yu and hatzivassiloglou, 2003; <papid> W03-1017 </papid>wiebe et al, 2004)), <papid> J04-3002 </papid>as well as phrases and lexico-syntactic patterns (e.g, (kim and hovy, 2004; <papid> C04-1200 </papid>hu and liu, 2004; popescu and etzioni, 2005; <papid> H05-1043 </papid>riloff and wiebe, 2003; <papid> W03-1014 </papid>whitelaw et al, 2005)).</citsent>
<aftsection>
<nextsent>while many of these studies investigate combinations of features and feature selection,this is the first work that uses the notion of sub sump tion to compare ngrams and lexico-syntacticpatterns to identify complex features that outperform simpler counterparts and to reduce combined feature set to improve opinion classification.
</nextsent>
<nextsent>this paper uses subsumption hierarchy of feature representations as (1) an analytic tool to compare features of different complexities,and (2) an automatic tool to remove unnecessary features to improve opinion classification performance.
</nextsent>
<nextsent>experiments with three opinion datasets showed that subsumption can improve classification accuracy, especially when combined with feature selection.
</nextsent>
<nextsent>acknowledgments this research was supported by nsf grants iis 0208798 and iis-0208985, the arda aquaintprogram, and the institute for scientific computing research and the center for applied scientific computing within lawrence livermore national laboratory.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2737">
<title id=" W06-1652.xml">feature subsumption for opinion analysis </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>for the mpqa data, the improvement over baseline is 447 statistically significant at the   0.10 level.
</prevsent>
<prevsent>many features and classification algorithms have been explored in sentiment analysis and opinionrecognition.
</prevsent>
</prevsection>
<citsent citstr=" C04-1200 ">
lexical cues of differing complexities have been used, including single words and ngrams (e.g., (mullen and collier, 2004; <papid> W04-3253 </papid>pang et al., 2002; <papid> W02-1011 </papid>turney, 2002; <papid> P02-1053 </papid>yu and hatzivassiloglou, 2003; <papid> W03-1017 </papid>wiebe et al, 2004)), <papid> J04-3002 </papid>as well as phrases and lexico-syntactic patterns (e.g, (kim and hovy, 2004; <papid> C04-1200 </papid>hu and liu, 2004; popescu and etzioni, 2005; <papid> H05-1043 </papid>riloff and wiebe, 2003; <papid> W03-1014 </papid>whitelaw et al, 2005)).</citsent>
<aftsection>
<nextsent>while many of these studies investigate combinations of features and feature selection,this is the first work that uses the notion of sub sump tion to compare ngrams and lexico-syntacticpatterns to identify complex features that outperform simpler counterparts and to reduce combined feature set to improve opinion classification.
</nextsent>
<nextsent>this paper uses subsumption hierarchy of feature representations as (1) an analytic tool to compare features of different complexities,and (2) an automatic tool to remove unnecessary features to improve opinion classification performance.
</nextsent>
<nextsent>experiments with three opinion datasets showed that subsumption can improve classification accuracy, especially when combined with feature selection.
</nextsent>
<nextsent>acknowledgments this research was supported by nsf grants iis 0208798 and iis-0208985, the arda aquaintprogram, and the institute for scientific computing research and the center for applied scientific computing within lawrence livermore national laboratory.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2738">
<title id=" W06-1652.xml">feature subsumption for opinion analysis </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>for the mpqa data, the improvement over baseline is 447 statistically significant at the   0.10 level.
</prevsent>
<prevsent>many features and classification algorithms have been explored in sentiment analysis and opinionrecognition.
</prevsent>
</prevsection>
<citsent citstr=" H05-1043 ">
lexical cues of differing complexities have been used, including single words and ngrams (e.g., (mullen and collier, 2004; <papid> W04-3253 </papid>pang et al., 2002; <papid> W02-1011 </papid>turney, 2002; <papid> P02-1053 </papid>yu and hatzivassiloglou, 2003; <papid> W03-1017 </papid>wiebe et al, 2004)), <papid> J04-3002 </papid>as well as phrases and lexico-syntactic patterns (e.g, (kim and hovy, 2004; <papid> C04-1200 </papid>hu and liu, 2004; popescu and etzioni, 2005; <papid> H05-1043 </papid>riloff and wiebe, 2003; <papid> W03-1014 </papid>whitelaw et al, 2005)).</citsent>
<aftsection>
<nextsent>while many of these studies investigate combinations of features and feature selection,this is the first work that uses the notion of sub sump tion to compare ngrams and lexico-syntacticpatterns to identify complex features that outperform simpler counterparts and to reduce combined feature set to improve opinion classification.
</nextsent>
<nextsent>this paper uses subsumption hierarchy of feature representations as (1) an analytic tool to compare features of different complexities,and (2) an automatic tool to remove unnecessary features to improve opinion classification performance.
</nextsent>
<nextsent>experiments with three opinion datasets showed that subsumption can improve classification accuracy, especially when combined with feature selection.
</nextsent>
<nextsent>acknowledgments this research was supported by nsf grants iis 0208798 and iis-0208985, the arda aquaintprogram, and the institute for scientific computing research and the center for applied scientific computing within lawrence livermore national laboratory.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2740">
<title id=" W06-2004.xml">improving name discrimination a language salad approach </title>
<section> discrimination by clustering contexts.  </section>
<citcontext>
<prevsection>
<prevsent>these problems of scarcity can make it difficult to apply these methods and discriminate ambiguous names, especially in languages with fewer online resources.this paper presents method of name discrimination is based on using larger number of contexts in english that include an ambiguous name,and applying information derived from these contexts to the discrimination of that name in another language, where there are many fewer contexts.
</prevsent>
<prevsent>we also show that mixing english contexts with the contexts to be discriminated can result in aperformance improvement over only using the english or the original contexts alone.
</prevsent>
</prevsection>
<citsent citstr=" W97-0322 ">
our method of name discrimination is described inmore detail in (pedersen et al, 2005), but in general is based on an unsupervised approach to word sense discrimination introduced by (purandare and 25 pedersen, 2004), which builds upon earlier work in word sense discrimination, including (schutze, 1998) and (pedersen and bruce, 1997).<papid> W97-0322 </papid>our method treats each occurrence of an ambiguous name as context that is to be clustered with other contexts that also include the same name.</citsent>
<aftsection>
<nextsent>in this paper, each context consists of about 50 words, where the ambiguous name is generally in the middle of the context.
</nextsent>
<nextsent>the goal is to cluster similar contexts together, based on the presumption that the occurrences of name that appear in similar contexts will refer to the same underlying entity.
</nextsent>
<nextsent>this approach is motivated by both the distributional hypothesis (harris, 1968) and the strong contextual hypothesis (miller and charles, 1991).
</nextsent>
<nextsent>2.1 feature selection.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2741">
<title id=" W06-2004.xml">improving name discrimination a language salad approach </title>
<section> discrimination by clustering contexts.  </section>
<citcontext>
<prevsection>
<prevsent>then these contexts are clustered into pre?
</prevsent>
<prevsent>specified number of clusters using the kmeansalgorithm.
</prevsent>
</prevsection>
<citsent citstr=" E06-2007 ">
note that we are currently developing methods to automatically select the number of clusters in the data (e.g., (pedersen and kulkarni, 2006)), <papid> E06-2007 </papid>although we have not yet applied them to this particular work.</citsent>
<aftsection>
<nextsent>in this paper, we explore the creation of second order representation for set of evaluation contexts using three different sets of feature selection data.
</nextsent>
<nextsent>the cooccurrence matrix may be derived from the evaluation contexts themselves, or from separate set of contexts in different language, or from the combination of these two (the salad or mix).
</nextsent>
<nextsent>for example, suppose we have 100 romanian evaluation contexts that include an ambiguous name, and that same name also occurs 10,000 times in an english language corpus.2 our goal is to cluster the 100 romanian contexts, which contain all the information that we have about thename in romanian.
</nextsent>
<nextsent>while we could derive second order representation of the contexts, there sulting cooccurrence matrix would likely be very small and sparse, and insufficient for making good discrimination decisions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2742">
<title id=" W06-2004.xml">improving name discrimination a language salad approach </title>
<section> experimental data.  </section>
<citcontext>
<prevsection>
<prevsent>note that some of these names have different spelling sin some of our languages, so we look for and conflate the native spelling of the names in the different language corpora.
</prevsent>
<prevsent>these pairs were selected because they occur in all four of our languages,and they represent name distinctions that are commonly of interest, that is they represent ambiguity in names of people and places.
</prevsent>
</prevsection>
<citsent citstr=" N03-2023 ">
with these pairs we are also following (nakov and hearst, 2003) <papid> N03-2023 </papid>who suggest that if one is introducing ambiguity by creating pseudo words or conflating names, then these words should be related in some way(in order to avoid the creation of very sharp or obvious sense distinctions).</citsent>
<aftsection>
<nextsent>4.3 discussion.
</nextsent>
<nextsent>for each of the three evaluation languages (bul garian, romanian, and spanish) we have contexts for five different name conflate pairs that we wish to discriminate.
</nextsent>
<nextsent>we have corresponding english contexts for each evaluation language, where the dates of both are approximately the same.
</nextsent>
<nextsent>this temporal consistency between the evaluation language and english is important because the contexts in which name is used may change over time.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2743">
<title id=" W06-2004.xml">improving name discrimination a language salad approach </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>this argument might suggest that first order cooccurrences would be sufficient to discriminate among the names.
</prevsent>
<prevsent>that is, simply group the evaluation contexts based on the features that occur within them, and essentially cluster evaluation 31 contexts based on the number of features they have in common with other evaluation contexts.
</prevsent>
</prevsection>
<citsent citstr=" W04-2406 ">
in fact, results on word sense discrimination (purandareand pedersen, 2004) <papid> W04-2406 </papid>suggest that first order representations are more effective with larger number of context than second order methods.</citsent>
<aftsection>
<nextsent>however, we see examples in these results that suggests thismay not always be the case.
</nextsent>
<nextsent>in the bulgarian results, the largest number of bulgarian contexts are for nato-usa, but the mix performs quite bit better than bulgarian only.
</nextsent>
<nextsent>in the case of romanian, again nato-usa has the largest number of contexts, but the mix still does better than romanian only.
</nextsent>
<nextsent>and in spanish, mexico-india has the largest number of contexts and english-only doesbetter.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2744">
<title id=" W06-1606.xml">spmt statistical machine translation with syntactified target language phrases </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we introduce spmt, new class of statistical translation models that use syn tactified target language phrases.
</prevsent>
<prevsent>the spmt models outperform state of the art phrase-based baseline model by 2.64 bleu points on the nist 2003 chinese-englishtest corpus and 0.28 points on human based quality metric that ranks translations on scale from 1 to 5.
</prevsent>
</prevsection>
<citsent citstr=" W02-1018 ">
during the last four years, various implementations and extent ions to phrase-based statistical models (marcu and wong, 2002; <papid> W02-1018 </papid>koehn et al ,2003; <papid> N03-1017 </papid>och and ney, 2004) <papid> J04-4002 </papid>have led to significant increases in machine translation accuracy.</citsent>
<aftsection>
<nextsent>although phrase-based models yield high-qualitytranslations for language pairs that exhibit similar word order, they fail to produce grammatical outputs for language pairs that are syntactically divergent.
</nextsent>
<nextsent>recent models that exploit syntactic information of the source language (quirk et al , 2005) <papid> P05-1034 </papid>have been shown to produce better outputs than phrase-based systems when evaluated on relatively small scale, domain specific corpora.</nextsent>
<nextsent>and syntax-inspired formal models (chiang, 2005), <papid> P05-1033 </papid>inspite of being trained on significantly less data, have shown promising results when compared onthe same test sets with mature phrase-based systems.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2745">
<title id=" W06-1606.xml">spmt statistical machine translation with syntactified target language phrases </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we introduce spmt, new class of statistical translation models that use syn tactified target language phrases.
</prevsent>
<prevsent>the spmt models outperform state of the art phrase-based baseline model by 2.64 bleu points on the nist 2003 chinese-englishtest corpus and 0.28 points on human based quality metric that ranks translations on scale from 1 to 5.
</prevsent>
</prevsection>
<citsent citstr=" N03-1017 ">
during the last four years, various implementations and extent ions to phrase-based statistical models (marcu and wong, 2002; <papid> W02-1018 </papid>koehn et al ,2003; <papid> N03-1017 </papid>och and ney, 2004) <papid> J04-4002 </papid>have led to significant increases in machine translation accuracy.</citsent>
<aftsection>
<nextsent>although phrase-based models yield high-qualitytranslations for language pairs that exhibit similar word order, they fail to produce grammatical outputs for language pairs that are syntactically divergent.
</nextsent>
<nextsent>recent models that exploit syntactic information of the source language (quirk et al , 2005) <papid> P05-1034 </papid>have been shown to produce better outputs than phrase-based systems when evaluated on relatively small scale, domain specific corpora.</nextsent>
<nextsent>and syntax-inspired formal models (chiang, 2005), <papid> P05-1033 </papid>inspite of being trained on significantly less data, have shown promising results when compared onthe same test sets with mature phrase-based systems.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2746">
<title id=" W06-1606.xml">spmt statistical machine translation with syntactified target language phrases </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we introduce spmt, new class of statistical translation models that use syn tactified target language phrases.
</prevsent>
<prevsent>the spmt models outperform state of the art phrase-based baseline model by 2.64 bleu points on the nist 2003 chinese-englishtest corpus and 0.28 points on human based quality metric that ranks translations on scale from 1 to 5.
</prevsent>
</prevsection>
<citsent citstr=" J04-4002 ">
during the last four years, various implementations and extent ions to phrase-based statistical models (marcu and wong, 2002; <papid> W02-1018 </papid>koehn et al ,2003; <papid> N03-1017 </papid>och and ney, 2004) <papid> J04-4002 </papid>have led to significant increases in machine translation accuracy.</citsent>
<aftsection>
<nextsent>although phrase-based models yield high-qualitytranslations for language pairs that exhibit similar word order, they fail to produce grammatical outputs for language pairs that are syntactically divergent.
</nextsent>
<nextsent>recent models that exploit syntactic information of the source language (quirk et al , 2005) <papid> P05-1034 </papid>have been shown to produce better outputs than phrase-based systems when evaluated on relatively small scale, domain specific corpora.</nextsent>
<nextsent>and syntax-inspired formal models (chiang, 2005), <papid> P05-1033 </papid>inspite of being trained on significantly less data, have shown promising results when compared onthe same test sets with mature phrase-based systems.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2747">
<title id=" W06-1606.xml">spmt statistical machine translation with syntactified target language phrases </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>during the last four years, various implementations and extent ions to phrase-based statistical models (marcu and wong, 2002; <papid> W02-1018 </papid>koehn et al ,2003; <papid> N03-1017 </papid>och and ney, 2004) <papid> J04-4002 </papid>have led to significant increases in machine translation accuracy.</prevsent>
<prevsent>although phrase-based models yield high-qualitytranslations for language pairs that exhibit similar word order, they fail to produce grammatical outputs for language pairs that are syntactically divergent.</prevsent>
</prevsection>
<citsent citstr=" P05-1034 ">
recent models that exploit syntactic information of the source language (quirk et al , 2005) <papid> P05-1034 </papid>have been shown to produce better outputs than phrase-based systems when evaluated on relatively small scale, domain specific corpora.</citsent>
<aftsection>
<nextsent>and syntax-inspired formal models (chiang, 2005), <papid> P05-1033 </papid>inspite of being trained on significantly less data, have shown promising results when compared onthe same test sets with mature phrase-based systems.</nextsent>
<nextsent>to our knowledge though, no previous research has demonstrated that syntax-based statistical translation system could produce better results than phrase-based system on large-scale, well-established, open domain translation task.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2748">
<title id=" W06-1606.xml">spmt statistical machine translation with syntactified target language phrases </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>although phrase-based models yield high-qualitytranslations for language pairs that exhibit similar word order, they fail to produce grammatical outputs for language pairs that are syntactically divergent.
</prevsent>
<prevsent>recent models that exploit syntactic information of the source language (quirk et al , 2005) <papid> P05-1034 </papid>have been shown to produce better outputs than phrase-based systems when evaluated on relatively small scale, domain specific corpora.</prevsent>
</prevsection>
<citsent citstr=" P05-1033 ">
and syntax-inspired formal models (chiang, 2005), <papid> P05-1033 </papid>inspite of being trained on significantly less data, have shown promising results when compared onthe same test sets with mature phrase-based systems.</citsent>
<aftsection>
<nextsent>to our knowledge though, no previous research has demonstrated that syntax-based statistical translation system could produce better results than phrase-based system on large-scale, well-established, open domain translation task.
</nextsent>
<nextsent>in this paper we present such system.
</nextsent>
<nextsent>our translation models rely upon and naturally exploit sub models (feature functions) that have been initially developed in phrase-based systems for choosing target translations of source language phrases, and use new, syntax-based translation and target language sub models for assembling target phrases into well-formed, grammatical outputs.
</nextsent>
<nextsent>after we introduce our models intuitively, we discuss their formal underpinning and parameter training in section 2.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2750">
<title id=" W06-1606.xml">spmt statistical machine translation with syntactified target language phrases </title>
<section> spmt: statistical machine translation.  </section>
<citcontext>
<prevsection>
<prevsent>= (pi, f,a).
</prevsent>
<prevsent>the probability of each derivation is given by the product of the probabilities of all the rules p(rj) in the derivation (see equation 4).
</prevsent>
</prevsection>
<citsent citstr=" J03-4003 ">
pr(pi, f,a) = ? i??,c(?)=(pi,f,a) ? rji p(rj) (4) in order to acquire the rules specific to our model and to induce their probabilities, we parse the english side of our corpus with an in-houseimplementation (soricut, 2005) of collins parsing models (collins, 2003) <papid> J03-4003 </papid>and we word-align the parallel corpus with the giza++2 implementation of the ibm models (brown et al , 1993).<papid> J93-2003 </papid></citsent>
<aftsection>
<nextsent>we use the automatically derived english-parse-tree,english-sentence, foreign-sentence, word-level alignment?
</nextsent>
<nextsent>tuples in order to induce xrs rules for several models.
</nextsent>
<nextsent>2.2.2 spmt model 1 in our simplest model, we assume that each tuple (pi, f,a) in our automatically annotated corpus could be produced by applying combination of minimally syntactified, lexicalized,phrase-based compatible xrs rules, and mini mal/necessary, non-lexicalized xrs rules.
</nextsent>
<nextsent>we call rule non-lexicalized whenever it does not have any directly aligned source-to-target words.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2751">
<title id=" W06-1606.xml">spmt statistical machine translation with syntactified target language phrases </title>
<section> spmt: statistical machine translation.  </section>
<citcontext>
<prevsection>
<prevsent>= (pi, f,a).
</prevsent>
<prevsent>the probability of each derivation is given by the product of the probabilities of all the rules p(rj) in the derivation (see equation 4).
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
pr(pi, f,a) = ? i??,c(?)=(pi,f,a) ? rji p(rj) (4) in order to acquire the rules specific to our model and to induce their probabilities, we parse the english side of our corpus with an in-houseimplementation (soricut, 2005) of collins parsing models (collins, 2003) <papid> J03-4003 </papid>and we word-align the parallel corpus with the giza++2 implementation of the ibm models (brown et al , 1993).<papid> J93-2003 </papid></citsent>
<aftsection>
<nextsent>we use the automatically derived english-parse-tree,english-sentence, foreign-sentence, word-level alignment?
</nextsent>
<nextsent>tuples in order to induce xrs rules for several models.
</nextsent>
<nextsent>2.2.2 spmt model 1 in our simplest model, we assume that each tuple (pi, f,a) in our automatically annotated corpus could be produced by applying combination of minimally syntactified, lexicalized,phrase-based compatible xrs rules, and mini mal/necessary, non-lexicalized xrs rules.
</nextsent>
<nextsent>we call rule non-lexicalized whenever it does not have any directly aligned source-to-target words.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2752">
<title id=" W06-1606.xml">spmt statistical machine translation with syntactified target language phrases </title>
<section> spmt: statistical machine translation.  </section>
<citcontext>
<prevsection>
<prevsent>it is worth mentioning that, in our framework, rule is defined to be minimal?
</prevsent>
<prevsent>with respect to aforeign/source language phrase, i.e., it is the minimal xrs rule that yields that source phrase.
</prevsent>
</prevsection>
<citsent citstr=" N04-1035 ">
in contrast, in the work of galley et al  (2004), <papid> N04-1035 </papid>galley et al  (2006), <papid> P06-1121 </papid>rule is defined to be minimal when it is necessary in order to explain (pi, f,a) tuple.</citsent>
<aftsection>
<nextsent>under spmt model 1, the tree in figure 2 canbe produced, for example, by the following deriva tion: r4(r9(r7), r3(r6(r12(r8)))).
</nextsent>
<nextsent>2.2.3 spmt model 1 composed we hypothesize that composed rules, i.e., rules that can be decomposed via the application of asequence of model 1 rules may improve the performance of an spmt system.
</nextsent>
<nextsent>for example, although the minimal model 1 rules r11 and r13 are 46 figure 3: problematic syntactifications of phrasal translations.
</nextsent>
<nextsent>sufficient for building an english np on top of two nps separated by the chinese conjunction and, the composed rule r14 in figure 1 accomplishes the same result in only one step.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2753">
<title id=" W06-1606.xml">spmt statistical machine translation with syntactified target language phrases </title>
<section> spmt: statistical machine translation.  </section>
<citcontext>
<prevsection>
<prevsent>it is worth mentioning that, in our framework, rule is defined to be minimal?
</prevsent>
<prevsent>with respect to aforeign/source language phrase, i.e., it is the minimal xrs rule that yields that source phrase.
</prevsent>
</prevsection>
<citsent citstr=" P06-1121 ">
in contrast, in the work of galley et al  (2004), <papid> N04-1035 </papid>galley et al  (2006), <papid> P06-1121 </papid>rule is defined to be minimal when it is necessary in order to explain (pi, f,a) tuple.</citsent>
<aftsection>
<nextsent>under spmt model 1, the tree in figure 2 canbe produced, for example, by the following deriva tion: r4(r9(r7), r3(r6(r12(r8)))).
</nextsent>
<nextsent>2.2.3 spmt model 1 composed we hypothesize that composed rules, i.e., rules that can be decomposed via the application of asequence of model 1 rules may improve the performance of an spmt system.
</nextsent>
<nextsent>for example, although the minimal model 1 rules r11 and r13 are 46 figure 3: problematic syntactifications of phrasal translations.
</nextsent>
<nextsent>sufficient for building an english np on top of two nps separated by the chinese conjunction and, the composed rule r14 in figure 1 accomplishes the same result in only one step.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2754">
<title id=" W06-1606.xml">spmt statistical machine translation with syntactified target language phrases </title>
<section> spmt: statistical machine translation.  </section>
<citcontext>
<prevsection>
<prevsent>lm(e) is the language model probability ofthe target translation under an ngram language model.
</prevsent>
<prevsent>wp(e) is word penalty model designed to favor longer translations.all these models are combined log-linearly during decoding.
</prevsent>
</prevsection>
<citsent citstr=" P03-1021 ">
the weights of the models are computed automatically using variant of the maximum bleu training procedure proposed by och (2003).<papid> P03-1021 </papid></citsent>
<aftsection>
<nextsent>the phrase-based-like sub models have been proved useful in phrase-based approaches tosmt (och and ney, 2004).<papid> J04-4002 </papid></nextsent>
<nextsent>the first two syntax based sub models implement fused?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2756">
<title id=" W06-1606.xml">spmt statistical machine translation with syntactified target language phrases </title>
<section> decoding.  </section>
<citcontext>
<prevsection>
<prevsent>3.1 decoding with one spmt model.
</prevsent>
<prevsent>we decode with each of our spmt models using straightforward, bottom-up, cky-style decoder that builds english syntactic constituents on thetop of chinese sentences.
</prevsent>
</prevsection>
<citsent citstr=" N06-1033 ">
the decoder uses bina rized representation of the rules, which is obtained via syncronous binarization procedure (zhang et al ., 2006).<papid> N06-1033 </papid></citsent>
<aftsection>
<nextsent>the cky-style decoder computes the probability of english syntactic constituents in bottom up fashion, by log-linearly interpol ating all the sub model scores described in section 2.3.
</nextsent>
<nextsent>the decoder is capable of producing nbest derivations and nbest lists (knight and graehl,2005), which are used for maximum bleu training (och, 2003).<papid> P03-1021 </papid></nextsent>
<nextsent>when decoding the test corpus, the decoder returns the translation that has the most probable derivation; in other words, the sum operator in equation 4 is replaced with an argmax.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2759">
<title id=" W06-1606.xml">spmt statistical machine translation with syntactified target language phrases </title>
<section> decoding.  </section>
<citcontext>
<prevsection>
<prevsent>when decoding the test corpus, the decoder returns the translation that has the most probable derivation; in other words, the sum operator in equation 4 is replaced with an argmax.
</prevsent>
<prevsent>3.2 decoding with multiple spmt models.
</prevsent>
</prevsection>
<citsent citstr=" E06-1005 ">
combining multiple mt outputs to increase performance is, in general, difficult task (matusov et al , 2006) <papid> E06-1005 </papid>when significantly different engines compete for producing the best outputs.</citsent>
<aftsection>
<nextsent>in our case, combining multiple mt outputs is much simpler because the sub model probabilities across the four models described here are mostly iden tifical, with the exception of the root normalized and cfg-like sub models which are scaled differently ? since model 2 composed has, for example, more rules than model 1, the root normalized and cfg-like sub models have smaller probabilities for identical rules in model 2 composed than in model 1.
</nextsent>
<nextsent>we compare these two probabilities across the.
</nextsent>
<nextsent>sub models and we scale all model probabilities to be compatible with those of model 2 composed.with this scaling procedure into place, we produce 6,000 non-unique nbest lists for all sentence sin our development corpus, using all spmt sub models.
</nextsent>
<nextsent>we concatenate the lists and we learn new combination of weights that maximizes the bleu score of the combined nbest list using the same development corpus we used for tuning the individual systems (och, 2003).<papid> P03-1021 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2770">
<title id=" W06-1621.xml">lexical reference a semantic matching subtask </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>for example, many qa systems perform expansion in the retrieval phase using query related words based on wordnets lexical relations such as synonymy or hyponymy (e.g(harabagiu et al , 2000; hovy et al , 2001)).
</prevsent>
<prevsent>lexical similarity measures (e.g.
</prevsent>
</prevsection>
<citsent citstr=" P98-2127 ">
(lin, 1998)) <papid> P98-2127 </papid>have also been suggested to measure semantic similarity.</citsent>
<aftsection>
<nextsent>they are based on the distributional hypothesis, suggesting that words that occur within similar contexts are semantically similar.
</nextsent>
<nextsent>2.2 textual entailment.
</nextsent>
<nextsent>the recognising textual entailment (rte-1) challenge (dagan et al , 2006) is an attempt to promote an abstract generic task that captures major semantic inference needs across applications.
</nextsent>
<nextsent>the task requires to recognize, given two text fragments, whether the meaning of one text can be inferred (entailed) from another text.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2771">
<title id=" W06-1621.xml">lexical reference a semantic matching subtask </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>different techniques and heuristics were applied on the rte-1 dataset to specifically model textual entailment.
</prevsent>
<prevsent>interestingly, number of works (e.g.
</prevsent>
</prevsection>
<citsent citstr=" H05-1079 ">
(bos and markert, 2005; <papid> H05-1079 </papid>corley and mihalcea, 2005; <papid> W05-1203 </papid>jijkoun and de rijke, 2005; glickman et al , 2006)) applied or utilized lexical based word overlap measures.</citsent>
<aftsection>
<nextsent>various word-to-word similarity measures where applied, including distributional similarity (such as (lin, 1998)), <papid> P98-2127 </papid>web-based co-occurrence statistics and wordnet based similarity measures (such as (leacock et al , 1998)).<papid> J98-1006 </papid></nextsent>
<nextsent>2.3 paraphrase acquisition.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2772">
<title id=" W06-1621.xml">lexical reference a semantic matching subtask </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>different techniques and heuristics were applied on the rte-1 dataset to specifically model textual entailment.
</prevsent>
<prevsent>interestingly, number of works (e.g.
</prevsent>
</prevsection>
<citsent citstr=" W05-1203 ">
(bos and markert, 2005; <papid> H05-1079 </papid>corley and mihalcea, 2005; <papid> W05-1203 </papid>jijkoun and de rijke, 2005; glickman et al , 2006)) applied or utilized lexical based word overlap measures.</citsent>
<aftsection>
<nextsent>various word-to-word similarity measures where applied, including distributional similarity (such as (lin, 1998)), <papid> P98-2127 </papid>web-based co-occurrence statistics and wordnet based similarity measures (such as (leacock et al , 1998)).<papid> J98-1006 </papid></nextsent>
<nextsent>2.3 paraphrase acquisition.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2774">
<title id=" W06-1621.xml">lexical reference a semantic matching subtask </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>interestingly, number of works (e.g.
</prevsent>
<prevsent>(bos and markert, 2005; <papid> H05-1079 </papid>corley and mihalcea, 2005; <papid> W05-1203 </papid>jijkoun and de rijke, 2005; glickman et al , 2006)) applied or utilized lexical based word overlap measures.</prevsent>
</prevsection>
<citsent citstr=" J98-1006 ">
various word-to-word similarity measures where applied, including distributional similarity (such as (lin, 1998)), <papid> P98-2127 </papid>web-based co-occurrence statistics and wordnet based similarity measures (such as (leacock et al , 1998)).<papid> J98-1006 </papid></citsent>
<aftsection>
<nextsent>2.3 paraphrase acquisition.
</nextsent>
<nextsent>a substantial body of work has been dedicated to learning patterns of semantic equivalency between different language expressions, typically considered as paraphrases.
</nextsent>
<nextsent>recently, several works addressed the task of acquiring paraphrases (semi-) automatically from corpora.
</nextsent>
<nextsent>most attempts were based on identifying corresponding sentences in parallel or comparable?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2775">
<title id=" W06-1621.xml">lexical reference a semantic matching subtask </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>most attempts were based on identifying corresponding sentences in parallel or comparable?
</prevsent>
<prevsent>corpora, where each corpus is known to include texts that largely correspond to texts in another corpus (e.g.
</prevsent>
</prevsection>
<citsent citstr=" P01-1008 ">
(barzilay and mckeown, 2001)).<papid> P01-1008 </papid></citsent>
<aftsection>
<nextsent>distributional similarity was also used to identify paraphrase patterns from single corpus rather than from comparable set of corpora (lin and pantel, 2001).
</nextsent>
<nextsent>similarly, (glickman and dagan, 2004) developed statistical methods that match verb paraphrases within regular corpus.
</nextsent>
<nextsent>3.1 motivation and definition.
</nextsent>
<nextsent>one of the major observations of the 1st recognizing textual entailment (rte-1) challenge referred to the rich structure of entailment modeling systems and the need to evaluate and optimize individual components within them.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2777">
<title id=" W06-1659.xml">unsupervised information extraction approach using graph mutual reinforcement </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>most of the previous work on information extraction (ie) focused on supervised learning.
</prevsent>
<prevsent>relation detection and characterization (rdc) was introduced in the automatic content extraction program (ace) (ace, 2004).
</prevsent>
</prevsection>
<citsent citstr=" P04-3022 ">
the approaches proposed to the ace rdc task such as kernel methods (zelenko et al, 2002) and maximum entropy methods (kambhatla, 2004) <papid> P04-3022 </papid>required the availability of large set of human annotated corpora which are tagged with relation instances.</citsent>
<aftsection>
<nextsent>however human annotated instances are limited, expensive, and time consuming to obtain, due to the lack of experienced human annotators and the low inter-annotator agreements.
</nextsent>
<nextsent>some previous work adopted weakly supervised or unsupervised learning approaches.
</nextsent>
<nextsent>these approaches have the advantage of not needing large tagged corpora but need seed examples or seed extraction patterns.
</nextsent>
<nextsent>the major drawback of these approaches is their dependency on seed examples or seed patterns which may lead to limited generalization due to dependency on handcrafted examples.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2778">
<title id=" W06-1659.xml">unsupervised information extraction approach using graph mutual reinforcement </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>they exploit the fact that, for some problems, each example can be described by multiple representations.
</prevsent>
<prevsent>(riloff &amp; jones, 1999) presented the meta bootstrapping algorithm that uses an unannotated training dataset and set of seeds to learn dictionary of extraction patterns and domain specific semantic lexicon.
</prevsent>
</prevsection>
<citsent citstr=" W02-1028 ">
other works tried to exploit the duality of patterns and their extractions for the purpose of inferring these mantic class of words like (thelen &amp; riloff, 2002) <papid> W02-1028 </papid>and (lin et al 2003).</citsent>
<aftsection>
<nextsent>(muslea et al, 1999) introduced an inductive algorithm to generate extraction rules based on user labeled training examples.
</nextsent>
<nextsent>this approach suffers from the labeled data bottleneck.
</nextsent>
<nextsent>(agichtein et. al, 2000) presented an approach using seed examples to generate initial patterns and to iteratively obtain further patterns.
</nextsent>
<nextsent>then ad-hoc measures were deployed to estimate the relevancy of the patterns that have been newly obtained.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2779">
<title id=" W06-1659.xml">unsupervised information extraction approach using graph mutual reinforcement </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>then ad-hoc measures were deployed to estimate the relevancy of the patterns that have been newly obtained.
</prevsent>
<prevsent>the major drawbacks of this approach are: its dependency on seed examples leads to limited capability of generalization, and the estimation of patterns relevancy requires the deployment of ad-hoc measures.
</prevsent>
</prevsection>
<citsent citstr=" P04-1053 ">
(hasegawa et. al. 2004) <papid> P04-1053 </papid>introduced unsupervised approach for relation extraction depending on clustering context words between named enti ties; this approach depends on ad-hoc context similarity between phrases in the context and focused on certain types of relations.</citsent>
<aftsection>
<nextsent>(etzioni et al 2005) proposed system for building lists of named entities found on the web.
</nextsent>
<nextsent>their system uses set of eight domain independent extraction patterns to generate candidate facts.
</nextsent>
<nextsent>all approaches, proposed so far, suffer from either requiring large amount of labeled data or the dependency on seed patterns (or examples) that result in limited generalization.
</nextsent>
<nextsent>in graph theory, graph is set of objects called vertices joined by links called edges.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2780">
<title id=" W06-1659.xml">unsupervised information extraction approach using graph mutual reinforcement </title>
<section> the approach.  </section>
<citcontext>
<prevsection>
<prevsent>we use the similarity p p t t t p t patterns
</prevsent>
<prevsent>tuples
</prevsent>
</prevsection>
<citsent citstr=" P94-1019 ">
figure 2: bipartite graph representing patterns and tuples 504 measure described in (wu and palmer, 1994) <papid> P94-1019 </papid>which finds the path length to the root node from the least common subsumer (lcs) of the two word senses which is the most specific word sense they share as an ancestor.</citsent>
<aftsection>
<nextsent>the similarity score of two tuples, st, is calculated as follows: 2 2 2 1 eet sss += (5).
</nextsent>
<nextsent>where se1, and se2 are the similarity scores of the first entities in the two tuples, and their second entitles respectively.
</nextsent>
<nextsent>the tuple matching procedure assigns similarity measure to each pair of tuples in the dataset.
</nextsent>
<nextsent>using this measure we can construct an undirected graph g. the vertices of are the tuples.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2782">
<title id=" W06-3807.xml">learning of graph based question answering rules </title>
<section> question answering rules.  </section>
<citcontext>
<prevsection>
<prevsent>the extension graph contains information that simulates the difference between question sentence and sentence containing an answer.
</prevsent>
<prevsent>for example, lets us use graph representations of syntactic dependency structures.
</prevsent>
</prevsection>
<citsent citstr=" A97-1011 ">
we will base our representation on the output of conn exor (tapanainen and jarvinen, 1997), <papid> A97-1011 </papid>but the choice of parser is arbitrary.</citsent>
<aftsection>
<nextsent>the same method applies to the output of any parser, as long as it can be represented as graph.
</nextsent>
<nextsent>in our choice, the dependency structure is represented as bipartite graph where the lexical entries are the vertices represented in boxes and the dependency labels are the vertices represented in ovals.
</nextsent>
<nextsent>figure 2 shows the graphs of question and an answer sentence, and an extension of the question graph.
</nextsent>
<nextsent>the answer is shown in thick lines, and the extension is shown in dashed lines.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2783">
<title id=" W06-3807.xml">learning of graph based question answering rules </title>
<section> application: qa with logical graphs.  </section>
<citcontext>
<prevsection>
<prevsent>the weight w(r) of rule is computed according to its precision on the training set: w(r) = # correct answers found # answers found
</prevsent>
<prevsent>the above method has been applied to graphs representing the logical contents of sentences.
</prevsent>
</prevsection>
<citsent citstr=" P79-1010 ">
there has been long tradition on the use of graphs for this kind of sentence representation, such as sowasconceptual graphs (sowa, 1979), <papid> P79-1010 </papid>and quill ians semantic nets (quillian, 1968).</citsent>
<aftsection>
<nextsent>in our particular experiment we have used graph representation thatcan be built automatically and that can be used efficiently for qa (molla?
</nextsent>
<nextsent>and van zaanen, 2006).
</nextsent>
<nextsent>a logical graph (lg) is directed, bipartitegraph with two types of vertices, concepts andre lations.concepts examples of concepts are objects dog, table, events and states run, love, and properties red, quick.
</nextsent>
<nextsent>relations relations act as links between concepts.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2786">
<title id=" W06-3807.xml">learning of graph based question answering rules </title>
<section> related research.  </section>
<citcontext>
<prevsection>
<prevsent>given that the qa system does not do any kind of question classification and itdoes not use any ne recogniser, the results are satisfactory.
</prevsent>
<prevsent>there have been other attempts to learn qa rules automatically.
</prevsent>
</prevsection>
<citsent citstr=" P02-1006 ">
for example, ravichandran and hovy (2002) <papid> P02-1006 </papid>learns rules based on simple surface patterns.</citsent>
<aftsection>
<nextsent>given that surface patterns ignore much linguistic information, it becomes necessary to gather large corpus of questions together with their answers and sentences containing the answers.
</nextsent>
<nextsent>to obtain such corpus ravichandran and hovy (2002) <papid> P02-1006 </papid>mine the web to gather the relevant data.</nextsent>
<nextsent>other methods learn patterns based on syntacticinformation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2790">
<title id=" W06-3807.xml">learning of graph based question answering rules </title>
<section> related research.  </section>
<citcontext>
<prevsection>
<prevsent>other methods learn patterns based on syntacticinformation.
</prevsent>
<prevsent>for example, shen et al (2005) develop method of extracting dependency paths connecting answers with words found in the question.however we are not aware of any method that attempts to learn patterns based on logical information, other than our own.
</prevsent>
</prevsection>
<citsent citstr=" P04-3020 ">
there is recent interest on the use of graph methods for natural language processing, suchas document summarisation (mihalcea, 2004) <papid> P04-3020 </papid>document retrieval (montes-y-gomez et al, 2000; mishne, 2004), and recognition of textual entailment (pazienza et al, 2005).</citsent>
<aftsection>
<nextsent>the present very workshop shows the current interest on the area.
</nextsent>
<nextsent>however, we are not aware of any significant research about the use of conceptual graphs (or any other form of graph representation) for question answering other than our own.
</nextsent>
<nextsent>we have presented method to learn question answering rules by applying graph manipulation methods on the representations of questions and answer sentences.
</nextsent>
<nextsent>the method is independent of the actual graph representation formalism.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2793">
<title id=" W06-3305.xml">a priority model for named entities </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the priority model achieves an f-measure of 0.958-0.960, consistently higher than the statistical language model and probabilistic context-free grammar.
</prevsent>
<prevsent>automatic recognition of gene and protein names is challenging first step towards text mining the biomedical literature.
</prevsent>
</prevsection>
<citsent citstr=" W05-1306 ">
advances in the area of gene and protein named entity recognition (ner) have been accelerated by freely available tagged corpora (kim et al, 2003, cohen et al, 2005, <papid> W05-1306 </papid>smith et al, 2005, <papid> W05-1305 </papid>tanabe et al, 2005).</citsent>
<aftsection>
<nextsent>such corpora have made it possible for standardized evaluations such as task 1a of the first bio creative workshop (yeh et al, 2005).
</nextsent>
<nextsent>although state-of-the-art systems now perform at the level of 80-83% f-measure, this is still well below the range of 90-97% for non-biomedical ner.
</nextsent>
<nextsent>the main reasons for this performance disparity are 1) the complexity of the genetic nomenclature and 2) the confusion of gene and protein names with other biomedical entities, as well as with common english words.
</nextsent>
<nextsent>in an effort to alleviate the confusion with other biomedical entities we have assembled database consisting of named entities appearing in the literature of biomedicine together with information on their onto logical categories.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2794">
<title id=" W06-3305.xml">a priority model for named entities </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the priority model achieves an f-measure of 0.958-0.960, consistently higher than the statistical language model and probabilistic context-free grammar.
</prevsent>
<prevsent>automatic recognition of gene and protein names is challenging first step towards text mining the biomedical literature.
</prevsent>
</prevsection>
<citsent citstr=" W05-1305 ">
advances in the area of gene and protein named entity recognition (ner) have been accelerated by freely available tagged corpora (kim et al, 2003, cohen et al, 2005, <papid> W05-1306 </papid>smith et al, 2005, <papid> W05-1305 </papid>tanabe et al, 2005).</citsent>
<aftsection>
<nextsent>such corpora have made it possible for standardized evaluations such as task 1a of the first bio creative workshop (yeh et al, 2005).
</nextsent>
<nextsent>although state-of-the-art systems now perform at the level of 80-83% f-measure, this is still well below the range of 90-97% for non-biomedical ner.
</nextsent>
<nextsent>the main reasons for this performance disparity are 1) the complexity of the genetic nomenclature and 2) the confusion of gene and protein names with other biomedical entities, as well as with common english words.
</nextsent>
<nextsent>in an effort to alleviate the confusion with other biomedical entities we have assembled database consisting of named entities appearing in the literature of biomedicine together with information on their onto logical categories.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2795">
<title id=" W06-3305.xml">a priority model for named entities </title>
<section> methods.  </section>
<citcontext>
<prevsection>
<prevsent>currently, semcat has 77 semantic types, and 5.11m non-unique entries.
</prevsent>
<prevsent>additional entities from medline are being manually classified via an annotation website.
</prevsent>
</prevsection>
<citsent citstr=" W04-3110 ">
unlike the ter mino database (harkema et al (2004), <papid> W04-3110 </papid>which contains terminology annotated with morphosyntactic and conceptual information, semcat currently consists of gazetteer lists only.</citsent>
<aftsection>
<nextsent>for our experiments, we generated two sets of training data from semcat, gene-protein (gp) and not-gene-protein (ngp).
</nextsent>
<nextsent>gp consists of specific terms from the semantic types dna molecule, protein molecule, dna family, protein family, protein complex and protein subunit.
</nextsent>
<nextsent>ngp consists of entities from all other semcat types, along with generic entities from the gp semantic types.
</nextsent>
<nextsent>generic entities were automatically eliminated from gp using pattern matching to manually tagged generic phrases like abnormal protein, acid domain, and rna.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2796">
<title id=" W06-3316.xml">semi supervised anaphora resolution in biomedical texts </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the system is integrated into an interactive tool designed to assistflybase curators by aiding the identification of the salient entities in given paperas first step in the aggregation of information about them.
</prevsent>
<prevsent>the number of articles being published in biomedical journals per year is increasing exponentially.
</prevsent>
</prevsection>
<citsent citstr=" W03-1301 ">
for example, morgan et al (2003) <papid> W03-1301 </papid>report that more than 8000 articles were published in 2000 just in relation to flybase1, database of genomic research on the fruit fly drosophila melanogaster.</citsent>
<aftsection>
<nextsent>the growth in the literature makes it difficult for researchers to keep track of information, even in very small sub fields of biology.
</nextsent>
<nextsent>progress in the field often relies on the work of professional curators, typically postdoctoral-level scientists, who are 1http://www.flybase.orgtrained to identify important information in scientific article.
</nextsent>
<nextsent>this is very time-consuming task which first requires identification of gene, allele and protein names and their synonyms, as well as several interactions and relations between them.the information extracted from each article is then used to fill in template per gene or allele.
</nextsent>
<nextsent>to extract all information about specific biomedical entity in the text and be able to fill in the corresponding template, useful first step isthe identification of all textual mentions that are referring to or are related with that entity.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2797">
<title id=" W06-3316.xml">semi supervised anaphora resolution in biomedical texts </title>
<section> an mrna is part-of gene..  </section>
<citcontext>
<prevsection>
<prevsent>table 1 shows the distributions of the anaphoric expressions according to the anaphoric relations they hold to their closest antecedent.
</prevsent>
<prevsent>co referent associative no ant.
</prevsent>
</prevsection>
<citsent citstr=" J00-4003 ">
total dds 34 51 12 97 pns 132 62 23 217 total 166 113 35 314 table 1: anaphoric relation distribution dds and pns in associative relations account for27% of all nps in the test data, which is almost double the number of bridging cases (associative plus co referent cases where head nouns are not the same) reported for newspaper texts in vieira and poesio (2000).<papid> J00-4003 </papid>table 2 shows the distribution of the different bio types present in the corpus.</citsent>
<aftsection>
<nextsent>100 gene part subtype super type product 67 62 1 7 244 table 2: biotype distribution 3.1 results.
</nextsent>
<nextsent>the anaphora resolution system reaches 58.8% precision and 57.3% recall when looking for the closest antecedent for dds and pns, after having been provided with hand-corrected input (that is, perfect gene name recognition, np typing and selection).
</nextsent>
<nextsent>ifwe account separately for coreference and associative relations, we get 59.47% precision and 81.3% recall for the co referent cases, and 55.5% precision and 22.1% recall for the associative ones.
</nextsent>
<nextsent>the performance of the system is improved if we consider that it is able to find an antecedent other than the closest, which is still coreferential to the anaphor.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2798">
<title id=" W06-3316.xml">semi supervised anaphora resolution in biomedical texts </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>when the ner system fails to recognise gene name, it can decrease the parser performance (as it would have to deal with an unknown word) and influences the semantic tagging (the np containing such gene name wont be selected as possible antecedent or anaphor unless it contains another word that is part of so).
</prevsent>
<prevsent>when just the ner step iscor rected by hand, the system reaches 71.8% precision 101 and 64.1% recall.
</prevsent>
</prevsection>
<citsent citstr=" W97-1301 ">
previous approaches to solve associative anaphora have made use of knowledge resources like wordnet (poesio et al, 1997), <papid> W97-1301 </papid>the internet (bunescu, 2003) and corpus (poesio et al, 2002) to check if there isan associative link between the anaphor and possible antecedent.</citsent>
<aftsection>
<nextsent>in the medical domain, castano et al (2002) used umls (unified medical language system)7 as their knowledge source.
</nextsent>
<nextsent>they treat coreferential pronominal anaphora and anaphoric dds and aim to improve the extraction of biomolecular relations from medline abstracts.
</nextsent>
<nextsent>the resolution process relies on syntactic features, semantic information fromumls, and the string itself.
</nextsent>
<nextsent>they try to resolve just the dds that refer to relevant bio types (corresponding to umls types) such as amino acids, proteins or cells.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2799">
<title id=" W06-3316.xml">semi supervised anaphora resolution in biomedical texts </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>for selecting the antecedents, they calculate salience values based on string similarity, per son/number agreement, semantic type matching and other features.
</prevsent>
<prevsent>they report precision of 74% andre call of 75% on very small test set.
</prevsent>
</prevsection>
<citsent citstr=" C04-1033 ">
yang et al (2004) <papid> C04-1033 </papid>test supervised learning-based approach for anaphora resolution, evaluating it on medline abstracts from the genia corpus.</citsent>
<aftsection>
<nextsent>they focus only on co referent cases and do not attempt to resolve associative links.
</nextsent>
<nextsent>18 features describe the relationship between an anaphoric expression and its possible antecedent - their source of semantic knowledge is the biotype information provided bythe ner component of genia.
</nextsent>
<nextsent>they achieved recall of 80.2% and precision of 77.4%.
</nextsent>
<nextsent>they also experiment with exploring the relationships between nps and coreferential clusters (i.e. chains), selecting an antecedent based not just on single candidate but also on the cluster that the candidate is part of.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2800">
<title id=" W06-1633.xml">bestcut a graph algorithm for coreference resolution </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>recent coreference resolution algorithms tackle the problem of identifying co referent mentions of the same entity in text as two step procedure: (1) classification phase that decides whether pairs ofnoun phrases corefer or not; and (2) clusteriza tion phase that groups together all mentions that refer to the same entity.
</prevsent>
<prevsent>an entity is an object ora set of objects in the real world, while mention is textual reference to an entity1.
</prevsent>
</prevsection>
<citsent citstr=" J01-4004 ">
most of the previous coreference resolution methods have similar classification phases, implemented either as decision trees (soon et al, 2001) <papid> J01-4004 </papid>or as maximum entropy classifiers (luo et al, 2004)<papid> P04-1018 </papid></citsent>
<aftsection>
<nextsent>more over, these methods employ similar feature sets.
</nextsent>
<nextsent>the cluster ization phase is different across current approaches.
</nextsent>
<nextsent>for example, there are several linking decisions for clusterization.
</nextsent>
<nextsent>(soon et al, 2001) <papid> J01-4004 </papid>advocate the link-first decision, which links mention to its closest candidate referent, while (ng andcardie, 2002) <papid> P02-1014 </papid>consider instead the link-best decision, which links mention to its most confident 1this definition was introduced in (nist, 2003).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2802">
<title id=" W06-1633.xml">bestcut a graph algorithm for coreference resolution </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>recent coreference resolution algorithms tackle the problem of identifying co referent mentions of the same entity in text as two step procedure: (1) classification phase that decides whether pairs ofnoun phrases corefer or not; and (2) clusteriza tion phase that groups together all mentions that refer to the same entity.
</prevsent>
<prevsent>an entity is an object ora set of objects in the real world, while mention is textual reference to an entity1.
</prevsent>
</prevsection>
<citsent citstr=" P04-1018 ">
most of the previous coreference resolution methods have similar classification phases, implemented either as decision trees (soon et al, 2001) <papid> J01-4004 </papid>or as maximum entropy classifiers (luo et al, 2004)<papid> P04-1018 </papid></citsent>
<aftsection>
<nextsent>more over, these methods employ similar feature sets.
</nextsent>
<nextsent>the cluster ization phase is different across current approaches.
</nextsent>
<nextsent>for example, there are several linking decisions for clusterization.
</nextsent>
<nextsent>(soon et al, 2001) <papid> J01-4004 </papid>advocate the link-first decision, which links mention to its closest candidate referent, while (ng andcardie, 2002) <papid> P02-1014 </papid>consider instead the link-best decision, which links mention to its most confident 1this definition was introduced in (nist, 2003).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2808">
<title id=" W06-1633.xml">bestcut a graph algorithm for coreference resolution </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the cluster ization phase is different across current approaches.
</prevsent>
<prevsent>for example, there are several linking decisions for clusterization.
</prevsent>
</prevsection>
<citsent citstr=" P02-1014 ">
(soon et al, 2001) <papid> J01-4004 </papid>advocate the link-first decision, which links mention to its closest candidate referent, while (ng andcardie, 2002) <papid> P02-1014 </papid>consider instead the link-best decision, which links mention to its most confident 1this definition was introduced in (nist, 2003).</citsent>
<aftsection>
<nextsent>candidate referent.
</nextsent>
<nextsent>both these clustering decisions are locally optimized.
</nextsent>
<nextsent>in contrast, globally optimized clustering decisions were reported in (luo et al, 2004)<papid> P04-1018 </papid>and (daumeiii and marcu, 2005a), where all clustering possibilities are considered by searching on bell tree representation or by using the learning as search optimization (laso)framework (daumeiii and marcu, 2005b) respectively, but the first search is partial and driven by heuristics and the second one only looks back in text.</nextsent>
<nextsent>we argue that more adequate cluster ization phase for coreference resolution can be obtained by using graph representation.in this paper we describe novel representation of the coreference space as an undirectededge-weighted graph in which the nodes represent all the mentions from text, whereas the edges between nodes constitute the confidence values derived from the coreference classification phase.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2821">
<title id=" W06-1633.xml">bestcut a graph algorithm for coreference resolution </title>
<section> bestcut coreference resolution.  </section>
<citcontext>
<prevsection>
<prevsent>the coreference confidence values that become the weights in the starting graphs are provided bya maximum entropy model, trained on the training datasets of the corpora used in our experiments.
</prevsent>
<prevsent>for maximum entropy classification we used maxent4 tool.
</prevsent>
</prevsection>
<citsent citstr=" J96-1002 ">
based on the data seen, amaximum entropy model (berger et al, 1996) <papid> J96-1002 </papid>offers an expression (1) for the probability that there exists coreference between mention mi and mention mj . (c|mi,mj) = e( ? kgk(mi,mj ,c)) z(mi,mj) (1) where gk(mi,mj , c) is feature and is its weight; z(mi,mj) is normalizing factor.</citsent>
<aftsection>
<nextsent>we created the training examples in the sameway as (luo et al, 2004)<papid> P04-1018 </papid>by pairing all mentions of the same type, obtaining their feature vectors and taking the outcome (coreferent/non coreferent) from the key files.</nextsent>
<nextsent>2.2 feature representation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2837">
<title id=" W06-1633.xml">bestcut a graph algorithm for coreference resolution </title>
<section> mention detection.  </section>
<citcontext>
<prevsection>
<prevsent>this was motivated by the fact that we need to classify mentions according to the context in which they appear, and not in general way.
</prevsent>
<prevsent>only contextual information is useful further in coreference resolution.
</prevsent>
</prevsection>
<citsent citstr=" P05-3014 ">
we have experimentally discovered that the use of word sense disambiguation improves the performance tremendously (a boost in score of 10%), therefore all the features use the word senses from previously-applied word sense disambiguation program, taken from (mihalcea and csomai, 2005).<papid> P05-3014 </papid></citsent>
<aftsection>
<nextsent>for creating training instances, we associated an outcome to each mark able (np) detected in the training files: the markables that were present in the key files took their outcome from the key file annotation, while all the other markables were associated with outcome unk.
</nextsent>
<nextsent>we then created training example for each of the markables, with the feature vector described below and as target function the outcome.
</nextsent>
<nextsent>the aforementioned out come can be of three different types.
</nextsent>
<nextsent>the first type of outcome that we tried was the entity type (onemember of the set person, organization, location, facility, gpe and unk); the second type was the genericity information (generic orspecific), whereas the third type was combination between the two (pairwise combinations of the entity types set and the genericity set, e.g. person specific).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2844">
<title id=" W06-1633.xml">bestcut a graph algorithm for coreference resolution </title>
<section> experimental results.  </section>
<citcontext>
<prevsection>
<prevsent>we experimented on the ace phase 2 (nist,.
</prevsent>
<prevsent>2003) and muc6 (muc-6, 1995) corpora.
</prevsent>
</prevsection>
<citsent citstr=" M95-1005 ">
sincewe aimed to measure the performance of coreference, the metrics used for evaluation are the ecm (luo et al, 2004)<papid> P04-1018 </papid>and the muc p, and scores (vilain et al, 1995).<papid> M95-1005 </papid></citsent>
<aftsection>
<nextsent>in our first experiment, we tested the three coreference cluster ization algorithms on the development-test set of the ace phase 2 corpus, first on true mentions (i.e. the mentions annotated in the key files), then on detected mentions (i.e.the mentions output by our mention detection system presented in section 3) and finally without any prior knowledge of the mention types.
</nextsent>
<nextsent>the results obtained are tabulated in table 4.
</nextsent>
<nextsent>as can be observed, when it has prior knowledge of the mention types bestcut performs significantly better than the other two systems in the ecm-f score and slightly better in the muc metrics.
</nextsent>
<nextsent>the more knowledge it has about the mentions, the better it performs.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2849">
<title id=" W06-1633.xml">bestcut a graph algorithm for coreference resolution </title>
<section> experimental results.  </section>
<citcontext>
<prevsection>
<prevsent>it is apparent that the muc score does not vary significantly between systems.
</prevsent>
<prevsent>this only shows that none of them is particularly poor, but it is not relevant way of comparing methods?
</prevsent>
</prevsection>
<citsent citstr=" M98-1022 ">
the muc metric has been found too indulgent by researchers ((luo et al, 2004)<papid> P04-1018 </papid>(baldwin et al, 1998)).<papid> M98-1022 </papid></citsent>
<aftsection>
<nextsent>the muc scorer counts the common links between the 281 muc score cluster ization algorithm mentions ecm-f% muc p% muc r% muc f% bestcut key 82.7 91.1 88.2 89.63 detected 73.0 88.3 75.1 81.17 undetected 41.2 52.0 82.4 63.76 bell tree (luo et al, 2004)<papid> P04-1018 </papid>key 77.9 88.5 89.3 88.90 detected 70.8 86.0 76.6 81.03 undetected 52.6 40.3 87.1 55.10 link-best (ng and cardie, 2002) <papid> P02-1014 </papid>key 77.9 88.0 90.0 88.99 detected 70.7 85.1 77.3 81.01 undetected 51.6 39.6 88.5 54.72 table 4: comparison of results between three cluster ization algorithms on ace phase 2.</nextsent>
<nextsent>the learning algorithms are maxent for coreference and svm for stopping the cut in bestcut.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2867">
<title id=" W06-1633.xml">bestcut a graph algorithm for coreference resolution </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>similarly to greedy search.
</prevsent>
<prevsent>moreover, the fact that the two implementations are comparable is not inconceivable once we consider that (luo et al, 2004)<papid> P04-1018 </papid>never compared their system to another coreference re solver and reported their competitive results on true mentions only.</prevsent>
</prevsection>
<citsent citstr=" P05-1020 ">
(ng, 2005)<papid> P05-1020 </papid>treats coreference resolution as problem of ranking candidate partitions generated by set of coreference systems.</citsent>
<aftsection>
<nextsent>the overall performance of the system is limited by the performance of its best component.
</nextsent>
<nextsent>the main difference between this approach and ours is that (ng, 2005)<papid> P05-1020 </papid>s approach takes coreference resolution one step further, by comparing the results of multiple systems, while our system is single resolver; furthermore, he emphasizes the global optimization of ranking clusters obtained locally, whereas our focus is on globally optimizing the cluster ization method inside the resolver.</nextsent>
<nextsent>(daumeiii and marcu, 2005a) use the learning as search optimization framework to take into account the non-locality behavior of the coreferencefeatures.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2870">
<title id=" W07-0715.xml">an iterativelytrained segmentation free phrase translation model for statistical machine translation </title>
<section> abstract </section>
<citcontext>
<prevsection>


</prevsection>
<citsent citstr=" N03-1017 ">
attempts to estimate phrase translationprobablities for statistical machine translation using iteratively-trained models have repeatedly failed to produce translations as good as those obtained by estimating phrase translation probablities from surface statistics of bilingual word alignments as described by koehn, et al  (2003).<papid> N03-1017 </papid></citsent>
<aftsection>
<nextsent>we propose new iteratively-trained phrase translation model that produces translations of quality equal to or better than those produced by koehn, et al model.
</nextsent>
<nextsent>moreover,with the new model, translation quality degrades much more slowly as pruning is tigh tend to reduce translation time.
</nextsent>
<nextsent>estimates of conditional phrase translation probabilities provide major source of translation knowledge in phrase-based statistical machine translation (smt) systems.
</nextsent>
<nextsent>the most widely used method for estimating these probabilities is that of koehn, et al .
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2871">
<title id=" W07-0715.xml">an iterativelytrained segmentation free phrase translation model for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we will refer to this approach as the standard model?.
</prevsent>
<prevsent>there have been several attempts to estimate phrase translation probabilities directly, using generative models trained iteratively on parallel corpus using the expectation maximization (em) algorithm.
</prevsent>
</prevsection>
<citsent citstr=" W02-1018 ">
the first of these models, that of marcu and wong (2002), <papid> W02-1018 </papid>was found by koehn, et al  (2003), <papid> N03-1017 </papid>to produce translations not quite as good as their method.</citsent>
<aftsection>
<nextsent>recently, birch et al  (2006) <papid> W06-3123 </papid>tried the marcu and wong model constrained by word alignment and also found that koehn, et al model worked better, with the advantage of the standard model increasing as more features were added to the overall translation model.</nextsent>
<nextsent>denero et al  (2006) <papid> W06-3105 </papid>trieda different generative phrase translation model analogous to ibm word-translation model 3 (brown et al ., 1993), <papid> J93-2003 </papid>and again found that the standard model outperformed their generative model.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2873">
<title id=" W07-0715.xml">an iterativelytrained segmentation free phrase translation model for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>there have been several attempts to estimate phrase translation probabilities directly, using generative models trained iteratively on parallel corpus using the expectation maximization (em) algorithm.
</prevsent>
<prevsent>the first of these models, that of marcu and wong (2002), <papid> W02-1018 </papid>was found by koehn, et al  (2003), <papid> N03-1017 </papid>to produce translations not quite as good as their method.</prevsent>
</prevsection>
<citsent citstr=" W06-3123 ">
recently, birch et al  (2006) <papid> W06-3123 </papid>tried the marcu and wong model constrained by word alignment and also found that koehn, et al model worked better, with the advantage of the standard model increasing as more features were added to the overall translation model.</citsent>
<aftsection>
<nextsent>denero et al  (2006) <papid> W06-3105 </papid>trieda different generative phrase translation model analogous to ibm word-translation model 3 (brown et al ., 1993), <papid> J93-2003 </papid>and again found that the standard model outperformed their generative model.</nextsent>
<nextsent>denero et al  (2006) <papid> W06-3105 </papid>attribute the inferiority oftheir model and the marcu and wong model to hidden segmentation variable, which enables the emalgorithm to maximize the probability of the training data without really improving the quality of the model.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2874">
<title id=" W07-0715.xml">an iterativelytrained segmentation free phrase translation model for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the first of these models, that of marcu and wong (2002), <papid> W02-1018 </papid>was found by koehn, et al  (2003), <papid> N03-1017 </papid>to produce translations not quite as good as their method.</prevsent>
<prevsent>recently, birch et al  (2006) <papid> W06-3123 </papid>tried the marcu and wong model constrained by word alignment and also found that koehn, et al model worked better, with the advantage of the standard model increasing as more features were added to the overall translation model.</prevsent>
</prevsection>
<citsent citstr=" W06-3105 ">
denero et al  (2006) <papid> W06-3105 </papid>trieda different generative phrase translation model analogous to ibm word-translation model 3 (brown et al ., 1993), <papid> J93-2003 </papid>and again found that the standard model outperformed their generative model.</citsent>
<aftsection>
<nextsent>denero et al  (2006) <papid> W06-3105 </papid>attribute the inferiority oftheir model and the marcu and wong model to hidden segmentation variable, which enables the emalgorithm to maximize the probability of the training data without really improving the quality of the model.</nextsent>
<nextsent>we propose an iteratively-trained phrase translation model that does not require different seg ment ations to compete against one another, and we show that this produces translations of quality equal to or better than those produced by the standard model.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2876">
<title id=" W07-0715.xml">an iterativelytrained segmentation free phrase translation model for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the first of these models, that of marcu and wong (2002), <papid> W02-1018 </papid>was found by koehn, et al  (2003), <papid> N03-1017 </papid>to produce translations not quite as good as their method.</prevsent>
<prevsent>recently, birch et al  (2006) <papid> W06-3123 </papid>tried the marcu and wong model constrained by word alignment and also found that koehn, et al model worked better, with the advantage of the standard model increasing as more features were added to the overall translation model.</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
denero et al  (2006) <papid> W06-3105 </papid>trieda different generative phrase translation model analogous to ibm word-translation model 3 (brown et al ., 1993), <papid> J93-2003 </papid>and again found that the standard model outperformed their generative model.</citsent>
<aftsection>
<nextsent>denero et al  (2006) <papid> W06-3105 </papid>attribute the inferiority oftheir model and the marcu and wong model to hidden segmentation variable, which enables the emalgorithm to maximize the probability of the training data without really improving the quality of the model.</nextsent>
<nextsent>we propose an iteratively-trained phrase translation model that does not require different seg ment ations to compete against one another, and we show that this produces translations of quality equal to or better than those produced by the standard model.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2881">
<title id=" W07-0715.xml">an iterativelytrained segmentation free phrase translation model for statistical machine translation </title>
<section> previous approaches.  </section>
<citcontext>
<prevsection>
<prevsent>stochastic ally segment into some number of.
</prevsent>
<prevsent>phrases.
</prevsent>
</prevsection>
<citsent citstr=" W99-0604 ">
choose phrase position in the target sentence that is being generated.1this method of phrase pair extraction was originally described by och et al  (1999).<papid> W99-0604 </papid></citsent>
<aftsection>
<nextsent>4.
</nextsent>
<nextsent>for each selected phrase in and the corre-.
</nextsent>
<nextsent>spon ding phrase position in b, stochastic ally choose target phrase.
</nextsent>
<nextsent>5.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2882">
<title id=" W07-0715.xml">an iterativelytrained segmentation free phrase translation model for statistical machine translation </title>
<section> a segmentation-free model </section>
<citcontext>
<prevsection>
<prevsent>be cause we have not proved any of the mathematical properties usually associated with generative mod els; e.g., that the training procedure maximizes the likelihood of the training data.
</prevsent>
<prevsent>we will motivate the model, however, with generative story as to how phrase alignments are produced, given pair of source and target sentences.
</prevsent>
</prevsection>
<citsent citstr=" P03-1012 ">
our model extends to phrase alignment the concept of sentence pair generating word alignment developed by cherry and lin (2003).<papid> P03-1012 </papid></citsent>
<aftsection>
<nextsent>our model is defined in terms of two stochastic processes, selection and alignment, as follows: 1.
</nextsent>
<nextsent>for each word-aligned sentence pair, we iden-.
</nextsent>
<nextsent>tify all the possible phrase pair instances according to the criteria used by koehn et al  113 2.
</nextsent>
<nextsent>each source phrase instance that is included in.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2883">
<title id=" W07-0715.xml">an iterativelytrained segmentation free phrase translation model for statistical machine translation </title>
<section> a segmentation-free model </section>
<citcontext>
<prevsection>
<prevsent>we also have not proved that 114 the procedure maximizes the likelihood of anything,although we find empirically that each iteration decreases the conditional entropy of the phrase translation model.
</prevsent>
<prevsent>in any case, the training procedure seems to work well in practice.
</prevsent>
</prevsection>
<citsent citstr=" N06-1014 ">
it is also very similar to the joint training procedure for hmm word alignment models in both directions described byliang et al  (2006), <papid> N06-1014 </papid>which was the original inspiration for our training procedure.</citsent>
<aftsection>
<nextsent>we evaluated our phrase translation model compared to the standard model of koehn et al  in the context of fairly typical end-to-end phrase-based smt system.
</nextsent>
<nextsent>the overall translation model score consists of weighted sum of the following eight aggregated feature values for each translation hypoth esis: ? the sum of the log probabilities of each source phrase in the hypothesis given the corresponding target phrase, computed either by our model or the standard model,?
</nextsent>
<nextsent>the sum of the log probabilities of each target phrase in the hypothesis given the corresponding source phrase, computed either by our model or the standard model, ? the sum of lexical scores for each source phrase given the corresponding target phrase, ? the sum of lexical scores for each target phrase given the corresponding source phrase, ? the log of the target language model probability for the sequence of target phrases in the hypothesis, ? the total number of words in the target phrases in the hypothesis, ? the total number of source/target phrase pairs composing the hypothesis, ? the distortion penalty as implemented in the pharaoh decoder (koehn, 2003).the lexical scores are computed as the (unnor malized) log probability of the viterbi alignment for phrase pair under ibm word-translation model 1 (brown et al , 1993).<papid> J93-2003 </papid></nextsent>
<nextsent>the feature weights for the overall translation models were trained using ochs (2003) minimum-error-rate training procedure.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2885">
<title id=" W07-0715.xml">an iterativelytrained segmentation free phrase translation model for statistical machine translation </title>
<section> experimental set-up and data.  </section>
<citcontext>
<prevsection>
<prevsent>the feature weights for the overall translation models were trained using ochs (2003) minimum-error-rate training procedure.
</prevsent>
<prevsent>the weights were optimized separately for our model and for the standard phrase translation model.
</prevsent>
</prevsection>
<citsent citstr=" W03-0301 ">
our decoder is re implementation in perl of the algorithm used by the pharaoh decoder as described by koehn (2003).2 the data we used comes from an english-frenchbilingual corpus of canadian hansa rds parliamentary proceedings supplied for the bilingual word alignment workshop held at hlt-naacl 2003 (mihalcea and pedersen, 2003).<papid> W03-0301 </papid></citsent>
<aftsection>
<nextsent>automatic sentence alignment of this data was provided by ulrich germann.
</nextsent>
<nextsent>we used 500,000 sentences pairs from this corpus for training both the phrase translation models and ibm model 1 lexical scores.
</nextsent>
<nextsent>these 500,000sentence pairs were word-aligned using state-of the-art word-alignment method (moore et al , 2006).<papid> P06-1065 </papid></nextsent>
<nextsent>a separate set of 500 sentence pairs was used to train the translation model weights, and two additional held-out sets of 2000 sentence pairs each were used as test data.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2886">
<title id=" W07-0715.xml">an iterativelytrained segmentation free phrase translation model for statistical machine translation </title>
<section> experimental set-up and data.  </section>
<citcontext>
<prevsection>
<prevsent>automatic sentence alignment of this data was provided by ulrich germann.
</prevsent>
<prevsent>we used 500,000 sentences pairs from this corpus for training both the phrase translation models and ibm model 1 lexical scores.
</prevsent>
</prevsection>
<citsent citstr=" P06-1065 ">
these 500,000sentence pairs were word-aligned using state-of the-art word-alignment method (moore et al , 2006).<papid> P06-1065 </papid></citsent>
<aftsection>
<nextsent>a separate set of 500 sentence pairs was used to train the translation model weights, and two additional held-out sets of 2000 sentence pairs each were used as test data.
</nextsent>
<nextsent>the two phrase translation models were trained using the same set of possible phrase pairs extracted from the word-aligned 500,000 sentence pair corpus, finding all possible phrase pairs permitted by the criteria followed by koehn et al , up to phrase length of seven words.
</nextsent>
<nextsent>this produced approximately 69 million distinct phrase pair types.
</nextsent>
<nextsent>no pruning of the set of possible phrase pairs was done during or before training the phrase translation models.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2887">
<title id=" W07-0715.xml">an iterativelytrained segmentation free phrase translation model for statistical machine translation </title>
<section> translation experiments.  </section>
<citcontext>
<prevsection>
<prevsent>denero et al  obtained corresponding measurements of 1.55 bits per phrase and3.76 bits per phrase, for their model and the standard model, using different dataset and slightly different estimation method.
</prevsent>
<prevsent>we wanted to look at the trade-off between decoding time and translation quality for our new phrase translation model compared to the standard model.
</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
since this trade-off is also affected by the settings of various pruning parameters, we compared decoding time and translation quality, as measured by bleu score (papineni et al  2002), <papid> P02-1040 </papid>for the two models on our first test set overbroad range of settings for the decoder pruning parameters.</citsent>
<aftsection>
<nextsent>the pharaoh decoding algorithm, has five pruning parameters that affect decoding time: ? distortion limit ? translation table limit ? translation table threshold ? beam limit ? beam threshold the distortion limit is the maximum distance allowed between two source phrases that produce adjacent target phrases in the decoder output.
</nextsent>
<nextsent>the distortion limit can be viewed as model parameter, as well as pruning paramter, because setting it toan optimum value usually improves translation quality over leaving it unrestricted.
</nextsent>
<nextsent>we carried out experiments with the distortion limit set to 1, which seemed to produce the highest bleu scores on our dataset with the standard model, and also set to 5,which is perhaps more typical value for phrase based smt systems.
</nextsent>
<nextsent>translation model weights were trained separately for these two settings, be cause the greater the distortion limit, the higher the distortion penalty weight needed for optimal translation quality.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2888">
<title id=" W06-3404.xml">you are what you say using meeting participants speech to detect their roles and expertise </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>one such task is the retrieval of information from previous meetings, which is typically difficult and time consuming task for the human to perform (banerjee et al, 2005).
</prevsent>
<prevsent>another task isto automatically record the action items being discussed at meetings, along with details such as when the action is due, who is responsible for it, etc. meeting analysis is quickly growing field ofstudy.
</prevsent>
</prevsection>
<citsent citstr=" P03-1071 ">
in recent years, research has focussed on automatic speech recognition in meetings (stolcke etal., 2004; metze et al, 2004; hain et al, 2005), activity recognition (rybski and veloso, 2004), automatic meeting summarization (murray et al, 2005), meeting phase detection (banerjee and rudnicky,2004) and topic detection (galley et al, 2003).<papid> P03-1071 </papid></citsent>
<aftsection>
<nextsent>relatively little research has been performed on automatically detecting the roles that meeting participants play as they participate in meetings.
</nextsent>
<nextsent>these roles canbe functional (e.g. the facilitator who runs the meeting, and the scribe who is the designated note taker at the meeting), discourse based (e.g. the presenter, and the discussion participant), and expertise related (e.g. the hardware acquisition expert and the speech recognition research expert).
</nextsent>
<nextsent>some roles are tightly scoped, relevant to just one meeting or even partof meeting.
</nextsent>
<nextsent>for example, person can be the facilitator of one meeting and the scribe of another, or the same person can be presenter for one part of the meeting and discussion participant for another part.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2889">
<title id=" W06-2506.xml">characterizing response types and revealing noun ambiguity in german association norms </title>
<section> analysis of noun senses.  </section>
<citcontext>
<prevsection>
<prevsent>closely related to our work, schvaneveldts pathfinder networks(schvaneveldt, 1990) were based on word associations and were used to identify word senses.an enourmous number of approaches in computational linguistics can be found on the senseval webpage (senseval, ), which hosts word sense disambiguation competition.
</prevsent>
<prevsent>we applied latent semantic clusters (lsc) to our association data.
</prevsent>
</prevsection>
<citsent citstr=" P99-1014 ">
the lsc algorithm is an instance of the expectation-maximisation algorithm (baum,1972) for unsupervised training based on unannotated data, and has been applied to model the selectional dependency between two sets of words participating in grammatical relationship (rooth,1998; rooth et al, 1999).<papid> P99-1014 </papid></citsent>
<aftsection>
<nextsent>the resulting cluster analysis defines two-dimensional soft clusters which are able to generalise over hidden data.
</nextsent>
<nextsent>lsc training learns three probability distributions, one for the probabilities of the clusters, and one foreach tuple input item and each cluster (i.e., probability distribution for the target nouns and each cluster, and one for the associations and each cluster), thus the two dimensions.
</nextsent>
<nextsent>we use an implementation of the lsc algorithm as provided by helmut schmid.the lsc output depends not only on the distributional input, but also on the number of clusters to model.
</nextsent>
<nextsent>as rule, the more clusters are modeled,the more skewed the resulting probability distributions for cluster membership are.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2890">
<title id=" W06-3210.xml">a naive theory of affix ation and an algorithm for extraction </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>for example, if we have play, played, playing, player, players, playground and we wish to test where to segment plays, the suc cesor count for the prefix pla would be 1 because only occurs after whereas the number of successors of play peak at three (i.e {e, i, g}).
</prevsent>
<prevsent>although the heuristic has had some success it was shown (in various interpretations) as early as (hafer and weiss, 1974) that it is not really sound ? even for english.
</prevsent>
</prevsection>
<citsent citstr=" N03-2015 ">
a slightly better method is to compile set of words into trie and predict boundaries at nodes with high actitivity (e.g (johnson and martin, 2003; <papid> N03-2015 </papid>schone and jurafsky, 2001; <papid> N01-1024 </papid>kazakov and manandhar, 2001) and earlier papers by the same authors), but this not sound either as non-morphemic short common character sequences also show significant branching.</citsent>
<aftsection>
<nextsent>84 the algorithm in this paper is differs significantly from the harris-inspired varieties.
</nextsent>
<nextsent>first, we do not record the number of phonemes/character of agiven prefix/suffix but the total number of continuations.
</nextsent>
<nextsent>in the example above, that would be theset {ed, ing, er, ers, ground} rather than the three member set of continuing phonemes/characters.
</nextsent>
<nextsent>secondly, segmentation of given word is not the immediate objective and what amounts to identification of the end of lexical (thus generally low frequency) item is not within the direct reach of the model.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2891">
<title id=" W06-3210.xml">a naive theory of affix ation and an algorithm for extraction </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>for example, if we have play, played, playing, player, players, playground and we wish to test where to segment plays, the suc cesor count for the prefix pla would be 1 because only occurs after whereas the number of successors of play peak at three (i.e {e, i, g}).
</prevsent>
<prevsent>although the heuristic has had some success it was shown (in various interpretations) as early as (hafer and weiss, 1974) that it is not really sound ? even for english.
</prevsent>
</prevsection>
<citsent citstr=" N01-1024 ">
a slightly better method is to compile set of words into trie and predict boundaries at nodes with high actitivity (e.g (johnson and martin, 2003; <papid> N03-2015 </papid>schone and jurafsky, 2001; <papid> N01-1024 </papid>kazakov and manandhar, 2001) and earlier papers by the same authors), but this not sound either as non-morphemic short common character sequences also show significant branching.</citsent>
<aftsection>
<nextsent>84 the algorithm in this paper is differs significantly from the harris-inspired varieties.
</nextsent>
<nextsent>first, we do not record the number of phonemes/character of agiven prefix/suffix but the total number of continuations.
</nextsent>
<nextsent>in the example above, that would be theset {ed, ing, er, ers, ground} rather than the three member set of continuing phonemes/characters.
</nextsent>
<nextsent>secondly, segmentation of given word is not the immediate objective and what amounts to identification of the end of lexical (thus generally low frequency) item is not within the direct reach of the model.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2893">
<title id=" W06-3210.xml">a naive theory of affix ation and an algorithm for extraction </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in the example above, that would be theset {ed, ing, er, ers, ground} rather than the three member set of continuing phonemes/characters.
</prevsent>
<prevsent>secondly, segmentation of given word is not the immediate objective and what amounts to identification of the end of lexical (thus generally low frequency) item is not within the direct reach of the model.
</prevsent>
</prevsection>
<citsent citstr=" P00-1027 ">
thirdly, and most importantly, the algorithm in this paper looks at the slope of the frequency curve not at peaks in absolute frequency.a different approach, sometimes used in complement of other sources of information, is to select aligned pairs (or sets) of strings that share long character sequence (work includes (jacquemin, 1997; yarowsky and wicentowski, 2000; <papid> P00-1027 </papid>baroni et al., 2002; <papid> W02-0606 </papid>clark, 2001)).</citsent>
<aftsection>
<nextsent>a notable advantage is that one is not restricted to concatenative morphology.
</nextsent>
<nextsent>many publications (cavar et al, 2004; brent et al., 1995; goldsmith et al, 2001; dejean, 1998;snover et al, 2002; <papid> W02-0602 </papid>argamon et al, 2004; <papid> C04-1152 </papid>goldsmith, 2001; <papid> J01-2001 </papid>creutz and lagus, 2005; neuvel and fulop, 2002; <papid> W02-0604 </papid>baroni, 2003; gaussier, 1999; <papid> W99-0904 </papid>sharma et al, 2002; <papid> W02-0601 </papid>wicentowski, 2002; oliver, 2004),and various other works by the same authors, describe strategies that use frequencies, probabilities,and optimization criteria, often minimum description length (mdl), in various combinations.</nextsent>
<nextsent>so far, all these are unsatisfactory on two main accounts; on the theretical side, they still owe an explanation ofwhy compression or mdl should give birth to segment ations coinciding with morphemes as linguistically defined.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2894">
<title id=" W06-3210.xml">a naive theory of affix ation and an algorithm for extraction </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in the example above, that would be theset {ed, ing, er, ers, ground} rather than the three member set of continuing phonemes/characters.
</prevsent>
<prevsent>secondly, segmentation of given word is not the immediate objective and what amounts to identification of the end of lexical (thus generally low frequency) item is not within the direct reach of the model.
</prevsent>
</prevsection>
<citsent citstr=" W02-0606 ">
thirdly, and most importantly, the algorithm in this paper looks at the slope of the frequency curve not at peaks in absolute frequency.a different approach, sometimes used in complement of other sources of information, is to select aligned pairs (or sets) of strings that share long character sequence (work includes (jacquemin, 1997; yarowsky and wicentowski, 2000; <papid> P00-1027 </papid>baroni et al., 2002; <papid> W02-0606 </papid>clark, 2001)).</citsent>
<aftsection>
<nextsent>a notable advantage is that one is not restricted to concatenative morphology.
</nextsent>
<nextsent>many publications (cavar et al, 2004; brent et al., 1995; goldsmith et al, 2001; dejean, 1998;snover et al, 2002; <papid> W02-0602 </papid>argamon et al, 2004; <papid> C04-1152 </papid>goldsmith, 2001; <papid> J01-2001 </papid>creutz and lagus, 2005; neuvel and fulop, 2002; <papid> W02-0604 </papid>baroni, 2003; gaussier, 1999; <papid> W99-0904 </papid>sharma et al, 2002; <papid> W02-0601 </papid>wicentowski, 2002; oliver, 2004),and various other works by the same authors, describe strategies that use frequencies, probabilities,and optimization criteria, often minimum description length (mdl), in various combinations.</nextsent>
<nextsent>so far, all these are unsatisfactory on two main accounts; on the theretical side, they still owe an explanation ofwhy compression or mdl should give birth to segment ations coinciding with morphemes as linguistically defined.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2895">
<title id=" W06-3210.xml">a naive theory of affix ation and an algorithm for extraction </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>thirdly, and most importantly, the algorithm in this paper looks at the slope of the frequency curve not at peaks in absolute frequency.a different approach, sometimes used in complement of other sources of information, is to select aligned pairs (or sets) of strings that share long character sequence (work includes (jacquemin, 1997; yarowsky and wicentowski, 2000; <papid> P00-1027 </papid>baroni et al., 2002; <papid> W02-0606 </papid>clark, 2001)).</prevsent>
<prevsent>a notable advantage is that one is not restricted to concatenative morphology.</prevsent>
</prevsection>
<citsent citstr=" W02-0602 ">
many publications (cavar et al, 2004; brent et al., 1995; goldsmith et al, 2001; dejean, 1998;snover et al, 2002; <papid> W02-0602 </papid>argamon et al, 2004; <papid> C04-1152 </papid>goldsmith, 2001; <papid> J01-2001 </papid>creutz and lagus, 2005; neuvel and fulop, 2002; <papid> W02-0604 </papid>baroni, 2003; gaussier, 1999; <papid> W99-0904 </papid>sharma et al, 2002; <papid> W02-0601 </papid>wicentowski, 2002; oliver, 2004),and various other works by the same authors, describe strategies that use frequencies, probabilities,and optimization criteria, often minimum description length (mdl), in various combinations.</citsent>
<aftsection>
<nextsent>so far, all these are unsatisfactory on two main accounts; on the theretical side, they still owe an explanation ofwhy compression or mdl should give birth to segment ations coinciding with morphemes as linguistically defined.
</nextsent>
<nextsent>on the experimental side, thresholds, supervised/developed parametres and selective input still cloud the success of reported results, which, in any case, arent wide enough to sustain some too rash language independence claims.
</nextsent>
<nextsent>to be more specific, some mdl approaches aim to minimize the description of the set of words in the input corpus, some to describe all tokens in the corpus, but, none aims to minimize, what one would otherwise expect, the set of possible wordsin the language.
</nextsent>
<nextsent>more importantly, none of the reviewed works allow any variation in the description language (model?)
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2897">
<title id=" W06-3210.xml">a naive theory of affix ation and an algorithm for extraction </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>thirdly, and most importantly, the algorithm in this paper looks at the slope of the frequency curve not at peaks in absolute frequency.a different approach, sometimes used in complement of other sources of information, is to select aligned pairs (or sets) of strings that share long character sequence (work includes (jacquemin, 1997; yarowsky and wicentowski, 2000; <papid> P00-1027 </papid>baroni et al., 2002; <papid> W02-0606 </papid>clark, 2001)).</prevsent>
<prevsent>a notable advantage is that one is not restricted to concatenative morphology.</prevsent>
</prevsection>
<citsent citstr=" C04-1152 ">
many publications (cavar et al, 2004; brent et al., 1995; goldsmith et al, 2001; dejean, 1998;snover et al, 2002; <papid> W02-0602 </papid>argamon et al, 2004; <papid> C04-1152 </papid>goldsmith, 2001; <papid> J01-2001 </papid>creutz and lagus, 2005; neuvel and fulop, 2002; <papid> W02-0604 </papid>baroni, 2003; gaussier, 1999; <papid> W99-0904 </papid>sharma et al, 2002; <papid> W02-0601 </papid>wicentowski, 2002; oliver, 2004),and various other works by the same authors, describe strategies that use frequencies, probabilities,and optimization criteria, often minimum description length (mdl), in various combinations.</citsent>
<aftsection>
<nextsent>so far, all these are unsatisfactory on two main accounts; on the theretical side, they still owe an explanation ofwhy compression or mdl should give birth to segment ations coinciding with morphemes as linguistically defined.
</nextsent>
<nextsent>on the experimental side, thresholds, supervised/developed parametres and selective input still cloud the success of reported results, which, in any case, arent wide enough to sustain some too rash language independence claims.
</nextsent>
<nextsent>to be more specific, some mdl approaches aim to minimize the description of the set of words in the input corpus, some to describe all tokens in the corpus, but, none aims to minimize, what one would otherwise expect, the set of possible wordsin the language.
</nextsent>
<nextsent>more importantly, none of the reviewed works allow any variation in the description language (model?)
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2899">
<title id=" W06-3210.xml">a naive theory of affix ation and an algorithm for extraction </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>thirdly, and most importantly, the algorithm in this paper looks at the slope of the frequency curve not at peaks in absolute frequency.a different approach, sometimes used in complement of other sources of information, is to select aligned pairs (or sets) of strings that share long character sequence (work includes (jacquemin, 1997; yarowsky and wicentowski, 2000; <papid> P00-1027 </papid>baroni et al., 2002; <papid> W02-0606 </papid>clark, 2001)).</prevsent>
<prevsent>a notable advantage is that one is not restricted to concatenative morphology.</prevsent>
</prevsection>
<citsent citstr=" J01-2001 ">
many publications (cavar et al, 2004; brent et al., 1995; goldsmith et al, 2001; dejean, 1998;snover et al, 2002; <papid> W02-0602 </papid>argamon et al, 2004; <papid> C04-1152 </papid>goldsmith, 2001; <papid> J01-2001 </papid>creutz and lagus, 2005; neuvel and fulop, 2002; <papid> W02-0604 </papid>baroni, 2003; gaussier, 1999; <papid> W99-0904 </papid>sharma et al, 2002; <papid> W02-0601 </papid>wicentowski, 2002; oliver, 2004),and various other works by the same authors, describe strategies that use frequencies, probabilities,and optimization criteria, often minimum description length (mdl), in various combinations.</citsent>
<aftsection>
<nextsent>so far, all these are unsatisfactory on two main accounts; on the theretical side, they still owe an explanation ofwhy compression or mdl should give birth to segment ations coinciding with morphemes as linguistically defined.
</nextsent>
<nextsent>on the experimental side, thresholds, supervised/developed parametres and selective input still cloud the success of reported results, which, in any case, arent wide enough to sustain some too rash language independence claims.
</nextsent>
<nextsent>to be more specific, some mdl approaches aim to minimize the description of the set of words in the input corpus, some to describe all tokens in the corpus, but, none aims to minimize, what one would otherwise expect, the set of possible wordsin the language.
</nextsent>
<nextsent>more importantly, none of the reviewed works allow any variation in the description language (model?)
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2900">
<title id=" W06-3210.xml">a naive theory of affix ation and an algorithm for extraction </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>thirdly, and most importantly, the algorithm in this paper looks at the slope of the frequency curve not at peaks in absolute frequency.a different approach, sometimes used in complement of other sources of information, is to select aligned pairs (or sets) of strings that share long character sequence (work includes (jacquemin, 1997; yarowsky and wicentowski, 2000; <papid> P00-1027 </papid>baroni et al., 2002; <papid> W02-0606 </papid>clark, 2001)).</prevsent>
<prevsent>a notable advantage is that one is not restricted to concatenative morphology.</prevsent>
</prevsection>
<citsent citstr=" W02-0604 ">
many publications (cavar et al, 2004; brent et al., 1995; goldsmith et al, 2001; dejean, 1998;snover et al, 2002; <papid> W02-0602 </papid>argamon et al, 2004; <papid> C04-1152 </papid>goldsmith, 2001; <papid> J01-2001 </papid>creutz and lagus, 2005; neuvel and fulop, 2002; <papid> W02-0604 </papid>baroni, 2003; gaussier, 1999; <papid> W99-0904 </papid>sharma et al, 2002; <papid> W02-0601 </papid>wicentowski, 2002; oliver, 2004),and various other works by the same authors, describe strategies that use frequencies, probabilities,and optimization criteria, often minimum description length (mdl), in various combinations.</citsent>
<aftsection>
<nextsent>so far, all these are unsatisfactory on two main accounts; on the theretical side, they still owe an explanation ofwhy compression or mdl should give birth to segment ations coinciding with morphemes as linguistically defined.
</nextsent>
<nextsent>on the experimental side, thresholds, supervised/developed parametres and selective input still cloud the success of reported results, which, in any case, arent wide enough to sustain some too rash language independence claims.
</nextsent>
<nextsent>to be more specific, some mdl approaches aim to minimize the description of the set of words in the input corpus, some to describe all tokens in the corpus, but, none aims to minimize, what one would otherwise expect, the set of possible wordsin the language.
</nextsent>
<nextsent>more importantly, none of the reviewed works allow any variation in the description language (model?)
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2901">
<title id=" W06-3210.xml">a naive theory of affix ation and an algorithm for extraction </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>thirdly, and most importantly, the algorithm in this paper looks at the slope of the frequency curve not at peaks in absolute frequency.a different approach, sometimes used in complement of other sources of information, is to select aligned pairs (or sets) of strings that share long character sequence (work includes (jacquemin, 1997; yarowsky and wicentowski, 2000; <papid> P00-1027 </papid>baroni et al., 2002; <papid> W02-0606 </papid>clark, 2001)).</prevsent>
<prevsent>a notable advantage is that one is not restricted to concatenative morphology.</prevsent>
</prevsection>
<citsent citstr=" W99-0904 ">
many publications (cavar et al, 2004; brent et al., 1995; goldsmith et al, 2001; dejean, 1998;snover et al, 2002; <papid> W02-0602 </papid>argamon et al, 2004; <papid> C04-1152 </papid>goldsmith, 2001; <papid> J01-2001 </papid>creutz and lagus, 2005; neuvel and fulop, 2002; <papid> W02-0604 </papid>baroni, 2003; gaussier, 1999; <papid> W99-0904 </papid>sharma et al, 2002; <papid> W02-0601 </papid>wicentowski, 2002; oliver, 2004),and various other works by the same authors, describe strategies that use frequencies, probabilities,and optimization criteria, often minimum description length (mdl), in various combinations.</citsent>
<aftsection>
<nextsent>so far, all these are unsatisfactory on two main accounts; on the theretical side, they still owe an explanation ofwhy compression or mdl should give birth to segment ations coinciding with morphemes as linguistically defined.
</nextsent>
<nextsent>on the experimental side, thresholds, supervised/developed parametres and selective input still cloud the success of reported results, which, in any case, arent wide enough to sustain some too rash language independence claims.
</nextsent>
<nextsent>to be more specific, some mdl approaches aim to minimize the description of the set of words in the input corpus, some to describe all tokens in the corpus, but, none aims to minimize, what one would otherwise expect, the set of possible wordsin the language.
</nextsent>
<nextsent>more importantly, none of the reviewed works allow any variation in the description language (model?)
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2903">
<title id=" W06-3210.xml">a naive theory of affix ation and an algorithm for extraction </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>thirdly, and most importantly, the algorithm in this paper looks at the slope of the frequency curve not at peaks in absolute frequency.a different approach, sometimes used in complement of other sources of information, is to select aligned pairs (or sets) of strings that share long character sequence (work includes (jacquemin, 1997; yarowsky and wicentowski, 2000; <papid> P00-1027 </papid>baroni et al., 2002; <papid> W02-0606 </papid>clark, 2001)).</prevsent>
<prevsent>a notable advantage is that one is not restricted to concatenative morphology.</prevsent>
</prevsection>
<citsent citstr=" W02-0601 ">
many publications (cavar et al, 2004; brent et al., 1995; goldsmith et al, 2001; dejean, 1998;snover et al, 2002; <papid> W02-0602 </papid>argamon et al, 2004; <papid> C04-1152 </papid>goldsmith, 2001; <papid> J01-2001 </papid>creutz and lagus, 2005; neuvel and fulop, 2002; <papid> W02-0604 </papid>baroni, 2003; gaussier, 1999; <papid> W99-0904 </papid>sharma et al, 2002; <papid> W02-0601 </papid>wicentowski, 2002; oliver, 2004),and various other works by the same authors, describe strategies that use frequencies, probabilities,and optimization criteria, often minimum description length (mdl), in various combinations.</citsent>
<aftsection>
<nextsent>so far, all these are unsatisfactory on two main accounts; on the theretical side, they still owe an explanation ofwhy compression or mdl should give birth to segment ations coinciding with morphemes as linguistically defined.
</nextsent>
<nextsent>on the experimental side, thresholds, supervised/developed parametres and selective input still cloud the success of reported results, which, in any case, arent wide enough to sustain some too rash language independence claims.
</nextsent>
<nextsent>to be more specific, some mdl approaches aim to minimize the description of the set of words in the input corpus, some to describe all tokens in the corpus, but, none aims to minimize, what one would otherwise expect, the set of possible wordsin the language.
</nextsent>
<nextsent>more importantly, none of the reviewed works allow any variation in the description language (model?)
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2905">
<title id=" W06-3210.xml">a naive theory of affix ation and an algorithm for extraction </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>a systematic test of segmentation algorithms over many different types of languages has yet to be published.
</prevsent>
<prevsent>for three reasons, it will not be undertaken here either.
</prevsent>
</prevsection>
<citsent citstr=" W98-1240 ">
first, as e.g already manning (1998) <papid> W98-1240 </papid>notes for sandhi phenomena, it is far from clear what the gold standard should be (even though we may agree or agree to disagree on some familiar european languages).</citsent>
<aftsection>
<nextsent>secondly, segmentation algorithms may have different purposes and it might not make good sense to study segmentation in isolation from induction of paradigms.
</nextsent>
<nextsent>lastly, and most importantly, all of the reviewed techniques (wicen towski, 2004; <papid> W04-0109 </papid>wicentowski, 2002; snover et al, 2002; <papid> W02-0602 </papid>baroni et al, 2002; <papid> W02-0606 </papid>andreev, 1965; cavar et al, 2004; snover and brent, 2003; snover and brent, 2001; <papid> P01-1063 </papid>snover, 2002; schone and jurafsky, 2001; <papid> N01-1024 </papid>jacquemin, 1997; goldsmith and hu, 2004;sharma et al, 2002; <papid> W02-0601 </papid>clark, 2001; kazakov andman andhar, 1998; dejean, 1998; oliver, 2004; creutz and lagus, 2002; creutz and lagus, 2003; creutz and lagus, 2004; hirsimaki et al, 2003; creutz and lagus, 2005; argamon et al, 2004; <papid> C04-1152 </papid>gaussier, 1999; <papid> W99-0904 </papid>lehmann, 1973; langer, 1991; flenner, 1995;klenk and langer, 1989; goldsmith, 2001; <papid> J01-2001 </papid>goldsmith, 2000; hu et al, 2005<papid> W05-0504 </papid>b; hu et al, 2005<papid> W05-0504 </papid>a; brent et al, 1995), as they are described, have threshold-parameters of some sort, explicitly claim not to work well for an open set of languages, or require noise-free all-form input (albright, 2002; manning, 1998; <papid> W98-1240 </papid>borin, 1991).</nextsent>
<nextsent>therefore it is not possible to even design fair test.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2906">
<title id=" W06-3210.xml">a naive theory of affix ation and an algorithm for extraction </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>first, as e.g already manning (1998) <papid> W98-1240 </papid>notes for sandhi phenomena, it is far from clear what the gold standard should be (even though we may agree or agree to disagree on some familiar european languages).</prevsent>
<prevsent>secondly, segmentation algorithms may have different purposes and it might not make good sense to study segmentation in isolation from induction of paradigms.</prevsent>
</prevsection>
<citsent citstr=" W04-0109 ">
lastly, and most importantly, all of the reviewed techniques (wicen towski, 2004; <papid> W04-0109 </papid>wicentowski, 2002; snover et al, 2002; <papid> W02-0602 </papid>baroni et al, 2002; <papid> W02-0606 </papid>andreev, 1965; cavar et al, 2004; snover and brent, 2003; snover and brent, 2001; <papid> P01-1063 </papid>snover, 2002; schone and jurafsky, 2001; <papid> N01-1024 </papid>jacquemin, 1997; goldsmith and hu, 2004;sharma et al, 2002; <papid> W02-0601 </papid>clark, 2001; kazakov andman andhar, 1998; dejean, 1998; oliver, 2004; creutz and lagus, 2002; creutz and lagus, 2003; creutz and lagus, 2004; hirsimaki et al, 2003; creutz and lagus, 2005; argamon et al, 2004; <papid> C04-1152 </papid>gaussier, 1999; <papid> W99-0904 </papid>lehmann, 1973; langer, 1991; flenner, 1995;klenk and langer, 1989; goldsmith, 2001; <papid> J01-2001 </papid>goldsmith, 2000; hu et al, 2005<papid> W05-0504 </papid>b; hu et al, 2005<papid> W05-0504 </papid>a; brent et al, 1995), as they are described, have threshold-parameters of some sort, explicitly claim not to work well for an open set of languages, or require noise-free all-form input (albright, 2002; manning, 1998; <papid> W98-1240 </papid>borin, 1991).</citsent>
<aftsection>
<nextsent>therefore it is not possible to even design fair test.
</nextsent>
<nextsent>in any event, we wish to appeal to the merits of developing theory in parallel with experimentation ? as opposed to only ad hoc result chasing.
</nextsent>
<nextsent>if we have theory and we dont get the results we want, 85 wemay scrutinize the assumptions behind the theory in order to modify or reject it (understanding why we did so).
</nextsent>
<nextsent>without theory theres no telling what to do or how to interpret intermediate numbers in long series of calculations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2910">
<title id=" W06-3210.xml">a naive theory of affix ation and an algorithm for extraction </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>first, as e.g already manning (1998) <papid> W98-1240 </papid>notes for sandhi phenomena, it is far from clear what the gold standard should be (even though we may agree or agree to disagree on some familiar european languages).</prevsent>
<prevsent>secondly, segmentation algorithms may have different purposes and it might not make good sense to study segmentation in isolation from induction of paradigms.</prevsent>
</prevsection>
<citsent citstr=" P01-1063 ">
lastly, and most importantly, all of the reviewed techniques (wicen towski, 2004; <papid> W04-0109 </papid>wicentowski, 2002; snover et al, 2002; <papid> W02-0602 </papid>baroni et al, 2002; <papid> W02-0606 </papid>andreev, 1965; cavar et al, 2004; snover and brent, 2003; snover and brent, 2001; <papid> P01-1063 </papid>snover, 2002; schone and jurafsky, 2001; <papid> N01-1024 </papid>jacquemin, 1997; goldsmith and hu, 2004;sharma et al, 2002; <papid> W02-0601 </papid>clark, 2001; kazakov andman andhar, 1998; dejean, 1998; oliver, 2004; creutz and lagus, 2002; creutz and lagus, 2003; creutz and lagus, 2004; hirsimaki et al, 2003; creutz and lagus, 2005; argamon et al, 2004; <papid> C04-1152 </papid>gaussier, 1999; <papid> W99-0904 </papid>lehmann, 1973; langer, 1991; flenner, 1995;klenk and langer, 1989; goldsmith, 2001; <papid> J01-2001 </papid>goldsmith, 2000; hu et al, 2005<papid> W05-0504 </papid>b; hu et al, 2005<papid> W05-0504 </papid>a; brent et al, 1995), as they are described, have threshold-parameters of some sort, explicitly claim not to work well for an open set of languages, or require noise-free all-form input (albright, 2002; manning, 1998; <papid> W98-1240 </papid>borin, 1991).</citsent>
<aftsection>
<nextsent>therefore it is not possible to even design fair test.
</nextsent>
<nextsent>in any event, we wish to appeal to the merits of developing theory in parallel with experimentation ? as opposed to only ad hoc result chasing.
</nextsent>
<nextsent>if we have theory and we dont get the results we want, 85 wemay scrutinize the assumptions behind the theory in order to modify or reject it (understanding why we did so).
</nextsent>
<nextsent>without theory theres no telling what to do or how to interpret intermediate numbers in long series of calculations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2920">
<title id=" W06-3210.xml">a naive theory of affix ation and an algorithm for extraction </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>first, as e.g already manning (1998) <papid> W98-1240 </papid>notes for sandhi phenomena, it is far from clear what the gold standard should be (even though we may agree or agree to disagree on some familiar european languages).</prevsent>
<prevsent>secondly, segmentation algorithms may have different purposes and it might not make good sense to study segmentation in isolation from induction of paradigms.</prevsent>
</prevsection>
<citsent citstr=" W05-0504 ">
lastly, and most importantly, all of the reviewed techniques (wicen towski, 2004; <papid> W04-0109 </papid>wicentowski, 2002; snover et al, 2002; <papid> W02-0602 </papid>baroni et al, 2002; <papid> W02-0606 </papid>andreev, 1965; cavar et al, 2004; snover and brent, 2003; snover and brent, 2001; <papid> P01-1063 </papid>snover, 2002; schone and jurafsky, 2001; <papid> N01-1024 </papid>jacquemin, 1997; goldsmith and hu, 2004;sharma et al, 2002; <papid> W02-0601 </papid>clark, 2001; kazakov andman andhar, 1998; dejean, 1998; oliver, 2004; creutz and lagus, 2002; creutz and lagus, 2003; creutz and lagus, 2004; hirsimaki et al, 2003; creutz and lagus, 2005; argamon et al, 2004; <papid> C04-1152 </papid>gaussier, 1999; <papid> W99-0904 </papid>lehmann, 1973; langer, 1991; flenner, 1995;klenk and langer, 1989; goldsmith, 2001; <papid> J01-2001 </papid>goldsmith, 2000; hu et al, 2005<papid> W05-0504 </papid>b; hu et al, 2005<papid> W05-0504 </papid>a; brent et al, 1995), as they are described, have threshold-parameters of some sort, explicitly claim not to work well for an open set of languages, or require noise-free all-form input (albright, 2002; manning, 1998; <papid> W98-1240 </papid>borin, 1991).</citsent>
<aftsection>
<nextsent>therefore it is not possible to even design fair test.
</nextsent>
<nextsent>in any event, we wish to appeal to the merits of developing theory in parallel with experimentation ? as opposed to only ad hoc result chasing.
</nextsent>
<nextsent>if we have theory and we dont get the results we want, 85 wemay scrutinize the assumptions behind the theory in order to modify or reject it (understanding why we did so).
</nextsent>
<nextsent>without theory theres no telling what to do or how to interpret intermediate numbers in long series of calculations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2929">
<title id=" W06-1611.xml">exploiting discourse structure for spoken dialogue performance analysis </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>an extensive set of parameters can be found in (mller, 2005a).
</prevsent>
<prevsent>in this paper we study the utility of discourse structure as an information source for sds performance analysis.
</prevsent>
</prevsection>
<citsent citstr=" P96-1038 ">
the discourse structure hierarchy has been shown to be useful for other tasks: understanding specific lexical and prosodic phenomena (hirschberg and nakatani, 1996; <papid> P96-1038 </papid>levow, 2004), <papid> W04-2318 </papid>natural language generation (hovy, 1993), predictive/generative models of postural shifts (cassell et al, 2001), <papid> P01-1016 </papid>and essay scoring (higgins et al, 2004).<papid> N04-1024 </papid></citsent>
<aftsection>
<nextsent>we perform our analysis on corpus of speech-based tutoring dialogues.
</nextsent>
<nextsent>a tutoring sds (litman and silliman, 2004; <papid> N04-3002 </papid>pon-barry et al, 2004) has to discuss concepts, laws and relationships and to engage in complex sub dialogues to correct student misconceptions.</nextsent>
<nextsent>as result, dialogues with such systems have rich discourse structure.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2931">
<title id=" W06-1611.xml">exploiting discourse structure for spoken dialogue performance analysis </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>an extensive set of parameters can be found in (mller, 2005a).
</prevsent>
<prevsent>in this paper we study the utility of discourse structure as an information source for sds performance analysis.
</prevsent>
</prevsection>
<citsent citstr=" W04-2318 ">
the discourse structure hierarchy has been shown to be useful for other tasks: understanding specific lexical and prosodic phenomena (hirschberg and nakatani, 1996; <papid> P96-1038 </papid>levow, 2004), <papid> W04-2318 </papid>natural language generation (hovy, 1993), predictive/generative models of postural shifts (cassell et al, 2001), <papid> P01-1016 </papid>and essay scoring (higgins et al, 2004).<papid> N04-1024 </papid></citsent>
<aftsection>
<nextsent>we perform our analysis on corpus of speech-based tutoring dialogues.
</nextsent>
<nextsent>a tutoring sds (litman and silliman, 2004; <papid> N04-3002 </papid>pon-barry et al, 2004) has to discuss concepts, laws and relationships and to engage in complex sub dialogues to correct student misconceptions.</nextsent>
<nextsent>as result, dialogues with such systems have rich discourse structure.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2933">
<title id=" W06-1611.xml">exploiting discourse structure for spoken dialogue performance analysis </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>an extensive set of parameters can be found in (mller, 2005a).
</prevsent>
<prevsent>in this paper we study the utility of discourse structure as an information source for sds performance analysis.
</prevsent>
</prevsection>
<citsent citstr=" P01-1016 ">
the discourse structure hierarchy has been shown to be useful for other tasks: understanding specific lexical and prosodic phenomena (hirschberg and nakatani, 1996; <papid> P96-1038 </papid>levow, 2004), <papid> W04-2318 </papid>natural language generation (hovy, 1993), predictive/generative models of postural shifts (cassell et al, 2001), <papid> P01-1016 </papid>and essay scoring (higgins et al, 2004).<papid> N04-1024 </papid></citsent>
<aftsection>
<nextsent>we perform our analysis on corpus of speech-based tutoring dialogues.
</nextsent>
<nextsent>a tutoring sds (litman and silliman, 2004; <papid> N04-3002 </papid>pon-barry et al, 2004) has to discuss concepts, laws and relationships and to engage in complex sub dialogues to correct student misconceptions.</nextsent>
<nextsent>as result, dialogues with such systems have rich discourse structure.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2934">
<title id=" W06-1611.xml">exploiting discourse structure for spoken dialogue performance analysis </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>an extensive set of parameters can be found in (mller, 2005a).
</prevsent>
<prevsent>in this paper we study the utility of discourse structure as an information source for sds performance analysis.
</prevsent>
</prevsection>
<citsent citstr=" N04-1024 ">
the discourse structure hierarchy has been shown to be useful for other tasks: understanding specific lexical and prosodic phenomena (hirschberg and nakatani, 1996; <papid> P96-1038 </papid>levow, 2004), <papid> W04-2318 </papid>natural language generation (hovy, 1993), predictive/generative models of postural shifts (cassell et al, 2001), <papid> P01-1016 </papid>and essay scoring (higgins et al, 2004).<papid> N04-1024 </papid></citsent>
<aftsection>
<nextsent>we perform our analysis on corpus of speech-based tutoring dialogues.
</nextsent>
<nextsent>a tutoring sds (litman and silliman, 2004; <papid> N04-3002 </papid>pon-barry et al, 2004) has to discuss concepts, laws and relationships and to engage in complex sub dialogues to correct student misconceptions.</nextsent>
<nextsent>as result, dialogues with such systems have rich discourse structure.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2935">
<title id=" W06-1611.xml">exploiting discourse structure for spoken dialogue performance analysis </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the discourse structure hierarchy has been shown to be useful for other tasks: understanding specific lexical and prosodic phenomena (hirschberg and nakatani, 1996; <papid> P96-1038 </papid>levow, 2004), <papid> W04-2318 </papid>natural language generation (hovy, 1993), predictive/generative models of postural shifts (cassell et al, 2001), <papid> P01-1016 </papid>and essay scoring (higgins et al, 2004).<papid> N04-1024 </papid></prevsent>
<prevsent>we perform our analysis on corpus of speech-based tutoring dialogues.</prevsent>
</prevsection>
<citsent citstr=" N04-3002 ">
a tutoring sds (litman and silliman, 2004; <papid> N04-3002 </papid>pon-barry et al, 2004) has to discuss concepts, laws and relationships and to engage in complex sub dialogues to correct student misconceptions.</citsent>
<aftsection>
<nextsent>as result, dialogues with such systems have rich discourse structure.
</nextsent>
<nextsent>we perform three experiments to measure three ways of exploiting the discourse structure.
</nextsent>
<nextsent>in our first experiment, we test the predictive utility of the discourse structure in itself.
</nextsent>
<nextsent>forex ample, we look at whether the number of pop-up transitions in the discourse structure hierarchy predicts performance in our system.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2936">
<title id=" W06-1611.xml">exploiting discourse structure for spoken dialogue performance analysis </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the intuition behind this experiment is that interaction events should be treated differently based on their position in the discourse structure hierarchy.
</prevsent>
<prevsent>for example, we test if the number of incorrect answers after pop-up transition has higher predictive utility than the total number of incorrect student answers.
</prevsent>
</prevsection>
<citsent citstr=" P01-1066 ">
in contrast, the majority of the previous work either ignores this contextual information (mller, 2005a; walker et al, 2000) or makes limited use of the 85 discourse structure hierarchy by flattening it (walker et al, 2001) (<papid> P01-1066 </papid>section 5).</citsent>
<aftsection>
<nextsent>as another way to exploit the discourse structure, in our third experiment we look at whether specific trajectories in the discourse structure are indicative of performance.
</nextsent>
<nextsent>for example, we test if two consecutive pushes in the discourse structure are correlated with higher learning.
</nextsent>
<nextsent>to measure the predictive utility of our interaction parameters, we focus primarily on correlations with our performance metric (section 4).
</nextsent>
<nextsent>there are two reasons for this.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2938">
<title id=" W06-1611.xml">exploiting discourse structure for spoken dialogue performance analysis </title>
<section> annotation.  </section>
<citcontext>
<prevsection>
<prevsent>the resulting corpus had 2334 student turns and comparable number of system turns.
</prevsent>
<prevsent>2.1 discourse structure.
</prevsent>
</prevsection>
<citsent citstr=" J86-3001 ">
we base our annotation of discourse structure on the grosz &amp; sidner theory of discourse structure (grosz and sidner, 1986).<papid> J86-3001 </papid></citsent>
<aftsection>
<nextsent>a critical ingredient of this theory is the intentional structure.
</nextsent>
<nextsent>according to the theory, each discourse has discourse pur pose/intention.
</nextsent>
<nextsent>satisfying the main discourse purpose is achieved by satisfying several smaller purposes/intentions organized in hierarchical structure.
</nextsent>
<nextsent>as result, the discourse is segmented in discourse segments each with an associated discourse segment purpose/intention.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2943">
<title id=" W06-1611.xml">exploiting discourse structure for spoken dialogue performance analysis </title>
<section> results.  </section>
<citcontext>
<prevsection>
<prevsent>in our corpus, we find that the more such big knowledge gaps are discovered and addressed by the system the higher the learning gain.
</prevsent>
<prevsent>the samegoalpush bigram captures another type of behavior after system rejections that is positively correlated with learning (recall the samegoalneutral bigram, table 4).
</prevsent>
</prevsection>
<citsent citstr=" P06-1025 ">
in our previous work (rotaru and litman, 2006), <papid> P06-1025 </papid>we per 90 formed an analysis of the rejected student turns and studied how rejections affect the student state.</citsent>
<aftsection>
<nextsent>the results of our analysis suggested new strategy for handling rejections in the tutoring domain: instead of rejecting student answers, tutoring sds should make use of the available information.
</nextsent>
<nextsent>since the recognition hypothesis for rejected student turn would be interpreted most likely as an incorrect answer thus activating remediation sub dialogue, the positive correlation between samegoalpush and learning suggests that the new strategy will not impact learning.
</nextsent>
<nextsent>similar to the second experiment, the results of our third experiment are also positive: in contrast to transition unigrams, our domain independent trajectories can produce parameters with high predictive utility.
</nextsent>
<nextsent>4.4 paradise modeling.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2953">
<title id=" W06-1611.xml">exploiting discourse structure for spoken dialogue performance analysis </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>all these parameters can be linked to the discourse structure but flatten the discourse structure.
</prevsent>
<prevsent>moreover, the most informative of these parameters (the task model parameters) are domain dependent.
</prevsent>
</prevsection>
<citsent citstr=" P04-1044 ">
similar approximations of the discourse structure are also common for other sds tasks like predictive models of speech recognition problems (gabsdil and lemon, 2004).<papid> P04-1044 </papid></citsent>
<aftsection>
<nextsent>we extend over previous work in several areas.
</nextsent>
<nextsent>first, we exploit in more detail the hierarchical information in the discourse structure.
</nextsent>
<nextsent>we quantify this information by recording the discourse structure transitions.
</nextsent>
<nextsent>second, in contrast to previous work, our usage of discourse structure is domain independent (the transitions).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2954">
<title id=" W06-3408.xml">chat a time linked system for conversational analysis </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>as such, number of tools exist to automatically categorize, cluster, and extract information from documents.
</prevsent>
<prevsent>however, these tools do not transfer well to data sources that are more conversational in nature, such as multi-party meetings, telephone conversations, email, chat rooms, etc. given the plethora of these data sources, there is need to be able to quickly and accurately extract and process pertinent information from these sources without having to cull them manually.
</prevsent>
</prevsection>
<citsent citstr=" P03-1071 ">
much of the work on computational analysis of dialogue has focused on automatic topic segmentation of conversational data, and in particular, using features of the discourse to aid in segmentation (galley et al 2003; <papid> P03-1071 </papid>stolcke et al, 1999; hirschberg &amp; haka tani, 1996.).<papid> P96-1038 </papid></citsent>
<aftsection>
<nextsent>detailed discourse and conversational analytics have been the focus of much linguistic research and have been used by the computational community for creating models of dialogue to aid in natural language understanding and generation (allen &amp; core, 1997; carletta et al, 1997; van deemter et al, 2005; walker et al, 1996).
</nextsent>
<nextsent>however, there has been much less focus on computational tools that can aid in either the analysis of conversations themselves, or in rendering conversational data in ways such that it can be used with traditional data mining techniques that have been successful for document understanding.
</nextsent>
<nextsent>this current work is most similar to the nite xml toolkit (carletta &amp; kilgour, 2005) which was designed for annotating conversational data.
</nextsent>
<nextsent>nite xml is system in which transcripts of conversations are viewable and time aligned with their audio transcripts.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2955">
<title id=" W06-3408.xml">chat a time linked system for conversational analysis </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>as such, number of tools exist to automatically categorize, cluster, and extract information from documents.
</prevsent>
<prevsent>however, these tools do not transfer well to data sources that are more conversational in nature, such as multi-party meetings, telephone conversations, email, chat rooms, etc. given the plethora of these data sources, there is need to be able to quickly and accurately extract and process pertinent information from these sources without having to cull them manually.
</prevsent>
</prevsection>
<citsent citstr=" P96-1038 ">
much of the work on computational analysis of dialogue has focused on automatic topic segmentation of conversational data, and in particular, using features of the discourse to aid in segmentation (galley et al 2003; <papid> P03-1071 </papid>stolcke et al, 1999; hirschberg &amp; haka tani, 1996.).<papid> P96-1038 </papid></citsent>
<aftsection>
<nextsent>detailed discourse and conversational analytics have been the focus of much linguistic research and have been used by the computational community for creating models of dialogue to aid in natural language understanding and generation (allen &amp; core, 1997; carletta et al, 1997; van deemter et al, 2005; walker et al, 1996).
</nextsent>
<nextsent>however, there has been much less focus on computational tools that can aid in either the analysis of conversations themselves, or in rendering conversational data in ways such that it can be used with traditional data mining techniques that have been successful for document understanding.
</nextsent>
<nextsent>this current work is most similar to the nite xml toolkit (carletta &amp; kilgour, 2005) which was designed for annotating conversational data.
</nextsent>
<nextsent>nite xml is system in which transcripts of conversations are viewable and time aligned with their audio transcripts.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2957">
<title id=" W06-3408.xml">chat a time linked system for conversational analysis </title>
<section> chat architecture.  </section>
<citcontext>
<prevsection>
<prevsent>the output of the ingest process is list of utterance that include time (or sequence) stamp, participant name, and an utterance.
</prevsent>
<prevsent>topic segmentation is then performed on the utterances to chunk them into topically cohesive units.
</prevsent>
</prevsection>
<citsent citstr=" J97-1003 ">
traditionally, algorithms for segmentation have relied on textual cues (hearst, 1997; <papid> J97-1003 </papid>miller et al 1998; beeferman et al 1999; choi, 2000).<papid> A00-2004 </papid></citsent>
<aftsection>
<nextsent>these techniques have proved useful in segmenting single authored documents that are rich in content and where there is great deal of topic continuity.
</nextsent>
<nextsent>topic segmentation of conversational data is much more difficult due to often sparse content, intertwined topics, and lack of topic continuity.
</nextsent>
<nextsent>topic segmentation algorithms generally relyon lexical cohesion signal that requires smoothing in order to eliminate noise from changes of word choices in adjoining statements that do not indicate topic shifts (hearst, 1997; <papid> J97-1003 </papid>barzilay and elhadad, 1997).<papid> W97-0703 </papid></nextsent>
<nextsent>many state of the art techniques use sliding window for smoothing (hearst, 1997; <papid> J97-1003 </papid>miller et al. 1998; galley et al, 2003).<papid> P03-1071 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2960">
<title id=" W06-3408.xml">chat a time linked system for conversational analysis </title>
<section> chat architecture.  </section>
<citcontext>
<prevsection>
<prevsent>the output of the ingest process is list of utterance that include time (or sequence) stamp, participant name, and an utterance.
</prevsent>
<prevsent>topic segmentation is then performed on the utterances to chunk them into topically cohesive units.
</prevsent>
</prevsection>
<citsent citstr=" A00-2004 ">
traditionally, algorithms for segmentation have relied on textual cues (hearst, 1997; <papid> J97-1003 </papid>miller et al 1998; beeferman et al 1999; choi, 2000).<papid> A00-2004 </papid></citsent>
<aftsection>
<nextsent>these techniques have proved useful in segmenting single authored documents that are rich in content and where there is great deal of topic continuity.
</nextsent>
<nextsent>topic segmentation of conversational data is much more difficult due to often sparse content, intertwined topics, and lack of topic continuity.
</nextsent>
<nextsent>topic segmentation algorithms generally relyon lexical cohesion signal that requires smoothing in order to eliminate noise from changes of word choices in adjoining statements that do not indicate topic shifts (hearst, 1997; <papid> J97-1003 </papid>barzilay and elhadad, 1997).<papid> W97-0703 </papid></nextsent>
<nextsent>many state of the art techniques use sliding window for smoothing (hearst, 1997; <papid> J97-1003 </papid>miller et al. 1998; galley et al, 2003).<papid> P03-1071 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2964">
<title id=" W06-3408.xml">chat a time linked system for conversational analysis </title>
<section> chat architecture.  </section>
<citcontext>
<prevsection>
<prevsent>these techniques have proved useful in segmenting single authored documents that are rich in content and where there is great deal of topic continuity.
</prevsent>
<prevsent>topic segmentation of conversational data is much more difficult due to often sparse content, intertwined topics, and lack of topic continuity.
</prevsent>
</prevsection>
<citsent citstr=" W97-0703 ">
topic segmentation algorithms generally relyon lexical cohesion signal that requires smoothing in order to eliminate noise from changes of word choices in adjoining statements that do not indicate topic shifts (hearst, 1997; <papid> J97-1003 </papid>barzilay and elhadad, 1997).<papid> W97-0703 </papid></citsent>
<aftsection>
<nextsent>many state of the art techniques use sliding window for smoothing (hearst, 1997; <papid> J97-1003 </papid>miller et al. 1998; galley et al, 2003).<papid> P03-1071 </papid></nextsent>
<nextsent>we employ windowless method (wlm) for calculating suitable cohesion signal which does not relyon sliding window to achieve the requisite smoothing for an effective segmentation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2977">
<title id=" W06-3201.xml">a combined phonetic phonological approach to estimating cross language phoneme similarity in an asr environment </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>not surprisingly, label-based cross-language transfer experiments have produced poor performance results.
</prevsent>
<prevsent>in contrast to the subjective, label-based strategy, researchers in such fields as language reconstruction, dialectometry, and child language development, commonly use automatic feature based approaches to articulatory similarity between phonemes.
</prevsent>
</prevsection>
<citsent citstr=" E95-1009 ">
in these methods, phonemes are represented by distinctive feature vector and phonetic distance or similarity algorithm is used to align phoneme strings between related words (connolly 1997; kessler 1995, <papid> E95-1009 </papid>2005; kondrak 2002; nerbonne and heeringa 1997; <papid> W97-1102 </papid>somers 1998).<papid> P98-2200 </papid></citsent>
<aftsection>
<nextsent>significantly, in these approaches, phonological similarity is generally assumed.
</nextsent>
<nextsent>in principle, the feature-based approach to phonetic distance admits more precise specification of phonemes because it supports allophonic variance.
</nextsent>
<nextsent>for example, standard feature-based approach to allo phony representation restricts feature inclusion to only those features relevant to all realizations of the phoneme.
</nextsent>
<nextsent>another common approach retains features that are relevant to all allophonic variants, but leaves their values underspecified (archangeli 1988).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2978">
<title id=" W06-3201.xml">a combined phonetic phonological approach to estimating cross language phoneme similarity in an asr environment </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>not surprisingly, label-based cross-language transfer experiments have produced poor performance results.
</prevsent>
<prevsent>in contrast to the subjective, label-based strategy, researchers in such fields as language reconstruction, dialectometry, and child language development, commonly use automatic feature based approaches to articulatory similarity between phonemes.
</prevsent>
</prevsection>
<citsent citstr=" W97-1102 ">
in these methods, phonemes are represented by distinctive feature vector and phonetic distance or similarity algorithm is used to align phoneme strings between related words (connolly 1997; kessler 1995, <papid> E95-1009 </papid>2005; kondrak 2002; nerbonne and heeringa 1997; <papid> W97-1102 </papid>somers 1998).<papid> P98-2200 </papid></citsent>
<aftsection>
<nextsent>significantly, in these approaches, phonological similarity is generally assumed.
</nextsent>
<nextsent>in principle, the feature-based approach to phonetic distance admits more precise specification of phonemes because it supports allophonic variance.
</nextsent>
<nextsent>for example, standard feature-based approach to allo phony representation restricts feature inclusion to only those features relevant to all realizations of the phoneme.
</nextsent>
<nextsent>another common approach retains features that are relevant to all allophonic variants, but leaves their values underspecified (archangeli 1988).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2979">
<title id=" W06-3201.xml">a combined phonetic phonological approach to estimating cross language phoneme similarity in an asr environment </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>not surprisingly, label-based cross-language transfer experiments have produced poor performance results.
</prevsent>
<prevsent>in contrast to the subjective, label-based strategy, researchers in such fields as language reconstruction, dialectometry, and child language development, commonly use automatic feature based approaches to articulatory similarity between phonemes.
</prevsent>
</prevsection>
<citsent citstr=" P98-2200 ">
in these methods, phonemes are represented by distinctive feature vector and phonetic distance or similarity algorithm is used to align phoneme strings between related words (connolly 1997; kessler 1995, <papid> E95-1009 </papid>2005; kondrak 2002; nerbonne and heeringa 1997; <papid> W97-1102 </papid>somers 1998).<papid> P98-2200 </papid></citsent>
<aftsection>
<nextsent>significantly, in these approaches, phonological similarity is generally assumed.
</nextsent>
<nextsent>in principle, the feature-based approach to phonetic distance admits more precise specification of phonemes because it supports allophonic variance.
</nextsent>
<nextsent>for example, standard feature-based approach to allo phony representation restricts feature inclusion to only those features relevant to all realizations of the phoneme.
</nextsent>
<nextsent>another common approach retains features that are relevant to all allophonic variants, but leaves their values underspecified (archangeli 1988).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2980">
<title id=" W06-3201.xml">a combined phonetic phonological approach to estimating cross language phoneme similarity in an asr environment </title>
<section> phonetic distance.  </section>
<citcontext>
<prevsection>
<prevsent>in our experiments to date, we use the manhattan distance where the distance between phonemes equals the sum of the absolute values of individual feature distances.
</prevsent>
<prevsent>this approach is fairly standard in the literature, though the euclidean distance has also been reported to attain good results (kessler 2005).
</prevsent>
</prevsection>
<citsent citstr=" C69-5701 ">
because features are known to differ in relative importance (ladefoged 1969), <papid> C69-5701 </papid>some researchers apply weights or saliencies to the individual features for distance calculation.</citsent>
<aftsection>
<nextsent>nerbonne and heeringa (1997), <papid> W97-1102 </papid>for example, weighted each feature by information gain, or entropy reduction.</nextsent>
<nextsent>kondrak (2002) expressed weights as coefficients that could 4 be changed to any numeric value.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2982">
<title id=" W06-3703.xml">speech to speech translation for medical triage in korean </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>in 1992, these groups joined atr in the c-star consortium (consortium for speech translation advanced research) and in january 1993 gave successful public demonstration of telephone translation between english, german and japanese, within the limited domain of conference registrations (woszczyna, 1993).
</prevsent>
<prevsent>a number of other large companies and laboratories including nec (isotani, et al, 2003) in japan, the verb mobil consortium (wahlster, 2000), nespole!
</prevsent>
</prevsection>
<citsent citstr=" N01-1018 ">
consortium (florian et al, 2002), at&t; (bangalore and riccardi, 2001), <papid> N01-1018 </papid>and atr have been making their own research effort (yasuda et al, 2003).</citsent>
<aftsection>
<nextsent>lc-star and tc-star are two recent european efforts to gather the data and the industrial requirements to enable pervasive speech to-speech translation (zhang, 2003).
</nextsent>
<nextsent>most recently, the darpa tran stac program (previously known as babylon) has been focusing on developing deployable systems for english to iraqi arabic.
</nextsent>
<nextsent>unlike other systems that try to solve the speech translation problem with the assumption that there is moderate amount of data available, s-minds focuses on rapid building and deployment of speech translation systems in languages where little or no data is available.
</nextsent>
<nextsent>s-minds allows the user to communicate easily in question-andanswer, interview-style conversation across languages in limited domains such as border control, hospital admissions or medical triage, or other narrow interview fields.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2983">
<title id=" W07-0212.xml">how difficult is it to develop a perfect spell checker a cross linguistic analysis through complex network approach </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a similar observation has been previously reported in (bhatt et al , 2005) for rwes in bengali and english.
</prevsent>
<prevsent>apart from providing insight into spell-checking, the complex structure of spell net al reveals theself-organization and evolutionary dynamics underlying the orthographic properties of natural languages.
</prevsent>
</prevsection>
<citsent citstr=" P06-2017 ">
in recent times, complex networks have been successfully employed to model and explain the structure and organization of several natural and social phenomena, such as the foodweb, protien interaction, formation of language inventories (choudhury et al , 2006), <papid> P06-2017 </papid>syntactic structure of languages (i cancho and sole?, 2004), www, social collaboration, scientific citations and many more (see (albert and barabasi, 2002; newman, 2003) and references therein).</citsent>
<aftsection>
<nextsent>this work is inspired by the aforementioned models, and more specificallya couple of similar works on phonological neighbors?
</nextsent>
<nextsent>network of words (kapatsinski, 2006; vitevitch, 2005), which try to explain the human perceptual and cognitive processes in terms of the organization of the mental lexicon.the rest of the paper is organized as follows.
</nextsent>
<nextsent>section 2 defines the structure and construction procedure of spellnet.
</nextsent>
<nextsent>section 3 and 4 describes the degree and clustering related properties of spell net and their significance in the context of spell checking, respectively.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2985">
<title id=" W07-0212.xml">how difficult is it to develop a perfect spell checker a cross linguistic analysis through complex network approach </title>
<section> conclusion.  </section>
<citcontext>
<prevsection>
<prevsent>another interesting observation is that the phonemic nature of the orthography strongly correlates with the difficulty of spell-checking.
</prevsent>
<prevsent>among the three languages, hindi has the most phonemic and english the least phonemic orthography.
</prevsent>
</prevsection>
<citsent citstr=" P06-3002 ">
this correlation calls for further investigation.throughout the present discussion, we have focussed on spell-checkers that ignore the context; consequently, many of the aforementioned results, especially those involving spelling correction, are valid only for context-insensitive spell-checkers.nevertheless, many of the practically useful spell checkers incorporate context information and the current analysis on spell net can be extended for such spell-checkers by conceptualizing network of words that capture the word co-occurrence patterns (biemann, 2006).<papid> P06-3002 </papid></citsent>
<aftsection>
<nextsent>the word co-occurrence network can be superimposed on spell net and the properties of the resulting structure can be appropriately analyzed to obtain similar bounds on hardness of context-sensitive spell-checkers.
</nextsent>
<nextsent>we deem this to be part of our future work.
</nextsent>
<nextsent>another way to improve the study could be to incorporate more realistic measure for the orthographic similarity between the words.
</nextsent>
<nextsent>nevertheless, such modification will have no effect on the analysis technique, though the results of the analysis may be different from the ones reported here.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2986">
<title id=" W07-0212.xml">how difficult is it to develop a perfect spell checker a cross linguistic analysis through complex network approach </title>
<section> conclusion.  </section>
<citcontext>
<prevsection>
<prevsent>another way to improve the study could be to incorporate more realistic measure for the orthographic similarity between the words.
</prevsent>
<prevsent>nevertheless, such modification will have no effect on the analysis technique, though the results of the analysis may be different from the ones reported here.
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
appendix a: derivation of the probability of rwewe take noisy channel approach, which is common technique in nlp (for example (brown et al , 1993)), <papid> J93-2003 </papid>including spell checking (kernighan et al , 1990).</citsent>
<aftsection>
<nextsent>depending on the situation.
</nextsent>
<nextsent>the channel may model typing or ocr errors.
</nextsent>
<nextsent>suppose that word w, while passing through the channel, gets transformed to word w?.
</nextsent>
<nextsent>therefore, the aim of spelling correction is to find the w?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2987">
<title id=" W06-1617.xml">semantic role labeling of nombank a maximum entropy approach </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>on test section 23, our best system achieves f1 score of 72.73 (69.14) when correct (automatic) syntactic parse trees are used.
</prevsent>
<prevsent>to our knowledge, this is the first reported automatic nombank srl system.
</prevsent>
</prevsection>
<citsent citstr=" J05-1004 ">
automatic semantic role labeling (srl) systems, made possible by the availability of propbank (kingsbury and palmer, 2003; palmer etal., 2005), <papid> J05-1004 </papid>and encouraged by evaluation efforts in (carreras and marquez, 2005; litkowski, 2004), <papid> W04-0803 </papid>have been shown to accurately determine the argument structure of verb predicates.</citsent>
<aftsection>
<nextsent>a successful propbank-based srl system would correctly determine that ben bernanke?
</nextsent>
<nextsent>is the subject (labeled as arg0 in propbank) of predicate replace?, and greenspan?
</nextsent>
<nextsent>is the object (labeled as arg1): ? ben bern anke replaced greenspan as fed chair.
</nextsent>
<nextsent>greenspan was replaced by ben bern anke as fed chair.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2988">
<title id=" W06-1617.xml">semantic role labeling of nombank a maximum entropy approach </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>on test section 23, our best system achieves f1 score of 72.73 (69.14) when correct (automatic) syntactic parse trees are used.
</prevsent>
<prevsent>to our knowledge, this is the first reported automatic nombank srl system.
</prevsent>
</prevsection>
<citsent citstr=" W04-0803 ">
automatic semantic role labeling (srl) systems, made possible by the availability of propbank (kingsbury and palmer, 2003; palmer etal., 2005), <papid> J05-1004 </papid>and encouraged by evaluation efforts in (carreras and marquez, 2005; litkowski, 2004), <papid> W04-0803 </papid>have been shown to accurately determine the argument structure of verb predicates.</citsent>
<aftsection>
<nextsent>a successful propbank-based srl system would correctly determine that ben bernanke?
</nextsent>
<nextsent>is the subject (labeled as arg0 in propbank) of predicate replace?, and greenspan?
</nextsent>
<nextsent>is the object (labeled as arg1): ? ben bern anke replaced greenspan as fed chair.
</nextsent>
<nextsent>greenspan was replaced by ben bern anke as fed chair.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC2989">
<title id=" W06-1617.xml">semantic role labeling of nombank a maximum entropy approach </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>is the object (labeled as arg1): ? ben bern anke replaced greenspan as fed chair.
</prevsent>
<prevsent>greenspan was replaced by ben bern anke as fed chair.
</prevsent>
</prevsection>
<citsent citstr=" W04-2705 ">
the recent release of nombank (meyers et al,2004<papid> W04-2705 </papid>c; meyers et al, 2004<papid> W04-2705 </papid>b), databank that annotates argument structure for instances of common nouns in the penn treebank ii corpus, made it possible to develop automatic srl systems that analyze the argument structures of noun predi cates.</citsent>
<aftsection>
<nextsent>given the following two noun phrases and one sentence, successful nombank-based srl system should label ben bernanke?
</nextsent>
<nextsent>as the subject (arg0) and greenspan?
</nextsent>
<nextsent>as the object (arg1) of the noun predicate replacement?.
</nextsent>
<nextsent>greenspans replacement ben bern anke ? ben bernankes replacement of greenspan ? ben bern anke was nominated as greenspans replacement.the ability to automatically analyze the argument structures of verb and noun predicates would greatly facilitate nlp tasks like question answering, information extraction, etc. this paper focuses on our efforts at building an accurate automatic nombank-based srl system.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3010">
<title id=" W06-1617.xml">semantic role labeling of nombank a maximum entropy approach </title>
<section> overview of nombank.  </section>
<citcontext>
<prevsection>
<prevsent>as an argument of replacement?.
</prevsent>
<prevsent>the support construct will be explained in detail in section 4.2.3.we are not aware of any nombank-based automatic srl systems.
</prevsent>
</prevsection>
<citsent citstr=" N04-4036 ">
the work in (pradhan et al,2004) <papid> N04-4036 </papid>experimented with an automatic srl system developed using relatively small set of manually selected nominalizations from framenet and penn chinese treebank.</citsent>
<aftsection>
<nextsent>the srl accuracy of their system is not directly comparable to ours.
</nextsent>
<nextsent>we treat the nombank-based srl task as classification problem and divide it into two phases:argument identification and argument classification.
</nextsent>
<nextsent>during the argument identification phase, each parse tree node is marked as either argument or non-argument.
</nextsent>
<nextsent>each node marked as argument is then labeled with specific class during the argument classification phase.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3011">
<title id=" W06-1617.xml">semantic role labeling of nombank a maximum entropy approach </title>
<section> model training and testing.  </section>
<citcontext>
<prevsection>
<prevsent>each node marked as argument is then labeled with specific class during the argument classification phase.
</prevsent>
<prevsent>the identification model is binary classifier , while the classification model is multi-class classifier.opennlp maxent1, an implementation of maximum entropy (me) modeling, is used as the classification tool.
</prevsent>
</prevsection>
<citsent citstr=" J96-1002 ">
since its introduction to the natural language processing (nlp) community (berger et al, 1996), <papid> J96-1002 </papid>me-based classifiers have been shown to be effective in various nlp tasks.</citsent>
<aftsection>
<nextsent>me modeling is based on the insight that the best model is consistent with the set of constraints imposed and otherwise as uniform as possible.
</nextsent>
<nextsent>me models the probability of label given input as in equation 1.
</nextsent>
<nextsent>fi(l, x) is feature function that maps label and input to either 0 or 1, while the summation is over all feature functions and withi as the weight parameter for each feature function fi(l, x).
</nextsent>
<nextsent>zx is normalization factor.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3012">
<title id=" W06-1617.xml">semantic role labeling of nombank a maximum entropy approach </title>
<section> model training and testing.  </section>
<citcontext>
<prevsection>
<prevsent>the classification output is the label with the highest conditional probability p(l|x).
</prevsent>
<prevsent>p(l|x) = exp( i=1 ifi(l, x)) zx (1) to train the me-based identification model, training data is gathered by treating each parse tree node that is an argument as positive example andthe rest as negative examples.
</prevsent>
</prevsection>
<citsent citstr=" P05-1073 ">
classification training data is generated from argument nodes only.during testing, the algorithm of enforcing nonoverlapping arguments by (toutanova et al, 2005)<papid> P05-1073 </papid>is used.</citsent>
<aftsection>
<nextsent>the algorithm maximizes the log probability of the entire nombank labeled parse 1http://maxent.sourceforge.net/ 139 tree.
</nextsent>
<nextsent>specifically, assuming we only have two classes arg?
</nextsent>
<nextsent>and none?, the log-probability of nombank labeled parse tree is defined by equation 2.
</nextsent>
<nextsent>max(t ) = max { none(t ) +?(max(child)) arg(t ) +?(nonetree(child)) (2) max(t ) is the maximum log-probability of tree , none(t ) and arg(t ) are respectively the log-probability of assigning label none?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3013">
<title id=" W06-1617.xml">semantic role labeling of nombank a maximum entropy approach </title>
<section> features and feature selection.  </section>
<citcontext>
<prevsection>
<prevsent>we do not perform similar pruning on the test data.
</prevsent>
<prevsent>4.1 baseline nombank srl features.
</prevsent>
</prevsection>
<citsent citstr=" W04-3212 ">
table 1 lists the baseline features we adapted from previous propbank-based srl systems (pradhan et al, 2005; xue and palmer, 2004).<papid> W04-3212 </papid></citsent>
<aftsection>
<nextsent>for ease of description, related features are grouped, witha specific individual feature given individual reference name.
</nextsent>
<nextsent>for example, feature b11fw in the group b11 denotes the first word spanned bythe constituent and b13lh denotes the left sisters head word.
</nextsent>
<nextsent>we also experimented with various feature combinations, inspired by the features used in (xue and palmer, 2004).<papid> W04-3212 </papid></nextsent>
<nextsent>these are listed as features b31 to b34 in table 1.suppose the current constituent under identification or classification is np-ben bernanke?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3034">
<title id=" W06-1617.xml">semantic role labeling of nombank a maximum entropy approach </title>
<section> discussion and future work.  </section>
<citcontext>
<prevsection>
<prevsent>we believe the flexible argument structure of nombank noun predicates contributes to the lower automatic srl accuracy as compared to that of the propbank srl task.
</prevsent>
<prevsent>6.3 integrating propbank and nombank.
</prevsent>
</prevsection>
<citsent citstr=" W05-0302 ">
srl work in (pustejovsky et al, 2005) <papid> W05-0302 </papid>discussed the possibility of merging various treebank annotation efforts including propbank, nombank, and others.</citsent>
<aftsection>
<nextsent>future work involves studying ways of concurrently producing automatic propbank and nombank srl, and improving the accuracy by exploiting the inter-relationship between verb predicate-argument and noun predicate-argument structures.
</nextsent>
<nextsent>besides the obvious correspondence between verb and its nominalizations, e.g., replace?
</nextsent>
<nextsent>andreplacement?, there is also correspondence between verb predicates in propbank and support verbs in nombank.
</nextsent>
<nextsent>statistics from nombank sections 02-21 show that 86% of the support verbs in nombank are also predicate verbs in propbank.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3035">
<title id=" W06-2926.xml">a pipeline model for bottom up dependency parsing </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>1.3 dealing with crossing edges.
</prevsent>
<prevsent>the algorithm described in previous section is primarily designed for projective languages.
</prevsent>
</prevsection>
<citsent citstr=" P05-1013 ">
to deal with non-projective languages, we use similar approach of (nivre and nilsson, 2005) <papid> P05-1013 </papid>to map non projective trees to projective trees.</citsent>
<aftsection>
<nextsent>any single rooted projective dependency tree can be mapped into projective tree by the lift operation.
</nextsent>
<nextsent>the definition of lift is as follows: lift(wj ? wk) = parent(wj) ? wk, where ? means that is the parent of b, and parent is function which returns the parent word of the given word.
</nextsent>
<nextsent>the procedure is as follows.
</nextsent>
<nextsent>first, the mapping algorithm examines if there is crossing edge in the current tree.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3036">
<title id=" W06-2931.xml">dependency parsing based on dynamic local optimization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the algorithm parses sentences in linear time and labeling is integrated with the parsing.this parser achieves 63.29% labeled attachment score on the average in conll shared task.
</prevsent>
<prevsent>recently, dependency grammar has gained renewed attention in the parsing community.
</prevsent>
</prevsection>
<citsent citstr=" W04-2407 ">
good results have been achieved in some dependency parsers (yamada and matsumoto, 2003; nivre et al, 2004).<papid> W04-2407 </papid></citsent>
<aftsection>
<nextsent>with the availability of many dependency treebanks (van der beek et al, 2002; hajic?
</nextsent>
<nextsent>et al, 2004; bohmova?
</nextsent>
<nextsent>et al, 2003; kromann, 2003; dzeroski et al., 2006) and more other treebanks which can be converted to dependency annotation (brants et al, 2002; nilsson et al, 2005; chen et al, 2003; kawataand bartels, 2000), multi-lingual dependency parsing is proposed in conll shared task (buchholz et al., 2006).
</nextsent>
<nextsent>many previous works focus on unlabeled parsing,in which exhaustive methods are often used (eis ner, 1996).<papid> C96-1058 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3038">
<title id=" W06-2931.xml">dependency parsing based on dynamic local optimization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>et al, 2004; bohmova?
</prevsent>
<prevsent>et al, 2003; kromann, 2003; dzeroski et al., 2006) and more other treebanks which can be converted to dependency annotation (brants et al, 2002; nilsson et al, 2005; chen et al, 2003; kawataand bartels, 2000), multi-lingual dependency parsing is proposed in conll shared task (buchholz et al., 2006).
</prevsent>
</prevsection>
<citsent citstr=" C96-1058 ">
many previous works focus on unlabeled parsing,in which exhaustive methods are often used (eis ner, 1996).<papid> C96-1058 </papid></citsent>
<aftsection>
<nextsent>their global searching performs well in the unlabeled dependency parsing.
</nextsent>
<nextsent>but with the increase of parameters, efficiency has to be considered in labeled dependency parsing.
</nextsent>
<nextsent>thus deterministic parsing was proposed as robust and efficient method in recent years.
</nextsent>
<nextsent>such method breaks the construction of dependency tree into series of actions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3040">
<title id=" W06-2931.xml">dependency parsing based on dynamic local optimization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>deterministic parsing derives an analysis without redundancy or backtracking, and linear time can be achieved.
</prevsent>
<prevsent>but when searching the local optimum in the order of left-to-right, some wrong reduce may prevent next analysis with more possibility.
</prevsent>
</prevsection>
<citsent citstr=" I05-2044 ">
(jin et al., 2005) <papid> I05-2044 </papid>used two-phase shift-reduce to decrease such errors, and improved the accuracy of long distance dependencies.in this paper deterministic parsing based on dynamic local optimization is proposed.</citsent>
<aftsection>
<nextsent>according to the probabilities of dependency arcs, the algorithm dynamically finds the one with the highest probabilities instead of dealing with the sentence in order.
</nextsent>
<nextsent>a procedure of constraint which can integrate more structure information is made to check the rationality of the reduce.
</nextsent>
<nextsent>finally our results and error analysis are presented.
</nextsent>
<nextsent>an example of chinese dependency tree is showed in figure1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3043">
<title id=" W06-3607.xml">reranking algorithms for name tagging </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in recent years, re-ranking techniques have been successfully applied to enhance the performance of nlp analysis components based on generative models.
</prevsent>
<prevsent>a baseline generative model produces best candidates, which are then re-ranked using rich set of local and global features in order to select the best analysis.
</prevsent>
</prevsection>
<citsent citstr=" P05-1022 ">
various supervised learning algorithms have been adapted to the task of reranking for nlp systems, such as maxent-rank (charniak and johnson, 2005; <papid> P05-1022 </papid>ji and grishman, 2005), <papid> P05-1051 </papid>svmrank (shen and joshi, 2003), <papid> W03-0402 </papid>voted perceptron (collins, 2002; <papid> P02-1062 </papid>collins and duffy, 2002; <papid> P02-1034 </papid>shen and joshi, 2004), kernel based methods (henderson and titov, 2005), <papid> P05-1023 </papid>and rank boost (collins, 2002; <papid> P02-1062 </papid>collins and koo, 2003; kudo et al, 2005).<papid> P05-1024 </papid></citsent>
<aftsection>
<nextsent>these algorithms have been used primarily within the context of single nlp analysis component, with the most intensive study devoted to improving parsing performance.
</nextsent>
<nextsent>the re-ranking models for parsing, for example, normally relyon structures generated within the baseline parser itself.
</nextsent>
<nextsent>achieving really high performance for some analysis components, however, requires that we take broader view, one that looks outside single component in order to bring to bear knowledge from the entire nl analysis process.
</nextsent>
<nextsent>in this paper we will demonstrate the potential of this approach in enhancing the performance of chinese name tagging within an information extraction application.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3044">
<title id=" W06-3607.xml">reranking algorithms for name tagging </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in recent years, re-ranking techniques have been successfully applied to enhance the performance of nlp analysis components based on generative models.
</prevsent>
<prevsent>a baseline generative model produces best candidates, which are then re-ranked using rich set of local and global features in order to select the best analysis.
</prevsent>
</prevsection>
<citsent citstr=" P05-1051 ">
various supervised learning algorithms have been adapted to the task of reranking for nlp systems, such as maxent-rank (charniak and johnson, 2005; <papid> P05-1022 </papid>ji and grishman, 2005), <papid> P05-1051 </papid>svmrank (shen and joshi, 2003), <papid> W03-0402 </papid>voted perceptron (collins, 2002; <papid> P02-1062 </papid>collins and duffy, 2002; <papid> P02-1034 </papid>shen and joshi, 2004), kernel based methods (henderson and titov, 2005), <papid> P05-1023 </papid>and rank boost (collins, 2002; <papid> P02-1062 </papid>collins and koo, 2003; kudo et al, 2005).<papid> P05-1024 </papid></citsent>
<aftsection>
<nextsent>these algorithms have been used primarily within the context of single nlp analysis component, with the most intensive study devoted to improving parsing performance.
</nextsent>
<nextsent>the re-ranking models for parsing, for example, normally relyon structures generated within the baseline parser itself.
</nextsent>
<nextsent>achieving really high performance for some analysis components, however, requires that we take broader view, one that looks outside single component in order to bring to bear knowledge from the entire nl analysis process.
</nextsent>
<nextsent>in this paper we will demonstrate the potential of this approach in enhancing the performance of chinese name tagging within an information extraction application.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3045">
<title id=" W06-3607.xml">reranking algorithms for name tagging </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in recent years, re-ranking techniques have been successfully applied to enhance the performance of nlp analysis components based on generative models.
</prevsent>
<prevsent>a baseline generative model produces best candidates, which are then re-ranked using rich set of local and global features in order to select the best analysis.
</prevsent>
</prevsection>
<citsent citstr=" W03-0402 ">
various supervised learning algorithms have been adapted to the task of reranking for nlp systems, such as maxent-rank (charniak and johnson, 2005; <papid> P05-1022 </papid>ji and grishman, 2005), <papid> P05-1051 </papid>svmrank (shen and joshi, 2003), <papid> W03-0402 </papid>voted perceptron (collins, 2002; <papid> P02-1062 </papid>collins and duffy, 2002; <papid> P02-1034 </papid>shen and joshi, 2004), kernel based methods (henderson and titov, 2005), <papid> P05-1023 </papid>and rank boost (collins, 2002; <papid> P02-1062 </papid>collins and koo, 2003; kudo et al, 2005).<papid> P05-1024 </papid></citsent>
<aftsection>
<nextsent>these algorithms have been used primarily within the context of single nlp analysis component, with the most intensive study devoted to improving parsing performance.
</nextsent>
<nextsent>the re-ranking models for parsing, for example, normally relyon structures generated within the baseline parser itself.
</nextsent>
<nextsent>achieving really high performance for some analysis components, however, requires that we take broader view, one that looks outside single component in order to bring to bear knowledge from the entire nl analysis process.
</nextsent>
<nextsent>in this paper we will demonstrate the potential of this approach in enhancing the performance of chinese name tagging within an information extraction application.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3047">
<title id=" W06-3607.xml">reranking algorithms for name tagging </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in recent years, re-ranking techniques have been successfully applied to enhance the performance of nlp analysis components based on generative models.
</prevsent>
<prevsent>a baseline generative model produces best candidates, which are then re-ranked using rich set of local and global features in order to select the best analysis.
</prevsent>
</prevsection>
<citsent citstr=" P02-1062 ">
various supervised learning algorithms have been adapted to the task of reranking for nlp systems, such as maxent-rank (charniak and johnson, 2005; <papid> P05-1022 </papid>ji and grishman, 2005), <papid> P05-1051 </papid>svmrank (shen and joshi, 2003), <papid> W03-0402 </papid>voted perceptron (collins, 2002; <papid> P02-1062 </papid>collins and duffy, 2002; <papid> P02-1034 </papid>shen and joshi, 2004), kernel based methods (henderson and titov, 2005), <papid> P05-1023 </papid>and rank boost (collins, 2002; <papid> P02-1062 </papid>collins and koo, 2003; kudo et al, 2005).<papid> P05-1024 </papid></citsent>
<aftsection>
<nextsent>these algorithms have been used primarily within the context of single nlp analysis component, with the most intensive study devoted to improving parsing performance.
</nextsent>
<nextsent>the re-ranking models for parsing, for example, normally relyon structures generated within the baseline parser itself.
</nextsent>
<nextsent>achieving really high performance for some analysis components, however, requires that we take broader view, one that looks outside single component in order to bring to bear knowledge from the entire nl analysis process.
</nextsent>
<nextsent>in this paper we will demonstrate the potential of this approach in enhancing the performance of chinese name tagging within an information extraction application.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3049">
<title id=" W06-3607.xml">reranking algorithms for name tagging </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in recent years, re-ranking techniques have been successfully applied to enhance the performance of nlp analysis components based on generative models.
</prevsent>
<prevsent>a baseline generative model produces best candidates, which are then re-ranked using rich set of local and global features in order to select the best analysis.
</prevsent>
</prevsection>
<citsent citstr=" P02-1034 ">
various supervised learning algorithms have been adapted to the task of reranking for nlp systems, such as maxent-rank (charniak and johnson, 2005; <papid> P05-1022 </papid>ji and grishman, 2005), <papid> P05-1051 </papid>svmrank (shen and joshi, 2003), <papid> W03-0402 </papid>voted perceptron (collins, 2002; <papid> P02-1062 </papid>collins and duffy, 2002; <papid> P02-1034 </papid>shen and joshi, 2004), kernel based methods (henderson and titov, 2005), <papid> P05-1023 </papid>and rank boost (collins, 2002; <papid> P02-1062 </papid>collins and koo, 2003; kudo et al, 2005).<papid> P05-1024 </papid></citsent>
<aftsection>
<nextsent>these algorithms have been used primarily within the context of single nlp analysis component, with the most intensive study devoted to improving parsing performance.
</nextsent>
<nextsent>the re-ranking models for parsing, for example, normally relyon structures generated within the baseline parser itself.
</nextsent>
<nextsent>achieving really high performance for some analysis components, however, requires that we take broader view, one that looks outside single component in order to bring to bear knowledge from the entire nl analysis process.
</nextsent>
<nextsent>in this paper we will demonstrate the potential of this approach in enhancing the performance of chinese name tagging within an information extraction application.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3050">
<title id=" W06-3607.xml">reranking algorithms for name tagging </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in recent years, re-ranking techniques have been successfully applied to enhance the performance of nlp analysis components based on generative models.
</prevsent>
<prevsent>a baseline generative model produces best candidates, which are then re-ranked using rich set of local and global features in order to select the best analysis.
</prevsent>
</prevsection>
<citsent citstr=" P05-1023 ">
various supervised learning algorithms have been adapted to the task of reranking for nlp systems, such as maxent-rank (charniak and johnson, 2005; <papid> P05-1022 </papid>ji and grishman, 2005), <papid> P05-1051 </papid>svmrank (shen and joshi, 2003), <papid> W03-0402 </papid>voted perceptron (collins, 2002; <papid> P02-1062 </papid>collins and duffy, 2002; <papid> P02-1034 </papid>shen and joshi, 2004), kernel based methods (henderson and titov, 2005), <papid> P05-1023 </papid>and rank boost (collins, 2002; <papid> P02-1062 </papid>collins and koo, 2003; kudo et al, 2005).<papid> P05-1024 </papid></citsent>
<aftsection>
<nextsent>these algorithms have been used primarily within the context of single nlp analysis component, with the most intensive study devoted to improving parsing performance.
</nextsent>
<nextsent>the re-ranking models for parsing, for example, normally relyon structures generated within the baseline parser itself.
</nextsent>
<nextsent>achieving really high performance for some analysis components, however, requires that we take broader view, one that looks outside single component in order to bring to bear knowledge from the entire nl analysis process.
</nextsent>
<nextsent>in this paper we will demonstrate the potential of this approach in enhancing the performance of chinese name tagging within an information extraction application.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3053">
<title id=" W06-3607.xml">reranking algorithms for name tagging </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in recent years, re-ranking techniques have been successfully applied to enhance the performance of nlp analysis components based on generative models.
</prevsent>
<prevsent>a baseline generative model produces best candidates, which are then re-ranked using rich set of local and global features in order to select the best analysis.
</prevsent>
</prevsection>
<citsent citstr=" P05-1024 ">
various supervised learning algorithms have been adapted to the task of reranking for nlp systems, such as maxent-rank (charniak and johnson, 2005; <papid> P05-1022 </papid>ji and grishman, 2005), <papid> P05-1051 </papid>svmrank (shen and joshi, 2003), <papid> W03-0402 </papid>voted perceptron (collins, 2002; <papid> P02-1062 </papid>collins and duffy, 2002; <papid> P02-1034 </papid>shen and joshi, 2004), kernel based methods (henderson and titov, 2005), <papid> P05-1023 </papid>and rank boost (collins, 2002; <papid> P02-1062 </papid>collins and koo, 2003; kudo et al, 2005).<papid> P05-1024 </papid></citsent>
<aftsection>
<nextsent>these algorithms have been used primarily within the context of single nlp analysis component, with the most intensive study devoted to improving parsing performance.
</nextsent>
<nextsent>the re-ranking models for parsing, for example, normally relyon structures generated within the baseline parser itself.
</nextsent>
<nextsent>achieving really high performance for some analysis components, however, requires that we take broader view, one that looks outside single component in order to bring to bear knowledge from the entire nl analysis process.
</nextsent>
<nextsent>in this paper we will demonstrate the potential of this approach in enhancing the performance of chinese name tagging within an information extraction application.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3057">
<title id=" W06-3607.xml">reranking algorithms for name tagging </title>
<section> prior work.  </section>
<citcontext>
<prevsection>
<prevsent>the parameter p? determines how much the algorithm concentrates at the top.
</prevsent>
<prevsent>2.2 enhancing named entity taggers.
</prevsent>
</prevsection>
<citsent citstr=" C96-1079 ">
there have been very large number of ne tagger implementations since this task was introduced at muc-6 (grishman and sundheim, 1996).<papid> C96-1079 </papid></citsent>
<aftsection>
<nextsent>most implementations use local features and unifying learning algorithm based on, e.g., an hmm, max ent, or svm.
</nextsent>
<nextsent>collins (2002) <papid> P02-1062 </papid>augmented baseline ne tagger with re-ranker that used only local, ne-oriented features.</nextsent>
<nextsent>roth and yih (2002) <papid> C02-1151 </papid>combined ne and semantic relation tagging, but within quite different framework (using linear programming model for joint inference).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3060">
<title id=" W06-3607.xml">reranking algorithms for name tagging </title>
<section> prior work.  </section>
<citcontext>
<prevsection>
<prevsent>most implementations use local features and unifying learning algorithm based on, e.g., an hmm, max ent, or svm.
</prevsent>
<prevsent>collins (2002) <papid> P02-1062 </papid>augmented baseline ne tagger with re-ranker that used only local, ne-oriented features.</prevsent>
</prevsection>
<citsent citstr=" C02-1151 ">
roth and yih (2002) <papid> C02-1151 </papid>combined ne and semantic relation tagging, but within quite different framework (using linear programming model for joint inference).</citsent>
<aftsection>
<nextsent>3.1 the information extraction pipeline.
</nextsent>
<nextsent>the extraction task we are addressing is that of the automatic content extraction (ace)1 evaluations.
</nextsent>
<nextsent>the 2005 ace evaluation had 7 types of entities, of which the most common were per (persons), org (organizations), loc (natural locations) and gpe (geo-political entities?
</nextsent>
<nextsent>locations which are also political units, such as countries, counties, and cities).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3061">
<title id=" W06-3607.xml">reranking algorithms for name tagging </title>
<section> a framework for name re-ranking  </section>
<citcontext>
<prevsection>
<prevsent>is one hypothesis for the sentence john was born in new york?.
</prevsent>
<prevsent>we apply hmm tagger to identify four named entity types: person, gpe, organization and location.
</prevsent>
</prevsection>
<citsent citstr=" A97-1029 ">
the hmm tagger generally follows the nymble model (bikel et al 1997), <papid> A97-1029 </papid>and uses best first search to generate n-best hypotheses.</citsent>
<aftsection>
<nextsent>it also computes the margin?, which is the difference between the log probabilities of the top two hypotheses.
</nextsent>
<nextsent>this is used as rough measure of confidence in the top hypothesis.
</nextsent>
<nextsent>a large margin indicates greater confidence that the first hypothesis is correct.
</nextsent>
<nextsent>the margin also determines the number of hypotheses (n) that we will store.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3064">
<title id=" W06-3607.xml">reranking algorithms for name tagging </title>
<section> features for re-ranking.  </section>
<citcontext>
<prevsection>
<prevsent>to this end, we incorporate several features based on coreference, such as the number of mentions referring to name candidate.
</prevsent>
<prevsent>each of these features is defined for individual name candidates; the value of the feature for hypothesis is the sum of its values over all names in the hypothesis.
</prevsent>
</prevsection>
<citsent citstr=" P06-2055 ">
the complete set of detailed features is listed in (ji and grishman, 2006).<papid> P06-2055 </papid></citsent>
<aftsection>
<nextsent>4.2 handling cross-sentence features by.
</nextsent>
<nextsent>multi-stage re-ranking coreference is potentially powerful contributor for enhancing ne recognition, because it provides information from other sentences and even documents, and it applies to all sentences that include names.
</nextsent>
<nextsent>for name candidate, 62% of its coreference relations span sentence boundaries.
</nextsent>
<nextsent>how ever, this breadth poses problem because it means that the score of hypothesis forgiven 51 sentence may depend on the tags assigned to the same names in other sentences.2 ideally, when we re-rank the hypotheses for one sentence s, the other sentences that include mentions of the same name should already have been re-ranked, but this is not possible because of the mutual dependence.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3065">
<title id=" W06-3607.xml">reranking algorithms for name tagging </title>
<section> re-ranking algorithms.  </section>
<citcontext>
<prevsection>
<prevsent>to make the training and 52 test procedures more efficient, we prune the data in several ways.
</prevsent>
<prevsent>we perform pruning by beam setting, removing candidate hypotheses that possess very low probabilities from the hmm, and during training we discard the hypotheses with very low f-measure scores.
</prevsent>
</prevsection>
<citsent citstr=" P05-1033 ">
additionally, we incorporate the pruning techniques used in (chiang 2005), <papid> P05-1033 </papid>by which any hypothesis with probability lower than times the highest probability for one sentence is dis carded.</citsent>
<aftsection>
<nextsent>we also discard the pairs very close in performance or probability.
</nextsent>
<nextsent>5.1.2 decoding if is the ranking function, the maxent model produces probability for each un-pruned cru cial?
</nextsent>
<nextsent>pair: prob(f(hi, hj) = 1), i.e., the probability that for the given sentence, hi is better hypothesis than hj.
</nextsent>
<nextsent>we need an additional decoding step to select the best hypothesis.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3070">
<title id=" W06-3607.xml">reranking algorithms for name tagging </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>since these feature sets are essentially disjoint, it is quite possible that combination of the two could yield even further improvements.
</prevsent>
<prevsent>his boosting algorithm is modification of the method in (freund et al, 1998), an adaptation of ada boost, whereas our p-norm push ranking algorithm can emphasize the hypotheses near the top, matching our objective.
</prevsent>
</prevsection>
<citsent citstr=" W04-2401 ">
roth and yih (2004) <papid> W04-2401 </papid>combined information from named entities and semantic relation tagging, adopting similar overall goal but using quite different approach based on linear programming.</citsent>
<aftsection>
<nextsent>7 the features were initially developed and tested using the maxent re-ranker,.
</nextsent>
<nextsent>so it is encouraging that they worked equally well with the p-norm push ranker without further tuning.
</nextsent>
<nextsent>55 they limited themselves to name classification, assuming the identification given.
</nextsent>
<nextsent>this may be natural subtask for english, where capitalization is strong indicator of name, but is much less useful for chinese, where there is no capitalization or word segmentation, and boundary errors on name identification are frequent.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3071">
<title id=" W06-2916.xml">unsupervised grammar induction by distribution and attachment </title>
<section> previous approaches.  </section>
<citcontext>
<prevsection>
<prevsent>em has the advantage that constituent probabilities are only compared when constituents compete, which removes the inherent bias towards shorter constituents, which tend to have higher probability.
</prevsent>
<prevsent>however, em methods are more susceptible to data sparsity issues associated with raw text, because there is no generalization during constituent proposal.
</prevsent>
</prevsection>
<citsent citstr=" W01-0713 ">
examples of em learning systems are context distribution clustering (cdc) (clark, 2001) <papid> W01-0713 </papid>and constituent-context model (ccm) (klein, 2005,chapter 5), which avoid the aforementioned data sparsity issues by using part-of-speech (pos) tagged corpus, rather than raw text.</citsent>
<aftsection>
<nextsent>alignment based learning (abl) (van zaanen, 2000) is the only em system applied directly to raw text.
</nextsent>
<nextsent>abluses minimal string-edit distance between sentences to propose constituents, from which the most probable combination is chosen.
</nextsent>
<nextsent>however, abl is relatively inefficient and has only been applied to small corpora.the second category is that of incremental learning systems.
</nextsent>
<nextsent>an incremental system analyzes corpus in bottom-up fashion: each time new constituent type is found, it is inserted into the corpus to provide data for later learning.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3072">
<title id=" W07-0407.xml">discriminative word alignment by learning the alignment structure and syntactic divergence between a language pair </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we have tested our model on the english-hindi language pair.
</prevsent>
<prevsent>here is an example of an alignment between english-hindi which shows the complexity of the alignment task for this language pair.
</prevsent>
</prevsection>
<citsent citstr=" H05-1066 ">
figure 1: an example of an alignment between an english and hindi sentence to learn the weights associated with the parameters used in our model, we have used learning framework called mira (the margin infused relaxed algorithm) (mcdonald et al, 2005; <papid> H05-1066 </papid>crammer and singer, 2003).</citsent>
<aftsection>
<nextsent>this is an online learning algorithm which looks at one sentence pair at time and compares the k-best predictions of the alignment algorithm with the gold alignment to update the parameter weights appropriately.
</nextsent>
<nextsent>in the past, popular approaches for doing word alignment have largely been generative (och andney, 2003; <papid> J03-1002 </papid>vogel et al, 1996).<papid> C96-2141 </papid></nextsent>
<nextsent>in the past couple of years, the discriminative models for doing word alignment have gained popularity because of 49 the flexibility they offer in using large variety of features and in combining information from various sources.(taskar et al, 2005) <papid> H05-1010 </papid>cast the problem of alignment as maximum weight bipartite matching problem, where nodes correspond to the words in the two sentences.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3073">
<title id=" W07-0407.xml">discriminative word alignment by learning the alignment structure and syntactic divergence between a language pair </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>figure 1: an example of an alignment between an english and hindi sentence to learn the weights associated with the parameters used in our model, we have used learning framework called mira (the margin infused relaxed algorithm) (mcdonald et al, 2005; <papid> H05-1066 </papid>crammer and singer, 2003).</prevsent>
<prevsent>this is an online learning algorithm which looks at one sentence pair at time and compares the k-best predictions of the alignment algorithm with the gold alignment to update the parameter weights appropriately.</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
in the past, popular approaches for doing word alignment have largely been generative (och andney, 2003; <papid> J03-1002 </papid>vogel et al, 1996).<papid> C96-2141 </papid></citsent>
<aftsection>
<nextsent>in the past couple of years, the discriminative models for doing word alignment have gained popularity because of 49 the flexibility they offer in using large variety of features and in combining information from various sources.(taskar et al, 2005) <papid> H05-1010 </papid>cast the problem of alignment as maximum weight bipartite matching problem, where nodes correspond to the words in the two sentences.</nextsent>
<nextsent>the link between pair of words, (ep,hq) is associated with score(score(ep,hq)) reflecting the desirability of the existence of the link.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3074">
<title id=" W07-0407.xml">discriminative word alignment by learning the alignment structure and syntactic divergence between a language pair </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>figure 1: an example of an alignment between an english and hindi sentence to learn the weights associated with the parameters used in our model, we have used learning framework called mira (the margin infused relaxed algorithm) (mcdonald et al, 2005; <papid> H05-1066 </papid>crammer and singer, 2003).</prevsent>
<prevsent>this is an online learning algorithm which looks at one sentence pair at time and compares the k-best predictions of the alignment algorithm with the gold alignment to update the parameter weights appropriately.</prevsent>
</prevsection>
<citsent citstr=" C96-2141 ">
in the past, popular approaches for doing word alignment have largely been generative (och andney, 2003; <papid> J03-1002 </papid>vogel et al, 1996).<papid> C96-2141 </papid></citsent>
<aftsection>
<nextsent>in the past couple of years, the discriminative models for doing word alignment have gained popularity because of 49 the flexibility they offer in using large variety of features and in combining information from various sources.(taskar et al, 2005) <papid> H05-1010 </papid>cast the problem of alignment as maximum weight bipartite matching problem, where nodes correspond to the words in the two sentences.</nextsent>
<nextsent>the link between pair of words, (ep,hq) is associated with score(score(ep,hq)) reflecting the desirability of the existence of the link.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3075">
<title id=" W07-0407.xml">discriminative word alignment by learning the alignment structure and syntactic divergence between a language pair </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this is an online learning algorithm which looks at one sentence pair at time and compares the k-best predictions of the alignment algorithm with the gold alignment to update the parameter weights appropriately.
</prevsent>
<prevsent>in the past, popular approaches for doing word alignment have largely been generative (och andney, 2003; <papid> J03-1002 </papid>vogel et al, 1996).<papid> C96-2141 </papid></prevsent>
</prevsection>
<citsent citstr=" H05-1010 ">
in the past couple of years, the discriminative models for doing word alignment have gained popularity because of 49 the flexibility they offer in using large variety of features and in combining information from various sources.(taskar et al, 2005) <papid> H05-1010 </papid>cast the problem of alignment as maximum weight bipartite matching problem, where nodes correspond to the words in the two sentences.</citsent>
<aftsection>
<nextsent>the link between pair of words, (ep,hq) is associated with score(score(ep,hq)) reflecting the desirability of the existence of the link.
</nextsent>
<nextsent>the matching problem is solved by formulating it as linear programming problem.
</nextsent>
<nextsent>the parameter estimation is done with inthe framework of large margin estimation by reducing the problem to quadratic program (qp).the main limitation of this work is that the features considered are local to the alignment links joining pairs of words.
</nextsent>
<nextsent>the score of an alignment is the sum of scores of individual alignment links measured independently i.e., it is assumed that there is no dependence between the alignment links.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3076">
<title id=" W07-0407.xml">discriminative word alignment by learning the alignment structure and syntactic divergence between a language pair </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>another positive aspect of our approach is the application of mira.
</prevsent>
<prevsent>it, being an online approach, converges fast and still retains the generalizing capability of the large margin approach.
</prevsent>
</prevsection>
<citsent citstr=" H05-1011 ">
(moore, 2005) <papid> H05-1011 </papid>has proposed an approach which does not impose any restrictions on the form ofmodel features.</citsent>
<aftsection>
<nextsent>but, the search technique has certain heuristic procedures dependent on the typesof features used.
</nextsent>
<nextsent>for example, there is little variation in the alignment search between the llr (log-likelihood ratio) based model and the clp (conditional-link probability) based model.
</nextsent>
<nextsent>llr and clp are the word association statistics used in moores work (moore, 2005).<papid> H05-1011 </papid></nextsent>
<nextsent>in contrast to the above approach, our search technique is more general.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3079">
<title id=" W07-0407.xml">discriminative word alignment by learning the alignment structure and syntactic divergence between a language pair </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>it achieves this by breaking the search into two steps, first by using local features to getthe k-best alignments and then by using structural features to re-rank the list.
</prevsent>
<prevsent>also, by usin gall the k-best alignments for updating the parameters through mira, it is possible to model the entire inference algorithm but in moores work, only the best alignment is used to update the weights of parameters.
</prevsent>
</prevsection>
<citsent citstr=" P06-1097 ">
(fraser and marcu, 2006) <papid> P06-1097 </papid>have proposed an algorithm for doing word alignment which applies discriminative step at every iteration of the traditional expectation-maximization algorithm used in ibm models.</citsent>
<aftsection>
<nextsent>this model still relies on the generative story and achieves only alimited freedom in choosing the features.
</nextsent>
<nextsent>(blunsom and cohn, 2006) <papid> P06-1009 </papid>do word alignment by combining features using conditional random fields.</nextsent>
<nextsent>even though their approach allows one to include overlapping features while training discriminative model, it still does not allow us to use features that capture information of the entire alignment structure.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3080">
<title id=" W07-0407.xml">discriminative word alignment by learning the alignment structure and syntactic divergence between a language pair </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(fraser and marcu, 2006) <papid> P06-1097 </papid>have proposed an algorithm for doing word alignment which applies discriminative step at every iteration of the traditional expectation-maximization algorithm used in ibm models.</prevsent>
<prevsent>this model still relies on the generative story and achieves only alimited freedom in choosing the features.</prevsent>
</prevsection>
<citsent citstr=" P06-1009 ">
(blunsom and cohn, 2006) <papid> P06-1009 </papid>do word alignment by combining features using conditional random fields.</citsent>
<aftsection>
<nextsent>even though their approach allows one to include overlapping features while training discriminative model, it still does not allow us to use features that capture information of the entire alignment structure.
</nextsent>
<nextsent>in section 2, we describe the alignment search in detail.
</nextsent>
<nextsent>section 3 describes the features that we have considered in our paper.
</nextsent>
<nextsent>section 4 talks about the parameter optimization.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3086">
<title id=" W06-2402.xml">grouping multiword expressions according to partofspeech in statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>evaluations are performed by using real-life data, namely the european parliament corpus.
</prevsent>
<prevsent>results from translation tasks from english-to-spanish and from spanish-to-english are presented and discussed.
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
statistical machine translation (smt) was originally focused on word to word translation and was based on the noisy channel approach (brown et al., 1993).<papid> J93-2003 </papid></citsent>
<aftsection>
<nextsent>present smt systems have evolved from the original ones in such way that mainly differ from them in two issues: first, word-basedtranslation models have been replaced by phrase based translation models (zens et al, 2002) and(koehn et al, 2003); and second, the noisy channel approach has been expanded to more general maximum entropy approach in which log-linearcombination of multiple feature functions is implemented (och and ney, 2002).<papid> P02-1038 </papid>nevertheless, it is interesting to call the attention about one important fact.</nextsent>
<nextsent>despite the change from word-based to phrase-based translation approach, word to word approaches for inferring alignment models from bilingual data (vogel et al, 1996; <papid> C96-2141 </papid>och and ney, 2003) <papid> J03-1002 </papid>continue to be widely used.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3087">
<title id=" W06-2402.xml">grouping multiword expressions according to partofspeech in statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>results from translation tasks from english-to-spanish and from spanish-to-english are presented and discussed.
</prevsent>
<prevsent>statistical machine translation (smt) was originally focused on word to word translation and was based on the noisy channel approach (brown et al., 1993).<papid> J93-2003 </papid></prevsent>
</prevsection>
<citsent citstr=" P02-1038 ">
present smt systems have evolved from the original ones in such way that mainly differ from them in two issues: first, word-basedtranslation models have been replaced by phrase based translation models (zens et al, 2002) and(koehn et al, 2003); and second, the noisy channel approach has been expanded to more general maximum entropy approach in which log-linearcombination of multiple feature functions is implemented (och and ney, 2002).<papid> P02-1038 </papid>nevertheless, it is interesting to call the attention about one important fact.</citsent>
<aftsection>
<nextsent>despite the change from word-based to phrase-based translation approach, word to word approaches for inferring alignment models from bilingual data (vogel et al, 1996; <papid> C96-2141 </papid>och and ney, 2003) <papid> J03-1002 </papid>continue to be widely used.</nextsent>
<nextsent>on the other hand, from observing bilingual datasets, it becomes evident that in some cases itis just impossible to perform word to word alignment between two phrases that are translations of each other.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3088">
<title id=" W06-2402.xml">grouping multiword expressions according to partofspeech in statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>statistical machine translation (smt) was originally focused on word to word translation and was based on the noisy channel approach (brown et al., 1993).<papid> J93-2003 </papid></prevsent>
<prevsent>present smt systems have evolved from the original ones in such way that mainly differ from them in two issues: first, word-basedtranslation models have been replaced by phrase based translation models (zens et al, 2002) and(koehn et al, 2003); and second, the noisy channel approach has been expanded to more general maximum entropy approach in which log-linearcombination of multiple feature functions is implemented (och and ney, 2002).<papid> P02-1038 </papid>nevertheless, it is interesting to call the attention about one important fact.</prevsent>
</prevsection>
<citsent citstr=" C96-2141 ">
despite the change from word-based to phrase-based translation approach, word to word approaches for inferring alignment models from bilingual data (vogel et al, 1996; <papid> C96-2141 </papid>och and ney, 2003) <papid> J03-1002 </papid>continue to be widely used.</citsent>
<aftsection>
<nextsent>on the other hand, from observing bilingual datasets, it becomes evident that in some cases itis just impossible to perform word to word alignment between two phrases that are translations of each other.
</nextsent>
<nextsent>for example, certain combination of words might convey meaning which is somehow independent from the words it contains.
</nextsent>
<nextsent>this is the case of bilingual pairs such as fire engine?
</nextsent>
<nextsent>and camion de bomberos?.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3089">
<title id=" W06-2402.xml">grouping multiword expressions according to partofspeech in statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>statistical machine translation (smt) was originally focused on word to word translation and was based on the noisy channel approach (brown et al., 1993).<papid> J93-2003 </papid></prevsent>
<prevsent>present smt systems have evolved from the original ones in such way that mainly differ from them in two issues: first, word-basedtranslation models have been replaced by phrase based translation models (zens et al, 2002) and(koehn et al, 2003); and second, the noisy channel approach has been expanded to more general maximum entropy approach in which log-linearcombination of multiple feature functions is implemented (och and ney, 2002).<papid> P02-1038 </papid>nevertheless, it is interesting to call the attention about one important fact.</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
despite the change from word-based to phrase-based translation approach, word to word approaches for inferring alignment models from bilingual data (vogel et al, 1996; <papid> C96-2141 </papid>och and ney, 2003) <papid> J03-1002 </papid>continue to be widely used.</citsent>
<aftsection>
<nextsent>on the other hand, from observing bilingual datasets, it becomes evident that in some cases itis just impossible to perform word to word alignment between two phrases that are translations of each other.
</nextsent>
<nextsent>for example, certain combination of words might convey meaning which is somehow independent from the words it contains.
</nextsent>
<nextsent>this is the case of bilingual pairs such as fire engine?
</nextsent>
<nextsent>and camion de bomberos?.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3090">
<title id=" W06-2402.xml">grouping multiword expressions according to partofspeech in statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in that study, bmwes identified in small corpus2 were grouped as unique to 1of course, alignment results strongly depends on corpus statistics.
</prevsent>
<prevsent>2verbmobil (arranz et al, 2003) 9ken before training alignment models.
</prevsent>
</prevsection>
<citsent citstr=" W02-2001 ">
as result, both alignment quality and translation accuracy were slightly improved.in this paper we applied the same bmwe extraction technique, with various improvements, to large corpus (epps, described in section 4.1).since this is statistical technique, and frequencies of multi-word expressions are low (baldwin and villavicencio, 2002), <papid> W02-2001 </papid>the size of the corpus is an important factor.</citsent>
<aftsection>
<nextsent>a few very basic rules based on part-of-speech have also been added to filter out noisy entries in the dictionary.
</nextsent>
<nextsent>finally, bmwes have been classified into three categories (nouns, verbs and others).
</nextsent>
<nextsent>in addition to the impact of the whole set, the impact of each category has been evaluated separately.the technique will be explained in section 3, after presenting the baseline translation system used (section 2).
</nextsent>
<nextsent>experimental results are presented in section 4.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3092">
<title id=" W06-2402.xml">grouping multiword expressions according to partofspeech in statistical machine translation </title>
<section> experimental procedure.  </section>
<citcontext>
<prevsection>
<prevsent>in the identification phase (sec tion 3.3), these scores permit to prioritise forthe selection of some entries with respect tooth ers.
</prevsent>
<prevsent>previous experiments (lambert and banchs, 2005) have shown than the large set of bilingual phrases described in the following section provides better statistics than the set of asymmetry based bmwes.
</prevsent>
</prevsection>
<citsent citstr=" J04-4002 ">
3.1.2 scoring based on bilingual phrases here we refer to bilingual phrase (bp) as the bilingual phrases used by och and ney (2004).<papid> J04-4002 </papid>the bp are pairs of word groups which are supposed to be the translation of each other.</citsent>
<aftsection>
<nextsent>the setof bp is consistent with the alignment and consists of all phrase pairs in which all words within the target language are only aligned to the words of the source language and vice versa.
</nextsent>
<nextsent>at least one word of the target language phrase has to be aligned with at least one word of the source language phrase.
</nextsent>
<nextsent>finally, the algorithm takes into account possibly unaligned words at the boundaries of the target or source language phrases.
</nextsent>
<nextsent>we extracted all bp of length up to four words, with the algorithm described by och and ney.then we estimated the phrase translation probability distribution by relative frequency: p(t|s) = n(t, s) n(s) (2) in equation 2, and stand for the source and target side of the bp, respectively.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3093">
<title id=" W06-2402.xml">grouping multiword expressions according to partofspeech in statistical machine translation </title>
<section> experimental procedure.  </section>
<citcontext>
<prevsection>
<prevsent>11?
</prevsent>
<prevsent>its source or target side begins with coordination conjunction (except nor?, in english)?
</prevsent>
</prevsection>
<citsent citstr=" A00-1031 ">
its source or target side ends with an indefinite determiner english data have been pos-tagged using the tnt tagger (brants, 2000), <papid> A00-1031 </papid>after the lemmas have been extracted with wnmorph, included in the wordnet package (miller et al, 1991).</citsent>
<aftsection>
<nextsent>pos-tagging for spanish has been performed using the free ling analysis tool (carreras et al, 2004).
</nextsent>
<nextsent>finally, the bmwe set has been divided in three subsets, according to the following criteria, applied in this order:?
</nextsent>
<nextsent>if source and target sides of bmwe contain at least verb, it is assigned to the verb?
</nextsent>
<nextsent>class.?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3094">
<title id=" W06-3306.xml">human gene name normalization using text matching with automatically extracted synonym dictionaries </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this competition resulted in many novel and useful approaches, but the results clearly identified that more important work is necessary, especially for normalization, the subject of the current work.
</prevsent>
<prevsent>compared with gene ner, gene normalization is syntactically easier because identification of the textual boundaries of each mention is not required.however, gene normalization poses significant semantic challenges, as it requires detection of the actual gene intended, along with reporting of the gene in standardized form (crim et al, 2005).
</prevsent>
</prevsection>
<citsent citstr=" W05-1303 ">
several approaches have been proposed for gene normalization, including classification techniques (crim etal., 2005; mcdonald et al, 2004), rule-based systems (hanisch et al, 2005), text matching with dictionaries (cohen, 2005), <papid> W05-1303 </papid>and combinations of theseapproaches.</citsent>
<aftsection>
<nextsent>integrated systems for gene identification typically have three stages: identifying candidate mentions in text, identifying the semantic intent of each mention, and normalizing mentions by associating each mention with unique gene identifier (morgan et al, 2004).
</nextsent>
<nextsent>in our current work, wefocus upon normalization, which is currently under explored for human gene names.
</nextsent>
<nextsent>our objective isto create systems for automatically identifying human gene mentions with high accuracy that can beused for practical tasks in biomedical literature retrieval and extraction.
</nextsent>
<nextsent>our current approach relies on manually created and tuned set of rules.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3096">
<title id=" W06-3306.xml">human gene name normalization using text matching with automatically extracted synonym dictionaries </title>
<section> removal of parenthesized materials..  </section>
<citcontext>
<prevsection>
<prevsent>7.
</prevsent>
<prevsent>removal of all spaces..
</prevsent>
</prevsection>
<citsent citstr=" W02-0303 ">
the first four transformations are derived from(cohen et al, 2002).<papid> W02-0303 </papid></citsent>
<aftsection>
<nextsent>not all the rules we experimented with demonstrated good results for human gene name normalization.
</nextsent>
<nextsent>for example, we found that stemming is inappropriate for this task.to amend potential boundary errors of tagged mentions, or to match the variants of the synonyms, four 3ftp://ftp.cs.cornell.edu/pub/smart/english.stop mention reductions (cohen et al, 2002) <papid> W02-0303 </papid>were also applied to the mentions or synonyms: 1.</nextsent>
<nextsent>removal of the first character..</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3098">
<title id=" W06-3306.xml">human gene name normalization using text matching with automatically extracted synonym dictionaries </title>
<section> removal of parenthesized materials..  </section>
<citcontext>
<prevsection>
<prevsent>our program also allows optional string transformations and provides user defined parameter for determining the minimal mention length for approximate string matching.
</prevsent>
<prevsent>the decision on the method chosen may be affected by several factors, such as the application domain, features of the strings representing the entity class, andthe particular datasets used.
</prevsent>
</prevsection>
<citsent citstr=" W05-1302 ">
for gene ner, various scoring methods have been favored (crim et al, 2005; cohen et al, 2003; wellner et al, 2005).<papid> W05-1302 </papid>approximate string matching is usually considered more aggressive than exact string matching with transformations; hence, we applied it as the last step of our normalization sequence.</citsent>
<aftsection>
<nextsent>to assess the usefulness of approximate string matching, we began with our best dictionary subset in subsection 3 44(i.e., using hgnc and swissprot), and applied approximate string matching as an additional normalization step.
</nextsent>
<nextsent>0.35 0.4 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0 0.2 0.4 0.6 0.8 1 pr ec is io q-gram match ratio 0.7 0.71 0.72 0.73 0.74 0.75 0.76 0.77 0.78 0 0.2 0.4 0.6 0.8 1 ec al q-gram match ratio jaro jarowinkler smith waterman tfidf unsmoothedjs jaccard figure 1: performance of approximate string matching for gene normalization.we selected six existing distance metrics that appeared to be useful for human gene normalization:jaro, jarowinkler, smith waterman, tfidf, un smoothedjs, and jaccard.
</nextsent>
<nextsent>our experiment showed that tfidf, unsmoothedjs and jaccard outperformed the others for human gene normalization inour system, as shown in figure 1.
</nextsent>
<nextsent>by incorporating approximate string matching using either of these metrics into our system, overall performance was slightly improved to 0.718 f-measure (0.724 precision and 0.713 recall) when employing high threshold (0.95).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3099">
<title id=" W06-2609.xml">learning to identify definitions using syntactic features </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>klavans and muresan (2000) set up pattern extractor for their def inder system using tagger and finite state grammar.
</prevsent>
<prevsent>joho and sanderson (2000) retrieve descriptive phrases (dp) of query nouns (qn) from text to answer definition questions like whois qn?
</prevsent>
</prevsection>
<citsent citstr=" C92-2082 ">
patterns such as dp especially qn?, as utilized by hearst (1992), <papid> C92-2082 </papid>are used to extract names and their descriptions.</citsent>
<aftsection>
<nextsent>similar patterns are also applied by liu et al(2003) to mine definitions of topic-specific concepts on the web.
</nextsent>
<nextsent>as an additional assumption, specific documents dedicated to the concepts canbe identified if they have particular html and hy perlink structures.hildebrandt et al (2004) <papid> N04-1007 </papid>exploit surface patterns to extract as many relevant nuggets?</nextsent>
<nextsent>of information of concept as possible.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3100">
<title id=" W06-2609.xml">learning to identify definitions using syntactic features </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>patterns such as dp especially qn?, as utilized by hearst (1992), <papid> C92-2082 </papid>are used to extract names and their descriptions.</prevsent>
<prevsent>similar patterns are also applied by liu et al(2003) to mine definitions of topic-specific concepts on the web.</prevsent>
</prevsection>
<citsent citstr=" N04-1007 ">
as an additional assumption, specific documents dedicated to the concepts canbe identified if they have particular html and hy perlink structures.hildebrandt et al (2004) <papid> N04-1007 </papid>exploit surface patterns to extract as many relevant nuggets?</citsent>
<aftsection>
<nextsent>of information of concept as possible.
</nextsent>
<nextsent>similar to our work, copular pattern np1 be np2 is used as one of the extraction patterns.
</nextsent>
<nextsent>nuggets which donot begin with determiner are discarded to filter out spurious nuggets (e.g., progressive tense).
</nextsent>
<nextsent>nuggets extracted from every article in corpus are then stored in relational database.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3101">
<title id=" W06-2609.xml">learning to identify definitions using syntactic features </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>nuggets extracted from every article in corpus are then stored in relational database.
</prevsent>
<prevsent>in the end, answering definition questions becomes as simple as looking up relevant terms from the database.this strategy is similar to our approach for answering definition questions.
</prevsent>
</prevsection>
<citsent citstr=" C04-1199 ">
the use of machine learning techniques can be found in miliaraki and androutsopoulos (2004)<papid> C04-1199 </papid>and androutsopoulos and galanis (2005) <papid> H05-1041 </papid>they use similar patterns as (joho and sanderson, 2000) to construct training attributes.</citsent>
<aftsection>
<nextsent>sager and lhomme (1994) note that the definition of term should at least always contain genus (termscategory) and species (terms properties).
</nextsent>
<nextsent>blairgoldensohn et al (2004) uses machine learning and manually crafted lexico-syntactic patterns to match sentences containing both genus and species phrase forgiven term.
</nextsent>
<nextsent>there is an intuition that most of definition sentences are located at the beginning of documents.
</nextsent>
<nextsent>this lead to the use of sentence number as good indicator of potential definition sentences.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3103">
<title id=" W06-2609.xml">learning to identify definitions using syntactic features </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>nuggets extracted from every article in corpus are then stored in relational database.
</prevsent>
<prevsent>in the end, answering definition questions becomes as simple as looking up relevant terms from the database.this strategy is similar to our approach for answering definition questions.
</prevsent>
</prevsection>
<citsent citstr=" H05-1041 ">
the use of machine learning techniques can be found in miliaraki and androutsopoulos (2004)<papid> C04-1199 </papid>and androutsopoulos and galanis (2005) <papid> H05-1041 </papid>they use similar patterns as (joho and sanderson, 2000) to construct training attributes.</citsent>
<aftsection>
<nextsent>sager and lhomme (1994) note that the definition of term should at least always contain genus (termscategory) and species (terms properties).
</nextsent>
<nextsent>blairgoldensohn et al (2004) uses machine learning and manually crafted lexico-syntactic patterns to match sentences containing both genus and species phrase forgiven term.
</nextsent>
<nextsent>there is an intuition that most of definition sentences are located at the beginning of documents.
</nextsent>
<nextsent>this lead to the use of sentence number as good indicator of potential definition sentences.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3117">
<title id=" W07-0305.xml">olympus an open source framework for conversational spoken language interface research </title>
<section> research.  </section>
<citcontext>
<prevsection>
<prevsent>multi-participant command and-control vera telephone-based task able agent that can be instructed to de-liver messages to third party and make wake-up calls.
</prevsent>
<prevsent>voice mail / message delivery madeleine text-based dialog system for medical diagnosis.
</prevsent>
</prevsection>
<citsent citstr=" N07-2003 ">
diagnosis conquest (bohus et al 2007) <papid> N07-2003 </papid>telephone-based spoken dialog system that provides conference schedule information.</citsent>
<aftsection>
<nextsent>information access (mixed-initiative) raven calendar (stenchikova et al 2007).<papid> N07-4008 </papid></nextsent>
<nextsent>multimodal dialog system for managing personal calendar information, such as meetings, classes, appointments and reminders (uses google calendar as back-end) information access and scheduling table 1.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3118">
<title id=" W07-0305.xml">olympus an open source framework for conversational spoken language interface research </title>
<section> research.  </section>
<citcontext>
<prevsection>
<prevsent>voice mail / message delivery madeleine text-based dialog system for medical diagnosis.
</prevsent>
<prevsent>diagnosis conquest (bohus et al 2007) <papid> N07-2003 </papid>telephone-based spoken dialog system that provides conference schedule information.</prevsent>
</prevsection>
<citsent citstr=" N07-4008 ">
information access (mixed-initiative) raven calendar (stenchikova et al 2007).<papid> N07-4008 </papid></citsent>
<aftsection>
<nextsent>multimodal dialog system for managing personal calendar information, such as meetings, classes, appointments and reminders (uses google calendar as back-end) information access and scheduling table 1.
</nextsent>
<nextsent>olympus-based spoken dialog systems (shaded cells indicate deployed systems) 36veloping rich repertoire of error recovery strategies and (3) developing scalable, data-driven approaches for building error recovery policies1.
</nextsent>
<nextsent>two of the dialog systems from table 1 (lets go!
</nextsent>
<nextsent>and roomline) have provided realistic experimental platform for investigating these issues and evaluating the proposed solutions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3119">
<title id=" W06-3308.xml">bio smile adapting semantic role labeling for biomedical verbs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this paper, we aim to build such biomedical srl system.
</prevsent>
<prevsent>to achieve this goal we roughly implement the following three steps as proposed by wattarujeekrit et al, (2004): (1) create semantic roles for each biomedical verb; (2) construct biomedical corpus annotated with verbs and their corresponding semantic roles (following definitions created in (1) as reference resource;) (3) build an automatic semantic interpretation model using the annotated text as training corpus for machine learning.
</prevsent>
</prevsection>
<citsent citstr=" J05-1004 ">
in the first step, we adopt the definitions found in propbank (palmer et al, 2005), <papid> J05-1004 </papid>defining our own frame sets for verbs not in propbank, such as phosphorylate?.</citsent>
<aftsection>
<nextsent>in the second step, we first use an srl system (tsai et al, 2005) <papid> W05-0638 </papid>trained on the wall street journal (wsj) to automatically tag our corpus.</nextsent>
<nextsent>we then have the results double-checked by human annotators.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3120">
<title id=" W06-3308.xml">bio smile adapting semantic role labeling for biomedical verbs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>to achieve this goal we roughly implement the following three steps as proposed by wattarujeekrit et al, (2004): (1) create semantic roles for each biomedical verb; (2) construct biomedical corpus annotated with verbs and their corresponding semantic roles (following definitions created in (1) as reference resource;) (3) build an automatic semantic interpretation model using the annotated text as training corpus for machine learning.
</prevsent>
<prevsent>in the first step, we adopt the definitions found in propbank (palmer et al, 2005), <papid> J05-1004 </papid>defining our own frame sets for verbs not in propbank, such as phosphorylate?.</prevsent>
</prevsection>
<citsent citstr=" W05-0638 ">
in the second step, we first use an srl system (tsai et al, 2005) <papid> W05-0638 </papid>trained on the wall street journal (wsj) to automatically tag our corpus.</citsent>
<aftsection>
<nextsent>we then have the results double-checked by human annotators.
</nextsent>
<nextsent>finally, we add automatically-generated template features to our srl system to identify adjunct (modifier) arguments, especially those highly relevant to the biomedical domain.
</nextsent>
<nextsent>as proposition banks are semantically annotated versions of penn-style treebank, they provide consistent semantic role labels across different syntactic realizations of the same verb (palmer et al, 2005).<papid> J05-1004 </papid></nextsent>
<nextsent>the annotation captures predicate-argument structures based on the sense tags of polysemous verbs (called framesets) and semantic role labels for each argument of the verb.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3122">
<title id=" W06-3308.xml">bio smile adapting semantic role labeling for biomedical verbs </title>
<section> biomedical proposition bank.  </section>
<citcontext>
<prevsection>
<prevsent>the text of the title and content is segmented into sentences, in which biological terms are annotated with their semantic classes.
</prevsent>
<prevsent>the genia corpus is also annotated with part-ofspeech (pos) tags (tateisi et al, 2004), and co references (yang et al, 2004).
</prevsent>
</prevsection>
<citsent citstr=" I05-2038 ">
the penn-style treebank for genia, created by tateisi et al (2005), <papid> I05-2038 </papid>currently contains 500 ab stracts.</citsent>
<aftsection>
<nextsent>the annotation scheme of the genia treebank (gtb), which basically follows the penn treebank ii (ptb) scheme (bies et al, 1995), is encoded in xml.
</nextsent>
<nextsent>however, in contrast to the wsj corpus, genia lacks proposition bank.
</nextsent>
<nextsent>we therefore use its 500 abstracts with gtb as our corpus.
</nextsent>
<nextsent>to develop our biomedical proposition bank, bioprop, we add the proposition bank annotation on top of the gtb annotation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3123">
<title id=" W06-3308.xml">bio smile adapting semantic role labeling for biomedical verbs </title>
<section> semantic role labeling on bioprop.  </section>
<citcontext>
<prevsection>
<prevsent>like pos tagging, chunking, and named entity recognition, srl can be formulated as sentence tagging problem.
</prevsent>
<prevsent>a sentence can be represented by sequence of words, sequence of phrases, or parsing tree; the basic units of sentence are words, phrases, and constituents arranged in the above representations, respectively.
</prevsent>
</prevsection>
<citsent citstr=" W04-2416 ">
hacioglu et al (2004) <papid> W04-2416 </papid>showed that tagging phrase by phrase (p-by-p) is better than word by word (w-by-w).</citsent>
<aftsection>
<nextsent>punyakanok et al, (2004) <papid> C04-1197 </papid>further showed that constituent-by-constituent (c by-c) tagging is better than p-by-p.</nextsent>
<nextsent>therefore, we choose c-by-c tagging for srl.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3124">
<title id=" W06-3308.xml">bio smile adapting semantic role labeling for biomedical verbs </title>
<section> semantic role labeling on bioprop.  </section>
<citcontext>
<prevsection>
<prevsent>a sentence can be represented by sequence of words, sequence of phrases, or parsing tree; the basic units of sentence are words, phrases, and constituents arranged in the above representations, respectively.
</prevsent>
<prevsent>hacioglu et al (2004) <papid> W04-2416 </papid>showed that tagging phrase by phrase (p-by-p) is better than word by word (w-by-w).</prevsent>
</prevsection>
<citsent citstr=" C04-1197 ">
punyakanok et al, (2004) <papid> C04-1197 </papid>further showed that constituent-by-constituent (c by-c) tagging is better than p-by-p.</citsent>
<aftsection>
<nextsent>therefore, we choose c-by-c tagging for srl.
</nextsent>
<nextsent>the gold standard srl corpus, propbank, was designed as an additional layer of annotation on top of the syntactic structures of the penn treebank.
</nextsent>
<nextsent>srl can be broken into two steps.
</nextsent>
<nextsent>first, we must identify all the predicates.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3125">
<title id=" W06-3308.xml">bio smile adapting semantic role labeling for biomedical verbs </title>
<section> semantic role labeling on bioprop.  </section>
<citcontext>
<prevsection>
<prevsent>the features used in the baseline argument classification model 3.2 basic features.
</prevsent>
<prevsent>table 2 shows the features that are used in our baseline argument classification model.
</prevsent>
</prevsection>
<citsent citstr=" P03-1002 ">
their effectiveness has been previously shown by (pradhan et al, 2004; surdeanu et al, 2003; <papid> P03-1002 </papid>xue et al, 2004).</citsent>
<aftsection>
<nextsent>detailed descriptions of these features can be found in (tsai et al, 2005).<papid> W05-0638 </papid></nextsent>
<nextsent>3.3 named entity features.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3129">
<title id=" W06-1655.xml">a hybrid markovsemimarkov conditional random field for sequence segmentation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the problem of segmenting sequence data into chunks arises in many natural language applications, such as named-entity recognition, shallow parsing, and word segmentation in east asian languages.
</prevsent>
<prevsent>two popular discriminative models that have been proposed for these tasks are the conditional random field (crfs) (lafferty et al, 2001) and the semi-markov conditional random field (semi-crf) (sarawagi and cohen, 2004).a crf in its basic form is model for labeling tokens in sequence; however it can easily be adapted to perform segmentation via labeling each token as begin or continuation, or according to some similar scheme.
</prevsent>
</prevsection>
<citsent citstr=" C04-1081 ">
crfs using this technique have been shown to be very successful at thetask of chinese word segmentation (cws), starting with the model of peng et al (2004).<papid> C04-1081 </papid></citsent>
<aftsection>
<nextsent>in the second international chinese word segmentationbakeoff (emerson, 2005), <papid> I05-3017 </papid>two of the highest scoring systems in the closed track competition we rebased on crf model.</nextsent>
<nextsent>(tseng et al, 2005; <papid> I05-3027 </papid>asahara et al, 2005) <papid> I05-3018 </papid>while the crf is quite effective compared with other models designed for cws, one wonders whether it may be limited by its restrictive independence assumptions on non-adjacent labels: anorder-m crf satisfies the order-m markov assumption that, globally conditioned on the input sequence, each label is independent of all other labels given the labels to its left and right.consequently, the model only sees?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3130">
<title id=" W06-1655.xml">a hybrid markovsemimarkov conditional random field for sequence segmentation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>two popular discriminative models that have been proposed for these tasks are the conditional random field (crfs) (lafferty et al, 2001) and the semi-markov conditional random field (semi-crf) (sarawagi and cohen, 2004).a crf in its basic form is model for labeling tokens in sequence; however it can easily be adapted to perform segmentation via labeling each token as begin or continuation, or according to some similar scheme.
</prevsent>
<prevsent>crfs using this technique have been shown to be very successful at thetask of chinese word segmentation (cws), starting with the model of peng et al (2004).<papid> C04-1081 </papid></prevsent>
</prevsection>
<citsent citstr=" I05-3017 ">
in the second international chinese word segmentationbakeoff (emerson, 2005), <papid> I05-3017 </papid>two of the highest scoring systems in the closed track competition we rebased on crf model.</citsent>
<aftsection>
<nextsent>(tseng et al, 2005; <papid> I05-3027 </papid>asahara et al, 2005) <papid> I05-3018 </papid>while the crf is quite effective compared with other models designed for cws, one wonders whether it may be limited by its restrictive independence assumptions on non-adjacent labels: anorder-m crf satisfies the order-m markov assumption that, globally conditioned on the input sequence, each label is independent of all other labels given the labels to its left and right.consequently, the model only sees?</nextsent>
<nextsent>word boundaries within moving window of + 1 characters, which prohibits it from explicitly modeling the tendency of strings longer than that window to form words, or from modeling the lengths of the words.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3132">
<title id=" W06-1655.xml">a hybrid markovsemimarkov conditional random field for sequence segmentation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>crfs using this technique have been shown to be very successful at thetask of chinese word segmentation (cws), starting with the model of peng et al (2004).<papid> C04-1081 </papid></prevsent>
<prevsent>in the second international chinese word segmentationbakeoff (emerson, 2005), <papid> I05-3017 </papid>two of the highest scoring systems in the closed track competition we rebased on crf model.</prevsent>
</prevsection>
<citsent citstr=" I05-3027 ">
(tseng et al, 2005; <papid> I05-3027 </papid>asahara et al, 2005) <papid> I05-3018 </papid>while the crf is quite effective compared with other models designed for cws, one wonders whether it may be limited by its restrictive independence assumptions on non-adjacent labels: anorder-m crf satisfies the order-m markov assumption that, globally conditioned on the input sequence, each label is independent of all other labels given the labels to its left and right.consequently, the model only sees?</citsent>
<aftsection>
<nextsent>word boundaries within moving window of + 1 characters, which prohibits it from explicitly modeling the tendency of strings longer than that window to form words, or from modeling the lengths of the words.
</nextsent>
<nextsent>although the window can in principle be widened by increasing , this is not practical solution as the complexity of training and decoding linear sequence crf grows exponentially with the markov order.the semi-crf is sequence model that is designed to address this difficulty via careful relaxation of the markov assumption.
</nextsent>
<nextsent>rather than recasting the segmentation problem as labeling problem, the semi-crf directly models the distribution of chunk boundaries.1 in terms of inde1as it was originally described, the semi-crf also assigns labels to each chunk, effectively performing joint segmentation and labeling, but in pure segmentation problem such as cws, the use of labels is unnecessary.
</nextsent>
<nextsent>465 pend ence, using an order-m semi-crf entails the assumption that, globally conditioned on the input sequence, the position of each chunk boundary is independent of all other boundaries given the positions of the boundaries to its left and right regardless of how far away they are.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3133">
<title id=" W06-1655.xml">a hybrid markovsemimarkov conditional random field for sequence segmentation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>crfs using this technique have been shown to be very successful at thetask of chinese word segmentation (cws), starting with the model of peng et al (2004).<papid> C04-1081 </papid></prevsent>
<prevsent>in the second international chinese word segmentationbakeoff (emerson, 2005), <papid> I05-3017 </papid>two of the highest scoring systems in the closed track competition we rebased on crf model.</prevsent>
</prevsection>
<citsent citstr=" I05-3018 ">
(tseng et al, 2005; <papid> I05-3027 </papid>asahara et al, 2005) <papid> I05-3018 </papid>while the crf is quite effective compared with other models designed for cws, one wonders whether it may be limited by its restrictive independence assumptions on non-adjacent labels: anorder-m crf satisfies the order-m markov assumption that, globally conditioned on the input sequence, each label is independent of all other labels given the labels to its left and right.consequently, the model only sees?</citsent>
<aftsection>
<nextsent>word boundaries within moving window of + 1 characters, which prohibits it from explicitly modeling the tendency of strings longer than that window to form words, or from modeling the lengths of the words.
</nextsent>
<nextsent>although the window can in principle be widened by increasing , this is not practical solution as the complexity of training and decoding linear sequence crf grows exponentially with the markov order.the semi-crf is sequence model that is designed to address this difficulty via careful relaxation of the markov assumption.
</nextsent>
<nextsent>rather than recasting the segmentation problem as labeling problem, the semi-crf directly models the distribution of chunk boundaries.1 in terms of inde1as it was originally described, the semi-crf also assigns labels to each chunk, effectively performing joint segmentation and labeling, but in pure segmentation problem such as cws, the use of labels is unnecessary.
</nextsent>
<nextsent>465 pend ence, using an order-m semi-crf entails the assumption that, globally conditioned on the input sequence, the position of each chunk boundary is independent of all other boundaries given the positions of the boundaries to its left and right regardless of how far away they are.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3134">
<title id=" W06-1655.xml">a hybrid markovsemimarkov conditional random field for sequence segmentation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>orthogonally, we explore in this paper the useof very powerful feature for the semi-crf derived from generative model.it is common in statistical nlp to use as features in discriminative model the (logarithm ofthe) estimated probability of some event according to generative model.
</prevsent>
<prevsent>for example, collins (2000) uses discriminative classifier for choosing among the top parse trees output by generative baseline model, and uses the log-probability of parse according to the baseline model as feature in the reranker.
</prevsent>
</prevsection>
<citsent citstr=" P02-1038 ">
similarly, the machine translation system of och and ney uses log-probabilities of phrasal translations and other events as features in log-linear model (och and ney, 2002; <papid> P02-1038 </papid>och andney, 2004).<papid> J04-4002 </papid></citsent>
<aftsection>
<nextsent>there are many reasons for incorporating these types of features, including the desire to combine the higher accuracy of discriminative model with the simple parameter estimation and inference of generative one, and also the fact that generative models are more robust in data sparse scenarios (ng and jordan, 2001).
</nextsent>
<nextsent>for word segmentation, one might want to use as local feature the log-probability that segment is word, given the character sequence it spans.
</nextsent>
<nextsent>a curious property of this feature is that it induces counter intuitive asymmetry between the is-word and is-not-word cases: the component generative model can effectively dictate that certain chunk is not word, by assigning it very low probability (driving the feature value to negative infinity), but it cannot dictate that chunk is word, because the log-probability is bounded above.2 if instead the log conditional odds log pi(y|x)pi(y|x) is used, the asymmetry disappears.
</nextsent>
<nextsent>we show that such log odds feature provides much greater benefit than the log-probability, and that it is useful to include such feature even when the model also includes indicator function features for every word in the training corpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3135">
<title id=" W06-1655.xml">a hybrid markovsemimarkov conditional random field for sequence segmentation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>orthogonally, we explore in this paper the useof very powerful feature for the semi-crf derived from generative model.it is common in statistical nlp to use as features in discriminative model the (logarithm ofthe) estimated probability of some event according to generative model.
</prevsent>
<prevsent>for example, collins (2000) uses discriminative classifier for choosing among the top parse trees output by generative baseline model, and uses the log-probability of parse according to the baseline model as feature in the reranker.
</prevsent>
</prevsection>
<citsent citstr=" J04-4002 ">
similarly, the machine translation system of och and ney uses log-probabilities of phrasal translations and other events as features in log-linear model (och and ney, 2002; <papid> P02-1038 </papid>och andney, 2004).<papid> J04-4002 </papid></citsent>
<aftsection>
<nextsent>there are many reasons for incorporating these types of features, including the desire to combine the higher accuracy of discriminative model with the simple parameter estimation and inference of generative one, and also the fact that generative models are more robust in data sparse scenarios (ng and jordan, 2001).
</nextsent>
<nextsent>for word segmentation, one might want to use as local feature the log-probability that segment is word, given the character sequence it spans.
</nextsent>
<nextsent>a curious property of this feature is that it induces counter intuitive asymmetry between the is-word and is-not-word cases: the component generative model can effectively dictate that certain chunk is not word, by assigning it very low probability (driving the feature value to negative infinity), but it cannot dictate that chunk is word, because the log-probability is bounded above.2 if instead the log conditional odds log pi(y|x)pi(y|x) is used, the asymmetry disappears.
</nextsent>
<nextsent>we show that such log odds feature provides much greater benefit than the log-probability, and that it is useful to include such feature even when the model also includes indicator function features for every word in the training corpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3144">
<title id=" W06-1655.xml">a hybrid markovsemimarkov conditional random field for sequence segmentation </title>
<section> relation to previous work.  </section>
<citcontext>
<prevsection>
<prevsent>it would be interesting to see the log-odds applied in more contexts where log-probabilities are typically used as features.
</prevsent>
<prevsent>we have presented the intuitive argument that the log-odds may be advantageous because it does not exhibit the 0-1 asymmetry of the log-probability, but it would be satisfying to justify the choice on more theoretical grounds.
</prevsent>
</prevsection>
<citsent citstr=" P05-1045 ">
there is significant volume of work exploring the use of crfs for variety of chunking tasks, including named-entity recognition, gene prediction, shallow parsing and others (finkel et al., 2005; <papid> P05-1045 </papid>culotta et al, 2005; sha and pereira,2003).<papid> N03-1028 </papid></citsent>
<aftsection>
<nextsent>the current work indicates that these systems might be improved by moving to semi-crf model.
</nextsent>
<nextsent>there have not been large number of studies using the semi-crf, but the few that have been done found only marginal improvements over pure crf systems (sarawagi and cohen, 2004; liang, 2005; daume?
</nextsent>
<nextsent>iii and marcu, 2005).
</nextsent>
<nextsent>notably, none of those studies experimented with features of chunk non-boundaries, as is achieved by the use of crf-type features involving the label c, andwe take this to be the reason for their not obtaining higher results.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3145">
<title id=" W06-1655.xml">a hybrid markovsemimarkov conditional random field for sequence segmentation </title>
<section> relation to previous work.  </section>
<citcontext>
<prevsection>
<prevsent>it would be interesting to see the log-odds applied in more contexts where log-probabilities are typically used as features.
</prevsent>
<prevsent>we have presented the intuitive argument that the log-odds may be advantageous because it does not exhibit the 0-1 asymmetry of the log-probability, but it would be satisfying to justify the choice on more theoretical grounds.
</prevsent>
</prevsection>
<citsent citstr=" N03-1028 ">
there is significant volume of work exploring the use of crfs for variety of chunking tasks, including named-entity recognition, gene prediction, shallow parsing and others (finkel et al., 2005; <papid> P05-1045 </papid>culotta et al, 2005; sha and pereira,2003).<papid> N03-1028 </papid></citsent>
<aftsection>
<nextsent>the current work indicates that these systems might be improved by moving to semi-crf model.
</nextsent>
<nextsent>there have not been large number of studies using the semi-crf, but the few that have been done found only marginal improvements over pure crf systems (sarawagi and cohen, 2004; liang, 2005; daume?
</nextsent>
<nextsent>iii and marcu, 2005).
</nextsent>
<nextsent>notably, none of those studies experimented with features of chunk non-boundaries, as is achieved by the use of crf-type features involving the label c, andwe take this to be the reason for their not obtaining higher results.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3146">
<title id=" W06-1520.xml">handling unlike coordinated phrases in tag by mixing syntactic category and grammatical function </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>large scale handcrafted grammars for many languages have been built based on this paradigm (joshi, 2001; xtag research group, 2001; kroch and joshi, 1985; abeille?
</prevsent>
<prevsent>and candito, 2000; candito, 1998; becker, 1993; frank, 2002; joshi and schabes, 1997; abeille?
</prevsent>
</prevsection>
<citsent citstr=" P00-1058 ">
and rambow, 2000) as well as grammars extracted from corpora (chen and vijay-shanker, 2000; chiang, 2000; <papid> P00-1058 </papid>hwa, 1999; <papid> P99-1010 </papid>xia et al, 2001; xia, 2001).</citsent>
<aftsection>
<nextsent>the latter is partly due to the fact that large scale annotated corpora such as the penn treebank(marcus et al, 1994; <papid> H94-1020 </papid>bies et al, 1995) give primacy to syntactic categories.</nextsent>
<nextsent>after all this is the most strongly sedimented generative approach at least since (chomsky, 1957).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3147">
<title id=" W06-1520.xml">handling unlike coordinated phrases in tag by mixing syntactic category and grammatical function </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>large scale handcrafted grammars for many languages have been built based on this paradigm (joshi, 2001; xtag research group, 2001; kroch and joshi, 1985; abeille?
</prevsent>
<prevsent>and candito, 2000; candito, 1998; becker, 1993; frank, 2002; joshi and schabes, 1997; abeille?
</prevsent>
</prevsection>
<citsent citstr=" P99-1010 ">
and rambow, 2000) as well as grammars extracted from corpora (chen and vijay-shanker, 2000; chiang, 2000; <papid> P00-1058 </papid>hwa, 1999; <papid> P99-1010 </papid>xia et al, 2001; xia, 2001).</citsent>
<aftsection>
<nextsent>the latter is partly due to the fact that large scale annotated corpora such as the penn treebank(marcus et al, 1994; <papid> H94-1020 </papid>bies et al, 1995) give primacy to syntactic categories.</nextsent>
<nextsent>after all this is the most strongly sedimented generative approach at least since (chomsky, 1957).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3148">
<title id=" W06-1520.xml">handling unlike coordinated phrases in tag by mixing syntactic category and grammatical function </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>and candito, 2000; candito, 1998; becker, 1993; frank, 2002; joshi and schabes, 1997; abeille?
</prevsent>
<prevsent>and rambow, 2000) as well as grammars extracted from corpora (chen and vijay-shanker, 2000; chiang, 2000; <papid> P00-1058 </papid>hwa, 1999; <papid> P99-1010 </papid>xia et al, 2001; xia, 2001).</prevsent>
</prevsection>
<citsent citstr=" H94-1020 ">
the latter is partly due to the fact that large scale annotated corpora such as the penn treebank(marcus et al, 1994; <papid> H94-1020 </papid>bies et al, 1995) give primacy to syntactic categories.</citsent>
<aftsection>
<nextsent>after all this is the most strongly sedimented generative approach at least since (chomsky, 1957).
</nextsent>
<nextsent>computational approaches of grammar based on grammatical function such as that of susumukuno (kuno, 1987) have been given less importance.
</nextsent>
<nextsent>although we can think of simply inserting functional labels in elementary trees or use them in meta-level to generate the grammar, such as in (candito, 1998; kinyon, 2000; <papid> C00-1065 </papid>clement and kinyon, 2003), <papid> P03-1024 </papid>such tags are generally not seen as an essential part of the derivational process.</nextsent>
<nextsent>nevertheless coordination is such an inherently functional phenomenon as we show next.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3149">
<title id=" W06-1520.xml">handling unlike coordinated phrases in tag by mixing syntactic category and grammatical function </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>after all this is the most strongly sedimented generative approach at least since (chomsky, 1957).
</prevsent>
<prevsent>computational approaches of grammar based on grammatical function such as that of susumukuno (kuno, 1987) have been given less importance.
</prevsent>
</prevsection>
<citsent citstr=" C00-1065 ">
although we can think of simply inserting functional labels in elementary trees or use them in meta-level to generate the grammar, such as in (candito, 1998; kinyon, 2000; <papid> C00-1065 </papid>clement and kinyon, 2003), <papid> P03-1024 </papid>such tags are generally not seen as an essential part of the derivational process.</citsent>
<aftsection>
<nextsent>nevertheless coordination is such an inherently functional phenomenon as we show next.
</nextsent>
<nextsent>consider the sentences in (1) and (2).
</nextsent>
<nextsent>these are examples of regular coordination between phrases of the same category.
</nextsent>
<nextsent>they can easily be handled in the traditional grammar approaches of syntactic category.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3150">
<title id=" W06-1520.xml">handling unlike coordinated phrases in tag by mixing syntactic category and grammatical function </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>after all this is the most strongly sedimented generative approach at least since (chomsky, 1957).
</prevsent>
<prevsent>computational approaches of grammar based on grammatical function such as that of susumukuno (kuno, 1987) have been given less importance.
</prevsent>
</prevsection>
<citsent citstr=" P03-1024 ">
although we can think of simply inserting functional labels in elementary trees or use them in meta-level to generate the grammar, such as in (candito, 1998; kinyon, 2000; <papid> C00-1065 </papid>clement and kinyon, 2003), <papid> P03-1024 </papid>such tags are generally not seen as an essential part of the derivational process.</citsent>
<aftsection>
<nextsent>nevertheless coordination is such an inherently functional phenomenon as we show next.
</nextsent>
<nextsent>consider the sentences in (1) and (2).
</nextsent>
<nextsent>these are examples of regular coordination between phrases of the same category.
</nextsent>
<nextsent>they can easily be handled in the traditional grammar approaches of syntactic category.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3151">
<title id=" W06-1520.xml">handling unlike coordinated phrases in tag by mixing syntactic category and grammatical function </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>they are different in the sense that the coordination is across categories.
</prevsent>
<prevsent>this poses strong problem to the traditional grammar of syntactic categories.
</prevsent>
</prevsection>
<citsent citstr=" W02-1509 ">
this has been noticed for tags in (prolo, 2002).<papid> W02-1509 </papid></citsent>
<aftsection>
<nextsent>recently this has also been tackled in the hpsg framework by (sag, 2003) and (abeille?, 2004).
</nextsent>
<nextsent>the penn treebank calls this constituents ucp for unlike coordinated phrases?
</nextsent>
<nextsent>(bies et al, 1995).
</nextsent>
<nextsent>the problem is that we would need rules of the kind below (using context-free rules for short ? see (prolo, 2002) <papid> W02-1509 </papid>for tags).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3154">
<title id=" W06-1644.xml">style and topic language model adaptation using hmmlda </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>with the rapid growth of audio-visual materials available over the web, effective language modeling of the diverse content, both in style and topic, becomes essential for efficient access and management of this information.
</prevsent>
<prevsent>as prime example, successful language modeling for academic lectures not only enables the initial transcription via automatic speech recognition, but also assists educators and students in the creation and navigation of these materials through annotation, retrieval, summarization, and even translation of the embedded content.
</prevsent>
</prevsection>
<citsent citstr=" W04-2902 ">
compared with other types of audio content, lecture speech often exhibits high degree of spontaneity and focuses on narrow topics with specific terminology (furui, 2003; glass et al  2004).<papid> W04-2902 </papid></citsent>
<aftsection>
<nextsent>unfortunately, training corpora available for language modeling rarely match the target lecture in both style and topic.
</nextsent>
<nextsent>while transcripts from other lectures better match the style of the target lecture than written text, it is often difficult to find transcripts on the target topic.
</nextsent>
<nextsent>on the other hand, although topic-specific vocabulary can be gleaned from related text materials, such as the textbook and lecture slides, written language is poor predictor of how words are actually spoken.
</nextsent>
<nextsent>furthermore, given that the precise topic of target lecture is often unknown priori and may even shift over time, it is generally difficult to identify topically related documents.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3156">
<title id=" W07-0610.xml">the topology of synonymy and homonymy networks </title>
<section> semantics networks.  </section>
<citcontext>
<prevsection>
<prevsent>this distinction is significant 75 in language acquisition, but as yet little research has been performed on the learning of polysemes (casenhiser, 2005).
</prevsent>
<prevsent>it is also significant for natural language processing.
</prevsent>
</prevsection>
<citsent citstr=" H05-1051 ">
the effect of disambiguating homonyms is markedly different from polysemes in information retrieval (stokoe, 2005).<papid> H05-1051 </papid></citsent>
<aftsection>
<nextsent>we do not have access to these distinctions, as they are not available in most resources, nor are there techniques to automatically acquire these distinctions (kilgarriff and yallop, 2000).
</nextsent>
<nextsent>for simplicity, will conflate the categories under homonymy.
</nextsent>
<nextsent>there have been several studies into synonymy and homonymy acquisition in children, and these have shown that it lags behind vocabulary growth (doherty and perner, 1998; garnham et al, 2000).
</nextsent>
<nextsent>a child will associate both rabbit and bunny with the same concept, but before the age of four, most children have difficulty in choosing the word bunny if they have already been presented with the word rabbit.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3157">
<title id=" W07-0610.xml">the topology of synonymy and homonymy networks </title>
<section> distributional similarity networks.  </section>
<citcontext>
<prevsection>
<prevsent>here words are projected into vector space using the contexts in which they appear as axes.
</prevsent>
<prevsent>contexts can be as 1e-05 1e-04 0.001 0.01 0.1 1 1 10 100 1000 10000 p( k) k=5 *k=5 k=50 *k=50 figure 3: degree distributions of jaccard wide as document (landauer and dumais, 1997) or close as grammatical dependencies (grefenstette,1994).
</prevsent>
</prevsection>
<citsent citstr=" P06-1046 ">
the distance between words in this space approximates the similarity measured by synonymy.we use the noun similarities produced by gorman and curran (2006) <papid> P06-1046 </papid>using the weighted jaccard measure and the t-test weight and grammatical relations extracted from their large corpus,the method found to perform best against their gold standard evaluation.</citsent>
<aftsection>
<nextsent>only words with corpus frequency higher than 100 are included.
</nextsent>
<nextsent>this method is comparable to that used in lsa, although using grammatical relations as context produces similarity much more like synonymy than those taken at document level (kilgarriff and yallop, 2000).distributional similarity produces list of vocabulary words, their similar neighbours and the similarity to the neighbours.
</nextsent>
<nextsent>these lists approximate synonymy by measuring substitutability in context, and do not only find synonyms as near neighbour sas both antonyms and hyponyms are frequently substitutable in grammatical context (weeds, 2003).
</nextsent>
<nextsent>from this we generate graphs by taking either the knearest neighbours to each word (k-nn), or by using threshold.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3158">
<title id=" W06-2935.xml">language independent probabilistic context free parsing bolstered by machine learning </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>un lexicalized probabilistic context-free parsing is general and flexible approach that sometimes reaches competitive results in multilingual dependency parsing even if minimum of language-specificinformation is supplied.
</prevsent>
<prevsent>furthermore, integrating parser results (good at long de pendencies) and tagger results (good at short range dependencies, and more easily adaptable to treebank peculiarities) gives competitive results in all languages.
</prevsent>
</prevsection>
<citsent citstr=" P03-1054 ">
un lexicalized probabilistic context-free parsing is simple and flexible approach that nevertheless has shown good performance (klein and manning, 2003).<papid> P03-1054 </papid></citsent>
<aftsection>
<nextsent>we applied this approach to the shared task (buchholz et al , 2006) for arabic (hajic?
</nextsent>
<nextsent>et al ,2004), chinese (chen et al , 2003), czech (bh mov?
</nextsent>
<nextsent>et al , 2003), danish (kromann, 2003), dutch (van der beek et al , 2002), german (brants et al ,2002), japanese (kawata and bartels, 2000), portuguese (afonso et al , 2002), slovene (deroski etal., 2006), spanish (civit torruella and mart?
</nextsent>
<nextsent>antonn, 2002), swedish (nilsson et al , 2005), turkish (oflazer et al , 2003; atalay et al , 2003), butnot bulgarian (simov et al , 2005).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3159">
<title id=" W06-2935.xml">language independent probabilistic context free parsing bolstered by machine learning </title>
<section> context free parsing.  </section>
<citcontext>
<prevsection>
<prevsent>3.1 the parser.
</prevsent>
<prevsent>basically, we investigated the performance of straightforward un lexicalized statistical parser, viz.
</prevsent>
</prevsection>
<citsent citstr=" C04-1024 ">
bitpar (schmid, 2004).<papid> C04-1024 </papid></citsent>
<aftsection>
<nextsent>bitpar is cky parser that uses bit vectors for efficient representation of the chart and its items.
</nextsent>
<nextsent>if frequencies for the grammatical and lexical rules in training set are available, bitpar uses the viterbi algorithm to extract the most probable parse tree (according to pcfg) from the chart.
</nextsent>
<nextsent>231 3.2 converting dependency structure to.
</nextsent>
<nextsent>constituency structure in order to determine the grammar rules required by the context-free parser, the dependency trees in the conll format have to be converted to constituencytrees.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3160">
<title id=" W06-2935.xml">language independent probabilistic context free parsing bolstered by machine learning </title>
<section> context free parsing.  </section>
<citcontext>
<prevsection>
<prevsent>gaifman (1965) proved that projective dependency grammars can be mapped to context-free grammars.
</prevsent>
<prevsent>the main information that needs to be added in going from dependency to constituency structure is the category of non-terminals.
</prevsent>
</prevsection>
<citsent citstr=" H01-1014 ">
the usage of special knowledge bases to determine projections of categories (xia and palmer, 2001) <papid> H01-1014 </papid>would have presupposed language-dependent knowledge, so we investigated two other options: flat rules (collinset al , 1999) <papid> P99-1065 </papid>and binary rules.</citsent>
<aftsection>
<nextsent>in the flat rules approach, each lexical category projects to exactly one phrasal category, and every projection chain has length of at most one.
</nextsent>
<nextsent>the binary rules approach makes use of the x-bar-scheme and thus introduces along with the phrasal category an intermediate category.
</nextsent>
<nextsent>the phrasal category must not occur more than once in projection chain, and projection chain must not end in an intermediate category.
</nextsent>
<nextsent>in both approaches, projection is only triggered if dependents are present; in case category occurs as dependent itself, no projection is required.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3161">
<title id=" W06-2935.xml">language independent probabilistic context free parsing bolstered by machine learning </title>
<section> context free parsing.  </section>
<citcontext>
<prevsection>
<prevsent>gaifman (1965) proved that projective dependency grammars can be mapped to context-free grammars.
</prevsent>
<prevsent>the main information that needs to be added in going from dependency to constituency structure is the category of non-terminals.
</prevsent>
</prevsection>
<citsent citstr=" P99-1065 ">
the usage of special knowledge bases to determine projections of categories (xia and palmer, 2001) <papid> H01-1014 </papid>would have presupposed language-dependent knowledge, so we investigated two other options: flat rules (collinset al , 1999) <papid> P99-1065 </papid>and binary rules.</citsent>
<aftsection>
<nextsent>in the flat rules approach, each lexical category projects to exactly one phrasal category, and every projection chain has length of at most one.
</nextsent>
<nextsent>the binary rules approach makes use of the x-bar-scheme and thus introduces along with the phrasal category an intermediate category.
</nextsent>
<nextsent>the phrasal category must not occur more than once in projection chain, and projection chain must not end in an intermediate category.
</nextsent>
<nextsent>in both approaches, projection is only triggered if dependents are present; in case category occurs as dependent itself, no projection is required.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3163">
<title id=" W06-2935.xml">language independent probabilistic context free parsing bolstered by machine learning </title>
<section> context free parsing.  </section>
<citcontext>
<prevsection>
<prevsent>given knowledge about complement relations, it is fairly easy to construct subcategorization frames for word occurrences: subcategorization frame is simply the set of the complement relations by which dependents are attached to the word.
</prevsent>
<prevsent>to give the parser access to these lists, we annotated the category of sub categorizing word with its subcategorization frame.
</prevsent>
</prevsection>
<citsent citstr=" C04-1056 ">
in this way, the parser can learn to associate the subcategorization requirements of word with its local syntactic context (schiehlen, 2004).<papid> C04-1056 </papid></citsent>
<aftsection>
<nextsent>coordination constructions are marked either in the conjuncts (ch, cz, da, du, ge, po, sw) orthe coordinator (ar, sl).
</nextsent>
<nextsent>if conjuncts show coordination, common representation of asyndetic coordination has one conjunct point to another conjunct.
</nextsent>
<nextsent>it is therefore important to distinguish coordinators from conjuncts.
</nextsent>
<nextsent>coordinators are either singled out by special dependency relations (da, po, sw) or bytheir pos tags (ch, du).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3167">
<title id=" W06-2935.xml">language independent probabilistic context free parsing bolstered by machine learning </title>
<section> context free parsing.  </section>
<citcontext>
<prevsection>
<prevsent>and 1   % %
</prevsent>
<prevsent> 5 ).
</prevsent>
</prevsection>
<citsent citstr=" P06-1023 ">
we used script of schmid (2006) <papid> P06-1023 </papid>to markovize infrequent rules in this manner (i.e. all rules with less than 50 occurrences that are not coordination rules).</citsent>
<aftsection>
<nextsent>for time reasons, markov ization was not taken into account in the submitted results.
</nextsent>
<nextsent>we refer to figures 2 and 3 (line labelled cf+markov) for listing of the results attainable by markov ization on the individual treebanks.
</nextsent>
<nextsent>performance gains are even more dramatic if in addition dependency relations + manual pos tag classes are used as categories (line labelled cfm+newcl in figures 2 and 3).
</nextsent>
<nextsent>3.6 from constituency structure back to.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3170">
<title id=" W06-2935.xml">language independent probabilistic context free parsing bolstered by machine learning </title>
<section> machine learning.  </section>
<citcontext>
<prevsection>
<prevsent>for this work, we recast the dependency parsing problem as classification problem:given some feature information on the word token, in which dependency relations does it standto which head?
</prevsent>
<prevsent>while the representation of dependency relations is straightforward, the representation of heads is more difficult.
</prevsent>
</prevsection>
<citsent citstr=" P03-1015 ">
building on past experiments (schiehlen, 2003), <papid> P03-1015 </papid>we chose the nth-tagrepresentation which consists of three pieces of in formation: the pos tag of the head, the direction in which the head lies (left or right), and the number ofwords with the same pos tag between head and de pendent.</citsent>
<aftsection>
<nextsent>we used the following features to describe word token: the fine-grained pos tag, the lemma(or full form) if it occurs at least 10 times, the morphosyntactic features, and the pos tags of the four preceding and the four following word tokens.
</nextsent>
<nextsent>the learner was trained in standard configuration (30 it erations).
</nextsent>
<nextsent>the results for this method on the test data are shown in figure 2 (line maxent).
</nextsent>
<nextsent>in second experiment we added parsing results (obtained by 10-fold cross validation on the training set) in two features: proposed dependency relation and proposed head.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3173">
<title id=" W06-1654.xml">random indexing using statistical weight functions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>existing approaches differ primarily in their definition of context, e.g. the surrounding words or the entire document, and their choice of distance metric for calculating similarity between the context vectors representing each term.in this paper, we analyse the use of random indexing (kanerva et al, 2000) for semantic similarity measurement.
</prevsent>
<prevsent>random indexing is an approximation technique proposed as an alternative to latent semantic analysis (lsa, landauer and du mais, 1997).
</prevsent>
</prevsection>
<citsent citstr=" P02-1030 ">
random indexing is more scalable and allows for the incremental learning of context information.curran and moens (2002)<papid> P02-1030 </papid>found that dramatically increasing the volume of raw input data for distributional similarity tasks increases the accuracy of synonyms extracted.</citsent>
<aftsection>
<nextsent>random indexing performs poorly on these volumes of data.
</nextsent>
<nextsent>noting that in many nlp tasks, including distributional similarity, statistical weighting is used to improve performance, we modify the random indexing algorithm to allow for weighted contexts.
</nextsent>
<nextsent>we test the performance of the original and our modified system using existing evaluation metrics.we further evaluate against bilingual lexicon extraction using distributional similarity (sahlgren and karlgren, 2005).
</nextsent>
<nextsent>the paper concludes with more detailed analysis of random indexing in terms of both task and corpus composition.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3175">
<title id=" W06-1654.xml">random indexing using statistical weight functions </title>
<section> semantic similarity.  </section>
<citcontext>
<prevsection>
<prevsent>4.2 bilingual lexicon acquisition.
</prevsent>
<prevsent>a variation on the extraction of synonymy relations, is the extraction of bilingual lexicons.
</prevsent>
</prevsection>
<citsent citstr=" C88-1016 ">
this is the task of finding for word in one language words of similar meaning in second language.the results of this can be used to aid manual construction of resources or directly aid translation.this task was first approached as distributional similarity-like problem by brown et al (1988).<papid> C88-1016 </papid></citsent>
<aftsection>
<nextsent>their approach uses aligned corpora in two or more languages: the source language, from which we are translating, and the target language, to which we are translating.
</nextsent>
<nextsent>for each aligned segment, they measure co-occurrence scores between each word in the source segment and each word in the target segment.
</nextsent>
<nextsent>these co-occurrence scores are used to measure the similarity between source and target language terms sahlgren and karlgrens approach models the problem as distributional similarity problem us source context target language language aaabbc xxyzzz bcc ii wxy aab iii xzz table 2: paragraph aligned corpora ing the paragraph as context.
</nextsent>
<nextsent>in table 2, the source language is limited to the words a, and and the target language to the words x, and z. three paragraphs in each of these languages are presented as pairs of translations labelled as context: aaabbc is translated as xxyzzz and labelled context i. the frequency weighted context vector for is {i:3, iii:2} and for is {i:2, ii:1, iii:1}.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3177">
<title id=" W06-1654.xml">random indexing using statistical weight functions </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>because of the size of the smallest corpora meant that high cutoff would remove to many terms for fair test, cut off of 5 was applied.
</prevsent>
<prevsent>the values = 1000 and ? = 6 were used.for our experiments in bilingual lexicon acquisition we follow sahlgren and karlgren (2005).we use the spanish-swedish and the english german portions of the europarl corpora (koehn,2005).1 these consist of 37,379 aligned paragraphs in spanish swedish and 45,556 in englishgerman.
</prevsent>
</prevsection>
<citsent citstr=" A97-1011 ">
the text was lemmatised using con nexor machin ese (tapanainen and javinen, 1997)<papid> A97-1011 </papid>2producing vocabularies of 42,671 terms of spanish, 100,891 terms of swedish, 40,181 terms of english and 70,384 terms of german.</citsent>
<aftsection>
<nextsent>we used = 600 and ? = 6 and apply frequency cut off of 100.
</nextsent>
<nextsent>the simplest method for evaluation is the direct comparison of extracted synonyms with manually created gold standard (grefenstette, 1994).
</nextsent>
<nextsent>to reduce the problem of limited coverage, our evaluation of the extraction of synonyms combines three electronic thesauri: the macquarie, rogets and moby thesauri.we follow curran (2004) and use two performance measures: direct matches (direct) and inverse rank (invr).
</nextsent>
<nextsent>direct is the number of returned synonyms found in the gold standard.invr is the sum of the inverse rank of each matching synonym, e.g. matches at ranks 3, 5 and 28 give an inverse rank score of 13 + 15 + 128 . with at most 100 matching synonyms, the maximuminvr is 5.187.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3179">
<title id=" W06-1654.xml">random indexing using statistical weight functions </title>
<section> results.  </section>
<citcontext>
<prevsection>
<prevsent>the quality of context extracted influences how weights perform individually, but random indexing using weights still outperforms not using weights.
</prevsent>
<prevsent>the relative performance of milog?
</prevsent>
</prevsection>
<citsent citstr=" P06-1046 ">
has been reduced when compared with tf-idf?, but is still greater then freq.gorman and curran (2006) <papid> P06-1046 </papid>showed random indexing to be much faster than full vector space techniques, but with 4656% reduction in accuracy compared to using jaccard and ttest?</citsent>
<aftsection>
<nextsent>using the mi?
</nextsent>
<nextsent>weight kept the improvement inspeed but with only 1018% reduction in accuracy.
</nextsent>
<nextsent>when jaccard and ttest?
</nextsent>
<nextsent>are used with our low quality contexts they perform consistently worse that random indexing.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3180">
<title id=" W06-3502.xml">backbone extraction and pruning for speeding up a deep parser for dialogue systems </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for example, in educational applications of dialogue it is important to elicit deep explanation from students and then offer focused feedback based on the details of what students say.the choice of instructional dialogue as target application influenced the choice of parser we needed to use for interpretation in dialogue system.
</prevsent>
<prevsent>several deep, wide-coverage parsers are currently available (copestake and flickinger, 2000; rose?, 2000; baldridge, 2002; maxwell and kaplan, 1994), butmany of these have not been designed with issues related to interpretation in dialogue context in mind.
</prevsent>
</prevsection>
<citsent citstr=" W05-1525 ">
the trips grammar (dzikovska et al, 2005) <papid> W05-1525 </papid>is wide-coverage unification grammar that has been used very successfully in several task-oriented dialogue systems.</citsent>
<aftsection>
<nextsent>it supports interpretation of fragments and lexical semantic features (see section 2for more detailed discussion), and provides additional robustness through robust?
</nextsent>
<nextsent>rules that cover common grammar mistakes found in dialogue such as missing articles or incorrect agreement.
</nextsent>
<nextsent>these enhancements help parsing dialogue (both spoken and typed), but they significantly increase grammar ambiguity, common concern in building grammars for robust parsing (schneider and mccoy, 1998).<papid> P98-2196 </papid></nextsent>
<nextsent>it is specifically these robustness-efficiency trade-offs that we address in this paper.much work has been done related to enhancing the efficiency of deep interpretation systems (copestake and flickinger, 2000; swift et al, 2004;<papid> C04-1055 </papid>maxwell and kaplan, 1994), which forms the foundation that we build on in this work.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3181">
<title id=" W06-3502.xml">backbone extraction and pruning for speeding up a deep parser for dialogue systems </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>it supports interpretation of fragments and lexical semantic features (see section 2for more detailed discussion), and provides additional robustness through robust?
</prevsent>
<prevsent>rules that cover common grammar mistakes found in dialogue such as missing articles or incorrect agreement.
</prevsent>
</prevsection>
<citsent citstr=" P98-2196 ">
these enhancements help parsing dialogue (both spoken and typed), but they significantly increase grammar ambiguity, common concern in building grammars for robust parsing (schneider and mccoy, 1998).<papid> P98-2196 </papid></citsent>
<aftsection>
<nextsent>it is specifically these robustness-efficiency trade-offs that we address in this paper.much work has been done related to enhancing the efficiency of deep interpretation systems (copestake and flickinger, 2000; swift et al, 2004;<papid> C04-1055 </papid>maxwell and kaplan, 1994), which forms the foundation that we build on in this work.</nextsent>
<nextsent>for example, techniques for speeding up unification in hpsg lead to dramatic improvements inefficiency (kiefer et al., 1999).<papid> P99-1061 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3182">
<title id=" W06-3502.xml">backbone extraction and pruning for speeding up a deep parser for dialogue systems </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>rules that cover common grammar mistakes found in dialogue such as missing articles or incorrect agreement.
</prevsent>
<prevsent>these enhancements help parsing dialogue (both spoken and typed), but they significantly increase grammar ambiguity, common concern in building grammars for robust parsing (schneider and mccoy, 1998).<papid> P98-2196 </papid></prevsent>
</prevsection>
<citsent citstr=" C04-1055 ">
it is specifically these robustness-efficiency trade-offs that we address in this paper.much work has been done related to enhancing the efficiency of deep interpretation systems (copestake and flickinger, 2000; swift et al, 2004;<papid> C04-1055 </papid>maxwell and kaplan, 1994), which forms the foundation that we build on in this work.</citsent>
<aftsection>
<nextsent>for example, techniques for speeding up unification in hpsg lead to dramatic improvements inefficiency (kiefer et al., 1999).<papid> P99-1061 </papid></nextsent>
<nextsent>likewise ambiguity packing and cfg backbone parsing (maxwell and kaplan, 1994; vannoord, 1997) are known to increase parsing effi ciency.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3183">
<title id=" W06-3502.xml">backbone extraction and pruning for speeding up a deep parser for dialogue systems </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these enhancements help parsing dialogue (both spoken and typed), but they significantly increase grammar ambiguity, common concern in building grammars for robust parsing (schneider and mccoy, 1998).<papid> P98-2196 </papid></prevsent>
<prevsent>it is specifically these robustness-efficiency trade-offs that we address in this paper.much work has been done related to enhancing the efficiency of deep interpretation systems (copestake and flickinger, 2000; swift et al, 2004;<papid> C04-1055 </papid>maxwell and kaplan, 1994), which forms the foundation that we build on in this work.</prevsent>
</prevsection>
<citsent citstr=" P99-1061 ">
for example, techniques for speeding up unification in hpsg lead to dramatic improvements inefficiency (kiefer et al., 1999).<papid> P99-1061 </papid></citsent>
<aftsection>
<nextsent>likewise ambiguity packing and cfg backbone parsing (maxwell and kaplan, 1994; vannoord, 1997) are known to increase parsing efficiency.
</nextsent>
<nextsent>however, as we show in this paper, these techniques depend on specific grammar properties that do not hold for all grammars.
</nextsent>
<nextsent>this claim is consistent with observations of carroll (1994) <papid> P94-1040 </papid>that parsing software optimisation techniques tend to be limited in their applicability to the individual grammars they were developed for.</nextsent>
<nextsent>while we used trips asour example unification-based grammar, this investigation is important not only for this project, but in the general context of speeding up wide-coverage unification grammar which incorporates fragment 9rules and lexical semantics, which may not be immediately provided by other available systems.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3184">
<title id=" W06-3502.xml">backbone extraction and pruning for speeding up a deep parser for dialogue systems </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>likewise ambiguity packing and cfg backbone parsing (maxwell and kaplan, 1994; vannoord, 1997) are known to increase parsing efficiency.
</prevsent>
<prevsent>however, as we show in this paper, these techniques depend on specific grammar properties that do not hold for all grammars.
</prevsent>
</prevsection>
<citsent citstr=" P94-1040 ">
this claim is consistent with observations of carroll (1994) <papid> P94-1040 </papid>that parsing software optimisation techniques tend to be limited in their applicability to the individual grammars they were developed for.</citsent>
<aftsection>
<nextsent>while we used trips asour example unification-based grammar, this investigation is important not only for this project, but in the general context of speeding up wide-coverage unification grammar which incorporates fragment 9rules and lexical semantics, which may not be immediately provided by other available systems.
</nextsent>
<nextsent>in the remainder of the paper, we begin with brief description of the trips parser and grammar,and motivate the choice of lcflex parsing algorithm to provide fast parsing foundation.
</nextsent>
<nextsent>we then discuss the backbone extraction and pruning techniques that we used, and evaluate them in comparison with the original parsing algorithm.
</nextsent>
<nextsent>we conclude with discussion of some implications for implementing grammars that build deep syntactic and semantic representations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3185">
<title id=" W06-3502.xml">backbone extraction and pruning for speeding up a deep parser for dialogue systems </title>
<section> motivation.  </section>
<citcontext>
<prevsection>
<prevsent>we conclude with discussion of some implications for implementing grammars that build deep syntactic and semantic representations.
</prevsent>
<prevsent>the work reported in this paper was done as part of the process of developing dialogue system that incorporates deep natural language understanding.we needed grammar that provides lexical semantic interpretation, supports parsing fragmentary utterance in dialogue, and could be used to start development without large quantities of corpus data.trips fulfilled our requirements better than similar alternatives, such as lingo erg (copestake and flickinger, 2000) or xle (maxwell and kaplan, 1994).trips produces logical forms which include semantic classes and roles in domain-independent frame-based formalism derived from framenet and verbnet (dzikovska et al, 2004; kipper et al, 2000).
</prevsent>
</prevsection>
<citsent citstr=" C04-1100 ">
lexical semantic features are known to be helpful in both deep (tetreault, 2005) and shallow interpretation tasks (narayanan and harabagiu,2004).<papid> C04-1100 </papid></citsent>
<aftsection>
<nextsent>apart from trips, these have not been integrated into existing deep grammars.
</nextsent>
<nextsent>while both lingo-erg and xle include semantic features related to scoping, in our applications the availability of semantic classes and semantic role assignments was more important to interpretation, and these features are not currently available from those parsers.
</nextsent>
<nextsent>finally, trips provides domain independent parse selection model, as well as rules for interpreting discourse fragments (as was also done in hpsg (schlangen and lascarides, 2003), feature actively used in interpretation.
</nextsent>
<nextsent>while trips provides the capabilities we need, its parse times for long sentences (above 15 wordslong) are intolerably long.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3188">
<title id=" W06-3502.xml">backbone extraction and pruning for speeding up a deep parser for dialogue systems </title>
<section> the trips and lcflex algorithms.  </section>
<citcontext>
<prevsection>
<prevsent>3.1 the trips parser.
</prevsent>
<prevsent>the trips parser we use as baseline is bottom up chart parser with lexical entries and rules represented as attribute-value structures.
</prevsent>
</prevsection>
<citsent citstr=" W05-1526 ">
to achieve parsing efficiency, trips uses best-first beam search algorithm based on the scores from parse selection model (dzikovska et al, 2005; <papid> W05-1525 </papid>elsner et al, 2005).<papid> W05-1526 </papid></citsent>
<aftsection>
<nextsent>the constituents on the parsers agenda are grouped into buckets based on their scores.
</nextsent>
<nextsent>at each step, the bucket with the highest scoring constituents is selected to build/extend chart edges.
</nextsent>
<nextsent>the parsing stops once requested analyses are found.
</nextsent>
<nextsent>this guarantees that the parser returns the -best list of analyses according to the parse selection model used, unless the parser reaches the chart size limit.1other enhancements used by lingo depend on disallowing disjunctive features, and relying instead on the type system.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3189">
<title id=" W06-3502.xml">backbone extraction and pruning for speeding up a deep parser for dialogue systems </title>
<section> the trips and lcflex algorithms.  </section>
<citcontext>
<prevsection>
<prevsent>this guarantees that the parser returns the -best list of analyses according to the parse selection model used, unless the parser reaches the chart size limit.1other enhancements used by lingo depend on disallowing disjunctive features, and relying instead on the type system.
</prevsent>
<prevsent>the trips grammar is untyped and uses disjunctive features,and converting it to typed system would require as yet undetermined amount of additional work.
</prevsent>
</prevsection>
<citsent citstr=" P03-1014 ">
10 in addition to best-first parsing, the trips parser uses chart size limit, to prevent the parser from running too long on unparseable utterances, similar to (frank et al, 2003).<papid> P03-1014 </papid></citsent>
<aftsection>
<nextsent>trips is much slower processing utterances not covered in the grammar, be cause it continues its search until it reaches the chart limit.
</nextsent>
<nextsent>thus, lower chart limit improves parsing efficiency.
</nextsent>
<nextsent>however, we show in our evaluation that the chart limit necessary to obtain good performance in most cases is too low to find parses for utterances with 15 or more words, even if they are covered by the grammar.
</nextsent>
<nextsent>the integration of lexical semantics in the trips lexicon has major impact on parsing in trips.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3194">
<title id=" W06-3806.xml">similarity between pairs of co indexed trees for textual entailment recognition </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>an extension is needed to consider also if two pairs show compatible relations between their sentences.in this paper, we propose to observe textual entailment pairs as pairs of syntactic trees with co-indexednodes.
</prevsent>
<prevsent>this shuold help to cosider both the structural similarity between syntactic tree pairs and the similarity between relations among sentences within pair.
</prevsent>
</prevsection>
<citsent citstr=" W05-1203 ">
then, we use this cross-pair similarity with more traditional intra-pair similarities (e.g., (corleyand mihalcea, 2005)) <papid> W05-1203 </papid>to define novel kernel function.</citsent>
<aftsection>
<nextsent>we experimented with such kernel using support vector machines on the recognizing textual entailment (rte) challenge test-beds.
</nextsent>
<nextsent>the comparative results show that (a) we have designed an effective way to automatically learn entailment rules 33from examples and (b) our approach is highly accurate and exceeds the accuracy of the current state-of the-art models.
</nextsent>
<nextsent>in the remainder of this paper, sec.
</nextsent>
<nextsent>2 introduces the cross-pair similarity and sec.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3199">
<title id=" W06-3806.xml">similarity between pairs of co indexed trees for textual entailment recognition </title>
<section> experimental investigation.  </section>
<citcontext>
<prevsection>
<prevsent>we produced also random split of d2.
</prevsent>
<prevsent>the two folds are d2(50%)?
</prevsent>
</prevsection>
<citsent citstr=" A00-2018 ">
and d2(50%)??.we also used the following resources: the charniak parser (charniak, 2000) <papid> A00-2018 </papid>to carry out the syntactic analysis; the wn::similarity package (ped ersen et al, 2004) <papid> N04-3012 </papid>to compute the jiang&conrath; (j&c;) distance (jiang and conrath, 1997) needed to implement the lexical similarity siml(t,h) as defined in (corley and mihalcea, 2005)<papid> W05-1203 </papid>; <papid> W05-1203 </papid>svm-light tk (moschitti, 2004) <papid> P04-1043 </papid>to encode the basic tree kernel function, kt , in svm-light (joachims, 1999).</citsent>
<aftsection>
<nextsent>3.3 results and analysis.
</nextsent>
<nextsent>table 1 reports the accuracy of different similarity kernels on the different training and test split described in the previous section.
</nextsent>
<nextsent>the table shows some important result.
</nextsent>
<nextsent>first, as observed in (corley and mihalcea, 2005)<papid> W05-1203 </papid>the lexical-based distance kernel kl shows an accuracy significantly higher than the random baseline, i.e. 50%.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3200">
<title id=" W06-3806.xml">similarity between pairs of co indexed trees for textual entailment recognition </title>
<section> experimental investigation.  </section>
<citcontext>
<prevsection>
<prevsent>we produced also random split of d2.
</prevsent>
<prevsent>the two folds are d2(50%)?
</prevsent>
</prevsection>
<citsent citstr=" N04-3012 ">
and d2(50%)??.we also used the following resources: the charniak parser (charniak, 2000) <papid> A00-2018 </papid>to carry out the syntactic analysis; the wn::similarity package (ped ersen et al, 2004) <papid> N04-3012 </papid>to compute the jiang&conrath; (j&c;) distance (jiang and conrath, 1997) needed to implement the lexical similarity siml(t,h) as defined in (corley and mihalcea, 2005)<papid> W05-1203 </papid>; <papid> W05-1203 </papid>svm-light tk (moschitti, 2004) <papid> P04-1043 </papid>to encode the basic tree kernel function, kt , in svm-light (joachims, 1999).</citsent>
<aftsection>
<nextsent>3.3 results and analysis.
</nextsent>
<nextsent>table 1 reports the accuracy of different similarity kernels on the different training and test split described in the previous section.
</nextsent>
<nextsent>the table shows some important result.
</nextsent>
<nextsent>first, as observed in (corley and mihalcea, 2005)<papid> W05-1203 </papid>the lexical-based distance kernel kl shows an accuracy significantly higher than the random baseline, i.e. 50%.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3203">
<title id=" W06-3806.xml">similarity between pairs of co indexed trees for textual entailment recognition </title>
<section> experimental investigation.  </section>
<citcontext>
<prevsection>
<prevsent>we produced also random split of d2.
</prevsent>
<prevsent>the two folds are d2(50%)?
</prevsent>
</prevsection>
<citsent citstr=" P04-1043 ">
and d2(50%)??.we also used the following resources: the charniak parser (charniak, 2000) <papid> A00-2018 </papid>to carry out the syntactic analysis; the wn::similarity package (ped ersen et al, 2004) <papid> N04-3012 </papid>to compute the jiang&conrath; (j&c;) distance (jiang and conrath, 1997) needed to implement the lexical similarity siml(t,h) as defined in (corley and mihalcea, 2005)<papid> W05-1203 </papid>; <papid> W05-1203 </papid>svm-light tk (moschitti, 2004) <papid> P04-1043 </papid>to encode the basic tree kernel function, kt , in svm-light (joachims, 1999).</citsent>
<aftsection>
<nextsent>3.3 results and analysis.
</nextsent>
<nextsent>table 1 reports the accuracy of different similarity kernels on the different training and test split described in the previous section.
</nextsent>
<nextsent>the table shows some important result.
</nextsent>
<nextsent>first, as observed in (corley and mihalcea, 2005)<papid> W05-1203 </papid>the lexical-based distance kernel kl shows an accuracy significantly higher than the random baseline, i.e. 50%.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3205">
<title id=" W06-1907.xml">monolingual web based factoid question answering in chinese swedish english and japanese </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>given these data sources the remaining components can be obtained automatically for each language.so, in contrast to other contemporary approaches to qa our english language system does not use wordnet as in (hovy et al, 2001; moldovan et al, 2002), ne extraction, or any other linguistic information e.g. from semantic analysis (hovy et al, 2001) or from question parsing (hovy et al, 2001; moldovan et al, 2002) anduses capitalised (where appropriate for the language) word tokens as the only features for mod eacl 2006 workshop on multilingual question answering - mlqa06 45elling.
</prevsent>
<prevsent>for our japanese system, although we currently use chasen to segment japanese character sequences into units that resemble words, we make no use of any morphological information as used for example in (fuchigami et al, 2004).
</prevsent>
</prevsection>
<citsent citstr=" W02-1033 ">
moreover, it should be noted that our approach is at the same time very different to other purely web-based approaches such as askmsr (brill etal., 2002) <papid> W02-1033 </papid>and aranea (lin et al, 2002).</citsent>
<aftsection>
<nextsent>for example, we use entire documents rather than the snippets of text returned by web search engines; we do not use structured document sources or database sand we do not transform the query in any way either by term re-ordering or by modifying the tense of verbs.
</nextsent>
<nextsent>these basic principles apply to each ofour language-specific qa systems thus simplifying and accelerating development.the approach to qa that we adopt has previously been described in (whittaker et al, 2005a; whittaker et al, 2005b; whittaker et al, 2005c) where the details of the mathematical model and how it was trained for english and japanese were given.
</nextsent>
<nextsent>our approach has also been successfully evaluated in the text retrieval conference (trec) 2005 qa track evaluation (voorhees, 2003) where.
</nextsent>
<nextsent>our group placed eleventh out of thirty participants (whittaker et al, 2005a).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3206">
<title id=" W06-2305.xml">robust parsing more with less </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we show that accuracy of analysing sentences from the negra corpus can be improved not only for sentences that do not need the extra coverage, but even when including those that do.
</prevsent>
<prevsent>traditionally, broad coverage has always been considered to be desirable property of grammar: the more linguistic phenomena are treated properly by the grammar, the better results can be expected when applying it to unrestricted text (c.f.
</prevsent>
</prevsection>
<citsent citstr=" C94-2149 ">
(grover et al, 1993; doran et al, 1994)).<papid> C94-2149 </papid></citsent>
<aftsection>
<nextsent>with the advent of empirical methods and the corresponding evaluation metrics, however, this view changed considerably.
</nextsent>
<nextsent>(abney, 1996) was among the first who noted that the relationship between cover age and statistical parsing quality is more complexone.
</nextsent>
<nextsent>adding new rules to the grammar, i.e. increasing its coverage, does not only allow the parser to deal with more phenomena, hence more sentences; at the same time it opens up new possibilities for abusing the newly introduced rules to mis-analyse constructions which were already treated properly before.
</nextsent>
<nextsent>as consequence, net reduction in parsing quality might be observed for simple statistical reasons, since the gain usually is obtained for relatively rare phenomena,while the adverse effects might well affect frequent ones.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3207">
<title id=" W06-2305.xml">robust parsing more with less </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>as consequence, net reduction in parsing quality might be observed for simple statistical reasons, since the gain usually is obtained for relatively rare phenomena,while the adverse effects might well affect frequent ones.
</prevsent>
<prevsent>(abney, 1996) uses this observation to argue in favour of stochastic models which attempt to choose the optimal structural interpretation instead of only providing list of equally probable alternatives.
</prevsent>
</prevsection>
<citsent citstr=" A00-2018 ">
however, using such an optimization procedure is not necessarily sufficient precondition to completely rule out the effect.compared to traditional handwritten grammars, successful stochastic models like (collins, 1999; charniak, 2000) <papid> A00-2018 </papid>open up an even greater space of alternatives forthe parser and accordingly offer great deal of opportunities to construct odd structural descriptions from them.</citsent>
<aftsection>
<nextsent>whether the guidance of the stochastic model can really prevent the parser from making use of these unwanted opportunities so far remains unclear.
</nextsent>
<nextsent>in the following we make first attempt to quantify the consequences that different degrees of coverage have for the output quality of wide-coverage parser.
</nextsent>
<nextsent>for this purpose we use weighted constraint dependency grammar (wcdg), which covers even relatively rare syntactic phenomena of german and performs reliably across wide variety of different text genres (foth et al., 2005).
</nextsent>
<nextsent>by combining hand-written rules with an optimization procedure for hypothesis selection, sucha parser makes it possible to successively exclude certain rare phenomena from the coverage of the grammar and to study the impact of these modifications on its output quality
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3208">
<title id=" W06-2305.xml">robust parsing more with less </title>
<section> robust behaviour under limited.  </section>
<citcontext>
<prevsection>
<prevsent>also, if we consider only those sentences on which the local search originally exceeded the time limit of 500 and therefore had to be interrupted, the accuracy rises from 85.2%/83.0% to 86.5%/84.4%, i.e. even more pronounced than overall.
</prevsent>
<prevsent>4.2 experiment 2: stepwise refinement.
</prevsent>
</prevsection>
<citsent citstr=" P03-1013 ">
for comparison with previous work and to investigate corpus-specific effects, we repeated the experiment with the test set of the negra corpus as defined by (dubey and keller, 2003).<papid> P03-1013 </papid></citsent>
<aftsection>
<nextsent>for that purpose the negra annotations were automatically transformed to dependency trees with the freely available tool depsy (daum et al, 2004).
</nextsent>
<nextsent>some manual corrections we remade to its output to conform to the annotation guidelines of the wcdg of german; altogether, 1% of all words had their regents changed for this purpose.
</nextsent>
<nextsent>table 3 shows that the proportion of sentences with rare phenomena is somewhat higher in the negra sentences, and consequently the net gain in parsing accuracy is smaller; apparently the advantage of reducing the problem size is almost cancelled by the disadvantage of losing necessary coverage.
</nextsent>
<nextsent>to test this theory, we then reduced the coverage of the grammar in smaller steps.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3209">
<title id=" W06-2305.xml">robust parsing more with less </title>
<section> parenthetical matrix clause 13.3 23.0.  </section>
<citcontext>
<prevsection>
<prevsent>disregarding rare phenomena is something that can be achieved in stochastic framework by putting threshold on the minimum number of occurrences to be considered.
</prevsent>
<prevsent>such an approach is mainly used to either exclude rare phenomena in grammar induction (c.f.
</prevsent>
</prevsection>
<citsent citstr=" W97-0302 ">
(solsona et al, 2002)) or to prune the search space by adjusting beam width during parsing itself (goodman, 1997).<papid> W97-0302 </papid></citsent>
<aftsection>
<nextsent>the direct use of thresholding techniques at the level of the stochastic model, however, has not been investigated extensively so far.
</nextsent>
<nextsent>stochastic models of syntax suffer to such degree from data sparseness that in effect strong efforts in the opposite direction becomenecessary: instead of ignoring rare events in the training data, even unseen events are included by smoothing techniques.
</nextsent>
<nextsent>the only experimental investigation of the impact of rare events we are aware of is (bod, 2003), where heuristics are explored to constrain the modelin the dop framework by ignoring certain tree fragments.
</nextsent>
<nextsent>contrary to the results of our experiments, very few constraints have been found that do not decrease the parse accuracy.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3210">
<title id=" W07-0203.xml">extractive automatic summarization does more linguistic knowledge make a difference </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for extractive automatic summarization (as), several methods have been suggested that are based upon statistics or data readily available in the source text.
</prevsent>
<prevsent>word frequency (luhn, 1958) and sentence position (edmundson, 1969) methods are classic examples of that.
</prevsent>
</prevsection>
<citsent citstr=" P05-3013 ">
usually, extractive as does not take into account linguistic and semantic knowledge in order to be portable to distinct domains or languages (mihalcea, 2005).<papid> P05-3013 </papid></citsent>
<aftsection>
<nextsent>graph-based methods aim at the same and have been gaining lot of interest because they usually do not relyon any linguistic resource and run pretty fast.
</nextsent>
<nextsent>exemplars of those are lexrank (erkan and radev, 2004) and text rank (mihalcea and tarau, 2004).
</nextsent>
<nextsent>inspite of their potentialities, we claim that there is compromise in pursuing language-free setting: how ever portable system may be, it may also produce extracts that lack the degree of informative ness needed for use.
</nextsent>
<nextsent>informative ness, in the current context, refers to the ability of an automatic sum marizer to produce summaries that convey most information of reference, or ideal, summaries.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3211">
<title id=" W07-0203.xml">extractive automatic summarization does more linguistic knowledge make a difference </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>supor-2 is the only machine learning-based system amongst the four ones, and it was built to summarize texts in brazilian portuguese, although it may be customized to other languages.
</prevsent>
<prevsent>unlike the others, it embeds more sophisticated decision features that relyon varied linguistic resources.
</prevsent>
</prevsection>
<citsent citstr=" W97-0703 ">
some of them correspond to full summarization methods by themselves: lexical chaining (barzilay and elhadad, 1997), <papid> W97-0703 </papid>relationship mapping (salton et al., 1997), and importance of topics (larocca neto et al, 2000).</citsent>
<aftsection>
<nextsent>this is its unique and distinguishing characteristic.
</nextsent>
<nextsent>in what follows we first review the different levels of processing in extractive as (section 2), then we describe text rank and its implementation to summarize brazilian portuguese texts (section 3).
</nextsent>
<nextsent>our suggested modifications of text rank are presented in section 4, whilst supor-2 is described in section 5.
</nextsent>
<nextsent>finally, we compare the results of the four automatic summarizers when running on brazilian portuguese texts (section 6), and make some remarks on linguistic independence for extractive as in section 7.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3224">
<title id=" W06-2406.xml">collocation extraction needs feeds and results of an extraction system for german </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>element (the col locate, which may lose (some of) its meaning in collocation) (adapted from (hausmann, 1979; hausmann, 1989; hausmann, 2003)).
</prevsent>
<prevsent>b. collocation is word combination whose semantic and/or syntactic properties cannot be fully predicted from those of its components, and which therefore has to be listed in lexicon (evert, 2004).
</prevsent>
</prevsection>
<citsent citstr=" J93-1007 ">
we argue that linguistic knowledge could not only improve results (krenn, 2000b; smadja,1993) <papid> J93-1007 </papid>but is essential when extracting collocations from certain languages: this knowledge provides other applications (or lexicon user, respec tively) with fine-grained description of how the extracted collocations are to be used in context.</citsent>
<aftsection>
<nextsent>additional requirements resulting from the needs of dictionary users are described in (haus mann, 2003; heid and gouws, 2005) and are of interest not only in lexicography but can also be transferred to the field of natural language generation.
</nextsent>
<nextsent>these requirements influence the development of collocation extraction systems, which motivates this paper.the structure of the paper is as follows: in chapter 2, the requirements, depending on factors likethe targeted language, are presented.
</nextsent>
<nextsent>we then discuss and suggest methods to meet the given needs.a documentation of ongoing work on the extraction of noun + verb collocations from german texts is given in chapter 3.
</nextsent>
<nextsent>chapter 4 gives conclusion and an outlook on work still to be done.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3226">
<title id=" W07-0706.xml">a dependency treelet string correspondence model for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we argue that the dtscmodel proposed here is capable of lexicalization, generalization, and handling discontinuous phrases which are very desirable for machine translation.
</prevsent>
<prevsent>we finally evaluate our current implementation of simplified version of dtsc for statistical machine translation.
</prevsent>
</prevsection>
<citsent citstr=" C04-1090 ">
over the last several years, various statistical syntax based models were proposed to extend traditional word/phrase based models in statistical machine translation (smt) (lin, 2004; <papid> C04-1090 </papid>chiang, 2005; <papid> P05-1033 </papid>ding et al, 2005; quirk et al, 2005; <papid> P05-1034 </papid>marcu et al, 2006; <papid> W06-1606 </papid>liu et al, 2006).<papid> P06-1077 </papid></citsent>
<aftsection>
<nextsent>it is believed that these model scan improve the quality of smt significantly.
</nextsent>
<nextsent>compared with phrase-based models, syntax-based models lead to better reordering and higher flexibility by introducing hierarchical structures and variables which make syntax-based models capable of hierarchical reordering and generalization.
</nextsent>
<nextsent>due to these advantages, syntax-based approaches are becoming an active area of research in machine translation.
</nextsent>
<nextsent>in this paper, we propose novel model based on dependency structures: dependency treelet string correspondence model (dtsc).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3227">
<title id=" W07-0706.xml">a dependency treelet string correspondence model for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we argue that the dtscmodel proposed here is capable of lexicalization, generalization, and handling discontinuous phrases which are very desirable for machine translation.
</prevsent>
<prevsent>we finally evaluate our current implementation of simplified version of dtsc for statistical machine translation.
</prevsent>
</prevsection>
<citsent citstr=" P05-1033 ">
over the last several years, various statistical syntax based models were proposed to extend traditional word/phrase based models in statistical machine translation (smt) (lin, 2004; <papid> C04-1090 </papid>chiang, 2005; <papid> P05-1033 </papid>ding et al, 2005; quirk et al, 2005; <papid> P05-1034 </papid>marcu et al, 2006; <papid> W06-1606 </papid>liu et al, 2006).<papid> P06-1077 </papid></citsent>
<aftsection>
<nextsent>it is believed that these model scan improve the quality of smt significantly.
</nextsent>
<nextsent>compared with phrase-based models, syntax-based models lead to better reordering and higher flexibility by introducing hierarchical structures and variables which make syntax-based models capable of hierarchical reordering and generalization.
</nextsent>
<nextsent>due to these advantages, syntax-based approaches are becoming an active area of research in machine translation.
</nextsent>
<nextsent>in this paper, we propose novel model based on dependency structures: dependency treelet string correspondence model (dtsc).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3228">
<title id=" W07-0706.xml">a dependency treelet string correspondence model for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we argue that the dtscmodel proposed here is capable of lexicalization, generalization, and handling discontinuous phrases which are very desirable for machine translation.
</prevsent>
<prevsent>we finally evaluate our current implementation of simplified version of dtsc for statistical machine translation.
</prevsent>
</prevsection>
<citsent citstr=" P05-1034 ">
over the last several years, various statistical syntax based models were proposed to extend traditional word/phrase based models in statistical machine translation (smt) (lin, 2004; <papid> C04-1090 </papid>chiang, 2005; <papid> P05-1033 </papid>ding et al, 2005; quirk et al, 2005; <papid> P05-1034 </papid>marcu et al, 2006; <papid> W06-1606 </papid>liu et al, 2006).<papid> P06-1077 </papid></citsent>
<aftsection>
<nextsent>it is believed that these model scan improve the quality of smt significantly.
</nextsent>
<nextsent>compared with phrase-based models, syntax-based models lead to better reordering and higher flexibility by introducing hierarchical structures and variables which make syntax-based models capable of hierarchical reordering and generalization.
</nextsent>
<nextsent>due to these advantages, syntax-based approaches are becoming an active area of research in machine translation.
</nextsent>
<nextsent>in this paper, we propose novel model based on dependency structures: dependency treelet string correspondence model (dtsc).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3229">
<title id=" W07-0706.xml">a dependency treelet string correspondence model for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we argue that the dtscmodel proposed here is capable of lexicalization, generalization, and handling discontinuous phrases which are very desirable for machine translation.
</prevsent>
<prevsent>we finally evaluate our current implementation of simplified version of dtsc for statistical machine translation.
</prevsent>
</prevsection>
<citsent citstr=" W06-1606 ">
over the last several years, various statistical syntax based models were proposed to extend traditional word/phrase based models in statistical machine translation (smt) (lin, 2004; <papid> C04-1090 </papid>chiang, 2005; <papid> P05-1033 </papid>ding et al, 2005; quirk et al, 2005; <papid> P05-1034 </papid>marcu et al, 2006; <papid> W06-1606 </papid>liu et al, 2006).<papid> P06-1077 </papid></citsent>
<aftsection>
<nextsent>it is believed that these model scan improve the quality of smt significantly.
</nextsent>
<nextsent>compared with phrase-based models, syntax-based models lead to better reordering and higher flexibility by introducing hierarchical structures and variables which make syntax-based models capable of hierarchical reordering and generalization.
</nextsent>
<nextsent>due to these advantages, syntax-based approaches are becoming an active area of research in machine translation.
</nextsent>
<nextsent>in this paper, we propose novel model based on dependency structures: dependency treelet string correspondence model (dtsc).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3232">
<title id=" W07-0706.xml">a dependency treelet string correspondence model for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we argue that the dtscmodel proposed here is capable of lexicalization, generalization, and handling discontinuous phrases which are very desirable for machine translation.
</prevsent>
<prevsent>we finally evaluate our current implementation of simplified version of dtsc for statistical machine translation.
</prevsent>
</prevsection>
<citsent citstr=" P06-1077 ">
over the last several years, various statistical syntax based models were proposed to extend traditional word/phrase based models in statistical machine translation (smt) (lin, 2004; <papid> C04-1090 </papid>chiang, 2005; <papid> P05-1033 </papid>ding et al, 2005; quirk et al, 2005; <papid> P05-1034 </papid>marcu et al, 2006; <papid> W06-1606 </papid>liu et al, 2006).<papid> P06-1077 </papid></citsent>
<aftsection>
<nextsent>it is believed that these model scan improve the quality of smt significantly.
</nextsent>
<nextsent>compared with phrase-based models, syntax-based models lead to better reordering and higher flexibility by introducing hierarchical structures and variables which make syntax-based models capable of hierarchical reordering and generalization.
</nextsent>
<nextsent>due to these advantages, syntax-based approaches are becoming an active area of research in machine translation.
</nextsent>
<nextsent>in this paper, we propose novel model based on dependency structures: dependency treelet string correspondence model (dtsc).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3249">
<title id=" W07-0706.xml">a dependency treelet string correspondence model for statistical machine translation </title>
<section> the word penalty wp..  </section>
<citcontext>
<prevsection>
<prevsent>in pi do substitute the corresponding wild card on the target side with translations from the stack of n?
</prevsent>
<prevsent>end for for each uncovered node n@ by pi do attach the translations from the stack of n@ to the target side at the attaching point end for end for end for figure 4: chart-style decoding algorithm for the dtsc model.
</prevsent>
</prevsection>
<citsent citstr=" P04-1083 ">
melamed (2004) <papid> P04-1083 </papid>also used similar way to integrate the language model.</citsent>
<aftsection>
<nextsent>5 decoding.
</nextsent>
<nextsent>our decoding algorithm is similar to the bottom-up chart parsing.
</nextsent>
<nextsent>the distinction is that the input is atree rather than string and therefore the chart is indexed by nodes of the tree rather than spans of thestring.
</nextsent>
<nextsent>also, several other tree-based decoding algorithms introduced by eisner (2003), <papid> P03-2041 </papid>quirk et al (2005) <papid> P05-1034 </papid>and liu et al (2006) <papid> P06-1077 </papid>can be classified as the chart-style parsing algorithm too.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3250">
<title id=" W07-0706.xml">a dependency treelet string correspondence model for statistical machine translation </title>
<section> the word penalty wp..  </section>
<citcontext>
<prevsection>
<prevsent>our decoding algorithm is similar to the bottom-up chart parsing.
</prevsent>
<prevsent>the distinction is that the input is atree rather than string and therefore the chart is indexed by nodes of the tree rather than spans of thestring.
</prevsent>
</prevsection>
<citsent citstr=" P03-2041 ">
also, several other tree-based decoding algorithms introduced by eisner (2003), <papid> P03-2041 </papid>quirk et al (2005) <papid> P05-1034 </papid>and liu et al (2006) <papid> P06-1077 </papid>can be classified as the chart-style parsing algorithm too.</citsent>
<aftsection>
<nextsent>our decoding algorithm is shown in figure 4.
</nextsent>
<nextsent>given an input dependency tree, firstly we generate the bottom-up order by post order transversal.
</nextsent>
<nextsent>this order guarantees that any sub nodes of node have been translated before node is done.
</nextsent>
<nextsent>for each node in the bottom-up order, all matched dtscs rooted at are found, and stack is also built for it to store the candidate translations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3260">
<title id=" W07-0706.xml">a dependency treelet string correspondence model for statistical machine translation </title>
<section> current implementation.  </section>
<citcontext>
<prevsection>
<prevsent>we then converted the phrasal structure trees into dependency trees using the way introduced by xia (1999).
</prevsent>
<prevsent>to obtain the word alignments, we use the way of koehn et al (2005).
</prevsent>
</prevsection>
<citsent citstr=" P00-1056 ">
after running giza++ (och and ney, 2000) <papid> P00-1056 </papid>in both directions, we apply the grow-diag-final?</citsent>
<aftsection>
<nextsent>refinement rule on the intersection alignments for each sentence pair.
</nextsent>
<nextsent>the training corpus consists of 31, 149 sentence pairs with 823k chinese words and 927k englishwords.
</nextsent>
<nextsent>for the language model, we used sri language modeling toolkit (stolcke, 2002) to train atrigram model with modified kneser-ney smooth systems bleu-4 pb 20.88 ? 0.87 dtsc 20.20 ? 0.81 dtsc + phrases 21.46 ? 0.83 table 2: bleu-4 scores for our system and phrase-based system.
</nextsent>
<nextsent>ing on the 31, 149 english sentences.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3261">
<title id=" W07-0706.xml">a dependency treelet string correspondence model for statistical machine translation </title>
<section> current implementation.  </section>
<citcontext>
<prevsection>
<prevsent>for the language model, we used sri language modeling toolkit (stolcke, 2002) to train atrigram model with modified kneser-ney smooth systems bleu-4 pb 20.88 ? 0.87 dtsc 20.20 ? 0.81 dtsc + phrases 21.46 ? 0.83 table 2: bleu-4 scores for our system and phrase-based system.
</prevsent>
<prevsent>ing on the 31, 149 english sentences.
</prevsent>
</prevsection>
<citsent citstr=" P03-1021 ">
we selected 580 short sentences of length at most 50 characters from the 2002 nist mt evaluation test set as our development corpus and used it to tune by maximizing the bleu score (och, 2003), <papid> P03-1021 </papid>and used the 2005 nist mt evaluation test set as our test corpus..</citsent>
<aftsection>
<nextsent>from the training corpus, we learned 2, 729,964 distinct dtscs with the configuration { ary limit = 4, depth-limit = 4, len-limit = 15, gap-limit = 2, comb-limit = 20 }.
</nextsent>
<nextsent>among them, 160,694dtscs are used for the test set.
</nextsent>
<nextsent>to run our decoder on the development and test set, we set stack thrshold = 0.0001, stack-limit = 100, node-limit = 100, operation-limit = 20.
</nextsent>
<nextsent>we also ran phrase-based system (pb) with distortion reordering model (xiong et al, 2006) <papid> P06-1066 </papid>on the same corpus.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3262">
<title id=" W07-0706.xml">a dependency treelet string correspondence model for statistical machine translation </title>
<section> current implementation.  </section>
<citcontext>
<prevsection>
<prevsent>among them, 160,694dtscs are used for the test set.
</prevsent>
<prevsent>to run our decoder on the development and test set, we set stack thrshold = 0.0001, stack-limit = 100, node-limit = 100, operation-limit = 20.
</prevsent>
</prevsection>
<citsent citstr=" P06-1066 ">
we also ran phrase-based system (pb) with distortion reordering model (xiong et al, 2006) <papid> P06-1066 </papid>on the same corpus.</citsent>
<aftsection>
<nextsent>the results are shown in table 2.for all bleu scores, we also show the 95% confidence intervals computed using zhangs significant tester (zhang et al, 2004) which was modified to conform to nists definition of the bleu brevity penalty.
</nextsent>
<nextsent>the bleu score of our current system withthe dtsc model is lower than that of the phrase based system.
</nextsent>
<nextsent>however, with phrases integrated, the performance is improved greatly, and the new bleu score is higher than that of the phrase-based smt.
</nextsent>
<nextsent>this difference is significant according to zhangs tester.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3277">
<title id=" W07-0713.xml">human evaluation of machine translation through binary system comparisons </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>evaluation of machine translation (mt) output is adifficult and still open problem.
</prevsent>
<prevsent>as in other natural language processing tasks, automatic measures which try to asses the quality of the translation can be computed.
</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
the most widely known are the word error rate (wer), the position independent word error rate (per), the nist score (dodding ton, 2002) and, especially in recent years, the bleu score (papineni et al, 2002) <papid> P02-1040 </papid>and the translation error rate (ter) (snover et al, 2005).</citsent>
<aftsection>
<nextsent>all of theses measures compare the system output with oneor more gold standard references and produce numerical value (score or error rate) which measures the similarity between the machine translation and ahuman produced one.
</nextsent>
<nextsent>once such reference translations are available, the evaluation can be carried out in quick, efficient and reproducible manner.however, automatic measures also have big dis advantages; (callison-burch et al, 2006) describes some of them.
</nextsent>
<nextsent>a major problem is that given sentence in one language can have several correct translations in another language and thus, the measure of similarity with one or even small amount of reference translations will never be flexible enough to truly reflect the wide range of correct possibilities ofa translation.
</nextsent>
<nextsent>1 this holds in particular for long sentences and wide- or open-domain tasks like the ones dealt with in current mt projects and evaluations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3278">
<title id=" W07-0713.xml">human evaluation of machine translation through binary system comparisons </title>
<section> state-of-the-art.  </section>
<citcontext>
<prevsection>
<prevsent>large amounts of sentences must therefore be evaluated and procedures like evaluation normalization must be carried out before significant conclusions from the evaluation can be drawn.
</prevsent>
<prevsent>another important drawback,which is also one of the causes of the aforementioned problems, is that it is very difficult to define the meaning of the numerical scores precisely.
</prevsent>
</prevsection>
<citsent citstr=" W06-3114 ">
even if human judges have explicit evaluation guideline sat hand, they still find it difficult to assign numerical value which represents the quality of the translation for many sentences (koehn and monz, 2006).<papid> W06-3114 </papid>in this paper we present an alternative to this evaluation scheme.</citsent>
<aftsection>
<nextsent>our method starts from the observation that normally the final objective of human evaluation is to find ranking?
</nextsent>
<nextsent>of different systems, and the absolute score for each system is not relevant(and it can even not be comparable between different evaluations).
</nextsent>
<nextsent>we focus on method that aims to simplify the task of the judges and allows to rank the systems according to their translation quality.
</nextsent>
<nextsent>the main idea of our method relies in the fact that human evaluator, when presented two different translations of the same sentence, can normally choose the best one out of them in more or less 2with the exception of cross-language information retrieval and similar tasks.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3279">
<title id=" W07-0713.xml">human evaluation of machine translation through binary system comparisons </title>
<section> binary system comparisons.  </section>
<citcontext>
<prevsection>
<prevsent>1 (m ? 1)(l ? 1) m?
</prevsent>
<prevsent>i=1 ( di li ? )2 ? li (9) with this equation, monte-carlo-estimates are no longer necessary for examining the significance ofwer, per, ter, etc. unfortunately, we do not expect such short explicit formula to exist for the standard bleu score.
</prevsent>
</prevsection>
<citsent citstr=" P03-1021 ">
still, confidence range for bleu can be estimated by bootstrapping (och, 2003; <papid> P03-1021 </papid>zhang and vogel, 2004).</citsent>
<aftsection>
<nextsent>spanish english train sentences 1.2m words 32m 31m vocabulary 159k 111k singletons 63k 46k test sentences 1 117 words 26k oov words 72 table 1: statistics of the epps corpus.
</nextsent>
<nextsent>the evaluation procedure was carried out on the data generated in the second evaluation campaign of the tc-star project4.
</nextsent>
<nextsent>the goal of this project is to build speech-to-speech translation system that can deal with real life data.
</nextsent>
<nextsent>three translation directions are dealt with in the project: spanish to english, english to spanish and chinese to english.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3280">
<title id=" W07-0713.xml">human evaluation of machine translation through binary system comparisons </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>however,the suppression of explicit numerical scores eliminates an additional bias of evaluators.
</prevsent>
<prevsent>it has been observed that human judges often give scores within 6note however that possible evaluator biases can have great influence in these statistics.
</prevsent>
</prevsection>
<citsent citstr=" W05-0903 ">
102a certain range (e.g. in the mid-range or only extreme values), which constitute an additional difficulty when carrying out the evaluation (leusch et al., 2005).<papid> W05-0903 </papid></citsent>
<aftsection>
<nextsent>our method suppresses this kind of bias.another advantage of our method is the possibility of assessing improvements within one system.with one evaluation we can decide if some modifications actually improve performance.
</nextsent>
<nextsent>this evaluation even gives us confidence interval to weight the significance of an improvement.
</nextsent>
<nextsent>carrying out full adequacy-fluency analysis would require lot more effort, without giving more useful results.
</nextsent>
<nextsent>we presented novel human evaluation technique that simplifies the task of the evaluators.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3281">
<title id=" W07-0606.xml">a cognitive model for the representation and acquisition of verb selectional preferences </title>
<section> related computational models.  </section>
<citcontext>
<prevsection>
<prevsent>the selectional association of verb with class is also defined as the contribution of that class to the total selectional preference strength.
</prevsent>
<prevsent>resnik estimates the prior and posterior probabilities based on the frequencies of each verb and its relevant argument in corpus.
</prevsent>
</prevsection>
<citsent citstr=" J98-2002 ">
li and abe (1998) <papid> J98-2002 </papid>model selectional preferences of verb (for an argument position) as set of nodes in the semantic class hierarchy with probability distribution over them.</citsent>
<aftsection>
<nextsent>they use the minimum description length (mdl) principle to find the best set for each verb and argument based on the usages of that verb in the training data.
</nextsent>
<nextsent>clark and weir (2002)<papid> J02-2003 </papid>also find an appropriate set of concept nodes to represent the selectional preferences for verb, but do so using 2 test over corpus frequencies mapped to concepts to determine when to generalize from node to its parent.</nextsent>
<nextsent>ciaramita and johnson (2000) <papid> C00-1028 </papid>use bayesian network with the same topology as wordnet to estimate the probability distribution of the relevant set of nodes in the hierarchy.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3282">
<title id=" W07-0606.xml">a cognitive model for the representation and acquisition of verb selectional preferences </title>
<section> related computational models.  </section>
<citcontext>
<prevsection>
<prevsent>li and abe (1998) <papid> J98-2002 </papid>model selectional preferences of verb (for an argument position) as set of nodes in the semantic class hierarchy with probability distribution over them.</prevsent>
<prevsent>they use the minimum description length (mdl) principle to find the best set for each verb and argument based on the usages of that verb in the training data.</prevsent>
</prevsection>
<citsent citstr=" J02-2003 ">
clark and weir (2002)<papid> J02-2003 </papid>also find an appropriate set of concept nodes to represent the selectional preferences for verb, but do so using 2 test over corpus frequencies mapped to concepts to determine when to generalize from node to its parent.</citsent>
<aftsection>
<nextsent>ciaramita and johnson (2000) <papid> C00-1028 </papid>use bayesian network with the same topology as wordnet to estimate the probability distribution of the relevant set of nodes in the hierarchy.</nextsent>
<nextsent>abney 42and light (1999) use different representational ap proach: they train separate hidden markov model for each verb, and the selectional preference is represented as probability distribution over words instead of semantic classes.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3283">
<title id=" W07-0606.xml">a cognitive model for the representation and acquisition of verb selectional preferences </title>
<section> related computational models.  </section>
<citcontext>
<prevsection>
<prevsent>they use the minimum description length (mdl) principle to find the best set for each verb and argument based on the usages of that verb in the training data.
</prevsent>
<prevsent>clark and weir (2002)<papid> J02-2003 </papid>also find an appropriate set of concept nodes to represent the selectional preferences for verb, but do so using 2 test over corpus frequencies mapped to concepts to determine when to generalize from node to its parent.</prevsent>
</prevsection>
<citsent citstr=" C00-1028 ">
ciaramita and johnson (2000) <papid> C00-1028 </papid>use bayesian network with the same topology as wordnet to estimate the probability distribution of the relevant set of nodes in the hierarchy.</citsent>
<aftsection>
<nextsent>abney 42and light (1999) use different representational ap proach: they train separate hidden markov model for each verb, and the selectional preference is represented as probability distribution over words instead of semantic classes.
</nextsent>
<nextsent>3.1 overview of the model.
</nextsent>
<nextsent>our model learns the set of argument structure frames for each verb, and their grouping across verbs into constructions.
</nextsent>
<nextsent>an argument structure frame isa set of features of verb usage that are both syntactic (the number of arguments, the syntactic pattern of the usage) and semantic (the semantic properties of the verb, the semantic properties of each argument).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3284">
<title id=" W07-0606.xml">a cognitive model for the representation and acquisition of verb selectional preferences </title>
<section> experimental results.  </section>
<citcontext>
<prevsection>
<prevsent>to evaluate the new version of our model for the task of learning selectional preferences, we need wide selection of verbs and their arguments that is impractical to compile by hand.
</prevsent>
<prevsent>the training data for our experiments here are generated as follows.
</prevsent>
</prevsection>
<citsent citstr=" E03-1034 ">
we use 20,000 sentences randomly selected from the british national corpus (bnc),4 automatically parsed using the collins parser (collins, 1999), and further processed with tgrep2,5 and an np-head extraction software.6 for 2to our knowledge, the only work that considers selectional preferences of subjects and prepositional phrases as well as direct objects is brockmann and lapata (2003).<papid> E03-1034 </papid></citsent>
<aftsection>
<nextsent>3computational models of verb selectional preference have been evaluated through disambiguation tasks (li and abe, 1998; <papid> J98-2002 </papid>abney and light, 1999; <papid> W99-0901 </papid>ciaramita and johnson, 2000; <papid> C00-1028 </papid>clark and weir, 2002), <papid> J02-2003 </papid>but for to evaluate our cognitive model, the experiments from resnik (1996) are the most interesting.</nextsent>
<nextsent>4http://www.natcorp.ox.ac.uk 5http://tedlab.mit.edu/dr/tgrep26the software was provided to us by eric joanis, and af each verb usage in sentence, we construct frame by recording the verb in root form, the number of the arguments for that verb, and the syntactic pattern of the verb usage (i.e., the word order of the verb and the arguments).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3286">
<title id=" W07-0606.xml">a cognitive model for the representation and acquisition of verb selectional preferences </title>
<section> experimental results.  </section>
<citcontext>
<prevsection>
<prevsent>the training data for our experiments here are generated as follows.
</prevsent>
<prevsent>we use 20,000 sentences randomly selected from the british national corpus (bnc),4 automatically parsed using the collins parser (collins, 1999), and further processed with tgrep2,5 and an np-head extraction software.6 for 2to our knowledge, the only work that considers selectional preferences of subjects and prepositional phrases as well as direct objects is brockmann and lapata (2003).<papid> E03-1034 </papid></prevsent>
</prevsection>
<citsent citstr=" W99-0901 ">
3computational models of verb selectional preference have been evaluated through disambiguation tasks (li and abe, 1998; <papid> J98-2002 </papid>abney and light, 1999; <papid> W99-0901 </papid>ciaramita and johnson, 2000; <papid> C00-1028 </papid>clark and weir, 2002), <papid> J02-2003 </papid>but for to evaluate our cognitive model, the experiments from resnik (1996) are the most interesting.</citsent>
<aftsection>
<nextsent>4http://www.natcorp.ox.ac.uk 5http://tedlab.mit.edu/dr/tgrep26the software was provided to us by eric joanis, and af each verb usage in sentence, we construct frame by recording the verb in root form, the number of the arguments for that verb, and the syntactic pattern of the verb usage (i.e., the word order of the verb and the arguments).
</nextsent>
<nextsent>we also record in the frame the semantic properties of the verb and each of the argument heads (each noun is also converted to root form); these properties are extracted from wordnet(as discussed in section 3.1 and illustrated in figure 1).
</nextsent>
<nextsent>this process results in 16,300 frames which serve as input data to our learning model.
</nextsent>
<nextsent>4.2 formation of semantic profiles for verbs.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3289">
<title id=" W06-3304.xml">integrating onto logical knowledge and textual evidence in estimating gene and gene product similarity </title>
<section> textual evidence selection.  </section>
<citcontext>
<prevsection>
<prevsent>several approaches can be used to carry out this prerequisite.
</prevsent>
<prevsent>for example, one possibility is to collect documents relevant to the task at hand, e.g. through pubmed queries, and use feature weighting and selection techniques from the information retrieval literaturee.g. tf*idf (buckley 1985) and information gain (e.g. yang and pedersen 1997)to distill the most relevant information.
</prevsent>
</prevsection>
<citsent citstr=" W02-0312 ">
an other possibility is to use information extraction algorithms tailored to the biomedical domain such as meds tract (http://www.medstract.org, pustejovsky et al 2002) <papid> W02-0312 </papid>to extract entity-relationship structures of relevance.</citsent>
<aftsection>
<nextsent>yet another possibility is to use specialized tools such as gopubmed (doms and schroeder 2005) where traditional keyword based capabilities are coupled with term extraction and onto logical annotation techniques.
</nextsent>
<nextsent>in our study, we opted for the latter solution, using generic information retrieval techniques to normalize and weigh the textual evidence extracted.
</nextsent>
<nextsent>the main advantage of this choice is that tools such as gopubmed provide very high quality term extraction at no cost.
</nextsent>
<nextsent>less appealing is the fact that the textual evidence provided is go-based and therefore does not offer information which is orthogonal to the gene ontology.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3291">
<title id=" W06-1616.xml">incremental integer linear programming for non projective dependency parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>many inference algorithms require models tomake strong assumptions of conditional independence between variables.
</prevsent>
<prevsent>for example, the viterbialgorithm used for decoding in conditional random fields requires the model to be markovian.
</prevsent>
</prevsection>
<citsent citstr=" P05-1012 ">
strong assumptions are also made in the case of mcdonald et al (2005<papid> P05-1012 </papid>b) non-projective dependency parsing model.</citsent>
<aftsection>
<nextsent>here attachment decisions are made independently of one another1.
</nextsent>
<nextsent>however, often such assumptions can not be justified.
</nextsent>
<nextsent>for example in dependency parsing, if subject has already been identified forgiven verb, then the probability of attaching second subject to theverb is zero.
</nextsent>
<nextsent>similarly, if we find that one coordination argument is noun, then the other argu 1if we ignore the constraint that dependency trees must be cycle-free (see sections 2 and 3 for details).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3295">
<title id=" W06-1616.xml">incremental integer linear programming for non projective dependency parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>ment cannot be verb.
</prevsent>
<prevsent>thus decisions are often co-dependent.
</prevsent>
</prevsection>
<citsent citstr=" W04-2401 ">
integer linear programming (ilp) has recently been applied to inference in sequential conditional random fields (roth and yih, 2004), <papid> W04-2401 </papid>this has allowed the use of truly global constraints during inference.</citsent>
<aftsection>
<nextsent>however, it is not possible to use this approach directly for complex task likenon-projective dependency parsing due to the exponential number of constraints required to prevent cycles occurring in the dependency graph.to model all these constraints explicitly would result in an ilp formulation too large to solve efficiently (williams, 2002).
</nextsent>
<nextsent>a similar problem also occurs in an ilp formulation for machine translation which treats decoding as the travelling salesman problem (germann et al , 2001).<papid> P01-1030 </papid></nextsent>
<nextsent>in this paper we present method which extends the applicability of ilp to more complex set of problems.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3296">
<title id=" W06-1616.xml">incremental integer linear programming for non projective dependency parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>integer linear programming (ilp) has recently been applied to inference in sequential conditional random fields (roth and yih, 2004), <papid> W04-2401 </papid>this has allowed the use of truly global constraints during inference.</prevsent>
<prevsent>however, it is not possible to use this approach directly for complex task likenon-projective dependency parsing due to the exponential number of constraints required to prevent cycles occurring in the dependency graph.to model all these constraints explicitly would result in an ilp formulation too large to solve efficiently (williams, 2002).</prevsent>
</prevsection>
<citsent citstr=" P01-1030 ">
a similar problem also occurs in an ilp formulation for machine translation which treats decoding as the travelling salesman problem (germann et al , 2001).<papid> P01-1030 </papid></citsent>
<aftsection>
<nextsent>in this paper we present method which extends the applicability of ilp to more complex set of problems.
</nextsent>
<nextsent>instead of adding all the constraints we wish to capture to the formulation, we first solve the program with fraction of the constraints.
</nextsent>
<nextsent>the solution is then examined and, if required, additional constraints are added.
</nextsent>
<nextsent>this procedure is repeated until all constraints are satisfied.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3309">
<title id=" W06-1616.xml">incremental integer linear programming for non projective dependency parsing </title>
<section> dependency parsing.  </section>
<citcontext>
<prevsection>
<prevsent>ik??
</prevsent>
<prevsent>en?.
</prevsent>
</prevsection>
<citsent citstr=" W04-2407 ">
for more formal definition see previous work (nivre et al , 2004).<papid> W04-2407 </papid></citsent>
<aftsection>
<nextsent>an important distinction between dependency trees is whether they are projective or nonprojective.
</nextsent>
<nextsent>figure 1 is an example of projective dependency tree, in such trees dependencies do not cross.
</nextsent>
<nextsent>in dutch and other flexible word order languages such as german and czech we also encounter non-projective trees, in these cases the trees contain crossing dependencies.
</nextsent>
<nextsent>dependency parsing is useful for applications such as relation extraction (culotta and sorensen, 2004) <papid> P04-1054 </papid>and machine translation (ding and palmer, 2005).<papid> P05-1067 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3310">
<title id=" W06-1616.xml">incremental integer linear programming for non projective dependency parsing </title>
<section> dependency parsing.  </section>
<citcontext>
<prevsection>
<prevsent>figure 1 is an example of projective dependency tree, in such trees dependencies do not cross.
</prevsent>
<prevsent>in dutch and other flexible word order languages such as german and czech we also encounter non-projective trees, in these cases the trees contain crossing dependencies.
</prevsent>
</prevsection>
<citsent citstr=" P04-1054 ">
dependency parsing is useful for applications such as relation extraction (culotta and sorensen, 2004) <papid> P04-1054 </papid>and machine translation (ding and palmer, 2005).<papid> P05-1067 </papid></citsent>
<aftsection>
<nextsent>although less informative than lexicalised phrase structures, dependency structures still capture most of the predicate-argument information needed for applications.
</nextsent>
<nextsent>it has the advantage of being more efficient to learn and parse.mcdonald et al  (2005<papid> P05-1012 </papid>a) introduce dependency parsing framework which treats the task as searching for the projective tree that maximises the sum of local dependency scores.</nextsent>
<nextsent>this frame figure 2: an incorrect partial dependency tree.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3311">
<title id=" W06-1616.xml">incremental integer linear programming for non projective dependency parsing </title>
<section> dependency parsing.  </section>
<citcontext>
<prevsection>
<prevsent>figure 1 is an example of projective dependency tree, in such trees dependencies do not cross.
</prevsent>
<prevsent>in dutch and other flexible word order languages such as german and czech we also encounter non-projective trees, in these cases the trees contain crossing dependencies.
</prevsent>
</prevsection>
<citsent citstr=" P05-1067 ">
dependency parsing is useful for applications such as relation extraction (culotta and sorensen, 2004) <papid> P04-1054 </papid>and machine translation (ding and palmer, 2005).<papid> P05-1067 </papid></citsent>
<aftsection>
<nextsent>although less informative than lexicalised phrase structures, dependency structures still capture most of the predicate-argument information needed for applications.
</nextsent>
<nextsent>it has the advantage of being more efficient to learn and parse.mcdonald et al  (2005<papid> P05-1012 </papid>a) introduce dependency parsing framework which treats the task as searching for the projective tree that maximises the sum of local dependency scores.</nextsent>
<nextsent>this frame figure 2: an incorrect partial dependency tree.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3332">
<title id=" W06-1616.xml">incremental integer linear programming for non projective dependency parsing </title>
<section> dependency parsing.  </section>
<citcontext>
<prevsection>
<prevsent>an example of this for dutch is illustrated in figure 2 which was produced by the parser of mcdonald et al  (2005<papid> P05-1012 </papid>b).</prevsent>
<prevsent>here the parse contains coordination of incompatible word classes (a preposition and verb).our approach is able to include additional constraints which forbid configurations such as those in figure 2.</prevsent>
</prevsection>
<citsent citstr=" E06-1011 ">
while mcdonald and pereira (2006) <papid> E06-1011 </papid>address the issue of local attachment decisions by defining scores over attachment pairs, our solution is more general.</citsent>
<aftsection>
<nextsent>furthermore, it is complementary in the sense that we could formulate their model using ilp and then add constraints.
</nextsent>
<nextsent>the method we present is not the only one thatcan take global constraints into account.
</nextsent>
<nextsent>deterministic dependency parsing (nivre et al , 2004; <papid> W04-2407 </papid>yamada and matsumoto, 2003) can apply global constraints by conditioning attachment decision son the intermediate parse built.</nextsent>
<nextsent>however, for efficiency greedy search is used which may produce sub-optimal solutions.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3381">
<title id=" W06-1616.xml">incremental integer linear programming for non projective dependency parsing </title>
<section> experimental set-up.  </section>
<citcontext>
<prevsection>
<prevsent>133 4.1 data.
</prevsent>
<prevsent>we use the alpino treebank (van der beek et al ,2002), taken from the conll shared task of multilingual dependency parsing3.
</prevsent>
</prevsection>
<citsent citstr=" W96-0102 ">
the conll data differs slightly from the original alpino treebank as the corpus has been part-of-speech tagged using memory-based-tagger (daelemans et al , 1996).<papid> W96-0102 </papid></citsent>
<aftsection>
<nextsent>it consists of 13,300 sentences with an average length of 14.6 tokens.
</nextsent>
<nextsent>the data is non-projective, more specifically 5.4% of all dependencies are crossed by at least one other dependency.
</nextsent>
<nextsent>it contains approximately 6000 sentences more than the alpino corpus used by malouf and van noord (2004).the training set was divided into 10% development set (dev) while the remaining 90% is usedas training and cross-validation set (cross).
</nextsent>
<nextsent>feature sets, constraints and training parameters were selected through training on cross and optimising against dev.our final accuracy scores and runtime evaluations were acquired using nine-fold cross validation on cross 4.2 environment and implementation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3386">
<title id=" W06-1616.xml">incremental integer linear programming for non projective dependency parsing </title>
<section> results.  </section>
<citcontext>
<prevsection>
<prevsent>however, they use subset of the conll alpino treebank with higher average number of tokens per sentences and also evaluate control relations, thus results are not directly comparable.
</prevsent>
<prevsent>we have also run our parser on the relatively small (approx imately 400 sentences) connl test data.
</prevsent>
</prevsection>
<citsent citstr=" W06-2932 ">
the best performing system (mcdonald et al  2006; <papid> W06-2932 </papid>note: this system is different to our baseline) achieves79.2% labelled accuracy while our baseline system achieves 78.6% and our constrained version 79.8%.</citsent>
<aftsection>
<nextsent>however, significant difference is only observed between our baseline and our constraint based system.
</nextsent>
<nextsent>examining the errors produced using the dev set highlight two key reasons why we do not see greater improvement using our constraint-basedsystem.
</nextsent>
<nextsent>firstly, we cannot improve on coordinations that include words ending with en?
</nextsent>
<nextsent>based onthe work around present in section 4.4.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3412">
<title id=" W06-2808.xml">anomaly detecting within dynamic chinese chat text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>chat text holds anomalous characteristics informing non-alphabetical characters, words, and phrases.
</prevsent>
<prevsent>it uses ill-edited terms and anomalous writing styles.
</prevsent>
</prevsection>
<citsent citstr=" I05-3013 ">
typical examples of anomalous chinese chat terms can be found in (xia et. al., 2005<papid> I05-3013 </papid>a).</citsent>
<aftsection>
<nextsent>besides the anomalous characteristics, our observations reveal remarkable dynamic nature of the chat text.
</nextsent>
<nextsent>the anomaly is created and discarded very quickly.
</nextsent>
<nextsent>although there is no idea how tomorrows chat text would look like, the changing will never stop.
</nextsent>
<nextsent>instead, the changing gets faster and faster.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3449">
<title id=" W07-0412.xml">probabilistic synchronous tree adjoining grammars for machine translation the argument from bilingual dictionaries </title>
<section> expressivity.  </section>
<citcontext>
<prevsection>
<prevsent>finite-state string transducers naturally capture these simple relationships, but so do more (and less) expressive formalisms.
</prevsent>
<prevsent>simple word-by-word replacement is not viable translation method; this was noted even as early as weavers famous memorandum (weaver, 1955).
</prevsent>
</prevsection>
<citsent citstr=" J90-2002 ">
systems based on word-to-word lexicons, such as the ibm systems (brown et al, 1990; <papid> J90-2002 </papid>brown etal., 1993), <papid> J93-2003 </papid>incorporate further devices that allow reordering of words (a distortion model?)</citsent>
<aftsection>
<nextsent>and ranking of alternatives (a monolingual language model).
</nextsent>
<nextsent>together, these allow for the possibility that the word principle: words translate differently when adjacent to other words.
</nextsent>
<nextsent>this property of the translation relation is patently true.
</nextsent>
<nextsent>even word-to-word system with the ability to reorder words and rank alternatives has obvious limitations, which have motivated the machine translation research community toward progressively more expressive formalisms.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3450">
<title id=" W07-0412.xml">probabilistic synchronous tree adjoining grammars for machine translation the argument from bilingual dictionaries </title>
<section> expressivity.  </section>
<citcontext>
<prevsection>
<prevsent>finite-state string transducers naturally capture these simple relationships, but so do more (and less) expressive formalisms.
</prevsent>
<prevsent>simple word-by-word replacement is not viable translation method; this was noted even as early as weavers famous memorandum (weaver, 1955).
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
systems based on word-to-word lexicons, such as the ibm systems (brown et al, 1990; <papid> J90-2002 </papid>brown etal., 1993), <papid> J93-2003 </papid>incorporate further devices that allow reordering of words (a distortion model?)</citsent>
<aftsection>
<nextsent>and ranking of alternatives (a monolingual language model).
</nextsent>
<nextsent>together, these allow for the possibility that the word principle: words translate differently when adjacent to other words.
</nextsent>
<nextsent>this property of the translation relation is patently true.
</nextsent>
<nextsent>even word-to-word system with the ability to reorder words and rank alternatives has obvious limitations, which have motivated the machine translation research community toward progressively more expressive formalisms.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3451">
<title id=" W07-0412.xml">probabilistic synchronous tree adjoining grammars for machine translation the argument from bilingual dictionaries </title>
<section> expressivity.  </section>
<citcontext>
<prevsection>
<prevsent>even word-to-word system with the ability to reorder words and rank alternatives has obvious limitations, which have motivated the machine translation research community toward progressively more expressive formalisms.
</prevsent>
<prevsent>again, we see precedent for the move in bilingual dictionaries, which providephrasal translations in addition to simple word trans lations: by and large / nel complesso86, full moon / luna piena406.
</prevsent>
</prevsection>
<citsent citstr=" N03-1017 ">
the insight at work here is the phrase principle: phrases (not words) translate differently when adjacent to other phrases.and again, we see this insight informing statistical machine translation systems, for instance, in the phrase-based approaches of och (2003) and koehn et al (2003).<papid> N03-1017 </papid></citsent>
<aftsection>
<nextsent>these two principles, while true, do not exhaust the insights implicit in the structure of bilingual dictionaries.
</nextsent>
<nextsent>a fuller view is accomplished by moving from words and phrases to constructions.
</nextsent>
<nextsent>2.1 the construction principle.
</nextsent>
<nextsent>the phenomenon that underlies the use of synchronous grammars for mt is simply this: 1throughout, we notate entries in hcicd with the notation entry form / translation form page, providing the italian and english forms, along with the page number of the cited entry.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3452">
<title id=" W07-0412.xml">probabilistic synchronous tree adjoining grammars for machine translation the argument from bilingual dictionaries </title>
<section> expressivity.  </section>
<citcontext>
<prevsection>
<prevsent>where this assumption fails, however, explicit marking is found in the dictionary, either by using passive alternation piacere qn / to be liked by sb424, or implicit linking mi piace / like it424.tries would be subject to non contiguity through insertion of other lexical material.
</prevsent>
<prevsent>at the type level,then, there is plenty of evidence for the phrase principle and the construction principle.at the token level, the general interest in socalled syntax-aware statistical mt approaches is itself evidence that researchers believe that the tokens accounting for the performance gap in current systems based on the word and phrase principles transcend those principles in some way, presumably because they manifest the construction prin ciple.3 only time will tell if such syntax-awaresystems are able to display performance improvements over their nonstructural alternatives.
</prevsent>
</prevsection>
<citsent citstr=" P05-1033 ">
successful experiments such as those of chiang (2005)<papid> P05-1033 </papid>using synchronous context-free grammar are good first start.4 2.3 heritage of the construction principle.</citsent>
<aftsection>
<nextsent>we have argued that formalism expressive enough to model the translation relation implicit in bilingual dictionaries must be based on relations over constructions, the primitive relations found in such bilingual dictionaries and founded by the construction principle.
</nextsent>
<nextsent>the fundamentality of this principle is evidenced by the fact that it has informed bilingual dictionaries literally since their inception.
</nextsent>
<nextsent>the earliest known bilingual dictionaries are those incorporated in the so-called lexical texts of ancient mesopotamia from four millennia ago.
</nextsent>
<nextsent>even there, we find evidence of the construction principle in entries that describe translation of words dependent upon words they are in construction with.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3454">
<title id=" W07-0412.xml">probabilistic synchronous tree adjoining grammars for machine translation the argument from bilingual dictionaries </title>
<section> synchronous grammars reviewed.  </section>
<citcontext>
<prevsection>
<prevsent>the links between the np nodes play the same role as the linked variables sb and qn in the bilingual dictionary entry.
</prevsent>
<prevsent>they allow for substitution of tree pairs for eli and its translation and his father and its.
</prevsent>
</prevsection>
<citsent citstr=" C90-3045 ">
the additional links allow for further modification, as in eli recently took his father by surprise by preparing dinner, the modifiers recently and by preparing dinner adjoining at the vp and links, respectively.expressing this relation in other frameworks involves either limiting its scope (for instance, to particular objects and intervening material), expanding its scope (by separating the translations of the contiguous portions of the constructions), or mimicking the structure of the stag (as described at the end of section 5).the basic idea of using synchronous tag forma chine translation dates from the original definition(shieber and schabes, 1990), <papid> C90-3045 </papid>and has been pursued by several researchers (abeille et al, 1990; <papid> C90-3001 </papid>dras, 1999; <papid> P99-1011 </papid>prigent, 1994; palmer et al, 1999), but only recently in its probabilistic form (nesson et al,2006).</citsent>
<aftsection>
<nextsent>the directness with which the formalism follows from the structure of bilingual dictionaries has not to our knowledge been previously noted.
</nextsent>
<nextsent>it leads to the possibility of making direct use of bilingual dictionary material in statistical machine translation system.5 but even if the formalism is not usedin that way, there is import to the fact that its expressivity matches that thought by lexicographers of the last several millennia to be needed for capturing the translation relation; this fact indicates at least that stags use as substrate for mt systems may be promising research direction to pursue, should other necessary properties be satisfiable as well.
</nextsent>
<nextsent>we turn next to two of these properties: tra inability and efficiency.
</nextsent>
<nextsent>the mere ability to formally represent the contents of manually developed bilingual dictionaries is not sufficient for the building of robust machine translation systems.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3455">
<title id=" W07-0412.xml">probabilistic synchronous tree adjoining grammars for machine translation the argument from bilingual dictionaries </title>
<section> synchronous grammars reviewed.  </section>
<citcontext>
<prevsection>
<prevsent>the links between the np nodes play the same role as the linked variables sb and qn in the bilingual dictionary entry.
</prevsent>
<prevsent>they allow for substitution of tree pairs for eli and its translation and his father and its.
</prevsent>
</prevsection>
<citsent citstr=" C90-3001 ">
the additional links allow for further modification, as in eli recently took his father by surprise by preparing dinner, the modifiers recently and by preparing dinner adjoining at the vp and links, respectively.expressing this relation in other frameworks involves either limiting its scope (for instance, to particular objects and intervening material), expanding its scope (by separating the translations of the contiguous portions of the constructions), or mimicking the structure of the stag (as described at the end of section 5).the basic idea of using synchronous tag forma chine translation dates from the original definition(shieber and schabes, 1990), <papid> C90-3045 </papid>and has been pursued by several researchers (abeille et al, 1990; <papid> C90-3001 </papid>dras, 1999; <papid> P99-1011 </papid>prigent, 1994; palmer et al, 1999), but only recently in its probabilistic form (nesson et al,2006).</citsent>
<aftsection>
<nextsent>the directness with which the formalism follows from the structure of bilingual dictionaries has not to our knowledge been previously noted.
</nextsent>
<nextsent>it leads to the possibility of making direct use of bilingual dictionary material in statistical machine translation system.5 but even if the formalism is not usedin that way, there is import to the fact that its expressivity matches that thought by lexicographers of the last several millennia to be needed for capturing the translation relation; this fact indicates at least that stags use as substrate for mt systems may be promising research direction to pursue, should other necessary properties be satisfiable as well.
</nextsent>
<nextsent>we turn next to two of these properties: tra inability and efficiency.
</nextsent>
<nextsent>the mere ability to formally represent the contents of manually developed bilingual dictionaries is not sufficient for the building of robust machine translation systems.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3456">
<title id=" W07-0412.xml">probabilistic synchronous tree adjoining grammars for machine translation the argument from bilingual dictionaries </title>
<section> synchronous grammars reviewed.  </section>
<citcontext>
<prevsection>
<prevsent>the links between the np nodes play the same role as the linked variables sb and qn in the bilingual dictionary entry.
</prevsent>
<prevsent>they allow for substitution of tree pairs for eli and its translation and his father and its.
</prevsent>
</prevsection>
<citsent citstr=" P99-1011 ">
the additional links allow for further modification, as in eli recently took his father by surprise by preparing dinner, the modifiers recently and by preparing dinner adjoining at the vp and links, respectively.expressing this relation in other frameworks involves either limiting its scope (for instance, to particular objects and intervening material), expanding its scope (by separating the translations of the contiguous portions of the constructions), or mimicking the structure of the stag (as described at the end of section 5).the basic idea of using synchronous tag forma chine translation dates from the original definition(shieber and schabes, 1990), <papid> C90-3045 </papid>and has been pursued by several researchers (abeille et al, 1990; <papid> C90-3001 </papid>dras, 1999; <papid> P99-1011 </papid>prigent, 1994; palmer et al, 1999), but only recently in its probabilistic form (nesson et al,2006).</citsent>
<aftsection>
<nextsent>the directness with which the formalism follows from the structure of bilingual dictionaries has not to our knowledge been previously noted.
</nextsent>
<nextsent>it leads to the possibility of making direct use of bilingual dictionary material in statistical machine translation system.5 but even if the formalism is not usedin that way, there is import to the fact that its expressivity matches that thought by lexicographers of the last several millennia to be needed for capturing the translation relation; this fact indicates at least that stags use as substrate for mt systems may be promising research direction to pursue, should other necessary properties be satisfiable as well.
</nextsent>
<nextsent>we turn next to two of these properties: tra inability and efficiency.
</nextsent>
<nextsent>the mere ability to formally represent the contents of manually developed bilingual dictionaries is not sufficient for the building of robust machine translation systems.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3458">
<title id=" W07-0412.xml">probabilistic synchronous tree adjoining grammars for machine translation the argument from bilingual dictionaries </title>
<section> efficiency.  </section>
<citcontext>
<prevsection>
<prevsent>one reviewer notes that the admittedly perplexing reality is that exponential decoders run much faster than polynomial ones, pre here, the stag situation is equivocal.
</prevsent>
<prevsent>bilingual parsing of corpus relative to an stag is necessary first step in parameter training.
</prevsent>
</prevsection>
<citsent citstr=" H05-1101 ">
the recognition problem for stag, like that for synchronous context-free grammar (scfg) is np-hard (satta and peserico, 2005).<papid> H05-1101 </papid></citsent>
<aftsection>
<nextsent>under appropriate restrictions of binarizability, scfg parsing can be done in o(n6)time, doubling the exponent of cfg parsing.
</nextsent>
<nextsent>similarly, stag parsing under suitable limitations (nesson et al (2005)) can be done in o(n12) time doubling the exponent of monolingual tag parsing.
</nextsent>
<nextsent>onthe positive side, recent work exploring the automatic binarization of synchronous grammars (zhanget al, 2006) <papid> N06-1033 </papid>has indicated that non-binarizable constructions seem to be relatively rare in practice.</nextsent>
<nextsent>nonetheless, such high-degree polynomial makes the complete algorithm impractical.nesson et al (2006) use synchronous tree insertion grammar (stig) (schabes and waters, 1995) <papid> J95-4002 </papid>rather than stag for this very reason.stig retains the ability to express universal normal form, while allowing o(n6) bilingual parsing.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3459">
<title id=" W07-0412.xml">probabilistic synchronous tree adjoining grammars for machine translation the argument from bilingual dictionaries </title>
<section> efficiency.  </section>
<citcontext>
<prevsection>
<prevsent>under appropriate restrictions of binarizability, scfg parsing can be done in o(n6)time, doubling the exponent of cfg parsing.
</prevsent>
<prevsent>similarly, stag parsing under suitable limitations (nesson et al (2005)) can be done in o(n12) time doubling the exponent of monolingual tag parsing.
</prevsent>
</prevsection>
<citsent citstr=" N06-1033 ">
onthe positive side, recent work exploring the automatic binarization of synchronous grammars (zhanget al, 2006) <papid> N06-1033 </papid>has indicated that non-binarizable constructions seem to be relatively rare in practice.</citsent>
<aftsection>
<nextsent>nonetheless, such high-degree polynomial makes the complete algorithm impractical.nesson et al (2006) use synchronous tree insertion grammar (stig) (schabes and waters, 1995) <papid> J95-4002 </papid>rather than stag for this very reason.stig retains the ability to express universal normal form, while allowing o(n6) bilingual parsing.</nextsent>
<nextsent>(again, limitations on the formalism are required to achieve this complexity.)</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3460">
<title id=" W07-0412.xml">probabilistic synchronous tree adjoining grammars for machine translation the argument from bilingual dictionaries </title>
<section> efficiency.  </section>
<citcontext>
<prevsection>
<prevsent>similarly, stag parsing under suitable limitations (nesson et al (2005)) can be done in o(n12) time doubling the exponent of monolingual tag parsing.
</prevsent>
<prevsent>onthe positive side, recent work exploring the automatic binarization of synchronous grammars (zhanget al, 2006) <papid> N06-1033 </papid>has indicated that non-binarizable constructions seem to be relatively rare in practice.</prevsent>
</prevsection>
<citsent citstr=" J95-4002 ">
nonetheless, such high-degree polynomial makes the complete algorithm impractical.nesson et al (2006) use synchronous tree insertion grammar (stig) (schabes and waters, 1995) <papid> J95-4002 </papid>rather than stag for this very reason.stig retains the ability to express universal normal form, while allowing o(n6) bilingual parsing.</citsent>
<aftsection>
<nextsent>(again, limitations on the formalism are required to achieve this complexity.)
</nextsent>
<nextsent>even this complexity may be too high.
</nextsent>
<nextsent>methods such as those of chiang (2005)<papid> P05-1033 </papid>have been proposed for further reducing the complexity of scfg parsing; they may be applicable to stig (and stag) parsing as well.the stig formalism can be shown to be expressively equivalent to synchronous tree-substitutiongrammar (stsg) and even scfg.</nextsent>
<nextsent>does this vitiate the argument for stig as natural formalism for mt? no.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3462">
<title id=" W07-0412.xml">probabilistic synchronous tree adjoining grammars for machine translation the argument from bilingual dictionaries </title>
<section> efficiency.  </section>
<citcontext>
<prevsection>
<prevsent>these tricks essentially amount to treating the formalism as an stig, not an scfg.
</prevsent>
<prevsent>that is, even if an scfg were to be used, its structure would best be built on the observations found here.
</prevsent>
</prevsection>
<citsent citstr=" W06-1628 ">
for example, the method of cowan et al (2006) <papid> W06-1628 </papid>synchronizes elementary trees of prescribed formto handle translation of clauses (verbs plus their ar guments) essentially implementing kind of stsg.</citsent>
<aftsection>
<nextsent>however, because modifiers can make these trees dis contiguous, they augment the model by allowing for free insertion of modifiers in certain locations.one view of this is as an implementation of the principle that motivates adjoining, without using adjoining itself.
</nextsent>
<nextsent>thus, systems that are designed to take account of the principles adduced in this paper are likely to be implementing aspects of stag implicitly, even if not explicitly.
</nextsent>
<nextsent>similarly, recent research is beginning to unify synchronous grammar formalisms and tree transducers (shieber, 2004; shieber, 2006).<papid> E06-1048 </papid></nextsent>
<nextsent>there maywell be equally direct transducer formalisms that elegantly express construction-based translation relations.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3463">
<title id=" W07-0412.xml">probabilistic synchronous tree adjoining grammars for machine translation the argument from bilingual dictionaries </title>
<section> efficiency.  </section>
<citcontext>
<prevsection>
<prevsent>however, because modifiers can make these trees dis contiguous, they augment the model by allowing for free insertion of modifiers in certain locations.one view of this is as an implementation of the principle that motivates adjoining, without using adjoining itself.
</prevsent>
<prevsent>thus, systems that are designed to take account of the principles adduced in this paper are likely to be implementing aspects of stag implicitly, even if not explicitly.
</prevsent>
</prevsection>
<citsent citstr=" E06-1048 ">
similarly, recent research is beginning to unify synchronous grammar formalisms and tree transducers (shieber, 2004; shieber, 2006).<papid> E06-1048 </papid></citsent>
<aftsection>
<nextsent>there maywell be equally direct transducer formalisms that elegantly express construction-based translation relations.
</nextsent>
<nextsent>this would not be denial of the present thesis but happy acknowledgment of it.
</nextsent>
<nextsent>we have argued that probabilistic synchronous tagor some closely related formalism possesses constellation of properties expressivity, tra inability, and efficiency that make it good candidate ata conceptual level for founding machine translation system.
</nextsent>
<nextsent>what would such system look likeit would start with universal normal form sub grammar serving as the robust backoff?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3464">
<title id=" W07-0412.xml">probabilistic synchronous tree adjoining grammars for machine translation the argument from bilingual dictionaries </title>
<section> conclusion.  </section>
<citcontext>
<prevsection>
<prevsent>we have argued that probabilistic synchronous tagor some closely related formalism possesses constellation of properties expressivity, tra inability, and efficiency that make it good candidate ata conceptual level for founding machine translation system.
</prevsent>
<prevsent>what would such system look likeit would start with universal normal form sub grammar serving as the robust backoff?
</prevsent>
</prevsection>
<citsent citstr=" C04-1154 ">
relation to which additional more articulated bilingual material could be added in the form of additional tree pairs.these tree pairs might be manually generated, automatically reconstructed from re purposed bilingual dictionaries, or automatically induced from aligned bilingual treebanks (groves et al, 2004; <papid> C04-1154 </papid>groves and way, 2005) <papid> W05-0833 </papid>or even unannotated bilingual corpora (chiang, 2005).<papid> P05-1033 </papid></citsent>
<aftsection>
<nextsent>in fact, since all of these sources of data yield interacting tree pairs, more than one of these techniques might be used.
</nextsent>
<nextsent>in any case, further training would automatically determine the interactions of these information sources.the conclusions of this paper are admittedly programmatic.
</nextsent>
<nextsent>but plausible arguments for program of research may be just the thing for clarifying research direction and even promoting its pursual.
</nextsent>
<nextsent>inthat sense, this paper can be read as kind of manifesto for the use of probabilistic synchronous tag as substrate for mt research.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3465">
<title id=" W07-0412.xml">probabilistic synchronous tree adjoining grammars for machine translation the argument from bilingual dictionaries </title>
<section> conclusion.  </section>
<citcontext>
<prevsection>
<prevsent>we have argued that probabilistic synchronous tagor some closely related formalism possesses constellation of properties expressivity, tra inability, and efficiency that make it good candidate ata conceptual level for founding machine translation system.
</prevsent>
<prevsent>what would such system look likeit would start with universal normal form sub grammar serving as the robust backoff?
</prevsent>
</prevsection>
<citsent citstr=" W05-0833 ">
relation to which additional more articulated bilingual material could be added in the form of additional tree pairs.these tree pairs might be manually generated, automatically reconstructed from re purposed bilingual dictionaries, or automatically induced from aligned bilingual treebanks (groves et al, 2004; <papid> C04-1154 </papid>groves and way, 2005) <papid> W05-0833 </papid>or even unannotated bilingual corpora (chiang, 2005).<papid> P05-1033 </papid></citsent>
<aftsection>
<nextsent>in fact, since all of these sources of data yield interacting tree pairs, more than one of these techniques might be used.
</nextsent>
<nextsent>in any case, further training would automatically determine the interactions of these information sources.the conclusions of this paper are admittedly programmatic.
</nextsent>
<nextsent>but plausible arguments for program of research may be just the thing for clarifying research direction and even promoting its pursual.
</nextsent>
<nextsent>inthat sense, this paper can be read as kind of manifesto for the use of probabilistic synchronous tag as substrate for mt research.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3467">
<title id=" W06-3126.xml">the ldv combo system for smt </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we show that liniguistic information may be helpful, specially when the target language has rich morphology.
</prevsent>
<prevsent>the main motivation behind our work is to introduce linguistic information, other than lexical units, to the process of building word and phrase alignments.
</prevsent>
</prevsection>
<citsent citstr=" P01-1067 ">
in the last years, many efforts have been devoted to this matter (yamada and knight, 2001; <papid> P01-1067 </papid>gildea, 2003).<papid> P03-1011 </papid></citsent>
<aftsection>
<nextsent>following our previous work (gimenez andma`rquez, 2005), we use shallow syntactic information to generate more precise alignments.
</nextsent>
<nextsent>far from full syntactic complexity, we suggest going back to the simpler alignment methods first described byibm (1993).
</nextsent>
<nextsent>our approach exploits the possibility of working with alignments at two different levels of granularity, lexical (words) and shallow parsing (chunks).
</nextsent>
<nextsent>apart from redefining the scope of the alignment unit, we may use different linguistic data views.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3468">
<title id=" W06-3126.xml">the ldv combo system for smt </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we show that liniguistic information may be helpful, specially when the target language has rich morphology.
</prevsent>
<prevsent>the main motivation behind our work is to introduce linguistic information, other than lexical units, to the process of building word and phrase alignments.
</prevsent>
</prevsection>
<citsent citstr=" P03-1011 ">
in the last years, many efforts have been devoted to this matter (yamada and knight, 2001; <papid> P01-1067 </papid>gildea, 2003).<papid> P03-1011 </papid></citsent>
<aftsection>
<nextsent>following our previous work (gimenez andma`rquez, 2005), we use shallow syntactic information to generate more precise alignments.
</nextsent>
<nextsent>far from full syntactic complexity, we suggest going back to the simpler alignment methods first described byibm (1993).
</nextsent>
<nextsent>our approach exploits the possibility of working with alignments at two different levels of granularity, lexical (words) and shallow parsing (chunks).
</nextsent>
<nextsent>apart from redefining the scope of the alignment unit, we may use different linguistic data views.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3469">
<title id=" W06-3126.xml">the ldv combo system for smt </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>of course, there is natural trade-off between the use of linguistic data views and data sparsity.
</prevsent>
<prevsent>fortunately, we hava data enough so that statistical parameter estimation remains reliable.
</prevsent>
</prevsection>
<citsent citstr=" W03-1002 ">
the approach which is closest to ours is that by schafer and yarowsky (2003) <papid> W03-1002 </papid>who suggested combination of models based on shallow syntactic analysis (part-of-speech tagging and phrase chunking).</citsent>
<aftsection>
<nextsent>they followed backoff strategy in the application of their models.
</nextsent>
<nextsent>decoding was based on finite state automata.
</nextsent>
<nextsent>although no significant improvement in mt quality was reported, results were promising taking into account the short time spent in the development of the linguistic tools utilized.our system is further described in section 2.
</nextsent>
<nextsent>results are reported in section 3.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3470">
<title id=" W06-3126.xml">the ldv combo system for smt </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>in this paper we focus on data views at the word level.
</prevsent>
<prevsent>6 different data views have been built: (w) word, (l) lemma,(wp) word and pos, (wc) word and chunk iob label, (wpc) word, pos and chunk iob label, (lc) lemma and chunk iob label.
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
then, running giza++ (och and ney, 2003), <papid> J03-1002 </papid>we obtain token alignments for each of the data views.</citsent>
<aftsection>
<nextsent>combined phrase-based translation models are built on top of the viterbi alignments output by giza++.phrase extraction is performed following the phrase extract algorithm depicted by och (2002).
</nextsent>
<nextsent>we do not apply any heuristic refinement.
</nextsent>
<nextsent>we work with phrases up to 5 tokens.
</nextsent>
<nextsent>phrase pairs appearing only once have been discarded.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3471">
<title id=" W06-3126.xml">the ldv combo system for smt </title>
<section> experimental work.  </section>
<citcontext>
<prevsection>
<prevsent>3.1 setting.
</prevsent>
<prevsent>we have used only the datasets and language model provided by the organization.
</prevsent>
</prevsection>
<citsent citstr=" N03-2021 ">
for evaluation wehave selected set of 8 metric variants corresponding to seven different families: bleu (n = 4) (pa pineni et al, 2001), nist (n = 5) (lin and hovy, 2002), gtm f1-measure (e = 1, 2) (melamed et al, 2003), <papid> N03-2021 </papid>1-wer (nieen et al, 2000), 1-per (leusch et al, 2003), rouge (rouge-s*) (lin and och, 2004) <papid> P04-1077 </papid>and meteor3 (banerjee and lavie, 2005).<papid> W05-0909 </papid></citsent>
<aftsection>
<nextsent>optimization of the decoding parameters (tm, lm, w) is performed by means of the downhill simplex method in multi dimensions (william h. press and flannery, 2002) over the bleu metric.
</nextsent>
<nextsent>3for spanish-to-english we applied all available modules: exact + stemming + wordnet stemming + wordnet synonymy lookup.
</nextsent>
<nextsent>however, for english-to-spanish we were forced to use the exact module alone.
</nextsent>
<nextsent>167 spanish-to-english system 1-per 1-wer bleu-4 gtm-1 gtm-2 meteor nist-5 rouge-s* baseline 0.5514 0.3741 0.2709 0.6159 0.2579 0.5836 7.2958 0.3643 ldv-combo 0.5478 0.3657 0.2708 0.6202 0.2585 0.5928 7.2433 0.3671 english-to-spanish system 1-per 1-wer bleu-4 gtm-1 gtm-2 meteor nist-5 rouge-s* baseline 0.5158 0.3776 0.2272 0.5673 0.2418 0.4954 6.6835 0.3028 ldv-combo 0.5382 0.3560 0.2611 0.5910 0.2462 0.5400 7.1054 0.3240 table 1: mt results comparing the ldv-combo system to baseline system, for the test set both on the spanish-to-english and english-to-spanish tasks.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3472">
<title id=" W06-3126.xml">the ldv combo system for smt </title>
<section> experimental work.  </section>
<citcontext>
<prevsection>
<prevsent>3.1 setting.
</prevsent>
<prevsent>we have used only the datasets and language model provided by the organization.
</prevsent>
</prevsection>
<citsent citstr=" P04-1077 ">
for evaluation wehave selected set of 8 metric variants corresponding to seven different families: bleu (n = 4) (pa pineni et al, 2001), nist (n = 5) (lin and hovy, 2002), gtm f1-measure (e = 1, 2) (melamed et al, 2003), <papid> N03-2021 </papid>1-wer (nieen et al, 2000), 1-per (leusch et al, 2003), rouge (rouge-s*) (lin and och, 2004) <papid> P04-1077 </papid>and meteor3 (banerjee and lavie, 2005).<papid> W05-0909 </papid></citsent>
<aftsection>
<nextsent>optimization of the decoding parameters (tm, lm, w) is performed by means of the downhill simplex method in multi dimensions (william h. press and flannery, 2002) over the bleu metric.
</nextsent>
<nextsent>3for spanish-to-english we applied all available modules: exact + stemming + wordnet stemming + wordnet synonymy lookup.
</nextsent>
<nextsent>however, for english-to-spanish we were forced to use the exact module alone.
</nextsent>
<nextsent>167 spanish-to-english system 1-per 1-wer bleu-4 gtm-1 gtm-2 meteor nist-5 rouge-s* baseline 0.5514 0.3741 0.2709 0.6159 0.2579 0.5836 7.2958 0.3643 ldv-combo 0.5478 0.3657 0.2708 0.6202 0.2585 0.5928 7.2433 0.3671 english-to-spanish system 1-per 1-wer bleu-4 gtm-1 gtm-2 meteor nist-5 rouge-s* baseline 0.5158 0.3776 0.2272 0.5673 0.2418 0.4954 6.6835 0.3028 ldv-combo 0.5382 0.3560 0.2611 0.5910 0.2462 0.5400 7.1054 0.3240 table 1: mt results comparing the ldv-combo system to baseline system, for the test set both on the spanish-to-english and english-to-spanish tasks.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3473">
<title id=" W06-3126.xml">the ldv combo system for smt </title>
<section> experimental work.  </section>
<citcontext>
<prevsection>
<prevsent>3.1 setting.
</prevsent>
<prevsent>we have used only the datasets and language model provided by the organization.
</prevsent>
</prevsection>
<citsent citstr=" W05-0909 ">
for evaluation wehave selected set of 8 metric variants corresponding to seven different families: bleu (n = 4) (pa pineni et al, 2001), nist (n = 5) (lin and hovy, 2002), gtm f1-measure (e = 1, 2) (melamed et al, 2003), <papid> N03-2021 </papid>1-wer (nieen et al, 2000), 1-per (leusch et al, 2003), rouge (rouge-s*) (lin and och, 2004) <papid> P04-1077 </papid>and meteor3 (banerjee and lavie, 2005).<papid> W05-0909 </papid></citsent>
<aftsection>
<nextsent>optimization of the decoding parameters (tm, lm, w) is performed by means of the downhill simplex method in multi dimensions (william h. press and flannery, 2002) over the bleu metric.
</nextsent>
<nextsent>3for spanish-to-english we applied all available modules: exact + stemming + wordnet stemming + wordnet synonymy lookup.
</nextsent>
<nextsent>however, for english-to-spanish we were forced to use the exact module alone.
</nextsent>
<nextsent>167 spanish-to-english system 1-per 1-wer bleu-4 gtm-1 gtm-2 meteor nist-5 rouge-s* baseline 0.5514 0.3741 0.2709 0.6159 0.2579 0.5836 7.2958 0.3643 ldv-combo 0.5478 0.3657 0.2708 0.6202 0.2585 0.5928 7.2433 0.3671 english-to-spanish system 1-per 1-wer bleu-4 gtm-1 gtm-2 meteor nist-5 rouge-s* baseline 0.5158 0.3776 0.2272 0.5673 0.2418 0.4954 6.6835 0.3028 ldv-combo 0.5382 0.3560 0.2611 0.5910 0.2462 0.5400 7.1054 0.3240 table 1: mt results comparing the ldv-combo system to baseline system, for the test set both on the spanish-to-english and english-to-spanish tasks.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3474">
<title id=" W06-3126.xml">the ldv combo system for smt </title>
<section> conclusions and further work.  </section>
<citcontext>
<prevsection>
<prevsent>the reason is that weare working with the union of alignments from different data views, thus adding more constraints into the phrase extraction step.
</prevsent>
<prevsent>fewer phrase pairs are extracted, and as consequence we are also effectively eliminating noise from translation models.
</prevsent>
</prevsection>
<citsent citstr=" P05-1066 ">
many researchers remain sceptical about the usefulness of linguistic information in smt, because, except in couple of cases (charniak et al, 2003; collins et al, 2005), <papid> P05-1066 </papid>little success has been reported.in this work we have shown that liniguistic information may be helpful, specially when the target language has rich morphology (e.g. spanish).</citsent>
<aftsection>
<nextsent>moreover, it has often been argued that linguistic information does not yield significant improvement sin mt quality, because (i) linguistic processors introduce many errors and (ii) the bleu score is not specially sensitive to the grammaticality of mt output.
</nextsent>
<nextsent>we have minimized the impact of the first argument by using highly accurate tools for both languages.
</nextsent>
<nextsent>in order to solve the second problem more sophisticated metrics are required.
</nextsent>
<nextsent>current mt evaluation metrics fail to capture many aspects of mt 168quality that characterize human translations with respect to those produced by mt systems.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3475">
<title id=" W06-1663.xml">quality assessment of large scale knowledge resources </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>that is, around twelve thousand semantic relations per year.
</prevsent>
<prevsent>however, during the last years the research community has devised large set of innovative processes and tools for large-scale automatic acquisition of lexical knowledge from structured or unstructuredcorpora.
</prevsent>
</prevsection>
<citsent citstr=" W01-0703 ">
among others we can mention extended wordnet (mihalcea and moldovan, 2001), large collections of semantic preferences acquired from semcor (agirre and martinez, 2001; <papid> W01-0703 </papid>agirreand martinez, 2002) or acquired from british national corpus (bnc) (mccarthy, 2001), large scale topic signatures for each synset acquired from the web (agirre and de lacalle, 2004) or acquired from the bnc (cuadros et al, 2005).</citsent>
<aftsection>
<nextsent>obviously, all these semantic resources have been acquired using very different set of methods, tools and corpora, resulting on different set of new semantic relations between synsets.
</nextsent>
<nextsent>in fact, each resource has different volume and accuracy figures.
</nextsent>
<nextsent>although isolated evaluations have been performed by their developers in different experi 1symmetric relations are counted only once.
</nextsent>
<nextsent>534 mental settings, to date no comparable evaluation has been carried out in common and controlled framework.this work tries to establish the relative quality of these semantic resources in neutral environment.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3484">
<title id=" W06-1663.xml">quality assessment of large scale knowledge resources </title>
<section> large scale knowledge resources.  </section>
<citcontext>
<prevsection>
<prevsent>mcr (atserias et al, 2004): this knowledge resource uses the direct relations included in mcr.
</prevsent>
<prevsent>2.2 automatically retrieved topic signatures.
</prevsent>
</prevsection>
<citsent citstr=" C00-1072 ">
topic signatures (ts) are word vectors related to particular topic (lin and hovy, 2000).<papid> C00-1072 </papid></citsent>
<aftsection>
<nextsent>topic signatures are built by retrieving context words of target topic from large volumes of text.
</nextsent>
<nextsent>inour case, we consider word senses as topics.
</nextsent>
<nextsent>basically, the acquisition of ts consists of a) acquiring the best possible corpus examples for particular word sense (usually characterizing each word sense as query and performing search on the corpus for those examples that best match thequeries), and then, b) building the ts by deriving the context words that best represent the word sense from the selected corpora.for this study, we use the large-scale topic signatures acquired from the web (agirre and de lacalle, 2004) and those acquired from the bnc (cuadros et al, 2005).?
</nextsent>
<nextsent>tsweb3: inspired by the work of (lea cock et al, 1998), <papid> J98-1006 </papid>these topic signatures were constructed using monosemous relatives from wordnet (synonyms, hypernyms, direct and indirect hyponyms, and siblings), querying google and retrieving up to one thousand snippets per query (that is, wordsense).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3485">
<title id=" W06-1663.xml">quality assessment of large scale knowledge resources </title>
<section> large scale knowledge resources.  </section>
<citcontext>
<prevsection>
<prevsent>inour case, we consider word senses as topics.
</prevsent>
<prevsent>basically, the acquisition of ts consists of a) acquiring the best possible corpus examples for particular word sense (usually characterizing each word sense as query and performing search on the corpus for those examples that best match thequeries), and then, b) building the ts by deriving the context words that best represent the word sense from the selected corpora.for this study, we use the large-scale topic signatures acquired from the web (agirre and de lacalle, 2004) and those acquired from the bnc (cuadros et al, 2005).?
</prevsent>
</prevsection>
<citsent citstr=" J98-1006 ">
tsweb3: inspired by the work of (lea cock et al, 1998), <papid> J98-1006 </papid>these topic signatures were constructed using monosemous relatives from wordnet (synonyms, hypernyms, direct and indirect hyponyms, and siblings), querying google and retrieving up to one thousand snippets per query (that is, wordsense).</citsent>
<aftsection>
<nextsent>in particular, the method was as fol lows: ? organizing the retrieved examples from the web in collections, one collection per word sense.?
</nextsent>
<nextsent>extracting the words and their frequencies for each collection.
</nextsent>
<nextsent>comparing these frequencies with those pertaining to other word senses using tfidf (see formula 1).
</nextsent>
<nextsent>gathering in an ordered list, the words with distinctive frequency for one of the collections, which constitutes the topic signature for the respective word sense.this constitutes the largest available semantic resource with around 100 million relations (between synsets and words).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3486">
<title id=" W06-3207.xml">richness of the base and probabilistic unsupervised learning in optimality theory </title>
<section> conclusion.  </section>
<citcontext>
<prevsection>
<prevsent>the algorithm is shown to be successful on three constructed languages featuring different types of neutralization and hidden structure.
</prevsent>
<prevsent>one potential extension of the proposed algorithm involves combining system for unsupervised learning of morphological relations with the proposed algorithm for learning phonology.
</prevsent>
</prevsection>
<citsent citstr=" J01-2001 ">
several algorithms have been proposed for automatically inducing morphological relations, like those assumed by the present learner (goldsmith, 2001; <papid> J01-2001 </papid>snover and brent, 2001).<papid> P01-1063 </papid></citsent>
<aftsection>
<nextsent>the task of uncovering morphological relations is complicated by allo morphic alternations that obscure the underlying identity of related morphemes.
</nextsent>
<nextsent>while these algorithms are very promising, their performance may be significantly enhanced if they were combined with an algorithm that models such phonological alternations.
</nextsent>
<nextsent>in conclusion, this is the first proposed unsupervised algorithm for ot learning that takes advan 57 tage of the power of probabilistic modeling to learn grammar and lexicon simultaneously.
</nextsent>
<nextsent>this paper demonstrates that combining ot theoretic principles with results from computational language learning is worthwhile pursuit that may inform both disciplines.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3487">
<title id=" W06-3207.xml">richness of the base and probabilistic unsupervised learning in optimality theory </title>
<section> conclusion.  </section>
<citcontext>
<prevsection>
<prevsent>the algorithm is shown to be successful on three constructed languages featuring different types of neutralization and hidden structure.
</prevsent>
<prevsent>one potential extension of the proposed algorithm involves combining system for unsupervised learning of morphological relations with the proposed algorithm for learning phonology.
</prevsent>
</prevsection>
<citsent citstr=" P01-1063 ">
several algorithms have been proposed for automatically inducing morphological relations, like those assumed by the present learner (goldsmith, 2001; <papid> J01-2001 </papid>snover and brent, 2001).<papid> P01-1063 </papid></citsent>
<aftsection>
<nextsent>the task of uncovering morphological relations is complicated by allo morphic alternations that obscure the underlying identity of related morphemes.
</nextsent>
<nextsent>while these algorithms are very promising, their performance may be significantly enhanced if they were combined with an algorithm that models such phonological alternations.
</nextsent>
<nextsent>in conclusion, this is the first proposed unsupervised algorithm for ot learning that takes advan 57 tage of the power of probabilistic modeling to learn grammar and lexicon simultaneously.
</nextsent>
<nextsent>this paper demonstrates that combining ot theoretic principles with results from computational language learning is worthwhile pursuit that may inform both disciplines.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3488">
<title id=" W06-2501.xml">using wordnet based context vectors to estimate the semantic relatedness of concepts </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a reader likely uses domain knowledge of sports along with the realization that the baseball senses of hitting, bat, ball and stands are all semantically related, in order to determine that the event being described is baseball game.
</prevsent>
<prevsent>consequently, number of techniques have been proposed over the years, that attempt to automatically compute the semantic relatedness of concepts to correspond closely with human judgments (resnik, 1995; jiang and conrath, 1997; lin, 1998; leacock and chodorow, 1998).
</prevsent>
</prevsection>
<citsent citstr=" P05-1047 ">
it hasalso been shown that these techniques prove useful for tasks such as word sense disambiguation(patwardhan et al, 2003), real-word spelling correction (budanitsky and hirst, 2001) and information extraction (stevenson and greenwood, 2005), <papid> P05-1047 </papid>among others.</citsent>
<aftsection>
<nextsent>in this paper we introduce wordnet-basedmeasure of semantic relatedness inspired by har ris?
</nextsent>
<nextsent>distributional hypothesis (harris, 1985).
</nextsent>
<nextsent>the distributional hypothesis suggests that words thatare similar in meaning tend to occur in similar linguistic contexts.
</nextsent>
<nextsent>additionally, numerous studies (carnine et al, 1984; miller and charles, 1991; mcdonald and ramscar, 2001) have shown that context plays vital role in defining the meanings of words.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3489">
<title id=" W06-2501.xml">using wordnet based context vectors to estimate the semantic relatedness of concepts </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>this adds another level of indirection inthe comparison and attempts to use only the relevant meanings of the context words.
</prevsent>
<prevsent>secondly, we use the structure of wordnet to augment the short glosses with other related glosses.
</prevsent>
</prevsection>
<citsent citstr=" C94-1049 ">
(niwa and nitta, 1994) <papid> C94-1049 </papid>compare dictionary based vectors with cooccurrence based vectors, where the vector of word is the probability that an origin word occurs in the context of the word.these two representations are evaluated by applying them to real world applications and quantifying the results.</citsent>
<aftsection>
<nextsent>both measures are first applied toword sense disambiguation and then to the learning of positives or negatives, where it is required to determine whether word has positive or negative connotation.
</nextsent>
<nextsent>it was observed that the co? occurrence based idea works better for the word sense disambiguation and the dictionary based approach gives better results for the learning of positives or negatives.
</nextsent>
<nextsent>from this, the conclusion isthat the dictionary based vectors contain some different semantic information about the words and warrants further investigation.
</nextsent>
<nextsent>it is also observed that for the dictionary based vectors, the network of words is almost independent of the dictionary that is used, i.e. any dictionary should give us almost the same network.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3490">
<title id=" W06-1643.xml">a skip chain conditional random field for ranking meeting utterances by importance </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we discuss the problem of estimating the class posterior probability of each utterance in sequence in order to extract the most probable ones, and show that the cost assigned by acrf to each utterance needs to be locally normalized in order to outperform bns.
</prevsent>
<prevsent>after analyzing the predictive power of large set of dura tional, acoustical, lexical, structural, and information retrieval features, we perform feature selection to have competitive set of predictors to test the different models.
</prevsent>
</prevsection>
<citsent citstr=" N04-1019 ">
empirical evaluations using two standard summarization metrics the pyramid method (nenkova and passonneau, 2004<papid> N04-1019 </papid>b) and rouge (lin, 2004)<papid> W04-1013 </papid>show that the best performing system is crf incorporating bothorder-2 markov dependencies and skip-chain dependencies, which achieves 91.3% of human performance in pyramid score, and outperforms our best-performing non-sequential model by 3.9%.</citsent>
<aftsection>
<nextsent>the work presented here was applied to the icsi meeting corpus (janin et al, 2003), corpus of naturally-occurring?
</nextsent>
<nextsent>meetings, i.e. meetings that would have taken place anyway.
</nextsent>
<nextsent>their styleis quite informal, and topics are primarily concerned with speech, natural language, artificial 364intelligence, and networking research.
</nextsent>
<nextsent>the corpus contains 75 meetings, which are 60 minutes long on average, and involve number of participants ranging from 3 to 10 (6 on average).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3494">
<title id=" W06-1643.xml">a skip chain conditional random field for ranking meeting utterances by importance </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we discuss the problem of estimating the class posterior probability of each utterance in sequence in order to extract the most probable ones, and show that the cost assigned by acrf to each utterance needs to be locally normalized in order to outperform bns.
</prevsent>
<prevsent>after analyzing the predictive power of large set of dura tional, acoustical, lexical, structural, and information retrieval features, we perform feature selection to have competitive set of predictors to test the different models.
</prevsent>
</prevsection>
<citsent citstr=" W04-1013 ">
empirical evaluations using two standard summarization metrics the pyramid method (nenkova and passonneau, 2004<papid> N04-1019 </papid>b) and rouge (lin, 2004)<papid> W04-1013 </papid>show that the best performing system is crf incorporating bothorder-2 markov dependencies and skip-chain dependencies, which achieves 91.3% of human performance in pyramid score, and outperforms our best-performing non-sequential model by 3.9%.</citsent>
<aftsection>
<nextsent>the work presented here was applied to the icsi meeting corpus (janin et al, 2003), corpus of naturally-occurring?
</nextsent>
<nextsent>meetings, i.e. meetings that would have taken place anyway.
</nextsent>
<nextsent>their styleis quite informal, and topics are primarily concerned with speech, natural language, artificial 364intelligence, and networking research.
</nextsent>
<nextsent>the corpus contains 75 meetings, which are 60 minutes long on average, and involve number of participants ranging from 3 to 10 (6 on average).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3495">
<title id=" W06-1643.xml">a skip chain conditional random field for ranking meeting utterances by importance </title>
<section> corpus.  </section>
<citcontext>
<prevsection>
<prevsent>for automatic recognition, we used the icsi sri-uw speech recognition system (mirghaforiet al, 2004), state-of-the-art conversational telephone speech (cts) recognizer whose language and acoustic models were adapted to the meetingdomain.
</prevsent>
<prevsent>it achieves 34.8% wer on the icsi corpus, which is indicative of the difficulty involved in processing meetings automatically.
</prevsent>
</prevsection>
<citsent citstr=" W04-2319 ">
we also used additional annotation that has been developed to support higher-level analyses of meeting structure, in particular the icsi meeting recorder dialog act (mrda) corpus (shriberg et al., 2004).<papid> W04-2319 </papid></citsent>
<aftsection>
<nextsent>dialog act (da) labels describe the pragmatic function of utterances, e.g. statement or backchannel.
</nextsent>
<nextsent>this auxiliary corpus consists of over 180,000 human-annotated dialog act labels (?
</nextsent>
<nextsent>= .8), for which so-calledadjacency pair (ap) relations (e.g., apologydownplay) were also labeled.
</nextsent>
<nextsent>this latter annotation was used to train an ap classifier that is instrumental in automatically determining the structure of our sequence models.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3496">
<title id=" W06-1643.xml">a skip chain conditional random field for ranking meeting utterances by importance </title>
<section> corpus.  </section>
<citcontext>
<prevsection>
<prevsent>this latter annotation was used to train an ap classifier that is instrumental in automatically determining the structure of our sequence models.
</prevsent>
<prevsent>note that, in the caseof three or more speakers, adjacency pair is admittedly an unfortunate term, since labeled aps are generally not adjacent (e.g., see table 1), but we will nevertheless use the same terminology to enforce consistency with previous work.
</prevsent>
</prevsection>
<citsent citstr=" W05-0905 ">
to train and evaluate our summarizer, we used corpus of extractive summaries produced at the university of edinburgh (murray et al, 2005).<papid> W05-0905 </papid></citsent>
<aftsection>
<nextsent>for each of the 75 meetings, human judges were asked to select transcription utterances segmented by da to include in summaries, resulting in an average compression ratio of 6.26% (though no strict limit was imposed).
</nextsent>
<nextsent>inter-labeler agreement was measured using six meetings that were summarized by multiple coders (average ? = .323).
</nextsent>
<nextsent>while this level of agreement is quite low, this situation is not uncommon to summarization, since there may be many good summaries forgiven document; main challenge lies in using evaluation schemes that properly accounts for this diversity.
</nextsent>
<nextsent>state sequence markov models such as hidden markov models (rabiner, 1989) have been highly successful in many speech and natural language processing applications, including summarization.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3497">
<title id=" W06-1643.xml">a skip chain conditional random field for ranking meeting utterances by importance </title>
<section> content selection.  </section>
<citcontext>
<prevsection>
<prevsent>state sequence markov models such as hidden markov models (rabiner, 1989) have been highly successful in many speech and natural language processing applications, including summarization.
</prevsent>
<prevsent>following an intuition that the probability of given sentence may be locally conditioned on the previous one, conroy (2004) built hmm-based summarizer that consistently ranked among the top systems in recent document understanding conference (duc) evaluations.inter-sentential influences become more complex in the case of dialogues or correspondences, especially when they involve multiple parties.
</prevsent>
</prevsection>
<citsent citstr=" J02-4003 ">
in the case of summarization of conversational speech, zechner (2002) <papid> J02-4003 </papid>found, for instance, that simple technique consisting of linking together questions and answers in summaries and thus preventing the selection of orphan questions or answers significantly improved their readability according to various human summary evaluations.</citsent>
<aftsection>
<nextsent>in email summarization (rambow et al, 2004),<papid> N04-4027 </papid>shrestha and mckeown (2004) <papid> C04-1128 </papid>obtained good performance in automatic detection of questions and answers, which can help produce summaries that highlight or focus on the question and answer exchange.</nextsent>
<nextsent>in combined chat and email summarization task, technique (zhou and hovy, 2005) <papid> P05-1037 </papid>consisting of identifying aps and appending any relevant responses to topic initiating messages was instrumental in outperforming two competitive summarization baselines.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3498">
<title id=" W06-1643.xml">a skip chain conditional random field for ranking meeting utterances by importance </title>
<section> content selection.  </section>
<citcontext>
<prevsection>
<prevsent>following an intuition that the probability of given sentence may be locally conditioned on the previous one, conroy (2004) built hmm-based summarizer that consistently ranked among the top systems in recent document understanding conference (duc) evaluations.inter-sentential influences become more complex in the case of dialogues or correspondences, especially when they involve multiple parties.
</prevsent>
<prevsent>in the case of summarization of conversational speech, zechner (2002) <papid> J02-4003 </papid>found, for instance, that simple technique consisting of linking together questions and answers in summaries and thus preventing the selection of orphan questions or answers significantly improved their readability according to various human summary evaluations.</prevsent>
</prevsection>
<citsent citstr=" N04-4027 ">
in email summarization (rambow et al, 2004),<papid> N04-4027 </papid>shrestha and mckeown (2004) <papid> C04-1128 </papid>obtained good performance in automatic detection of questions and answers, which can help produce summaries that highlight or focus on the question and answer exchange.</citsent>
<aftsection>
<nextsent>in combined chat and email summarization task, technique (zhou and hovy, 2005) <papid> P05-1037 </papid>consisting of identifying aps and appending any relevant responses to topic initiating messages was instrumental in outperforming two competitive summarization baselines.</nextsent>
<nextsent>the need to model pragmatic influences, suchas between question and an answer, is also prevalent in meeting summarization.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3499">
<title id=" W06-1643.xml">a skip chain conditional random field for ranking meeting utterances by importance </title>
<section> content selection.  </section>
<citcontext>
<prevsection>
<prevsent>following an intuition that the probability of given sentence may be locally conditioned on the previous one, conroy (2004) built hmm-based summarizer that consistently ranked among the top systems in recent document understanding conference (duc) evaluations.inter-sentential influences become more complex in the case of dialogues or correspondences, especially when they involve multiple parties.
</prevsent>
<prevsent>in the case of summarization of conversational speech, zechner (2002) <papid> J02-4003 </papid>found, for instance, that simple technique consisting of linking together questions and answers in summaries and thus preventing the selection of orphan questions or answers significantly improved their readability according to various human summary evaluations.</prevsent>
</prevsection>
<citsent citstr=" C04-1128 ">
in email summarization (rambow et al, 2004),<papid> N04-4027 </papid>shrestha and mckeown (2004) <papid> C04-1128 </papid>obtained good performance in automatic detection of questions and answers, which can help produce summaries that highlight or focus on the question and answer exchange.</citsent>
<aftsection>
<nextsent>in combined chat and email summarization task, technique (zhou and hovy, 2005) <papid> P05-1037 </papid>consisting of identifying aps and appending any relevant responses to topic initiating messages was instrumental in outperforming two competitive summarization baselines.</nextsent>
<nextsent>the need to model pragmatic influences, suchas between question and an answer, is also prevalent in meeting summarization.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3500">
<title id=" W06-1643.xml">a skip chain conditional random field for ranking meeting utterances by importance </title>
<section> content selection.  </section>
<citcontext>
<prevsection>
<prevsent>in the case of summarization of conversational speech, zechner (2002) <papid> J02-4003 </papid>found, for instance, that simple technique consisting of linking together questions and answers in summaries and thus preventing the selection of orphan questions or answers significantly improved their readability according to various human summary evaluations.</prevsent>
<prevsent>in email summarization (rambow et al, 2004),<papid> N04-4027 </papid>shrestha and mckeown (2004) <papid> C04-1128 </papid>obtained good performance in automatic detection of questions and answers, which can help produce summaries that highlight or focus on the question and answer exchange.</prevsent>
</prevsection>
<citsent citstr=" P05-1037 ">
in combined chat and email summarization task, technique (zhou and hovy, 2005) <papid> P05-1037 </papid>consisting of identifying aps and appending any relevant responses to topic initiating messages was instrumental in outperforming two competitive summarization baselines.</citsent>
<aftsection>
<nextsent>the need to model pragmatic influences, suchas between question and an answer, is also prevalent in meeting summarization.
</nextsent>
<nextsent>in fact, question answer pairs are not the only discourse relations that we need to preserve in order to create coherent summaries, and, as we will see, most instances of aps would need to be preserved together, either inside or outside the summary.
</nextsent>
<nextsent>table 1 displays an ap construction with one statement (a part) and three respondents (b parts).
</nextsent>
<nextsent>this example illustrates that the number of turns between constituents of aps is variable and thus difficult to model with standard sequence models.this example also illustrates some of the predictors investigated in this paper.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3501">
<title id=" W06-1643.xml">a skip chain conditional random field for ranking meeting utterances by importance </title>
<section> content selection.  </section>
<citcontext>
<prevsection>
<prevsent>note that the second table is not symmetric, because the data allows an part to be linked to multiple parts, but notvice-versa.
</prevsent>
<prevsent>while counts in table 2 reflect human labels, we only use automatically predicted (s, d) pairs in the experiments of the remaining part of this paper.
</prevsent>
</prevsection>
<citsent citstr=" P04-1085 ">
to find these pairs automatically, we trained non-sequential log-linear model that achieves .902 accuracy (galley et al, 2004).<papid> P04-1085 </papid></citsent>
<aftsection>
<nextsent>in this paper, we investigate conditional models for paired sequences of observations and labels.
</nextsent>
<nextsent>inthe case of utterance selection, the observation sequence = x1:t = (x1, . . .
</nextsent>
<nextsent>, xt ) represents local state men x 1 2 3 4 5 back channel state men tst atem ent state men y 1 2 3 4 5 figure 1: skip-chain crf with pragmatic-level links.
</nextsent>
<nextsent>linear-chain edges yt = 1 yt = 1 yt1 = 1 529 7742 yt1 = 1 7742 116040 skip-chain edges yd = 1 yd = 1 ys = 1 6792 2191 ys = 1 1479 121591 table 2: contingency tables: while the correlation between adjacent labels yt1 and yt is not significant (2 = 2.3,   .05), empirical evidence clearly shows that ys and yd influence each other (2 = 78948,   .001).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3502">
<title id=" W06-1643.xml">a skip chain conditional random field for ranking meeting utterances by importance </title>
<section> skip-chain sequence models.  </section>
<citcontext>
<prevsection>
<prevsent>j=1 jfj(x,ytk:t, t) log skip(x, yst , yt, t) = ??
</prevsent>
<prevsent>j=1 jgj(x, yst , yt, st, t) 4.1 inference and parameter estimation.
</prevsent>
</prevsection>
<citsent citstr=" P05-1045 ">
our crf and bn models were designed using mallet (mccallum, 2002), which provides tools for training log-linear models with l-bfgsoptimization techniques and maximize the log likelihood of our training datad = (x(i),y(i)) i=1, and provides probabilistic inference algorithms for linear-chain bns and crfs.most previous work with crfs containing non local dependencies used approximate probabilistic inference techniques, including trp (sutton and mccallum, 2004) and gibbs sampling (finkel et al, 2005).<papid> P05-1045 </papid></citsent>
<aftsection>
<nextsent>approximation is needed whenthe junction tree of graphical model is associated with prohibitively large cliques.
</nextsent>
<nextsent>for example, the worse case reported in (sutton and mccallum, 2004) is clique of 61 nodes.
</nextsent>
<nextsent>in the case of skip-chain models representing aps, the inference problem is somewhat simpler: loops in the graph are relatively short, 98% of ap edges span no more than 5 time slices, and the maximum clique size in the entire data is 5.
</nextsent>
<nextsent>while exact inference might be possible in our case, we used the simpler approach of adapting standard inference algorithms for linear-chain models.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3503">
<title id=" W06-1643.xml">a skip chain conditional random field for ranking meeting utterances by importance </title>
<section> skip-chain sequence models.  </section>
<citcontext>
<prevsection>
<prevsent>in the case of skip-chain models representing aps, the inference problem is somewhat simpler: loops in the graph are relatively short, 98% of ap edges span no more than 5 time slices, and the maximum clique size in the entire data is 5.
</prevsent>
<prevsent>while exact inference might be possible in our case, we used the simpler approach of adapting standard inference algorithms for linear-chain models.
</prevsent>
</prevsection>
<citsent citstr=" N03-1028 ">
specifically, to account for skip-edges, we used technique inspired by (sha and pereira, 2003), <papid> N03-1028 </papid>in which multiple state dependencies, such as anorder-2 markov model, are encoded using auxiliary tags.</citsent>
<aftsection>
<nextsent>for instance, an order-2 markov model is parameterized using state triples yt2:t, and each possible triple is converted to label zt = yt2:t. using these auxiliary labels only, we can then use the standard forward-backward algorithm for computing marginal distributions in linear-chain crfs, and viterbi decoding in linear-chain crfs and bns.
</nextsent>
<nextsent>the only requirement is to ensure that transition between zt and zt+1 is forbidden if the sub-states yt1:t common to both states differ, i.e., is assigned an infinite cost.
</nextsent>
<nextsent>this approach can be extended to the case of skip-chain transitions.for instance, an order-1 markov model with skip edges can be constructed using zt = (yst , yt1, yt) triples, where the first element yst represents the label at the source of the skip-edge.
</nextsent>
<nextsent>similarly to the case of order-2 markov models, we need to ensure that only valid sequences of labels are considered, which is trivial to enforce if we assume that no skip edge ranges more than predefined threshold of time slices.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3504">
<title id=" W06-1643.xml">a skip chain conditional random field for ranking meeting utterances by importance </title>
<section> ranking utterances by importance.  </section>
<citcontext>
<prevsection>
<prevsent>min/max/mean/median/stddev/onset/outset f0 of utterance t, and of first and last word ? min/max/mean/stddev energy ? .05, .25, .5, .75, .95 quan tiles of f0 and energy ? pitch range ? f0 mean absolute slope dura tional and structural features: ? duration of the previous/current/next utterance ? relative position within meeting (i.e., index t) ? relative position within speaker turn?
</prevsent>
<prevsent>large number of structural predicates, i.e. is the previous utterance of the same speaker??
</prevsent>
</prevsection>
<citsent citstr=" P94-1002 ">
number of aps initiated in yt discourse features: ? lexical cohesion score (for topic shifts) (hearst, 1994) ? <papid> P94-1002 </papid>first and second word of utterance, if in cue word list ? number of pronouns ? number of fillers and fluency devices (e.g., uh?, um?)</citsent>
<aftsection>
<nextsent>number of back channel and acknowledgment tokens (e.g., uh-huh?, ok?, right?)table 3: features for extractive summarization.
</nextsent>
<nextsent>unless otherwise mentioned, we refer to features of utterance whose label yt we are trying to predict.
</nextsent>
<nextsent>we started our analyses with large collection of features found to be good predictors in either speech (inoue et al, 2004; maskey and hirschberg, 2005; murray et al, 2005) <papid> W05-0905 </papid>or text summarization (mani and maybury, 1999).</nextsent>
<nextsent>our goal is to build very competitive feature set that capitalizes on recent advances in summarization of both genres.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3523">
<title id=" W06-1643.xml">a skip chain conditional random field for ranking meeting utterances by importance </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>there maining 69 meetings were used for training, which represent in total more than 103,000 training instances (or da units), of which 6,464 are positives (6.24%).
</prevsent>
<prevsent>the multi-reference test set contains more than 28,000 instances.the goal of preliminary experiment was to devise set of useful predictors from full set of1171.
</prevsent>
</prevsection>
<citsent citstr=" J96-1002 ">
we performed feature selection by incrementally growing log-linear model with order0 features f(x, yt) using forward feature selection procedure similar to (berger et al, 1996).<papid> J96-1002 </papid></citsent>
<aftsection>
<nextsent>probably due to the imbalance between positive and negative samples, we found it more effective to rank candidate features by gains in -measure(through 5-fold cross validation on the entire training set).
</nextsent>
<nextsent>the increase inf1 by adding new features to the model is displayed in table 4; this greedy search resulted in set of 217 features.
</nextsent>
<nextsent>we now analyze the performance of different sequence models on our test set.
</nextsent>
<nextsent>the target length of each summary was set to 12.7% of the number of words of the full document, which is the aver 370 age on the entire training data (the average on the test data is 12.9%).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3532">
<title id=" W06-2909.xml">semantic role labeling via tree kernel joint inference </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for this purpose, we need to extract features from the sentences syntactic parse tree that encodes the target semantic structure.
</prevsent>
<prevsent>this task is rather complex since we do not exactly know which are the syntactic clues that capture the relation between the predicate and its arguments.
</prevsent>
</prevsection>
<citsent citstr=" W04-3222 ">
for example, to detect the interesting context, the modeling of syntax/semantics-based features should take into account linguistic aspects like ancestor nodes or semantic dependencies (toutanova et al, 2004).<papid> W04-3222 </papid></citsent>
<aftsection>
<nextsent>a viable approach to generate large number of features has been proposed in (collins and duffy,2002), <papid> P02-1034 </papid>where convolution kernels were used to implicitly define tree substructure space.</nextsent>
<nextsent>the selection of the relevant structural features was left to the voted perceptron learning algorithm.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3533">
<title id=" W06-2909.xml">semantic role labeling via tree kernel joint inference </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this task is rather complex since we do not exactly know which are the syntactic clues that capture the relation between the predicate and its arguments.
</prevsent>
<prevsent>for example, to detect the interesting context, the modeling of syntax/semantics-based features should take into account linguistic aspects like ancestor nodes or semantic dependencies (toutanova et al, 2004).<papid> W04-3222 </papid></prevsent>
</prevsection>
<citsent citstr=" P02-1034 ">
a viable approach to generate large number of features has been proposed in (collins and duffy,2002), <papid> P02-1034 </papid>where convolution kernels were used to implicitly define tree substructure space.</citsent>
<aftsection>
<nextsent>the selection of the relevant structural features was left to the voted perceptron learning algorithm.
</nextsent>
<nextsent>such successful experimentation shows that tree kernels are very promising for automatic feature engineering, especially when the available knowledge about the phenomenon is limited.
</nextsent>
<nextsent>in similar way, we can model srl systems with tree kernels to generate large feature spaces.
</nextsent>
<nextsent>morein detail, most srl systems split the labeling process into two different steps: boundary detection (i.e. to determine the text boundaries of predicate arguments) and role classification (i.e. labeling such arguments with semantic role, e.g. arg0 or arg1 as defined in (kingsbury and palmer, 2002)).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3534">
<title id=" W06-2909.xml">semantic role labeling via tree kernel joint inference </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the former relates to the detection of syntactic parse tree nodes associated with constituents that correspond to arguments, whereas the latter considers the boundary nodes for the assignment of the suitable label.
</prevsent>
<prevsent>both steps require the design and extraction of features from parse trees.
</prevsent>
</prevsection>
<citsent citstr=" P05-1072 ">
as capturing the tightly interdependent relations among predicate and its arguments is complex task, we can apply tree kernels on the subtrees that span the whole predicate argument structure to generate the feature space of all the possible subtrees.in this paper, we apply the traditional boundary (tbc) and role (trc) classifiers (pradhanet al, 2005<papid> P05-1072 </papid>a), which are based on binary predi cate/argument relations, to label all parse tree nodes corresponding to potential arguments.</citsent>
<aftsection>
<nextsent>then, we ex 61 tract the subtrees which span the predicate-argument dependencies of such arguments, i.e. argument spanning trees (ast s).
</nextsent>
<nextsent>these are used in tree kernel function to generate all possible substructures that encode n-ary argument relations, i.e. we carry out an automatic feature engineering process.
</nextsent>
<nextsent>to validate our approach, we experimented withour model and support vector machines for the classification of valid and invalid ast s. the results show that this classification problem can be learned with high accuracy.
</nextsent>
<nextsent>moreover, we modeled srl as re-ranking task in line with (toutanova et al, 2005).<papid> P05-1073 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3535">
<title id=" W06-2909.xml">semantic role labeling via tree kernel joint inference </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these are used in tree kernel function to generate all possible substructures that encode n-ary argument relations, i.e. we carry out an automatic feature engineering process.
</prevsent>
<prevsent>to validate our approach, we experimented withour model and support vector machines for the classification of valid and invalid ast s. the results show that this classification problem can be learned with high accuracy.
</prevsent>
</prevsection>
<citsent citstr=" P05-1073 ">
moreover, we modeled srl as re-ranking task in line with (toutanova et al, 2005).<papid> P05-1073 </papid></citsent>
<aftsection>
<nextsent>the large number of complex features provided by tree kernels for structured learning allows svms to reach the state-of-the-art accuracy.the paper is organized as follows: section 2 introduces the semantic role labeling based on svmsand the tree kernel spaces; section 3 formally defines the ast and the algorithm for their classification and re-ranking; section 4 shows the comparative results between our approach and the traditional one; section 5 presents the related work; and finally, section 6 summarizes the conclusions.
</nextsent>
<nextsent>in the last years, several machine learning approaches have been developed for automatic role labeling, e.g.
</nextsent>
<nextsent>(gildea and jurasfky, 2002; pradhan et al, 2005<papid> P05-1072 </papid>a).</nextsent>
<nextsent>their common characteristic is the adoption of attribute-value representations forpredicate-argument structures.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3543">
<title id=" W06-2909.xml">semantic role labeling via tree kernel joint inference </title>
<section> semantic role labeling.  </section>
<citcontext>
<prevsection>
<prevsent>these sets can be directly used to train boundary classifier (e.g. an svm).
</prevsent>
<prevsent>regarding the argument type classifier, binary labeler for role r(e.g. an svm) can be trained on the t+r , i.e. its positive examples and tr , i.e. its negative examples,where t+ = t+r ? tr , according to the one-vs all scheme.
</prevsent>
</prevsection>
<citsent citstr=" P04-1043 ">
the binary classifiers are then usedto build general role multi classifier by simply selecting the argument associated with the maximum among the svm scores.regarding the design of features for predicate argument pairs, we can use the attribute-values defined in (gildea and jurasfky, 2002) or tree structures (moschitti, 2004).<papid> P04-1043 </papid></citsent>
<aftsection>
<nextsent>although we focus onthe latter approach, short description of the former is still relevant as they are used by tbc and trc.
</nextsent>
<nextsent>they include the phrase type, predicate word, head word, governing category, position and voice features.
</nextsent>
<nextsent>for example, the phrase type indicates the syntactic type of the phrase labeled asa predicate argument and the parse tree path contains the path in the parse tree between the predicate and the argument phrase, expressed as sequence of nonterminal labels linked by direction (up or down) symbols, e.g. ? vp ? np.a viable alternative to manual design of syntactic features is the use of tree-kernel functions.
</nextsent>
<nextsent>these implicitly define feature space based on all possible tree substructures.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3559">
<title id=" W06-2909.xml">semantic role labeling via tree kernel joint inference </title>
<section> tree kernel-based classification of.  </section>
<citcontext>
<prevsection>
<prevsent>these include the features for each arguments (gildea and juras fky, 2002) and global features like the sequence of argument labels, e.g. arg0, arg1, argm?.
</prevsent>
<prevsent>finally, we prepare the training examples for the re-ranker considering the best annotations of each predicate structure.
</prevsent>
</prevsection>
<citsent citstr=" W03-1012 ">
we use the approach adopted in (shen et al, 2003), <papid> W03-1012 </papid>which generates all possible pairs from the examples, i.e.</citsent>
<aftsection>
<nextsent>(m2 ) pairs.
</nextsent>
<nextsent>each pairis assigned to positive example if the first member of the pair has higher score than the second member.
</nextsent>
<nextsent>the score that we use is the f1 measure of the annotated structure with respect to the goldstandard.
</nextsent>
<nextsent>more in detail, given training/testing examples ei = t1i , t2i , v1i , v2i ?, where t1i and t2i are twoast and v1i and v2i are two feature vectors associated with two candidate predicate structures s1 and s2, we define the following kernels: 1) ktr(e1, e2) = kt(t11, t12) +kt(t21, t22) kt(t11, t22)kt(t21, t12), where tji is the j-th ast of the pair ei, kt is the tree kernel function defined in section 2 and i, ? {1, 2}.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3566">
<title id=" W06-2909.xml">semantic role labeling via tree kernel joint inference </title>
<section> the experiments.  </section>
<citcontext>
<prevsection>
<prevsent>the resulting f1 of 88.59% is the upper bound of our approach.second, we selected the top ranked annotation indicated by the viterbis algorithm.
</prevsent>
<prevsent>this provides our baseline f1 measure, i.e. 75.91%.
</prevsent>
</prevsection>
<citsent citstr=" W05-0630 ">
such outcome is slightly higher than our official conll result (mos chitti et al, 2005) <papid> W05-0630 </papid>obtained without converting svm scores into probabilities.</citsent>
<aftsection>
<nextsent>third, we applied the svm re-ranker to select 2with the aim of improving the state-of-the-art, we applied the polynomial kernel for all basic classifiers, at this time.
</nextsent>
<nextsent>we used the models developed during our participation to the conll 2005 shared task (moschitti et al, 2005).<papid> W05-0630 </papid></nextsent>
<nextsent>the best structures according to the core roles.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3568">
<title id=" W06-2909.xml">semantic role labeling via tree kernel joint inference </title>
<section> the experiments.  </section>
<citcontext>
<prevsection>
<prevsent>this confirms that the classification of the non-core roles is more complex than the other arguments.finally, the high computation time of there ranker prevented us to use the larger structures which include all arguments.
</prevsent>
<prevsent>the major complexity issue was the slow training and classification time of svms.
</prevsent>
</prevsection>
<citsent citstr=" E06-1015 ">
the time needed for tree kernel function was not so problematic as we could use the fast evaluation proposed in (moschitti, 2006).<papid> E06-1015 </papid></citsent>
<aftsection>
<nextsent>this roughly reduces the computation time to the one required by polynomial kernel.
</nextsent>
<nextsent>the real burden is therefore the learning time of svms that is quadratic in the number of training instances.
</nextsent>
<nextsent>for example, to carry out the re-ranking experiments required approximately one month of 64 bits machine (2.4 ghz and 4gb ram).
</nextsent>
<nextsent>to solve this problem, we are going to study the impact on the accuracy of fast learning algorithms such as the voted perceptron.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3575">
<title id=" W06-2606.xml">reranking translation hypotheses using structural properties </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we introduce several methods that try to establish this kind of linkage between the words of hypothesis and, thus, determine its well-formedness, or fluency?.
</prevsent>
<prevsent>we perform rescoring experiments that rerank n-best lists according to the presented framework.
</prevsent>
</prevsection>
<citsent citstr=" J99-2004 ">
as methodologies deriving well-formedness of sentence we use super tagging (bangalore and joshi, 1999) <papid> J99-2004 </papid>with lightweight dependency analysis (lda)1 (bangalore, 2000), link grammars(sleator and temperley, 1993) and maximum entropy (me) based chunk parser (bender et al, 2003).<papid> E03-1055 </papid></citsent>
<aftsection>
<nextsent>the former two approaches explicitly model the syntactic dependencies between words.
</nextsent>
<nextsent>each hypothesis that contains irregularities, such as broken linkages or non-satisfied dependencies, should be penalized or rejected accordingly.
</nextsent>
<nextsent>forthe me chunker, the idea is to train n-gram models on the chunk or pos sequences and directly use the log-probability as feature score.
</nextsent>
<nextsent>in general, these concepts and the underlying programs should be robust and fast in order to be able to cope with large amounts of data (as it is the case for n-best lists).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3576">
<title id=" W06-2606.xml">reranking translation hypotheses using structural properties </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we introduce several methods that try to establish this kind of linkage between the words of hypothesis and, thus, determine its well-formedness, or fluency?.
</prevsent>
<prevsent>we perform rescoring experiments that rerank n-best lists according to the presented framework.
</prevsent>
</prevsection>
<citsent citstr=" E03-1055 ">
as methodologies deriving well-formedness of sentence we use super tagging (bangalore and joshi, 1999) <papid> J99-2004 </papid>with lightweight dependency analysis (lda)1 (bangalore, 2000), link grammars(sleator and temperley, 1993) and maximum entropy (me) based chunk parser (bender et al, 2003).<papid> E03-1055 </papid></citsent>
<aftsection>
<nextsent>the former two approaches explicitly model the syntactic dependencies between words.
</nextsent>
<nextsent>each hypothesis that contains irregularities, such as broken linkages or non-satisfied dependencies, should be penalized or rejected accordingly.
</nextsent>
<nextsent>forthe me chunker, the idea is to train n-gram models on the chunk or pos sequences and directly use the log-probability as feature score.
</nextsent>
<nextsent>in general, these concepts and the underlying programs should be robust and fast in order to be able to cope with large amounts of data (as it is the case for n-best lists).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3578">
<title id=" W06-2606.xml">reranking translation hypotheses using structural properties </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the corpora and the experiments are discussed in section 4.
</prevsent>
<prevsent>the paper is concluded in section 5.
</prevsent>
</prevsection>
<citsent citstr=" N04-1021 ">
in (och et al, 2004), <papid> N04-1021 </papid>the effects of integrating syntactic structure into state-of-the-art statistical machine translation system are investigated.</citsent>
<aftsection>
<nextsent>the approach is similar to the approach presented here: 1in the context of this work, the term lda is not to be confused with linear discriminant analysis.
</nextsent>
<nextsent>41firstly, word graph is generated using the baseline smt system and n-best lists are extracted accordingly, then additional feature functions representing syntactic knowledge are added and the corresponding scaling factors are trained discrimina tively on development n-best list.
</nextsent>
<nextsent>och and colleagues investigated large amount of different feature functions.
</nextsent>
<nextsent>the field of application varies from simple syntactic features, suchas ibm model 1 score, over shallow parsing techniques to more complex methods using grammars and intricate parsing procedures.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3580">
<title id=" W06-2606.xml">reranking translation hypotheses using structural properties </title>
<section> framework.  </section>
<citcontext>
<prevsection>
<prevsent>the first probability on the right-hand side of the equation denotes the translation model whereas the second is the target language model.
</prevsent>
<prevsent>an alternative to this classical source-channel approach is the direct modeling of the posterior probability pr(ei1|fj1 ) which is utilized here.
</prevsent>
</prevsection>
<citsent citstr=" P02-1038 ">
using log-linear model (och and ney, 2002), <papid> P02-1038 </papid>we obtain pr(ei1|fj1 ) = exp ( m=1 mhm(ei1, fj1 ) ) ? ei1 exp ( m=1 mhm(ei ? 1 , fj1 ) ) , (2)where are the scaling factors of the models denoted by feature functions hm(?).</citsent>
<aftsection>
<nextsent>the denominator represents normalization factor that depends only on the source sentence fj1 . therefore, we can omit it during the search process, leading to the following decision rule: ei1 = argmax i,ei1 { ? m=1 mhm(ei1, fj1 ) } (3)this approach is generalization of the source channel approach.
</nextsent>
<nextsent>it has the advantage that additional models h(?)
</nextsent>
<nextsent>can be easily integrated into the overall system.
</nextsent>
<nextsent>the model scaling factorsm1 are trained according to the maximum entropy principle, e.g., using the gis algorithm.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3581">
<title id=" W06-2606.xml">reranking translation hypotheses using structural properties </title>
<section> framework.  </section>
<citcontext>
<prevsection>
<prevsent>can be easily integrated into the overall system.
</prevsent>
<prevsent>the model scaling factorsm1 are trained according to the maximum entropy principle, e.g., using the gis algorithm.
</prevsent>
</prevsection>
<citsent citstr=" P03-1021 ">
alternatively, one can train them with respect to the final translation quality measured by an error criterion (och, 2003).<papid> P03-1021 </papid></citsent>
<aftsection>
<nextsent>for the results reported in this paper, we optimized the scaling factors with respect to linear interpolation of word error rate (wer), position-independent word error rate (per), bleu and nist score using the downhill simplex algorithm (press et al, 2002).
</nextsent>
<nextsent>3.2 supertagging/lda.
</nextsent>
<nextsent>super tagging (bangalore and joshi, 1999) <papid> J99-2004 </papid>uses the lexicalized tree adjoining grammar formalism(ltag) (xtag research group, 2001).</nextsent>
<nextsent>tree adjoining grammars incorporate tree-rewriting formalism using elementary trees that can be combined by two operations, namely substitution and adjunction, to derive more complex tree structures of the sentence considered.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3587">
<title id=" W06-2606.xml">reranking translation hypotheses using structural properties </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>a word graph containing the most likely translation hypotheses is generated during the search process.
</prevsent>
<prevsent>out of this compact 44 supplied data track arabic chinese japanese english train sentences 20 000 running words 180 075 176 199 198 453 189 927 vocabulary 15 371 8 687 9 277 6 870 singletons 8 319 4 006 4 431 2 888 c-star03 sentences 506 running words 3 552 3 630 4 130 3 823 oovs (running words) 133 114 61 65 iwslt04 sentences 500 running words 3 597 3 681 4 131 3 837 oovs (running words) 142 83 71 58 table 1: corpus statistics after preprocessing.
</prevsent>
</prevsection>
<citsent citstr=" W05-0834 ">
representation, we extract n-best lists as described in (zens and ney, 2005).<papid> W05-0834 </papid></citsent>
<aftsection>
<nextsent>these n-best lists serveas starting point for our experiments.
</nextsent>
<nextsent>the methods presented in section 3 produce scores that are used as additional features for the n-best lists.
</nextsent>
<nextsent>4.1 corpora.
</nextsent>
<nextsent>the experiments are carried out on subset of the basic travel expression corpus (btec)(takezawa et al, 2002), as it is used for the supplied data track condition of the iwslt evaluation campaign.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3590">
<title id=" W06-3309.xml">generative content models for structural analysis of medical abstracts </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>as an example, scientific abstracts across many different fields generally follow the pattern of introduction?,methods?, results?, and conclusions?
</prevsent>
<prevsent>(salanger meyer, 1990; swales, 1990; orasan, 2001).
</prevsent>
</prevsection>
<citsent citstr=" W00-1302 ">
the ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization (teufel and moens, 2000), <papid> W00-1302 </papid>information retrieval (tbahriti et al, 2005), information extraction (mizuta et al, 2005), and question answering.</citsent>
<aftsection>
<nextsent>although there is trend towards analysis of full article texts, we believe that abstracts still provide tremendous amount of information, and much value can still be extracted from them.
</nextsent>
<nextsent>for example, gay et al (2005) experimented with abstracts and full article texts inthe task of automatically generating index term recommendations and discovered that using full article texts yields at most 7.4% improvement in f-score.
</nextsent>
<nextsent>demner-fushman et al (2005) found correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.
</nextsent>
<nextsent>this paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four-section pattern discussed above (salanger-meyer, 1990).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3591">
<title id=" W06-3309.xml">generative content models for structural analysis of medical abstracts </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>mcknight and srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.
</prevsent>
<prevsent>building on the work of ruch et al (2003) in the same domain, we present generative approach that attempts to directly mode lthe discourse structure of medline abstracts using hidden markov models (hmms); cf.
</prevsent>
</prevsection>
<citsent citstr=" N04-1015 ">
(barzilayand lee, 2004).<papid> N04-1015 </papid></citsent>
<aftsection>
<nextsent>although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well.
</nextsent>
<nextsent>discriminative approaches (especially svms) have been shown to be very effective for many supervised classification tasks; see, for example, (joachims, 1998; ng and jordan, 2001).
</nextsent>
<nextsent>how ever, their high computational complexity (quadraticin the number of training samples) renders them prohibitive for massive data processing.
</nextsent>
<nextsent>under certain conditions, generative approaches with linear complexity are preferable, even if their performance islower than that which can be achieved through discriminative training.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3595">
<title id=" W06-3309.xml">generative content models for structural analysis of medical abstracts </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>whereas barzilay and lee evaluated their work inthe context of document summarization, the four part structure of medical abstracts allows us to conduct meaningful intrinsic evaluations and focus on the sentence classification task.
</prevsent>
<prevsent>nevertheless, their work bolsters our claims regarding the usefulness of generative models in extrinsic tasks, which we do not describe here.
</prevsent>
</prevsection>
<citsent citstr=" P02-1047 ">
although this study falls under the general topicof discourse modeling, our work differs from previous attempts to characterize text in terms of domain independent rhetorical elements (mckeown, 1985; marcu and echihabi, 2002).<papid> P02-1047 </papid></citsent>
<aftsection>
<nextsent>our task is closer to the work of teufel and moens (2000), <papid> W00-1302 </papid>who looked at the problem of intellectual attribution in scientific texts.</nextsent>
<nextsent>we believe that there are two contributions as result of our work.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3597">
<title id=" W06-1320.xml">an analysis of quantitative aspects in the evaluation of thematic segmentation algorithms </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>giving rigorous definition of the notion of topic is difficult, but the task of discourse/dialogue segmentation into thematic episodes is usually described by invoking an intuitive notion of topic?
</prevsent>
<prevsent>(brown and yule, 1998).thematic segmentation also relates to several notions such as speakers intention, topic flow and cohesion.
</prevsent>
</prevsection>
<citsent citstr=" P96-1038 ">
since it is elusive what mental representations humans use in order to distinguish coherent text, different surface markers (hirschberg and nakatani, 1996; <papid> P96-1038 </papid>passonneau and litman, 1997)<papid> J97-1005 </papid>and external knowledge sources (kozima and furugori, 1994) have been exploited for the purpose of automatic thematic segmentation.</citsent>
<aftsection>
<nextsent>halliday and hasan (1976) claim that the text meaning is realised through certain language resources and they refer to these resources by the term of cohesion.
</nextsent>
<nextsent>the major classes of such text-forming resources identified in (halliday and hasan, 1976) are: substitution, ellipsis, conjunction, reiteration and collocation.
</nextsent>
<nextsent>in this paper, we examine one form of lexical cohesion, namely lexical reiteration.following some of the most prominent discourse theories in literature (grosz and sidner, 1986; <papid> J86-3001 </papid>marcu, 2000), hierarchical representation of the thematic episodes can be proposed.</nextsent>
<nextsent>the basis for this is the idea that topics can be recursively divided into subtopics.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3598">
<title id=" W06-1320.xml">an analysis of quantitative aspects in the evaluation of thematic segmentation algorithms </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>giving rigorous definition of the notion of topic is difficult, but the task of discourse/dialogue segmentation into thematic episodes is usually described by invoking an intuitive notion of topic?
</prevsent>
<prevsent>(brown and yule, 1998).thematic segmentation also relates to several notions such as speakers intention, topic flow and cohesion.
</prevsent>
</prevsection>
<citsent citstr=" J97-1005 ">
since it is elusive what mental representations humans use in order to distinguish coherent text, different surface markers (hirschberg and nakatani, 1996; <papid> P96-1038 </papid>passonneau and litman, 1997)<papid> J97-1005 </papid>and external knowledge sources (kozima and furugori, 1994) have been exploited for the purpose of automatic thematic segmentation.</citsent>
<aftsection>
<nextsent>halliday and hasan (1976) claim that the text meaning is realised through certain language resources and they refer to these resources by the term of cohesion.
</nextsent>
<nextsent>the major classes of such text-forming resources identified in (halliday and hasan, 1976) are: substitution, ellipsis, conjunction, reiteration and collocation.
</nextsent>
<nextsent>in this paper, we examine one form of lexical cohesion, namely lexical reiteration.following some of the most prominent discourse theories in literature (grosz and sidner, 1986; <papid> J86-3001 </papid>marcu, 2000), hierarchical representation of the thematic episodes can be proposed.</nextsent>
<nextsent>the basis for this is the idea that topics can be recursively divided into subtopics.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3599">
<title id=" W06-1320.xml">an analysis of quantitative aspects in the evaluation of thematic segmentation algorithms </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>halliday and hasan (1976) claim that the text meaning is realised through certain language resources and they refer to these resources by the term of cohesion.
</prevsent>
<prevsent>the major classes of such text-forming resources identified in (halliday and hasan, 1976) are: substitution, ellipsis, conjunction, reiteration and collocation.
</prevsent>
</prevsection>
<citsent citstr=" J86-3001 ">
in this paper, we examine one form of lexical cohesion, namely lexical reiteration.following some of the most prominent discourse theories in literature (grosz and sidner, 1986; <papid> J86-3001 </papid>marcu, 2000), hierarchical representation of the thematic episodes can be proposed.</citsent>
<aftsection>
<nextsent>the basis for this is the idea that topics can be recursively divided into subtopics.
</nextsent>
<nextsent>real texts exhibit more intricate structure, including semantic returns?
</nextsent>
<nextsent>by which topic is suspended at one point and resumed later in the discourse.
</nextsent>
<nextsent>however,we focus here on reduced segmentation problem, which involves identifying non-overlapping and non-hierarchical segments at coarse level of granularity.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3600">
<title id=" W06-1320.xml">an analysis of quantitative aspects in the evaluation of thematic segmentation algorithms </title>
<section> comparison of systems.  </section>
<citcontext>
<prevsection>
<prevsent>section 5 presents our experimental set-up and shows comparisons between the performance of different systems.
</prevsent>
<prevsent>finally, some conclusions are drawn in section 6.
</prevsent>
</prevsection>
<citsent citstr=" P03-1071 ">
combinations of different features (derived forex ample from linguistic, prosodic information) have been explored in previous studies like (galley et al., 2003) <papid> P03-1071 </papid>and (kauchak and chen, 2005).<papid> W05-0405 </papid></citsent>
<aftsection>
<nextsent>inthis paper, we selected for comparison three systems based merely on the lexical reiteration fea ture: text tiling (hearst, 1997), <papid> J97-1003 </papid>c99 (choi, 2000) <papid> A00-2004 </papid>and textseg (utiyama and isahara, 2001).<papid> P01-1064 </papid></nextsent>
<nextsent>in the following, we briefly review these approaches.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3602">
<title id=" W06-1320.xml">an analysis of quantitative aspects in the evaluation of thematic segmentation algorithms </title>
<section> comparison of systems.  </section>
<citcontext>
<prevsection>
<prevsent>section 5 presents our experimental set-up and shows comparisons between the performance of different systems.
</prevsent>
<prevsent>finally, some conclusions are drawn in section 6.
</prevsent>
</prevsection>
<citsent citstr=" W05-0405 ">
combinations of different features (derived forex ample from linguistic, prosodic information) have been explored in previous studies like (galley et al., 2003) <papid> P03-1071 </papid>and (kauchak and chen, 2005).<papid> W05-0405 </papid></citsent>
<aftsection>
<nextsent>inthis paper, we selected for comparison three systems based merely on the lexical reiteration fea ture: text tiling (hearst, 1997), <papid> J97-1003 </papid>c99 (choi, 2000) <papid> A00-2004 </papid>and textseg (utiyama and isahara, 2001).<papid> P01-1064 </papid></nextsent>
<nextsent>in the following, we briefly review these approaches.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3603">
<title id=" W06-1320.xml">an analysis of quantitative aspects in the evaluation of thematic segmentation algorithms </title>
<section> comparison of systems.  </section>
<citcontext>
<prevsection>
<prevsent>finally, some conclusions are drawn in section 6.
</prevsent>
<prevsent>combinations of different features (derived forex ample from linguistic, prosodic information) have been explored in previous studies like (galley et al., 2003) <papid> P03-1071 </papid>and (kauchak and chen, 2005).<papid> W05-0405 </papid></prevsent>
</prevsection>
<citsent citstr=" J97-1003 ">
inthis paper, we selected for comparison three systems based merely on the lexical reiteration fea ture: text tiling (hearst, 1997), <papid> J97-1003 </papid>c99 (choi, 2000) <papid> A00-2004 </papid>and textseg (utiyama and isahara, 2001).<papid> P01-1064 </papid></citsent>
<aftsection>
<nextsent>in the following, we briefly review these approaches.
</nextsent>
<nextsent>2.1 text tiling algorithm.
</nextsent>
<nextsent>the text tiling algorithm was initially developed by hearst (1997) <papid> J97-1003 </papid>for segmentation of expository texts into multi-paragraph thematic episode shaving linear, non-overlapping structure (as reflected by the name of the algorithm).</nextsent>
<nextsent>texttilingis widely used as de-facto standard in the evaluation of alternative segmentation systems, e.g.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3604">
<title id=" W06-1320.xml">an analysis of quantitative aspects in the evaluation of thematic segmentation algorithms </title>
<section> comparison of systems.  </section>
<citcontext>
<prevsection>
<prevsent>finally, some conclusions are drawn in section 6.
</prevsent>
<prevsent>combinations of different features (derived forex ample from linguistic, prosodic information) have been explored in previous studies like (galley et al., 2003) <papid> P03-1071 </papid>and (kauchak and chen, 2005).<papid> W05-0405 </papid></prevsent>
</prevsection>
<citsent citstr=" A00-2004 ">
inthis paper, we selected for comparison three systems based merely on the lexical reiteration fea ture: text tiling (hearst, 1997), <papid> J97-1003 </papid>c99 (choi, 2000) <papid> A00-2004 </papid>and textseg (utiyama and isahara, 2001).<papid> P01-1064 </papid></citsent>
<aftsection>
<nextsent>in the following, we briefly review these approaches.
</nextsent>
<nextsent>2.1 text tiling algorithm.
</nextsent>
<nextsent>the text tiling algorithm was initially developed by hearst (1997) <papid> J97-1003 </papid>for segmentation of expository texts into multi-paragraph thematic episode shaving linear, non-overlapping structure (as reflected by the name of the algorithm).</nextsent>
<nextsent>texttilingis widely used as de-facto standard in the evaluation of alternative segmentation systems, e.g.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3607">
<title id=" W06-1320.xml">an analysis of quantitative aspects in the evaluation of thematic segmentation algorithms </title>
<section> comparison of systems.  </section>
<citcontext>
<prevsection>
<prevsent>finally, some conclusions are drawn in section 6.
</prevsent>
<prevsent>combinations of different features (derived forex ample from linguistic, prosodic information) have been explored in previous studies like (galley et al., 2003) <papid> P03-1071 </papid>and (kauchak and chen, 2005).<papid> W05-0405 </papid></prevsent>
</prevsection>
<citsent citstr=" P01-1064 ">
inthis paper, we selected for comparison three systems based merely on the lexical reiteration fea ture: text tiling (hearst, 1997), <papid> J97-1003 </papid>c99 (choi, 2000) <papid> A00-2004 </papid>and textseg (utiyama and isahara, 2001).<papid> P01-1064 </papid></citsent>
<aftsection>
<nextsent>in the following, we briefly review these approaches.
</nextsent>
<nextsent>2.1 text tiling algorithm.
</nextsent>
<nextsent>the text tiling algorithm was initially developed by hearst (1997) <papid> J97-1003 </papid>for segmentation of expository texts into multi-paragraph thematic episode shaving linear, non-overlapping structure (as reflected by the name of the algorithm).</nextsent>
<nextsent>texttilingis widely used as de-facto standard in the evaluation of alternative segmentation systems, e.g.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3610">
<title id=" W06-1320.xml">an analysis of quantitative aspects in the evaluation of thematic segmentation algorithms </title>
<section> comparison of systems.  </section>
<citcontext>
<prevsection>
<prevsent>the text tiling algorithm was initially developed by hearst (1997) <papid> J97-1003 </papid>for segmentation of expository texts into multi-paragraph thematic episode shaving linear, non-overlapping structure (as reflected by the name of the algorithm).</prevsent>
<prevsent>texttilingis widely used as de-facto standard in the evaluation of alternative segmentation systems, e.g.</prevsent>
</prevsection>
<citsent citstr=" C02-1033 ">
(reynar, 1998; ferret, 2002; <papid> C02-1033 </papid>galley et al, 2003).<papid> P03-1071 </papid>the algorithm can briefly be described by the following steps.step 1 includes stop-word removal, lemmatiza tion and division of the text into token-sequences?</citsent>
<aftsection>
<nextsent>(i.e. text blocks having fixed number of words).
</nextsent>
<nextsent>step 2 determines score for each gap between two consecutive token-sequences, by computing the cosine similarity (manning and schutze, 1999)between the two vectors representing the frequencies of the words in the two blocks.step 3 computes depth score?
</nextsent>
<nextsent>for each token sequence gap, based on the local minima of the score computed in step 2.
</nextsent>
<nextsent>step 4 consists in smoothing the scores.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3625">
<title id=" W06-1320.xml">an analysis of quantitative aspects in the evaluation of thematic segmentation algorithms </title>
<section> evaluation metrics.  </section>
<citcontext>
<prevsection>
<prevsent>pseg), where: pmiss = pnk i=1 [hyp(i,i+k)]?[1ref (i,i+k)] pnk i=1 [1ref (i,i+k)] , pfalsealarm = pnk i=1 [1hyp(i,i+k)]?[ref (i,i+k)] pnk i=1 ref (i,i+k) , and pseg is the priori probability that in the reference data boundary occurs within an interval of words.
</prevsent>
<prevsent>therefore pk is calculated by moving window of certain width k, where is usually set to half of the average number of words per segment in the gold standard.
</prevsent>
</prevsection>
<citsent citstr=" J02-1002 ">
pevzner and hearst (2002) <papid> J02-1002 </papid>highlighted several problems of the pk metric.</citsent>
<aftsection>
<nextsent>we illustrate below what we consider the main problems of the pk metric, based on two examples.let r(i, k) be the number of boundaries between positions and + in the gold standard segmentation and h(i, k) be the number of boundaries between positions and i+k in the automatic hypothesized segmentation.
</nextsent>
<nextsent>example 1: if r(i, k) = 2 and h(i, k) = 1 then obviously missing boundary should2let ref be correct segmentation and hyp be segmentation proposed by text segmentation system.
</nextsent>
<nextsent>we will keep this notations in equations introduced below.be counted in pk, i.e. pmiss should be increased.
</nextsent>
<nextsent>example 2: if r(i, k) = 1 and h(i, k) =2 then obviously pfalsealarm should be increased.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3637">
<title id=" W06-1651.xml">joint extraction of entities and relations for opinion recognition </title>
<section> abstract </section>
<citcontext>
<prevsection>
<prevsent>we present an approach for the joint extraction of entities and relations in the context of opinion recognition and analysis.
</prevsent>
<prevsent>we identify two types of opinion-related entities ? expressions of opinions and sources of opinions ? along with the linking relation that exists between them.
</prevsent>
</prevsection>
<citsent citstr=" W04-2401 ">
inspired by roth and yih (2004), <papid> W04-2401 </papid>we employ an integer linear programming approach to solve the joint opinion recognition task,and show that global, constraint-based inference can significantly boost the performance of both relation extraction and the extraction of opinion-related entities.</citsent>
<aftsection>
<nextsent>performance further improves when semantic role labeling system is incorporated.
</nextsent>
<nextsent>the resulting system achieves f-measuresof 79 and 69 for entity and relation extraction, respectively, improving substantially over prior results in the area.
</nextsent>
<nextsent>information extraction tasks such as recognizing entities and relations have long been considered critical to many domain-specific nlp tasks (e.g. mooney and bunescu (2005), prager et al (2000), white et al (2001)).<papid> H01-1054 </papid></nextsent>
<nextsent>researchers have further shown that opinion-oriented information extraction can provide analogous benefits to variety of practical applications including product reputation tracking (morinaga et al, 2002), opinion-oriented question answering (stoyanov et al, 2005), <papid> H05-1116 </papid>and opinion-oriented summarization (e.g. cardie et al.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3639">
<title id=" W06-1651.xml">joint extraction of entities and relations for opinion recognition </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>performance further improves when semantic role labeling system is incorporated.
</prevsent>
<prevsent>the resulting system achieves f-measuresof 79 and 69 for entity and relation extraction, respectively, improving substantially over prior results in the area.
</prevsent>
</prevsection>
<citsent citstr=" H01-1054 ">
information extraction tasks such as recognizing entities and relations have long been considered critical to many domain-specific nlp tasks (e.g. mooney and bunescu (2005), prager et al (2000), white et al (2001)).<papid> H01-1054 </papid></citsent>
<aftsection>
<nextsent>researchers have further shown that opinion-oriented information extraction can provide analogous benefits to variety of practical applications including product reputation tracking (morinaga et al, 2002), opinion-oriented question answering (stoyanov et al, 2005), <papid> H05-1116 </papid>and opinion-oriented summarization (e.g. cardie et al.</nextsent>
<nextsent>(2004), liu et al (2005)).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3640">
<title id=" W06-1651.xml">joint extraction of entities and relations for opinion recognition </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the resulting system achieves f-measuresof 79 and 69 for entity and relation extraction, respectively, improving substantially over prior results in the area.
</prevsent>
<prevsent>information extraction tasks such as recognizing entities and relations have long been considered critical to many domain-specific nlp tasks (e.g. mooney and bunescu (2005), prager et al (2000), white et al (2001)).<papid> H01-1054 </papid></prevsent>
</prevsection>
<citsent citstr=" H05-1116 ">
researchers have further shown that opinion-oriented information extraction can provide analogous benefits to variety of practical applications including product reputation tracking (morinaga et al, 2002), opinion-oriented question answering (stoyanov et al, 2005), <papid> H05-1116 </papid>and opinion-oriented summarization (e.g. cardie et al.</citsent>
<aftsection>
<nextsent>(2004), liu et al (2005)).
</nextsent>
<nextsent>moreover, much progress has been made in the area of opinion extraction: it is possible to identify sources of opinions (i.e. the opinion holders) (e.g. choi et al (2005) <papid> H05-1045 </papid>and kim and hovy (2005<papid> I05-2011 </papid>b)), to determine the polarity and strength of opinion expressions(e.g. wilson et al (2005)), <papid> H05-1044 </papid>and to recognize propositional opinions and their sources (e.g. bethard et al (2004)) with reasonable accuracy.</nextsent>
<nextsent>to date,however, there has been no effort to simultaneously identify arbitrary opinion expressions, their sources, and the relations between them.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3641">
<title id=" W06-1651.xml">joint extraction of entities and relations for opinion recognition </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>researchers have further shown that opinion-oriented information extraction can provide analogous benefits to variety of practical applications including product reputation tracking (morinaga et al, 2002), opinion-oriented question answering (stoyanov et al, 2005), <papid> H05-1116 </papid>and opinion-oriented summarization (e.g. cardie et al.</prevsent>
<prevsent>(2004), liu et al (2005)).</prevsent>
</prevsection>
<citsent citstr=" H05-1045 ">
moreover, much progress has been made in the area of opinion extraction: it is possible to identify sources of opinions (i.e. the opinion holders) (e.g. choi et al (2005) <papid> H05-1045 </papid>and kim and hovy (2005<papid> I05-2011 </papid>b)), to determine the polarity and strength of opinion expressions(e.g. wilson et al (2005)), <papid> H05-1044 </papid>and to recognize propositional opinions and their sources (e.g. bethard et al (2004)) with reasonable accuracy.</citsent>
<aftsection>
<nextsent>to date,however, there has been no effort to simultaneously identify arbitrary opinion expressions, their sources, and the relations between them.
</nextsent>
<nextsent>without progress on the joint extraction of opinion entities and their relations, the capabilities of opinion based applications will remain limited.
</nextsent>
<nextsent>fortunately, research in machine learning has produced methods for global inference and joint classification that can help to address this deficiency (e.g. bunescu and mooney (2004), <papid> P04-1056 </papid>roth and yih (2004)).<papid> W04-2401 </papid></nextsent>
<nextsent>moreover, it has been shown that exploiting dependencies among entities and/or relations via global inference not only solves the joint extraction task, but often boosts performance on the individual tasks when compared to classifiers that handle the tasks independently ? for semantic role labeling (e.g. punyakanok et al (2004)), <papid> C04-1197 </papid>information extraction (e.g. roth and yih (2004)), <papid> W04-2401 </papid>and sequence tagging (e.g. sutton et al (2004)).in this paper, we present global inference approach (roth and yih, 2004) <papid> W04-2401 </papid>to the extraction of opinion-related entities and relations.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3644">
<title id=" W06-1651.xml">joint extraction of entities and relations for opinion recognition </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>researchers have further shown that opinion-oriented information extraction can provide analogous benefits to variety of practical applications including product reputation tracking (morinaga et al, 2002), opinion-oriented question answering (stoyanov et al, 2005), <papid> H05-1116 </papid>and opinion-oriented summarization (e.g. cardie et al.</prevsent>
<prevsent>(2004), liu et al (2005)).</prevsent>
</prevsection>
<citsent citstr=" I05-2011 ">
moreover, much progress has been made in the area of opinion extraction: it is possible to identify sources of opinions (i.e. the opinion holders) (e.g. choi et al (2005) <papid> H05-1045 </papid>and kim and hovy (2005<papid> I05-2011 </papid>b)), to determine the polarity and strength of opinion expressions(e.g. wilson et al (2005)), <papid> H05-1044 </papid>and to recognize propositional opinions and their sources (e.g. bethard et al (2004)) with reasonable accuracy.</citsent>
<aftsection>
<nextsent>to date,however, there has been no effort to simultaneously identify arbitrary opinion expressions, their sources, and the relations between them.
</nextsent>
<nextsent>without progress on the joint extraction of opinion entities and their relations, the capabilities of opinion based applications will remain limited.
</nextsent>
<nextsent>fortunately, research in machine learning has produced methods for global inference and joint classification that can help to address this deficiency (e.g. bunescu and mooney (2004), <papid> P04-1056 </papid>roth and yih (2004)).<papid> W04-2401 </papid></nextsent>
<nextsent>moreover, it has been shown that exploiting dependencies among entities and/or relations via global inference not only solves the joint extraction task, but often boosts performance on the individual tasks when compared to classifiers that handle the tasks independently ? for semantic role labeling (e.g. punyakanok et al (2004)), <papid> C04-1197 </papid>information extraction (e.g. roth and yih (2004)), <papid> W04-2401 </papid>and sequence tagging (e.g. sutton et al (2004)).in this paper, we present global inference approach (roth and yih, 2004) <papid> W04-2401 </papid>to the extraction of opinion-related entities and relations.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3648">
<title id=" W06-1651.xml">joint extraction of entities and relations for opinion recognition </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>researchers have further shown that opinion-oriented information extraction can provide analogous benefits to variety of practical applications including product reputation tracking (morinaga et al, 2002), opinion-oriented question answering (stoyanov et al, 2005), <papid> H05-1116 </papid>and opinion-oriented summarization (e.g. cardie et al.</prevsent>
<prevsent>(2004), liu et al (2005)).</prevsent>
</prevsection>
<citsent citstr=" H05-1044 ">
moreover, much progress has been made in the area of opinion extraction: it is possible to identify sources of opinions (i.e. the opinion holders) (e.g. choi et al (2005) <papid> H05-1045 </papid>and kim and hovy (2005<papid> I05-2011 </papid>b)), to determine the polarity and strength of opinion expressions(e.g. wilson et al (2005)), <papid> H05-1044 </papid>and to recognize propositional opinions and their sources (e.g. bethard et al (2004)) with reasonable accuracy.</citsent>
<aftsection>
<nextsent>to date,however, there has been no effort to simultaneously identify arbitrary opinion expressions, their sources, and the relations between them.
</nextsent>
<nextsent>without progress on the joint extraction of opinion entities and their relations, the capabilities of opinion based applications will remain limited.
</nextsent>
<nextsent>fortunately, research in machine learning has produced methods for global inference and joint classification that can help to address this deficiency (e.g. bunescu and mooney (2004), <papid> P04-1056 </papid>roth and yih (2004)).<papid> W04-2401 </papid></nextsent>
<nextsent>moreover, it has been shown that exploiting dependencies among entities and/or relations via global inference not only solves the joint extraction task, but often boosts performance on the individual tasks when compared to classifiers that handle the tasks independently ? for semantic role labeling (e.g. punyakanok et al (2004)), <papid> C04-1197 </papid>information extraction (e.g. roth and yih (2004)), <papid> W04-2401 </papid>and sequence tagging (e.g. sutton et al (2004)).in this paper, we present global inference approach (roth and yih, 2004) <papid> W04-2401 </papid>to the extraction of opinion-related entities and relations.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3650">
<title id=" W06-1651.xml">joint extraction of entities and relations for opinion recognition </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>to date,however, there has been no effort to simultaneously identify arbitrary opinion expressions, their sources, and the relations between them.
</prevsent>
<prevsent>without progress on the joint extraction of opinion entities and their relations, the capabilities of opinion based applications will remain limited.
</prevsent>
</prevsection>
<citsent citstr=" P04-1056 ">
fortunately, research in machine learning has produced methods for global inference and joint classification that can help to address this deficiency (e.g. bunescu and mooney (2004), <papid> P04-1056 </papid>roth and yih (2004)).<papid> W04-2401 </papid></citsent>
<aftsection>
<nextsent>moreover, it has been shown that exploiting dependencies among entities and/or relations via global inference not only solves the joint extraction task, but often boosts performance on the individual tasks when compared to classifiers that handle the tasks independently ? for semantic role labeling (e.g. punyakanok et al (2004)), <papid> C04-1197 </papid>information extraction (e.g. roth and yih (2004)), <papid> W04-2401 </papid>and sequence tagging (e.g. sutton et al (2004)).in this paper, we present global inference approach (roth and yih, 2004) <papid> W04-2401 </papid>to the extraction of opinion-related entities and relations.</nextsent>
<nextsent>in particular, we aim to identify two types of entities(i.e. spans of text): entities that express opinions and entities that denote sources of opinions.more specifically, we use the term opinion expression to denote all direct expressions of subjectivity including opinions, emotions, beliefs, sentiment, etc., as well as all speech expressions that introduce subjective propositions; and use the term source to denote the person or entity (e.g. re 431 port) that holds the opinion.1 in addition, weaim to identify the relations between opinion expression entities and source entities.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3652">
<title id=" W06-1651.xml">joint extraction of entities and relations for opinion recognition </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>without progress on the joint extraction of opinion entities and their relations, the capabilities of opinion based applications will remain limited.
</prevsent>
<prevsent>fortunately, research in machine learning has produced methods for global inference and joint classification that can help to address this deficiency (e.g. bunescu and mooney (2004), <papid> P04-1056 </papid>roth and yih (2004)).<papid> W04-2401 </papid></prevsent>
</prevsection>
<citsent citstr=" C04-1197 ">
moreover, it has been shown that exploiting dependencies among entities and/or relations via global inference not only solves the joint extraction task, but often boosts performance on the individual tasks when compared to classifiers that handle the tasks independently ? for semantic role labeling (e.g. punyakanok et al (2004)), <papid> C04-1197 </papid>information extraction (e.g. roth and yih (2004)), <papid> W04-2401 </papid>and sequence tagging (e.g. sutton et al (2004)).in this paper, we present global inference approach (roth and yih, 2004) <papid> W04-2401 </papid>to the extraction of opinion-related entities and relations.</citsent>
<aftsection>
<nextsent>in particular, we aim to identify two types of entities(i.e. spans of text): entities that express opinions and entities that denote sources of opinions.more specifically, we use the term opinion expression to denote all direct expressions of subjectivity including opinions, emotions, beliefs, sentiment, etc., as well as all speech expressions that introduce subjective propositions; and use the term source to denote the person or entity (e.g. re 431 port) that holds the opinion.1 in addition, weaim to identify the relations between opinion expression entities and source entities.
</nextsent>
<nextsent>that is, forgiven opinion expression oi and source entity sj , we determine whether the relation li,j def= (sj expresses oi) obtains, i.e. whether sj is the source of opinion expression oi.
</nextsent>
<nextsent>we refer to this particular relation as the link relation in the rest of the paper.
</nextsent>
<nextsent>consider, for example, the following sentences: s1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3675">
<title id=" W06-1651.xml">joint extraction of entities and relations for opinion recognition </title>
<section> high-level approach and related.  </section>
<citcontext>
<prevsection>
<prevsent>in addition, the link relation model explicitly exploits mutual dependencies among entities and relations, while bethard et al (2004) does not directly capture the potential influence among entities.
</prevsent>
<prevsent>kim and hovy (2005<papid> I05-2011 </papid>b) and choi et al (2005) <papid> H05-1045 </papid>focus only on the extraction of sources of opinions, without extracting opinion expressions.specifically, kim and hovy (2005<papid> I05-2011 </papid>b) assume priori existence of the opinion expressions and extract single source for each, while choi et al(2005) <papid> H05-1045 </papid>do not explicitly extract opinion expressions nor link an opinion expression to source even though their model implicitly learns approximations of opinion expressions in order to identify opinion sources.</prevsent>
</prevsection>
<citsent citstr=" H05-1068 ">
other previous research focuses only on the extraction of opinion expressions (e.g. kim and hovy (2005<papid> I05-2011 </papid>a), munson et al (2005) <papid> H05-1068 </papid>and wilson et al (2005)), <papid> H05-1044 </papid>omitting source identification altogether.there have also been previous efforts to simultaneously extract entities and relations by exploiting their mutual dependencies.</citsent>
<aftsection>
<nextsent>roth and yih (2002) <papid> C02-1151 </papid>formulated global inference using abayesian network, where they captured the influence between relation and pair of entities via the conditional probability of relation, given pair of entities.</nextsent>
<nextsent>this approach however, could not exploit dependencies between relations.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3678">
<title id=" W06-1651.xml">joint extraction of entities and relations for opinion recognition </title>
<section> high-level approach and related.  </section>
<citcontext>
<prevsection>
<prevsent>kim and hovy (2005<papid> I05-2011 </papid>b) and choi et al (2005) <papid> H05-1045 </papid>focus only on the extraction of sources of opinions, without extracting opinion expressions.specifically, kim and hovy (2005<papid> I05-2011 </papid>b) assume priori existence of the opinion expressions and extract single source for each, while choi et al(2005) <papid> H05-1045 </papid>do not explicitly extract opinion expressions nor link an opinion expression to source even though their model implicitly learns approximations of opinion expressions in order to identify opinion sources.</prevsent>
<prevsent>other previous research focuses only on the extraction of opinion expressions (e.g. kim and hovy (2005<papid> I05-2011 </papid>a), munson et al (2005) <papid> H05-1068 </papid>and wilson et al (2005)), <papid> H05-1044 </papid>omitting source identification altogether.there have also been previous efforts to simultaneously extract entities and relations by exploiting their mutual dependencies.</prevsent>
</prevsection>
<citsent citstr=" C02-1151 ">
roth and yih (2002) <papid> C02-1151 </papid>formulated global inference using abayesian network, where they captured the influence between relation and pair of entities via the conditional probability of relation, given pair of entities.</citsent>
<aftsection>
<nextsent>this approach however, could not exploit dependencies between relations.
</nextsent>
<nextsent>roth and yih (2004) <papid> W04-2401 </papid>later formulated global inference using integer linear programming, which is the approach that we apply here.</nextsent>
<nextsent>in contrast to our work, roth and yih (2004) <papid> W04-2401 </papid>operated in the domain of factual information extraction rather than opinion extraction, and assumed that the exact boundaries of entities from the gold standard are known priori, which may not be available in practice.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3689">
<title id=" W06-1619.xml">extremely lexicalized models for accurate and fast hpsg parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for the last decade, accurate and wide-coverage parsing for real-world text has been intensively and extensively pursued.
</prevsent>
<prevsent>in most of state-of-the art parsers, probabilistic events are defined over phrase structures because phrase structures are supposed to dominate syntactic configurations ofsentences.
</prevsent>
</prevsection>
<citsent citstr=" P03-1054 ">
for example, probabilities were defined over grammar rules in probabilistic cfg(collins, 1999; klein and manning, 2003; <papid> P03-1054 </papid>charniak and johnson, 2005) <papid> P05-1022 </papid>or over complex phrase structures of head-driven phrase structure grammar (hpsg) or combinatory categorial grammar (ccg) (clark and curran, 2004<papid> P04-1014 </papid>b; malouf and van noord, 2004; miyao and tsujii, 2005).<papid> P05-1011 </papid></citsent>
<aftsection>
<nextsent>although these studies vary in the design of the probabilistic models, the fundamental conception of probabilistic modeling is intended to capture characteristics of phrase structures or grammar rules.
</nextsent>
<nextsent>although lexical information, such as headwords, is known to significantly improve the parsing accuracy, it was also used to augment information on phrase structures.
</nextsent>
<nextsent>another interesting approach to this problem was using super tagging (clark and curran, 2004<papid> P04-1014 </papid>b; clark and curran, 2004<papid> P04-1014 </papid>a; wang and harper, 2004; <papid> W04-0307 </papid>nasr and rambow, 2004), which was originally developed for lexicalized tree adjoining grammars(ltag) (bangalore and joshi, 1999).<papid> J99-2004 </papid></nextsent>
<nextsent>super tagging is process where words in an input sentence are tagged with supertags,?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3690">
<title id=" W06-1619.xml">extremely lexicalized models for accurate and fast hpsg parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for the last decade, accurate and wide-coverage parsing for real-world text has been intensively and extensively pursued.
</prevsent>
<prevsent>in most of state-of-the art parsers, probabilistic events are defined over phrase structures because phrase structures are supposed to dominate syntactic configurations ofsentences.
</prevsent>
</prevsection>
<citsent citstr=" P05-1022 ">
for example, probabilities were defined over grammar rules in probabilistic cfg(collins, 1999; klein and manning, 2003; <papid> P03-1054 </papid>charniak and johnson, 2005) <papid> P05-1022 </papid>or over complex phrase structures of head-driven phrase structure grammar (hpsg) or combinatory categorial grammar (ccg) (clark and curran, 2004<papid> P04-1014 </papid>b; malouf and van noord, 2004; miyao and tsujii, 2005).<papid> P05-1011 </papid></citsent>
<aftsection>
<nextsent>although these studies vary in the design of the probabilistic models, the fundamental conception of probabilistic modeling is intended to capture characteristics of phrase structures or grammar rules.
</nextsent>
<nextsent>although lexical information, such as headwords, is known to significantly improve the parsing accuracy, it was also used to augment information on phrase structures.
</nextsent>
<nextsent>another interesting approach to this problem was using super tagging (clark and curran, 2004<papid> P04-1014 </papid>b; clark and curran, 2004<papid> P04-1014 </papid>a; wang and harper, 2004; <papid> W04-0307 </papid>nasr and rambow, 2004), which was originally developed for lexicalized tree adjoining grammars(ltag) (bangalore and joshi, 1999).<papid> J99-2004 </papid></nextsent>
<nextsent>super tagging is process where words in an input sentence are tagged with supertags,?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3691">
<title id=" W06-1619.xml">extremely lexicalized models for accurate and fast hpsg parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for the last decade, accurate and wide-coverage parsing for real-world text has been intensively and extensively pursued.
</prevsent>
<prevsent>in most of state-of-the art parsers, probabilistic events are defined over phrase structures because phrase structures are supposed to dominate syntactic configurations ofsentences.
</prevsent>
</prevsection>
<citsent citstr=" P04-1014 ">
for example, probabilities were defined over grammar rules in probabilistic cfg(collins, 1999; klein and manning, 2003; <papid> P03-1054 </papid>charniak and johnson, 2005) <papid> P05-1022 </papid>or over complex phrase structures of head-driven phrase structure grammar (hpsg) or combinatory categorial grammar (ccg) (clark and curran, 2004<papid> P04-1014 </papid>b; malouf and van noord, 2004; miyao and tsujii, 2005).<papid> P05-1011 </papid></citsent>
<aftsection>
<nextsent>although these studies vary in the design of the probabilistic models, the fundamental conception of probabilistic modeling is intended to capture characteristics of phrase structures or grammar rules.
</nextsent>
<nextsent>although lexical information, such as headwords, is known to significantly improve the parsing accuracy, it was also used to augment information on phrase structures.
</nextsent>
<nextsent>another interesting approach to this problem was using super tagging (clark and curran, 2004<papid> P04-1014 </papid>b; clark and curran, 2004<papid> P04-1014 </papid>a; wang and harper, 2004; <papid> W04-0307 </papid>nasr and rambow, 2004), which was originally developed for lexicalized tree adjoining grammars(ltag) (bangalore and joshi, 1999).<papid> J99-2004 </papid></nextsent>
<nextsent>super tagging is process where words in an input sentence are tagged with supertags,?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3703">
<title id=" W06-1619.xml">extremely lexicalized models for accurate and fast hpsg parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for the last decade, accurate and wide-coverage parsing for real-world text has been intensively and extensively pursued.
</prevsent>
<prevsent>in most of state-of-the art parsers, probabilistic events are defined over phrase structures because phrase structures are supposed to dominate syntactic configurations ofsentences.
</prevsent>
</prevsection>
<citsent citstr=" P05-1011 ">
for example, probabilities were defined over grammar rules in probabilistic cfg(collins, 1999; klein and manning, 2003; <papid> P03-1054 </papid>charniak and johnson, 2005) <papid> P05-1022 </papid>or over complex phrase structures of head-driven phrase structure grammar (hpsg) or combinatory categorial grammar (ccg) (clark and curran, 2004<papid> P04-1014 </papid>b; malouf and van noord, 2004; miyao and tsujii, 2005).<papid> P05-1011 </papid></citsent>
<aftsection>
<nextsent>although these studies vary in the design of the probabilistic models, the fundamental conception of probabilistic modeling is intended to capture characteristics of phrase structures or grammar rules.
</nextsent>
<nextsent>although lexical information, such as headwords, is known to significantly improve the parsing accuracy, it was also used to augment information on phrase structures.
</nextsent>
<nextsent>another interesting approach to this problem was using super tagging (clark and curran, 2004<papid> P04-1014 </papid>b; clark and curran, 2004<papid> P04-1014 </papid>a; wang and harper, 2004; <papid> W04-0307 </papid>nasr and rambow, 2004), which was originally developed for lexicalized tree adjoining grammars(ltag) (bangalore and joshi, 1999).<papid> J99-2004 </papid></nextsent>
<nextsent>super tagging is process where words in an input sentence are tagged with supertags,?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3731">
<title id=" W06-1619.xml">extremely lexicalized models for accurate and fast hpsg parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>although these studies vary in the design of the probabilistic models, the fundamental conception of probabilistic modeling is intended to capture characteristics of phrase structures or grammar rules.
</prevsent>
<prevsent>although lexical information, such as headwords, is known to significantly improve the parsing accuracy, it was also used to augment information on phrase structures.
</prevsent>
</prevsection>
<citsent citstr=" W04-0307 ">
another interesting approach to this problem was using super tagging (clark and curran, 2004<papid> P04-1014 </papid>b; clark and curran, 2004<papid> P04-1014 </papid>a; wang and harper, 2004; <papid> W04-0307 </papid>nasr and rambow, 2004), which was originally developed for lexicalized tree adjoining grammars(ltag) (bangalore and joshi, 1999).<papid> J99-2004 </papid></citsent>
<aftsection>
<nextsent>super tagging is process where words in an input sentence are tagged with supertags,?
</nextsent>
<nextsent>which are lexical entries in lexicalized grammars, e.g., elementary trees in ltag, lexical categories in ccg, and lexical entries in hpsg.
</nextsent>
<nextsent>super tagging was, in the first place, technique to reduce the cost of parsing with lexicalized grammars; ambiguity in assigning lexical entries to words is reduced by the light-weight process of super tagging before the heavy process of parsing.
</nextsent>
<nextsent>bangalore and joshi (1999) <papid> J99-2004 </papid>claimed that if words can be assigned correct supertags, syntactic parsing is almost trivial.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3732">
<title id=" W06-1619.xml">extremely lexicalized models for accurate and fast hpsg parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>although these studies vary in the design of the probabilistic models, the fundamental conception of probabilistic modeling is intended to capture characteristics of phrase structures or grammar rules.
</prevsent>
<prevsent>although lexical information, such as headwords, is known to significantly improve the parsing accuracy, it was also used to augment information on phrase structures.
</prevsent>
</prevsection>
<citsent citstr=" J99-2004 ">
another interesting approach to this problem was using super tagging (clark and curran, 2004<papid> P04-1014 </papid>b; clark and curran, 2004<papid> P04-1014 </papid>a; wang and harper, 2004; <papid> W04-0307 </papid>nasr and rambow, 2004), which was originally developed for lexicalized tree adjoining grammars(ltag) (bangalore and joshi, 1999).<papid> J99-2004 </papid></citsent>
<aftsection>
<nextsent>super tagging is process where words in an input sentence are tagged with supertags,?
</nextsent>
<nextsent>which are lexical entries in lexicalized grammars, e.g., elementary trees in ltag, lexical categories in ccg, and lexical entries in hpsg.
</nextsent>
<nextsent>super tagging was, in the first place, technique to reduce the cost of parsing with lexicalized grammars; ambiguity in assigning lexical entries to words is reduced by the light-weight process of super tagging before the heavy process of parsing.
</nextsent>
<nextsent>bangalore and joshi (1999) <papid> J99-2004 </papid>claimed that if words can be assigned correct supertags, syntactic parsing is almost trivial.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3764">
<title id=" W06-1619.xml">extremely lexicalized models for accurate and fast hpsg parsing </title>
<section> hpsg and probabilistic models.  </section>
<citcontext>
<prevsection>
<prevsent>156 spring head nounsubj    comps     2head verbsubj    comps    1 has head verbsubj    comps    1 come 2 head-comp head verb subj     comps     head nounsubj    comps    1 =?
</prevsent>
<prevsent>spring head nounsubj    comps     2head verbsubj    comps    1 has head verbsubj    comps    1 come 2 head verbsubj    comps    1 head verbsubj    comps     1 subject-head head-comp figure 1: hpsg parsing.
</prevsent>
</prevsection>
<citsent citstr=" J97-4005 ">
previous studies (abney, 1997; <papid> J97-4005 </papid>johnson et al, 1999; <papid> P99-1069 </papid>riezler et al, 2000; <papid> P00-1061 </papid>malouf and van noord, 2004; kaplan et al, 2004; <papid> N04-1013 </papid>miyao and tsujii, 2005) <papid> P05-1011 </papid>defined probabilistic model of unification-based grammars including hpsg as log-linear model or maximum entropy model (berger et al, 1996).<papid> J96-1002 </papid></citsent>
<aftsection>
<nextsent>the probability that parse result is assigned to given sentence = w1, . . .
</nextsent>
<nextsent>, wn?
</nextsent>
<nextsent>is phpsg(t |w) = 1zw exp (?
</nextsent>
<nextsent>u ufu(t ) ) zw = ? ? exp (?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3765">
<title id=" W06-1619.xml">extremely lexicalized models for accurate and fast hpsg parsing </title>
<section> hpsg and probabilistic models.  </section>
<citcontext>
<prevsection>
<prevsent>156 spring head nounsubj    comps     2head verbsubj    comps    1 has head verbsubj    comps    1 come 2 head-comp head verb subj     comps     head nounsubj    comps    1 =?
</prevsent>
<prevsent>spring head nounsubj    comps     2head verbsubj    comps    1 has head verbsubj    comps    1 come 2 head verbsubj    comps    1 head verbsubj    comps     1 subject-head head-comp figure 1: hpsg parsing.
</prevsent>
</prevsection>
<citsent citstr=" P99-1069 ">
previous studies (abney, 1997; <papid> J97-4005 </papid>johnson et al, 1999; <papid> P99-1069 </papid>riezler et al, 2000; <papid> P00-1061 </papid>malouf and van noord, 2004; kaplan et al, 2004; <papid> N04-1013 </papid>miyao and tsujii, 2005) <papid> P05-1011 </papid>defined probabilistic model of unification-based grammars including hpsg as log-linear model or maximum entropy model (berger et al, 1996).<papid> J96-1002 </papid></citsent>
<aftsection>
<nextsent>the probability that parse result is assigned to given sentence = w1, . . .
</nextsent>
<nextsent>, wn?
</nextsent>
<nextsent>is phpsg(t |w) = 1zw exp (?
</nextsent>
<nextsent>u ufu(t ) ) zw = ? ? exp (?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3766">
<title id=" W06-1619.xml">extremely lexicalized models for accurate and fast hpsg parsing </title>
<section> hpsg and probabilistic models.  </section>
<citcontext>
<prevsection>
<prevsent>156 spring head nounsubj    comps     2head verbsubj    comps    1 has head verbsubj    comps    1 come 2 head-comp head verb subj     comps     head nounsubj    comps    1 =?
</prevsent>
<prevsent>spring head nounsubj    comps     2head verbsubj    comps    1 has head verbsubj    comps    1 come 2 head verbsubj    comps    1 head verbsubj    comps     1 subject-head head-comp figure 1: hpsg parsing.
</prevsent>
</prevsection>
<citsent citstr=" P00-1061 ">
previous studies (abney, 1997; <papid> J97-4005 </papid>johnson et al, 1999; <papid> P99-1069 </papid>riezler et al, 2000; <papid> P00-1061 </papid>malouf and van noord, 2004; kaplan et al, 2004; <papid> N04-1013 </papid>miyao and tsujii, 2005) <papid> P05-1011 </papid>defined probabilistic model of unification-based grammars including hpsg as log-linear model or maximum entropy model (berger et al, 1996).<papid> J96-1002 </papid></citsent>
<aftsection>
<nextsent>the probability that parse result is assigned to given sentence = w1, . . .
</nextsent>
<nextsent>, wn?
</nextsent>
<nextsent>is phpsg(t |w) = 1zw exp (?
</nextsent>
<nextsent>u ufu(t ) ) zw = ? ? exp (?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3767">
<title id=" W06-1619.xml">extremely lexicalized models for accurate and fast hpsg parsing </title>
<section> hpsg and probabilistic models.  </section>
<citcontext>
<prevsection>
<prevsent>156 spring head nounsubj    comps     2head verbsubj    comps    1 has head verbsubj    comps    1 come 2 head-comp head verb subj     comps     head nounsubj    comps    1 =?
</prevsent>
<prevsent>spring head nounsubj    comps     2head verbsubj    comps    1 has head verbsubj    comps    1 come 2 head verbsubj    comps    1 head verbsubj    comps     1 subject-head head-comp figure 1: hpsg parsing.
</prevsent>
</prevsection>
<citsent citstr=" N04-1013 ">
previous studies (abney, 1997; <papid> J97-4005 </papid>johnson et al, 1999; <papid> P99-1069 </papid>riezler et al, 2000; <papid> P00-1061 </papid>malouf and van noord, 2004; kaplan et al, 2004; <papid> N04-1013 </papid>miyao and tsujii, 2005) <papid> P05-1011 </papid>defined probabilistic model of unification-based grammars including hpsg as log-linear model or maximum entropy model (berger et al, 1996).<papid> J96-1002 </papid></citsent>
<aftsection>
<nextsent>the probability that parse result is assigned to given sentence = w1, . . .
</nextsent>
<nextsent>, wn?
</nextsent>
<nextsent>is phpsg(t |w) = 1zw exp (?
</nextsent>
<nextsent>u ufu(t ) ) zw = ? ? exp (?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3769">
<title id=" W06-1619.xml">extremely lexicalized models for accurate and fast hpsg parsing </title>
<section> hpsg and probabilistic models.  </section>
<citcontext>
<prevsection>
<prevsent>156 spring head nounsubj    comps     2head verbsubj    comps    1 has head verbsubj    comps    1 come 2 head-comp head verb subj     comps     head nounsubj    comps    1 =?
</prevsent>
<prevsent>spring head nounsubj    comps     2head verbsubj    comps    1 has head verbsubj    comps    1 come 2 head verbsubj    comps    1 head verbsubj    comps     1 subject-head head-comp figure 1: hpsg parsing.
</prevsent>
</prevsection>
<citsent citstr=" J96-1002 ">
previous studies (abney, 1997; <papid> J97-4005 </papid>johnson et al, 1999; <papid> P99-1069 </papid>riezler et al, 2000; <papid> P00-1061 </papid>malouf and van noord, 2004; kaplan et al, 2004; <papid> N04-1013 </papid>miyao and tsujii, 2005) <papid> P05-1011 </papid>defined probabilistic model of unification-based grammars including hpsg as log-linear model or maximum entropy model (berger et al, 1996).<papid> J96-1002 </papid></citsent>
<aftsection>
<nextsent>the probability that parse result is assigned to given sentence = w1, . . .
</nextsent>
<nextsent>, wn?
</nextsent>
<nextsent>is phpsg(t |w) = 1zw exp (?
</nextsent>
<nextsent>u ufu(t ) ) zw = ? ? exp (?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3770">
<title id=" W06-1619.xml">extremely lexicalized models for accurate and fast hpsg parsing </title>
<section> hpsg and probabilistic models.  </section>
<citcontext>
<prevsection>
<prevsent>u ufu(t ) ) zw = ? ? exp (?
</prevsent>
<prevsent>u ufu(t ?) ) , where is model parameter, fu is feature function that represents characteristic of parse tree , and zw is the sum over the set of all possible parse trees for the sentence.
</prevsent>
</prevsection>
<citsent citstr=" W02-2018 ">
intuitively, the probability is defined as the normalized product of the weights exp(u) when characteristic corresponding to fu appears in parse result . the model parameters, u, are estimated using numerical optimization methods (malouf, 2002) <papid> W02-2018 </papid>to maximize the log-likelihood of the training data.however, the above model cannot be easily estimated because the estimation requires the computation of p(t |w) for all parse candidates assigned to sentence w. because the number of parse candidates is exponentially related to the length of the sentence, the estimation is intractable for long sentences.</citsent>
<aftsection>
<nextsent>to make the model estimation tractable, geman and johnson (geman and johnson, 2002) <papid> P02-1036 </papid>and miyao and tsujii (miyao and tsujii, 2002) proposed dynamic programming algorithm for estimating p(t |w).</nextsent>
<nextsent>miyao and tsujii head verbsubj   comps    head nounsubj   comps    head verbsubj    comps    head verbsubj    comps     head verbsubj    comps    subject-head head-comp spring/nn has/vbz come/vbn 1 1 11 22 froot=  s, has, vbz,  head verbsubj  np comps  vp  fbinary= head-comp, 1, 0, 1, vp, has, vbz, , 1, vp, come, vbn, head verbsubj  np comps  vp  head verbsubj  np comps    flex=  spring, nn,   head nounsubj   comps    figure 2: example of features.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3771">
<title id=" W06-1619.xml">extremely lexicalized models for accurate and fast hpsg parsing </title>
<section> hpsg and probabilistic models.  </section>
<citcontext>
<prevsection>
<prevsent>u ufu(t ?) ) , where is model parameter, fu is feature function that represents characteristic of parse tree , and zw is the sum over the set of all possible parse trees for the sentence.
</prevsent>
<prevsent>intuitively, the probability is defined as the normalized product of the weights exp(u) when characteristic corresponding to fu appears in parse result . the model parameters, u, are estimated using numerical optimization methods (malouf, 2002) <papid> W02-2018 </papid>to maximize the log-likelihood of the training data.however, the above model cannot be easily estimated because the estimation requires the computation of p(t |w) for all parse candidates assigned to sentence w. because the number of parse candidates is exponentially related to the length of the sentence, the estimation is intractable for long sentences.</prevsent>
</prevsection>
<citsent citstr=" P02-1036 ">
to make the model estimation tractable, geman and johnson (geman and johnson, 2002) <papid> P02-1036 </papid>and miyao and tsujii (miyao and tsujii, 2002) proposed dynamic programming algorithm for estimating p(t |w).</citsent>
<aftsection>
<nextsent>miyao and tsujii head verbsubj   comps    head nounsubj   comps    head verbsubj    comps    head verbsubj    comps     head verbsubj    comps    subject-head head-comp spring/nn has/vbz come/vbn 1 1 11 22 froot=  s, has, vbz,  head verbsubj  np comps  vp  fbinary= head-comp, 1, 0, 1, vp, has, vbz, , 1, vp, come, vbn, head verbsubj  np comps  vp  head verbsubj  np comps    flex=  spring, nn,   head nounsubj   comps    figure 2: example of features.
</nextsent>
<nextsent>(2005) also introduced preliminary probabilistic model p0(t |w) whose estimation does not require the parsing of treebank.
</nextsent>
<nextsent>this model is introduced as reference distribution of the probabilistic hpsg model; i.e., the computation of parse trees given low probabilities by the model is omitted in the estimation stage.
</nextsent>
<nextsent>we have (previous probabilistic hpsg) phpsg?(t |w) = p0(t |w) 1zw exp (?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3773">
<title id=" W06-1619.xml">extremely lexicalized models for accurate and fast hpsg parsing </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>the feature templates used in our model are listed in table 2 and are word trigrams and pos 5-grams.
</prevsent>
<prevsent>4.1 implementation.
</prevsent>
</prevsection>
<citsent citstr=" W05-1511 ">
we implemented the iterative parsing algorithm (ninomiya et al, 2005) <papid> W05-1511 </papid>for the probabilistic hpsg models.</citsent>
<aftsection>
<nextsent>it first starts parsing with narrow beam.
</nextsent>
<nextsent>if the parsing fails, then the beam is widened, and parsing continues until the parser outputs results or the beam width reaches some limit.
</nextsent>
<nextsent>though the probabilities of lexical entry selection are introduced, the algorithm for the presented probabilistic models is almost the same as the original iterative parsing algorithm.
</nextsent>
<nextsent>the pseudo-code of the algorithm is shown in figure 3.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3775">
<title id=" W06-1619.xml">extremely lexicalized models for accurate and fast hpsg parsing </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>thefom for models 1 and 2 is computed by only summing the values of ? of the daughters; i.e., weights exp(u) in the figure are assigned zero.
</prevsent>
<prevsent>the terms ? and ? are the thresholds of the number of phrasal signs in the chart cell and the beam width for signsin the chart cell.
</prevsent>
</prevsection>
<citsent citstr=" W97-0302 ">
the terms ? and ? are the thresholds of the number and the beam width of lexical entries, and ? is the beam width for global thresholding (goodman, 1997).<papid> W97-0302 </papid></citsent>
<aftsection>
<nextsent>4.2 evaluation.
</nextsent>
<nextsent>we evaluated the speed and accuracy of parsing with extremely lexicalized models by using enju 2.1, the hpsg grammar for english (miyao et al, 2005; miyao and tsujii, 2005).<papid> P05-1011 </papid></nextsent>
<nextsent>the lexicon of the grammar was extracted from sections 02-21 of the penn treebank (marcus et al, 1994) (39,832sentences).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3783">
<title id=" W06-1619.xml">extremely lexicalized models for accurate and fast hpsg parsing </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>labeled precision(lp)/labeled recall (lr) is the ratio of tuples correctly identified by the parser3.
</prevsent>
<prevsent>unlabeled precision (up)/unlabeled recall (ur) is the ratio oftuples without the predicate type and the argument label.
</prevsent>
</prevsection>
<citsent citstr=" P03-1046 ">
this evaluation scheme was the same as used in previous evaluations of lexicalized grammars (hockenmaier, 2003; <papid> P03-1046 </papid>clark and cur 2deep parsing techniques include quick check (malouf et al, 2000) and large constituent inhibition (kaplan et al, 2004) <papid> N04-1013 </papid>as described by ninomiya et al (2005), <papid> W05-1511 </papid>but hybrid parsing with cfg chunk parser was not used.</citsent>
<aftsection>
<nextsent>this is be cause we did not observe significant improvement for the development set by the hybrid parsing and observed only small improvement in the parsing speed by around 10 ms. 3when parsing fails, precision and recall are evaluated, although nothing is output by the parser; i.e., recall decreases greatly.ran, 2004b; miyao and tsujii, 2005).<papid> P05-1011 </papid></nextsent>
<nextsent>the experiments were conducted on an amd opteron server with 2.4-ghz cpu.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3790">
<title id=" W06-1619.xml">extremely lexicalized models for accurate and fast hpsg parsing </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>table 3 details the numbers and average lengths ofthe tested sentences of ? 40 and 100 words in sections 23 and 24, and the total numbers of sentences in sections 23 and 24.
</prevsent>
<prevsent>the parsing performance for section 23 is shown in table 4.
</prevsent>
</prevsection>
<citsent citstr=" H05-1059 ">
the upper half of the table shows the performance using the correct poss in the penn treebank, and the lower half shows the performance using the poss given by pos tagger (tsuruoka and tsujii, 2005).<papid> H05-1059 </papid></citsent>
<aftsection>
<nextsent>the left and right sides of the table show the performances for the sentences of ? 40 and ? 100 words.
</nextsent>
<nextsent>our models significantly increased not only the parsing speed but also the parsing accuracy.
</nextsent>
<nextsent>model 3 was around three to four times faster and had around two points higher precision and recall than the previous model.
</nextsent>
<nextsent>surprisingly, model 1, which used only lexical information, was very fast and as accurate as the previous model.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3804">
<title id=" W06-1619.xml">extremely lexicalized models for accurate and fast hpsg parsing </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>the accuracy of the resulting super tagger on our development set (sec tion 22) is given in table 5 and table 6.
</prevsent>
<prevsent>the test sentences were automatically pos-tagged.
</prevsent>
</prevsection>
<citsent citstr=" E03-1071 ">
results of other super taggers for automatically ex test data accuracy (%) hpsg super tagger 22 87.51 (this paper) ccg super tagger 00/23 91.70 / 91.45 (curran and clark, 2003) <papid> E03-1071 </papid>ltag super tagger 22/23 86.01 / 86.27 (shen and joshi, 2003) <papid> P03-1064 </papid>table 5: accuracy of single-tag supertaggers.</citsent>
<aftsection>
<nextsent>the numbers under test data?
</nextsent>
<nextsent>are the ptb section numbers of the test data.
</nextsent>
<nextsent>tags/word word acc.
</nextsent>
<nextsent>(%) sentence acc.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3805">
<title id=" W06-1619.xml">extremely lexicalized models for accurate and fast hpsg parsing </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>the accuracy of the resulting super tagger on our development set (sec tion 22) is given in table 5 and table 6.
</prevsent>
<prevsent>the test sentences were automatically pos-tagged.
</prevsent>
</prevsection>
<citsent citstr=" P03-1064 ">
results of other super taggers for automatically ex test data accuracy (%) hpsg super tagger 22 87.51 (this paper) ccg super tagger 00/23 91.70 / 91.45 (curran and clark, 2003) <papid> E03-1071 </papid>ltag super tagger 22/23 86.01 / 86.27 (shen and joshi, 2003) <papid> P03-1064 </papid>table 5: accuracy of single-tag supertaggers.</citsent>
<aftsection>
<nextsent>the numbers under test data?
</nextsent>
<nextsent>are the ptb section numbers of the test data.
</nextsent>
<nextsent>tags/word word acc.
</nextsent>
<nextsent>(%) sentence acc.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3810">
<title id=" W06-1609.xml">statistical machine reordering </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in addition, the use of classes in smr helps to infer new word reorderings.
</prevsent>
<prevsent>experiments are reported in the esen wmt06 tasks and the zheniwslt05 task and show significant improvement in translation quality.
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
during the last few years, smt system shave evolved from the original word-based approach (brown et al , 1993) <papid> J93-2003 </papid>to phrase-based translation systems (koehn et al , 2003).<papid> N03-1017 </papid></citsent>
<aftsection>
<nextsent>in parallel to the phrase-based approach, the use of bilingual n-grams gives comparable results, as shownby crego et al  (2005a).
</nextsent>
<nextsent>two basic issues differentiate the n-gram-based system from the phrase based: training data are monotonously segmented into bilingual units; and, the model considers gram probabilities rather than relative frequencies.
</nextsent>
<nextsent>this translation approach is described in detail by marino et al  (2005).
</nextsent>
<nextsent>the n-gram-based system follows maximum entropy approach, in which alog-linear combination of multiple models is implemented (och and ney, 2002), <papid> P02-1038 </papid>as an alternative to the source-channel approach.in both systems, introducing reordering capabilities is of crucial importance for certain language pairs.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3811">
<title id=" W06-1609.xml">statistical machine reordering </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in addition, the use of classes in smr helps to infer new word reorderings.
</prevsent>
<prevsent>experiments are reported in the esen wmt06 tasks and the zheniwslt05 task and show significant improvement in translation quality.
</prevsent>
</prevsection>
<citsent citstr=" N03-1017 ">
during the last few years, smt system shave evolved from the original word-based approach (brown et al , 1993) <papid> J93-2003 </papid>to phrase-based translation systems (koehn et al , 2003).<papid> N03-1017 </papid></citsent>
<aftsection>
<nextsent>in parallel to the phrase-based approach, the use of bilingual n-grams gives comparable results, as shownby crego et al  (2005a).
</nextsent>
<nextsent>two basic issues differentiate the n-gram-based system from the phrase based: training data are monotonously segmented into bilingual units; and, the model considers gram probabilities rather than relative frequencies.
</nextsent>
<nextsent>this translation approach is described in detail by marino et al  (2005).
</nextsent>
<nextsent>the n-gram-based system follows maximum entropy approach, in which alog-linear combination of multiple models is implemented (och and ney, 2002), <papid> P02-1038 </papid>as an alternative to the source-channel approach.in both systems, introducing reordering capabilities is of crucial importance for certain language pairs.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3812">
<title id=" W06-1609.xml">statistical machine reordering </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>two basic issues differentiate the n-gram-based system from the phrase based: training data are monotonously segmented into bilingual units; and, the model considers gram probabilities rather than relative frequencies.
</prevsent>
<prevsent>this translation approach is described in detail by marino et al  (2005).
</prevsent>
</prevsection>
<citsent citstr=" P02-1038 ">
the n-gram-based system follows maximum entropy approach, in which alog-linear combination of multiple models is implemented (och and ney, 2002), <papid> P02-1038 </papid>as an alternative to the source-channel approach.in both systems, introducing reordering capabilities is of crucial importance for certain language pairs.</citsent>
<aftsection>
<nextsent>recently, new reordering strategies have been proposed in the literature on smt such as the reordering of each source sentence to match the word order in the corresponding target sentence, see kanthak et al  (2005) <papid> W05-0831 </papid>and crego et al  (2005b).</nextsent>
<nextsent>similarly, matusov et al  (2006) describe method for simultaneously aligning and monotonizing the training corpus.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3813">
<title id=" W06-1609.xml">statistical machine reordering </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this translation approach is described in detail by marino et al  (2005).
</prevsent>
<prevsent>the n-gram-based system follows maximum entropy approach, in which alog-linear combination of multiple models is implemented (och and ney, 2002), <papid> P02-1038 </papid>as an alternative to the source-channel approach.in both systems, introducing reordering capabilities is of crucial importance for certain language pairs.</prevsent>
</prevsection>
<citsent citstr=" W05-0831 ">
recently, new reordering strategies have been proposed in the literature on smt such as the reordering of each source sentence to match the word order in the corresponding target sentence, see kanthak et al  (2005) <papid> W05-0831 </papid>and crego et al  (2005b).</citsent>
<aftsection>
<nextsent>similarly, matusov et al  (2006) describe method for simultaneously aligning and monotonizing the training corpus.
</nextsent>
<nextsent>the main problems of these approaches are: (1) the fact that the proposed mono ton ization is based on the alignment and cannot be applied to the test sets, and (2) the lack of reordering generalization.
</nextsent>
<nextsent>this paper presents reordering approach called statistical machine reordering (smr) which improves the reordering capabilities of smt systems without incurring any of the problems mentioned above.
</nextsent>
<nextsent>smr is first-pass translation performed on the source corpus, which converts it into an intermediate representation, in which source-language words are presented in an order that more closely matches that of the target language.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3815">
<title id=" W06-1414.xml">generic querying of relational databases using natural language generation techniques </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for reviews on various nlidbs , the reader is referred to (androutsopoulos et al, 1995).
</prevsent>
<prevsent>interpretation of the input query, including parsing and semantic disambiguation, semantic interpretation and transformation of the query to an intermediary logical form (hendrix et al, 1978; zhang et al, 1999; tang and mooney, 2001; popescu et al, 2003; kate et al, 2005); ? translation to database query language (lowden et al, 1991; androutsopoulos, 1992);?
</prevsent>
</prevsection>
<citsent citstr=" A83-1002 ">
portability (templeton and burger, 1983; <papid> A83-1002 </papid>kaplan, 1984; hafner and godden, 1985; androutsopoulos et al, 1993; popescu et al, 2003) in order to recover from errors in any either of these steps, most advanced nlidb systems will also incorporate some sort of cooperative user feedback module that will inform the user of the inability of the system to construct their query and ask for clarification.</citsent>
<aftsection>
<nextsent>we report here on generic method we have developed to automatically infer the set of possible queries that can apply to given database, andan interface that allows users to pose these questions in natural language but without the previously mentioned drawbacks of most nlidbs . ourwork is substantially different from previous research in that it does not require the user to input free text queries, but it assists the user in composing query through natural language-like interface.
</nextsent>
<nextsent>consequently, the necessity for syntactic parsing and semantic interpretation is eliminated.
</nextsent>
<nextsent>also, since users are in control of the meaning of the query they compose, ambiguity is not an issue.
</nextsent>
<nextsent>our work builds primarily on two directions of research: conceptual authoring of queries via 95 wysiwym interfaces, as described in section 2,and nlidb portability research.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3816">
<title id=" W06-1414.xml">generic querying of relational databases using natural language generation techniques </title>
<section> wysiwym interfaces for database.  </section>
<citcontext>
<prevsection>
<prevsent>our query editing interface employs natural language generation techniques for rendering queries influent language; it also allows the editing of the semantic content of query rather than its surface form, which allows seamless translation to sql . as in (zhang et al, 1999), our system makes use of semantic graph as mean of representing the database model.
</prevsent>
<prevsent>however, whilst zhang et al (1999) use the semantic graph as resource for providing and interpreting keywords in the input query, we use this information as the main means of automatically generating query frames.
</prevsent>
</prevsection>
<citsent citstr=" P98-2173 ">
querying conceptual authoring through wysiwym editing alleviates the need for expensive syntactic and semantic processing of the queries by providing the users with an interface for editing the conceptual meaning of query instead of the surface text (power and scott, 1998).<papid> P98-2173 </papid></citsent>
<aftsection>
<nextsent>the wysiwym interface presents the contents of knowledge base to the user in the form of natural language feedback text.
</nextsent>
<nextsent>in the case of query editing, the content of the knowledge base is yet to be completed formal representation of the users query.
</nextsent>
<nextsent>the interface presents the user witha natural language text that corresponds to the in complete query and guides them towards editing semantically consistent and complete query.
</nextsent>
<nextsent>inthis way, the users are able to control the interpretation that the system gives to their queries.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3817">
<title id=" W06-1414.xml">generic querying of relational databases using natural language generation techniques </title>
<section> current approach.  </section>
<citcontext>
<prevsection>
<prevsent>finally, it implements module that translates the user-composed query into sql . the components highlighted in grey are those that are constructed by the current system.
</prevsent>
<prevsent>the t-box describes the high-level components of the queries.
</prevsent>
</prevsection>
<citsent citstr=" E95-1025 ">
it is represented in profit notation (erbach, 1995) <papid> E95-1025 </papid>and describes the composition of the query frames (the elements that contribute to query and their type) . fragment of the semantic graph displayed in 2 will generate the following fragment of t-box: query   [about_company, about_state, about_phone, about_ext].</citsent>
<aftsection>
<nextsent>about_company   [company_state, company_phone, company_ext].
</nextsent>
<nextsent>company_state intro [company:company_desc].
</nextsent>
<nextsent>company_desc intro [comp:comp_desc, phone:phone_desc, ext:ext_desc].
</nextsent>
<nextsent>state_desc   external(dbo_vworgsbystate_statename?).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3818">
<title id=" W06-3403.xml">computational measures for language similarity across time in online communities </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>latent semantic analysis is another technique used for measuring document similarity.
</prevsent>
<prevsent>lsa employs vector-based model to capture the semantics of words by applying singular value decomposition on term-document matrix (landauer, foltz, &amp; laham, 1998).
</prevsent>
</prevsection>
<citsent citstr=" P04-1088 ">
lsa has been successfully applied to tasks such as measuring semantic similarity among corpora of texts (coccaro &amp; jurafsky, 1998), measuring cohesion (foltz, kintsch, &amp; landauer, 1998 ), assessing correctness of answers in tutoring systems (wiemerhastings &amp; graesser, 2000) and dialogue act classification (serafin &amp; di eugenio, 2004).<papid> P04-1088 </papid></citsent>
<aftsection>
<nextsent>to our knowledge, statistical measures like scc, zipping compression algorithms, or lsa have never been used to measure similarity of messages over time, nor have they been applied to online communities.
</nextsent>
<nextsent>however, it is not obvious how we would verify their performance, and given the nature of the task ? similarity in over 15,000 mail messages ? it is impossible to compare the computational methods to hand-coding.
</nextsent>
<nextsent>as preliminary approach, we therefore decided to apply all three methods in turn to the messages in an online community to examine change in linguistic similarity over time, and to compare their results.
</nextsent>
<nextsent>through the combination of lexical, phrasal and semantic similarity metrics, we hope to gain in sight into the questions of whether entrainment occurs in online communities, and of what computational measures can be used to measure it.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3819">
<title id=" W06-3403.xml">computational measures for language similarity across time in online communities </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>responsible for language change online?
</prevsent>
<prevsent>we also plan to analyze words in terms of their local contexts, to see if this changes over time and how it impacts our results.
</prevsent>
</prevsection>
<citsent citstr=" P99-1077 ">
furthermore, we intend to go beyond word frequency to classify topic changes over time to get better understanding of the dynamics of the groups (kaufmann, 1999).<papid> P99-1077 </papid></citsent>
<aftsection>
<nextsent>finally, as we have done in the past with our analyses of this dataset, we would like to perform percentage of hand-coded, human content analysis to check reliability of these statistical methods.
</nextsent>
<nextsent>acknowledgements thanks to members of the articulab, stefan kaufmann, stefan wuchty, will thompson, debbie zutty and lauren olson for invaluable input.
</nextsent>
<nextsent>this research was in part supported by generous grant from the kellogg foundation.
</nextsent>

</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3820">
<title id=" W06-1603.xml">paraphrase recognition via dissimilarity significance classification </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in the second phase, any unpaired nuggets are classified as significant or not (leading to pp and +pp classifications, respectively).
</prevsent>
<prevsent>if the sentences do not contain unpaired nuggets, or if all unpaired nuggets are insignificant, then the sentences are considered paraphrases.
</prevsent>
</prevsection>
<citsent citstr=" C04-1051 ">
experiments on the widely-used msr corpus (dolan et al, 2004) <papid> C04-1051 </papid>show favorable results.</citsent>
<aftsection>
<nextsent>we first review related work in section 2.
</nextsent>
<nextsent>we then present the overall methodology and describe the implemented system in section 3.
</nextsent>
<nextsent>sections 4and 5 detail the algorithms for the two phases respectively.
</nextsent>
<nextsent>this is followed with our evaluation and discussion of the results.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3822">
<title id=" W06-1603.xml">paraphrase recognition via dissimilarity significance classification </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>strategy.
</prevsent>
<prevsent>this strategy calculates cosine similarity score for the given sentence set, and if the similarity exceeds threshold (either empirically determined or learned from supervised training data), the sentences are paraphrases.
</prevsent>
</prevsection>
<citsent citstr=" W05-1203 ">
pr systems that can be broadly categorized as ir-based include (corley and mihalcea, 2005; <papid> W05-1203 </papid>brockett and dolan, 2005).<papid> I05-5001 </papid></citsent>
<aftsection>
<nextsent>in the former work, the authors defined directional similarity formula reflecting the semantic similarity of one text with respect to?
</nextsent>
<nextsent>another.
</nextsent>
<nextsent>a word contributes to the directional similarity only when its counterpart hasbeen identified in the opposing sentence.
</nextsent>
<nextsent>the associated word similarity scores, weighted by the words specificity (represented as inverted document frequency, idf ), sum to make up the directional similarity.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3823">
<title id=" W06-1603.xml">paraphrase recognition via dissimilarity significance classification </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>strategy.
</prevsent>
<prevsent>this strategy calculates cosine similarity score for the given sentence set, and if the similarity exceeds threshold (either empirically determined or learned from supervised training data), the sentences are paraphrases.
</prevsent>
</prevsection>
<citsent citstr=" I05-5001 ">
pr systems that can be broadly categorized as ir-based include (corley and mihalcea, 2005; <papid> W05-1203 </papid>brockett and dolan, 2005).<papid> I05-5001 </papid></citsent>
<aftsection>
<nextsent>in the former work, the authors defined directional similarity formula reflecting the semantic similarity of one text with respect to?
</nextsent>
<nextsent>another.
</nextsent>
<nextsent>a word contributes to the directional similarity only when its counterpart hasbeen identified in the opposing sentence.
</nextsent>
<nextsent>the associated word similarity scores, weighted by the words specificity (represented as inverted document frequency, idf ), sum to make up the directional similarity.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3825">
<title id=" W06-1603.xml">paraphrase recognition via dissimilarity significance classification </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>brockett and dolan (2005) <papid> I05-5001 </papid>represented sentence pairs asa feature vector, including features (among oth ers) for sentence length, edit distance, number of shared words, morphologically similar word pairs, synonym pairs (as suggested by wordnet and asemi-automatically constructed thesaurus).</prevsent>
<prevsent>a support vector machine is then trained to learn the {+pp,pp} classifier.strategies based on bags of words largely ignore the semantic interactions between words.</prevsent>
</prevsection>
<citsent citstr=" W05-1202 ">
weeds et al (2005) <papid> W05-1202 </papid>addressed this problem by utilizing parses for pr.</citsent>
<aftsection>
<nextsent>their system for phrasal paraphrases equates paraphrasing as distributional similarity of the partial sub-parses of candidatetext.
</nextsent>
<nextsent>wu (2005)<papid> W05-1205 </papid>s approach relies on the generative framework of inversion transduction grammar (itg) to measure how similar two sentences arrange their words based on edit distance.</nextsent>
<nextsent>barzilay and lee (2003) <papid> N03-1003 </papid>proposed to applymultiple-sequence alignment (msa) for traditional, sentence-level pr.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3826">
<title id=" W06-1603.xml">paraphrase recognition via dissimilarity significance classification </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>weeds et al (2005) <papid> W05-1202 </papid>addressed this problem by utilizing parses for pr.</prevsent>
<prevsent>their system for phrasal paraphrases equates paraphrasing as distributional similarity of the partial sub-parses of candidatetext.</prevsent>
</prevsection>
<citsent citstr=" W05-1205 ">
wu (2005)<papid> W05-1205 </papid>s approach relies on the generative framework of inversion transduction grammar (itg) to measure how similar two sentences arrange their words based on edit distance.</citsent>
<aftsection>
<nextsent>barzilay and lee (2003) <papid> N03-1003 </papid>proposed to applymultiple-sequence alignment (msa) for traditional, sentence-level pr.</nextsent>
<nextsent>given multiple articles on certain type of event, sentence clusters are first generated.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3827">
<title id=" W06-1603.xml">paraphrase recognition via dissimilarity significance classification </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>their system for phrasal paraphrases equates paraphrasing as distributional similarity of the partial sub-parses of candidatetext.
</prevsent>
<prevsent>wu (2005)<papid> W05-1205 </papid>s approach relies on the generative framework of inversion transduction grammar (itg) to measure how similar two sentences arrange their words based on edit distance.</prevsent>
</prevsection>
<citsent citstr=" N03-1003 ">
barzilay and lee (2003) <papid> N03-1003 </papid>proposed to applymultiple-sequence alignment (msa) for traditional, sentence-level pr.</citsent>
<aftsection>
<nextsent>given multiple articles on certain type of event, sentence clusters are first generated.
</nextsent>
<nextsent>sentences within the same cluster, presumably similar in structure and content,are then used to construct lattice with back bone?
</nextsent>
<nextsent>nodes corresponding to words shared by the majority and slots?
</nextsent>
<nextsent>corresponding to different realization of arguments.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3828">
<title id=" W06-1603.xml">paraphrase recognition via dissimilarity significance classification </title>
<section> methodology.  </section>
<citcontext>
<prevsection>
<prevsent>in comparison, using fine-grained units such aswords, including nouns and verbs may result in inaccuracy (sentences that share vocabulary may not be paraphrases), while using coarser-grained units may cause key differences to be missed.
</prevsent>
<prevsent>in the rest of this paper, we use the term tuple for conciseness when no ambiguity is introduced.an overview of our paraphrase recognition system is shown in figure 2.
</prevsent>
</prevsection>
<citsent citstr=" A00-2018 ">
a pair of sentences is first fed to syntactic parser (charniak, 2000) <papid> A00-2018 </papid>and then passed to semantic role labeler (assert;(pradhan et al, 2004)), <papid> N04-1030 </papid>to label predicate argument tuples.</citsent>
<aftsection>
<nextsent>we then calculate normalized tuplesimilarity scores over the tuple pairs using metric that accounts for similarities in both syntactic structure and content of each tuple.
</nextsent>
<nextsent>a thesaurus constructed from corpus statistics (lin, 1998) <papid> P98-2127 </papid>is utilized for the content similarity.</nextsent>
<nextsent>we utilize this metric to greedily pair together the most similar predicate argument tuples across figure 2: system architecturesentences.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3829">
<title id=" W06-1603.xml">paraphrase recognition via dissimilarity significance classification </title>
<section> methodology.  </section>
<citcontext>
<prevsection>
<prevsent>in comparison, using fine-grained units such aswords, including nouns and verbs may result in inaccuracy (sentences that share vocabulary may not be paraphrases), while using coarser-grained units may cause key differences to be missed.
</prevsent>
<prevsent>in the rest of this paper, we use the term tuple for conciseness when no ambiguity is introduced.an overview of our paraphrase recognition system is shown in figure 2.
</prevsent>
</prevsection>
<citsent citstr=" N04-1030 ">
a pair of sentences is first fed to syntactic parser (charniak, 2000) <papid> A00-2018 </papid>and then passed to semantic role labeler (assert;(pradhan et al, 2004)), <papid> N04-1030 </papid>to label predicate argument tuples.</citsent>
<aftsection>
<nextsent>we then calculate normalized tuplesimilarity scores over the tuple pairs using metric that accounts for similarities in both syntactic structure and content of each tuple.
</nextsent>
<nextsent>a thesaurus constructed from corpus statistics (lin, 1998) <papid> P98-2127 </papid>is utilized for the content similarity.</nextsent>
<nextsent>we utilize this metric to greedily pair together the most similar predicate argument tuples across figure 2: system architecturesentences.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3830">
<title id=" W06-1603.xml">paraphrase recognition via dissimilarity significance classification </title>
<section> methodology.  </section>
<citcontext>
<prevsection>
<prevsent>a pair of sentences is first fed to syntactic parser (charniak, 2000) <papid> A00-2018 </papid>and then passed to semantic role labeler (assert;(pradhan et al, 2004)), <papid> N04-1030 </papid>to label predicate argument tuples.</prevsent>
<prevsent>we then calculate normalized tuplesimilarity scores over the tuple pairs using metric that accounts for similarities in both syntactic structure and content of each tuple.</prevsent>
</prevsection>
<citsent citstr=" P98-2127 ">
a thesaurus constructed from corpus statistics (lin, 1998) <papid> P98-2127 </papid>is utilized for the content similarity.</citsent>
<aftsection>
<nextsent>we utilize this metric to greedily pair together the most similar predicate argument tuples across figure 2: system architecturesentences.
</nextsent>
<nextsent>any remaining unpaired tuples represent extra information and are passed to dissimilarity classifier to decide whether such information is significant.
</nextsent>
<nextsent>the dissimilarity classifier uses supervised machine learning to make such decision.
</nextsent>
<nextsent>we illustrate this advantage of using predicate argument tuples from our running example.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3832">
<title id=" W06-1603.xml">paraphrase recognition via dissimilarity significance classification </title>
<section> dissimilarity significance.  </section>
<citcontext>
<prevsection>
<prevsent>currently, only two features are correlated in improved classification, which we detail now.
</prevsent>
<prevsent>syntactic parse tree path: this is series of features that reflect how the unpaired tuple connects with the context: the rest of the sentence.
</prevsent>
</prevsection>
<citsent citstr=" P02-1031 ">
it models the syntactic connection between the constituents on both ends of the path (gildea and palmer, 2002; <papid> P02-1031 </papid>pradhan et al, 2004).<papid> N04-1030 </papid></citsent>
<aftsection>
<nextsent>here, we model the ends of the path as the unpaired tupleand the paired tuple with the closest shared ancestor, and model the path itself as sequence of constituent category tags and directions to reach the destination (the paired target) from the source (the 21 unpaired target) via the the shared ancestor.
</nextsent>
<nextsent>when no tuples have been paired in the sentence pair, the destination defaults to the root of the syntactic parse tree.
</nextsent>
<nextsent>for example, the tuples with target in jured?
</nextsent>
<nextsent>are unpaired when the model sentence and the non-paraphrasing modification in table 1 are being compared.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3839">
<title id=" W06-2902.xml">porting statistical parsers with data defined kernels </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>with svm classifier and neural network probabilistic model, this method achieves improved performance over the probabilistic model alone.
</prevsent>
<prevsent>in recent years, significant progress has been madein the area of natural language parsing.
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
this research has focused mostly on the development of statistical parsers trained on large annotated corpora, in particular the penn treebank wsj corpus (marcus et al , 1993).<papid> J93-2004 </papid></citsent>
<aftsection>
<nextsent>the best statistical parsers have shown good results on this benchmark, but these statistical parsers demonstrate far worse results when they are applied to data from different domain (roark and bacchiani, 2003; <papid> N03-1027 </papid>gildea, 2001; <papid> W01-0521 </papid>ratnaparkhi, 1999).this is an important problem because we cannot expect to have large annotated corpora available formost domains.</nextsent>
<nextsent>while identifying this problem, previous work has not proposed parsing methods which are specifically designed for porting parsers.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3841">
<title id=" W06-2902.xml">porting statistical parsers with data defined kernels </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in recent years, significant progress has been madein the area of natural language parsing.
</prevsent>
<prevsent>this research has focused mostly on the development of statistical parsers trained on large annotated corpora, in particular the penn treebank wsj corpus (marcus et al , 1993).<papid> J93-2004 </papid></prevsent>
</prevsection>
<citsent citstr=" N03-1027 ">
the best statistical parsers have shown good results on this benchmark, but these statistical parsers demonstrate far worse results when they are applied to data from different domain (roark and bacchiani, 2003; <papid> N03-1027 </papid>gildea, 2001; <papid> W01-0521 </papid>ratnaparkhi, 1999).this is an important problem because we cannot expect to have large annotated corpora available formost domains.</citsent>
<aftsection>
<nextsent>while identifying this problem, previous work has not proposed parsing methods which are specifically designed for porting parsers.
</nextsent>
<nextsent>instead they propose methods for training standard parser with large amount of out-of-domain data and small amount of in-domain data.in this paper, we propose using data-defined kernels and large margin methods to specifically address porting parser to new domain.
</nextsent>
<nextsent>data-definedkernels are used to construct new parser which exploits information from parser trained on large out-of-domain corpus.
</nextsent>
<nextsent>large margin methods are used to train this parser to optimize performance on small in-domain corpus.large margin methods have demonstrated substantial success in applications to many machine learning problems, because they optimize measure which is directly related to the expected testing performance.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3842">
<title id=" W06-2902.xml">porting statistical parsers with data defined kernels </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in recent years, significant progress has been madein the area of natural language parsing.
</prevsent>
<prevsent>this research has focused mostly on the development of statistical parsers trained on large annotated corpora, in particular the penn treebank wsj corpus (marcus et al , 1993).<papid> J93-2004 </papid></prevsent>
</prevsection>
<citsent citstr=" W01-0521 ">
the best statistical parsers have shown good results on this benchmark, but these statistical parsers demonstrate far worse results when they are applied to data from different domain (roark and bacchiani, 2003; <papid> N03-1027 </papid>gildea, 2001; <papid> W01-0521 </papid>ratnaparkhi, 1999).this is an important problem because we cannot expect to have large annotated corpora available formost domains.</citsent>
<aftsection>
<nextsent>while identifying this problem, previous work has not proposed parsing methods which are specifically designed for porting parsers.
</nextsent>
<nextsent>instead they propose methods for training standard parser with large amount of out-of-domain data and small amount of in-domain data.in this paper, we propose using data-defined kernels and large margin methods to specifically address porting parser to new domain.
</nextsent>
<nextsent>data-definedkernels are used to construct new parser which exploits information from parser trained on large out-of-domain corpus.
</nextsent>
<nextsent>large margin methods are used to train this parser to optimize performance on small in-domain corpus.large margin methods have demonstrated substantial success in applications to many machine learning problems, because they optimize measure which is directly related to the expected testing performance.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3843">
<title id=" W06-2902.xml">porting statistical parsers with data defined kernels </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>most of the large margin methods need the definition of kernel.
</prevsent>
<prevsent>work on kernels for natural language parsing has been mostly focused on the definition of kernels over parse trees (e.g.
</prevsent>
</prevsection>
<citsent citstr=" P02-1034 ">
(collins and duffy, 2002)), <papid> P02-1034 </papid>which are chosen on the basis of domain knowledge.</citsent>
<aftsection>
<nextsent>in (henderson and titov, 2005) <papid> P05-1023 </papid>it was proposed to apply class of kernels derived from probabilistic models to the natural language parsing problem.in (henderson and titov, 2005),<papid> P05-1023 </papid>the kernel is constructed using the parameters of trained probabilistic model.</nextsent>
<nextsent>this type of kernel is called data defined kernel, because the kernel incorporates information from the data used to train the probabilistic model.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3844">
<title id=" W06-2902.xml">porting statistical parsers with data defined kernels </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>work on kernels for natural language parsing has been mostly focused on the definition of kernels over parse trees (e.g.
</prevsent>
<prevsent>(collins and duffy, 2002)), <papid> P02-1034 </papid>which are chosen on the basis of domain knowledge.</prevsent>
</prevsection>
<citsent citstr=" P05-1023 ">
in (henderson and titov, 2005) <papid> P05-1023 </papid>it was proposed to apply class of kernels derived from probabilistic models to the natural language parsing problem.in (henderson and titov, 2005),<papid> P05-1023 </papid>the kernel is constructed using the parameters of trained probabilistic model.</citsent>
<aftsection>
<nextsent>this type of kernel is called data defined kernel, because the kernel incorporates information from the data used to train the probabilistic model.
</nextsent>
<nextsent>we propose to exploit this property to transfer information from large corpus to statis 6 tical parser for different domain.
</nextsent>
<nextsent>specifically, we propose to train statistical parser on data including the large corpus, and to derive the kernel from this trained model.
</nextsent>
<nextsent>then this derived kernel is used in large margin classifier trained on the small amount of training data available for the target domain.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3851">
<title id=" W06-2902.xml">porting statistical parsers with data defined kernels </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this classifier is trained tore rank the candidate parses selected by the associated probabilistic model.
</prevsent>
<prevsent>we use the penn treebank wall street journal corpus as the large corpus and individual sections of the brown corpus as the target corpora (marcus et al , 1993).<papid> J93-2004 </papid></prevsent>
</prevsection>
<citsent citstr=" N03-1014 ">
the probabilistic model is neural network statistical parser (hen derson, 2003), <papid> N03-1014 </papid>and the data-defined kernel is top reranking kernel (henderson and titov, 2005).<papid> P05-1023 </papid>with both scenarios, the resulting parser demonstrates improved accuracy on the target domain overthe probabilistic model alone.</citsent>
<aftsection>
<nextsent>in additional experiments, we evaluate the hypothesis that the primary issue for porting parsers between domains is differences in the distributions of words in structures, and not in the distributions of the structures themselves.
</nextsent>
<nextsent>we partition the parameters of the probability model into those which define the distributions of words and those that only involve structural decisions, and derive separate kernels for these two subsets of parameters.
</nextsent>
<nextsent>the former model achieves virtually identical accuracy to the full model, but the later model does worse, confirming the hypothesis.
</nextsent>
<nextsent>previous work has shown how data-defined kernels can be applied to the parsing task (henderson and titov, 2005).<papid> P05-1023 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3859">
<title id=" W06-2902.xml">porting statistical parsers with data defined kernels </title>
<section> porting with data-defined kernels.  </section>
<citcontext>
<prevsection>
<prevsent>the resulting parser consists of the original parser plus very computationally cheap procedure to rerank its best parses.whereas training of standard large margin methods, like svms, isnt feasible on large corpus, itis quite tractable to train them on small target cor pus.1 also, the choice of the large margin classifier is motivated by their good generalization properties on small datasets, on which accurate probabilistic models are usually difficult to learn.
</prevsent>
<prevsent>we hypothesize that differences in vocabulary across domains is one of the main difficulties with parser portability.
</prevsent>
</prevsection>
<citsent citstr=" W03-0402 ">
to address this problem, we propose constructing the kernel from probabilistic model which has been re parameterized to better suit1in (shen and joshi, 2003) <papid> W03-0402 </papid>it was proposed to use an ensemble of svms trained the wall street journal corpus, but we believe that the generalization performance of the resulting classifier is compromised in this approach.</citsent>
<aftsection>
<nextsent>the target domain vocabulary.
</nextsent>
<nextsent>as in other lexicalized statistical parsers, the probabilistic model we use treats words which are not frequent enough in the training set as unknown?
</nextsent>
<nextsent>words (henderson, 2003).<papid> N03-1014 </papid></nextsent>
<nextsent>thus there are no parameters in this model which are specifically for these words.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3866">
<title id=" W06-2902.xml">porting statistical parsers with data defined kernels </title>
<section> an application to neural network.  </section>
<citcontext>
<prevsection>
<prevsent>with latent variables, the meaning of the variable (not just its value) is learned from the data, and the associated features ofthe data-defined kernel capture this induced meaning.
</prevsent>
<prevsent>there has been much recent work on latent variable models (e.g.
</prevsent>
</prevsection>
<citsent citstr=" P05-1010 ">
(matsuzaki et al , 2005; <papid> P05-1010 </papid>koo and collins, 2005)).<papid> H05-1064 </papid></citsent>
<aftsection>
<nextsent>we choose to use an earlier neural network based probabilistic model of parsing (henderson, 2003), <papid> N03-1014 </papid>whose hidden units can be viewed as approximations to latent variables.</nextsent>
<nextsent>this parsing model is also good candidate for our experiments because it achieves state-of-the-art results onthe standard wall street journal (wsj) parsing problem (henderson, 2003), <papid> N03-1014 </papid>and data-defined kernels derived from this parsing model have recently been used with the voted perceptron algorithm on thewsj parsing task, achieving significant improvement inaccuracy over the neural network parser alone (henderson and titov, 2005).<papid> P05-1023 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3867">
<title id=" W06-2902.xml">porting statistical parsers with data defined kernels </title>
<section> an application to neural network.  </section>
<citcontext>
<prevsection>
<prevsent>with latent variables, the meaning of the variable (not just its value) is learned from the data, and the associated features ofthe data-defined kernel capture this induced meaning.
</prevsent>
<prevsent>there has been much recent work on latent variable models (e.g.
</prevsent>
</prevsection>
<citsent citstr=" H05-1064 ">
(matsuzaki et al , 2005; <papid> P05-1010 </papid>koo and collins, 2005)).<papid> H05-1064 </papid></citsent>
<aftsection>
<nextsent>we choose to use an earlier neural network based probabilistic model of parsing (henderson, 2003), <papid> N03-1014 </papid>whose hidden units can be viewed as approximations to latent variables.</nextsent>
<nextsent>this parsing model is also good candidate for our experiments because it achieves state-of-the-art results onthe standard wall street journal (wsj) parsing problem (henderson, 2003), <papid> N03-1014 </papid>and data-defined kernels derived from this parsing model have recently been used with the voted perceptron algorithm on thewsj parsing task, achieving significant improvement inaccuracy over the neural network parser alone (henderson and titov, 2005).<papid> P05-1023 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3879">
<title id=" W06-2902.xml">porting statistical parsers with data defined kernels </title>
<section> experimental results.  </section>
<citcontext>
<prevsection>
<prevsent>5.4 discussion of results.
</prevsent>
<prevsent>for the experiments which directly test the usefulness of our proposed porting technique (ssn-wsj versus top-transfer), our technique demonstrated improvement for each of the brown sections (ta ble 2), and this improvement was significant for three out of four of the sections (k, n, and p).6 this demonstrates that data-defined kernels are an effective way to port parsers to new domain.
</prevsent>
</prevsection>
<citsent citstr=" C00-2137 ">
for the experiments which combine training new probability model with our porting technique (ssn-wsj+br versus top-focus), our technique still demonstrated improvement over training alone.there was improvement for each of the brown sections, and this improvement was significant for two 6we measured significance in f1 measure at the 5% level with the randomized significance test of (yeh, 2000).<papid> C00-2137 </papid></citsent>
<aftsection>
<nextsent>we think that the reason the improvement on section was only significant at the 10% level was that the baseline model (ssn-wsj) was particularly lucky, as indicated by the fact that it did even better than the model trained on the combination of datasets (ssn-wsj+br).
</nextsent>
<nextsent>11 lr lp f?=1 ssn-wsj 83.1 83.8 83.5 top-transfer 83.5 84.7 84.1 top-voc-transfer 83.5 84.7 84.1 top-str-transfer 83.1 84.3 83.7 ssn-wsj+br 83.8 85.0 84.4 top-focus 84.1 85.6 84.9 top-voc-focus 84.1 85.6 84.8 top-str-focus 83.9 85.4 84.7 table 3: average accuracy of the models on chapters f, k, and of the brown corpus.out of four of the sections (f and k).
</nextsent>
<nextsent>this demonstrates that, even when the probability model is well suited to the target domain, there is still room for improvement from using data-defined kernels to optimize the parser specifically to the target domain without losing information about the source domain.
</nextsent>
<nextsent>one potential criticism of these conclusions is that the improvement could be the result of reranking with the top kernel, and have nothing to do withporting.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3887">
<title id=" W06-2902.xml">porting statistical parsers with data defined kernels </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>they reported results when their parser was trained on the wsj training 12 set plus portion of 2,000 sentences from brown corpus section.
</prevsent>
<prevsent>they achieved 80.9%/80.3% re call/precision for section k, and 80.6%/81.3% for section n.7 our analogous method (top-focus)achieved much better accuracy (3.7% and 4.9% better f1, respectively).
</prevsent>
</prevsection>
<citsent citstr=" P97-1003 ">
in addition to portability experiments with the parsing model of (collins, 1997), (<papid> P97-1003 </papid>gildea, 2001)<papid> W01-0521 </papid>provided comprehensive analysis of parser portability.</citsent>
<aftsection>
<nextsent>on the basis of this analysis, technique for parameter pruning was proposed leading to significant reduction in the model size with out large decrease of accuracy.
</nextsent>
<nextsent>gildea (2001) <papid> W01-0521 </papid>only reports results on sentences of 40 or less words on all the brown corpus sections combined, for which he reports 80.3%/81.0% recall/precision when training only on data from the wsj corpus, and 83.9%/84.8% when training on data from the wsj corpus and all sections of the brown corpus.(roark and bacchiani, 2003) <papid> N03-1027 </papid>performed experiments on supervised and unsupervised pcfg adaptation to the target domain.</nextsent>
<nextsent>they propose to usethe statistics from source domain to define priors over weights.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3894">
<title id=" W06-1413.xml">the clarity brevity tradeoff in generating referring expressions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we sketch some more complex phenomena which might be amenable to this treatment.
</prevsent>
<prevsent>until recently, gre algorithms have focussed on the generation of distinguishing descriptions that are either as short as possible (e.g.
</prevsent>
</prevsection>
<citsent citstr=" P02-1013 ">
(dale, 1992; gardent, 2002)) <papid> P02-1013 </papid>or almost as short as possible (e.g.(dale and reiter, 1995)).</citsent>
<aftsection>
<nextsent>since reductions in ambiguity are achieved by increases in length, there is tension between these factors, and algorithms usually resolve this in some fixed way.
</nextsent>
<nextsent>however, the need for distinguishing description is usually assumed, and typically built in to gre algorithms.we will suggest way to make explicit this balance between clarity (i.e. lack of ambiguity) and brevity, and we indicate some phenomena which we believe may be illuminated by this approach.the ideas in this paper can be seen as loosening of some of the many simplifying assumptions often made in gre work.
</nextsent>
<nextsent>this work is supported by university of aberdeen sixth century studentship, and the tuna project (epsrc, uk) under grant number gr/s13330/01.
</nextsent>
<nextsent>we thank ielka vander sluis and albert gatt for valuable comments.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3895">
<title id=" W06-1413.xml">the clarity brevity tradeoff in generating referring expressions </title>
<section> clarity, brevity and cost.  </section>
<citcontext>
<prevsection>
<prevsent>our notation is as follows.
</prevsent>
<prevsent>a domain consists of set of objects, and set pof properties applicable to objects in d. description is subset of p. the denot ation of s, written [[ ]], is {x ? | ? : p(x)}.
</prevsent>
</prevsection>
<citsent citstr=" J03-1003 ">
(krahmer et al , 2003) <papid> J03-1003 </papid>describe an approach to grein which cost function guides search for suitable description, and show that some existing gre algorithms fit into this framework.</citsent>
<aftsection>
<nextsent>however, they follow the practice of concentrating solely on distinguishing descriptions, treating cost as matter of brevity.
</nextsent>
<nextsent>we suggest that decomposing cost into two components, for the clarity and brevity of descriptions, permits the examination of tradeoffs.
</nextsent>
<nextsent>for now, we will take the cost of description to be the sum of two terms: cost(s) = fc(s) + fb(s).
</nextsent>
<nextsent>where fc counts ambiguity (lack of clarity) and fb counts size (lack of brevity).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3896">
<title id=" W06-1413.xml">the clarity brevity tradeoff in generating referring expressions </title>
<section> b: its just fern plant. </section>
<citcontext>
<prevsection>
<prevsent>recently, interest has been growing in over spec ified?
</prevsent>
<prevsent>referring expressions, which contain more information than is required to identify their intended referent.
</prevsent>
</prevsection>
<citsent citstr=" P00-1024 ">
some of this work is mainly or exclusively experimental (jordan and walker, 2000; <papid> P00-1024 </papid>arts, 2004), but algorithmic consequences are also being explored (horacek, 2005; <papid> W05-1606 </papid>paraboni and van deemter, 2002; vander sluis and krahmer, 2005).</citsent>
<aftsection>
<nextsent>over-specification could also arise in dialogue situation (comparable to that in section 3.3) if speaker is unclear about the hearers knowledge,and so over-specifies (relative to his own knowl edge) to increase the chances of success.
</nextsent>
<nextsent>this goes beyond the classical algorithms,where the main goal is total clarity, with no reason for the algorithm to add further properties to an already unambiguous expression.
</nextsent>
<nextsent>that is, such algorithms assume that every description for which | [[ ]] |= 1 has the same level of clarity (fc value).
</nextsent>
<nextsent>this assumption could be relaxed.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3897">
<title id=" W06-1413.xml">the clarity brevity tradeoff in generating referring expressions </title>
<section> b: its just fern plant. </section>
<citcontext>
<prevsection>
<prevsent>recently, interest has been growing in over spec ified?
</prevsent>
<prevsent>referring expressions, which contain more information than is required to identify their intended referent.
</prevsent>
</prevsection>
<citsent citstr=" W05-1606 ">
some of this work is mainly or exclusively experimental (jordan and walker, 2000; <papid> P00-1024 </papid>arts, 2004), but algorithmic consequences are also being explored (horacek, 2005; <papid> W05-1606 </papid>paraboni and van deemter, 2002; vander sluis and krahmer, 2005).</citsent>
<aftsection>
<nextsent>over-specification could also arise in dialogue situation (comparable to that in section 3.3) if speaker is unclear about the hearers knowledge,and so over-specifies (relative to his own knowl edge) to increase the chances of success.
</nextsent>
<nextsent>this goes beyond the classical algorithms,where the main goal is total clarity, with no reason for the algorithm to add further properties to an already unambiguous expression.
</nextsent>
<nextsent>that is, such algorithms assume that every description for which | [[ ]] |= 1 has the same level of clarity (fc value).
</nextsent>
<nextsent>this assumption could be relaxed.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3899">
<title id=" W06-3508.xml">searching for grammar right </title>
<section> formalizing construction grammar.  </section>
<citcontext>
<prevsection>
<prevsent>this property defines that the accusative singular determiner construction and the corresponding noun construction can be requisite-for the compositional construction that combines these two lexical constructions into one noun phrase.
</prevsent>
<prevsent>5 for further information about which operators are used to.
</prevsent>
</prevsection>
<citsent citstr=" W04-2009 ">
model these features in ecg we refer to bergen and chang (2002), chang et al (2002) and bryant (2004).<papid> W04-2009 </papid></citsent>
<aftsection>
<nextsent>6 following bryants (2004) division of constructions into 5.
</nextsent>
<nextsent>levels of different degrees of schematicity.
</nextsent>
<nextsent>3.2 modeling of image schemas.
</nextsent>
<nextsent>following johnson and lakoff (johnson, 1987; lakoff and johnson, 1980; lakoff, 1987) image schemas are schematic representations that capture recurrent patterns of sensorimotor experience.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3900">
<title id=" W06-3508.xml">searching for grammar right </title>
<section> the web as corpus.  </section>
<citcontext>
<prevsection>
<prevsent>we search similar pages, adding add them to our original corpus c, as we expect the likelihood of still pretty good cover age together with some new constructions to be maximal, thereby enabling our incremental learning approach.
</prevsent>
<prevsent>the question emerging hereby is: what constitutes similar web page?
</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
what, therefore, has to be explored are various similarity metrics, defining similarity in concrete way and evaluate the results against human annotations (see papineni et al, 2002).<papid> P02-1040 </papid></citsent>
<aftsection>
<nextsent>4.1 similarity metric.
</nextsent>
<nextsent>to be able to answer the question which texts are actually similar, similarity needs to be defined precisely.
</nextsent>
<nextsent>different approaches could be employed, 61 i.e. regarding similarity in terms of syntactic or semantic phenomena or combination of both.
</nextsent>
<nextsent>since construction grammar makes no separation between syntax and semantics, phenomena that should be counted are both constructions and im age schemas.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3901">
<title id=" W06-3508.xml">searching for grammar right </title>
<section> grammar learning.  </section>
<citcontext>
<prevsection>
<prevsent>the analysis of term yields information about its stem, its part-of-speech, its case, its number, and its grammatical gender.
</prevsent>
<prevsent>this information can then easily be integrated automatically into the ectolog.
</prevsent>
</prevsection>
<citsent citstr=" W06-3505 ">
as already mentioned in section 4.3, we are not only trying to automatically acquire the form pole of the constructions, but also its image schematic meaning, that means the network of the schemas that hierarchically form the meaning pole of such term, applying ontology learning mechanisms (e.g. loos, 2006) <papid> W06-3505 </papid>and methods similar to those described in section 4.3.</citsent>
<aftsection>
<nextsent>additionally, investigations are underway to connect the grammar learning framework proposed herein to computervision system that provides supplementary feedback con 8 we are aware of the fact that fast mapping in humans is lim-.
</nextsent>
<nextsent>ited to color terms, shapes or texture terms, but are employing the method on other kinds of terms, nevertheless, since the grammar learning paradigm in our approach is still in its baby shoes.
</nextsent>
<nextsent>62 cerning the hypothesized semantics of individual forms in the case of multi-media information.
</nextsent>
<nextsent>5.2 learning compositional constructions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3903">
<title id=" W06-3508.xml">searching for grammar right </title>
<section> grammar learning.  </section>
<citcontext>
<prevsection>
<prevsent>these need to be merged, split or maybe thrown out again, depending on their utility, similarity etc. 5.3 ambiguity.
</prevsent>
<prevsent>currently the problem of ambiguity is solved by endowing the analyzer with chart and employing the semantic density algorithm described in (bry ant, 2004).<papid> W04-2009 </papid></prevsent>
</prevsection>
<citsent citstr=" W04-2312 ">
in the future probabilistic reasoning frameworks as proposed by (narayanan and jurafsky, 2005) in combination with ontology-based coherence measures as proposed by (loos and porzel, 2004) <papid> W04-2312 </papid>constitute promising approaches for handling problems of constr ual, whether it be on pragmatic, semantic, syntactic or phonological level.</citsent>
<aftsection>
<nextsent>in this paper we described our ongoing work in and thoughts on developing grammar learning system based on construction grammar formalism used in question-answering system.
</nextsent>
<nextsent>we described necessary modules and presented first results and challenges in formalizing construction grammar.
</nextsent>
<nextsent>furthermore, we pointed out our motivation for choosing construction grammar and the, therefore, resulting advantages.
</nextsent>
<nextsent>then our approach and ideas of learning new linguistic phenomena, ranging from holophrastic constructions to compositional ones, were presented.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3904">
<title id=" W06-2504.xml">whats in a name the automatic recognition of metonymical location names </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>examples like this one demonstrate that metonymy recognition should not be based onrigid rules, but rather, on information about these mantic class of the target word and the semantic and grammatical context in which it occurs.
</prevsent>
<prevsent>in psycho linguistics, this insight (among others) has given rise to theories claiming that figurative interpretation does not follow the failure of liter alone, but that both processes occur in parallel (frisson and pickering, 1999).
</prevsent>
</prevsection>
<citsent citstr=" W02-1027 ">
in computational linguistics, it has led to the development of statisti 25cal, corpus-based approaches to metonymy recog nition.this view was first put into computational practice by markert and nissim (2002<papid> W02-1027 </papid>a).</citsent>
<aftsection>
<nextsent>their key to success was the realization that metonymy recognition is sub-problem of word sense disambiguation (wsd).
</nextsent>
<nextsent>they found that most metonymies in the same semantic class belong to one of limited number of metonymical patterns that can be defined priori.
</nextsent>
<nextsent>the task of metonymyrecognition thus consists of the automatic assignment of one of these readings to target word.
</nextsent>
<nextsent>since all words in the same semantic class may undergo the same semantic shifts, there only has to be one classifier per class (and not per word, as in classic wsd).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3912">
<title id=" W06-2504.xml">whats in a name the automatic recognition of metonymical location names </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this paper will be concerned with the automatic identification of metonymical location names.
</prevsent>
<prevsent>more particularly, will test two new approaches to metonymy recognition on the basis of markert and nissims (2002<papid> W02-1027 </papid>b) corpora of 1,000 mixed country names and 1,000 instances ofthe country name hungary.1 the most important metonymical patterns in these corpora are place-for-people, place-for-event and place-for-product.</prevsent>
</prevsection>
<citsent citstr=" P03-1008 ">
in addition, there is label mixed for examples that have two readings, and other met for examples that do not belong to any of the pre-defined metonymical patterns.on the mixed country data, nissim and mark erts (2003) <papid> P03-1008 </papid>classifiers achieved an accuracy of 87%.</citsent>
<aftsection>
<nextsent>this was the result of combination of both grammatical and semantic information.
</nextsent>
<nextsent>their grammatical information included the function ofa target word and its head.
</nextsent>
<nextsent>the semantic information, in the form of dekang lins (1998) thesaurus of semantically similar words, allowed the classifier to search the training set for instances whose head was similar, and not just identical, to that of test instance.
</nextsent>
<nextsent>markert and nissims (2002<papid> W02-1027 </papid>a) and nissim and marker ts (2003) <papid> P03-1008 </papid>study is the only one to approachmetonymy recognition from data-driven, statistical perspective.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3921">
<title id=" W06-2504.xml">whats in a name the automatic recognition of metonymical location names </title>
<section> an unsupervised approach to.  </section>
<citcontext>
<prevsection>
<prevsent>second, this algorithm lies at the basis of latent semantic analysis (lsa).
</prevsent>
<prevsent>although the psycho linguistic merits of lsa are an object of debate, its performance in several language tasks compares well to that of humans (landauer and dumais, 1997).
</prevsent>
</prevsection>
<citsent citstr=" W04-2406 ">
let us therefore investigate if it is able to tackle metonymy recognition as well.schutzes (1998) approach has been implemented in the sense clusters program (purandare and pedersen, 2004)<papid> W04-2406 </papid>2, which also incorporates some interesting variations on and extensions tothe original algorithm.</citsent>
<aftsection>
<nextsent>first, purandare and pedersen (2004) <papid> W04-2406 </papid>defend the use of bigram features instead of simple word features.</nextsent>
<nextsent>bigrams are ordered pairs of words that co-occur within five positions of each other?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3959">
<title id=" W06-1410.xml">algorithms for generating referring expressions do they do what people do </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>on the basis of these observations, we suggest some ways forward that attempt to address these differences.
</prevsent>
<prevsent>the generation of referring expressions (hence forth gre) ? that is, the process of working out what properties of an entity should be used to describe it in such way as to distinguish it from other entities in the context ? is recurrent theme in the natural language generation literature.
</prevsent>
</prevsection>
<citsent citstr=" P89-1009 ">
the task is discussed informally in some of the earliest work on nlg (in particular, see (winograd, 1972; mcdonald, 1980; appelt, 1981)), but the first formally explicit algorithm was introduced in (dale, 1989); <papid> P89-1009 </papid>this algorithm,often referred to as the full brevity (fb) algorithm, has served as starting point for many subsequent gre algorithms.</citsent>
<aftsection>
<nextsent>to overcome its limitation to one-place predicates, dale and haddock (1991) <papid> E91-1028 </papid>introduced constraint-based procedure that could generate referring expressions involving relations; and as response to the computational complexity of greedy?</nextsent>
<nextsent>algorithms like fb, reiter and dale (reiter and dale, 1992; <papid> C92-1038 </papid>dale and reiter, 1995) introduced the psycholinguistically motivated incremental algorithm (ia).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3960">
<title id=" W06-1410.xml">algorithms for generating referring expressions do they do what people do </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the generation of referring expressions (hence forth gre) ? that is, the process of working out what properties of an entity should be used to describe it in such way as to distinguish it from other entities in the context ? is recurrent theme in the natural language generation literature.
</prevsent>
<prevsent>the task is discussed informally in some of the earliest work on nlg (in particular, see (winograd, 1972; mcdonald, 1980; appelt, 1981)), but the first formally explicit algorithm was introduced in (dale, 1989); <papid> P89-1009 </papid>this algorithm,often referred to as the full brevity (fb) algorithm, has served as starting point for many subsequent gre algorithms.</prevsent>
</prevsection>
<citsent citstr=" E91-1028 ">
to overcome its limitation to one-place predicates, dale and haddock (1991) <papid> E91-1028 </papid>introduced constraint-based procedure that could generate referring expressions involving relations; and as response to the computational complexity of greedy?</citsent>
<aftsection>
<nextsent>algorithms like fb, reiter and dale (reiter and dale, 1992; <papid> C92-1038 </papid>dale and reiter, 1995) introduced the psycholinguistically motivated incremental algorithm (ia).</nextsent>
<nextsent>in recent years there have been number of important extensions to the ia. the context-sensitive extension (krahmer and theune, 2002) is able to generate referring expressions for the most salient entity in context; the boolean expressions algorithm (van deemter, 2002) is able to derive expressions containing boolean operators, as in the cup that does not have handle; and the sets algorithm (van deemter, 2002) extends the basic approach to references to sets, as in the redcups.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3961">
<title id=" W06-1410.xml">algorithms for generating referring expressions do they do what people do </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the task is discussed informally in some of the earliest work on nlg (in particular, see (winograd, 1972; mcdonald, 1980; appelt, 1981)), but the first formally explicit algorithm was introduced in (dale, 1989); <papid> P89-1009 </papid>this algorithm,often referred to as the full brevity (fb) algorithm, has served as starting point for many subsequent gre algorithms.</prevsent>
<prevsent>to overcome its limitation to one-place predicates, dale and haddock (1991) <papid> E91-1028 </papid>introduced constraint-based procedure that could generate referring expressions involving relations; and as response to the computational complexity of greedy?</prevsent>
</prevsection>
<citsent citstr=" C92-1038 ">
algorithms like fb, reiter and dale (reiter and dale, 1992; <papid> C92-1038 </papid>dale and reiter, 1995) introduced the psycholinguistically motivated incremental algorithm (ia).</citsent>
<aftsection>
<nextsent>in recent years there have been number of important extensions to the ia. the context-sensitive extension (krahmer and theune, 2002) is able to generate referring expressions for the most salient entity in context; the boolean expressions algorithm (van deemter, 2002) is able to derive expressions containing boolean operators, as in the cup that does not have handle; and the sets algorithm (van deemter, 2002) extends the basic approach to references to sets, as in the redcups.
</nextsent>
<nextsent>some approaches reuse parts of other algorithms: the branch and bound algorithm (krah mer et al , 2003) <papid> J03-1003 </papid>uses the full brevity algorithm, but is able to generate referring expressions with both attributes and relational descriptions using agraph-based technique.</nextsent>
<nextsent>there are many other algorithms described in the literature: see, for example, (horacek, 1997; <papid> P97-1027 </papid>bateman, 1999; <papid> P99-1017 </papid>stone, 2000; <papid> W00-1416 </papid>gardent, 2002).<papid> P02-1013 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3963">
<title id=" W06-1410.xml">algorithms for generating referring expressions do they do what people do </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>algorithms like fb, reiter and dale (reiter and dale, 1992; <papid> C92-1038 </papid>dale and reiter, 1995) introduced the psycholinguistically motivated incremental algorithm (ia).</prevsent>
<prevsent>in recent years there have been number of important extensions to the ia. the context-sensitive extension (krahmer and theune, 2002) is able to generate referring expressions for the most salient entity in context; the boolean expressions algorithm (van deemter, 2002) is able to derive expressions containing boolean operators, as in the cup that does not have handle; and the sets algorithm (van deemter, 2002) extends the basic approach to references to sets, as in the redcups.</prevsent>
</prevsection>
<citsent citstr=" J03-1003 ">
some approaches reuse parts of other algorithms: the branch and bound algorithm (krah mer et al , 2003) <papid> J03-1003 </papid>uses the full brevity algorithm, but is able to generate referring expressions with both attributes and relational descriptions using agraph-based technique.</citsent>
<aftsection>
<nextsent>there are many other algorithms described in the literature: see, for example, (horacek, 1997; <papid> P97-1027 </papid>bateman, 1999; <papid> P99-1017 </papid>stone, 2000; <papid> W00-1416 </papid>gardent, 2002).<papid> P02-1013 </papid></nextsent>
<nextsent>their general aim is to produce naturalistic referring expressions, often explicitly by means of an attempt to follow the same kindsof principles that we believe people might be following when they produce language ? such as thegricean maxims (grice, 1975).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3964">
<title id=" W06-1410.xml">algorithms for generating referring expressions do they do what people do </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in recent years there have been number of important extensions to the ia. the context-sensitive extension (krahmer and theune, 2002) is able to generate referring expressions for the most salient entity in context; the boolean expressions algorithm (van deemter, 2002) is able to derive expressions containing boolean operators, as in the cup that does not have handle; and the sets algorithm (van deemter, 2002) extends the basic approach to references to sets, as in the redcups.
</prevsent>
<prevsent>some approaches reuse parts of other algorithms: the branch and bound algorithm (krah mer et al , 2003) <papid> J03-1003 </papid>uses the full brevity algorithm, but is able to generate referring expressions with both attributes and relational descriptions using agraph-based technique.</prevsent>
</prevsection>
<citsent citstr=" P97-1027 ">
there are many other algorithms described in the literature: see, for example, (horacek, 1997; <papid> P97-1027 </papid>bateman, 1999; <papid> P99-1017 </papid>stone, 2000; <papid> W00-1416 </papid>gardent, 2002).<papid> P02-1013 </papid></citsent>
<aftsection>
<nextsent>their general aim is to produce naturalistic referring expressions, often explicitly by means of an attempt to follow the same kindsof principles that we believe people might be following when they produce language ? such as thegricean maxims (grice, 1975).
</nextsent>
<nextsent>however, the algorithms have rarely been tested against real data from human referring expression generation.1 in this paper, we present dataset containing human-produced referring expressions in limited domain.
</nextsent>
<nextsent>focussing specifically on the algorithms 1the only exceptions we know of to this deficit are not directly concerned with the kinds of properties people select,but with phenomena such as how people group entities together (funakoshi et al , 2004; gatt, 2006), <papid> E06-1041 </papid>or with multimodal referring expressions where the linguistic part is not necessarily distinguishing by itself (van der sluis and krahmer, 2004).</nextsent>
<nextsent>63 presented in (dale, 1989), (<papid> P89-1009 </papid>dale and haddock, 1991) <papid> E91-1028 </papid>and (reiter and dale, 1992), <papid> C92-1038 </papid>we explore how well these algorithms perform in the same context.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3965">
<title id=" W06-1410.xml">algorithms for generating referring expressions do they do what people do </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in recent years there have been number of important extensions to the ia. the context-sensitive extension (krahmer and theune, 2002) is able to generate referring expressions for the most salient entity in context; the boolean expressions algorithm (van deemter, 2002) is able to derive expressions containing boolean operators, as in the cup that does not have handle; and the sets algorithm (van deemter, 2002) extends the basic approach to references to sets, as in the redcups.
</prevsent>
<prevsent>some approaches reuse parts of other algorithms: the branch and bound algorithm (krah mer et al , 2003) <papid> J03-1003 </papid>uses the full brevity algorithm, but is able to generate referring expressions with both attributes and relational descriptions using agraph-based technique.</prevsent>
</prevsection>
<citsent citstr=" P99-1017 ">
there are many other algorithms described in the literature: see, for example, (horacek, 1997; <papid> P97-1027 </papid>bateman, 1999; <papid> P99-1017 </papid>stone, 2000; <papid> W00-1416 </papid>gardent, 2002).<papid> P02-1013 </papid></citsent>
<aftsection>
<nextsent>their general aim is to produce naturalistic referring expressions, often explicitly by means of an attempt to follow the same kindsof principles that we believe people might be following when they produce language ? such as thegricean maxims (grice, 1975).
</nextsent>
<nextsent>however, the algorithms have rarely been tested against real data from human referring expression generation.1 in this paper, we present dataset containing human-produced referring expressions in limited domain.
</nextsent>
<nextsent>focussing specifically on the algorithms 1the only exceptions we know of to this deficit are not directly concerned with the kinds of properties people select,but with phenomena such as how people group entities together (funakoshi et al , 2004; gatt, 2006), <papid> E06-1041 </papid>or with multimodal referring expressions where the linguistic part is not necessarily distinguishing by itself (van der sluis and krahmer, 2004).</nextsent>
<nextsent>63 presented in (dale, 1989), (<papid> P89-1009 </papid>dale and haddock, 1991) <papid> E91-1028 </papid>and (reiter and dale, 1992), <papid> C92-1038 </papid>we explore how well these algorithms perform in the same context.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3966">
<title id=" W06-1410.xml">algorithms for generating referring expressions do they do what people do </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in recent years there have been number of important extensions to the ia. the context-sensitive extension (krahmer and theune, 2002) is able to generate referring expressions for the most salient entity in context; the boolean expressions algorithm (van deemter, 2002) is able to derive expressions containing boolean operators, as in the cup that does not have handle; and the sets algorithm (van deemter, 2002) extends the basic approach to references to sets, as in the redcups.
</prevsent>
<prevsent>some approaches reuse parts of other algorithms: the branch and bound algorithm (krah mer et al , 2003) <papid> J03-1003 </papid>uses the full brevity algorithm, but is able to generate referring expressions with both attributes and relational descriptions using agraph-based technique.</prevsent>
</prevsection>
<citsent citstr=" W00-1416 ">
there are many other algorithms described in the literature: see, for example, (horacek, 1997; <papid> P97-1027 </papid>bateman, 1999; <papid> P99-1017 </papid>stone, 2000; <papid> W00-1416 </papid>gardent, 2002).<papid> P02-1013 </papid></citsent>
<aftsection>
<nextsent>their general aim is to produce naturalistic referring expressions, often explicitly by means of an attempt to follow the same kindsof principles that we believe people might be following when they produce language ? such as thegricean maxims (grice, 1975).
</nextsent>
<nextsent>however, the algorithms have rarely been tested against real data from human referring expression generation.1 in this paper, we present dataset containing human-produced referring expressions in limited domain.
</nextsent>
<nextsent>focussing specifically on the algorithms 1the only exceptions we know of to this deficit are not directly concerned with the kinds of properties people select,but with phenomena such as how people group entities together (funakoshi et al , 2004; gatt, 2006), <papid> E06-1041 </papid>or with multimodal referring expressions where the linguistic part is not necessarily distinguishing by itself (van der sluis and krahmer, 2004).</nextsent>
<nextsent>63 presented in (dale, 1989), (<papid> P89-1009 </papid>dale and haddock, 1991) <papid> E91-1028 </papid>and (reiter and dale, 1992), <papid> C92-1038 </papid>we explore how well these algorithms perform in the same context.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3967">
<title id=" W06-1410.xml">algorithms for generating referring expressions do they do what people do </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in recent years there have been number of important extensions to the ia. the context-sensitive extension (krahmer and theune, 2002) is able to generate referring expressions for the most salient entity in context; the boolean expressions algorithm (van deemter, 2002) is able to derive expressions containing boolean operators, as in the cup that does not have handle; and the sets algorithm (van deemter, 2002) extends the basic approach to references to sets, as in the redcups.
</prevsent>
<prevsent>some approaches reuse parts of other algorithms: the branch and bound algorithm (krah mer et al , 2003) <papid> J03-1003 </papid>uses the full brevity algorithm, but is able to generate referring expressions with both attributes and relational descriptions using agraph-based technique.</prevsent>
</prevsection>
<citsent citstr=" P02-1013 ">
there are many other algorithms described in the literature: see, for example, (horacek, 1997; <papid> P97-1027 </papid>bateman, 1999; <papid> P99-1017 </papid>stone, 2000; <papid> W00-1416 </papid>gardent, 2002).<papid> P02-1013 </papid></citsent>
<aftsection>
<nextsent>their general aim is to produce naturalistic referring expressions, often explicitly by means of an attempt to follow the same kindsof principles that we believe people might be following when they produce language ? such as thegricean maxims (grice, 1975).
</nextsent>
<nextsent>however, the algorithms have rarely been tested against real data from human referring expression generation.1 in this paper, we present dataset containing human-produced referring expressions in limited domain.
</nextsent>
<nextsent>focussing specifically on the algorithms 1the only exceptions we know of to this deficit are not directly concerned with the kinds of properties people select,but with phenomena such as how people group entities together (funakoshi et al , 2004; gatt, 2006), <papid> E06-1041 </papid>or with multimodal referring expressions where the linguistic part is not necessarily distinguishing by itself (van der sluis and krahmer, 2004).</nextsent>
<nextsent>63 presented in (dale, 1989), (<papid> P89-1009 </papid>dale and haddock, 1991) <papid> E91-1028 </papid>and (reiter and dale, 1992), <papid> C92-1038 </papid>we explore how well these algorithms perform in the same context.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3968">
<title id=" W06-1410.xml">algorithms for generating referring expressions do they do what people do </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>their general aim is to produce naturalistic referring expressions, often explicitly by means of an attempt to follow the same kindsof principles that we believe people might be following when they produce language ? such as thegricean maxims (grice, 1975).
</prevsent>
<prevsent>however, the algorithms have rarely been tested against real data from human referring expression generation.1 in this paper, we present dataset containing human-produced referring expressions in limited domain.
</prevsent>
</prevsection>
<citsent citstr=" E06-1041 ">
focussing specifically on the algorithms 1the only exceptions we know of to this deficit are not directly concerned with the kinds of properties people select,but with phenomena such as how people group entities together (funakoshi et al , 2004; gatt, 2006), <papid> E06-1041 </papid>or with multimodal referring expressions where the linguistic part is not necessarily distinguishing by itself (van der sluis and krahmer, 2004).</citsent>
<aftsection>
<nextsent>63 presented in (dale, 1989), (<papid> P89-1009 </papid>dale and haddock, 1991) <papid> E91-1028 </papid>and (reiter and dale, 1992), <papid> C92-1038 </papid>we explore how well these algorithms perform in the same context.</nextsent>
<nextsent>there are significant differences between the referring expressions produced by humans, and those produced by the algorithms; we explore these differences and consider what it means for work in the generation of referring expressions.the remainder of this paper is structured as fol lows.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3985">
<title id=" W06-2006.xml">automatic generation of translation dictionaries using intermediary languages </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the resources required to exploit the method are not difficult to find since dictionaries already exist that translate between english and vast number of other languages.
</prevsent>
<prevsent>whereas at present the production of translation dictionaries is manual (e.g.
</prevsent>
</prevsection>
<citsent citstr=" C94-1044 ">
(serasset1994)), <papid> C94-1044 </papid>our method is automatic.</citsent>
<aftsection>
<nextsent>we believe that projects suchas (boitet et al 2002) <papid> W02-1705 </papid>and (wiktionary), which are currently generating translation dictionaries by hand could benefit greatly from using our method.</nextsent>
<nextsent>translation dictionaries are useful not only for end-user consumption but also for various multilingual tasks such as cross language question answering (e.g.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3986">
<title id=" W06-2006.xml">automatic generation of translation dictionaries using intermediary languages </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>whereas at present the production of translation dictionaries is manual (e.g.
</prevsent>
<prevsent>(serasset1994)), <papid> C94-1044 </papid>our method is automatic.</prevsent>
</prevsection>
<citsent citstr=" W02-1705 ">
we believe that projects suchas (boitet et al 2002) <papid> W02-1705 </papid>and (wiktionary), which are currently generating translation dictionaries by hand could benefit greatly from using our method.</citsent>
<aftsection>
<nextsent>translation dictionaries are useful not only for end-user consumption but also for various multilingual tasks such as cross language question answering (e.g.
</nextsent>
<nextsent>(ahn et al 2004)) and information retrieval (e.g.
</nextsent>
<nextsent>(argaw et al 2004)).
</nextsent>
<nextsent>we have applied our method to automatically generate spanish-to-german dictionary.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3987">
<title id=" W06-2901.xml">a mission for computational natural language learning </title>
<section> state of the art in computational.  </section>
<citcontext>
<prevsection>
<prevsent>heuristic limitations on the search process and restrictions onthe representations allowed for input and hypothesis representations together define this bias.
</prevsent>
<prevsent>there is not lot of work on matching properties of learning algorithms with properties of language processing 2tasks, or more specifically on how the bias of particular (families of) learning algorithms relates to the hypothesis spaces of particular (types of) language processing tasks.
</prevsent>
</prevsection>
<citsent citstr=" W00-0701 ">
as an example of such unifying approach, (roth, 2000) <papid> W00-0701 </papid>shows that several different algorithms (memory-based learning, tbl, snow, decision lists, various statistical learners, ...)</citsent>
<aftsection>
<nextsent>use the same type of knowledge representation, linear representation over feature space based on transformation of the original instance space.
</nextsent>
<nextsent>however, the only relation to language here is rather negative with the claim that this bias is not sufficient for learning higher level language processing tasks.
</nextsent>
<nextsent>as another example of this type of work, memory-based learning (mbl) (daelemans andvan den bosch, 2005), with its implicit similarity based smoothing, storage of all training evidence,and uniform modeling of regularities, subregulari ties and exceptions has been proposed as having the right bias for language processing tasks.
</nextsent>
<nextsent>language processing tasks are mostly governed by zipfian distributions and high disjunct ivity which makes it difficult to make principled distinction between noise and exceptions, which would put eager learning methods (i.e. most learning methods apart from mbl and kernel methods) at disadvantage.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3988">
<title id=" W06-2901.xml">a mission for computational natural language learning </title>
<section> state of the art in computational.  </section>
<citcontext>
<prevsection>
<prevsent>yet everyone knows that many factors potentially play role in the outcome of (comparative) machine learning experiment: the data used (the sample selection and the sample size), the information sources used (the features selected) and their representation (e.g. as nominal or binary features), the class representation (error coding, binarization of classes), and the algorithm parameter settings (most ml algorithms have various parameters that can betuned).
</prevsent>
<prevsent>moreover,all these factors are known to interact.
</prevsent>
</prevsection>
<citsent citstr=" H01-1052 ">
e.g., (banko and brill, 2001) <papid> H01-1052 </papid>demonstrated that for confusion set disambiguation, prototypical disambiguation in context problem, the amount of data used dominates the effect of the bias of the learning method employed.</citsent>
<aftsection>
<nextsent>the effect of training data size on relevance of pos-tag information on top of lexical information in relation finding was studied in (van den bosch and buchholz, 2001).
</nextsent>
<nextsent>the positive effect of pos-tags disappears with sufficient data.
</nextsent>
<nextsent>in (daelemans et al, 2003) it is shown thatthe joined optimization of feature selection and algorithm parameter optimization significantly improves accuracy compared to sequential optimization.
</nextsent>
<nextsent>results from comparative experiments may therefore not be reliable.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3989">
<title id=" W06-3805.xml">a study of two graph algorithms in topic driven summarization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we also extract all pairs of open-class words from the topic description, and check whether they are connected in the sentence graphs.
</prevsent>
<prevsent>both methods let us rank sentences; the top-ranking ones go into summary of at most 250 words.
</prevsent>
</prevsection>
<citsent citstr=" N04-1019 ">
we evaluate the summaries with the summary content units (scu) data made available after duc 2005 (nenkova and passonneau, 2004; <papid> N04-1019 </papid>copeck and szpakowicz, 2005).</citsent>
<aftsection>
<nextsent>the experiments show that using more information than just keywords leads to summaries with more scus (total and unique) and higher scu weight.
</nextsent>
<nextsent>we present related work in section 2, and the data and the representation we work with in section 3.
</nextsent>
<nextsent>section 4 shows the algorithms in more detail.
</nextsent>
<nextsent>we describe the experiments and their results in section 5, and draw few conclusions in section 6.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3990">
<title id=" W06-3805.xml">a study of two graph algorithms in topic driven summarization </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>section 4 shows the algorithms in more detail.
</prevsent>
<prevsent>we describe the experiments and their results in section 5, and draw few conclusions in section 6.
</prevsent>
</prevsection>
<citsent citstr=" P04-3020 ">
erkan and radev (2004), mihalcea (2004), <papid> P04-3020 </papid>mihalcea and tarau (2004) <papid> W04-3252 </papid>introduced graph methods for summarization, word sense disambiguation and other nlp applications.the summarization graph-based systems implement form of sentence ranking, based on the idea of prestige or centrality in social networks.</citsent>
<aftsection>
<nextsent>in this case the network consists of sentences, and significantly similar sentences are interconnected.
</nextsent>
<nextsent>various measures (such as node degree) help find the most central sentences, or to score each sentence.in topic-driven summarization, one or more sentences or questions describe an information need which the summaries must address.
</nextsent>
<nextsent>previous systems extracted keywords or phrases from topics and used them to focus the summary (fisher et al, 2005).
</nextsent>
<nextsent>our experiments show that there is more to topics than keywords or phrases.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3991">
<title id=" W06-3805.xml">a study of two graph algorithms in topic driven summarization </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>section 4 shows the algorithms in more detail.
</prevsent>
<prevsent>we describe the experiments and their results in section 5, and draw few conclusions in section 6.
</prevsent>
</prevsection>
<citsent citstr=" W04-3252 ">
erkan and radev (2004), mihalcea (2004), <papid> P04-3020 </papid>mihalcea and tarau (2004) <papid> W04-3252 </papid>introduced graph methods for summarization, word sense disambiguation and other nlp applications.the summarization graph-based systems implement form of sentence ranking, based on the idea of prestige or centrality in social networks.</citsent>
<aftsection>
<nextsent>in this case the network consists of sentences, and significantly similar sentences are interconnected.
</nextsent>
<nextsent>various measures (such as node degree) help find the most central sentences, or to score each sentence.in topic-driven summarization, one or more sentences or questions describe an information need which the summaries must address.
</nextsent>
<nextsent>previous systems extracted keywords or phrases from topics and used them to focus the summary (fisher et al, 2005).
</nextsent>
<nextsent>our experiments show that there is more to topics than keywords or phrases.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3992">
<title id=" W06-3805.xml">a study of two graph algorithms in topic driven summarization </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>previous systems extracted keywords or phrases from topics and used them to focus the summary (fisher et al, 2005).
</prevsent>
<prevsent>our experiments show that there is more to topics than keywords or phrases.
</prevsent>
</prevsection>
<citsent citstr=" H05-1049 ">
we will experiment with using grammatical dependency relations for the task of extractive summarization.in previous research, graph-matching using grammatical relations was used to detect textual entailment (haghighi et al, 2005).<papid> H05-1049 </papid></citsent>
<aftsection>
<nextsent>3.1 topics.
</nextsent>
<nextsent>we work with list of topics from the test data in the duc 2005 challenge.
</nextsent>
<nextsent>a topic has an identifier, category (general/specific), title and sequence of statements or questions, for example: 29 d307b specific new hydroelectric projects what hydroelectric projects are planned or in progress and what problems are associated with them?
</nextsent>
<nextsent>we apply minipar to the titles and contents of the topics, and to all documents.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3994">
<title id=" W06-2912.xml">unsupervised parsing with udop </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>during the last few years there has been considerable progress in unsupervised parsing.
</prevsent>
<prevsent>to give brief overview: van zaanen (2000) achieved 39.2% unlabeled f-score on atis word strings by sentence-aligning technique called abl.
</prevsent>
</prevsection>
<citsent citstr=" W01-0713 ">
clark (2001) <papid> W01-0713 </papid>reports 42.0% unlabeled f-score on the same data using distributional clustering, and klein and manning (2002) <papid> P02-1017 </papid>obtain 51.2% unlabeled f-score on atis part-of-speech strings using constituent-context model calledccm.</citsent>
<aftsection>
<nextsent>moreover, on penn wall street journal p-o s-strings ? 10 (wsj10), klein and manning (2002) <papid> P02-1017 </papid>report 71.1% unlabeled f-score.</nextsent>
<nextsent>and the hybrid approach of klein and manning (2004), <papid> P04-1061 </papid>which combines constituency and dependency model, leads to further increase of 77.6% f-score.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC3996">
<title id=" W06-2912.xml">unsupervised parsing with udop </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>during the last few years there has been considerable progress in unsupervised parsing.
</prevsent>
<prevsent>to give brief overview: van zaanen (2000) achieved 39.2% unlabeled f-score on atis word strings by sentence-aligning technique called abl.
</prevsent>
</prevsection>
<citsent citstr=" P02-1017 ">
clark (2001) <papid> W01-0713 </papid>reports 42.0% unlabeled f-score on the same data using distributional clustering, and klein and manning (2002) <papid> P02-1017 </papid>obtain 51.2% unlabeled f-score on atis part-of-speech strings using constituent-context model calledccm.</citsent>
<aftsection>
<nextsent>moreover, on penn wall street journal p-o s-strings ? 10 (wsj10), klein and manning (2002) <papid> P02-1017 </papid>report 71.1% unlabeled f-score.</nextsent>
<nextsent>and the hybrid approach of klein and manning (2004), <papid> P04-1061 </papid>which combines constituency and dependency model, leads to further increase of 77.6% f-score.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4000">
<title id=" W06-2912.xml">unsupervised parsing with udop </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>clark (2001) <papid> W01-0713 </papid>reports 42.0% unlabeled f-score on the same data using distributional clustering, and klein and manning (2002) <papid> P02-1017 </papid>obtain 51.2% unlabeled f-score on atis part-of-speech strings using constituent-context model calledccm.</prevsent>
<prevsent>moreover, on penn wall street journal p-o s-strings ? 10 (wsj10), klein and manning (2002) <papid> P02-1017 </papid>report 71.1% unlabeled f-score.</prevsent>
</prevsection>
<citsent citstr=" P04-1061 ">
and the hybrid approach of klein and manning (2004), <papid> P04-1061 </papid>which combines constituency and dependency model, leads to further increase of 77.6% f-score.</citsent>
<aftsection>
<nextsent>although there has thus been steady progress in unsupervised parsing, all these approaches have shortcomings in that they either constrain the lexical or the structural context that is taken into account, or both.
</nextsent>
<nextsent>for example, the ccm model by klein and manning (2005) is said to describe  all contiguous sub sequences of sentence  (klein and manning 2005: 1410).
</nextsent>
<nextsent>while this is very rich lexical model, it is still limited in that it neglects dependencies that are non-contiguous such as between more and than in  ba carried more people than cargo .
</nextsent>
<nextsent>moreover, by using an all-substrings  approach, ccm risks to under represent structural context.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4002">
<title id=" W06-2912.xml">unsupervised parsing with udop </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>similar shortcomings can be found in other unsupervised models.
</prevsent>
<prevsent>in this paper we will try to directly model structural as well as lexical context without constraining any dependencies beforehand.
</prevsent>
</prevsection>
<citsent citstr=" E03-1005 ">
an approach that may seem apt in this respect is an all subtrees approach (e.g bod 2003; <papid> E03-1005 </papid>goodman 2003; collins and duffy 2002).<papid> P02-1034 </papid></citsent>
<aftsection>
<nextsent>subtrees can model both contiguous and non-contiguous lexical dependencies (see section 2) and they also model constituents in ahierarchical context.
</nextsent>
<nextsent>moreover, we can view the all subtrees approach as generalization of klein and manning all-substrings approach and van zaanen abl model.in the current paper, we will use the all subtrees approach as proposed in data-oriented 85 parsing or dop (bod 1998).
</nextsent>
<nextsent>we will generalize the supervised version of dop to unsupervised parsing.
</nextsent>
<nextsent>the key idea of our approach is to initially assign all possible unlabeled binary trees to set of given sentences, and to next use counts of all subtrees from (a large random subset of) these binary trees to compute the most probable parse trees.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4003">
<title id=" W06-2912.xml">unsupervised parsing with udop </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>similar shortcomings can be found in other unsupervised models.
</prevsent>
<prevsent>in this paper we will try to directly model structural as well as lexical context without constraining any dependencies beforehand.
</prevsent>
</prevsection>
<citsent citstr=" P02-1034 ">
an approach that may seem apt in this respect is an all subtrees approach (e.g bod 2003; <papid> E03-1005 </papid>goodman 2003; collins and duffy 2002).<papid> P02-1034 </papid></citsent>
<aftsection>
<nextsent>subtrees can model both contiguous and non-contiguous lexical dependencies (see section 2) and they also model constituents in ahierarchical context.
</nextsent>
<nextsent>moreover, we can view the all subtrees approach as generalization of klein and manning all-substrings approach and van zaanen abl model.in the current paper, we will use the all subtrees approach as proposed in data-oriented 85 parsing or dop (bod 1998).
</nextsent>
<nextsent>we will generalize the supervised version of dop to unsupervised parsing.
</nextsent>
<nextsent>the key idea of our approach is to initially assign all possible unlabeled binary trees to set of given sentences, and to next use counts of all subtrees from (a large random subset of) these binary trees to compute the most probable parse trees.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4009">
<title id=" W06-2912.xml">unsupervised parsing with udop </title>
<section> unsupervised data-oriented parsing.  </section>
<citcontext>
<prevsection>
<prevsent>in this paper, we will present results both on entire corpora and on 90-10 splits of such corpora so as to make our results comparable to supervised pcfg using the treebank grammars of the same data ( pcfg  ).
</prevsent>
<prevsent>in the following we will first describe each of the three steps given above where we initially focus on inducing trees for p-o-s strings for the wsj10 (we will deal with other corpora and the much larger wsj40 in section 3).
</prevsent>
</prevsection>
<citsent citstr=" W00-0717 ">
as shown by klein and manning (2002), <papid> P02-1017 </papid>klein and manning (2004), <papid> P04-1061 </papid>the extension to inducing trees for words instead of p-o-s tags is rather straightforward since there exist several unsupervised part-of-speech taggers with high accuracy, which can be combined with unsupervised parsing (see e.g. schtze 1996; clark 2000).<papid> W00-0717 </papid></citsent>
<aftsection>
<nextsent>step 1: assign all binary trees to p-o-s strings from the wsj10 the wsj10 contains 7422 sentences ? 10 words after removing empty elements and punctuation.
</nextsent>
<nextsent>we assigned all possible binary trees to the corresponding part-of-speech sequences of these sentences, where each root node is labeled and each internal node is labeled . as an example, consider the p-o-s string nns vbd jj nns, which may correspond for instance to the sentence investors suffered heavy losses . this string has total of five binary trees shown in figure 1 -- where for readability we add words as well.
</nextsent>
<nextsent>nns vbd jj nns investors suffered heavy losses x nns vbd jj nns investors suffered heavy losses x nns vbd jj nns investors suffered heavy losses x nns vbd jj nns investors suffered heavy losses x nns vbd jj nns investors suffered heavy losses xx figure 1.
</nextsent>
<nextsent>all binary trees for nns vbd jj nns (investors suffered heavy losses) 86 the total number of binary trees for sentence of length is given by the catalan number cn1, where cn = (2n)!/((n+1)!n!).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4011">
<title id=" W06-2912.xml">unsupervised parsing with udop </title>
<section> unsupervised data-oriented parsing.  </section>
<citcontext>
<prevsection>
<prevsent>for the tree-set of 8.23 * 105 binary trees generated under step 1, goodman reduction method results in total number of 14.8 * 106 distinct pcfg rules.
</prevsent>
<prevsent>while it is still feasible to parse with rule-set of this size, it is evident that our approach can deal with longer sentences only if we further reduce the size of our binary tree-set.
</prevsent>
</prevsection>
<citsent citstr=" J02-1005 ">
it should be kept in mind that while the probabilities of all parse trees generated by dop sum up to 1, these probabilities do not converge to the  true  probabilities if the corpus grows to infinity (johnson 2002).<papid> J02-1005 </papid></citsent>
<aftsection>
<nextsent>in fact, in bod et al (2003) we showed that the most probable parse tree as defined above has tendency to be constructed by the shortest derivation (consisting of the fewest and thus largest subtrees).
</nextsent>
<nextsent>a large subtree is overruled only if the combined relative frequencies of smaller subtrees yields larger score.
</nextsent>
<nextsent>we refer to zollmann and sima an (2005) for recently proposed estimator that is statistically consistent (though it is not yet known how this estimator performs on the wsj) and to zuidema (2006) <papid> E06-2025 </papid>for theoretical comparison of existing estimators for dop.</nextsent>
<nextsent>1 as in bod (2003) <papid> E03-1005 </papid>and goodman (2003: 136), we additionally use correction factor to redress dop bias discussed in johnson (2002).<papid> J02-1005 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4012">
<title id=" W06-2912.xml">unsupervised parsing with udop </title>
<section> unsupervised data-oriented parsing.  </section>
<citcontext>
<prevsection>
<prevsent>in fact, in bod et al (2003) we showed that the most probable parse tree as defined above has tendency to be constructed by the shortest derivation (consisting of the fewest and thus largest subtrees).
</prevsent>
<prevsent>a large subtree is overruled only if the combined relative frequencies of smaller subtrees yields larger score.
</prevsent>
</prevsection>
<citsent citstr=" E06-2025 ">
we refer to zollmann and sima an (2005) for recently proposed estimator that is statistically consistent (though it is not yet known how this estimator performs on the wsj) and to zuidema (2006) <papid> E06-2025 </papid>for theoretical comparison of existing estimators for dop.</citsent>
<aftsection>
<nextsent>1 as in bod (2003) <papid> E03-1005 </papid>and goodman (2003: 136), we additionally use correction factor to redress dop bias discussed in johnson (2002).<papid> J02-1005 </papid></nextsent>
<nextsent>88 step 3: compute the most probable parse tree for each wsj10 string while goodman reduction method allows for efficiently computing the most probable derivation for each sentence (i.e. the viterbi parse), it does not allow for an efficient computation of (u-)dop most probable parse tree since there may be exponentially many derivations for each tree whose probabilities have to be summed up.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4015">
<title id=" W06-2912.xml">unsupervised parsing with udop </title>
<section> unsupervised data-oriented parsing.  </section>
<citcontext>
<prevsection>
<prevsent>in fact, the problem of computing the most probable tree in dop is known to be np hard (sima an 1996).
</prevsent>
<prevsent>yet, the pcfg reduction in figure 4 can be used to estimate dop most probable parse tree by viterbi n-best search in combination with cky parser which computes the most likely derivations and next sums up the probabilities of the derivations producing the same tree.
</prevsent>
</prevsection>
<citsent citstr=" W05-1506 ">
(we can considerably improve efficiency by using k-best hypergraph parsing as recently proposed by huang and chiang 2005, <papid> W05-1506 </papid>but this will be left to future research).</citsent>
<aftsection>
<nextsent>in this paper, we estimate the most probable parse tree from the 100 most probable derivations (at least for the relatively small wsj10).
</nextsent>
<nextsent>although such heuristic does not guarantee that the most probable parse is actually found, it is shown in bod (2000) <papid> P00-1009 </papid>to perform at least as well as the estimation of the most probable parse with monte carlo techniques.</nextsent>
<nextsent>however, in computing the 100 most probable derivations by means of viterbi it is prohibitive to keep track of all sub derivations at each edge in the chart.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4016">
<title id=" W06-2912.xml">unsupervised parsing with udop </title>
<section> unsupervised data-oriented parsing.  </section>
<citcontext>
<prevsection>
<prevsent>(we can considerably improve efficiency by using k-best hypergraph parsing as recently proposed by huang and chiang 2005, <papid> W05-1506 </papid>but this will be left to future research).</prevsent>
<prevsent>in this paper, we estimate the most probable parse tree from the 100 most probable derivations (at least for the relatively small wsj10).</prevsent>
</prevsection>
<citsent citstr=" P00-1009 ">
although such heuristic does not guarantee that the most probable parse is actually found, it is shown in bod (2000) <papid> P00-1009 </papid>to perform at least as well as the estimation of the most probable parse with monte carlo techniques.</citsent>
<aftsection>
<nextsent>however, in computing the 100 most probable derivations by means of viterbi it is prohibitive to keep track of all sub derivations at each edge in the chart.
</nextsent>
<nextsent>we therefore use pruning technique which deletes any item with probability less than 105 times of that of the best item from the chart.
</nextsent>
<nextsent>to make our parse results comparable to those of klein and manning (2002, <papid> P02-1017 </papid>2004, 2005), we will use exactly the same evaluation metrics for unlabeled precision (up) and unlabeled recall (ur), defined in klein (2005: 21-22).</nextsent>
<nextsent>klein definitions slightly differ from the standard parseval metrics: multiplicity of brackets is ignored, brackets of span one are ignored and the bracket labels are ignored.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4024">
<title id=" W06-2912.xml">unsupervised parsing with udop </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>3.1 comparing u-dop to previous work.
</prevsent>
<prevsent>using the method described above, our parsing experiment with all p-o-s strings from the wsj10results in an f-score of 78.5%.
</prevsent>
</prevsection>
<citsent citstr=" C02-1145 ">
we next tested dop on two additional domains from chinese and german which were also used in klein and manning (2002), <papid> P02-1017 </papid>klein and manning (2004): <papid> P04-1061 </papid>the chinese treebank (xue et al 2002) <papid> C02-1145 </papid>and the negra corpus (skut et al 1997).<papid> A97-1014 </papid></citsent>
<aftsection>
<nextsent>the ctb10 is the subset of p-o-s strings from the penn chinese treebank containing 10 words or less after removal of punctuation (2437 strings).
</nextsent>
<nextsent>the negra10 is the subset of p-o-s strings of the same length from the negra corpus using the supplied con verson into penn treebank format (2175 strings).
</nextsent>
<nextsent>table 1 shows the results of u-dop in terms of up, ur and f1 compared to the results of the ccm model by klein and manning (2002), <papid> P02-1017 </papid>the dmv dependency learning model by klein and manning (2004) <papid> P04-1061 </papid>together with their combined model dmv+ccm.</nextsent>
<nextsent>model english german chinese(wsj10) (negra10) (ctb10) up ur f1 up ur f1 up ur f1 ccm 64.2 81.6 71.9 48.1 85.5 61.6 34.6 64.3 45.0 dmv 46.6 59.2 52.1 38.4 69.5 49.5 35.9 66.7 46.7 dmv+ccm 69.3 88.0 77.6 49.6 89.7 63.9 33.3 62.0 43.3 u-dop 70.8 88.2 78.5 51.2 90.5 65.4 36.3 64.9 46.6 table 1.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4025">
<title id=" W06-2912.xml">unsupervised parsing with udop </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>3.1 comparing u-dop to previous work.
</prevsent>
<prevsent>using the method described above, our parsing experiment with all p-o-s strings from the wsj10results in an f-score of 78.5%.
</prevsent>
</prevsection>
<citsent citstr=" A97-1014 ">
we next tested dop on two additional domains from chinese and german which were also used in klein and manning (2002), <papid> P02-1017 </papid>klein and manning (2004): <papid> P04-1061 </papid>the chinese treebank (xue et al 2002) <papid> C02-1145 </papid>and the negra corpus (skut et al 1997).<papid> A97-1014 </papid></citsent>
<aftsection>
<nextsent>the ctb10 is the subset of p-o-s strings from the penn chinese treebank containing 10 words or less after removal of punctuation (2437 strings).
</nextsent>
<nextsent>the negra10 is the subset of p-o-s strings of the same length from the negra corpus using the supplied con verson into penn treebank format (2175 strings).
</nextsent>
<nextsent>table 1 shows the results of u-dop in terms of up, ur and f1 compared to the results of the ccm model by klein and manning (2002), <papid> P02-1017 </papid>the dmv dependency learning model by klein and manning (2004) <papid> P04-1061 </papid>together with their combined model dmv+ccm.</nextsent>
<nextsent>model english german chinese(wsj10) (negra10) (ctb10) up ur f1 up ur f1 up ur f1 ccm 64.2 81.6 71.9 48.1 85.5 61.6 34.6 64.3 45.0 dmv 46.6 59.2 52.1 38.4 69.5 49.5 35.9 66.7 46.7 dmv+ccm 69.3 88.0 77.6 49.6 89.7 63.9 33.3 62.0 43.3 u-dop 70.8 88.2 78.5 51.2 90.5 65.4 36.3 64.9 46.6 table 1.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4031">
<title id=" W06-2912.xml">unsupervised parsing with udop </title>
<section> in dt nn jj nns in dt.  </section>
<citcontext>
<prevsection>
<prevsent>it would be extremely interesting to see how dmv+ccm performs on the wsj40.
</prevsent>
<prevsent>it should be kept in mind that simple treebank pcfgs do not constitute state-of-the-art supervised parsers.
</prevsent>
</prevsection>
<citsent citstr=" P05-1022 ">
table 4 indicates that u-dop performance remains still far behind that of s-dop (and indeed of other state-of-the-art supervised parsers such as bod 2003 <papid> E03-1005 </papid>or charniak and johnson 2005).<papid> P05-1022 </papid></citsent>
<aftsection>
<nextsent>moreover, if s-dop is not post-binarized, its average f-score on the wsj40 is 90.1% -- and there are some hybrid dop models that obtain even higher scores (see bod 2003).<papid> E03-1005 </papid></nextsent>
<nextsent>our long-term goal is to try to outperform s-dop by u-dop.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4033">
<title id=" W07-0301.xml">applying pomdps to dialog systems in the troubleshooting domain </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>dialog models which explicitly model uncertainty have been shown to significantly outperform base line models which do not, primarily because they cope better with conflicting evidence introduced by speech recognition errors (roy et al, 2000; zhang et al, 2001; williams and young, 2007).
</prevsent>
<prevsent>however, past work has been confined to slot-filling tasks andhas not tackled the troubleshooting domain.
</prevsent>
</prevsection>
<citsent citstr=" J86-3001 ">
conversely, dialog systems for troubleshooting in the literature have not attempted to model uncertainty directly (grosz and sidner, 1986; <papid> J86-3001 </papid>lochbaum, 1998).<papid> J98-4001 </papid></citsent>
<aftsection>
<nextsent>the contribution of this paper is to show how to model troubleshooting spoken dialog system as partially observable markov decision process(pomdp).
</nextsent>
<nextsent>we argue that past work in the general troubleshooting literature represents simplifications or special cases of pomdp, then we show how troubleshooting pomdp can be combined with dialog system pomdp to create unified framework that admits global optimization.
</nextsent>
<nextsent>experiments with simulated users show how the pomdp formulation effectively balances diagnostic actions(such as network test) with communicative actions (such as giving the user instructions), and how the pomdp formulation outperforms hand-craftedbaseline both in terms of efficiency and task comple tion.this paper is organized as follows.
</nextsent>
<nextsent>section 2 reviews pomdps, the general troubleshooting problem, and pomdp-based spoken dialog systems; section 3 explains how these two pomdps can be combined to model troubleshooting spoken dialog system; sections 4-5 present results from simulation; and section 6 concludes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4034">
<title id=" W07-0301.xml">applying pomdps to dialog systems in the troubleshooting domain </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>dialog models which explicitly model uncertainty have been shown to significantly outperform base line models which do not, primarily because they cope better with conflicting evidence introduced by speech recognition errors (roy et al, 2000; zhang et al, 2001; williams and young, 2007).
</prevsent>
<prevsent>however, past work has been confined to slot-filling tasks andhas not tackled the troubleshooting domain.
</prevsent>
</prevsection>
<citsent citstr=" J98-4001 ">
conversely, dialog systems for troubleshooting in the literature have not attempted to model uncertainty directly (grosz and sidner, 1986; <papid> J86-3001 </papid>lochbaum, 1998).<papid> J98-4001 </papid></citsent>
<aftsection>
<nextsent>the contribution of this paper is to show how to model troubleshooting spoken dialog system as partially observable markov decision process(pomdp).
</nextsent>
<nextsent>we argue that past work in the general troubleshooting literature represents simplifications or special cases of pomdp, then we show how troubleshooting pomdp can be combined with dialog system pomdp to create unified framework that admits global optimization.
</nextsent>
<nextsent>experiments with simulated users show how the pomdp formulation effectively balances diagnostic actions(such as network test) with communicative actions (such as giving the user instructions), and how the pomdp formulation outperforms hand-craftedbaseline both in terms of efficiency and task comple tion.this paper is organized as follows.
</nextsent>
<nextsent>section 2 reviews pomdps, the general troubleshooting problem, and pomdp-based spoken dialog systems; section 3 explains how these two pomdps can be combined to model troubleshooting spoken dialog system; sections 4-5 present results from simulation; and section 6 concludes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4035">
<title id=" W06-1519.xml">extracting syntactic features from a korean treebank </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>several works have been on extracting grammars, especially using tag formalism proposed.
</prevsent>
<prevsent>chen (2001) has extracted lexicalized grammars from english penn treebank and there are other works based on chens procedure such as nasr (2004) for french and habash and rambow (2004) for arabic.
</prevsent>
</prevsection>
<citsent citstr=" W00-1307 ">
xia et al (2000) <papid> W00-1307 </papid>developed the uniform method of grammar extraction for english, chinese and korean.</citsent>
<aftsection>
<nextsent>neumann (2003) extracted lexicalized tree grammars from english penn treebank for english and from negra treebank for german.
</nextsent>
<nextsent>however, none of these works have tried to extract syntactic features for fb-ltag.
</nextsent>
<nextsent>we use with sejong treebank (sjtree) which contains 32 054 eojeols (the unity of segmentation in the korean sentence), that is, 2 526 sentences.
</nextsent>
<nextsent>sjtree uses 43 part-of-speech tags and 55 syntactic tags (sejong project 2003).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4036">
<title id=" W06-1516.xml">semtag the loria toolbox for tag based parsing and generation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this paper we introduce toolbox that allows forboth parsing and generation with tag.
</prevsent>
<prevsent>this toolbox combines existing software and aims at facilitating grammar development, more precisely, this toolbox includes1: ? xmg: grammar compiler which supports the generation of tag from factor ised tag (crabbe?
</prevsent>
</prevsection>
<citsent citstr=" W05-1605 ">
and duchier, 2004), ? llp2 and dyalog: two chart parsers, one with friendly user interface (lopez, 2000) and the other optimised for efficient parsing (villemonte de la clergerie, 2005)2 ? geni: chart generator which has been tested on middle size grammar for french (gardent and kow, 2005) <papid> W05-1605 </papid>1all these tools are freely available, more information and links at http://trac.loria.fr/semtag.2note that dyalog refers in fact to logic programming language, and tabular compiler for this language.</citsent>
<aftsection>
<nextsent>thedyalog system is well-adapted to the compilation of efficient tabular parsers.
</nextsent>
<nextsent>for tree based grammars xmg provides grammar writing environment fortree based grammars3 with three distinctive features.
</nextsent>
<nextsent>first, xmg supports highly factor ised and fully declarative description of tree based grammars.
</nextsent>
<nextsent>second, xmg permits the integration in tag of semantic dimension.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4037">
<title id=" W06-1516.xml">semtag the loria toolbox for tag based parsing and generation </title>
<section> xmg, grammar writing environment.  </section>
<citcontext>
<prevsection>
<prevsent>note that the inclusion of semantic information remains optional.
</prevsent>
<prevsent>that is, it is possible to use xmg to define purely syntactic tag.
</prevsent>
</prevsection>
<citsent citstr=" C96-2120 ">
xmg was used to develop core grammar for french (frag) which was evaluated to have 75% coverage4 on the test suite for natural language processing (tsnlp, (lehmann et al, 1996)).<papid> C96-2120 </papid></citsent>
<aftsection>
<nextsent>the frag grammar was furthermore enriched with semantic information using another 50 classes describing the semantic dimension (gardent, 2006).
</nextsent>
<nextsent>the resulting grammar (semfrag) describes both the syntax and the semantics of the french core constructions.compiling an xmg specification.
</nextsent>
<nextsent>by building on efficient techniques from logic programming and in particular, on the warrens abstract 4this means that for 75 % of the sentences, tag parser can build at least one derivation.
</nextsent>
<nextsent>figure 2: the llp2 parser.machine idea (ait-kaci, 1991), the xmg compiler allows for very reasonable compilation times(duchier et al, 2004).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4038">
<title id=" W06-1516.xml">semtag the loria toolbox for tag based parsing and generation </title>
<section> two tag parsers.  </section>
<citcontext>
<prevsection>
<prevsent>dyalog the dyalog system on the other hand, is highly optimised parsing system basedon tabulation and automata techniques (ville monte de la clergerie, 2005).
</prevsent>
<prevsent>it is implemented using the dyalog programming language (i.e., it is bootstrapped) and is also used to compile parsers for other types of grammars such as tree insertion grammars.the dyalog system is coupled with semantic construction module whose aim is to associate with each parsed string semantic representation5.
</prevsent>
</prevsection>
<citsent citstr=" E03-1030 ">
this module assumes tag of the type described in (gardent and kallmeyer, 2003; <papid> E03-1030 </papid>gardent, 2006) 5the corresponding system is called sem const (cf section 6).</citsent>
<aftsection>
<nextsent>116 figure 3: the sem const system where initial trees are associated with semantic information and unification is used to combine semantic representations.
</nextsent>
<nextsent>in such grammar, these mantic representation of derived tree is the unionof the semantic representations of the trees entering in the derivation of that derived tree modulo the unifications entailed by analysis.
</nextsent>
<nextsent>as detailed in (gardent and parmentier, 2005), such grammars support two strategies for semantic construction.
</nextsent>
<nextsent>the first possible strategy is to use the full grammar and to perform semantic construction during derivation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4043">
<title id=" W06-3702.xml">evaluating task performance for a unidirectional controlled language medical speech translation system </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, this is not necessarily the best alternative in safety critical medical applications.
</prevsent>
<prevsent>anecdotally, many doctors express reluctance to trust translation device whose output is not readily predictable, and most of the speech translation systems which have reached the stage of field testing relyon various types of grammar-based recognition and rule-based translation (phraselator, 2006; s-minds, 2006; med bridge, 2006).
</prevsent>
</prevsection>
<citsent citstr=" H05-2014 ">
even though statistical systems exhibit many desirable properties (purely data driven, domain independence), grammar-based systems utilizing probabilistic context-free grammar tuning appear to deliver better results when training data is sparse (rayner et al, 2005<papid> H05-2014 </papid>a).</citsent>
<aftsection>
<nextsent>one drawback of grammar-based systems is that out-of-coverage utterances will be neither recognized nor translated, an objection that critics have sometimes painted as decisive.
</nextsent>
<nextsent>it is by no means obvious, however, that restricted coverage is such serious problem.
</nextsent>
<nextsent>in text processing, work on several generations of controlled language systems has developed range of techniques for keeping users within the bounds of system coverage (kittredge, 2003; mitamura, 1999).
</nextsent>
<nextsent>if these techniques work for text processing, it is surely not inconceivable that variants of them will be equally successful for spoken language applications.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4046">
<title id=" W06-3702.xml">evaluating task performance for a unidirectional controlled language medical speech translation system </title>
<section> the medslt system.  </section>
<citcontext>
<prevsection>
<prevsent>medslt (medslt, 2005; bouillon et al, 2005) is unidirectional, grammar-based medical speech translation system intended for use in doctor patient diagnosis dialogues.
</prevsent>
<prevsent>the system is built on top of regulus (regulus, 2006), an open source platform for developing grammar-based speech applications.
</prevsent>
</prevsection>
<citsent citstr=" E03-2010 ">
regulus supports rapid construction of complex grammar-based language models using an example-based method (rayner et al, 2003; <papid> E03-2010 </papid>rayner et al, 2006), which extracts most of the structure of the model from general linguistically motivated resource grammar.</citsent>
<aftsection>
<nextsent>regulus-based rec ognizers are reasonably easy to maintain, and grammar structure is shared automatically across different subdomains.
</nextsent>
<nextsent>resource grammars are now available for several languages, including english, japanese (rayner et al, 2005<papid> H05-2014 </papid>b), french (bouillon et al, 2006) and spanish.</nextsent>
<nextsent>medslt includes help module, whose purpose is to add robustness to the system and guide the user towards the supported coverage.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4054">
<title id=" W06-1518.xml">using ltagbased features for semantic role labeling </title>
<section> background about srl.  </section>
<citcontext>
<prevsection>
<prevsent>a variety of machine learning methods have been applied to this task.
</prevsent>
<prevsent>one of the most important steps in building an accurate classifier is feature selection.
</prevsent>
</prevsection>
<citsent citstr=" J02-3001 ">
different from the widely used 127 feature functions that are based on the syntactic parse tree (gildea and jurafsky, 2002), <papid> J02-3001 </papid>we explore the use of ltag-based features in simple discriminative decision-list learner.</citsent>
<aftsection>
<nextsent>in this section, we introduce the main components of our system.
</nextsent>
<nextsent>first, we do pruning on the given parse trees with certain constraints.
</nextsent>
<nextsent>then we decompose the pruned parse trees into set of ltag elementary trees.
</nextsent>
<nextsent>for each constituent in question,we extract features from its corresponding derivation tree.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4055">
<title id=" W06-1518.xml">using ltagbased features for semantic role labeling </title>
<section> ltag based feature extraction.  </section>
<citcontext>
<prevsection>
<prevsent>given parse tree, the pruning component identifies the predicate in the tree and then only admits those nodes that are sisters to the path from the predicate to the root.
</prevsent>
<prevsent>it is commonly used in the srl community (cf.
</prevsent>
</prevsection>
<citsent citstr=" W04-3212 ">
(xue and palmer, 2004)) <papid> W04-3212 </papid>and our experiments show that 91% of the srl targets can be recovered despite this aggressive pruning.there are two advantages to this pruning: thema chine learning method used for prediction of srlsis not overwhelmed with large number of non srl nodes; and the process is far more efficient as 80% of the target nodes in full parse tree are pruned away in this step.</citsent>
<aftsection>
<nextsent>we make two enhancements to the pruned propbank tree: we enrich the sister nodes with their head information, which is part-of-speech tag and word pair: t, w?
</nextsent>
<nextsent>and pp nodes are expanded to include the np complement of the pp (including the head information).
</nextsent>
<nextsent>note that the target srl node is still the pp.
</nextsent>
<nextsent>figure 1 shows the pruned parse tree for sentence from propbank section 24.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4057">
<title id=" W06-1518.xml">using ltagbased features for semantic role labeling </title>
<section> experiments and results.  </section>
<citcontext>
<prevsection>
<prevsent>to avoid such situation, in this paper we use the conll 2005 shared srl task data (carreras and ma`rquez, 2005) which provides standard train/test split, standard method for training and testing on various problematic cases involving coordination.
</prevsent>
<prevsent>however, in some cases, the conll 2005 data is not ideal forthe use of ltag-based features as some deep?
</prevsent>
</prevsection>
<citsent citstr=" W03-1006 ">
information cannot be recovered due to the fact that trace information and other empty categories like pro are removed entirely from the training data.as result some of the features that undo long distance movement via trace information in the treebank as used in (chen and rambow, 2003) <papid> W03-1006 </papid>cannot be exploited in our model.</citsent>
<aftsection>
<nextsent>our results are shown in table 1.
</nextsent>
<nextsent>note that we test on the gold standard parse trees because we want to compare model using features from the derived parse trees to the model using the ltag derivation trees.
</nextsent>
<nextsent>in the community of srl researchers (cf.
</nextsent>
<nextsent>(gildea and jurafsky, 2002; <papid> J02-3001 </papid>punyakanok, roth and yih, 2005; pradhan et al 2005; <papid> W05-0634 </papid>toutanova et al, 2005)), <papid> P05-1073 </papid>the focus has been on two different aspects of the srl task: (a) finding appropriate features, and (b) resolving the parsing accuracy problem by combining multiple parsers/predictions.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4061">
<title id=" W06-1518.xml">using ltagbased features for semantic role labeling </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>note that we test on the gold standard parse trees because we want to compare model using features from the derived parse trees to the model using the ltag derivation trees.
</prevsent>
<prevsent>in the community of srl researchers (cf.
</prevsent>
</prevsection>
<citsent citstr=" W05-0634 ">
(gildea and jurafsky, 2002; <papid> J02-3001 </papid>punyakanok, roth and yih, 2005; pradhan et al 2005; <papid> W05-0634 </papid>toutanova et al, 2005)), <papid> P05-1073 </papid>the focus has been on two different aspects of the srl task: (a) finding appropriate features, and (b) resolving the parsing accuracy problem by combining multiple parsers/predictions.</citsent>
<aftsection>
<nextsent>systems that use parse trees as source of feature functions for their models have typically outperformed shallow parsing models on the srl task.
</nextsent>
<nextsent>typical features extracted from parse tree is the path from the predicate to the constituent and various generalizations based on this path (such as phrase type, position, etc.).
</nextsent>
<nextsent>notably the voice (passive or 131active) of the verb is often used and recovered using heuristic rule.
</nextsent>
<nextsent>we also use the passive/active voice by labeling this information into the parse tree.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4062">
<title id=" W06-1518.xml">using ltagbased features for semantic role labeling </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>note that we test on the gold standard parse trees because we want to compare model using features from the derived parse trees to the model using the ltag derivation trees.
</prevsent>
<prevsent>in the community of srl researchers (cf.
</prevsent>
</prevsection>
<citsent citstr=" P05-1073 ">
(gildea and jurafsky, 2002; <papid> J02-3001 </papid>punyakanok, roth and yih, 2005; pradhan et al 2005; <papid> W05-0634 </papid>toutanova et al, 2005)), <papid> P05-1073 </papid>the focus has been on two different aspects of the srl task: (a) finding appropriate features, and (b) resolving the parsing accuracy problem by combining multiple parsers/predictions.</citsent>
<aftsection>
<nextsent>systems that use parse trees as source of feature functions for their models have typically outperformed shallow parsing models on the srl task.
</nextsent>
<nextsent>typical features extracted from parse tree is the path from the predicate to the constituent and various generalizations based on this path (such as phrase type, position, etc.).
</nextsent>
<nextsent>notably the voice (passive or 131active) of the verb is often used and recovered using heuristic rule.
</nextsent>
<nextsent>we also use the passive/active voice by labeling this information into the parse tree.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4070">
<title id=" W06-1612.xml">learning information status of discourse entities </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>although there is fine line in the distinction between information status and information structure, it is fair to say that whereas the latter models wider discourse coherence, the former focuses mainly on the local levelof discourse entities.
</prevsent>
<prevsent>section 2 provides more details on how this notion is encoded in our corpus.
</prevsent>
</prevsection>
<citsent citstr=" P96-1038 ">
information status has generated large interest among researchers because of its complex interaction with other linguistic phenomena, thus affecting several natural language processing tasks.since it correlates with word order and pitch accent (lambrecht, 1994; hirschberg and nakatani, 1996), <papid> P96-1038 </papid>for instance, incorporating knowledge on information status would be helpful for natural language generation, and in particular text-tospeech systems.</citsent>
<aftsection>
<nextsent>stober and colleagues, forex ample, ascribe to the lack of such information the lower performance of text-to-speech compared toconcept-to-speech generation, where such knowledge could be made directly available to the system (stober et al , 2000).
</nextsent>
<nextsent>another area where information status can play an important role is anaphora resolution.
</nextsent>
<nextsent>a major obstacle in the resolution of definite noun phrases with full lexical heads is that only small proportion of them is actually anaphoric (ca.
</nextsent>
<nextsent>30%(vieira and poesio, 2000)).<papid> J00-4003 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4071">
<title id=" W06-1612.xml">learning information status of discourse entities </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>another area where information status can play an important role is anaphora resolution.
</prevsent>
<prevsent>a major obstacle in the resolution of definite noun phrases with full lexical heads is that only small proportion of them is actually anaphoric (ca.
</prevsent>
</prevsection>
<citsent citstr=" J00-4003 ">
30%(vieira and poesio, 2000)).<papid> J00-4003 </papid></citsent>
<aftsection>
<nextsent>therefore, in the absence of anaphoricity information, resolution system will try to find an antecedent also for non 94anaphoric definite noun phrases, thus severely affecting performance.
</nextsent>
<nextsent>there has been recent interest in determining anaphoricity before performing anaphora resolution (ng and cardie, 2002;<papid> C02-1139 </papid>uryupina, 2003), <papid> P03-2012 </papid>but results have not been entirely satisfactory.</nextsent>
<nextsent>given that old entities are more likely to be referred to by anaphors, for instance, identification of information status could improve anaphoricity determination.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4072">
<title id=" W06-1612.xml">learning information status of discourse entities </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>30%(vieira and poesio, 2000)).<papid> J00-4003 </papid></prevsent>
<prevsent>therefore, in the absence of anaphoricity information, resolution system will try to find an antecedent also for non 94anaphoric definite noun phrases, thus severely affecting performance.</prevsent>
</prevsection>
<citsent citstr=" C02-1139 ">
there has been recent interest in determining anaphoricity before performing anaphora resolution (ng and cardie, 2002;<papid> C02-1139 </papid>uryupina, 2003), <papid> P03-2012 </papid>but results have not been entirely satisfactory.</citsent>
<aftsection>
<nextsent>given that old entities are more likely to be referred to by anaphors, for instance, identification of information status could improve anaphoricity determination.
</nextsent>
<nextsent>postolache et al  (2005) <papid> H05-1002 </papid>have recently shown that learning information structure with high accuracy is feasible for czech.</nextsent>
<nextsent>however, there are yet no studies that explore such task for english.exploiting an existing annotated corpus, in this paper we report experiments on learning model for the automatic identification of information status in english.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4073">
<title id=" W06-1612.xml">learning information status of discourse entities </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>30%(vieira and poesio, 2000)).<papid> J00-4003 </papid></prevsent>
<prevsent>therefore, in the absence of anaphoricity information, resolution system will try to find an antecedent also for non 94anaphoric definite noun phrases, thus severely affecting performance.</prevsent>
</prevsection>
<citsent citstr=" P03-2012 ">
there has been recent interest in determining anaphoricity before performing anaphora resolution (ng and cardie, 2002;<papid> C02-1139 </papid>uryupina, 2003), <papid> P03-2012 </papid>but results have not been entirely satisfactory.</citsent>
<aftsection>
<nextsent>given that old entities are more likely to be referred to by anaphors, for instance, identification of information status could improve anaphoricity determination.
</nextsent>
<nextsent>postolache et al  (2005) <papid> H05-1002 </papid>have recently shown that learning information structure with high accuracy is feasible for czech.</nextsent>
<nextsent>however, there are yet no studies that explore such task for english.exploiting an existing annotated corpus, in this paper we report experiments on learning model for the automatic identification of information status in english.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4074">
<title id=" W06-1612.xml">learning information status of discourse entities </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>there has been recent interest in determining anaphoricity before performing anaphora resolution (ng and cardie, 2002;<papid> C02-1139 </papid>uryupina, 2003), <papid> P03-2012 </papid>but results have not been entirely satisfactory.</prevsent>
<prevsent>given that old entities are more likely to be referred to by anaphors, for instance, identification of information status could improve anaphoricity determination.</prevsent>
</prevsection>
<citsent citstr=" H05-1002 ">
postolache et al  (2005) <papid> H05-1002 </papid>have recently shown that learning information structure with high accuracy is feasible for czech.</citsent>
<aftsection>
<nextsent>however, there are yet no studies that explore such task for english.exploiting an existing annotated corpus, in this paper we report experiments on learning model for the automatic identification of information status in english.
</nextsent>
<nextsent>for our experiments we annotated portion of the transcribed switchboard corpus (godfrey et al , 1992), consisting of 147 dialogues (nissim et al , 2004).1 in the following section we provide brief description of the annotation categories.
</nextsent>
<nextsent>2.1 annotation.
</nextsent>
<nextsent>our annotation of information status mainly builds on (prince, 1992), and employs distinction into old, mediated, and new entities similar to the work of (strube, 1998; <papid> P98-2204 </papid>eckert and strube, 2001).all noun phrases (nps) were extracted as mark able entities using pre-existing parse information (carletta et al , 2004).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4075">
<title id=" W06-1612.xml">learning information status of discourse entities </title>
<section> data.  </section>
<citcontext>
<prevsection>
<prevsent>for our experiments we annotated portion of the transcribed switchboard corpus (godfrey et al , 1992), consisting of 147 dialogues (nissim et al , 2004).1 in the following section we provide brief description of the annotation categories.
</prevsent>
<prevsent>2.1 annotation.
</prevsent>
</prevsection>
<citsent citstr=" P98-2204 ">
our annotation of information status mainly builds on (prince, 1992), and employs distinction into old, mediated, and new entities similar to the work of (strube, 1998; <papid> P98-2204 </papid>eckert and strube, 2001).all noun phrases (nps) were extracted as mark able entities using pre-existing parse information (carletta et al , 2004).</citsent>
<aftsection>
<nextsent>an entity was annotated as new if it has not been previously referred to and is yet unknown to the hearer.
</nextsent>
<nextsent>the tag mediated was instead used whenever an entity that is newly mentioned in the dialogue can be inferred by the hearer thanks to prior or general context.2 typical examples of mediated entities are generally known objects (such as the sun?, or the pope?
</nextsent>
<nextsent>(lobner, 1985)), and bridging anaphors (clark, 1975; <papid> T75-2034 </papid>vieira and poesio, 2000), <papid> J00-4003 </papid>where an entity is related to previously introduced one.</nextsent>
<nextsent>when ever an entity was neither new nor mediated, it was considered as old.1switchboard is collection of spontaneous phone conversations, averaging six minutes in length, between speakers of american english on predetermined topics.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4076">
<title id=" W06-1612.xml">learning information status of discourse entities </title>
<section> data.  </section>
<citcontext>
<prevsection>
<prevsent>an entity was annotated as new if it has not been previously referred to and is yet unknown to the hearer.
</prevsent>
<prevsent>the tag mediated was instead used whenever an entity that is newly mentioned in the dialogue can be inferred by the hearer thanks to prior or general context.2 typical examples of mediated entities are generally known objects (such as the sun?, or the pope?
</prevsent>
</prevsection>
<citsent citstr=" T75-2034 ">
(lobner, 1985)), and bridging anaphors (clark, 1975; <papid> T75-2034 </papid>vieira and poesio, 2000), <papid> J00-4003 </papid>where an entity is related to previously introduced one.</citsent>
<aftsection>
<nextsent>when ever an entity was neither new nor mediated, it was considered as old.1switchboard is collection of spontaneous phone conversations, averaging six minutes in length, between speakers of american english on predetermined topics.
</nextsent>
<nextsent>a third of the corpus is syntactically parsed as part of the penn treebank (marcus et al , 1993)<papid> J93-2004 </papid>2this type corresponds to princes (1981), corresponds to princes (1992) in ferrables.</nextsent>
<nextsent>in order to account for the complexity of the notion of information status, the annotation also includes sub-type classification for old and mediated entities that provides finer-grained distinction with information on why given entity is mediated (e.g., set-relation, bridging) or old (e.g., coreference, generic pronouns).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4078">
<title id=" W06-1612.xml">learning information status of discourse entities </title>
<section> data.  </section>
<citcontext>
<prevsection>
<prevsent>(lobner, 1985)), and bridging anaphors (clark, 1975; <papid> T75-2034 </papid>vieira and poesio, 2000), <papid> J00-4003 </papid>where an entity is related to previously introduced one.</prevsent>
<prevsent>when ever an entity was neither new nor mediated, it was considered as old.1switchboard is collection of spontaneous phone conversations, averaging six minutes in length, between speakers of american english on predetermined topics.</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
a third of the corpus is syntactically parsed as part of the penn treebank (marcus et al , 1993)<papid> J93-2004 </papid>2this type corresponds to princes (1981), corresponds to princes (1992) in ferrables.</citsent>
<aftsection>
<nextsent>in order to account for the complexity of the notion of information status, the annotation also includes sub-type classification for old and mediated entities that provides finer-grained distinction with information on why given entity is mediated (e.g., set-relation, bridging) or old (e.g., coreference, generic pronouns).
</nextsent>
<nextsent>in order to testthe feasibility of automatically assigning information status to discourse entities, we took modular approach and only considered the coarser-grained distinctions for this first study.
</nextsent>
<nextsent>information about the finer-grained sub types will be used in future work.
</nextsent>
<nextsent>in addition to the main categories, we used two more annotation classes: tag non-applicable, used for entities that were wrongly extracted in the automatic selection of markables (e.g. course?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4079">
<title id=" W06-1612.xml">learning information status of discourse entities </title>
<section> data.  </section>
<citcontext>
<prevsection>
<prevsent>inof course?), for idiomatic occurrences, and expletive uses of it?; and tag not-understood to be applied whenever an annotator did not fully under stand the text.
</prevsent>
<prevsent>instances annotated with these twotags, as well as all traces, which were left unannotated, were excluded from all our experiments.
</prevsent>
</prevsection>
<citsent citstr=" J96-2004 ">
inter-annotator agreement was measured using the kappa (k) statistics (cohen, 1960; carletta,1996) <papid> J96-2004 </papid>on 1,502 instances (three switchboard dia logues) marked by two annotators who followed specific written guidelines.</citsent>
<aftsection>
<nextsent>given that the task involves fair amount of subjective judgement, agreement was remarkably high.
</nextsent>
<nextsent>over the three dialogues, the annotation yielded = .845 for the old/med/new classification (k = .788 when including the finer-grained subtype distinction).specifically, old?
</nextsent>
<nextsent>proved to be the easiest to distinguish, with = .902; for med?
</nextsent>
<nextsent>and new?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4080">
<title id=" W06-1612.xml">learning information status of discourse entities </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>partial = yes | full  = 1 | | det = def: med | | det = indef | | | length  = 2 | | | | gramm = subj: med | | | | gramm = subjpassive: new | | | | gramm = obj: med | | | | gramm = pp: med | | | | gramm = other | | | | | type = proper: med | | | | | type = common: new | | | | | type = pronoun: new | | | | | type = other: med | | | length   2: med | | det = dem | | | gramm = subj . . .
</prevsent>
<prevsent>figure 3: top of c.5, full training set, three classes figure 3 shows the top of c4.5 (trained on the full training set for the three-way classification), which looks remarkably different from the rules in figure 1.
</prevsent>
</prevsection>
<citsent citstr=" J95-2003 ">
we had based our decision of emphasising the importance of the np type on the linguistic evidence that different syntactic realisations reflect different degrees of availability of discourse entities (givon, 1983; ariel, 1990; grosz et al , 1995).<papid> J95-2003 </papid></citsent>
<aftsection>
<nextsent>in the learnt model, however, knowledge about np type is only used as subordinate to other features.
</nextsent>
<nextsent>this is indeed mirrored in the fact that removing np type information from the feature set causes accuracy to drop, but classifier building on nptype alone performs poorly (see table 8).3 interestingly, though, more informative knowledge about syntactic form seems to be derived from the determiner type, which helps distinguish degrees of oldness among common nouns.
</nextsent>
<nextsent>3the nptype-only classifier assigns old to pronouns and med to all other types; it never assigns new.
</nextsent>
<nextsent>5.5 naive bayes model.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4082">
<title id=" W06-1612.xml">learning information status of discourse entities </title>
<section> conclusions and future work.  </section>
<citcontext>
<prevsection>
<prevsent>first, the syntactic category other?
</prevsent>
<prevsent>is far too large, and finer distinctions must be made by means of better extraction rules from the trees.second, and most importantly, we believe that using more features will be the main trigger of higher accuracy.
</prevsent>
</prevsection>
<citsent citstr=" P98-1013 ">
in particular, we plan to use additional lexical and relational features derived from knowledge sources such as wordnet (fellbaum, 1998) and framenet (baker et al , 1998) <papid> P98-1013 </papid>which should be especially helpful in distinguishing mediated from new entities, the most difficult decision to make.</citsent>
<aftsection>
<nextsent>for example, an entity that is linked in wordnet (within given depth) and/or framenet to previously introduced one is more likely to be mediated than new.additionally, we will attempt to exploit dialogue turns, since knowing which speaker said what is clearly very valuable information.
</nextsent>
<nextsent>in similar vein, we will experiment with distance measures, in terms of turns, sentences, or even time, for determining when an introduced entity might stop to be available.we also plan to run experiments on the automatic classification of old and mediated sub types (the finer-grained classification) that is included in the corpus but that we did not consider for the present study (see section 2.1).
</nextsent>
<nextsent>the major benefit of this would be contribution to the resolution of bridging anaphora.
</nextsent>

</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4083">
<title id=" W06-1667.xml">unsupervised relation disambiguation with order identification capabilities </title>
<section> abstract </section>
<citcontext>
<prevsection>
<prevsent>we present an unsupervised learning approach to disambiguate various relations between name entities by use of various lexical and syntactic features from thecontexts.
</prevsent>
<prevsent>it works by calculating eigenvectors of an adjacency graphs lapla cian to recover sub manifold of data from high dimensionality space andthen performing cluster number estimation on the eigenvectors.
</prevsent>
</prevsection>
<citsent citstr=" P04-1053 ">
this method can address two difficulties encoutered in hasegawa et al (2004)<papid> P04-1053 </papid>s hierarchical clustering: no consideration of manifold structure in data, and requirement to provide cluster number by users.</citsent>
<aftsection>
<nextsent>experiment results on ace corpora show that this spectral clustering based approach outperforms hasegawa et al (2004)<papid> P04-1053 </papid>s hierarchical clustering method and plain k-means clustering method.</nextsent>
<nextsent>the task of relation extraction is to identify various semantic relations between name entities from text.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4093">
<title id=" W06-1667.xml">unsupervised relation disambiguation with order identification capabilities </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>experiment results on ace corpora show that this spectral clustering based approach outperforms hasegawa et al (2004)<papid> P04-1053 </papid>s hierarchical clustering method and plain k-means clustering method.</prevsent>
<prevsent>the task of relation extraction is to identify various semantic relations between name entities from text.</prevsent>
</prevsection>
<citsent citstr=" A00-2030 ">
prior work on automatic relation extraction come in three kinds: supervised learning algorithms (miller et al, 2000; <papid> A00-2030 </papid>zelenko et al, 2002; <papid> W02-1010 </papid>culotta and soresen, 2004; kambhatla, 2004; <papid> P04-3022 </papid>zhou et al, 2005), <papid> P05-1053 </papid>semi-supervised learning algorithms (brin, 1998; agichtein and gravano, 2000; zhang, 2004), and unsupervised learning algorithm (hasegawa et al., 2004).<papid> P04-1053 </papid>among these methods, supervised learning is usually more preferred when large amount of labeled training data is available.</citsent>
<aftsection>
<nextsent>however, it is time-consuming and labor-intensive to manually tag large amount of training data.
</nextsent>
<nextsent>semi-supervisedlearning methods have been put forward to minimize the corpus annotation requirement.
</nextsent>
<nextsent>most of semi-supervised methods employ the bootstrapping framework, which only need to pre-define some initial seeds for any particular relation, and then bootstrap from the seeds to acquire the relation.
</nextsent>
<nextsent>how ever, it is often quite difficult to enumerate all class labels in the initial seeds and decide an optimal?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4095">
<title id=" W06-1667.xml">unsupervised relation disambiguation with order identification capabilities </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>experiment results on ace corpora show that this spectral clustering based approach outperforms hasegawa et al (2004)<papid> P04-1053 </papid>s hierarchical clustering method and plain k-means clustering method.</prevsent>
<prevsent>the task of relation extraction is to identify various semantic relations between name entities from text.</prevsent>
</prevsection>
<citsent citstr=" W02-1010 ">
prior work on automatic relation extraction come in three kinds: supervised learning algorithms (miller et al, 2000; <papid> A00-2030 </papid>zelenko et al, 2002; <papid> W02-1010 </papid>culotta and soresen, 2004; kambhatla, 2004; <papid> P04-3022 </papid>zhou et al, 2005), <papid> P05-1053 </papid>semi-supervised learning algorithms (brin, 1998; agichtein and gravano, 2000; zhang, 2004), and unsupervised learning algorithm (hasegawa et al., 2004).<papid> P04-1053 </papid>among these methods, supervised learning is usually more preferred when large amount of labeled training data is available.</citsent>
<aftsection>
<nextsent>however, it is time-consuming and labor-intensive to manually tag large amount of training data.
</nextsent>
<nextsent>semi-supervisedlearning methods have been put forward to minimize the corpus annotation requirement.
</nextsent>
<nextsent>most of semi-supervised methods employ the bootstrapping framework, which only need to pre-define some initial seeds for any particular relation, and then bootstrap from the seeds to acquire the relation.
</nextsent>
<nextsent>how ever, it is often quite difficult to enumerate all class labels in the initial seeds and decide an optimal?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4096">
<title id=" W06-1667.xml">unsupervised relation disambiguation with order identification capabilities </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>experiment results on ace corpora show that this spectral clustering based approach outperforms hasegawa et al (2004)<papid> P04-1053 </papid>s hierarchical clustering method and plain k-means clustering method.</prevsent>
<prevsent>the task of relation extraction is to identify various semantic relations between name entities from text.</prevsent>
</prevsection>
<citsent citstr=" P04-3022 ">
prior work on automatic relation extraction come in three kinds: supervised learning algorithms (miller et al, 2000; <papid> A00-2030 </papid>zelenko et al, 2002; <papid> W02-1010 </papid>culotta and soresen, 2004; kambhatla, 2004; <papid> P04-3022 </papid>zhou et al, 2005), <papid> P05-1053 </papid>semi-supervised learning algorithms (brin, 1998; agichtein and gravano, 2000; zhang, 2004), and unsupervised learning algorithm (hasegawa et al., 2004).<papid> P04-1053 </papid>among these methods, supervised learning is usually more preferred when large amount of labeled training data is available.</citsent>
<aftsection>
<nextsent>however, it is time-consuming and labor-intensive to manually tag large amount of training data.
</nextsent>
<nextsent>semi-supervisedlearning methods have been put forward to minimize the corpus annotation requirement.
</nextsent>
<nextsent>most of semi-supervised methods employ the bootstrapping framework, which only need to pre-define some initial seeds for any particular relation, and then bootstrap from the seeds to acquire the relation.
</nextsent>
<nextsent>how ever, it is often quite difficult to enumerate all class labels in the initial seeds and decide an optimal?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4098">
<title id=" W06-1667.xml">unsupervised relation disambiguation with order identification capabilities </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>experiment results on ace corpora show that this spectral clustering based approach outperforms hasegawa et al (2004)<papid> P04-1053 </papid>s hierarchical clustering method and plain k-means clustering method.</prevsent>
<prevsent>the task of relation extraction is to identify various semantic relations between name entities from text.</prevsent>
</prevsection>
<citsent citstr=" P05-1053 ">
prior work on automatic relation extraction come in three kinds: supervised learning algorithms (miller et al, 2000; <papid> A00-2030 </papid>zelenko et al, 2002; <papid> W02-1010 </papid>culotta and soresen, 2004; kambhatla, 2004; <papid> P04-3022 </papid>zhou et al, 2005), <papid> P05-1053 </papid>semi-supervised learning algorithms (brin, 1998; agichtein and gravano, 2000; zhang, 2004), and unsupervised learning algorithm (hasegawa et al., 2004).<papid> P04-1053 </papid>among these methods, supervised learning is usually more preferred when large amount of labeled training data is available.</citsent>
<aftsection>
<nextsent>however, it is time-consuming and labor-intensive to manually tag large amount of training data.
</nextsent>
<nextsent>semi-supervisedlearning methods have been put forward to minimize the corpus annotation requirement.
</nextsent>
<nextsent>most of semi-supervised methods employ the bootstrapping framework, which only need to pre-define some initial seeds for any particular relation, and then bootstrap from the seeds to acquire the relation.
</nextsent>
<nextsent>how ever, it is often quite difficult to enumerate all class labels in the initial seeds and decide an optimal?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4118">
<title id=" W06-1667.xml">unsupervised relation disambiguation with order identification capabilities </title>
<section> set = 2;.  </section>
<citcontext>
<prevsection>
<prevsent>it supports multi-class classification.
</prevsent>
<prevsent>573 table 6: comparison of the existing efforts on ace rdc task.
</prevsent>
</prevsection>
<citsent citstr=" P04-1054 ">
relation dectection relation classification on types on sub types method r p f r fcu lotta and soresen (2004) <papid> P04-1054 </papid>tree kernel based 81.2 51.8 63.2 67.1 35.0 45.8 - - kambhatla (2004) <papid> P04-3022 </papid>feature based, maximum entropy - - - - - - 63.5 45.2 52.8 zhou et al (2005) <papid> P05-1053 </papid>feature based,svm 84.8 66.7 74.7 77.2 60.7 68.0 63.1 49.5 55.5 the cluster number as the number of ground truth classes.we also approached the relation extraction problem using the standard clustering technique, kmeans, where we adopted the same feature set defined in our proposed method to cluster the context vectors of entity mention pairs and pre-specified the cluster number as the number of ground truth classes.table 5 reports the performance of our proposed method comparing with svm-based supervised method and the other two unsupervised methods.</citsent>
<aftsection>
<nextsent>as the result shows, svm-based method by using the same feature set in our proposed method can achieve 61.2%/49.6%/54.8% in precision/recall/fmeasure.
</nextsent>
<nextsent>table 5 also shows our proposed spectral based method clearly outperforms the other two unsupervised methods by 12.5% and 9.5% inf-measure respectively.
</nextsent>
<nextsent>moreover, the incorporation of various lexical and syntactic features into hasegawa et al (2004)<papid> P04-1053 </papid>s method2 makes it outperform hasegawa et al (2004)<papid> P04-1053 </papid>s method1 which only uses word feature.</nextsent>
<nextsent>3.4 discussion.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4140">
<title id=" W06-1507.xml">negative concord and restructuring in palestinian arabic a comparison of tag and ccg analyses </title>
<section> a tag analysis </section>
<citcontext>
<prevsection>
<prevsent>the ip*-node in the scope set of wela-phrase can then adjoin to the ip* node in the pp scope set, which in turn adjoins to the ip-node of the initial tree.
</prevsent>
<prevsent>for example, (21) above can be analyzed withthe elementary trees in (23) (trees are in abbreviated form), producing the derivation tree in (24): (23) a. ? : { 1 : ip* , 2 : np wela ? ada } b. ? : { 1 : ip*00 , 2 : pp gnb np02 } c. ?: ip ma:- ip* ?: ip00 kaqatt pp02 (24) ? 1(00) 1(00) ?(00) 2(02) 2(02) however, given (24) it is still not possible to state generalization about negative concord locality in terms of sisterhood in the derivation tree.this can be remedied by adopting the node sharing?
</prevsent>
</prevsection>
<citsent citstr=" J05-2003 ">
relation proposed by (kallmeyer, 2005).<papid> J05-2003 </papid>informally, two nodes ? and ? are in node sharing relation in derivation tree iff they are either in mother-daughter relation in at node address a, or there is sequence of nodes n1 . . .</citsent>
<aftsection>
<nextsent>nn which is the transitive closure of mother-daughter relation in in which the node pairs are related in terms of the root node or foot node in an auxiliary tree.on this basis, the negative concord locality generalization is that wela-phrase and its licensor are shared-node sisters?
</nextsent>
<nextsent>in the derivation tree, where shared-node sisters are two nodes and which are each in shared-node relation with asingle node . for example, in (24) ? is shared node parent of both 1 and ?.
</nextsent>
<nextsent>accordingly, 1 and ? are shared-node sisters with respect to ?.
</nextsent>
<nextsent>2.2 trigger verbs and complement type.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4141">
<title id=" W06-3123.xml">constraining the phrase based joint probability statistical translation model </title>
<section> abstract </section>
<citcontext>
<prevsection>


</prevsection>
<citsent citstr=" W02-1018 ">
the joint probability model proposed by marcu and wong (2002) <papid> W02-1018 </papid>provides strong probabilistic framework for phrase-based statistical machine translation (smt).</citsent>
<aftsection>
<nextsent>the models usefulness is, however, limited bythe computational complexity of estimating parameters at the phrase level.
</nextsent>
<nextsent>we present the first model to use word alignments for constraining the space of phrasal alignments searched during expectation maximization (em) training.
</nextsent>
<nextsent>constraining the joint model improves performance,showing results that are very close to stateof-the-art phrase-based models.
</nextsent>
<nextsent>it also allows it to scale up to larger corpora and therefore be more widely applicable.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4143">
<title id=" W06-3123.xml">constraining the phrase based joint probability statistical translation model </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>machine translation is hard problem because of the highly complex, irregular and diverse nature of natural languages.
</prevsent>
<prevsent>it is impossible to accurately model all the linguistic rules that shape the translation process, and therefore principled approach uses statistical methods to make optimal decisions given incomplete data.
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
the original ibm models (brown et al, 1993) <papid> J93-2003 </papid>learn word-to-word alignment probabilities which makes it computationally feasible to estimate model parameters from large amounts of training data.</citsent>
<aftsection>
<nextsent>phrase-based smt models, such as the alignment template model (och, 2003), improve on word-based models because phrases provide local context which leads to better lexical choice and more reliable local reordering.
</nextsent>
<nextsent>however, most phrase-based models extract their phrase pairs from previously word-aligned corpora using ad hoc heuristics.
</nextsent>
<nextsent>these models perform no search for optimal phrasal alignments.
</nextsent>
<nextsent>even though this is an efficient strategy, it is departure from the rigorous statistical framework of the ibm models.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4146">
<title id=" W06-3123.xml">constraining the phrase based joint probability statistical translation model </title>
<section> translation models.  </section>
<citcontext>
<prevsection>
<prevsent>154
</prevsent>
<prevsent>2.1 standard phrase-based model.
</prevsent>
</prevsection>
<citsent citstr=" N03-1017 ">
most phrase-based translation models (och, 2003; koehn et al, 2003; <papid> N03-1017 </papid>vogel et al, 2003) relyon pre-existing set of word-based alignments from which they induce their parameters.</citsent>
<aftsection>
<nextsent>in this project we use the model described by koehn et al (2003) <papid> N03-1017 </papid>which extracts its phrase alignments from corpus that has been word aligned.</nextsent>
<nextsent>from now on we refer to this phrase-based translation model as the standard model.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4151">
<title id=" W06-3123.xml">constraining the phrase based joint probability statistical translation model </title>
<section> translation models.  </section>
<citcontext>
<prevsection>
<prevsent>commonly occurring phrases are more likely to lead to the creation of useful phrase pairs, and without this restriction the search space would be much larger.
</prevsent>
<prevsent>the probability of sentence and its translation is the sum of all possible alignments c, each of which is defined as the product of the probability of all individual concepts: p(f,e) = ? cc ?  ei,f c p(  ei, i  ) (1)the model is trained by initializing the translation table using stirling numbers of the second kind to efficiently estimate p(  ei, i  ) by calculating the proportion of alignments which contain p(  ei, i  ) compared to the total number of alignments in the sentence (marcu and wong, 2002).<papid> W02-1018 </papid></prevsent>
</prevsection>
<citsent citstr=" P97-1063 ">
em is then performed by first discovering an initial phrasal alignments using greedy algorithm similar to the competitive linking algorithm (melamed, 1997).<papid> P97-1063 </papid></citsent>
<aftsection>
<nextsent>the highest probability phrase pairs are iteratively selected until all phrases are are linked.
</nextsent>
<nextsent>then hill-climbing is performed by searching once for each iteration for all merges,splits, moves and swaps that improve the probability of the initial phrasal alignment.
</nextsent>
<nextsent>fractional counts are collected for all alignments visited.
</nextsent>
<nextsent>training the ibm models is computationally challenging, but the joint model is much more demanding.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4154">
<title id=" W06-3123.xml">constraining the phrase based joint probability statistical translation model </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>the impact of constraining the joint model trained on 10,000 sentences of the german-english europarl corpora and tested with the europarl test set used in koehn et al (2003) <papid> N03-1017 </papid>than 10 million times that of the phrase pair withthe highest count, are pruned from the phrase ta ble.</prevsent>
<prevsent>the model is also parallel ized in order to speed up training.</prevsent>
</prevsection>
<citsent citstr=" P02-1038 ">
the translation models are included within alog-linear model (och and ney, 2002) <papid> P02-1038 </papid>which allows weighted combination of features func tions.</citsent>
<aftsection>
<nextsent>for the comparison of the basic systems in table 2 only three features were used for both the joint and the standard model: p(e|f), p(f |e) and the language model, and they were given equal weights.
</nextsent>
<nextsent>the results in table 2 show that the joint model is capable of training on large datasets, with reasonable performance compared to the standard model.
</nextsent>
<nextsent>however, here it seems that the standard model has slight advantage.
</nextsent>
<nextsent>this is almost certainly related to the fact that the joint model results in much smaller phrase table.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4156">
<title id=" W06-3123.xml">constraining the phrase based joint probability statistical translation model </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>4.3 german-english submission.
</prevsent>
<prevsent>we also submitted german-english system using the standard approach to phrase extraction.
</prevsent>
</prevsection>
<citsent citstr=" P05-1066 ">
the purpose of this submission was to validate the syntactic reordering method that we previously proposed (collins et al, 2005).<papid> P05-1066 </papid></citsent>
<aftsection>
<nextsent>we parse the german training and test corpus and reorder it according to set of manually devised rules.
</nextsent>
<nextsent>then, weuse our phrase-based system with standard phrase extraction, lexicalized reordering, lexical scoring, 5-gram lm, and the pharaoh decoder.on the development test set, the syntactic reordering improved performance from 26.86 to 27.70.
</nextsent>
<nextsent>the best submission in last years shared task achieved score of 24.77 on this set.
</nextsent>
<nextsent>we presented the first attempt at creating systematic framework which uses word alignment constraints to guide phrase-based em training.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4158">
<title id=" W06-2906.xml">resolving and generating definite anaphora by modeling hypernymy using unlabeled corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we describe an unsupervised approach to this task that extracts examples containing definite nps from large corpus, considers all headwords appearing before the definite np as potential antecedents and then filters the noisy antecedent, definite-np  pair using mutual information space.
</prevsent>
<prevsent>the co-occurencestatistics of such pairs can then be used as mechanism for detecting hypernym relation between the definite np and its potential antecedents.
</prevsent>
</prevsection>
<citsent citstr=" J05-3004 ">
we compare this approach with wordnet-based algorithm and with an approach presented by markert and nissim (2005) <papid> J05-3004 </papid>on resolving definite np coreference that makes use of lexico-syntactic patterns such as and other ys?</citsent>
<aftsection>
<nextsent>as utilized by hearst (1992).<papid> C92-2082 </papid></nextsent>
<nextsent>there is rich tradition of work using lexical and semantic resources for anaphora and coreference res olution.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4159">
<title id=" W06-2906.xml">resolving and generating definite anaphora by modeling hypernymy using unlabeled corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the co-occurencestatistics of such pairs can then be used as mechanism for detecting hypernym relation between the definite np and its potential antecedents.
</prevsent>
<prevsent>we compare this approach with wordnet-based algorithm and with an approach presented by markert and nissim (2005) <papid> J05-3004 </papid>on resolving definite np coreference that makes use of lexico-syntactic patterns such as and other ys?</prevsent>
</prevsection>
<citsent citstr=" C92-2082 ">
as utilized by hearst (1992).<papid> C92-2082 </papid></citsent>
<aftsection>
<nextsent>there is rich tradition of work using lexical and semantic resources for anaphora and coreference resolution.
</nextsent>
<nextsent>several researchers have used wordnet as lexical and semantic resource for certain types of bridging anaphora (poesio et al , 1997; <papid> W97-1301 </papid>meyer anddale, 2002).</nextsent>
<nextsent>wordnet has also been used as an important feature in machine learning of coreference resolution using supervised training data (soon etal., 2001; <papid> J01-4004 </papid>ng and cardie, 2002).<papid> P02-1014 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4160">
<title id=" W06-2906.xml">resolving and generating definite anaphora by modeling hypernymy using unlabeled corpora </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>as utilized by hearst (1992).<papid> C92-2082 </papid></prevsent>
<prevsent>there is rich tradition of work using lexical and semantic resources for anaphora and coreference res olution.</prevsent>
</prevsection>
<citsent citstr=" W97-1301 ">
several researchers have used wordnet as lexical and semantic resource for certain types of bridging anaphora (poesio et al , 1997; <papid> W97-1301 </papid>meyer anddale, 2002).</citsent>
<aftsection>
<nextsent>wordnet has also been used as an important feature in machine learning of coreference resolution using supervised training data (soon etal., 2001; <papid> J01-4004 </papid>ng and cardie, 2002).<papid> P02-1014 </papid></nextsent>
<nextsent>however, several researchers have reported that knowledge incorporated via wordnet is still insufficient for definite anaphora resolution.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4161">
<title id=" W06-2906.xml">resolving and generating definite anaphora by modeling hypernymy using unlabeled corpora </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>there is rich tradition of work using lexical and semantic resources for anaphora and coreference resolution.
</prevsent>
<prevsent>several researchers have used wordnet as lexical and semantic resource for certain types of bridging anaphora (poesio et al , 1997; <papid> W97-1301 </papid>meyer anddale, 2002).</prevsent>
</prevsection>
<citsent citstr=" J01-4004 ">
wordnet has also been used as an important feature in machine learning of coreference resolution using supervised training data (soon etal., 2001; <papid> J01-4004 </papid>ng and cardie, 2002).<papid> P02-1014 </papid></citsent>
<aftsection>
<nextsent>however, several researchers have reported that knowledge incorporated via wordnet is still insufficient for definite anaphora resolution.
</nextsent>
<nextsent>and of course, wordnet is not available for all languages and is missing inclusion of large segments of the vocabulary even for covered languages.
</nextsent>
<nextsent>hence researchers have investigated use of corpus-based approaches to build wordnet like resource automatically (hearst, 1992; <papid> C92-2082 </papid>cara 1the test examples were selected as follows: first, all the sentences containing definite np the y?</nextsent>
<nextsent>were extracted from the corpus.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4162">
<title id=" W06-2906.xml">resolving and generating definite anaphora by modeling hypernymy using unlabeled corpora </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>there is rich tradition of work using lexical and semantic resources for anaphora and coreference resolution.
</prevsent>
<prevsent>several researchers have used wordnet as lexical and semantic resource for certain types of bridging anaphora (poesio et al , 1997; <papid> W97-1301 </papid>meyer anddale, 2002).</prevsent>
</prevsection>
<citsent citstr=" P02-1014 ">
wordnet has also been used as an important feature in machine learning of coreference resolution using supervised training data (soon etal., 2001; <papid> J01-4004 </papid>ng and cardie, 2002).<papid> P02-1014 </papid></citsent>
<aftsection>
<nextsent>however, several researchers have reported that knowledge incorporated via wordnet is still insufficient for definite anaphora resolution.
</nextsent>
<nextsent>and of course, wordnet is not available for all languages and is missing inclusion of large segments of the vocabulary even for covered languages.
</nextsent>
<nextsent>hence researchers have investigated use of corpus-based approaches to build wordnet like resource automatically (hearst, 1992; <papid> C92-2082 </papid>cara 1the test examples were selected as follows: first, all the sentences containing definite np the y?</nextsent>
<nextsent>were extracted from the corpus.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4164">
<title id=" W06-2906.xml">resolving and generating definite anaphora by modeling hypernymy using unlabeled corpora </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>were discarded.
</prevsent>
<prevsent>from this anaphoric set of sentences, 177 sentence instances covering 13 distinct hypernyms were randomly selected as the test set and annotated for the correct antecedent by human judges.
</prevsent>
</prevsection>
<citsent citstr=" P99-1008 ">
ballo, 1999; berland and charniak, 1999).<papid> P99-1008 </papid></citsent>
<aftsection>
<nextsent>also,several researchers have applied it to resolving different types of bridging anaphora (clark, 1975).<papid> T75-2034 </papid></nextsent>
<nextsent>poesio et al  (2002) have proposed extracting lexical knowledge about part-of relations using hearst-stylepatterns and applied it to the task of resolving bridging references.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4165">
<title id=" W06-2906.xml">resolving and generating definite anaphora by modeling hypernymy using unlabeled corpora </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>from this anaphoric set of sentences, 177 sentence instances covering 13 distinct hypernyms were randomly selected as the test set and annotated for the correct antecedent by human judges.
</prevsent>
<prevsent>ballo, 1999; berland and charniak, 1999).<papid> P99-1008 </papid></prevsent>
</prevsection>
<citsent citstr=" T75-2034 ">
also,several researchers have applied it to resolving different types of bridging anaphora (clark, 1975).<papid> T75-2034 </papid></citsent>
<aftsection>
<nextsent>poesio et al  (2002) have proposed extracting lexical knowledge about part-of relations using hearst-stylepatterns and applied it to the task of resolving bridging references.
</nextsent>
<nextsent>poesio et al  (2004) <papid> P04-1019 </papid>have suggested using google as source of computing lexical distance between antecedent and definite np for mere ological bridging references (references referring to parts of an object already introduced).</nextsent>
<nextsent>markert et al (2003) have applied relations extracted from lexico syntactic patterns such as and other ys?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4166">
<title id=" W06-2906.xml">resolving and generating definite anaphora by modeling hypernymy using unlabeled corpora </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>also,several researchers have applied it to resolving different types of bridging anaphora (clark, 1975).<papid> T75-2034 </papid></prevsent>
<prevsent>poesio et al  (2002) have proposed extracting lexical knowledge about part-of relations using hearst-stylepatterns and applied it to the task of resolving bridging references.</prevsent>
</prevsection>
<citsent citstr=" P04-1019 ">
poesio et al  (2004) <papid> P04-1019 </papid>have suggested using google as source of computing lexical distance between antecedent and definite np for mere ological bridging references (references referring to parts of an object already introduced).</citsent>
<aftsection>
<nextsent>markert et al (2003) have applied relations extracted from lexico syntactic patterns such as and other ys?
</nextsent>
<nextsent>for other anaphora (referential nps with modifiers other or another) and for bridging involving meronymy.there has generally been lack of work in the existing literature for automatically building lexical resources for definite anaphora resolution involving hyponyms relations such as presented in example (1).
</nextsent>
<nextsent>however, this issue was recently addressed by markert and nissim (2005) <papid> J05-3004 </papid>by extending their work on other-anaphora using lexico syntactic pattern and other ys to antecedent selection for definite npcoreference.</nextsent>
<nextsent>however, our task is more challenging since the anaphoric definite nps in our test set include only hypernym anaphors without including the much simpler cases of headword repetition and other instances of string matching.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4185">
<title id=" W07-0405.xml">binarization synchronous binarization and target side binarization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this paper presents new bina riza tion scheme, target-side binarization, and compares it with source-side and synchronous binarizations on both string based and tree-based systems using synchronous grammars.
</prevsent>
<prevsent>in particular, we demonstrate the effectiveness of target side binarization on large-scale tree-to string translation system.
</prevsent>
</prevsection>
<citsent citstr=" P05-1033 ">
several recent syntax-based models for machine translation (chiang, 2005; <papid> P05-1033 </papid>galley et al, 2006) <papid> P06-1121 </papid>can be seen as instances of the general framework of synchronous grammars and tree transducers.</citsent>
<aftsection>
<nextsent>in this framework, decoding can be thought of as parsing problems, whose complexity is in general exponential in the number of nonterminals on the right hand side of grammar rule.
</nextsent>
<nextsent>to alleviate this problem, one can borrow from parsing the technique of binarizing context-free grammars (into chomsky normal form) to reduce the complexity.
</nextsent>
<nextsent>with synchronous context-free grammars (scfg), however, this problem becomes more complicated with the additional dimension of target-side permutation.
</nextsent>
<nextsent>the simplest method of binarizing an scfg is to binarize (left-to-right) on the source-side as if treating it as monolingual cfg for the sourcelangauge.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4187">
<title id=" W07-0405.xml">binarization synchronous binarization and target side binarization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this paper presents new bina riza tion scheme, target-side binarization, and compares it with source-side and synchronous binarizations on both string based and tree-based systems using synchronous grammars.
</prevsent>
<prevsent>in particular, we demonstrate the effectiveness of target side binarization on large-scale tree-to string translation system.
</prevsent>
</prevsection>
<citsent citstr=" P06-1121 ">
several recent syntax-based models for machine translation (chiang, 2005; <papid> P05-1033 </papid>galley et al, 2006) <papid> P06-1121 </papid>can be seen as instances of the general framework of synchronous grammars and tree transducers.</citsent>
<aftsection>
<nextsent>in this framework, decoding can be thought of as parsing problems, whose complexity is in general exponential in the number of nonterminals on the right hand side of grammar rule.
</nextsent>
<nextsent>to alleviate this problem, one can borrow from parsing the technique of binarizing context-free grammars (into chomsky normal form) to reduce the complexity.
</nextsent>
<nextsent>with synchronous context-free grammars (scfg), however, this problem becomes more complicated with the additional dimension of target-side permutation.
</nextsent>
<nextsent>the simplest method of binarizing an scfg is to binarize (left-to-right) on the source-side as if treating it as monolingual cfg for the sourcelangauge.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4188">
<title id=" W07-0405.xml">binarization synchronous binarization and target side binarization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>i also wish to thank jonathan graehl, giorgio satta, hao zhang, and the three anonymous reviewers for helpful comments.tee contiguous spans on the target-side, due to the arbitrary re-ordering of nonterminals between the two languages.
</prevsent>
<prevsent>as result, decoding with an integrated language model still has an exponential complexity.
</prevsent>
</prevsection>
<citsent citstr=" N06-1033 ">
synchronous binarization (zhang et al, 2006) <papid> N06-1033 </papid>solves this problem by simultaneously binarizing both source and target-sides of synchronous rule, making sure of contiguous spans on both sides whenever possible.</citsent>
<aftsection>
<nextsent>neglecting the small amount of non-binarizable rules, the decoding complexity with an integrated language model becomes polyno mial and translation quality is significantly improved thanks to the better search.
</nextsent>
<nextsent>however, this method is more sophisticated to implement than the previous method and binarizability ratio decreases on freer word-order languages (wellington et al, 2006).<papid> P06-1123 </papid>this paper presents third alternative, target side binarization, which is the symmetric version of the simple source-side variant mentioned above.</nextsent>
<nextsent>we compare it with the other two schemes in two popular instantiations of mt systems based on scfgs: the string-based systems (chiang, 2005; <papid> P05-1033 </papid>galley et al., 2006) <papid> P06-1121 </papid>where the input is string to be parsed using the source-side of the scfg; and the tree based systems (liu et al, 2006; <papid> P06-1077 </papid>huang et al, 2006) where the input is parse tree and is recursively converted into target string using the scfg as tree-transducer.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4189">
<title id=" W07-0405.xml">binarization synchronous binarization and target side binarization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>synchronous binarization (zhang et al, 2006) <papid> N06-1033 </papid>solves this problem by simultaneously binarizing both source and target-sides of synchronous rule, making sure of contiguous spans on both sides whenever possible.</prevsent>
<prevsent>neglecting the small amount of non-binarizable rules, the decoding complexity with an integrated language model becomes polyno mial and translation quality is significantly improved thanks to the better search.</prevsent>
</prevsection>
<citsent citstr=" P06-1123 ">
however, this method is more sophisticated to implement than the previous method and binarizability ratio decreases on freer word-order languages (wellington et al, 2006).<papid> P06-1123 </papid>this paper presents third alternative, target side binarization, which is the symmetric version of the simple source-side variant mentioned above.</citsent>
<aftsection>
<nextsent>we compare it with the other two schemes in two popular instantiations of mt systems based on scfgs: the string-based systems (chiang, 2005; <papid> P05-1033 </papid>galley et al., 2006) <papid> P06-1121 </papid>where the input is string to be parsed using the source-side of the scfg; and the tree based systems (liu et al, 2006; <papid> P06-1077 </papid>huang et al, 2006) where the input is parse tree and is recursively converted into target string using the scfg as tree-transducer.</nextsent>
<nextsent>while synchronous binarization is the best strategy for string-based systems, we show that target-side binarization can achieve the same performance of synchronous binarization for tree based systems, with much simpler implementation and 100% binarizability.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4194">
<title id=" W07-0405.xml">binarization synchronous binarization and target side binarization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>neglecting the small amount of non-binarizable rules, the decoding complexity with an integrated language model becomes polyno mial and translation quality is significantly improved thanks to the better search.
</prevsent>
<prevsent>however, this method is more sophisticated to implement than the previous method and binarizability ratio decreases on freer word-order languages (wellington et al, 2006).<papid> P06-1123 </papid>this paper presents third alternative, target side binarization, which is the symmetric version of the simple source-side variant mentioned above.</prevsent>
</prevsection>
<citsent citstr=" P06-1077 ">
we compare it with the other two schemes in two popular instantiations of mt systems based on scfgs: the string-based systems (chiang, 2005; <papid> P05-1033 </papid>galley et al., 2006) <papid> P06-1121 </papid>where the input is string to be parsed using the source-side of the scfg; and the tree based systems (liu et al, 2006; <papid> P06-1077 </papid>huang et al, 2006) where the input is parse tree and is recursively converted into target string using the scfg as tree-transducer.</citsent>
<aftsection>
<nextsent>while synchronous binarization is the best strategy for string-based systems, we show that target-side binarization can achieve the same performance of synchronous binarization for tree based systems, with much simpler implementation and 100% binarizability.
</nextsent>
<nextsent>binarization scheme sin this section, we define synchronous context free grammars and present the three binarization 33 np pp vp chinese ??
</nextsent>
<nextsent>en gl ish ??
</nextsent>
<nextsent>np-pp np-pp vp contiguous ga np pp-vp contiguous co tig uo us pp np p np p gap co tig uo us (a) example rule (b) source-side (c) synchronous (d) target-side figure 1: illustration of the three binarization schemes, with virtual nonterminals in gray.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4199">
<title id=" W07-0405.xml">binarization synchronous binarization and target side binarization </title>
<section> synchronous grammars and.  </section>
<citcontext>
<prevsection>
<prevsent>decoding with an scfg (e.g., translating from chinese to english using the above grammar) can be cast as parsing problem (see section 3 for details), in which case we need to binarize synchronous rule with more than two nonterminals to achieve polyno mial time algorithms (zhang et al, 2006).<papid> N06-1033 </papid></prevsent>
<prevsent>we will next present the three different binarization schemes using example 1.</prevsent>
</prevsection>
<citsent citstr=" H05-1101 ">
1an alternative notation, used by satta and peserico (2005), <papid> H05-1101 </papid>allows co-indexed nonterminals to take different symbols across languages, which is convenient in describing syntactic divergences (see figure 2).</citsent>
<aftsection>
<nextsent>2.1 source-side binarization.
</nextsent>
<nextsent>the first and simplest scheme, source-side bina riza tion, works left-to-right on the source projection of the scfg without respecting the re-orderings on the target-side.
</nextsent>
<nextsent>so it will binarize the first rule as: (2) ? np-pp vpnp-pp ? np pp which corresponds to figure 1 (b).
</nextsent>
<nextsent>notice that the virtual nonterminal np-pp representing the intermediate symbol is discontinuous with two spans on the target (english) side, because this binarizationscheme completely ignores the reorderings of nonterminals.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4200">
<title id=" W07-0405.xml">binarization synchronous binarization and target side binarization </title>
<section> synchronous grammars and.  </section>
<citcontext>
<prevsection>
<prevsent>so it will binarize the first rule as: (2) ? np-pp vpnp-pp ? np pp which corresponds to figure 1 (b).
</prevsent>
<prevsent>notice that the virtual nonterminal np-pp representing the intermediate symbol is discontinuous with two spans on the target (english) side, because this binarizationscheme completely ignores the reorderings of nonterminals.
</prevsent>
</prevsection>
<citsent citstr=" N03-1021 ">
as result, the binarized grammar, with gap on the target-side, is no longer an scfg, but can be represented in the more general formalism of multi-text grammars (mtg) (melamed, 2003): (<papid> N03-1021 </papid>3) ( s ) ???</citsent>
<aftsection>
<nextsent>[1, 2][1, 2, 1] ( np-pp vp np-pp (2) vp ) here [1, 2, 1] denotes that on that target-side, the first nonterminal np-pp has two discontinuous spans, with the second nonterminal vp in the gap.
</nextsent>
<nextsent>intuitively speaking, the gaps on the target-side will lead to exponential complexity in decoding with integrated language models (see section 3), as well as synchronous parsing (zhang et al, 2006).<papid> N06-1033 </papid></nextsent>
<nextsent>2.2 synchronous binarization.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4204">
<title id=" W07-0405.xml">binarization synchronous binarization and target side binarization </title>
<section> synchronous grammars and.  </section>
<citcontext>
<prevsection>
<prevsent>1 2 3 4 chinese ??
</prevsent>
<prevsent>en gl ish ??
</prevsent>
</prevsection>
<citsent citstr=" P05-1057 ">
figure 2: an example of non-binarizable rule from the hand-aligned chinese-english data in liu et al (2005).<papid> P05-1057 </papid></citsent>
<aftsection>
<nextsent>the scfg rule is vp ? advp 1 pp 2 vb 3 nn 4 , vp ? vb 3 jj 1 nns 4 pp 2 in the notatoin of satta and peserico (2005).<papid> H05-1101 </papid></nextsent>
<nextsent>chomsky normal form in synchronous grammars.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4219">
<title id=" W07-0405.xml">binarization synchronous binarization and target side binarization </title>
<section> theoretical analysis.  </section>
<citcontext>
<prevsection>
<prevsent>for target-side binarized grammar as in (5), however, the source-side spans are discontinuous where cky can not apply, and we have to enumerate more free indices on the source side.
</prevsent>
<prevsent>for example, the first deduction (npi,j) : (w1, t1) (vpk,l) : (w2, t2) (np-vpi,jk,l) : (w1 + w2, t1t2) (9) leaves gap in the source-side span of the resulting item, which is later filled in when the item is combined with pp (see also figure 3 (b)): (np-vpi,jk,l) : (w1, t1) (ppj,k) : (w2, t2) (si,l) : (w1 + w2, t1t2) (10)both of the above deductions have four free indices, and thus of complexity o(|w|4) instead of cubic in the length of the input string w. more generally, the complexity of binarizationscheme depends on its source arity.
</prevsent>
</prevsection>
<citsent citstr=" W05-1507 ">
in the worst case, binarized grammar with source arity of swill require at most (2s+1) free indices in deduction, because otherwise if one rule needs (2s + 2) 36 indices, then there are s+1 spans, which contradicts the definition of arity (huang et al, 2005).<papid> W05-1507 </papid>4 these deductive systems represent the search space of decoding without language model.</citsent>
<aftsection>
<nextsent>whenone is instantiated for particular input string, it defines set of derivations, called forest, represented in compact structure that has structure of hypergraph.
</nextsent>
<nextsent>accordingly we call items like (pp1,3) nodes in the forest, and an instantiated deduction like (pp-vp1,6) ?
</nextsent>
<nextsent>(pp1,3)(vp3,6)we call hyperedge that connects one or more antecedent nodes to consequent node.
</nextsent>
<nextsent>in this representation, the time complexity of lm decoding,which we refer to as source-side complexity, is proportional to the size of the forest , i.e., the number of hyperedges (instantiated deductions) in . to summarize, the source-side complexity for bina rized grammar of source arity is |f | = o(|w|2s+1).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4220">
<title id=" W07-0405.xml">binarization synchronous binarization and target side binarization </title>
<section> theoretical analysis.  </section>
<citcontext>
<prevsection>
<prevsent>(pp1,3)(vp3,6)we call hyperedge that connects one or more antecedent nodes to consequent node.
</prevsent>
<prevsent>in this representation, the time complexity of lm decoding,which we refer to as source-side complexity, is proportional to the size of the forest , i.e., the number of hyperedges (instantiated deductions) in . to summarize, the source-side complexity for bina rized grammar of source arity is |f | = o(|w|2s+1).
</prevsent>
</prevsection>
<citsent citstr=" P96-1021 ">
3.1.2 adding language model to integrate with bigram language model, we can use the dynamic-programming algorithm of wu (1996), <papid> P96-1021 </papid>which we may think of as proceeding intwo passes.</citsent>
<aftsection>
<nextsent>the first pass is as above, and the second pass traverses the first-pass forest, assigning to each node a set of augmented items, which we call +lm items, of the form (vab), where and are target words and ? is place holder symbol for anelided part of target-language string.
</nextsent>
<nextsent>this item indicates that possible translation of the part of the input spanned by is target string that starts with and ends with b.here is an example deduction in the synchronously binarized grammar (4), for +lm itemfor the node (pp-vp1,6) based on the lm deduction (6): (pp with ? sharon1,3 ): (w1, t1) (vp held ? talk3,6 ): (w2, t2) (pp-vp held ? sharon1,6 ): (w?, t2t1) (11) 4actually this is true only if in any binarization scheme, non-contiguous item is always combined with contiguousitem.
</nextsent>
<nextsent>we define both source and target binarizations to be incremental (i.e., left-to-right or right-to-left), so this assumption trivially holds.
</nextsent>
<nextsent>more general binarization schemes are possible to have even higher complexities, but also possible to achieve better complexities.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4224">
<title id=" W07-0405.xml">binarization synchronous binarization and target side binarization </title>
<section> theoretical analysis.  </section>
<citcontext>
<prevsection>
<prevsent>figure 4: illustration of tree-to-string deduction.
</prevsent>
<prevsent>string, using the scfg as tree-transducer.
</prevsent>
</prevsection>
<citsent citstr=" P03-2041 ">
in this setting, the lm decoding phase is tree-parsingproblem (eisner, 2003) <papid> P03-2041 </papid>which aims to cover the entire tree by set of rules.</citsent>
<aftsection>
<nextsent>for example, deduction of the first rule in example 1 would be: (np1) : (w1, t1) (pp2) : (w2, t2) (vp3) : (w3, t3) (s?) : (w1 + w2 + w3, t1t3t2) (13) where ? and ? ?
</nextsent>
<nextsent>i(i = 1, 2, 3) are tree addresses (shieber et al, 1995), with ? ?
</nextsent>
<nextsent>i being the ith childof ?
</nextsent>
<nextsent>(the address of the root node is ?).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4230">
<title id=" W07-0604.xml">high accuracy annotation and parsing of childes transcripts </title>
<section> parsing.  </section>
<citcontext>
<prevsection>
<prevsent>edu/data/eng-usa/brown.zip.
</prevsent>
<prevsent>it was used for the domain adaptation task at the conll-2007 dependency parsing shared task (nivre, 2007).
</prevsent>
</prevsection>
<citsent citstr=" P05-1025 ">
although the childes annotation scheme proposed by sagae et al  (2004) has been used in practice for automatic parsing of child language transcripts (sagae et al , 2004; sagae et al , 2005), <papid> P05-1025 </papid>such work relied mainly on statistical parser (charniak, 2000) <papid> A00-2018 </papid>trained on the wall street journal portion of the penn treebank, since large enough corpus of annotated childes data was not available to train domain-specific parser.</citsent>
<aftsection>
<nextsent>having corpus of 65,000 words of childes data annotated with grammatical relations represented as labeled dependencies allows us to develop parser tailored for the childes domain.
</nextsent>
<nextsent>our overall parsing approach uses best-firstprobabilistic shift-reduce algorithm, working left-to right to find labeled dependencies one at time.
</nextsent>
<nextsent>the algorithm is essentially dependency version of thedata-driven constituent parsing algorithm for probabilistic glr-like parsing described by sagae andlavie (2006).<papid> P06-2089 </papid></nextsent>
<nextsent>because childes syntactic annotations are represented as labeled dependencies, using dependency parsing approach allows us to work with that representation directly.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4231">
<title id=" W07-0604.xml">high accuracy annotation and parsing of childes transcripts </title>
<section> parsing.  </section>
<citcontext>
<prevsection>
<prevsent>edu/data/eng-usa/brown.zip.
</prevsent>
<prevsent>it was used for the domain adaptation task at the conll-2007 dependency parsing shared task (nivre, 2007).
</prevsent>
</prevsection>
<citsent citstr=" A00-2018 ">
although the childes annotation scheme proposed by sagae et al  (2004) has been used in practice for automatic parsing of child language transcripts (sagae et al , 2004; sagae et al , 2005), <papid> P05-1025 </papid>such work relied mainly on statistical parser (charniak, 2000) <papid> A00-2018 </papid>trained on the wall street journal portion of the penn treebank, since large enough corpus of annotated childes data was not available to train domain-specific parser.</citsent>
<aftsection>
<nextsent>having corpus of 65,000 words of childes data annotated with grammatical relations represented as labeled dependencies allows us to develop parser tailored for the childes domain.
</nextsent>
<nextsent>our overall parsing approach uses best-firstprobabilistic shift-reduce algorithm, working left-to right to find labeled dependencies one at time.
</nextsent>
<nextsent>the algorithm is essentially dependency version of thedata-driven constituent parsing algorithm for probabilistic glr-like parsing described by sagae andlavie (2006).<papid> P06-2089 </papid></nextsent>
<nextsent>because childes syntactic annotations are represented as labeled dependencies, using dependency parsing approach allows us to work with that representation directly.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4233">
<title id=" W07-0604.xml">high accuracy annotation and parsing of childes transcripts </title>
<section> parsing.  </section>
<citcontext>
<prevsection>
<prevsent>having corpus of 65,000 words of childes data annotated with grammatical relations represented as labeled dependencies allows us to develop parser tailored for the childes domain.
</prevsent>
<prevsent>our overall parsing approach uses best-firstprobabilistic shift-reduce algorithm, working left-to right to find labeled dependencies one at time.
</prevsent>
</prevsection>
<citsent citstr=" P06-2089 ">
the algorithm is essentially dependency version of thedata-driven constituent parsing algorithm for probabilistic glr-like parsing described by sagae andlavie (2006).<papid> P06-2089 </papid></citsent>
<aftsection>
<nextsent>because childes syntactic annotations are represented as labeled dependencies, using dependency parsing approach allows us to work with that representation directly.
</nextsent>
<nextsent>this dependency parser has been shown to have state-of-the-art accuracy in the conll shared tasks on dependency parsing (buchholz and marsi, 2006; <papid> W06-2920 </papid>nivre, 2007)3.</nextsent>
<nextsent>sagae and tsujii (2007) <papid> D07-1111 </papid>present detailed description of the parsing approach used inour work, including the parsing algorithm.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4234">
<title id=" W07-0604.xml">high accuracy annotation and parsing of childes transcripts </title>
<section> parsing.  </section>
<citcontext>
<prevsection>
<prevsent>the algorithm is essentially dependency version of thedata-driven constituent parsing algorithm for probabilistic glr-like parsing described by sagae andlavie (2006).<papid> P06-2089 </papid></prevsent>
<prevsent>because childes syntactic annotations are represented as labeled dependencies, using dependency parsing approach allows us to work with that representation directly.</prevsent>
</prevsection>
<citsent citstr=" W06-2920 ">
this dependency parser has been shown to have state-of-the-art accuracy in the conll shared tasks on dependency parsing (buchholz and marsi, 2006; <papid> W06-2920 </papid>nivre, 2007)3.</citsent>
<aftsection>
<nextsent>sagae and tsujii (2007) <papid> D07-1111 </papid>present detailed description of the parsing approach used inour work, including the parsing algorithm.</nextsent>
<nextsent>in summary, the parser uses an algorithm similar to the lr parsing algorithm (knuth, 1965), keeping stack of partially built syntactic structures, and queue of remaining input tokens.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4235">
<title id=" W07-0604.xml">high accuracy annotation and parsing of childes transcripts </title>
<section> parsing.  </section>
<citcontext>
<prevsection>
<prevsent>because childes syntactic annotations are represented as labeled dependencies, using dependency parsing approach allows us to work with that representation directly.
</prevsent>
<prevsent>this dependency parser has been shown to have state-of-the-art accuracy in the conll shared tasks on dependency parsing (buchholz and marsi, 2006; <papid> W06-2920 </papid>nivre, 2007)3.</prevsent>
</prevsection>
<citsent citstr=" D07-1111 ">
sagae and tsujii (2007) <papid> D07-1111 </papid>present detailed description of the parsing approach used inour work, including the parsing algorithm.</citsent>
<aftsection>
<nextsent>in summary, the parser uses an algorithm similar to the lr parsing algorithm (knuth, 1965), keeping stack of partially built syntactic structures, and queue of remaining input tokens.
</nextsent>
<nextsent>at each step in the parsing process, the parser can apply shift action (re move token from the front of the queue and place it on top of the stack), or reduce action (pop thetwo topmost stack items, and push new item composed of the two popped items combined in single structure).
</nextsent>
<nextsent>this parsing approach is very similar to the one used successfully by nivre et al  (2006), <papid> W06-2933 </papid>but we use maximum entropy classifier (berger et al ., 1996) <papid> J96-1002 </papid>to determine parser actions, which makes parsing extremely fast.</nextsent>
<nextsent>in addition, our parsing approach performs search over the space of possible parser actions, while nivre et al approach is de terministic.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4237">
<title id=" W07-0604.xml">high accuracy annotation and parsing of childes transcripts </title>
<section> parsing.  </section>
<citcontext>
<prevsection>
<prevsent>in summary, the parser uses an algorithm similar to the lr parsing algorithm (knuth, 1965), keeping stack of partially built syntactic structures, and queue of remaining input tokens.
</prevsent>
<prevsent>at each step in the parsing process, the parser can apply shift action (re move token from the front of the queue and place it on top of the stack), or reduce action (pop thetwo topmost stack items, and push new item composed of the two popped items combined in single structure).
</prevsent>
</prevsection>
<citsent citstr=" W06-2933 ">
this parsing approach is very similar to the one used successfully by nivre et al  (2006), <papid> W06-2933 </papid>but we use maximum entropy classifier (berger et al ., 1996) <papid> J96-1002 </papid>to determine parser actions, which makes parsing extremely fast.</citsent>
<aftsection>
<nextsent>in addition, our parsing approach performs search over the space of possible parser actions, while nivre et al approach is deterministic.
</nextsent>
<nextsent>see sagae and tsujii (2007) <papid> D07-1111 </papid>for more information on the parser.</nextsent>
<nextsent>features used in classification to determine whether the parser takes shift or reduce action at any point during parsing are derived from the parsers current configuration (contents of the stack and queue) at that point.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4238">
<title id=" W07-0604.xml">high accuracy annotation and parsing of childes transcripts </title>
<section> parsing.  </section>
<citcontext>
<prevsection>
<prevsent>in summary, the parser uses an algorithm similar to the lr parsing algorithm (knuth, 1965), keeping stack of partially built syntactic structures, and queue of remaining input tokens.
</prevsent>
<prevsent>at each step in the parsing process, the parser can apply shift action (re move token from the front of the queue and place it on top of the stack), or reduce action (pop thetwo topmost stack items, and push new item composed of the two popped items combined in single structure).
</prevsent>
</prevsection>
<citsent citstr=" J96-1002 ">
this parsing approach is very similar to the one used successfully by nivre et al  (2006), <papid> W06-2933 </papid>but we use maximum entropy classifier (berger et al ., 1996) <papid> J96-1002 </papid>to determine parser actions, which makes parsing extremely fast.</citsent>
<aftsection>
<nextsent>in addition, our parsing approach performs search over the space of possible parser actions, while nivre et al approach is deterministic.
</nextsent>
<nextsent>see sagae and tsujii (2007) <papid> D07-1111 </papid>for more information on the parser.</nextsent>
<nextsent>features used in classification to determine whether the parser takes shift or reduce action at any point during parsing are derived from the parsers current configuration (contents of the stack and queue) at that point.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4248">
<title id=" W07-0410.xml">a walk on the other side using smt components in a transfer based translation system </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>should all the insight into the structure of languages be neglected?
</prevsent>
<prevsent>this might be too drastic reaction.
</prevsent>
</prevsection>
<citsent citstr=" J97-3002 ">
actually, now that smt has reached some maturity, we see several attempts to integrate more structure into these systems, ranging from simple hierarchical alignment models (wu 1997, <papid> J97-3002 </papid>chiang 2005) <papid> P05-1033 </papid>to syntax-based statistical systems (yamada and knight 2001, <papid> P01-1067 </papid>zollmann and venugopal 2006).<papid> W06-3119 </papid></citsent>
<aftsection>
<nextsent>what can traditional rule-based translation systems learn from these approaches?
</nextsent>
<nextsent>and would it not make sense to work from both sides towards that common goal: structurally rich statistical translation models.
</nextsent>
<nextsent>in this paper we study some enhancements for transfer-based translation system, using techniques and even components developed for statistical machine translation.
</nextsent>
<nextsent>while the core engine remains virtually untouched, additional features are added to re-score the n-best list generated by the transfer engine.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4249">
<title id=" W07-0410.xml">a walk on the other side using smt components in a transfer based translation system </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>should all the insight into the structure of languages be neglected?
</prevsent>
<prevsent>this might be too drastic reaction.
</prevsent>
</prevsection>
<citsent citstr=" P05-1033 ">
actually, now that smt has reached some maturity, we see several attempts to integrate more structure into these systems, ranging from simple hierarchical alignment models (wu 1997, <papid> J97-3002 </papid>chiang 2005) <papid> P05-1033 </papid>to syntax-based statistical systems (yamada and knight 2001, <papid> P01-1067 </papid>zollmann and venugopal 2006).<papid> W06-3119 </papid></citsent>
<aftsection>
<nextsent>what can traditional rule-based translation systems learn from these approaches?
</nextsent>
<nextsent>and would it not make sense to work from both sides towards that common goal: structurally rich statistical translation models.
</nextsent>
<nextsent>in this paper we study some enhancements for transfer-based translation system, using techniques and even components developed for statistical machine translation.
</nextsent>
<nextsent>while the core engine remains virtually untouched, additional features are added to re-score the n-best list generated by the transfer engine.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4250">
<title id=" W07-0410.xml">a walk on the other side using smt components in a transfer based translation system </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>should all the insight into the structure of languages be neglected?
</prevsent>
<prevsent>this might be too drastic reaction.
</prevsent>
</prevsection>
<citsent citstr=" P01-1067 ">
actually, now that smt has reached some maturity, we see several attempts to integrate more structure into these systems, ranging from simple hierarchical alignment models (wu 1997, <papid> J97-3002 </papid>chiang 2005) <papid> P05-1033 </papid>to syntax-based statistical systems (yamada and knight 2001, <papid> P01-1067 </papid>zollmann and venugopal 2006).<papid> W06-3119 </papid></citsent>
<aftsection>
<nextsent>what can traditional rule-based translation systems learn from these approaches?
</nextsent>
<nextsent>and would it not make sense to work from both sides towards that common goal: structurally rich statistical translation models.
</nextsent>
<nextsent>in this paper we study some enhancements for transfer-based translation system, using techniques and even components developed for statistical machine translation.
</nextsent>
<nextsent>while the core engine remains virtually untouched, additional features are added to re-score the n-best list generated by the transfer engine.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4251">
<title id=" W07-0410.xml">a walk on the other side using smt components in a transfer based translation system </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>should all the insight into the structure of languages be neglected?
</prevsent>
<prevsent>this might be too drastic reaction.
</prevsent>
</prevsection>
<citsent citstr=" W06-3119 ">
actually, now that smt has reached some maturity, we see several attempts to integrate more structure into these systems, ranging from simple hierarchical alignment models (wu 1997, <papid> J97-3002 </papid>chiang 2005) <papid> P05-1033 </papid>to syntax-based statistical systems (yamada and knight 2001, <papid> P01-1067 </papid>zollmann and venugopal 2006).<papid> W06-3119 </papid></citsent>
<aftsection>
<nextsent>what can traditional rule-based translation systems learn from these approaches?
</nextsent>
<nextsent>and would it not make sense to work from both sides towards that common goal: structurally rich statistical translation models.
</nextsent>
<nextsent>in this paper we study some enhancements for transfer-based translation system, using techniques and even components developed for statistical machine translation.
</nextsent>
<nextsent>while the core engine remains virtually untouched, additional features are added to re-score the n-best list generated by the transfer engine.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4252">
<title id=" W07-0410.xml">a walk on the other side using smt components in a transfer based translation system </title>
<section> building xfer system for new do-.  </section>
<citcontext>
<prevsection>
<prevsent>table 2 shows that, in this crude setting, different automatic metrics do not agree on the translation accuracy of both systems.
</prevsent>
<prevsent>on one hand, meteor (lavie et al  2004), which has been shown to correlate well with human judgments (snover et al  2006), indicates that the refined system outperforms the baseline system (as measured by the latest version v0.5.1,).
</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
on the other hand, both bleu (papineni et al , 2002) <papid> P02-1040 </papid>and nist (doddington 2002) scores are higher for the base line system (mteval-v11b.pl).</citsent>
<aftsection>
<nextsent>however, human inspection revealed that there fined grammar is able to augment the n-best list with correct translations that the baseline system was not able to generate.
</nextsent>
<nextsent>this suggests that these results reflect poor re-ranking and not n-best list quality.
</nextsent>
<nextsent>in the next section, we describe an oracle experiment to measure n-best list quality of both systems.
</nextsent>
<nextsent>3.4 oracle experiment.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4253">
<title id=" W07-0410.xml">a walk on the other side using smt components in a transfer based translation system </title>
<section> mer training.  </section>
<citcontext>
<prevsection>
<prevsent>when adding more features to the translation system, careful balancing of the individual contributions can make significant difference.
</prevsent>
<prevsent>how ever, with each feature added, manually tuning the system becomes less and less practical, and automatic optimization becomes necessary.
</prevsent>
</prevsection>
<citsent citstr=" P03-1021 ">
different optimization techniques are available, like the simplex algorithm or the special minimum error training as described in (och 2003).<papid> P03-1021 </papid></citsent>
<aftsection>
<nextsent>in minimum error rate (mer) training, the n-best list generated by the translation system is used to find feature weight, thereby re-ranking the n-best list.
</nextsent>
<nextsent>this improves the match between the 1-best translation and given reference translations.
</nextsent>
<nextsent>optimization can use any metric as objective function.
</nextsent>
<nextsent>typically, systems are tuned towards high bleu or high nist scores, more recently also towards meteor or ter (snover et al  2006).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4254">
<title id=" W06-3506.xml">catching metaphors </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>to answer such commonplace questions aswhich train should take to get to the airport?
</prevsent>
<prevsent>requires justifications, predictions and recommendations that can only be produced through inference.
</prevsent>
</prevsection>
<citsent citstr=" C04-1100 ">
one such question answering system (narayananand harabagiu, 2004) <papid> C04-1100 </papid>takes propbank/framenet annotations as input, uses the propbank targets to indicate which actions are being described with which arguments and produces an answer using probabilistic models of actions as the tools of inference.</citsent>
<aftsection>
<nextsent>initiating these action models is called simulation.such action models provide deep inferential capabilities for embodied domains.
</nextsent>
<nextsent>they can also, when provided with appropriate metaphoric mappings, be extended to cover metaphoric language (narayanan, 1997).
</nextsent>
<nextsent>exploiting the inferential capabilities of such action models over the broadest domain requires asystem to determine whether verb is being used literally or metaphorically.
</nextsent>
<nextsent>such system could then activate the necessary metaphoric mappings and initiate the appropriate simulation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4255">
<title id=" W06-3506.xml">catching metaphors </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>verbs like treat (0 metaphoric to 20 literal) and lead (345 metaphoric to 0 literal) are in this category.
</prevsent>
<prevsent>the two remaining errors are cases where the verb was not present in the training data.
</prevsent>
</prevsection>
<citsent citstr=" J91-1003 ">
previous work on automated metaphor detection includes fass (1991), <papid> J91-1003 </papid>martin (1990), and mason (2004).<papid> J04-1002 </papid></citsent>
<aftsection>
<nextsent>whereas our aim is to classify unseen sentences as literal or metaphorical, these projects address the related but distinct task of identifying metaphorical mappings.
</nextsent>
<nextsent>all three use the selectional preferences of verbs to identify metaphors.
</nextsent>
<nextsent>in literal usage, the arguments that fill particular roles ofa verb are frequently of common type.
</nextsent>
<nextsent>for instance, in the medical domain, the object of the 46 frame l total %tot %obl %vbl cause motion 78/78 1/10 79/88 89.77 88.64 88.64 cotheme 179/179 0/2 179/181 98.90 98.90 98.90 cure 26/30 3/3 29/33 87.88 90.91 90.91 motion directional 242/242 0/2 242/244 99.18 99.18 99.18 placing 176/181 13/25 189/206 91.75 87.86 91.26 self motion 87/90 14/19 101/109 92.66 82.57 91.74 all frames 788/800 31/61 819/861 95.12 92.92 94.89 figure 8: the results of the classifier on the test set, using feature set 6.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4256">
<title id=" W06-3506.xml">catching metaphors </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>verbs like treat (0 metaphoric to 20 literal) and lead (345 metaphoric to 0 literal) are in this category.
</prevsent>
<prevsent>the two remaining errors are cases where the verb was not present in the training data.
</prevsent>
</prevsection>
<citsent citstr=" J04-1002 ">
previous work on automated metaphor detection includes fass (1991), <papid> J91-1003 </papid>martin (1990), and mason (2004).<papid> J04-1002 </papid></citsent>
<aftsection>
<nextsent>whereas our aim is to classify unseen sentences as literal or metaphorical, these projects address the related but distinct task of identifying metaphorical mappings.
</nextsent>
<nextsent>all three use the selectional preferences of verbs to identify metaphors.
</nextsent>
<nextsent>in literal usage, the arguments that fill particular roles ofa verb are frequently of common type.
</nextsent>
<nextsent>for instance, in the medical domain, the object of the 46 frame l total %tot %obl %vbl cause motion 78/78 1/10 79/88 89.77 88.64 88.64 cotheme 179/179 0/2 179/181 98.90 98.90 98.90 cure 26/30 3/3 29/33 87.88 90.91 90.91 motion directional 242/242 0/2 242/244 99.18 99.18 99.18 placing 176/181 13/25 189/206 91.75 87.86 91.26 self motion 87/90 14/19 101/109 92.66 82.57 91.74 all frames 788/800 31/61 819/861 95.12 92.92 94.89 figure 8: the results of the classifier on the test set, using feature set 6.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4257">
<title id=" W06-3317.xml">using dependency parsing and probabilistic inference to extract relationships between genes proteins and malignancies implicit among multiple biomedical research abstracts </title>
<section> system overview.  </section>
<citcontext>
<prevsection>
<prevsent>each component, excluding the semantic map per and probabilistic reasoner, is realized as uima (gtz and suhre, 2004) annotator, with information being accumulated in each document as each phase occurs.2the gene/protein and malignancy taggers collectively constitute our entity extraction?
</prevsent>
<prevsent>subsystem.
</prevsent>
</prevsection>
<citsent citstr=" P05-1061 ">
our entity extraction subsystem and the tokenizer were adapted from pennbiotagger (mcdonald et al, 2005; <papid> P05-1061 </papid>jin et al 2005; lerman et al 2006).</citsent>
<aftsection>
<nextsent>the tokenizer uses maximum entropy model trained upon biomedical texts, mostly in the oncology do main.
</nextsent>
<nextsent>both the protein and malignancy taggers were built using conditional random fields.
</nextsent>
<nextsent>the nominal ization tagger detects nominaliza tions that represent possible relationships that would otherwise go unnoticed.
</nextsent>
<nextsent>for instance, in the sentence excerpt ??
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4258">
<title id=" W06-3812.xml">chinese whispers  an efficient graph clustering algorithm and its application to natural language processing problems </title>
<section> nlp experiments.  </section>
<citcontext>
<prevsection>
<prevsent>first, we define the notion of cooccurrence graphs, which are used in sections 4.1 and 4.3: two words co-occur if they can both be found in certain unit of text, here sentence.
</prevsent>
<prevsent>employing significance measure, we determine whether their co-occurrences are significant or random.
</prevsent>
</prevsection>
<citsent citstr=" J93-1003 ">
in this case, we use the log-likelihood measure as described in (dunning 1993).<papid> J93-1003 </papid></citsent>
<aftsection>
<nextsent>we use the words as nodes in the graph.
</nextsent>
<nextsent>the weight of an 1 defined for two random variables and as (h(x)+h(y) h(x,y))/max(h(x),h(y)) with h(x) entropy.
</nextsent>
<nextsent>a value of 0 denotes indepenence, 1 is perfect congruence.
</nextsent>
<nextsent>77 edge between two words is set to the significance value of their co-occurrence, if it exceeds certain threshold.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4259">
<title id=" W06-3812.xml">chinese whispers  an efficient graph clustering algorithm and its application to natural language processing problems </title>
<section> nlp experiments.  </section>
<citcontext>
<prevsection>
<prevsent>in total, cw produced 282 clusters, of which 26 exceed size of 100.
</prevsent>
<prevsent>the weighted average of cluster purity (i.e. the number of predominant tags divided by cluster size) was measured at 88.8%, which exceeds significantly the precision of 53% on word type as reported by schtze (1995) on related task.
</prevsent>
</prevsection>
<citsent citstr=" W96-0103 ">
how to use this kind of word clusters to improve the accuracy of pos-taggers is outlined in (ushioda, 1996).<papid> W96-0103 </papid></citsent>
<aftsection>
<nextsent>4.3 word sense induction.
</nextsent>
<nextsent>the task of word sense induction (wsi) is to find the different senses of word.
</nextsent>
<nextsent>the number of senses is not known in advance, therefore has to be determined by the method.
</nextsent>
<nextsent>similar to the approach as presented in (dorow and widdows, 2003) <papid> E03-1020 </papid>we construct word graph.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4260">
<title id=" W06-3812.xml">chinese whispers  an efficient graph clustering algorithm and its application to natural language processing problems </title>
<section> nlp experiments.  </section>
<citcontext>
<prevsection>
<prevsent>the task of word sense induction (wsi) is to find the different senses of word.
</prevsent>
<prevsent>the number of senses is not known in advance, therefore has to be determined by the method.
</prevsent>
</prevsection>
<citsent citstr=" E03-1020 ">
similar to the approach as presented in (dorow and widdows, 2003) <papid> E03-1020 </papid>we construct word graph.</citsent>
<aftsection>
<nextsent>while there, edges between words are drawn iff words co-occur in enumerations, we use the cooccurrence graph.
</nextsent>
<nextsent>dorow and widdows construct graph for target word by taking the sub-graph induced by the neighborhood of (without w) and clustering it with mcl.
</nextsent>
<nextsent>we replace mcl by cw.
</nextsent>
<nextsent>the clusters are interpreted as representations of word senses.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4261">
<title id=" W06-3812.xml">chinese whispers  an efficient graph clustering algorithm and its application to natural language processing problems </title>
<section> nlp experiments.  </section>
<citcontext>
<prevsection>
<prevsent>we replace mcl by cw.
</prevsent>
<prevsent>the clusters are interpreted as representations of word senses.
</prevsent>
</prevsection>
<citsent citstr=" E06-1018 ">
to judge results, the methodology of (bordag, 2006) <papid> E06-1018 </papid>is adopted: to evaluate word sense induction, two sub-graphs induced by the neighborhood of different words are merged.</citsent>
<aftsection>
<nextsent>the algorithm ability to separate the merged graph into its previous parts can be measured in an unsupervised way.
</nextsent>
<nextsent>bordag defines four measures: ? retrieval precision (rp): similarity of the found sense with the gold standard sense ? retrieval recall (rr): amount of words that have been correctly assigned to the gold standard sense ? precision (p): fraction of correctly found disambiguations ? recall (r): fraction of correctly found senses we used the same program to compute cooccurrences on the same corpus (the bnc).
</nextsent>
<nextsent>therefore it is possible to directly compare our results to bordags, who uses triplet-based hierarchical graph clustering approach.
</nextsent>
<nextsent>the method was chosen because of its appropriateness for un labelled data: without linguistic preprocessing like tagging or parsing, only the disambiguation mechanism is measured and not the quality of the preprocessing steps.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4270">
<title id=" W06-3106.xml">phrase based smt with shallow tree phrases </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we show that thephrase-based translation engine we implemented benefits from tree-phrases.
</prevsent>
<prevsent>phrase-based machine translation is now popularparadigm.
</prevsent>
</prevsection>
<citsent citstr=" N03-1017 ">
it has the advantage of naturally capturing local reorderings and is shown to outperform word-based machine translation (koehn et al,2003).<papid> N03-1017 </papid></citsent>
<aftsection>
<nextsent>the underlying unit (a pair of phrases), however, does not handle well languages with very different word orders and fails to derive generalizations from the training corpus.
</nextsent>
<nextsent>several alternatives have been recently proposed to tackle some of these weaknesses.
</nextsent>
<nextsent>(matusov etal., 2005) propose to reorder the source text in order to mimic the target word order, and then let phrase-based model do what it is good at.
</nextsent>
<nextsent>(simard et al, 2005) <papid> H05-1095 </papid>detail an approach where the standard phrases are extended to account for gaps?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4271">
<title id=" W06-3106.xml">phrase based smt with shallow tree phrases </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>several alternatives have been recently proposed to tackle some of these weaknesses.
</prevsent>
<prevsent>(matusov etal., 2005) propose to reorder the source text in order to mimic the target word order, and then let phrase-based model do what it is good at.
</prevsent>
</prevsection>
<citsent citstr=" H05-1095 ">
(simard et al, 2005) <papid> H05-1095 </papid>detail an approach where the standard phrases are extended to account for gaps?</citsent>
<aftsection>
<nextsent>either onthe target or source side.
</nextsent>
<nextsent>they show that this representation has the potential to better exploit the training corpus and to nicely handle differences such asnegations in french and english that are poorly handled by standard phrase-based models.others are considering translation as synchronous parsing process e.g.
</nextsent>
<nextsent>(melamed, 2004; <papid> P04-1083 </papid>ding and palmer, 2005)) <papid> P05-1067 </papid>and several algorithm shave been proposed to learn the underlying production rule probabilities (graehl and knight, 2004; <papid> N04-1014 </papid>ding and palmer, 2004).</nextsent>
<nextsent>(chiang, 2005) <papid> P05-1033 </papid>proposes an heuristic way of acquiring context free transfer rules that significantly improves upon standard phrase-based model.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4273">
<title id=" W06-3106.xml">phrase based smt with shallow tree phrases </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>either onthe target or source side.
</prevsent>
<prevsent>they show that this representation has the potential to better exploit the training corpus and to nicely handle differences such asnegations in french and english that are poorly handled by standard phrase-based models.others are considering translation as synchronous parsing process e.g.
</prevsent>
</prevsection>
<citsent citstr=" P04-1083 ">
(melamed, 2004; <papid> P04-1083 </papid>ding and palmer, 2005)) <papid> P05-1067 </papid>and several algorithm shave been proposed to learn the underlying production rule probabilities (graehl and knight, 2004; <papid> N04-1014 </papid>ding and palmer, 2004).</citsent>
<aftsection>
<nextsent>(chiang, 2005) <papid> P05-1033 </papid>proposes an heuristic way of acquiring context free transfer rules that significantly improves upon standard phrase-based model.</nextsent>
<nextsent>as mentioned in (ding and palmer, 2005), <papid> P05-1067 </papid>most of these approaches require some assumptions on the level of isomorphism (lexical and/or structural) between two languages.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4274">
<title id=" W06-3106.xml">phrase based smt with shallow tree phrases </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>either onthe target or source side.
</prevsent>
<prevsent>they show that this representation has the potential to better exploit the training corpus and to nicely handle differences such asnegations in french and english that are poorly handled by standard phrase-based models.others are considering translation as synchronous parsing process e.g.
</prevsent>
</prevsection>
<citsent citstr=" P05-1067 ">
(melamed, 2004; <papid> P04-1083 </papid>ding and palmer, 2005)) <papid> P05-1067 </papid>and several algorithm shave been proposed to learn the underlying production rule probabilities (graehl and knight, 2004; <papid> N04-1014 </papid>ding and palmer, 2004).</citsent>
<aftsection>
<nextsent>(chiang, 2005) <papid> P05-1033 </papid>proposes an heuristic way of acquiring context free transfer rules that significantly improves upon standard phrase-based model.</nextsent>
<nextsent>as mentioned in (ding and palmer, 2005), <papid> P05-1067 </papid>most of these approaches require some assumptions on the level of isomorphism (lexical and/or structural) between two languages.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4275">
<title id=" W06-3106.xml">phrase based smt with shallow tree phrases </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>either onthe target or source side.
</prevsent>
<prevsent>they show that this representation has the potential to better exploit the training corpus and to nicely handle differences such asnegations in french and english that are poorly handled by standard phrase-based models.others are considering translation as synchronous parsing process e.g.
</prevsent>
</prevsection>
<citsent citstr=" N04-1014 ">
(melamed, 2004; <papid> P04-1083 </papid>ding and palmer, 2005)) <papid> P05-1067 </papid>and several algorithm shave been proposed to learn the underlying production rule probabilities (graehl and knight, 2004; <papid> N04-1014 </papid>ding and palmer, 2004).</citsent>
<aftsection>
<nextsent>(chiang, 2005) <papid> P05-1033 </papid>proposes an heuristic way of acquiring context free transfer rules that significantly improves upon standard phrase-based model.</nextsent>
<nextsent>as mentioned in (ding and palmer, 2005), <papid> P05-1067 </papid>most of these approaches require some assumptions on the level of isomorphism (lexical and/or structural) between two languages.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4276">
<title id=" W06-3106.xml">phrase based smt with shallow tree phrases </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>they show that this representation has the potential to better exploit the training corpus and to nicely handle differences such asnegations in french and english that are poorly handled by standard phrase-based models.others are considering translation as synchronous parsing process e.g.
</prevsent>
<prevsent>(melamed, 2004; <papid> P04-1083 </papid>ding and palmer, 2005)) <papid> P05-1067 </papid>and several algorithm shave been proposed to learn the underlying production rule probabilities (graehl and knight, 2004; <papid> N04-1014 </papid>ding and palmer, 2004).</prevsent>
</prevsection>
<citsent citstr=" P05-1033 ">
(chiang, 2005) <papid> P05-1033 </papid>proposes an heuristic way of acquiring context free transfer rules that significantly improves upon standard phrase-based model.</citsent>
<aftsection>
<nextsent>as mentioned in (ding and palmer, 2005), <papid> P05-1067 </papid>most of these approaches require some assumptions on the level of isomorphism (lexical and/or structural) between two languages.</nextsent>
<nextsent>in this work, we considera simple kind of unit: tree-phrase (tp), combination of fully lexicalized treelet (tl) and an elastic phrase (ep), the tokens of which may be innon-contiguous positions.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4278">
<title id=" W06-3106.xml">phrase based smt with shallow tree phrases </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this work, we considera simple kind of unit: tree-phrase (tp), combination of fully lexicalized treelet (tl) and an elastic phrase (ep), the tokens of which may be innon-contiguous positions.
</prevsent>
<prevsent>tps capture some syntactic information between two languages and can easily be merged with standard phrase-based engines.
</prevsent>
</prevsection>
<citsent citstr=" P05-1034 ">
a tp can be seen as simplification of the treeletpairs manipulated in (quirk et al, 2005).<papid> P05-1034 </papid></citsent>
<aftsection>
<nextsent>in particular, we do not address the issue of projecting sourcetreelet into target one, but take the bet that collecting (without structure) the target words associated with the words encoded in the nodes of treelet will suffice to allow translation.
</nextsent>
<nextsent>this set of target words is what we call an elastic phrase.we show that these units lead to (modest) improvements in translation quality as measured by automatic metrics.
</nextsent>
<nextsent>we conducted all our experiments 39on an in-house version of the french-english canadian hansards.
</nextsent>
<nextsent>this paper is organized as follows.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4282">
<title id=" W06-3106.xml">phrase based smt with shallow tree phrases </title>
<section> the translation engine.  </section>
<citcontext>
<prevsection>
<prevsent>we built translation engine very similar to the statistical phrase-based engine pharaoh described in (koehn, 2004) that we extended to use tree-phrases.
</prevsent>
<prevsent>not only does our decoder differ from pharaoh by using tps, it also uses direct translation models.
</prevsent>
</prevsection>
<citsent citstr=" P02-1038 ">
we know from (och and ney, 2002) <papid> P02-1038 </papid>that not using the noisy-channel approach does not impact the quality of the translation produced.</citsent>
<aftsection>
<nextsent>3.1 the maximization setting.
</nextsent>
<nextsent>for source sentence , our engine incrementally generates set of translation hypotheses by combining tree-phrase (tp) units and phrase-phrase (pp) units.2 we define hypothesis in this set as = {ui ?
</nextsent>
<nextsent>(fi, ei)}i?[1,u], set of pairs of source (fi) and target sequences (ei) of ni and mi words respectively: fi ? {fjin : i ? [1, |f |]}n?[1,ni] ei ? {elim : i ? [1, |e|]}m?[1,mi] under the constraints that for all ? [1, u], jin  jin+1 ,n ? [1, ni[ for source treelet (similar constraints apply on the target side), and jin+1 = i + 1 ,n ? [1, ni[ for source phrase.
</nextsent>
<nextsent>the way the hypotheses are built imposes additional constraints between units that will be described in section 3.3.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4287">
<title id=" W06-3106.xml">phrase based smt with shallow tree phrases </title>
<section> the translation engine.  </section>
<citcontext>
<prevsection>
<prevsent>we define the source and target projection of hypothesis by the proj operator which collects in order the words of hypothesis along one language: projf (h) = { fp : ? i=1{j n}n?[1,ni] } proje(h) = { ep : ? i=1{l m}m?[1,mi] } if we denote by hf the set of hypotheses that have as source projection (that is, hf = {h : projf (h) ? f}), then our translation engine seeks e?
</prevsent>
<prevsent>= proje(h?) where: h?
</prevsent>
</prevsection>
<citsent citstr=" N04-1033 ">
= argmax hhf s(h)the function we seek to maximize s(h) is loglinear combination of 9 components, and might be better understood as the numerator of maximum entropy model popular in several statistical mt systems (och and ney, 2002; <papid> P02-1038 </papid>bertoldi et al, 2004; zens and ney, 2004; <papid> N04-1033 </papid>simard et al, 2005; <papid> H05-1095 </papid>quirk et al, 2005).<papid> P05-1034 </papid></citsent>
<aftsection>
<nextsent>the components are the so-called feature functions (described below) and the weighting coefficients (?)
</nextsent>
<nextsent>are the parameters of the model: s(h) = pprf log ppprf (h) + p|h|+ tprf log ptprf (h) + t|h|+ ppibm log pppibm(h)+ tpibm log ptpibm(h)+ lm log plm(proje(h))+ d(h) + w|proje(h)| 3.2 the components of the scoring function.
</nextsent>
<nextsent>we briefly enumerate the features used in this study.translation models even if tree-phrase is generalization of standard phrase-phrase unit, for investigation purposes, we differentiate in our mt system between two kinds of models: tp-based model ptp and phrase-phrase model ppp.
</nextsent>
<nextsent>both relyon conditional distributions whose parameters are learned over corpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4292">
<title id=" W06-3106.xml">phrase based smt with shallow tree phrases </title>
<section> experimental setting.  </section>
<citcontext>
<prevsection>
<prevsent>the test corpus is subdivided in 16 (disjoints) slices of 500 sentences each thatwe translated separately.
</prevsent>
<prevsent>the vocabulary is atypically large since some tokens are being merged by syntex, such as etaient#financees (were financed in english).
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
the training corpus has been aligned at the word level by two viterbi word-alignments (french2english and english2french) that we combined in heuristic way similar to the refined method described in (och and ney, 2003).<papid> J03-1002 </papid></citsent>
<aftsection>
<nextsent>the parameters of the word models (ibm model 2) were trained with the giza++ package (och and ney, 2000).<papid> P00-1056 </papid></nextsent>
<nextsent>train dev test sentences 1 699 592 500 8000 e-toks 27 717 389 8 160 130 192 f-toks 30 425 066 8 946 143 089 e-toks/sent 16.3 (?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4293">
<title id=" W06-3106.xml">phrase based smt with shallow tree phrases </title>
<section> experimental setting.  </section>
<citcontext>
<prevsection>
<prevsent>the vocabulary is atypically large since some tokens are being merged by syntex, such as etaient#financees (were financed in english).
</prevsent>
<prevsent>the training corpus has been aligned at the word level by two viterbi word-alignments (french2english and english2french) that we combined in heuristic way similar to the refined method described in (och and ney, 2003).<papid> J03-1002 </papid></prevsent>
</prevsection>
<citsent citstr=" P00-1056 ">
the parameters of the word models (ibm model 2) were trained with the giza++ package (och and ney, 2000).<papid> P00-1056 </papid></citsent>
<aftsection>
<nextsent>train dev test sentences 1 699 592 500 8000 e-toks 27 717 389 8 160 130 192 f-toks 30 425 066 8 946 143 089 e-toks/sent 16.3 (?
</nextsent>
<nextsent>9.0) 16.3 (?
</nextsent>
<nextsent>9.1) 16.3 (?
</nextsent>
<nextsent>9.0) f-toks/sent 17.9 (?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4295">
<title id=" W06-3106.xml">phrase based smt with shallow tree phrases </title>
<section> experimental setting.  </section>
<citcontext>
<prevsection>
<prevsent>we compared the performances of two versions ofour engine: one which employs tps ans pps (tp engine hereafter), and one which only uses pps(pp-engine).
</prevsent>
<prevsent>we translated the 16 disjoint sub corpora of the test corpus with and without tps.we measure the quality of the translation produced with three automatic metrics.
</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
two error rates: the sentence error rate (ser) and the word error rate (wer) that we seek to minimize, and bleu (papineni et al, 2002), <papid> P02-1040 </papid>that we seek to maximize.</citsent>
<aftsection>
<nextsent>this last metric was computed with the multi-bleu.perl script available at www.
</nextsent>
<nextsent>statmt.org/wmt06/shared-task/.we separately tuned both systems on the dev corpus by applying brute force strategy, i.e. by sampling uniformly the range of each parameter (?)
</nextsent>
<nextsent>and picking the configuration which led to the best bleuscore.
</nextsent>
<nextsent>this strategy is inelegant, but in early experiments we conducted, we found better configurations this way than by applying the simplex method with multiple starting points.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4299">
<title id=" W06-1630.xml">unsupervised named entity transliteration using temporal and phonetic correlation </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>named entity transliteration is the problem of producing, for name in source language, setof one or more transliteration candidates in target language.
</prevsent>
<prevsent>previous work ? e.g.
</prevsent>
</prevsection>
<citsent citstr=" J98-4003 ">
(knight and graehl, 1998; <papid> J98-4003 </papid>meng et al, 2001; al-onaizan and knight, 2002; gao et al, 2004) ? has mostly assumed that one has training lexicon of transliteration pairs, from which one can learn model, often source-channel or maxent-based model.comparable corpora have been studied extensively in the literature ? e.g.,(fung, 1995; <papid> P95-1032 </papid>rapp, 1995; <papid> P95-1050 </papid>tanaka and iwasaki, 1996; <papid> C96-2098 </papid>franz et al, 1998; ballesteros and croft, 1998; masuichi et al., 2000; sadat et al, 2004), but transliteration in the context of comparable corpora has not been well addressed.</citsent>
<aftsection>
<nextsent>the general idea of exploiting time correlations to acquire word translations from comparable corpora has been explored in several previous studies ? e.g., (fung, 1995; <papid> P95-1032 </papid>rapp, 1995; <papid> P95-1050 </papid>tanaka and iwasaki, 1996).<papid> C96-2098 </papid></nextsent>
<nextsent>recently, pearson correlation method was proposed to mine word pairs from comparable corpora (tao and zhai, 2005); this idea is similar to the method used in(kay and roscheisen, 1993) <papid> J93-1006 </papid>for sentence align ment.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4300">
<title id=" W06-1630.xml">unsupervised named entity transliteration using temporal and phonetic correlation </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>named entity transliteration is the problem of producing, for name in source language, setof one or more transliteration candidates in target language.
</prevsent>
<prevsent>previous work ? e.g.
</prevsent>
</prevsection>
<citsent citstr=" P95-1032 ">
(knight and graehl, 1998; <papid> J98-4003 </papid>meng et al, 2001; al-onaizan and knight, 2002; gao et al, 2004) ? has mostly assumed that one has training lexicon of transliteration pairs, from which one can learn model, often source-channel or maxent-based model.comparable corpora have been studied extensively in the literature ? e.g.,(fung, 1995; <papid> P95-1032 </papid>rapp, 1995; <papid> P95-1050 </papid>tanaka and iwasaki, 1996; <papid> C96-2098 </papid>franz et al, 1998; ballesteros and croft, 1998; masuichi et al., 2000; sadat et al, 2004), but transliteration in the context of comparable corpora has not been well addressed.</citsent>
<aftsection>
<nextsent>the general idea of exploiting time correlations to acquire word translations from comparable corpora has been explored in several previous studies ? e.g., (fung, 1995; <papid> P95-1032 </papid>rapp, 1995; <papid> P95-1050 </papid>tanaka and iwasaki, 1996).<papid> C96-2098 </papid></nextsent>
<nextsent>recently, pearson correlation method was proposed to mine word pairs from comparable corpora (tao and zhai, 2005); this idea is similar to the method used in(kay and roscheisen, 1993) <papid> J93-1006 </papid>for sentence align ment.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4302">
<title id=" W06-1630.xml">unsupervised named entity transliteration using temporal and phonetic correlation </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>named entity transliteration is the problem of producing, for name in source language, setof one or more transliteration candidates in target language.
</prevsent>
<prevsent>previous work ? e.g.
</prevsent>
</prevsection>
<citsent citstr=" P95-1050 ">
(knight and graehl, 1998; <papid> J98-4003 </papid>meng et al, 2001; al-onaizan and knight, 2002; gao et al, 2004) ? has mostly assumed that one has training lexicon of transliteration pairs, from which one can learn model, often source-channel or maxent-based model.comparable corpora have been studied extensively in the literature ? e.g.,(fung, 1995; <papid> P95-1032 </papid>rapp, 1995; <papid> P95-1050 </papid>tanaka and iwasaki, 1996; <papid> C96-2098 </papid>franz et al, 1998; ballesteros and croft, 1998; masuichi et al., 2000; sadat et al, 2004), but transliteration in the context of comparable corpora has not been well addressed.</citsent>
<aftsection>
<nextsent>the general idea of exploiting time correlations to acquire word translations from comparable corpora has been explored in several previous studies ? e.g., (fung, 1995; <papid> P95-1032 </papid>rapp, 1995; <papid> P95-1050 </papid>tanaka and iwasaki, 1996).<papid> C96-2098 </papid></nextsent>
<nextsent>recently, pearson correlation method was proposed to mine word pairs from comparable corpora (tao and zhai, 2005); this idea is similar to the method used in(kay and roscheisen, 1993) <papid> J93-1006 </papid>for sentence align ment.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4304">
<title id=" W06-1630.xml">unsupervised named entity transliteration using temporal and phonetic correlation </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>named entity transliteration is the problem of producing, for name in source language, setof one or more transliteration candidates in target language.
</prevsent>
<prevsent>previous work ? e.g.
</prevsent>
</prevsection>
<citsent citstr=" C96-2098 ">
(knight and graehl, 1998; <papid> J98-4003 </papid>meng et al, 2001; al-onaizan and knight, 2002; gao et al, 2004) ? has mostly assumed that one has training lexicon of transliteration pairs, from which one can learn model, often source-channel or maxent-based model.comparable corpora have been studied extensively in the literature ? e.g.,(fung, 1995; <papid> P95-1032 </papid>rapp, 1995; <papid> P95-1050 </papid>tanaka and iwasaki, 1996; <papid> C96-2098 </papid>franz et al, 1998; ballesteros and croft, 1998; masuichi et al., 2000; sadat et al, 2004), but transliteration in the context of comparable corpora has not been well addressed.</citsent>
<aftsection>
<nextsent>the general idea of exploiting time correlations to acquire word translations from comparable corpora has been explored in several previous studies ? e.g., (fung, 1995; <papid> P95-1032 </papid>rapp, 1995; <papid> P95-1050 </papid>tanaka and iwasaki, 1996).<papid> C96-2098 </papid></nextsent>
<nextsent>recently, pearson correlation method was proposed to mine word pairs from comparable corpora (tao and zhai, 2005); this idea is similar to the method used in(kay and roscheisen, 1993) <papid> J93-1006 </papid>for sentence align ment.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4310">
<title id=" W06-1630.xml">unsupervised named entity transliteration using temporal and phonetic correlation </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>(knight and graehl, 1998; <papid> J98-4003 </papid>meng et al, 2001; al-onaizan and knight, 2002; gao et al, 2004) ? has mostly assumed that one has training lexicon of transliteration pairs, from which one can learn model, often source-channel or maxent-based model.comparable corpora have been studied extensively in the literature ? e.g.,(fung, 1995; <papid> P95-1032 </papid>rapp, 1995; <papid> P95-1050 </papid>tanaka and iwasaki, 1996; <papid> C96-2098 </papid>franz et al, 1998; ballesteros and croft, 1998; masuichi et al., 2000; sadat et al, 2004), but transliteration in the context of comparable corpora has not been well addressed.</prevsent>
<prevsent>the general idea of exploiting time correlations to acquire word translations from comparable corpora has been explored in several previous studies ? e.g., (fung, 1995; <papid> P95-1032 </papid>rapp, 1995; <papid> P95-1050 </papid>tanaka and iwasaki, 1996).<papid> C96-2098 </papid></prevsent>
</prevsection>
<citsent citstr=" J93-1006 ">
recently, pearson correlation method was proposed to mine word pairs from comparable corpora (tao and zhai, 2005); this idea is similar to the method used in(kay and roscheisen, 1993) <papid> J93-1006 </papid>for sentence align ment.</citsent>
<aftsection>
<nextsent>in our work, we adopt the method proposed in (tao and zhai, 2005) and apply it to the problem of transliteration; note that (tao and zhai, 2005)compares several different metrics for time correlation, as we also note below ? and see (sproat et al., 2006).<papid> P06-1010 </papid></nextsent>
<nextsent>corpora we start from comparable corpora, consisting of newspaper articles in english and the target languages for the same time period.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4311">
<title id=" W06-1630.xml">unsupervised named entity transliteration using temporal and phonetic correlation </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>the general idea of exploiting time correlations to acquire word translations from comparable corpora has been explored in several previous studies ? e.g., (fung, 1995; <papid> P95-1032 </papid>rapp, 1995; <papid> P95-1050 </papid>tanaka and iwasaki, 1996).<papid> C96-2098 </papid></prevsent>
<prevsent>recently, pearson correlation method was proposed to mine word pairs from comparable corpora (tao and zhai, 2005); this idea is similar to the method used in(kay and roscheisen, 1993) <papid> J93-1006 </papid>for sentence align ment.</prevsent>
</prevsection>
<citsent citstr=" P06-1010 ">
in our work, we adopt the method proposed in (tao and zhai, 2005) and apply it to the problem of transliteration; note that (tao and zhai, 2005)compares several different metrics for time correlation, as we also note below ? and see (sproat et al., 2006).<papid> P06-1010 </papid></citsent>
<aftsection>
<nextsent>corpora we start from comparable corpora, consisting of newspaper articles in english and the target languages for the same time period.
</nextsent>
<nextsent>in this paper, the target languages are arabic, chinese and hindi.
</nextsent>
<nextsent>we then extract named-entities in the english text using the named-entity recognizer described in (li et al, 2004), <papid> N04-1003 </papid>which is based on the snow machine learning toolkit (carlson et al, 1999).</nextsent>
<nextsent>to perform transliteration, we use the following general ap proach: 1 extract named entities from the english corpus for each day; 2 extract candidates from the same days newspapers in the target language; 3 for each english named entity, score and rank thetarget-language candidates as potential trans liter ations.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4312">
<title id=" W06-1630.xml">unsupervised named entity transliteration using temporal and phonetic correlation </title>
<section> transliteration with comparable.  </section>
<citcontext>
<prevsection>
<prevsent>corpora we start from comparable corpora, consisting of newspaper articles in english and the target languages for the same time period.
</prevsent>
<prevsent>in this paper, the target languages are arabic, chinese and hindi.
</prevsent>
</prevsection>
<citsent citstr=" N04-1003 ">
we then extract named-entities in the english text using the named-entity recognizer described in (li et al, 2004), <papid> N04-1003 </papid>which is based on the snow machine learning toolkit (carlson et al, 1999).</citsent>
<aftsection>
<nextsent>to perform transliteration, we use the following general ap proach: 1 extract named entities from the english corpus for each day; 2 extract candidates from the same days newspapers in the target language; 3 for each english named entity, score and rank thetarget-language candidates as potential trans liter ations.
</nextsent>
<nextsent>we apply two unsupervised methods ? time correlation and pronunciation-based methods ? independently, and in combination.
</nextsent>
<nextsent>3.1 candidate scoring based on.
</nextsent>
<nextsent>pronunciation our phonetic transliteration score uses standard string-alignment and alignment-scoring technique based on (kruskal, 1999) in that the distance is determined by combination of substitution, insertion and deletion costs.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4313">
<title id=" W06-1630.xml">unsupervised named entity transliteration using temporal and phonetic correlation </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>to generate the hindi and arabic candidates, all words from the same seven days were extracted.
</prevsent>
<prevsent>the words were stemmed all possible ways using simple hand-developed affix lists: for example, given hindi word c1c2c3, if both c3 andc2c3 are in our suffix and ending list, then this single word generates three possible candidates: c1, c1c2, and c1c2c3.
</prevsent>
</prevsection>
<citsent citstr=" J96-3004 ">
in contrast, chinese candidates were extracted using list of 495 characters that are frequently used for foreign names (sproat etal., 1996).<papid> J96-3004 </papid></citsent>
<aftsection>
<nextsent>a sequence of three or more such characters from the list is taken as possible name.
</nextsent>
<nextsent>the number of candidates for each target language is presented in the last column of table 5.
</nextsent>
<nextsent>we measured the accuracy of transliteration by mean reciprocal rank (mrr), measure commonly used in information retrieval when there is precisely one correct answer (kantor and voorhees, 2000).we attempted to create complete set of answers for 200 english names in our test set, but small number of english names do not seem tohave any standard transliteration in the target language according to the resources that we looked at, and these names we removed from the evaluation set.
</nextsent>
<nextsent>thus, we ended up having list of less than 200 english names, shown in the second column of table 6 (all).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4314">
<title id=" W06-1630.xml">unsupervised named entity transliteration using temporal and phonetic correlation </title>
<section> conclusions and future work.  </section>
<citcontext>
<prevsection>
<prevsent>one particular area that we will continue to work on is phonetic distance.
</prevsent>
<prevsent>we believe our hand-assigned costs are reasonable starting point if one knows nothing about the particular pair of languages in question.
</prevsent>
</prevsection>
<citsent citstr=" P00-1027 ">
however one could also train such costs, either from an existing list of known transliterations, or as part of an iterative bootstrapping method as, for example, in yarowsky and wicentowskis (2000) <papid> P00-1027 </papid>work on morphological induction.</citsent>
<aftsection>
<nextsent>the work we report is ongoing and is part of alarger project on multilingual named entity recognition and transliteration.
</nextsent>
<nextsent>one of the goals of this project is to develop tools and resources for under resourced languages.
</nextsent>
<nextsent>insofar as the techniques we have proposed have been shown to work on three language pairs involving one source language (en glish) and three unrelated and quite different target languages, one can reasonably claim that the techniques are language-independent.
</nextsent>
<nextsent>furthermore, as 256the case of hindi shows, even with data from completely different news agencies we are able to extract useful correspondences.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4315">
<title id=" W07-0709.xml">meta structure transformation model for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>experiments with bleu metric show that the model significantly outperforms pharaoh, state-art-the art phrase-based system.
</prevsent>
<prevsent>the statistical approach has been widely used in machine translation, which use the noisy-channel based model.
</prevsent>
</prevsection>
<citsent citstr=" W02-1018 ">
a joint probability model, proposed by marcu and wong (2002), <papid> W02-1018 </papid>is kind of phrase based one.</citsent>
<aftsection>
<nextsent>och and ney (2004) gave framework of alignment templates for this kind of models.
</nextsent>
<nextsent>all of the phrase-based models outperformed the word-based models, by automatically learning word and phrase equivalents from bilingual corpus and reordering at the phrase level.
</nextsent>
<nextsent>but it has been found that phrases longer than three words have little improvement in the performance (koehn, 2003).
</nextsent>
<nextsent>above the phrase level, these models have simple distortion model that re orders phrases independently, without consideration of their contents and syntactic information.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4316">
<title id=" W07-0709.xml">meta structure transformation model for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>above the phrase level, these models have simple distortion model that re orders phrases independently, without consideration of their contents and syntactic information.
</prevsent>
<prevsent>in recent years, applying different statistical learning methods to structured data has attracted various researchers.
</prevsent>
</prevsection>
<citsent citstr=" J97-3002 ">
syntax-based mt approaches began with wu (1997), <papid> J97-3002 </papid>who introduced the inversion transduction grammars.</citsent>
<aftsection>
<nextsent>utilizing syntactic structure as the channel input was introduced into mt by yamada (2001).
</nextsent>
<nextsent>syntax-based models have been presented in different grammar formalisms.
</nextsent>
<nextsent>the model based on head-transducer was presented by alshawi (2000).
</nextsent>
<nextsent>daniel gildea (2003) <papid> P03-1011 </papid>dealt with the problem of the parse tree isomorphism with cloning operation to either tree-to string or tree-to-tree alignment models.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4317">
<title id=" W07-0709.xml">meta structure transformation model for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>syntax-based models have been presented in different grammar formalisms.
</prevsent>
<prevsent>the model based on head-transducer was presented by alshawi (2000).
</prevsent>
</prevsection>
<citsent citstr=" P03-1011 ">
daniel gildea (2003) <papid> P03-1011 </papid>dealt with the problem of the parse tree isomorphism with cloning operation to either tree-to string or tree-to-tree alignment models.</citsent>
<aftsection>
<nextsent>ding and palmer (2005) <papid> P05-1067 </papid>introduced version of probabilistic extension of synchronous dependency insertion grammars (sdig) to deal with the pervasive structure divergence.</nextsent>
<nextsent>all these approaches dont model the translation process, but formalize model that generates two languages at the same time, which can be considered as some kind of tree transducers.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4318">
<title id=" W07-0709.xml">meta structure transformation model for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the model based on head-transducer was presented by alshawi (2000).
</prevsent>
<prevsent>daniel gildea (2003) <papid> P03-1011 </papid>dealt with the problem of the parse tree isomorphism with cloning operation to either tree-to string or tree-to-tree alignment models.</prevsent>
</prevsection>
<citsent citstr=" P05-1067 ">
ding and palmer (2005) <papid> P05-1067 </papid>introduced version of probabilistic extension of synchronous dependency insertion grammars (sdig) to deal with the pervasive structure divergence.</citsent>
<aftsection>
<nextsent>all these approaches dont model the translation process, but formalize model that generates two languages at the same time, which can be considered as some kind of tree transducers.
</nextsent>
<nextsent>graehl and knight (2004) <papid> N04-1014 </papid>described the use of tree transducers for natural language processing and addressed the training problems for this kind of transducers.</nextsent>
<nextsent>in this paper, we define model based on the ms decomposition of the parse trees for statistical machine translation, which can capture structural variations and has proven generation capacity.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4319">
<title id=" W07-0709.xml">meta structure transformation model for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>ding and palmer (2005) <papid> P05-1067 </papid>introduced version of probabilistic extension of synchronous dependency insertion grammars (sdig) to deal with the pervasive structure divergence.</prevsent>
<prevsent>all these approaches dont model the translation process, but formalize model that generates two languages at the same time, which can be considered as some kind of tree transducers.</prevsent>
</prevsection>
<citsent citstr=" N04-1014 ">
graehl and knight (2004) <papid> N04-1014 </papid>described the use of tree transducers for natural language processing and addressed the training problems for this kind of transducers.</citsent>
<aftsection>
<nextsent>in this paper, we define model based on the ms decomposition of the parse trees for statistical machine translation, which can capture structural variations and has proven generation capacity.
</nextsent>
<nextsent>during the translation process of our model, the parse tree of the source language is decomposed into different levels of ms and then transformed into the ones of the target language in the form of rm.
</nextsent>
<nextsent>the source language can be reordered according to the structure transformation.
</nextsent>
<nextsent>at last, the target translation string is generated in the scopes of rm.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4320">
<title id=" W07-0709.xml">meta structure transformation model for statistical machine translation </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>to evaluate the result of the translation, the bleu metric (papineni et al 2002) was used.
</prevsent>
<prevsent>5.1 the baseline.
</prevsent>
</prevsection>
<citsent citstr=" N03-1017 ">
system used for comparison was pharaoh (koehn et al, 2003; <papid> N03-1017 </papid>koehn, 2004), which uses beam search algorithm for decoding.</citsent>
<aftsection>
<nextsent>in its model, it takes the following features: language model, phrase translation probability in the two directions, distortion model, word penalty and phrase penalty, all of which can be achieved with the training tool kits distributed by koehn.
</nextsent>
<nextsent>the training set and development set mentioned above were used to perform the training task and to tune the feature weights by the minimum error training algorithm.
</nextsent>
<nextsent>all the other settings were the same as the default ones.
</nextsent>
<nextsent>sri language modeling toolkit was used to train 3-gram language model.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4321">
<title id=" W06-1664.xml">graph based word clustering using a web search engine </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>numerous studies have examined the use of the web as corpus (kilgarriff, 2003).
</prevsent>
<prevsent>web-based models perform especially well against the sparse data problem: statistical techniques perform poorly when the words are rarely used.
</prevsent>
</prevsection>
<citsent citstr=" W02-1030 ">
for example, f. keller et al (2002) <papid> W02-1030 </papid>use the web to obtain frequencies for unseen bigrams in given corpus.</citsent>
<aftsection>
<nextsent>they count for adjective-noun, noun-noun, and verb-object bigrams by query inga search engine, and demonstrate that web frequencies (web counts) correlate with frequencies from carefully edited corpus such as the british national corpus (bnc).
</nextsent>
<nextsent>aside from counting bigrams, various tasks are attainable using web based models: spelling correction, adjective ordering, compound noun bracketing, count ability detection, and so on (lapata and keller, 2004).<papid> N04-1016 </papid></nextsent>
<nextsent>for some tasks, simple unsupervised models perform better when n-gram frequencies are obtained from the web rather than from standard large corpus; the web yields better counts than the bnc.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4322">
<title id=" W06-1664.xml">graph based word clustering using a web search engine </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for example, f. keller et al (2002) <papid> W02-1030 </papid>use the web to obtain frequencies for unseen bigrams in given corpus.</prevsent>
<prevsent>they count for adjective-noun, noun-noun, and verb-object bigrams by query inga search engine, and demonstrate that web frequencies (web counts) correlate with frequencies from carefully edited corpus such as the british national corpus (bnc).</prevsent>
</prevsection>
<citsent citstr=" N04-1016 ">
aside from counting bigrams, various tasks are attainable using web based models: spelling correction, adjective ordering, compound noun bracketing, count ability detection, and so on (lapata and keller, 2004).<papid> N04-1016 </papid></citsent>
<aftsection>
<nextsent>for some tasks, simple unsupervised models perform better when n-gram frequencies are obtained from the web rather than from standard large corpus; the web yields better counts than the bnc.
</nextsent>
<nextsent>the web is an excellent source of information on new words.
</nextsent>
<nextsent>therefore, automatic thesaurus construction (curran, 2002) <papid> W02-1029 </papid>offers great potential for various useful nlp applications.</nextsent>
<nextsent>several studies have addressed the extraction of hypernyms and hyponyms from the web (miura et al, 2004; cimiano et al, 2004).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4324">
<title id=" W06-1664.xml">graph based word clustering using a web search engine </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for some tasks, simple unsupervised models perform better when n-gram frequencies are obtained from the web rather than from standard large corpus; the web yields better counts than the bnc.
</prevsent>
<prevsent>the web is an excellent source of information on new words.
</prevsent>
</prevsection>
<citsent citstr=" W02-1029 ">
therefore, automatic thesaurus construction (curran, 2002) <papid> W02-1029 </papid>offers great potential for various useful nlp applications.</citsent>
<aftsection>
<nextsent>several studies have addressed the extraction of hypernyms and hyponyms from the web (miura et al, 2004; cimiano et al, 2004).
</nextsent>
<nextsent>p. turney (2001) presents method to recognize synonyms by obtaining word counts and calculating pointwise mutual information (pmi).
</nextsent>
<nextsent>for further development of automatic thesaurus construction, word clustering is beneficial, e.g. for obtaining synsets.
</nextsent>
<nextsent>it also contributes to word sense disambiguation (li and abe, 1998)<papid> P98-2124 </papid>and text classification (dhillon et al, 2002) be cause the dimensionality is reduced efficiently.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4325">
<title id=" W06-1664.xml">graph based word clustering using a web search engine </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>p. turney (2001) presents method to recognize synonyms by obtaining word counts and calculating pointwise mutual information (pmi).
</prevsent>
<prevsent>for further development of automatic thesaurus construction, word clustering is beneficial, e.g. for obtaining synsets.
</prevsent>
</prevsection>
<citsent citstr=" P98-2124 ">
it also contributes to word sense disambiguation (li and abe, 1998)<papid> P98-2124 </papid>and text classification (dhillon et al, 2002) be cause the dimensionality is reduced efficiently.</citsent>
<aftsection>
<nextsent>this paper presents an unsupervised algorithm for word clustering based on word similarity measure by web counts.
</nextsent>
<nextsent>given set of words, the algorithm clusters the words into groups so that the similar words are in the same cluster.
</nextsent>
<nextsent>each pairof words is queried to search engine, which results in co-occurrence matrix.
</nextsent>
<nextsent>by calculating the similarity of words, word co-occurrence graph is created.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4326">
<title id=" W06-1664.xml">graph based word clustering using a web search engine </title>
<section> related works.  </section>
<citcontext>
<prevsection>
<prevsent>sections 4 and 5 explain evaluations and advance discussion.
</prevsent>
<prevsent>finally, we conclude the paper.
</prevsent>
</prevsection>
<citsent citstr=" P04-1068 ">
a number of studies have explained the use of the web for nlp tasks e.g., creating multilingual translation lexicons (cheng et al, 2004), <papid> P04-1068 </papid>text classification (huang et al, 2004), and word sense disambiguation (turney, 2004).<papid> W04-0858 </papid></citsent>
<aftsection>
<nextsent>m. baroni and m. ueyama summarize three approaches to use theweb as corpus (baroni and ueyama, 2005): using web counts as frequency estimates, building corpora through search engine queries, and crawling the web for linguistic purposes.
</nextsent>
<nextsent>commercial search engines are optimized for ordinary users.
</nextsent>
<nextsent>therefore, it is desirable to crawl the web and to develop specific search engines for nlp applications (cafarella and etzioni, 2005).
</nextsent>
<nextsent>however, considering that great efforts are taken in commercial search engines to maintain quality of crawling and indexing, especially against spam mers, it is still important to pursue the possibility of using the current search engines for nlp applications.p. turney (turney, 2001) presents an unsupervised learning algorithm for recognizing synonyms by querying web search engine.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4327">
<title id=" W06-1664.xml">graph based word clustering using a web search engine </title>
<section> related works.  </section>
<citcontext>
<prevsection>
<prevsent>sections 4 and 5 explain evaluations and advance discussion.
</prevsent>
<prevsent>finally, we conclude the paper.
</prevsent>
</prevsection>
<citsent citstr=" W04-0858 ">
a number of studies have explained the use of the web for nlp tasks e.g., creating multilingual translation lexicons (cheng et al, 2004), <papid> P04-1068 </papid>text classification (huang et al, 2004), and word sense disambiguation (turney, 2004).<papid> W04-0858 </papid></citsent>
<aftsection>
<nextsent>m. baroni and m. ueyama summarize three approaches to use theweb as corpus (baroni and ueyama, 2005): using web counts as frequency estimates, building corpora through search engine queries, and crawling the web for linguistic purposes.
</nextsent>
<nextsent>commercial search engines are optimized for ordinary users.
</nextsent>
<nextsent>therefore, it is desirable to crawl the web and to develop specific search engines for nlp applications (cafarella and etzioni, 2005).
</nextsent>
<nextsent>however, considering that great efforts are taken in commercial search engines to maintain quality of crawling and indexing, especially against spam mers, it is still important to pursue the possibility of using the current search engines for nlp applications.p. turney (turney, 2001) presents an unsupervised learning algorithm for recognizing synonyms by querying web search engine.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4328">
<title id=" W06-1664.xml">graph based word clustering using a web search engine </title>
<section> related works.  </section>
<citcontext>
<prevsection>
<prevsent>it is evaluated using 80 synonym test questions from the test of english as foreign language (toefl) and 50 from the english as asecond language test (esl).
</prevsent>
<prevsent>the algorithm obtains score of 74%, contrasted to that of 64% by latent semantic analysis (lsa).
</prevsent>
</prevsection>
<citsent citstr=" N03-1032 ">
terra and clarke(terra and clarke, 2003) <papid> N03-1032 </papid>provide comparative investigation of co-occurrence frequency estimation on the performance of synonym tests.</citsent>
<aftsection>
<nextsent>they report that pmi (with certain window size) performs best on average.
</nextsent>
<nextsent>also, pmi-ir is useful for calculating semantic orientation and rating reviews (turney, 2002).<papid> P02-1053 </papid></nextsent>
<nextsent>as described, pmi is one of many measures to calculate the strength of word similarity or word association (manning and schutze, 2002).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4330">
<title id=" W06-1664.xml">graph based word clustering using a web search engine </title>
<section> related works.  </section>
<citcontext>
<prevsection>
<prevsent>terra and clarke(terra and clarke, 2003) <papid> N03-1032 </papid>provide comparative investigation of co-occurrence frequency estimation on the performance of synonym tests.</prevsent>
<prevsent>they report that pmi (with certain window size) performs best on average.</prevsent>
</prevsection>
<citsent citstr=" P02-1053 ">
also, pmi-ir is useful for calculating semantic orientation and rating reviews (turney, 2002).<papid> P02-1053 </papid></citsent>
<aftsection>
<nextsent>as described, pmi is one of many measures to calculate the strength of word similarity or word association (manning and schutze, 2002).
</nextsent>
<nextsent>an important assumption is that similarity between words is consequence of word co-occurrence, or that the proximity of words in text is indicative of relationship between them, such as synonymy or antonymy.
</nextsent>
<nextsent>a commonly used technique to obtain word groups is distributional clustering (baker and mccallum, 1998).
</nextsent>
<nextsent>distributional clustering of words was first proposed by pereira tishby &amp; leein (pereira et al, 1993): <papid> P93-1024 </papid>they cluster nouns according to their conditional verb distributions.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4331">
<title id=" W06-1664.xml">graph based word clustering using a web search engine </title>
<section> related works.  </section>
<citcontext>
<prevsection>
<prevsent>an important assumption is that similarity between words is consequence of word co-occurrence, or that the proximity of words in text is indicative of relationship between them, such as synonymy or antonymy.
</prevsent>
<prevsent>a commonly used technique to obtain word groups is distributional clustering (baker and mccallum, 1998).
</prevsent>
</prevsection>
<citsent citstr=" P93-1024 ">
distributional clustering of words was first proposed by pereira tishby &amp; leein (pereira et al, 1993): <papid> P93-1024 </papid>they cluster nouns according to their conditional verb distributions.</citsent>
<aftsection>
<nextsent>graphic representations for word similarity have also been advanced by several researchers.
</nextsent>
<nextsent>kageura et al (2000) <papid> C00-1058 </papid>propose automatic thesaurus generation based on graphic representation.</nextsent>
<nextsent>by applying minimum edge cut, the corresponding english terms and japanese terms are identified as cluster.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4332">
<title id=" W06-1664.xml">graph based word clustering using a web search engine </title>
<section> related works.  </section>
<citcontext>
<prevsection>
<prevsent>distributional clustering of words was first proposed by pereira tishby &amp; leein (pereira et al, 1993): <papid> P93-1024 </papid>they cluster nouns according to their conditional verb distributions.</prevsent>
<prevsent>graphic representations for word similarity have also been advanced by several researchers.</prevsent>
</prevsection>
<citsent citstr=" C00-1058 ">
kageura et al (2000) <papid> C00-1058 </papid>propose automatic thesaurus generation based on graphic representation.</citsent>
<aftsection>
<nextsent>by applying minimum edge cut, the corresponding english terms and japanese terms are identified as cluster.
</nextsent>
<nextsent>widdows and dorow (2002) <papid> C02-1114 </papid>use graph model for unsupervised lexical acquisition.</nextsent>
<nextsent>a graph is produced by linking pairs of words which participate in particular syntactic relation ships.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4333">
<title id=" W06-1664.xml">graph based word clustering using a web search engine </title>
<section> related works.  </section>
<citcontext>
<prevsection>
<prevsent>kageura et al (2000) <papid> C00-1058 </papid>propose automatic thesaurus generation based on graphic representation.</prevsent>
<prevsent>by applying minimum edge cut, the corresponding english terms and japanese terms are identified as cluster.</prevsent>
</prevsection>
<citsent citstr=" C02-1114 ">
widdows and dorow (2002) <papid> C02-1114 </papid>use graph model for unsupervised lexical acquisition.</citsent>
<aftsection>
<nextsent>a graph is produced by linking pairs of words which participate in particular syntactic relationships.
</nextsent>
<nextsent>an incremental cluster-building algorithm achieves 82% accuracy at lexical acquisition task, evaluated against wordnet classes.
</nextsent>
<nextsent>another study builds co-occurrence graph of terms and decomposes it to identify relevant terms by duplicating nodes and edges (tanaka-ishii and iwasaki, 1996).
</nextsent>
<nextsent>it focuses on transitivity: if transit ivity does not hold between three nodes (e.g., if edge a-b and b-c exist but edge a-c does not), the nodes should be in separate clusters.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4339">
<title id=" W07-0308.xml">experiences of an inservice wizardofoz data collection for the deployment of a call routing application </title>
<section> background and introduction.  </section>
<citcontext>
<prevsection>
<prevsent>system (gorin et al 1997; ammicht et al 1999), which comprised three batches of transactions with live customers, each involving up to 12,000 utterances.
</prevsent>
<prevsent>other well-known instances are voyager?
</prevsent>
</prevsection>
<citsent citstr=" H89-2018 ">
(zue et al 1989) <papid> H89-2018 </papid>and the individual atis collections (hirschman et al 1993) <papid> H93-1004 </papid>which involved up to hundred subjects or (again) up to 12,000 utterances.</citsent>
<aftsection>
<nextsent>while it is true that wizard-of-oz is labour intensive method, the effort can often be motivated on the ground that it enables significant design and evaluation to be carried out before implementation, thereby reducing the amount of re-design necessary for the actual system.
</nextsent>
<nextsent>however, one should also bear in mind the crucial advantage brought about by the possibility in production environment of running the wizard-of-oz collection in-service rather than in closed lab setting.
</nextsent>
<nextsent>as we shall discuss, the fact that real customers with real problems are involved instead of role-playing subjects with artificial tasks circumvents the key methodological problem that has been raised as an argument against wizard-of oz, namely, lack of realism.
</nextsent>
<nextsent>1 for backgrounds on wizard-of-oz methodology, see dahlbck et al (1993).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4340">
<title id=" W07-0308.xml">experiences of an inservice wizardofoz data collection for the deployment of a call routing application </title>
<section> background and introduction.  </section>
<citcontext>
<prevsection>
<prevsent>system (gorin et al 1997; ammicht et al 1999), which comprised three batches of transactions with live customers, each involving up to 12,000 utterances.
</prevsent>
<prevsent>other well-known instances are voyager?
</prevsent>
</prevsection>
<citsent citstr=" H93-1004 ">
(zue et al 1989) <papid> H89-2018 </papid>and the individual atis collections (hirschman et al 1993) <papid> H93-1004 </papid>which involved up to hundred subjects or (again) up to 12,000 utterances.</citsent>
<aftsection>
<nextsent>while it is true that wizard-of-oz is labour intensive method, the effort can often be motivated on the ground that it enables significant design and evaluation to be carried out before implementation, thereby reducing the amount of re-design necessary for the actual system.
</nextsent>
<nextsent>however, one should also bear in mind the crucial advantage brought about by the possibility in production environment of running the wizard-of-oz collection in-service rather than in closed lab setting.
</nextsent>
<nextsent>as we shall discuss, the fact that real customers with real problems are involved instead of role-playing subjects with artificial tasks circumvents the key methodological problem that has been raised as an argument against wizard-of oz, namely, lack of realism.
</nextsent>
<nextsent>1 for backgrounds on wizard-of-oz methodology, see dahlbck et al (1993).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4341">
<title id=" W07-0308.xml">experiences of an inservice wizardofoz data collection for the deployment of a call routing application </title>
<section> options for initial data collection.  </section>
<citcontext>
<prevsection>
<prevsent>3.3 wizard-of-oz.
</prevsent>
<prevsent>although wizard-of-oz is arguably the best method for collecting machine-directed data in the absence of running application, it is not without methodological problems.
</prevsent>
</prevsection>
<citsent citstr=" C86-1123 ">
the basic critique has always been aimed at the lack of realism (for example, von hahn 1986).<papid> C86-1123 </papid></citsent>
<aftsection>
<nextsent>in thorough analysis, allwood &amp; haglund (1992) point out that in wizard-of-oz simulation, both the subjects and the wizard(s) are playing roles, occupied and assigned.
</nextsent>
<nextsent>the researcher acting as the wizard is occupying the role of researcher interested in obtaining as natural as possible?
</nextsent>
<nextsent>language and speech data, while playing the role of the system.
</nextsent>
<nextsent>the subject, on the other hand, is occupying the role of subject in scientific study, and playing the role of client (or similar), communicating with system while carrying out tasks that are not genuine to the subject, but given to them by the experiment leader (who might be identical with the wizard).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4342">
<title id=" W07-0308.xml">experiences of an inservice wizardofoz data collection for the deployment of a call routing application </title>
<section> data-collection application.  </section>
<citcontext>
<prevsection>
<prevsent>as pointed out in section 3.3, one purpose of this is to let wizards take over calls that are problematic, thereby making sure that callers do not get maltreated during the data collection and reducing the risk that they hangup.
</prevsent>
<prevsent>a similar functionality was available in the data-collection application for at&ts; how may help you?
</prevsent>
</prevsection>
<citsent citstr=" A00-2028 ">
system (walker et al 2000).<papid> A00-2028 </papid></citsent>
<aftsection>
<nextsent>59 plate 1: the prompt piano client interface as configured towards the end of the data collection.
</nextsent>
<nextsent>the interface is divided into three fields with buttons.
</nextsent>
<nextsent>i: the left most field provides caller information, like a-nr (the phone number the customer is calling from) and cid (the phone number the customer provides as reason for the call).
</nextsent>
<nextsent>the wizard has two option buttons, mina tgrder (my actions?), at hand: the button bryt in / prata med kund (barge-in/talk to client?)
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4344">
<title id=" W07-0308.xml">experiences of an inservice wizardofoz data collection for the deployment of a call routing application </title>
<section> data collection.  </section>
<citcontext>
<prevsection>
<prevsent>by asking the disambiguation prompt.
</prevsent>
<prevsent>furthermore, in order to see to what extent these prompts also made callers provide more information, we manually tagged the transcribed utterances with semantic categories.
</prevsent>
</prevsection>
<citsent citstr=" W07-0310 ">
following the evaluation methodology suggested by boye &amp; wirn (2007, <papid> W07-0310 </papid>section 5), we then computed the difference with respect to concepts?</citsent>
<aftsection>
<nextsent>for utterances immediately following and preceding the two kinds of prompts.
</nextsent>
<nextsent>although the number of concepts gained is only slightly higher6 for the open prompt (as function of concepts per utterance), there are some palpable differences between the directed and the open prompt.
</nextsent>
<nextsent>one, shown in table 1, is that there are no instances where an already instantiated concept (e.g. fixedtelephony) is changed to something else (e.g. broadband), while this happens 18 times following the open prompt.
</nextsent>
<nextsent>the other, not shown in table 1, is that, following the directed prompt, one never gains?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4345">
<title id=" W06-1666.xml">loss minimization in parse reranking </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the proposed methods are applied to the parse reranking task, with various baseline models, achieving significant improvement both over the probabilistic models and the discriminative rerankers.
</prevsent>
<prevsent>when neural network parser is used as the probabilistic model and the voted perceptron algorithm with data-defined kernels as the learning algorithm, the loss minimization model achieves 90.0% labeled constituents f1 score on the standard wsj parsing task.
</prevsent>
</prevsection>
<citsent citstr=" J05-1003 ">
the reranking approach is widely used in parsing (collins and koo, 2005; <papid> J05-1003 </papid>koo and collins, 2005; <papid> H05-1064 </papid>henderson and titov, 2005; <papid> P05-1023 </papid>shen and joshi,2003) <papid> W03-0402 </papid>as well as in other structured classification problems.</citsent>
<aftsection>
<nextsent>for structured classification tasks, where labels are complex and have an internal structure of interdependency, the 0-1 loss considered in classical formulation of classification algorithms is not natural choice and different loss functions are normally employed.
</nextsent>
<nextsent>to tackle this problem, several approaches have been proposed to accommodate loss functions in learning algorithms (tsochantaridis et al, 2004; taskar et al,2004; <papid> W04-3201 </papid>henderson and titov, 2005).<papid> P05-1023 </papid></nextsent>
<nextsent>a very different use of loss functions was considered in the areas of signal processing and machine translation,where direct minimization of expected loss (min imum bayes risk decoding) on word sequences was considered (kumar and byrne, 2004; <papid> N04-1022 </papid>stolcke et al, 1997).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4347">
<title id=" W06-1666.xml">loss minimization in parse reranking </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the proposed methods are applied to the parse reranking task, with various baseline models, achieving significant improvement both over the probabilistic models and the discriminative rerankers.
</prevsent>
<prevsent>when neural network parser is used as the probabilistic model and the voted perceptron algorithm with data-defined kernels as the learning algorithm, the loss minimization model achieves 90.0% labeled constituents f1 score on the standard wsj parsing task.
</prevsent>
</prevsection>
<citsent citstr=" H05-1064 ">
the reranking approach is widely used in parsing (collins and koo, 2005; <papid> J05-1003 </papid>koo and collins, 2005; <papid> H05-1064 </papid>henderson and titov, 2005; <papid> P05-1023 </papid>shen and joshi,2003) <papid> W03-0402 </papid>as well as in other structured classification problems.</citsent>
<aftsection>
<nextsent>for structured classification tasks, where labels are complex and have an internal structure of interdependency, the 0-1 loss considered in classical formulation of classification algorithms is not natural choice and different loss functions are normally employed.
</nextsent>
<nextsent>to tackle this problem, several approaches have been proposed to accommodate loss functions in learning algorithms (tsochantaridis et al, 2004; taskar et al,2004; <papid> W04-3201 </papid>henderson and titov, 2005).<papid> P05-1023 </papid></nextsent>
<nextsent>a very different use of loss functions was considered in the areas of signal processing and machine translation,where direct minimization of expected loss (min imum bayes risk decoding) on word sequences was considered (kumar and byrne, 2004; <papid> N04-1022 </papid>stolcke et al, 1997).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4348">
<title id=" W06-1666.xml">loss minimization in parse reranking </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the proposed methods are applied to the parse reranking task, with various baseline models, achieving significant improvement both over the probabilistic models and the discriminative rerankers.
</prevsent>
<prevsent>when neural network parser is used as the probabilistic model and the voted perceptron algorithm with data-defined kernels as the learning algorithm, the loss minimization model achieves 90.0% labeled constituents f1 score on the standard wsj parsing task.
</prevsent>
</prevsection>
<citsent citstr=" P05-1023 ">
the reranking approach is widely used in parsing (collins and koo, 2005; <papid> J05-1003 </papid>koo and collins, 2005; <papid> H05-1064 </papid>henderson and titov, 2005; <papid> P05-1023 </papid>shen and joshi,2003) <papid> W03-0402 </papid>as well as in other structured classification problems.</citsent>
<aftsection>
<nextsent>for structured classification tasks, where labels are complex and have an internal structure of interdependency, the 0-1 loss considered in classical formulation of classification algorithms is not natural choice and different loss functions are normally employed.
</nextsent>
<nextsent>to tackle this problem, several approaches have been proposed to accommodate loss functions in learning algorithms (tsochantaridis et al, 2004; taskar et al,2004; <papid> W04-3201 </papid>henderson and titov, 2005).<papid> P05-1023 </papid></nextsent>
<nextsent>a very different use of loss functions was considered in the areas of signal processing and machine translation,where direct minimization of expected loss (min imum bayes risk decoding) on word sequences was considered (kumar and byrne, 2004; <papid> N04-1022 </papid>stolcke et al, 1997).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4349">
<title id=" W06-1666.xml">loss minimization in parse reranking </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the proposed methods are applied to the parse reranking task, with various baseline models, achieving significant improvement both over the probabilistic models and the discriminative rerankers.
</prevsent>
<prevsent>when neural network parser is used as the probabilistic model and the voted perceptron algorithm with data-defined kernels as the learning algorithm, the loss minimization model achieves 90.0% labeled constituents f1 score on the standard wsj parsing task.
</prevsent>
</prevsection>
<citsent citstr=" W03-0402 ">
the reranking approach is widely used in parsing (collins and koo, 2005; <papid> J05-1003 </papid>koo and collins, 2005; <papid> H05-1064 </papid>henderson and titov, 2005; <papid> P05-1023 </papid>shen and joshi,2003) <papid> W03-0402 </papid>as well as in other structured classification problems.</citsent>
<aftsection>
<nextsent>for structured classification tasks, where labels are complex and have an internal structure of interdependency, the 0-1 loss considered in classical formulation of classification algorithms is not natural choice and different loss functions are normally employed.
</nextsent>
<nextsent>to tackle this problem, several approaches have been proposed to accommodate loss functions in learning algorithms (tsochantaridis et al, 2004; taskar et al,2004; <papid> W04-3201 </papid>henderson and titov, 2005).<papid> P05-1023 </papid></nextsent>
<nextsent>a very different use of loss functions was considered in the areas of signal processing and machine translation,where direct minimization of expected loss (min imum bayes risk decoding) on word sequences was considered (kumar and byrne, 2004; <papid> N04-1022 </papid>stolcke et al, 1997).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4350">
<title id=" W06-1666.xml">loss minimization in parse reranking </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the reranking approach is widely used in parsing (collins and koo, 2005; <papid> J05-1003 </papid>koo and collins, 2005; <papid> H05-1064 </papid>henderson and titov, 2005; <papid> P05-1023 </papid>shen and joshi,2003) <papid> W03-0402 </papid>as well as in other structured classification problems.</prevsent>
<prevsent>for structured classification tasks, where labels are complex and have an internal structure of interdependency, the 0-1 loss considered in classical formulation of classification algorithms is not natural choice and different loss functions are normally employed.</prevsent>
</prevsection>
<citsent citstr=" W04-3201 ">
to tackle this problem, several approaches have been proposed to accommodate loss functions in learning algorithms (tsochantaridis et al, 2004; taskar et al,2004; <papid> W04-3201 </papid>henderson and titov, 2005).<papid> P05-1023 </papid></citsent>
<aftsection>
<nextsent>a very different use of loss functions was considered in the areas of signal processing and machine translation,where direct minimization of expected loss (min imum bayes risk decoding) on word sequences was considered (kumar and byrne, 2004; <papid> N04-1022 </papid>stolcke et al, 1997).</nextsent>
<nextsent>the only attempt to use minimum bayes risk (mbr) decoding in parsing wasmade in (goodman, 1996), <papid> P96-1024 </papid>where parsing algorithm for constituent recall minimization was constructed.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4355">
<title id=" W06-1666.xml">loss minimization in parse reranking </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for structured classification tasks, where labels are complex and have an internal structure of interdependency, the 0-1 loss considered in classical formulation of classification algorithms is not natural choice and different loss functions are normally employed.
</prevsent>
<prevsent>to tackle this problem, several approaches have been proposed to accommodate loss functions in learning algorithms (tsochantaridis et al, 2004; taskar et al,2004; <papid> W04-3201 </papid>henderson and titov, 2005).<papid> P05-1023 </papid></prevsent>
</prevsection>
<citsent citstr=" N04-1022 ">
a very different use of loss functions was considered in the areas of signal processing and machine translation,where direct minimization of expected loss (min imum bayes risk decoding) on word sequences was considered (kumar and byrne, 2004; <papid> N04-1022 </papid>stolcke et al, 1997).</citsent>
<aftsection>
<nextsent>the only attempt to use minimum bayes risk (mbr) decoding in parsing wasmade in (goodman, 1996), <papid> P96-1024 </papid>where parsing algorithm for constituent recall minimization was constructed.</nextsent>
<nextsent>however, their approach is limited to binarized pcfg models and, consequently, isnot applicable to state-of-the-art parsing methods (charniak and johnson, 2005; <papid> P05-1022 </papid>henderson, 2004; <papid> P04-1013 </papid>collins, 2000).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4356">
<title id=" W06-1666.xml">loss minimization in parse reranking </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>to tackle this problem, several approaches have been proposed to accommodate loss functions in learning algorithms (tsochantaridis et al, 2004; taskar et al,2004; <papid> W04-3201 </papid>henderson and titov, 2005).<papid> P05-1023 </papid></prevsent>
<prevsent>a very different use of loss functions was considered in the areas of signal processing and machine translation,where direct minimization of expected loss (min imum bayes risk decoding) on word sequences was considered (kumar and byrne, 2004; <papid> N04-1022 </papid>stolcke et al, 1997).</prevsent>
</prevsection>
<citsent citstr=" P96-1024 ">
the only attempt to use minimum bayes risk (mbr) decoding in parsing wasmade in (goodman, 1996), <papid> P96-1024 </papid>where parsing algorithm for constituent recall minimization was constructed.</citsent>
<aftsection>
<nextsent>however, their approach is limited to binarized pcfg models and, consequently, isnot applicable to state-of-the-art parsing methods (charniak and johnson, 2005; <papid> P05-1022 </papid>henderson, 2004; <papid> P04-1013 </papid>collins, 2000).</nextsent>
<nextsent>in this paper we consider several approaches to loss approximation on the basis of candidate list provided by baseline probabilistic model.the intuitive motivation for expected loss minimization can be seen from the following example.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4357">
<title id=" W06-1666.xml">loss minimization in parse reranking </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a very different use of loss functions was considered in the areas of signal processing and machine translation,where direct minimization of expected loss (min imum bayes risk decoding) on word sequences was considered (kumar and byrne, 2004; <papid> N04-1022 </papid>stolcke et al, 1997).</prevsent>
<prevsent>the only attempt to use minimum bayes risk (mbr) decoding in parsing wasmade in (goodman, 1996), <papid> P96-1024 </papid>where parsing algorithm for constituent recall minimization was constructed.</prevsent>
</prevsection>
<citsent citstr=" P05-1022 ">
however, their approach is limited to binarized pcfg models and, consequently, isnot applicable to state-of-the-art parsing methods (charniak and johnson, 2005; <papid> P05-1022 </papid>henderson, 2004; <papid> P04-1013 </papid>collins, 2000).</citsent>
<aftsection>
<nextsent>in this paper we consider several approaches to loss approximation on the basis of candidate list provided by baseline probabilistic model.the intuitive motivation for expected loss minimization can be seen from the following example.
</nextsent>
<nextsent>consider the situation where there are group of several very similar candidates and one very different candidate whose probability is just slightly larger than the probability of any individual candidate in the group, but much smaller than their totalprobability.
</nextsent>
<nextsent>a method which chooses the maximum probability candidate will choose this outlier candidate, which is correct if you are only interested in getting the label exactly correct (i.e. 0-1 loss), and you think the estimates are accurate.
</nextsent>
<nextsent>but if you are interested in loss function where the loss is small when you choose candidate which is similar to the correct candidate, then it is better to choose one of the candidates in the group.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4358">
<title id=" W06-1666.xml">loss minimization in parse reranking </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a very different use of loss functions was considered in the areas of signal processing and machine translation,where direct minimization of expected loss (min imum bayes risk decoding) on word sequences was considered (kumar and byrne, 2004; <papid> N04-1022 </papid>stolcke et al, 1997).</prevsent>
<prevsent>the only attempt to use minimum bayes risk (mbr) decoding in parsing wasmade in (goodman, 1996), <papid> P96-1024 </papid>where parsing algorithm for constituent recall minimization was constructed.</prevsent>
</prevsection>
<citsent citstr=" P04-1013 ">
however, their approach is limited to binarized pcfg models and, consequently, isnot applicable to state-of-the-art parsing methods (charniak and johnson, 2005; <papid> P05-1022 </papid>henderson, 2004; <papid> P04-1013 </papid>collins, 2000).</citsent>
<aftsection>
<nextsent>in this paper we consider several approaches to loss approximation on the basis of candidate list provided by baseline probabilistic model.the intuitive motivation for expected loss minimization can be seen from the following example.
</nextsent>
<nextsent>consider the situation where there are group of several very similar candidates and one very different candidate whose probability is just slightly larger than the probability of any individual candidate in the group, but much smaller than their totalprobability.
</nextsent>
<nextsent>a method which chooses the maximum probability candidate will choose this outlier candidate, which is correct if you are only interested in getting the label exactly correct (i.e. 0-1 loss), and you think the estimates are accurate.
</nextsent>
<nextsent>but if you are interested in loss function where the loss is small when you choose candidate which is similar to the correct candidate, then it is better to choose one of the candidates in the group.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4359">
<title id=" W06-1666.xml">loss minimization in parse reranking </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>it follows that an optimal reranker h?
</prevsent>
<prevsent>is one which chooses the label that minimizes the expected loss: h?(x) = arg min yg(x) ? p (y|x)?(y, y?), (2) where g(x) denotes candidate list provided by baseline probabilistic model for the input x. in this paper we propose different approaches to loss approximation.
</prevsent>
</prevsection>
<citsent citstr=" N03-1014 ">
we apply them to the parse reranking problem where the baseline probabilistic model is neural network parser (henderson,2003), <papid> N03-1014 </papid>and to parse reranking of candidates provided by the (collins, 1999) model.</citsent>
<aftsection>
<nextsent>the resulting reranking method achieves very significant improvement in the considered loss function and improvement in most other standard measures of accuracy.
</nextsent>
<nextsent>in the following three sections we will discuss three approaches to learning such classifier.
</nextsent>
<nextsent>the first two derive classification criteria for use witha predefined probability model (the first generative, the second discriminative).
</nextsent>
<nextsent>the third defines kernel for use with classification method for minimizing loss.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4374">
<title id=" W06-1666.xml">loss minimization in parse reranking </title>
<section> experimental evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>= log( ? yg(x) (y?|x, ??)(1??(y?, y)))?
</prevsent>
<prevsent>log( ? yg(x) (y?|x, ??)?(y?, y)).
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
to perform empirical evaluations of the proposed methods, we considered the task of parsing thepenn treebank wall street journal corpus (mar cus et al, 1993).<papid> J93-2004 </papid></citsent>
<aftsection>
<nextsent>first, we perform experiments with svm struct (tsochantaridis et al, 2004) as the learner.
</nextsent>
<nextsent>since svm struct already uses the loss function during training to rescale the margin or slack variables, this learner allows us to test the hypothesis that loss functions are useful in parsing not only to define the optimization criteria but also to define the classifier and to define the feature space.
</nextsent>
<nextsent>however, svm struct training for largescale parsing experiments is computationally ex pensive2, so here we use only small portion of the available training data to perform evaluations of the different approaches.
</nextsent>
<nextsent>in the other two sets of experiments, described below, we test our best model on the standard wall street journal parsing benchmark (collins, 1999) with the voted perceptron algorithm as the learner.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4384">
<title id=" W06-1666.xml">loss minimization in parse reranking </title>
<section> experimental evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>this approach allows the probability model to have an infinite number of pa rameters; the neural network only estimates the bounded number of parameters which are relevant to given partial parse.
</prevsent>
<prevsent>we define data-definedkernels in terms of the second level of parameterization (the network weights).
</prevsent>
</prevsection>
<citsent citstr=" P02-1034 ">
for the last set of experiments, we used the probabilistic model described in (collins, 1999) (model 2), and the tree kernel (collins and duffy, 2002).<papid> P02-1034 </papid></citsent>
<aftsection>
<nextsent>however, in these experiments we only used the estimates from the discriminative classifier, so the details of the probabilistic model are not relevant.
</nextsent>
<nextsent>5.2 experiments with svm struct.
</nextsent>
<nextsent>both the neural network probabilistic model andthe kernel based classifiers were trained on section 0 (1,921 sentences, 40,930 words).
</nextsent>
<nextsent>section 24 (1,346 sentences, 29,125 words) was used as the validation set during the neural network learning and for choosing parameters of the models.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4386">
<title id=" W06-1666.xml">loss minimization in parse reranking </title>
<section> experimental evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>both the neural network probabilistic model andthe kernel based classifiers were trained on section 0 (1,921 sentences, 40,930 words).
</prevsent>
<prevsent>section 24 (1,346 sentences, 29,125 words) was used as the validation set during the neural network learning and for choosing parameters of the models.
</prevsent>
</prevsection>
<citsent citstr=" W96-0213 ">
section 23 (2,416 sentences, 54,268 words) was used for the final testing of the models.we used publicly available tagger (ratna parkhi, 1996) <papid> W96-0213 </papid>to provide the part-of-speech tags for each word in the sentence.</citsent>
<aftsection>
<nextsent>for each tag, there is an unknown-word vocabulary item which is used for all those words which are not sufficiently frequent with that tag to be included individually in the vocabulary.
</nextsent>
<nextsent>for these experiments, we only included specific tag-word pair in the vocabu p f1 cm ssn 80.9 81.7 81.3 18.3 trk 81.1 82.4 81.7 18.2 ssn-estim 81.4 82.3 81.8 18.3 llk-learn 81.2 82.4 81.8 17.6 lk-learn 81.5 82.2 81.8 17.8 fk-estim 81.4 82.6 82.0 18.3 trk-estim 81.5 82.8 82.1 18.6 table 1: percentage labeled constituent recall (r),precision (p), combination of both (f1) and percentage complete match (cm) on the testing set.
</nextsent>
<nextsent>lary if it occurred at least 20 time in the training set, which (with tag-unknown-word pairs) led to the very small vocabulary of 271 tag-word pairs.
</nextsent>
<nextsent>the same model was used both for choosing the list of candidate parses and for the probabilistic model used for loss estimation and kernel feature extraction.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4392">
<title id=" W06-1666.xml">loss minimization in parse reranking </title>
<section> experimental evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>these experimental results demonstrate thatthe loss approximation reranking approaches proposed in this paper demonstrate significant improvement over the baseline models, achieving about the same relative error reduction as previously achieved with data-defined kernels (hender son and titov, 2005).<papid> P05-1023 </papid></prevsent>
<prevsent>this improvement is despite the fact that the loss function is already used in the definition of the training criteria for all the models except ssn.</prevsent>
</prevsection>
<citsent citstr=" C00-2137 ">
it is also interesting to note that the best result on the validation set for estimation 4we measured significance of all the experiments in this paper with the randomized significance test (yeh, 2000).<papid> C00-2137 </papid></citsent>
<aftsection>
<nextsent>of the loss with data-defined kernels (12) and (13) was achieved when the parameter is close to the inverse of the first component of the learned decision vector, which confirms the motivation for these kernels.
</nextsent>
<nextsent>5.3 experiments with voted perceptron and.
</nextsent>
<nextsent>data-defined kernels the above experiments with the svm structdemonstrate empirically the viability of our approaches.
</nextsent>
<nextsent>the aim of experiments on the entire wsj is to test whether our approaches still achieve significant improvement when more accurate generative models are used, and also to show that they generalize well to learning methods different from svms.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4409">
<title id=" W06-1666.xml">loss minimization in parse reranking </title>
<section> experimental evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>a list of on average 29 candidates was used, with an oracle f1 score on the testing set of 95.0%.
</prevsent>
<prevsent>we trainedvp using the same parameters for the tree kernel and probability feature weighting as described in (collins and duffy, 2002).<papid> P02-1034 </papid></prevsent>
</prevsection>
<citsent citstr=" P04-1043 ">
a publicly available efficient implementation of the tree kernel was utilized to speed up computations (moschitti,2004).<papid> P04-1043 </papid></citsent>
<aftsection>
<nextsent>as in the previous section, votes of the perceptron were used to define the probability estimate used in the classifier.the results for the mbr decoding method (tkestim), defined in section 3.3, along with the standard tree kernel vp results (collins and duffy, 2002) (<papid> P02-1034 </papid>tk) and the probabilistic baseline (collins,1999) (co99) are presented in table 3.</nextsent>
<nextsent>the proposed model improves in f1 score over the standard vp results.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4411">
<title id=" W06-1666.xml">loss minimization in parse reranking </title>
<section> experimental evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>it is important to note that the model improves in other accuracy measures as well.
</prevsent>
<prevsent>we would expect even better results with mbr-decoding if larger n-best lists are used.
</prevsent>
</prevsection>
<citsent citstr=" W05-1506 ">
the n-best parsing algorithm (huang and chiang, 2005) <papid> W05-1506 </papid>can be used to efficiently produce candidate lists as large as 106 p f1?</citsent>
<aftsection>
<nextsent>cb 0c 2c co99 88.1 88.3 88.2 1.06 64.0 85.1 tk 88.6 88.9 88.7 0.99 66.5 86.3 tk-estim 89.0 89.5 89.2 0.91 66.6 87.4 * f1 for previous models may have rounding errors.table 3: result on the testing set.
</nextsent>
<nextsent>percentage labeled constituent recall (r), precision (p), combination of both (f1), an average number of crossing brackets per sentence (cb), percentage of sentences with 0 and ? 2 crossing brackets (0c and 2c, respectively).
</nextsent>
<nextsent>parse trees with the model of (collins, 1999).
</nextsent>
<nextsent>this paper considers methods for the estimation of expected loss for parse reranking tasks.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4412">
<title id=" W06-1412.xml">noun phrase generation for situated dialogs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the algorithm was developed as decision tree and its output was evaluated by group ofhuman judges who rated 62.6% of the expressions generated by the system to be asgood as or better than the language originally produced by human dialog partners.
</prevsent>
<prevsent>in todays world of mobile, context-aware computing, intelligent software agents are being deployed in wide variety of domains to aid humans in performing navigation tasks.
</prevsent>
</prevsection>
<citsent citstr=" P02-1048 ">
examples include hand-held tourist information portals (johnston et al, 2002) <papid> P02-1048 </papid>campus tour guides (yang et al, 1999; long et al, 1996; striegnitz et al, 2005), direction-giving avatars for visitors to building (cassell et al, 2002; chou et al, 2005), in-car driving direction systems (dale et al,2003; wahlster et al, 2001), and pedestrian navigation systems (muller, 2002).</citsent>
<aftsection>
<nextsent>these applications present an exciting and challenging new frontier for dialog agents, since attributes of the real-world setting must be combined with other contextual factors for the agent to communicate successfully.
</nextsent>
<nextsent>in the current work, we focus on scenario in which the system provides incremental directions to mobile user who is following the instructions as they are produced.
</nextsent>
<nextsent>unlike the rigid directions produced by applications like mapquest,1which describes the entire route from start to finish, this task requires real time instructions issued while monitoring the users progress.
</nextsent>
<nextsent>instructions are based on dynamic local context variables such as the visibility of and distance to reference points.in referring to items in the setting, human speakers produce wide variety of noun phrase forms,including descriptions that are headed by common noun and that employ definite, indefinite, or demonstrative determiner, one anaphors, and pronouns such as it, this and that.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4413">
<title id=" W06-1412.xml">noun phrase generation for situated dialogs </title>
<section> generation for situated tasks.  </section>
<citcontext>
<prevsection>
<prevsent>some of the factors are intrinsic to the object being described, while others are features of the context in which the expression is spoken.
</prevsent>
<prevsent>the entitys status within the discourse, 1www.mapquest.com 81 spatial position, and the presence of similar items from which the target referent must be distinguished, have all been found to cause changes tothe lexical properties chosen for particular refer ring expression (i.e.
</prevsent>
</prevsection>
<citsent citstr=" J95-2003 ">
(gundel et al, 1993; prince,1981; grosz et al, 1995)).<papid> J95-2003 </papid></citsent>
<aftsection>
<nextsent>this variation is expressed in terms of the determiner chosen (e.g. that/a), the head noun (e.g. that/door/one), andthe presence of additional modifiers such as pre nominal adjectives or prepositional phrases.
</nextsent>
<nextsent>in natural language generation, the process of generating referring expressions occurs in stages (reiter and dale, 1992).<papid> C92-1038 </papid></nextsent>
<nextsent>the process we explore in this paper is the sentence planning stage, which determines whether the context supports generating particular referring expression as pronoun, description, one-anaphor, etc.there has been extensive research in both automatic route description and on general noun phrase (np) generation, but few projects consider extra-linguistic information as part of the context that influences dialog behavior.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4414">
<title id=" W06-1412.xml">noun phrase generation for situated dialogs </title>
<section> generation for situated tasks.  </section>
<citcontext>
<prevsection>
<prevsent>(gundel et al, 1993; prince,1981; grosz et al, 1995)).<papid> J95-2003 </papid></prevsent>
<prevsent>this variation is expressed in terms of the determiner chosen (e.g. that/a), the head noun (e.g. that/door/one), andthe presence of additional modifiers such as pre nominal adjectives or prepositional phrases.</prevsent>
</prevsection>
<citsent citstr=" C92-1038 ">
in natural language generation, the process of generating referring expressions occurs in stages (reiter and dale, 1992).<papid> C92-1038 </papid></citsent>
<aftsection>
<nextsent>the process we explore in this paper is the sentence planning stage, which determines whether the context supports generating particular referring expression as pronoun, description, one-anaphor, etc.there has been extensive research in both automatic route description and on general noun phrase (np) generation, but few projects consider extra-linguistic information as part of the context that influences dialog behavior.
</nextsent>
<nextsent>(poesio et al,1999) applies statistical techniques for the problem of np generation.
</nextsent>
<nextsent>however, even though the corpus used in that study contains descriptions of museum items visually accessible to the user, the features used in generation were mostly linguistic, and included little information about the visual or spatial properties of the referent.
</nextsent>
<nextsent>another related study in statistical np generation (cheng et al., 2001) <papid> N01-1002 </papid>focuses on choosing the modifiers to beincluded.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4415">
<title id=" W06-1412.xml">noun phrase generation for situated dialogs </title>
<section> generation for situated tasks.  </section>
<citcontext>
<prevsection>
<prevsent>(poesio et al,1999) applies statistical techniques for the problem of np generation.
</prevsent>
<prevsent>however, even though the corpus used in that study contains descriptions of museum items visually accessible to the user, the features used in generation were mostly linguistic, and included little information about the visual or spatial properties of the referent.
</prevsent>
</prevsection>
<citsent citstr=" N01-1002 ">
another related study in statistical np generation (cheng et al., 2001) <papid> N01-1002 </papid>focuses on choosing the modifiers to beincluded.</citsent>
<aftsection>
<nextsent>again, no features derived from the situated world were used in that study.
</nextsent>
<nextsent>(maass et al,1995) use features from the world, including ob jects?
</nextsent>
<nextsent>color, height, width, and visibility, as well asthe users direction of travel and distance from objects, for generating instructions in situated task.
</nextsent>
<nextsent>however, their focus is on selecting landmarks and descriptions under time pressure, rather than selecting the linguistic form to be produced.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4416">
<title id=" W06-1412.xml">noun phrase generation for situated dialogs </title>
<section> mod.  </section>
<citcontext>
<prevsection>
<prevsent>the most frequent tag gives 20.0% baseline performance using this strict match criterion.
</prevsent>
<prevsent>exact match results predicted all three features det mod head correct 31% 48% 72% 56% exact match: head feature per value predicted noun it none one that correct 65% 64% 0% 30% 38% exact match: det feature per value predicted none that the correct 0% 49% 36% 66% table 4: classifier results using exact-match criterion 5.2 comparison to centering.
</prevsent>
</prevsection>
<citsent citstr=" W00-1411 ">
for purposes of comparing the performance of our generation algorithm to existing work on generation of nps, we performed manual evaluation of the centering-style generation algorithm described in (kibble and power, 2000) <papid> W00-1411 </papid>against our dialog corpus.</citsent>
<aftsection>
<nextsent>algorithms developed according to the centering framework use discourse coherence to make decisions about pronominal ization (grosz et al., 1995), <papid> J95-2003 </papid>where coherence is measured in terms of topical continuity from one sentence to the next.centering designates the backward-looking center (cb) as the item in the current sentence thatwas most topical in the previous sentence.</nextsent>
<nextsent>therefore, to perform centering-style evaluation, the dialogs must be broken into sentence-like units, and ranking procedure must be devised for the items mentioned in each unit.the current evaluation corpus, being spoken dialog, has not been parsed to automatically determine the syntactic or dependency structure, but rather was manually segmented into utterance units, where each unit contained main predicate and its satellites.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4418">
<title id=" W06-1412.xml">noun phrase generation for situated dialogs </title>
<section> mod.  </section>
<citcontext>
<prevsection>
<prevsent>algorithms developed according to the centering framework use discourse coherence to make decisions about pronominal ization (grosz et al., 1995), <papid> J95-2003 </papid>where coherence is measured in terms of topical continuity from one sentence to the next.centering designates the backward-looking center (cb) as the item in the current sentence thatwas most topical in the previous sentence.</prevsent>
<prevsent>therefore, to perform centering-style evaluation, the dialogs must be broken into sentence-like units, and ranking procedure must be devised for the items mentioned in each unit.the current evaluation corpus, being spoken dialog, has not been parsed to automatically determine the syntactic or dependency structure, but rather was manually segmented into utterance units, where each unit contained main predicate and its satellites.</prevsent>
</prevsection>
<citsent citstr=" P98-2241 ">
the items mentioned in each unit were ranked according to thematic roles, using the ranking {agent   patient   comp   ad junct}, and excluding references to the speakers themselves, which often appear in agent position (byron and stent, 1998).<papid> P98-2241 </papid></citsent>
<aftsection>
<nextsent>the cb in each unit, if there is one, is the highest-ranked item from the prior units list that is repeated in the current units list.
</nextsent>
<nextsent>following procedure similar to that reported by kibble and power, our decision procedure recommends pronominalizing an item if it is the cb of its unit and if it is in subject position, otherwise description is generated.
</nextsent>
<nextsent>based on this rule, all items that are being mentioned for the first timein the discourse are predicted to require descrip tion.although most prior studies take the recommendation to pronominalize to mean that personal pronoun (e.g. it) should be generated, dueto the demonstrative nature of our domain, the decision to produce pronoun can result in either demonstrative or personal pronoun.
</nextsent>
<nextsent>therefore,we considered the algorithms output to match human production when the target expression in the human corpus was either personal or demonstrative pronoun, and the algorithm generated either category of pronoun.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4420">
<title id=" W06-3005.xml">a data driven approach to relevancy recognition for contextual question answering </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we refer to this procedure as relevancyrecognition.
</prevsent>
<prevsent>if question is recognized as follow up question, the next step is to make use of context information to interpret it and retrieve the answer.we refer to this procedure as context information fusion.
</prevsent>
</prevsection>
<citsent citstr=" P94-1002 ">
relevancy recognition is similar to text segmentation (hearst, 1994), <papid> P94-1002 </papid>but relevancy recognition focuses on the current question with the previous text while text segmentation has the full text available and is allowed to look ahead.de boni and manandhar (2005) developed rule based algorithm for relevancy recognition.</citsent>
<aftsection>
<nextsent>their rules were manually deduced by carefully analyzing the trec 2001 qa data.
</nextsent>
<nextsent>for example, if question has no verbs, it is follow-up question.
</nextsent>
<nextsent>this rule based algorithm achieves 81% inaccuracy when recognizing the question relevance in the trec 2001 qa dataset.
</nextsent>
<nextsent>the disadvantage of this approach isthat it involves good deal of human effort to research on specific dataset and summarize the rules.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4421">
<title id=" W06-3005.xml">a data driven approach to relevancy recognition for contextual question answering </title>
<section> data driven approach.  </section>
<citcontext>
<prevsection>
<prevsent>syntactic features capture whether question has certain syntactic components, such as verbs or pronouns.
</prevsent>
<prevsent>semantic features characterize the semantic similarity between the current question and previous questions.
</prevsent>
</prevsection>
<citsent citstr=" P02-1022 ">
3.2.1 syntactic features as the first step, we tagged each question with part-of-speech tags using gate (cunningham et al , 2002), <papid> P02-1022 </papid>software tool set for text engineering.</citsent>
<aftsection>
<nextsent>wethen extracted the following binary syntactic fea tures: pronoun: whether the question has pronoun or not.
</nextsent>
<nextsent>a more useful feature would be to label whether pronoun refers to an entity in the previous questions or in the current question.however, the performances of currently available tools for anaphora resolution are quite limited for our task.
</nextsent>
<nextsent>the tools we tried, including gate (cunningham et al , 2002), <papid> P02-1022 </papid>ling pipe (http://www.alias-i.com/lingpipe/) and javarap (qiu et al , 2004), tend to usethe nearest noun phrase as the referents for pro nouns.</nextsent>
<nextsent>while in the trec questions, pronouns tend to refer to the topic words (focus).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4425">
<title id=" W06-3005.xml">a data driven approach to relevancy recognition for contextual question answering </title>
<section> data driven approach.  </section>
<citcontext>
<prevsection>
<prevsent>it removes the bias towards long sentences by eliminating the accumulating effect; but on the other hand, it might cause the system to miss related question, forex ample, when two related sentences have only one key word in common.1 formula (2) shows that sentence level similarity depends on word-word similarity.
</prevsent>
<prevsent>researchers have proposed variety of ways in measuring the semantic similarity or relatedness between two words (tobe exact, word senses) based on wordnet.
</prevsent>
</prevsection>
<citsent citstr=" P94-1019 ">
forex ample, the path (path) measure is the inverse of the shortest path length between two word senses in wordnet; the wu and palmers (wup) measure(wu and palmer, 1994) <papid> P94-1019 </papid>is to find the most specific concept that two word senses share as ancestor (least common subsumer), and then scale thepath length of this concept to the root note (sup posed that there is virtual root note in wordnet) by the sum of the path lengths of the individual word sense to the root node; the lins (lin) measure (lin, 1998) is based on information content, which is corpus based measure of the specificity of word; the vector (vector) measure associates each word with gloss vector and calculates the similarity of two words as the cosine between their gloss vectors (patwardhan, 2003).</citsent>
<aftsection>
<nextsent>it was unclear which measure(s) would contribute the best information tothe task of relevancy recognition, so we just experimented on all four measures, path, wup, lin, and vector, in our decision tree training.
</nextsent>
<nextsent>we used pedersen et al (2004) tool wordnet::similarity to compute these four measures.
</nextsent>
<nextsent>wordnet::similarity implements nine different measures of word similarity.
</nextsent>
<nextsent>we here only used the four described above be cause they return value between 0 and 1, whichis suitable for using formula (2) to calculate sentence similarity, and we leave others as future work.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4426">
<title id=" W06-2202.xml">simple information extraction sie a portable and effective ie system </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>the fraction of positive examples varies from about 1% to 7http://www.cnts.ua.ac.be/conll2002/ ner/, http://www.cnts.ua.ac.be/conll2003/ ner/.
</prevsent>
<prevsent>8http://timex2.mitre.org/tern.html.
</prevsent>
</prevsection>
<citsent citstr=" W04-1219 ">
13 metric  train/test p f1 0 66.4 67.0 66.7 615 cc 1 64.1/62.3 67.5 67.3 67.4 420 2.5 80.1/78.0 66.6 69.1 67.8 226 5 88.9/86.4 64.8 68.1 66.4 109 or 1 70.7/68.9 68.3 67.3 67.8 308 2.5 81.0/79.1 67.5 68.3 67.9 193 5 87.8/85.6 65.4 68.2 66.8 114 ic 1 37.3/36.9 58.5 65.7 61.9 570 2.5 38.4/38.0 56.9 65.4 60.9 558 5 39.5/38.9 55.6 65.5 60.1 552 zhou and su (2004) <papid> W04-1219 </papid>76.0 69.4 72.6 baseline 52.6 43.6 47.7 table 5: filtering rate, micro-averaged recall, precision, f1 and time for jnlpba.</citsent>
<aftsection>
<nextsent>metric  train/test p f1 0 73.6 78.7 76.1 134 cc 1 64.4/64.4 71.6 79.9 75.5 70 2.5 75.1/73.3 72.8 80.3 76.4 50 5 88.6/84.2 66.6 64.7 65.6 24 or 1 71.5/71.6 72.0 78.3 75.0 61 2.5 82.1/80.7 73.6 78.9 76.2 39 5 90.5/86.1 66.8 64.5 65.6 19 ic 1 47.3/47.5 67.0 79.2 72.6 101 2.5 51.3/51.5 65.9 79.3 72.0 95 5 55.7/56.0 63.8 78.9 70.5 89 carreras et al (2002) <papid> W02-2004 </papid>76.3 77.8 77.1 baseline 45.4 81.3 58.3 table 6: filtering rate, micro-averaged recall, precision, f1 and total computation time for conll-2002 (dutch).about 2%.</nextsent>
<nextsent>the entire document collection is randomly partitioned five times into two sets of equal size, training and test (lavelli et al, 2004).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4427">
<title id=" W06-2202.xml">simple information extraction sie a portable and effective ie system </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>8http://timex2.mitre.org/tern.html.
</prevsent>
<prevsent>13 metric  train/test p f1 0 66.4 67.0 66.7 615 cc 1 64.1/62.3 67.5 67.3 67.4 420 2.5 80.1/78.0 66.6 69.1 67.8 226 5 88.9/86.4 64.8 68.1 66.4 109 or 1 70.7/68.9 68.3 67.3 67.8 308 2.5 81.0/79.1 67.5 68.3 67.9 193 5 87.8/85.6 65.4 68.2 66.8 114 ic 1 37.3/36.9 58.5 65.7 61.9 570 2.5 38.4/38.0 56.9 65.4 60.9 558 5 39.5/38.9 55.6 65.5 60.1 552 zhou and su (2004) <papid> W04-1219 </papid>76.0 69.4 72.6 baseline 52.6 43.6 47.7 table 5: filtering rate, micro-averaged recall, precision, f1 and time for jnlpba.</prevsent>
</prevsection>
<citsent citstr=" W02-2004 ">
metric  train/test p f1 0 73.6 78.7 76.1 134 cc 1 64.4/64.4 71.6 79.9 75.5 70 2.5 75.1/73.3 72.8 80.3 76.4 50 5 88.6/84.2 66.6 64.7 65.6 24 or 1 71.5/71.6 72.0 78.3 75.0 61 2.5 82.1/80.7 73.6 78.9 76.2 39 5 90.5/86.1 66.8 64.5 65.6 19 ic 1 47.3/47.5 67.0 79.2 72.6 101 2.5 51.3/51.5 65.9 79.3 72.0 95 5 55.7/56.0 63.8 78.9 70.5 89 carreras et al (2002) <papid> W02-2004 </papid>76.3 77.8 77.1 baseline 45.4 81.3 58.3 table 6: filtering rate, micro-averaged recall, precision, f1 and total computation time for conll-2002 (dutch).about 2%.</citsent>
<aftsection>
<nextsent>the entire document collection is randomly partitioned five times into two sets of equal size, training and test (lavelli et al, 2004).
</nextsent>
<nextsent>foreach partition, learning is performed on the training set and performance is measured on the corresponding test set.
</nextsent>
<nextsent>the resulting figures are averaged over the five test partitions.
</nextsent>
<nextsent>7.2 results.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4428">
<title id=" W06-2202.xml">simple information extraction sie a portable and effective ie system </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>to show the differences among filtering strategies for jnlpba, conll-2002, tern 2004 we used cc, or and ic filters, while the results forsa and conll-2003 are reported only for or filter (which usually produces the best performance).for all filters we report results obtained by setting four different values for parameter , the maximum value allowed for the filtering rate of positive examples.
</prevsent>
<prevsent> = 0 means that no filter is used.
</prevsent>
</prevsection>
<citsent citstr=" W03-0425 ">
metric  train/test p f1 0 76.7 90.5 83.1 228 or 1 70.4/83.9 78.2 88.1 82.8 74 2.5 83.6/95.6 76.4 62.6 68.8 33 5 90.5/97.2 75.3 66.5 70.7 14 florian et al (2003) <papid> W03-0425 </papid>88.5 89.0 88.8 baseline 50.9 71.9 59.6 table 7: filtering rate, micro-averaged recall, precision, f1 and total computation time for conll-2003 (english).</citsent>
<aftsection>
<nextsent>metric  train/test p f1 0 77.9 89.8 83.4 82 cc 1 41.8/41.2 76.6 90.7 83.1 57 2.5 64.5/62.8 60.3 88.6 71.7 41 5 86.9/81.7 59.7 76.0 66.9 14 or 1 56.4/54.6 77.5 91.1 83.8 48 2.5 69.4/66.7 59.8 88.1 71.2 36 5 82.9/79.0 59.5 88.6 71.2 20 ic 1 17.8/17.4 74.9 91.2 82.3 48 2.5 24.0/23.3 74.8 91.5 82.3 36 5 27.6/27.1 75.0 91.5 82.5 20 table 8: filtering rate, micro-averaged recall, precision, f1 and total computation time for tern.the results indicate that both cc and or do exhibit good performance and are far better than ic in all the tasks.
</nextsent>
<nextsent>for example, in the jnlpba dataset, or allows to remove more than 70% of the instances, losing less than 1% of the positive examples.
</nextsent>
<nextsent>these results pinpoint the importance of using supervised metric to collect stop words.
</nextsent>
<nextsent>the results also highlight that both cc and or are robust against over fitting, because the difference between the filtering rates in the training and test sets is minimal.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4431">
<title id=" W06-1808.xml">numerical data integration for cooperative question answering </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>1.1 related works.
</prevsent>
<prevsent>most of existing systems on the web produce aset of answers to question in the form of hyper links or page extracts, ranked according to relevance score.
</prevsent>
</prevsection>
<citsent citstr=" N03-1022 ">
for example, cogex (moldovan et al, 2003) <papid> N03-1022 </papid>uses its logic prover to extract lexical relationships between the question and its candidate answers.</citsent>
<aftsection>
<nextsent>the answers are then ranked basedon their proof scores.
</nextsent>
<nextsent>other systems define relationships between web page extracts or texts containing possible answers: for example, (radev and mckeown, 1998) <papid> J98-3005 </papid>and (harabagiu and lacatusu, 2004) <papid> W04-2501 </papid>define agreement (when two sources report the same information), addition (when second source reports additional information), contradiction (when two sources report conflicting informa tion), etc. these relations can be classified into the 4 relations defined by (webber et al, 2002),i.e. inclusion, equivalence, aggregation and alternative which we present below.most question-answering systems provide answers which take into account neither information given by all candidate answers nor their inconsistency.</nextsent>
<nextsent>this is the point we focus on in the following section.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4432">
<title id=" W06-1808.xml">numerical data integration for cooperative question answering </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for example, cogex (moldovan et al, 2003) <papid> N03-1022 </papid>uses its logic prover to extract lexical relationships between the question and its candidate answers.</prevsent>
<prevsent>the answers are then ranked basedon their proof scores.</prevsent>
</prevsection>
<citsent citstr=" J98-3005 ">
other systems define relationships between web page extracts or texts containing possible answers: for example, (radev and mckeown, 1998) <papid> J98-3005 </papid>and (harabagiu and lacatusu, 2004) <papid> W04-2501 </papid>define agreement (when two sources report the same information), addition (when second source reports additional information), contradiction (when two sources report conflicting informa tion), etc. these relations can be classified into the 4 relations defined by (webber et al, 2002),i.e. inclusion, equivalence, aggregation and alternative which we present below.most question-answering systems provide answers which take into account neither information given by all candidate answers nor their inconsistency.</citsent>
<aftsection>
<nextsent>this is the point we focus on in the following section.
</nextsent>
<nextsent>1.2 general typology of integration mechanisms to better characterize our problem, we collected corpus of about 100 question-answer pairs in french that reflect different inconsistency problems (most of pairs are obtained via google orqristal1).
</nextsent>
<nextsent>we first assume that all candidate answers obtained via an extraction engine are potentially correct, i.e. they are of the semantic type expected by the question.
</nextsent>
<nextsent>1www.qristal.fr, synapse developpement.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4433">
<title id=" W06-1808.xml">numerical data integration for cooperative question answering </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for example, cogex (moldovan et al, 2003) <papid> N03-1022 </papid>uses its logic prover to extract lexical relationships between the question and its candidate answers.</prevsent>
<prevsent>the answers are then ranked basedon their proof scores.</prevsent>
</prevsection>
<citsent citstr=" W04-2501 ">
other systems define relationships between web page extracts or texts containing possible answers: for example, (radev and mckeown, 1998) <papid> J98-3005 </papid>and (harabagiu and lacatusu, 2004) <papid> W04-2501 </papid>define agreement (when two sources report the same information), addition (when second source reports additional information), contradiction (when two sources report conflicting informa tion), etc. these relations can be classified into the 4 relations defined by (webber et al, 2002),i.e. inclusion, equivalence, aggregation and alternative which we present below.most question-answering systems provide answers which take into account neither information given by all candidate answers nor their inconsistency.</citsent>
<aftsection>
<nextsent>this is the point we focus on in the following section.
</nextsent>
<nextsent>1.2 general typology of integration mechanisms to better characterize our problem, we collected corpus of about 100 question-answer pairs in french that reflect different inconsistency problems (most of pairs are obtained via google orqristal1).
</nextsent>
<nextsent>we first assume that all candidate answers obtained via an extraction engine are potentially correct, i.e. they are of the semantic type expected by the question.
</nextsent>
<nextsent>1www.qristal.fr, synapse developpement.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4434">
<title id=" W06-1808.xml">numerical data integration for cooperative question answering </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for example, if the question when does autumn begin?
</prevsent>
<prevsent>has the candidate answers autumn begins on september 21st and autumn begins on september 20th, an answer such as autumn begins on either september 20th or september 21st can be proposed.
</prevsent>
</prevsection>
<citsent citstr=" W05-1625 ">
(moriceau, 2005) <papid> W05-1625 </papid>proposes an integration method for answers of type date.(2) if candidate answers have common characteristics, it is possible to integrate them according to these characteristics (greatest common denom inator?).</citsent>
<aftsection>
<nextsent>for example, the question when does thefete de la musique?
</nextsent>
<nextsent>take place?
</nextsent>
<nextsent>has the following answers june 1st 1982, june 21st 1983, ...,june 21st 2005.
</nextsent>
<nextsent>here, the extraction engine selects pages containing the dates of music festivals over the years.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4436">
<title id=" W06-1624.xml">a weakly supervised learning approach for spoken language understanding </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>spoken language understanding (slu) is one of the key components in spoken dialogue systems.
</prevsent>
<prevsent>its task is to identify the users goal and extract from the input utterance the information needed to complete the query.
</prevsent>
</prevsection>
<citsent citstr=" P93-1008 ">
traditionally, there are mainly two mainstreams in the slu researches: knowledge-based approaches, which are based on robust parsing or template matching techniques (sneff, 1992; dowding et al, 1993; <papid> P93-1008 </papid>ward and issar, 1994); <papid> H94-1039 </papid>and data-driven approaches, which are generally based on stochastic models (pieraccini and levin, 1993; miller et al, 1995).</citsent>
<aftsection>
<nextsent>both approaches have their drawbacks, however.
</nextsent>
<nextsent>the former approach is cost-expensive to develop since its grammar development is time consuming, labour some and requires linguistic skills.
</nextsent>
<nextsent>it is also strictly domain-dependent and hence difficult to be adapted to new domains.
</nextsent>
<nextsent>on the other hand, although addressing such drawbacks associated with knowledge-based approaches, the latter approach often suffers the data sparseness problem and hence needs fully annotated corpus in order to reliably estimate an accurate model.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4437">
<title id=" W06-1624.xml">a weakly supervised learning approach for spoken language understanding </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>spoken language understanding (slu) is one of the key components in spoken dialogue systems.
</prevsent>
<prevsent>its task is to identify the users goal and extract from the input utterance the information needed to complete the query.
</prevsent>
</prevsection>
<citsent citstr=" H94-1039 ">
traditionally, there are mainly two mainstreams in the slu researches: knowledge-based approaches, which are based on robust parsing or template matching techniques (sneff, 1992; dowding et al, 1993; <papid> P93-1008 </papid>ward and issar, 1994); <papid> H94-1039 </papid>and data-driven approaches, which are generally based on stochastic models (pieraccini and levin, 1993; miller et al, 1995).</citsent>
<aftsection>
<nextsent>both approaches have their drawbacks, however.
</nextsent>
<nextsent>the former approach is cost-expensive to develop since its grammar development is time consuming, labour some and requires linguistic skills.
</nextsent>
<nextsent>it is also strictly domain-dependent and hence difficult to be adapted to new domains.
</nextsent>
<nextsent>on the other hand, although addressing such drawbacks associated with knowledge-based approaches, the latter approach often suffers the data sparseness problem and hence needs fully annotated corpus in order to reliably estimate an accurate model.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4438">
<title id=" W06-1624.xml">a weakly supervised learning approach for spoken language understanding </title>
<section> the system architecture.  </section>
<citcontext>
<prevsection>
<prevsent>the first matched feature is applied to make predication.
</prevsent>
<prevsent>obviously, how to measure the confidence of features is very important issue for the decision list.
</prevsent>
</prevsection>
<citsent citstr=" P94-1013 ">
we use the metric described in (yarowsky, 1994; <papid> P94-1013 </papid>golding, 1995).<papid> W95-0104 </papid></citsent>
<aftsection>
<nextsent>provided that 1( | ) 0p f   for all : ( ) max ( | )iiconfidence p f= (1) this value measures the extent to which the context is unambiguously correlated with one particular slot is . 2.4 slot-value merging and semantic re-.
</nextsent>
<nextsent>classification the slot-value merger is to combine the slots assigned to the concepts in an input sentence.
</nextsent>
<nextsent>another simultaneous task of the slot-value merger is to check the consistency among the identified slot-values.
</nextsent>
<nextsent>since the topic-dependent classifiers corresponding to different concepts frame: show route slots: [route].[origin].[location].( the peoples square) [route].[destination].[location].(the bund) [route].[transport_type].[by_bus].(bus) 201are training and running independently, it possibly results in inconsistent predictions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4439">
<title id=" W06-1624.xml">a weakly supervised learning approach for spoken language understanding </title>
<section> the system architecture.  </section>
<citcontext>
<prevsection>
<prevsent>the first matched feature is applied to make predication.
</prevsent>
<prevsent>obviously, how to measure the confidence of features is very important issue for the decision list.
</prevsent>
</prevsection>
<citsent citstr=" W95-0104 ">
we use the metric described in (yarowsky, 1994; <papid> P94-1013 </papid>golding, 1995).<papid> W95-0104 </papid></citsent>
<aftsection>
<nextsent>provided that 1( | ) 0p f   for all : ( ) max ( | )iiconfidence p f= (1) this value measures the extent to which the context is unambiguously correlated with one particular slot is . 2.4 slot-value merging and semantic re-.
</nextsent>
<nextsent>classification the slot-value merger is to combine the slots assigned to the concepts in an input sentence.
</nextsent>
<nextsent>another simultaneous task of the slot-value merger is to check the consistency among the identified slot-values.
</nextsent>
<nextsent>since the topic-dependent classifiers corresponding to different concepts frame: show route slots: [route].[origin].[location].( the peoples square) [route].[destination].[location].(the bund) [route].[transport_type].[by_bus].(bus) 201are training and running independently, it possibly results in inconsistent predictions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4441">
<title id=" W06-1624.xml">a weakly supervised learning approach for spoken language understanding </title>
<section> weakly supervised training of the.  </section>
<citcontext>
<prevsection>
<prevsent>3.1 combining active learning and self-.
</prevsent>
<prevsent>training for topic classification we employ the strategy of combining active learning and self-training for training the topic classifier, which was firstly proposed in (tur et al., 2005) and applied to similar task.
</prevsent>
</prevsection>
<citsent citstr=" P02-1016 ">
one way to reduce the number of labeling examples is active learning, which have been applied in many domains (mccallum and nigam, 1998; tang et al, 2002; <papid> P02-1016 </papid>tur et al, 2005).</citsent>
<aftsection>
<nextsent>usually, the classifier is trained by randomly sampling the training examples.
</nextsent>
<nextsent>however, inactive learning, the classifier is trained by selectively sampling the training examples (cohn et al, 1994).
</nextsent>
<nextsent>the basic idea is that the most informative ones are selected from the unlabeled examples for human to label.
</nextsent>
<nextsent>that is to say, this strategy tries to always select the examples, which will have the largest improvement on performance, and hence minimizes the human labeling effort whilst keeping performance (tur et al, 2005).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4442">
<title id=" W06-1624.xml">a weakly supervised learning approach for spoken language understanding </title>
<section> weakly supervised training of the.  </section>
<citcontext>
<prevsection>
<prevsent>(a) get unlabeled sentences from us (b) apply the current classifier to unlabeled sentences (c) select examples which are most informative to the current classifier and manually label the selected examples (d) add the human-labeled examples and the remaining m?
</prevsent>
<prevsent>machine labeled examples to the training set ts (e) train new classifier on all labeled examples 3.2 bootstrapping the topic-dependent.
</prevsent>
</prevsection>
<citsent citstr=" P02-1046 ">
semantic classifiers bootstrapping refers to problem of inducing classifier given small set of labeled data and large set of unlabeled data (abney, 2002).<papid> P02-1046 </papid></citsent>
<aftsection>
<nextsent>it has been applied to problems such as word-sense disambiguation (yarowsky, 1995), web-page classification (blum and mitchell, 1998), named entity recognition (collins and singer, 1999) <papid> W99-0613 </papid>and automatic construction of semantic lexicon (thelen and riloff, 2003).</nextsent>
<nextsent>the key to the bootstrapping methods is to exploit the redundancy in the unlabeled data (collins and singer, 1999).<papid> W99-0613 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4443">
<title id=" W06-1624.xml">a weakly supervised learning approach for spoken language understanding </title>
<section> weakly supervised training of the.  </section>
<citcontext>
<prevsection>
<prevsent>machine labeled examples to the training set ts (e) train new classifier on all labeled examples 3.2 bootstrapping the topic-dependent.
</prevsent>
<prevsent>semantic classifiers bootstrapping refers to problem of inducing classifier given small set of labeled data and large set of unlabeled data (abney, 2002).<papid> P02-1046 </papid></prevsent>
</prevsection>
<citsent citstr=" W99-0613 ">
it has been applied to problems such as word-sense disambiguation (yarowsky, 1995), web-page classification (blum and mitchell, 1998), named entity recognition (collins and singer, 1999) <papid> W99-0613 </papid>and automatic construction of semantic lexicon (thelen and riloff, 2003).</citsent>
<aftsection>
<nextsent>the key to the bootstrapping methods is to exploit the redundancy in the unlabeled data (collins and singer, 1999).<papid> W99-0613 </papid></nextsent>
<nextsent>thus, many language processing problems can be dealt using the bootstrapping methods since language is highly redundant (yarowsky, 1995).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4445">
<title id=" W06-3802.xml">graph based semi supervised approach for information extraction </title>
<section> experimental setup.  </section>
<citcontext>
<prevsection>
<prevsent>to reduce the space of tuples, we seek matching criterion that group similar tuples together.
</prevsent>
<prevsent>using wordnet, we can measure the semantic similarity or relatedness between pair of concepts (or word senses), and by extension, between pair of sentences.
</prevsent>
</prevsection>
<citsent citstr=" P94-1019 ">
we use the similarity measure described in (wu and palmer, 1994) <papid> P94-1019 </papid>which finds the path length to the root node from the least common subsumer (lcs) of the two word senses which is the most specific word sense they share as an ancestor.</citsent>
<aftsection>
<nextsent>the similarity score of two tuples, st, is calculated as follows:.
</nextsent>
<nextsent>2 2 2 1 eet sss += (9).
</nextsent>
<nextsent>where se1, and se2 are the similarity scores of the first entities in the two tuples, and their second entitles respectively.
</nextsent>
<nextsent>the tuple matching procedure assigns similarity measure to each pair of tuples in the dataset.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4446">
<title id=" W06-1670.xml">broad coverage sense disambiguation and information extraction with a super sense sequence tagger </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>research.
</prevsent>
<prevsent>the tagger described in this paper is free software and can be downloaded from http://www.loa-cnr.it/ciaramita.html.
</prevsent>
</prevsection>
<citsent citstr=" W02-2004 ">
80s (carreras et al , 2002; <papid> W02-2004 </papid>florian et al , 2003), <papid> W03-0425 </papid>while bio-ner accuracy ranges between the low 70s and 80s, depending on the data-set used for training/evaluation (dingare et al , 2005).</citsent>
<aftsection>
<nextsent>one shortcoming of ner is its over-simplified onto logical model, leaving instances of other potentially informative categories unidentified.
</nextsent>
<nextsent>hence, the utility of named entity information is limited.in addition, instances to be detected are mainly restricted to (sequences of) proper nouns.
</nextsent>
<nextsent>word sense disambiguation (wsd) is the task of deciding the intended sense for ambiguous words in context.
</nextsent>
<nextsent>with respect toner, wsdlies at the other end of the semantic tagging spectrum, since the dictionary defines tens of thousand of very specific word senses, including ner categories.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4447">
<title id=" W06-1670.xml">broad coverage sense disambiguation and information extraction with a super sense sequence tagger </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>research.
</prevsent>
<prevsent>the tagger described in this paper is free software and can be downloaded from http://www.loa-cnr.it/ciaramita.html.
</prevsent>
</prevsection>
<citsent citstr=" W03-0425 ">
80s (carreras et al , 2002; <papid> W02-2004 </papid>florian et al , 2003), <papid> W03-0425 </papid>while bio-ner accuracy ranges between the low 70s and 80s, depending on the data-set used for training/evaluation (dingare et al , 2005).</citsent>
<aftsection>
<nextsent>one shortcoming of ner is its over-simplified onto logical model, leaving instances of other potentially informative categories unidentified.
</nextsent>
<nextsent>hence, the utility of named entity information is limited.in addition, instances to be detected are mainly restricted to (sequences of) proper nouns.
</nextsent>
<nextsent>word sense disambiguation (wsd) is the task of deciding the intended sense for ambiguous words in context.
</nextsent>
<nextsent>with respect toner, wsdlies at the other end of the semantic tagging spectrum, since the dictionary defines tens of thousand of very specific word senses, including ner categories.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4448">
<title id=" W06-1670.xml">broad coverage sense disambiguation and information extraction with a super sense sequence tagger </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our goal is to simplify the disambiguation task, for both nouns and verbs, to level at which it can be approached as any other tagging problem, and can be solved with state of the art methods.
</prevsent>
<prevsent>as by-product, this task includes and extends ner.
</prevsent>
</prevsection>
<citsent citstr=" W03-1022 ">
we define tagset based on wordnets lexicographers classes, or super senses (cia ramita and johnson, 2003), <papid> W03-1022 </papid>cf.</citsent>
<aftsection>
<nextsent>table 1.
</nextsent>
<nextsent>the sizeof the super sense tagset al ows us to adopt structured learning approach, which takes local dependencies between labels into account.
</nextsent>
<nextsent>to this extent, we cast the super sense tagging problem as sequence labeling task and train discriminative hidden markov model (hmm), based on that of collins (2002), <papid> W02-1001 </papid>on the manually annotated semcor corpus (miller et al , 1993).<papid> H93-1061 </papid></nextsent>
<nextsent>in two experiment swe evaluate the accuracy of the tagger on the sem cor corpus itself, and on the english all words?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4449">
<title id=" W06-1670.xml">broad coverage sense disambiguation and information extraction with a super sense sequence tagger </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>table 1.
</prevsent>
<prevsent>the sizeof the super sense tagset al ows us to adopt structured learning approach, which takes local dependencies between labels into account.
</prevsent>
</prevsection>
<citsent citstr=" W02-1001 ">
to this extent, we cast the super sense tagging problem as sequence labeling task and train discriminative hidden markov model (hmm), based on that of collins (2002), <papid> W02-1001 </papid>on the manually annotated semcor corpus (miller et al , 1993).<papid> H93-1061 </papid></citsent>
<aftsection>
<nextsent>in two experiment swe evaluate the accuracy of the tagger on the sem cor corpus itself, and on the english all words?
</nextsent>
<nextsent>senseval 3 shared task data (snyder and palmer, 2004).<papid> W04-0811 </papid></nextsent>
<nextsent>the model outperforms remarkably the best known baseline, the first sense heuristic ? to the best of our knowledge, for the first time on the most realistic all words?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4450">
<title id=" W06-1670.xml">broad coverage sense disambiguation and information extraction with a super sense sequence tagger </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>table 1.
</prevsent>
<prevsent>the sizeof the super sense tagset al ows us to adopt structured learning approach, which takes local dependencies between labels into account.
</prevsent>
</prevsection>
<citsent citstr=" H93-1061 ">
to this extent, we cast the super sense tagging problem as sequence labeling task and train discriminative hidden markov model (hmm), based on that of collins (2002), <papid> W02-1001 </papid>on the manually annotated semcor corpus (miller et al , 1993).<papid> H93-1061 </papid></citsent>
<aftsection>
<nextsent>in two experiment swe evaluate the accuracy of the tagger on the sem cor corpus itself, and on the english all words?
</nextsent>
<nextsent>senseval 3 shared task data (snyder and palmer, 2004).<papid> W04-0811 </papid></nextsent>
<nextsent>the model outperforms remarkably the best known baseline, the first sense heuristic ? to the best of our knowledge, for the first time on the most realistic all words?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4451">
<title id=" W06-1670.xml">broad coverage sense disambiguation and information extraction with a super sense sequence tagger </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>to this extent, we cast the super sense tagging problem as sequence labeling task and train discriminative hidden markov model (hmm), based on that of collins (2002), <papid> W02-1001 </papid>on the manually annotated semcor corpus (miller et al , 1993).<papid> H93-1061 </papid></prevsent>
<prevsent>in two experiment swe evaluate the accuracy of the tagger on the sem cor corpus itself, and on the english all words?</prevsent>
</prevsection>
<citsent citstr=" W04-0811 ">
senseval 3 shared task data (snyder and palmer, 2004).<papid> W04-0811 </papid></citsent>
<aftsection>
<nextsent>the model outperforms remarkably the best known baseline, the first sense heuristic ? to the best of our knowledge, for the first time on the most realistic all words?
</nextsent>
<nextsent>evaluation setting.
</nextsent>
<nextsent>the paper is organized as follows.
</nextsent>
<nextsent>section 2 introduces the tagset, section 3 discusses related work and section 4 the learning model.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4453">
<title id=" W06-1670.xml">broad coverage sense disambiguation and information extraction with a super sense sequence tagger </title>
<section> super sense tagset.  </section>
<citcontext>
<prevsection>
<prevsent>and has wider scope.corpus ? designed for supporting information extraction in the molecular biology domain.
</prevsent>
<prevsent>in addition, there is growing interest for extracting relations between entities, as more useful type of ie (cf.
</prevsent>
</prevsection>
<citsent citstr=" P04-1055 ">
(rosario and hearst, 2004)).<papid> P04-1055 </papid>super sense tagging is inspired by similar considerations, but in domain-independent setting;e.g., verb super senses can label semantic interactions between nominal concepts.</citsent>
<aftsection>
<nextsent>the following sentence (example 1), extracted from the data further described in section 5.1 ? shows the information captured by the super sense tagset: (1) clara harrisn.person, one of the guestsn.person in the boxn.artifact, stood upv.motion and demandedv.communication watern.substance.
</nextsent>
<nextsent>as example 1 shows there is more information that can be extracted from sentence than just the names; e.g. the fact that clara harris?
</nextsent>
<nextsent>and the following guests?
</nextsent>
<nextsent>are both tagged as person?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4454">
<title id=" W06-1670.xml">broad coverage sense disambiguation and information extraction with a super sense sequence tagger </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>sequential models are common inner, pos tagging, shallow parsing, etc..
</prevsent>
<prevsent>most of the work in wsd, instead, has focused on labeling each word individually, possibly revising the assignments of senses at the document level; e.g., following the one sense per discourse?
</prevsent>
</prevsection>
<citsent citstr=" H92-1045 ">
hypothesis (gale et al , 1992).<papid> H92-1045 </papid></citsent>
<aftsection>
<nextsent>although it seems reasonable to assume that occurrences of word senses in sentence canbe correlated, hence that structured learning methods could be successful, there has not been much work on sequential wsd.
</nextsent>
<nextsent>segond et al  (1997) <papid> W97-0811 </papid>are possibly the first to have applied an hmm tagger to semantic disambiguation.</nextsent>
<nextsent>interestingly, to make the method more tractable, they also used the super sense tagset and estimated the model on semcor.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4455">
<title id=" W06-1670.xml">broad coverage sense disambiguation and information extraction with a super sense sequence tagger </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>hypothesis (gale et al , 1992).<papid> H92-1045 </papid></prevsent>
<prevsent>although it seems reasonable to assume that occurrences of word senses in sentence canbe correlated, hence that structured learning methods could be successful, there has not been much work on sequential wsd.</prevsent>
</prevsection>
<citsent citstr=" W97-0811 ">
segond et al  (1997) <papid> W97-0811 </papid>are possibly the first to have applied an hmm tagger to semantic disambiguation.</citsent>
<aftsection>
<nextsent>interestingly, to make the method more tractable, they also used the super sense tagset and estimated the model on semcor.
</nextsent>
<nextsent>by cross-validation they show marked improvement over the first sense baseline.
</nextsent>
<nextsent>however, in (segond et al , 1997) <papid> W97-0811 </papid>the tagset is used differently, by defining equivalence classes of words with the same set of senses.</nextsent>
<nextsent>from similar perspective, de loupy et al  (de loupy et al , 1998) 596 also investigated the potential advantages of usinghmms for disambiguation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4457">
<title id=" W06-1670.xml">broad coverage sense disambiguation and information extraction with a super sense sequence tagger </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>however, in (segond et al , 1997) <papid> W97-0811 </papid>the tagset is used differently, by defining equivalence classes of words with the same set of senses.</prevsent>
<prevsent>from similar perspective, de loupy et al  (de loupy et al , 1998) 596 also investigated the potential advantages of usinghmms for disambiguation.</prevsent>
</prevsection>
<citsent citstr=" W04-0842 ">
more recently, variants of the generative hmm have been applied to wsd (molina et al , 2002; molina et al , 2004)<papid> W04-0842 </papid>and evaluated also on senseval data, showing performance comparable to the first sense baseline.</citsent>
<aftsection>
<nextsent>previous work on prediction at the super sense level (ciaramita and johnson, 2003; <papid> W03-1022 </papid>curran, 2005)<papid> P05-1004 </papid>has focused on lexical acquisition (nouns exclu sively), thus aiming at word type classification rather than tagging.</nextsent>
<nextsent>as far as applications are concerned, it has been shown that super sense information can support supervised wsd, by providing partial disambiguation step (ciaramita et al , 2003).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4459">
<title id=" W06-1670.xml">broad coverage sense disambiguation and information extraction with a super sense sequence tagger </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>from similar perspective, de loupy et al  (de loupy et al , 1998) 596 also investigated the potential advantages of usinghmms for disambiguation.
</prevsent>
<prevsent>more recently, variants of the generative hmm have been applied to wsd (molina et al , 2002; molina et al , 2004)<papid> W04-0842 </papid>and evaluated also on senseval data, showing performance comparable to the first sense baseline.</prevsent>
</prevsection>
<citsent citstr=" P05-1004 ">
previous work on prediction at the super sense level (ciaramita and johnson, 2003; <papid> W03-1022 </papid>curran, 2005)<papid> P05-1004 </papid>has focused on lexical acquisition (nouns exclu sively), thus aiming at word type classification rather than tagging.</citsent>
<aftsection>
<nextsent>as far as applications are concerned, it has been shown that super sense information can support supervised wsd, by providing partial disambiguation step (ciaramita et al , 2003).
</nextsent>
<nextsent>in syntactic parse re-ranking supersenseshave been used to build useful latent semantic features (koo and collins, 2005).<papid> H05-1064 </papid></nextsent>
<nextsent>we believe that super sense tagging has the potential to be useful, in combination with other sources of information such as part of speech, domain-specific ner models, chunking or shallow parsing, in tasks such as question answering and information extraction and retrieval, where large amounts of text need to be processed.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4460">
<title id=" W06-1670.xml">broad coverage sense disambiguation and information extraction with a super sense sequence tagger </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>previous work on prediction at the super sense level (ciaramita and johnson, 2003; <papid> W03-1022 </papid>curran, 2005)<papid> P05-1004 </papid>has focused on lexical acquisition (nouns exclu sively), thus aiming at word type classification rather than tagging.</prevsent>
<prevsent>as far as applications are concerned, it has been shown that super sense information can support supervised wsd, by providing partial disambiguation step (ciaramita et al , 2003).</prevsent>
</prevsection>
<citsent citstr=" H05-1064 ">
in syntactic parse re-ranking supersenseshave been used to build useful latent semantic features (koo and collins, 2005).<papid> H05-1064 </papid></citsent>
<aftsection>
<nextsent>we believe that super sense tagging has the potential to be useful, in combination with other sources of information such as part of speech, domain-specific ner models, chunking or shallow parsing, in tasks such as question answering and information extraction and retrieval, where large amounts of text need to be processed.
</nextsent>
<nextsent>it is also possible that this kindof shallow semantic information can help building more sophisticated linguistic analysis as in full syntactic parsing and semantic role labeling.
</nextsent>
<nextsent>we take sequence labeling approach to learning model for super sense tagging.
</nextsent>
<nextsent>our goal isto learn function from input vectors, the observations from labeled data, to response variables,the super sense labels.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4466">
<title id=" W06-1670.xml">broad coverage sense disambiguation and information extraction with a super sense sequence tagger </title>
<section> sequence tagging.  </section>
<citcontext>
<prevsection>
<prevsent>then 6: wt+1 ? wt + ?(xi,yi)?
</prevsent>
<prevsent>?(xi, y?)
</prevsent>
</prevsection>
<citsent citstr=" N03-1028 ">
7: end if 8: = 1t ? twt 9: end for 10: return the perceptron performance is comparable to that of conditional random field models (sha and pereira, 2003), <papid> N03-1028 </papid>the tendency to overfit of the perceptron can be mitigated in number of ways including regularization and voting.</citsent>
<aftsection>
<nextsent>here we apply averaging and straightforwardly extended collins algorithm, summarized in algorithm 1.
</nextsent>
<nextsent>4.2 features.
</nextsent>
<nextsent>we used the following combination ofspelling/morphological and contextual features.
</nextsent>
<nextsent>for each observed word xi in the data ? extracts the following features: 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4469">
<title id=" W06-1670.xml">broad coverage sense disambiguation and information extraction with a super sense sequence tagger </title>
<section> previous label: super sense label yi1..  </section>
<citcontext>
<prevsection>
<prevsent>summary of results for random and first sense baselines and super sense tagger, ? is the standard error computed on the five trials results.
</prevsent>
<prevsent>altogether (cf.
</prevsent>
</prevsection>
<citsent citstr=" P04-1036 ">
(mccarthy et al , 2004)).<papid> P04-1036 </papid></citsent>
<aftsection>
<nextsent>for this reason we include the first sense prediction as one of the features of our tagging model.
</nextsent>
<nextsent>we apply the heuristic as follows.
</nextsent>
<nextsent>first, in each sentence, we identify the longest sequence which has an entry in wordnet as either noun or verb.
</nextsent>
<nextsent>we carry out this step using the wordnets library functions, which perform also morphological simplification.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4470">
<title id=" W06-1670.xml">broad coverage sense disambiguation and information extraction with a super sense sequence tagger </title>
<section> previous label: super sense label yi1..  </section>
<citcontext>
<prevsection>
<prevsent>on thesemcor data the tagger improves over the base line by 10.71%, 31.19% error reduction, whileon senseval-3 the tagger improves over the base line by 6.45%, 17.96% error reduction.
</prevsent>
<prevsent>we can put these results in context, although indirectly,by comparison with the results of the senseval3 all words task systems.
</prevsent>
</prevsection>
<citsent citstr=" W04-0838 ">
there, with baseline of 62.40%, only 4 out of 26 systems performed above the baseline, with the two best systems (mihalcea and faruque, 2004; <papid> W04-0838 </papid>decadt et al ,2004) <papid> W04-0827 </papid>achieving an f-score of 65.2% (2.8% improvement, 7.45% error reduction).</citsent>
<aftsection>
<nextsent>the system based on the hmm tagger (molina et al , 2004)<papid> W04-0842 </papid>, 6scoring was performed with re-implementation of the conlleval?</nextsent>
<nextsent>script . achieved an f-score of 60.9%.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4471">
<title id=" W06-1670.xml">broad coverage sense disambiguation and information extraction with a super sense sequence tagger </title>
<section> previous label: super sense label yi1..  </section>
<citcontext>
<prevsection>
<prevsent>on thesemcor data the tagger improves over the base line by 10.71%, 31.19% error reduction, whileon senseval-3 the tagger improves over the base line by 6.45%, 17.96% error reduction.
</prevsent>
<prevsent>we can put these results in context, although indirectly,by comparison with the results of the senseval3 all words task systems.
</prevsent>
</prevsection>
<citsent citstr=" W04-0827 ">
there, with baseline of 62.40%, only 4 out of 26 systems performed above the baseline, with the two best systems (mihalcea and faruque, 2004; <papid> W04-0838 </papid>decadt et al ,2004) <papid> W04-0827 </papid>achieving an f-score of 65.2% (2.8% improvement, 7.45% error reduction).</citsent>
<aftsection>
<nextsent>the system based on the hmm tagger (molina et al , 2004)<papid> W04-0842 </papid>, 6scoring was performed with re-implementation of the conlleval?</nextsent>
<nextsent>script . achieved an f-score of 60.9%.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4473">
<title id=" W06-1670.xml">broad coverage sense disambiguation and information extraction with a super sense sequence tagger </title>
<section> previous label: super sense label yi1..  </section>
<citcontext>
<prevsection>
<prevsent>if we compare these figures with the accuracy of ner taggers the results are very encouraging.
</prevsent>
<prevsent>given the considerably larger ? one order of magnitude ? class space some loss has to be expected.
</prevsent>
</prevsection>
<citsent citstr=" W02-0301 ">
experiments with augmented tagsets in the biomedical domain also show performance loss with respect to smallertagsets; e.g., kazama et al  (2002) <papid> W02-0301 </papid>report an score of 56.2% on tagset of 25 genia classes, compared to the 75.9% achieved on the simplest binary case.</citsent>
<aftsection>
<nextsent>the sequence fragments from semv contribute about 1% f-score improvement.
</nextsent>
<nextsent>table 5 focuses on subsets of the evaluation.the upper part summarizes the results on sem cor for the classes comparable to standard ners:person?, group?, location?
</nextsent>
<nextsent>and time?.
</nextsent>
<nextsent>however, these categories here are composed of common nouns as well as proper names/named entities.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4474">
<title id=" W07-0703.xml">integration of an arabic transliteration module into a statistical machine translation system </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>so, to improve the usability of translation, it is particularly important to handle nes well.
</prevsent>
<prevsent>the importance of nes is not yet reflected in the evaluation methods used in the mt community, the most common of which is the bleu metric.
</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
bleu (papineni et al 2002) <papid> P02-1040 </papid>was devised to provide automatic evaluation of mt output.</citsent>
<aftsection>
<nextsent>in this metric n-gram similarity of the mt output is computed with one or more references made by human 17 translators.
</nextsent>
<nextsent>bleu does not distinguish between different words and gives equal weight to all.
</nextsent>
<nextsent>in this paper, we base our evaluation on the bleu metric and show that using transliteration has impact on it (and in some cases significant impact).
</nextsent>
<nextsent>however, we believe that such integration is more important for practical uses of mt than bleu indicates.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4475">
<title id=" W07-0703.xml">integration of an arabic transliteration module into a statistical machine translation system </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in these cases, the transliteration module often suggests correct transliteration that the decoder outputs correctly, but which fails to receive credit from the bleu metric because this transliteration is not found in the references.
</prevsent>
<prevsent>as an example, for the name ?????????, four references came up with four different interpretations: swerios, swiriyus, sever ius, sweires.
</prevsent>
</prevsection>
<citsent citstr=" N06-1011 ">
a quick query in google showed us another four acceptable interpretations (severios, sewerios, sweirios, saw erios).machine transliteration has been an active research field for quite while (al-onaizan and knight, 2002; abduljaleel and larkey, 2003; klementiev and roth, 2006; <papid> N06-1011 </papid>sproat et al 2006) <papid> P06-1010 </papid>but to 18 our knowledge there is little published work on evaluating transliteration within real mt system.</citsent>
<aftsection>
<nextsent>the closest work to ours is described in (hassan and sorensen, 2005) <papid> W05-0712 </papid>where they have list of names in arabic and feed this list as the input text to their mt system.</nextsent>
<nextsent>they evaluate their system in three different cases: as word-based ne translation, phrase-based ne translation and in presence of transliteration module.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4476">
<title id=" W07-0703.xml">integration of an arabic transliteration module into a statistical machine translation system </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in these cases, the transliteration module often suggests correct transliteration that the decoder outputs correctly, but which fails to receive credit from the bleu metric because this transliteration is not found in the references.
</prevsent>
<prevsent>as an example, for the name ?????????, four references came up with four different interpretations: swerios, swiriyus, sever ius, sweires.
</prevsent>
</prevsection>
<citsent citstr=" P06-1010 ">
a quick query in google showed us another four acceptable interpretations (severios, sewerios, sweirios, saw erios).machine transliteration has been an active research field for quite while (al-onaizan and knight, 2002; abduljaleel and larkey, 2003; klementiev and roth, 2006; <papid> N06-1011 </papid>sproat et al 2006) <papid> P06-1010 </papid>but to 18 our knowledge there is little published work on evaluating transliteration within real mt system.</citsent>
<aftsection>
<nextsent>the closest work to ours is described in (hassan and sorensen, 2005) <papid> W05-0712 </papid>where they have list of names in arabic and feed this list as the input text to their mt system.</nextsent>
<nextsent>they evaluate their system in three different cases: as word-based ne translation, phrase-based ne translation and in presence of transliteration module.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4477">
<title id=" W07-0703.xml">integration of an arabic transliteration module into a statistical machine translation system </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>as an example, for the name ?????????, four references came up with four different interpretations: swerios, swiriyus, sever ius, sweires.
</prevsent>
<prevsent>a quick query in google showed us another four acceptable interpretations (severios, sewerios, sweirios, saw erios).machine transliteration has been an active research field for quite while (al-onaizan and knight, 2002; abduljaleel and larkey, 2003; klementiev and roth, 2006; <papid> N06-1011 </papid>sproat et al 2006) <papid> P06-1010 </papid>but to 18 our knowledge there is little published work on evaluating transliteration within real mt system.</prevsent>
</prevsection>
<citsent citstr=" W05-0712 ">
the closest work to ours is described in (hassan and sorensen, 2005) <papid> W05-0712 </papid>where they have list of names in arabic and feed this list as the input text to their mt system.</citsent>
<aftsection>
<nextsent>they evaluate their system in three different cases: as word-based ne translation, phrase-based ne translation and in presence of transliteration module.
</nextsent>
<nextsent>then, they report the bleu score on the final output.
</nextsent>
<nextsent>since their text is comprised of only nes, the bleu increase is quite high.
</nextsent>
<nextsent>combining all three models, they get 24.9 bleu point increase over the nave baseline.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4478">
<title id=" W07-0703.xml">integration of an arabic transliteration module into a statistical machine translation system </title>
<section> our approach.  </section>
<citcontext>
<prevsection>
<prevsent>in section 4, an evaluation of the integration is provided.
</prevsent>
<prevsent>finally, section 5 concludes the paper.
</prevsent>
</prevsection>
<citsent citstr=" W05-0822 ">
before going into details of our approach, an overview of portage (sadat et al 2005), <papid> W05-0822 </papid>the machine translation system that we used for our experiments and some of its properties should be provided.</citsent>
<aftsection>
<nextsent>portage is statistical phrase-based smt system similar to pharaoh (koehn et al 2003).<papid> N03-1017 </papid></nextsent>
<nextsent>given source sentence, it tries to find the target sentence that maximizes the joint probability of target sentence and phrase alignment according to loglinear model.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4479">
<title id=" W07-0703.xml">integration of an arabic transliteration module into a statistical machine translation system </title>
<section> our approach.  </section>
<citcontext>
<prevsection>
<prevsent>finally, section 5 concludes the paper.
</prevsent>
<prevsent>before going into details of our approach, an overview of portage (sadat et al 2005), <papid> W05-0822 </papid>the machine translation system that we used for our experiments and some of its properties should be provided.</prevsent>
</prevsection>
<citsent citstr=" N03-1017 ">
portage is statistical phrase-based smt system similar to pharaoh (koehn et al 2003).<papid> N03-1017 </papid></citsent>
<aftsection>
<nextsent>given source sentence, it tries to find the target sentence that maximizes the joint probability of target sentence and phrase alignment according to loglinear model.
</nextsent>
<nextsent>features in the loglinear model consist of phrase-based translation model with relative frequency and lexical probability estimates; 4gram language model using kneser-ney smoothing, trained with the srilm toolkit; single parameter distortion penalty on phrase reordering; and word-length penalty.
</nextsent>
<nextsent>weights on the loglinear features are set using och algorithm (och, 2003) <papid> P03-1021 </papid>to maximize the system bleu score on development corpus.</nextsent>
<nextsent>to generate phrase pairs from parallel corpus, we use the  diag-and  phrase induction algorithm described in (koehn et al 2003), <papid> N03-1017 </papid>with symmetrized word alignments generated using ibm model 2 (brown et al 1993).<papid> J93-2003 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4480">
<title id=" W07-0703.xml">integration of an arabic transliteration module into a statistical machine translation system </title>
<section> our approach.  </section>
<citcontext>
<prevsection>
<prevsent>given source sentence, it tries to find the target sentence that maximizes the joint probability of target sentence and phrase alignment according to loglinear model.
</prevsent>
<prevsent>features in the loglinear model consist of phrase-based translation model with relative frequency and lexical probability estimates; 4gram language model using kneser-ney smoothing, trained with the srilm toolkit; single parameter distortion penalty on phrase reordering; and word-length penalty.
</prevsent>
</prevsection>
<citsent citstr=" P03-1021 ">
weights on the loglinear features are set using och algorithm (och, 2003) <papid> P03-1021 </papid>to maximize the system bleu score on development corpus.</citsent>
<aftsection>
<nextsent>to generate phrase pairs from parallel corpus, we use the  diag-and  phrase induction algorithm described in (koehn et al 2003), <papid> N03-1017 </papid>with symmetrized word alignments generated using ibm model 2 (brown et al 1993).<papid> J93-2003 </papid></nextsent>
<nextsent>portage allows the use of sgml-like markup for arbitrary entities within the input text.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4482">
<title id=" W07-0703.xml">integration of an arabic transliteration module into a statistical machine translation system </title>
<section> our approach.  </section>
<citcontext>
<prevsection>
<prevsent>features in the loglinear model consist of phrase-based translation model with relative frequency and lexical probability estimates; 4gram language model using kneser-ney smoothing, trained with the srilm toolkit; single parameter distortion penalty on phrase reordering; and word-length penalty.
</prevsent>
<prevsent>weights on the loglinear features are set using och algorithm (och, 2003) <papid> P03-1021 </papid>to maximize the system bleu score on development corpus.</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
to generate phrase pairs from parallel corpus, we use the  diag-and  phrase induction algorithm described in (koehn et al 2003), <papid> N03-1017 </papid>with symmetrized word alignments generated using ibm model 2 (brown et al 1993).<papid> J93-2003 </papid></citsent>
<aftsection>
<nextsent>portage allows the use of sgml-like markup for arbitrary entities within the input text.
</nextsent>
<nextsent>the markup can be used to specify translations provided by external sources for the entities, such as rule-based translations of numbers and dates, or transliteration module for oovs in our work.
</nextsent>
<nextsent>many smt systems have this capability, so although the details given here pertain to portage, the techniques described can be used in many different smt systems.
</nextsent>
<nextsent>as an example, suppose we already have two different transliterations with their probabilities for the arabic name ??????.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4483">
<title id=" W07-0102.xml">corpus driven metaphor harvesting </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>adopting broad notion of metaphor based on ctm, we refer to such non-literal usages (though often conventionalized) as lexical metaphors in thispaper.
</prevsent>
<prevsent>prominent conceptual metaphors are illustrated by larger number of lexical metaphors, which support the systematicity of their mapping.
</prevsent>
</prevsection>
<citsent citstr=" C02-2021 ">
earlier projects annotating metaphor in corpora include (martin, 1994) and (barnden et al, 2002).<papid> C02-2021 </papid></citsent>
<aftsection>
<nextsent>in what follows, we give two examples of recent work.
</nextsent>
<nextsent>gedigian et al (2006) <papid> W06-3506 </papid>annotated subset of the wall street journal for the senses of verbs from motion-related, placing, and cure frames which were extracted from framenet (fillmore et al, 2003).</nextsent>
<nextsent>the annotation shows that more than 90% of the 4,186 occurrences of these verbs in the corpus data are lexical metaphors in the above sense.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4484">
<title id=" W07-0102.xml">corpus driven metaphor harvesting </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>earlier projects annotating metaphor in corpora include (martin, 1994) and (barnden et al, 2002).<papid> C02-2021 </papid></prevsent>
<prevsent>in what follows, we give two examples of recent work.</prevsent>
</prevsection>
<citsent citstr=" W06-3506 ">
gedigian et al (2006) <papid> W06-3506 </papid>annotated subset of the wall street journal for the senses of verbs from motion-related, placing, and cure frames which were extracted from framenet (fillmore et al, 2003).</citsent>
<aftsection>
<nextsent>the annotation shows that more than 90% of the 4,186 occurrences of these verbs in the corpus data are lexical metaphors in the above sense.
</nextsent>
<nextsent>gedigian et al (2006) <papid> W06-3506 </papid>conclude that in the domain of economics, motion-related metaphors are used conventionally to describe market fluctuations and policydecisions.</nextsent>
<nextsent>a classifier trained on the annotated corpus can discriminate between literal and metaphorical usages of the verbs.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4489">
<title id=" W07-0102.xml">corpus driven metaphor harvesting </title>
<section> conclusion and outlook.  </section>
<citcontext>
<prevsection>
<prevsent>the method makes use of col locate exploration of target domain keyword, in order to identify the most promising source domains.
</prevsent>
<prevsent>over 1,000 manual annotations have been obtained and will be integrated into the hamburg metaphor database.this outnumbers by far the results of previous studies filed within hmd, which originated under similar conditions but did not resort to corpus manager.
</prevsent>
</prevsection>
<citsent citstr=" J04-1002 ">
our method is different from automated work on metaphor recognition such as (mason, 2004) <papid> J04-1002 </papid>and (gedigian et al, 2006) <papid> W06-3506 </papid>in that it includes nouns as parts of speech.</citsent>
<aftsection>
<nextsent>implementing it in an automated system would require more sophisticated lexical conceptual resources, representing information on concrete domains (possible source domains).
</nextsent>
<nextsent>in particular, the addition of lexical and conceptual links between verb and noun synsets is crucial for establishing connected source domain graph.
</nextsent>
<nextsent>acknowledgements thanks to patrick hanks, jana kla witter, and three anonymous reviewers for their helpful comments.
</nextsent>
<nextsent>this work was supported by fellowship within thepostdoc-programme of the german academic exchange service (daad), granted to the second author.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4491">
<title id=" W07-0712.xml">efficient handling of ngram language models for statistical machine translation </title>
<section> language model estimation.  </section>
<citcontext>
<prevsection>
<prevsent>2.4 lm quantization.
</prevsent>
<prevsent>quant ization provides an effective way of reducing the number of bits needed to store floating point variables.
</prevsent>
</prevsection>
<citsent citstr=" W06-3113 ">
(federico and bertoldi, 2006) <papid> W06-3113 </papid>showed that best results were achieved with the so-called binningmethod.</citsent>
<aftsection>
<nextsent>this method partitions data points into uniformly populated intervals or bins.
</nextsent>
<nextsent>bins are filled in in greedy manner, starting from the lowest value.
</nextsent>
<nextsent>the center of each bin corresponds to the mean value 1-gr 2-gr 3-gr 3 | bo | pr | idx 1 1 4 | pr
</nextsent>
<nextsent>figure 2: static data structure for lms.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4492">
<title id=" W06-1634.xml">automatic construction of predicate argument structure patterns for biomedical information extraction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>although such ie attempts have demonstrated near-practical performance, the same sets of patterns cannot be applied to different kinds of information.
</prevsent>
<prevsent>a real-world task requires several kinds of ie, thus manually engineering extraction current affiliation: ? fujitsu laboratories ltd. ? faculty of informatics, kogakuin university patterns, which is tedious and time-consuming process, is not really practical.
</prevsent>
</prevsection>
<citsent citstr=" P05-1053 ">
techniques based on machine learning (zhou et al., 2005; <papid> P05-1053 </papid>hao et al, 2005; bunescu and mooney, 2006) are expected to alleviate this problem in manually crafted ie.</citsent>
<aftsection>
<nextsent>however, in most cases, thecost of manually crafting patterns is simply transferred to that for constructing large amount of training data, which requires tedious amount of manual labor to annotate text.
</nextsent>
<nextsent>to systematically reduce the necessary amount of training data, we divided the task of constructing extraction patterns into subtask that general natural language processing techniques can solve and subtask that has specific properties according to the information to be extracted.
</nextsent>
<nextsent>the former subtask is of full parsing (i.e. recognizing syntactic structures of sentences), and the latter subtask is of constructing specific extraction patterns (i.e. finding clue words to extract information) based on the obtained syntactic structures.
</nextsent>
<nextsent>we adopted full parsing from various levels of parsing, because we believe that it offers thebest utility to generalize sentences into normalized syntactic relations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4495">
<title id=" W06-1634.xml">automatic construction of predicate argument structure patterns for biomedical information extraction </title>
<section> full parsing.  </section>
<citcontext>
<prevsection>
<prevsent>such structures did not include information of semantic subjects/objects, which full parsing can recognize.
</prevsent>
<prevsent>additionally, most relations they extracted from the ace corpus (linguistic data consortium, 2005) on broadcasts and new swires were within very short word-distance (70% where two entities are embedded in each other or separated by at most one word), and therefore shallow information was beneficial.
</prevsent>
</prevsection>
<citsent citstr=" P04-1056 ">
however, table 1shows that the word distance is long between interacting protein names annotated on the aimed corpus (bunescu and mooney, 2004), <papid> P04-1056 </papid>and we have to treat long-distance relations for information like protein-protein interactions.full parsing is more effective for acquiring generalized data from long-length words than shallowparsing.</citsent>
<aftsection>
<nextsent>the sentences at left in figure 1 exemplify the advantages of full parsing.
</nextsent>
<nextsent>the gerund activating?
</nextsent>
<nextsent>in the last sentence takes non-local semantic subject entity1?, and shallow parsing cannot recognize this relation because entity1and activating?
</nextsent>
<nextsent>are in different phrases.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4496">
<title id=" W06-1634.xml">automatic construction of predicate argument structure patterns for biomedical information extraction </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>what is important here is, thanks to the strong normalization of syntactic variations, that we can expect that the construction algorithm for extracting patterns that works on pass will need much smaller training corpus than those working on surface-word sequences.
</prevsent>
<prevsent>furthermore, because of the reduced diversity of surface-word sequences at the pas level, any ie system at this level should demonstrate improved recall.
</prevsent>
</prevsection>
<citsent citstr=" P03-1029 ">
sudo et al (2003), <papid> P03-1029 </papid>culotta and sorensen (2004)<papid> P04-1054 </papid>and bunescu and mooney (2005) <papid> H05-1091 </papid>acquired substructures derived from dependency trees as extraction patterns for ie in general domains.</citsent>
<aftsection>
<nextsent>their approaches were similar to our approach using pass derived from full parsing.
</nextsent>
<nextsent>however, one problem with their systems is that they could not treat non-local dependencies such as semantic subjects of gerund constructions (discussed in section 2), and thus rules acquired from the constructions were partial.bunescu and mooney (2006) also learned extraction patterns for protein-protein interactions by svm with generalized sub sequence kernel.their patterns are sequences of words, poss, entity types, etc., and they heuristic ally restricted length and word positions of the patterns.
</nextsent>
<nextsent>al 285 entity1 recognizes and activates entity2.
</nextsent>
<nextsent>entity2 activated by entity1 are not well characterized.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4497">
<title id=" W06-1634.xml">automatic construction of predicate argument structure patterns for biomedical information extraction </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>what is important here is, thanks to the strong normalization of syntactic variations, that we can expect that the construction algorithm for extracting patterns that works on pass will need much smaller training corpus than those working on surface-word sequences.
</prevsent>
<prevsent>furthermore, because of the reduced diversity of surface-word sequences at the pas level, any ie system at this level should demonstrate improved recall.
</prevsent>
</prevsection>
<citsent citstr=" P04-1054 ">
sudo et al (2003), <papid> P03-1029 </papid>culotta and sorensen (2004)<papid> P04-1054 </papid>and bunescu and mooney (2005) <papid> H05-1091 </papid>acquired substructures derived from dependency trees as extraction patterns for ie in general domains.</citsent>
<aftsection>
<nextsent>their approaches were similar to our approach using pass derived from full parsing.
</nextsent>
<nextsent>however, one problem with their systems is that they could not treat non-local dependencies such as semantic subjects of gerund constructions (discussed in section 2), and thus rules acquired from the constructions were partial.bunescu and mooney (2006) also learned extraction patterns for protein-protein interactions by svm with generalized sub sequence kernel.their patterns are sequences of words, poss, entity types, etc., and they heuristic ally restricted length and word positions of the patterns.
</nextsent>
<nextsent>al 285 entity1 recognizes and activates entity2.
</nextsent>
<nextsent>entity2 activated by entity1 are not well characterized.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4498">
<title id=" W06-1634.xml">automatic construction of predicate argument structure patterns for biomedical information extraction </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>what is important here is, thanks to the strong normalization of syntactic variations, that we can expect that the construction algorithm for extracting patterns that works on pass will need much smaller training corpus than those working on surface-word sequences.
</prevsent>
<prevsent>furthermore, because of the reduced diversity of surface-word sequences at the pas level, any ie system at this level should demonstrate improved recall.
</prevsent>
</prevsection>
<citsent citstr=" H05-1091 ">
sudo et al (2003), <papid> P03-1029 </papid>culotta and sorensen (2004)<papid> P04-1054 </papid>and bunescu and mooney (2005) <papid> H05-1091 </papid>acquired substructures derived from dependency trees as extraction patterns for ie in general domains.</citsent>
<aftsection>
<nextsent>their approaches were similar to our approach using pass derived from full parsing.
</nextsent>
<nextsent>however, one problem with their systems is that they could not treat non-local dependencies such as semantic subjects of gerund constructions (discussed in section 2), and thus rules acquired from the constructions were partial.bunescu and mooney (2006) also learned extraction patterns for protein-protein interactions by svm with generalized sub sequence kernel.their patterns are sequences of words, poss, entity types, etc., and they heuristic ally restricted length and word positions of the patterns.
</nextsent>
<nextsent>al 285 entity1 recognizes and activates entity2.
</nextsent>
<nextsent>entity2 activated by entity1 are not well characterized.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4499">
<title id=" W06-1634.xml">automatic construction of predicate argument structure patterns for biomedical information extraction </title>
<section> results and discussion.  </section>
<citcontext>
<prevsection>
<prevsent>to handle these attributives, we need distinguish necessary attributives from those that are general4 by semantic analysis or bootstrapping.
</prevsent>
<prevsent>another approach to improve our method is to include local information in sentences, such as surface words between protein names.
</prevsent>
</prevsection>
<citsent citstr=" P05-1052 ">
zhao and grishman (2005) <papid> P05-1052 </papid>reported that adding local information to deep syntactic information improved ie results.</citsent>
<aftsection>
<nextsent>this approach is also applicable to ie in other domains, where related entities are in short 4consider the case where source sentence for pattern isentity1 is an important homodimeric protein.?
</nextsent>
<nextsent>(homod imeric?
</nextsent>
<nextsent>represents that two molecules of entity1?
</nextsent>
<nextsent>interact with each other.)
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4501">
<title id=" W06-1708.xml">the problem of ontology alignment on the web a first report </title>
<section> conclusions and future work.  </section>
<citcontext>
<prevsection>
<prevsent>falling back on to algorithm 2 proved not to be solution.
</prevsent>
<prevsent>the impact of the incompleteness of the lexical resource should be investigated and assessed more precisely.
</prevsent>
</prevsection>
<citsent citstr=" W02-0908 ">
another venue of research may be to exploit different thesauri, such as the ones automatically derived as in (curran andmoens, 2002).<papid> W02-0908 </papid></citsent>
<aftsection>
<nextsent>the performance of algorithm 4 might be improved by using more sophisticated word sense disambiguation methods.
</nextsent>
<nextsent>it would also be interesting to explore the application ofthe unsupervised method described in (mc carthy et al, 2004).<papid> P04-1036 </papid></nextsent>
<nextsent>as regards our long term plans, first, structural properties of the ontologies could potentially be exploited for the computation of node signatures.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4502">
<title id=" W06-1708.xml">the problem of ontology alignment on the web a first report </title>
<section> conclusions and future work.  </section>
<citcontext>
<prevsection>
<prevsent>another venue of research may be to exploit different thesauri, such as the ones automatically derived as in (curran andmoens, 2002).<papid> W02-0908 </papid></prevsent>
<prevsent>the performance of algorithm 4 might be improved by using more sophisticated word sense disambiguation methods.</prevsent>
</prevsection>
<citsent citstr=" P04-1036 ">
it would also be interesting to explore the application ofthe unsupervised method described in (mc carthy et al, 2004).<papid> P04-1036 </papid></citsent>
<aftsection>
<nextsent>as regards our long term plans, first, structural properties of the ontologies could potentially be exploited for the computation of node signatures.
</nextsent>
<nextsent>this kind of enhancement would make our system move from purely instance based approach to combined hybrid approach based on schema and instances.
</nextsent>
<nextsent>more fundamentally, we need to address the lack of appropriate, domain specific resources that can support the training of algorithms and models appropriate for the task at hand.
</nextsent>
<nextsent>wordnet is very general lexicon that does not support domain specific vocabulary, such as that used in geoscience sor in medicine or simply that contained in sub ontology that users may define according to their interests.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4503">
<title id=" W06-1708.xml">the problem of ontology alignment on the web a first report </title>
<section> conclusions and future work.  </section>
<citcontext>
<prevsection>
<prevsent>of course, we do not want to develop by hand domain specific resources that we have to change each time new domain arises.the crucial research issue is how to exploit extremely scarce resources to build efficient and effective models.
</prevsent>
<prevsent>the issue of scarce resources makes it impossible to use methods that are succesful at discriminating documents based on the words they contain but that need large corpora for training, for example latent semantic analysis (landauer et al, 1998).
</prevsent>
</prevsection>
<citsent citstr=" W03-1015 ">
the experiments described in this paper could be seen as providing 57 boot strapped model (riloff and jones, 1999; ng and cardie, 2003)<papid> W03-1015 </papid>in ml, bootstrapping requires to seed the classifier with small number of well chosen target examples.</citsent>
<aftsection>
<nextsent>we could develop web spider, based on the work described on this paper,to automatically retrieve larger amounts of training and test data, that in turn could be processed with more sophisticated nlp techniques.
</nextsent>
<nextsent>acknowledgements this work was partially supported by nsf awards iis0133123, iis0326284, iis0513553, and onr grant n000140010640.
</nextsent>


</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4504">
<title id=" W07-0402.xml">extraction phenomena in synchronous tag syntax and semantics </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>nesson and shieber (2006) showed how now standard variant of the tree-adjoining grammar(tag) formalism (multi-component, multiple adjunction, finite-feature-based tag), when synchronized, leads to natural analysis of the syntax semantics relation, including handling of syntactic movement phenomena such as wh questions and rel ativization, semantic movement?
</prevsent>
<prevsent>phenomena such as quantification, quantifier scope ambiguity, andeven their interactions as found in pied-piped relative clauses.1 phenomena were previously viewed 1this work was supported in part by grant iis-0329089 from the national science foundation.as problematic for tag analyses, leading to the hypothesizing of various extensions to the tag formalism (kallmeyer and romero, 2004, and work cited therein).
</prevsent>
</prevsection>
<citsent citstr=" W06-1506 ">
independently, han (2006<papid> W06-1506 </papid>a) developed similar synchronous tag analysis of pied piping, providing evidence for the naturalness of the analysis.</citsent>
<aftsection>
<nextsent>here, we update the analyses of noun phrases found in the previous works in one simple way, again with no additional formal tag innovations,and show that it allows further coverage of extraction and quantification phenomena as well as in-situ wh-phrases and topicalization.
</nextsent>
<nextsent>we emphasize that no novel formal devices are postulated to achieve this increased coverage ? just simple, natural and uniform change to the canonical structure of nps and their semantics.
</nextsent>
<nextsent>a word may be useful on the pertinence of this work in workshop on syntax and structure in machine translation?, above and beyond the intrinsic importance of exploring the applications of [syn chronous/transduction grammars] to related areasincluding.
</nextsent>
<nextsent>formal semantics?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4507">
<title id=" W07-0402.xml">extraction phenomena in synchronous tag syntax and semantics </title>
<section> introduction to synchronous tag.  </section>
<citcontext>
<prevsection>
<prevsent>examples of the substitution and adjunction operations on sample elementary trees are shown in figure 1.
</prevsent>
<prevsent>for further information, refer to joshi and schabes (1997).
</prevsent>
</prevsection>
<citsent citstr=" C90-3045 ">
synchronous tag (shieber, 1994; shieber and schabes, 1990) <papid> C90-3045 </papid>extends tag by taking the elementary structures to be pairs of tag trees with links between particular nodes in those trees.</citsent>
<aftsection>
<nextsent>derivation proceeds as in tag except that all operations must be paired.
</nextsent>
<nextsent>that is, tree can only be substituted oradjoined at node if its pair is simultaneously substituted or adjoined at linked node.
</nextsent>
<nextsent>we notate the links by using boxed indices marking linked nodes.
</nextsent>
<nextsent>10 mary o n apparently likes 1 2 3 4 1 23 4 np np e p adv p?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4518">
<title id=" W07-0710.xml">training non parametric features for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in effect, this allows us to trade away some human expert feature design for data.
</prevsent>
<prevsent>preliminary results on standard task show an encouraging improvement.
</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
in recent years, statistical machine translation have experienced quantum leap in quality thanks to automatic evaluation (papineni et al, 2002) <papid> P02-1040 </papid>and error based optimization (och, 2003).<papid> P03-1021 </papid></citsent>
<aftsection>
<nextsent>the conditional log-linear feature combination framework (berger, della pietra and della pietra, 1996) is remarkably simple and effective in practice.
</nextsent>
<nextsent>therefore, recent efforts (och et al, 2004) <papid> N04-1021 </papid>have concentrated on feature design ? wherein more intelligent features may be added.</nextsent>
<nextsent>because of their simplicity, how ever, log-linear models impose some constraints onhow new information may be inserted into the system to achieve the best results.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4520">
<title id=" W07-0710.xml">training non parametric features for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in effect, this allows us to trade away some human expert feature design for data.
</prevsent>
<prevsent>preliminary results on standard task show an encouraging improvement.
</prevsent>
</prevsection>
<citsent citstr=" P03-1021 ">
in recent years, statistical machine translation have experienced quantum leap in quality thanks to automatic evaluation (papineni et al, 2002) <papid> P02-1040 </papid>and error based optimization (och, 2003).<papid> P03-1021 </papid></citsent>
<aftsection>
<nextsent>the conditional log-linear feature combination framework (berger, della pietra and della pietra, 1996) is remarkably simple and effective in practice.
</nextsent>
<nextsent>therefore, recent efforts (och et al, 2004) <papid> N04-1021 </papid>have concentrated on feature design ? wherein more intelligent features may be added.</nextsent>
<nextsent>because of their simplicity, how ever, log-linear models impose some constraints onhow new information may be inserted into the system to achieve the best results.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4523">
<title id=" W07-0710.xml">training non parametric features for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in recent years, statistical machine translation have experienced quantum leap in quality thanks to automatic evaluation (papineni et al, 2002) <papid> P02-1040 </papid>and error based optimization (och, 2003).<papid> P03-1021 </papid></prevsent>
<prevsent>the conditional log-linear feature combination framework (berger, della pietra and della pietra, 1996) is remarkably simple and effective in practice.</prevsent>
</prevsection>
<citsent citstr=" N04-1021 ">
therefore, recent efforts (och et al, 2004) <papid> N04-1021 </papid>have concentrated on feature design ? wherein more intelligent features may be added.</citsent>
<aftsection>
<nextsent>because of their simplicity, how ever, log-linear models impose some constraints onhow new information may be inserted into the system to achieve the best results.
</nextsent>
<nextsent>in other words,new information needs to be parameterized carefully into one or more real valued feature functions.
</nextsent>
<nextsent>therefore, that requires some human knowledge and understanding.
</nextsent>
<nextsent>when not readily available, thisis typically replaced with painstaking experimentation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4524">
<title id=" W07-0710.xml">training non parametric features for statistical machine translation </title>
<section> model.  </section>
<citcontext>
<prevsection>
<prevsent>in this section, we describe the general log-linear model used for statistical machine translation, as well as training objective function and algorithm.
</prevsent>
<prevsent>the goal is to translate french (source) sentence indexed by t, with surface string ft. among set ofkt outcomes, we denote an english (target) hypothesis with surface string e(t)k indexed by k. 2.1 log-linear model.
</prevsent>
</prevsection>
<citsent citstr=" P02-1038 ">
the prevalent translation model in modern systems is conditional log-linear model (och and ney, 2002).<papid> P02-1038 </papid></citsent>
<aftsection>
<nextsent>from hypothesis e(t)k , we extract features h(t)k , abbreviated hk, as function of (t) and ft. the conditional probability of hypothesis e(t)k given source sentence ft is: pk , p(e(t)k |ft) , exp[?
</nextsent>
<nextsent>hk] zft;?
</nextsent>
<nextsent>, 72 where the partition function zft;?
</nextsent>
<nextsent>is given by: zft;?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4531">
<title id=" W07-0710.xml">training non parametric features for statistical machine translation </title>
<section> model.  </section>
<citcontext>
<prevsection>
<prevsent>however, there exists linear-timealgorithm for exact line search against that objective.
</prevsent>
<prevsent>the method is often used in conjunction with coordinate projection to great success.
</prevsent>
</prevsection>
<citsent citstr=" P06-2101 ">
2.2.5 maximum empirical bayes reward the algorithm may be improved by giving partial credit for confidence pk of the model to partially correct hypotheses outside of the most likely hypothesis (smith and eisner, 2006): <papid> P06-2101 </papid>1 t ? t=1 kt ? k=1 pk logb({ek(t)}).instead of the bleu score, we use its logrithm, be cause we think it is exponentially hard to improve bleu.</citsent>
<aftsection>
<nextsent>this model is equivalent to the previous model when pk give all the probability mass to the top-1.
</nextsent>
<nextsent>that can be reached, for instance, when ? has very large norm.
</nextsent>
<nextsent>there is no known method to train against this objective directly, however, efficient approximations have been developed.
</nextsent>
<nextsent>again, it is not convex.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4537">
<title id=" W06-3114.xml">manual and automatic evaluation of machine translation between european languages </title>
<section> evaluation framework.  </section>
<citcontext>
<prevsection>
<prevsent>there is twice as much language modelling data, since training data for the machine translation system is filtered against sentences of length larger than 40 words.
</prevsent>
<prevsent>out-of-domain test data is from the project syndicate web site, compendium of political commentary.
</prevsent>
</prevsection>
<citsent citstr=" W06-3119 ">
103 id participant cmu carnegie mellon university, usa (zollmann and venugopal, 2006) <papid> W06-3119 </papid>lcc language computer corporation, usa (olteanu et al, 2006<papid> W06-3122 </papid>b) ms microsoft, usa (menezes et al, 2006) <papid> W06-3124 </papid>nrc national research council, canada (johnson et al, 2006) <papid> W06-3118 </papid>ntt nippon telegraph and telephone, japan (watanabe et al, 2006) <papid> W06-3115 </papid>rali rali, university of montreal, canada (patry et al, 2006) <papid> W06-3116 </papid>systran systran, france uedin-birch university of edinburgh, uk ? alexandra birch (birch et al, 2006) <papid> W06-3123 </papid>uedin-phi university of edinburgh, uk ? philipp koehn (birch et al, 2006) <papid> W06-3123 </papid>upc-jg university of catalonia, spain ? jesus gimenez (gimenez and ma`rquez, 2006) upc-jmc university of catalonia, spain ? josep maria crego (crego et al, 2006) <papid> W06-3125 </papid>upc-mr university of catalonia, spain ? marta ruiz costa-jussa` (costa-jussa` et al, 2006) upv university of valencia, spain (sanchez and bened??, 2006) utd university of texas at dallas, usa (olteanu et al, 2006<papid> W06-3122 </papid>a) figure 2: participants in the shared task.</citsent>
<aftsection>
<nextsent>not all groups participated in all translation directions.
</nextsent>
<nextsent>1.2 test data.
</nextsent>
<nextsent>the test data was again drawn from segment of the europarl corpus from the fourth quarter of 2000,which is excluded from the training data.
</nextsent>
<nextsent>participants were also provided with two sets of 2,000 sentences of parallel text to be used for system development and tuning.in addition to the europarl test set, we also collected 29 editorials from the project syndicate web site2, which are published in all the four languages of the shared task.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4538">
<title id=" W06-3114.xml">manual and automatic evaluation of machine translation between european languages </title>
<section> evaluation framework.  </section>
<citcontext>
<prevsection>
<prevsent>there is twice as much language modelling data, since training data for the machine translation system is filtered against sentences of length larger than 40 words.
</prevsent>
<prevsent>out-of-domain test data is from the project syndicate web site, compendium of political commentary.
</prevsent>
</prevsection>
<citsent citstr=" W06-3122 ">
103 id participant cmu carnegie mellon university, usa (zollmann and venugopal, 2006) <papid> W06-3119 </papid>lcc language computer corporation, usa (olteanu et al, 2006<papid> W06-3122 </papid>b) ms microsoft, usa (menezes et al, 2006) <papid> W06-3124 </papid>nrc national research council, canada (johnson et al, 2006) <papid> W06-3118 </papid>ntt nippon telegraph and telephone, japan (watanabe et al, 2006) <papid> W06-3115 </papid>rali rali, university of montreal, canada (patry et al, 2006) <papid> W06-3116 </papid>systran systran, france uedin-birch university of edinburgh, uk ? alexandra birch (birch et al, 2006) <papid> W06-3123 </papid>uedin-phi university of edinburgh, uk ? philipp koehn (birch et al, 2006) <papid> W06-3123 </papid>upc-jg university of catalonia, spain ? jesus gimenez (gimenez and ma`rquez, 2006) upc-jmc university of catalonia, spain ? josep maria crego (crego et al, 2006) <papid> W06-3125 </papid>upc-mr university of catalonia, spain ? marta ruiz costa-jussa` (costa-jussa` et al, 2006) upv university of valencia, spain (sanchez and bened??, 2006) utd university of texas at dallas, usa (olteanu et al, 2006<papid> W06-3122 </papid>a) figure 2: participants in the shared task.</citsent>
<aftsection>
<nextsent>not all groups participated in all translation directions.
</nextsent>
<nextsent>1.2 test data.
</nextsent>
<nextsent>the test data was again drawn from segment of the europarl corpus from the fourth quarter of 2000,which is excluded from the training data.
</nextsent>
<nextsent>participants were also provided with two sets of 2,000 sentences of parallel text to be used for system development and tuning.in addition to the europarl test set, we also collected 29 editorials from the project syndicate web site2, which are published in all the four languages of the shared task.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4542">
<title id=" W06-3114.xml">manual and automatic evaluation of machine translation between european languages </title>
<section> evaluation framework.  </section>
<citcontext>
<prevsection>
<prevsent>there is twice as much language modelling data, since training data for the machine translation system is filtered against sentences of length larger than 40 words.
</prevsent>
<prevsent>out-of-domain test data is from the project syndicate web site, compendium of political commentary.
</prevsent>
</prevsection>
<citsent citstr=" W06-3124 ">
103 id participant cmu carnegie mellon university, usa (zollmann and venugopal, 2006) <papid> W06-3119 </papid>lcc language computer corporation, usa (olteanu et al, 2006<papid> W06-3122 </papid>b) ms microsoft, usa (menezes et al, 2006) <papid> W06-3124 </papid>nrc national research council, canada (johnson et al, 2006) <papid> W06-3118 </papid>ntt nippon telegraph and telephone, japan (watanabe et al, 2006) <papid> W06-3115 </papid>rali rali, university of montreal, canada (patry et al, 2006) <papid> W06-3116 </papid>systran systran, france uedin-birch university of edinburgh, uk ? alexandra birch (birch et al, 2006) <papid> W06-3123 </papid>uedin-phi university of edinburgh, uk ? philipp koehn (birch et al, 2006) <papid> W06-3123 </papid>upc-jg university of catalonia, spain ? jesus gimenez (gimenez and ma`rquez, 2006) upc-jmc university of catalonia, spain ? josep maria crego (crego et al, 2006) <papid> W06-3125 </papid>upc-mr university of catalonia, spain ? marta ruiz costa-jussa` (costa-jussa` et al, 2006) upv university of valencia, spain (sanchez and bened??, 2006) utd university of texas at dallas, usa (olteanu et al, 2006<papid> W06-3122 </papid>a) figure 2: participants in the shared task.</citsent>
<aftsection>
<nextsent>not all groups participated in all translation directions.
</nextsent>
<nextsent>1.2 test data.
</nextsent>
<nextsent>the test data was again drawn from segment of the europarl corpus from the fourth quarter of 2000,which is excluded from the training data.
</nextsent>
<nextsent>participants were also provided with two sets of 2,000 sentences of parallel text to be used for system development and tuning.in addition to the europarl test set, we also collected 29 editorials from the project syndicate web site2, which are published in all the four languages of the shared task.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4543">
<title id=" W06-3114.xml">manual and automatic evaluation of machine translation between european languages </title>
<section> evaluation framework.  </section>
<citcontext>
<prevsection>
<prevsent>there is twice as much language modelling data, since training data for the machine translation system is filtered against sentences of length larger than 40 words.
</prevsent>
<prevsent>out-of-domain test data is from the project syndicate web site, compendium of political commentary.
</prevsent>
</prevsection>
<citsent citstr=" W06-3118 ">
103 id participant cmu carnegie mellon university, usa (zollmann and venugopal, 2006) <papid> W06-3119 </papid>lcc language computer corporation, usa (olteanu et al, 2006<papid> W06-3122 </papid>b) ms microsoft, usa (menezes et al, 2006) <papid> W06-3124 </papid>nrc national research council, canada (johnson et al, 2006) <papid> W06-3118 </papid>ntt nippon telegraph and telephone, japan (watanabe et al, 2006) <papid> W06-3115 </papid>rali rali, university of montreal, canada (patry et al, 2006) <papid> W06-3116 </papid>systran systran, france uedin-birch university of edinburgh, uk ? alexandra birch (birch et al, 2006) <papid> W06-3123 </papid>uedin-phi university of edinburgh, uk ? philipp koehn (birch et al, 2006) <papid> W06-3123 </papid>upc-jg university of catalonia, spain ? jesus gimenez (gimenez and ma`rquez, 2006) upc-jmc university of catalonia, spain ? josep maria crego (crego et al, 2006) <papid> W06-3125 </papid>upc-mr university of catalonia, spain ? marta ruiz costa-jussa` (costa-jussa` et al, 2006) upv university of valencia, spain (sanchez and bened??, 2006) utd university of texas at dallas, usa (olteanu et al, 2006<papid> W06-3122 </papid>a) figure 2: participants in the shared task.</citsent>
<aftsection>
<nextsent>not all groups participated in all translation directions.
</nextsent>
<nextsent>1.2 test data.
</nextsent>
<nextsent>the test data was again drawn from segment of the europarl corpus from the fourth quarter of 2000,which is excluded from the training data.
</nextsent>
<nextsent>participants were also provided with two sets of 2,000 sentences of parallel text to be used for system development and tuning.in addition to the europarl test set, we also collected 29 editorials from the project syndicate web site2, which are published in all the four languages of the shared task.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4544">
<title id=" W06-3114.xml">manual and automatic evaluation of machine translation between european languages </title>
<section> evaluation framework.  </section>
<citcontext>
<prevsection>
<prevsent>there is twice as much language modelling data, since training data for the machine translation system is filtered against sentences of length larger than 40 words.
</prevsent>
<prevsent>out-of-domain test data is from the project syndicate web site, compendium of political commentary.
</prevsent>
</prevsection>
<citsent citstr=" W06-3115 ">
103 id participant cmu carnegie mellon university, usa (zollmann and venugopal, 2006) <papid> W06-3119 </papid>lcc language computer corporation, usa (olteanu et al, 2006<papid> W06-3122 </papid>b) ms microsoft, usa (menezes et al, 2006) <papid> W06-3124 </papid>nrc national research council, canada (johnson et al, 2006) <papid> W06-3118 </papid>ntt nippon telegraph and telephone, japan (watanabe et al, 2006) <papid> W06-3115 </papid>rali rali, university of montreal, canada (patry et al, 2006) <papid> W06-3116 </papid>systran systran, france uedin-birch university of edinburgh, uk ? alexandra birch (birch et al, 2006) <papid> W06-3123 </papid>uedin-phi university of edinburgh, uk ? philipp koehn (birch et al, 2006) <papid> W06-3123 </papid>upc-jg university of catalonia, spain ? jesus gimenez (gimenez and ma`rquez, 2006) upc-jmc university of catalonia, spain ? josep maria crego (crego et al, 2006) <papid> W06-3125 </papid>upc-mr university of catalonia, spain ? marta ruiz costa-jussa` (costa-jussa` et al, 2006) upv university of valencia, spain (sanchez and bened??, 2006) utd university of texas at dallas, usa (olteanu et al, 2006<papid> W06-3122 </papid>a) figure 2: participants in the shared task.</citsent>
<aftsection>
<nextsent>not all groups participated in all translation directions.
</nextsent>
<nextsent>1.2 test data.
</nextsent>
<nextsent>the test data was again drawn from segment of the europarl corpus from the fourth quarter of 2000,which is excluded from the training data.
</nextsent>
<nextsent>participants were also provided with two sets of 2,000 sentences of parallel text to be used for system development and tuning.in addition to the europarl test set, we also collected 29 editorials from the project syndicate web site2, which are published in all the four languages of the shared task.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4545">
<title id=" W06-3114.xml">manual and automatic evaluation of machine translation between european languages </title>
<section> evaluation framework.  </section>
<citcontext>
<prevsection>
<prevsent>there is twice as much language modelling data, since training data for the machine translation system is filtered against sentences of length larger than 40 words.
</prevsent>
<prevsent>out-of-domain test data is from the project syndicate web site, compendium of political commentary.
</prevsent>
</prevsection>
<citsent citstr=" W06-3116 ">
103 id participant cmu carnegie mellon university, usa (zollmann and venugopal, 2006) <papid> W06-3119 </papid>lcc language computer corporation, usa (olteanu et al, 2006<papid> W06-3122 </papid>b) ms microsoft, usa (menezes et al, 2006) <papid> W06-3124 </papid>nrc national research council, canada (johnson et al, 2006) <papid> W06-3118 </papid>ntt nippon telegraph and telephone, japan (watanabe et al, 2006) <papid> W06-3115 </papid>rali rali, university of montreal, canada (patry et al, 2006) <papid> W06-3116 </papid>systran systran, france uedin-birch university of edinburgh, uk ? alexandra birch (birch et al, 2006) <papid> W06-3123 </papid>uedin-phi university of edinburgh, uk ? philipp koehn (birch et al, 2006) <papid> W06-3123 </papid>upc-jg university of catalonia, spain ? jesus gimenez (gimenez and ma`rquez, 2006) upc-jmc university of catalonia, spain ? josep maria crego (crego et al, 2006) <papid> W06-3125 </papid>upc-mr university of catalonia, spain ? marta ruiz costa-jussa` (costa-jussa` et al, 2006) upv university of valencia, spain (sanchez and bened??, 2006) utd university of texas at dallas, usa (olteanu et al, 2006<papid> W06-3122 </papid>a) figure 2: participants in the shared task.</citsent>
<aftsection>
<nextsent>not all groups participated in all translation directions.
</nextsent>
<nextsent>1.2 test data.
</nextsent>
<nextsent>the test data was again drawn from segment of the europarl corpus from the fourth quarter of 2000,which is excluded from the training data.
</nextsent>
<nextsent>participants were also provided with two sets of 2,000 sentences of parallel text to be used for system development and tuning.in addition to the europarl test set, we also collected 29 editorials from the project syndicate web site2, which are published in all the four languages of the shared task.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4546">
<title id=" W06-3114.xml">manual and automatic evaluation of machine translation between european languages </title>
<section> evaluation framework.  </section>
<citcontext>
<prevsection>
<prevsent>there is twice as much language modelling data, since training data for the machine translation system is filtered against sentences of length larger than 40 words.
</prevsent>
<prevsent>out-of-domain test data is from the project syndicate web site, compendium of political commentary.
</prevsent>
</prevsection>
<citsent citstr=" W06-3123 ">
103 id participant cmu carnegie mellon university, usa (zollmann and venugopal, 2006) <papid> W06-3119 </papid>lcc language computer corporation, usa (olteanu et al, 2006<papid> W06-3122 </papid>b) ms microsoft, usa (menezes et al, 2006) <papid> W06-3124 </papid>nrc national research council, canada (johnson et al, 2006) <papid> W06-3118 </papid>ntt nippon telegraph and telephone, japan (watanabe et al, 2006) <papid> W06-3115 </papid>rali rali, university of montreal, canada (patry et al, 2006) <papid> W06-3116 </papid>systran systran, france uedin-birch university of edinburgh, uk ? alexandra birch (birch et al, 2006) <papid> W06-3123 </papid>uedin-phi university of edinburgh, uk ? philipp koehn (birch et al, 2006) <papid> W06-3123 </papid>upc-jg university of catalonia, spain ? jesus gimenez (gimenez and ma`rquez, 2006) upc-jmc university of catalonia, spain ? josep maria crego (crego et al, 2006) <papid> W06-3125 </papid>upc-mr university of catalonia, spain ? marta ruiz costa-jussa` (costa-jussa` et al, 2006) upv university of valencia, spain (sanchez and bened??, 2006) utd university of texas at dallas, usa (olteanu et al, 2006<papid> W06-3122 </papid>a) figure 2: participants in the shared task.</citsent>
<aftsection>
<nextsent>not all groups participated in all translation directions.
</nextsent>
<nextsent>1.2 test data.
</nextsent>
<nextsent>the test data was again drawn from segment of the europarl corpus from the fourth quarter of 2000,which is excluded from the training data.
</nextsent>
<nextsent>participants were also provided with two sets of 2,000 sentences of parallel text to be used for system development and tuning.in addition to the europarl test set, we also collected 29 editorials from the project syndicate web site2, which are published in all the four languages of the shared task.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4550">
<title id=" W06-3114.xml">manual and automatic evaluation of machine translation between european languages </title>
<section> evaluation framework.  </section>
<citcontext>
<prevsection>
<prevsent>there is twice as much language modelling data, since training data for the machine translation system is filtered against sentences of length larger than 40 words.
</prevsent>
<prevsent>out-of-domain test data is from the project syndicate web site, compendium of political commentary.
</prevsent>
</prevsection>
<citsent citstr=" W06-3125 ">
103 id participant cmu carnegie mellon university, usa (zollmann and venugopal, 2006) <papid> W06-3119 </papid>lcc language computer corporation, usa (olteanu et al, 2006<papid> W06-3122 </papid>b) ms microsoft, usa (menezes et al, 2006) <papid> W06-3124 </papid>nrc national research council, canada (johnson et al, 2006) <papid> W06-3118 </papid>ntt nippon telegraph and telephone, japan (watanabe et al, 2006) <papid> W06-3115 </papid>rali rali, university of montreal, canada (patry et al, 2006) <papid> W06-3116 </papid>systran systran, france uedin-birch university of edinburgh, uk ? alexandra birch (birch et al, 2006) <papid> W06-3123 </papid>uedin-phi university of edinburgh, uk ? philipp koehn (birch et al, 2006) <papid> W06-3123 </papid>upc-jg university of catalonia, spain ? jesus gimenez (gimenez and ma`rquez, 2006) upc-jmc university of catalonia, spain ? josep maria crego (crego et al, 2006) <papid> W06-3125 </papid>upc-mr university of catalonia, spain ? marta ruiz costa-jussa` (costa-jussa` et al, 2006) upv university of valencia, spain (sanchez and bened??, 2006) utd university of texas at dallas, usa (olteanu et al, 2006<papid> W06-3122 </papid>a) figure 2: participants in the shared task.</citsent>
<aftsection>
<nextsent>not all groups participated in all translation directions.
</nextsent>
<nextsent>1.2 test data.
</nextsent>
<nextsent>the test data was again drawn from segment of the europarl corpus from the fourth quarter of 2000,which is excluded from the training data.
</nextsent>
<nextsent>participants were also provided with two sets of 2,000 sentences of parallel text to be used for system development and tuning.in addition to the europarl test set, we also collected 29 editorials from the project syndicate web site2, which are published in all the four languages of the shared task.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4555">
<title id=" W06-3114.xml">manual and automatic evaluation of machine translation between european languages </title>
<section> automatic evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>2.2 statistical significance.
</prevsent>
<prevsent>confidence interval: since bleu scores are not computed on the sentence level, traditional methods to compute statistical significance and confidence intervals do not apply.
</prevsent>
</prevsection>
<citsent citstr=" W04-3250 ">
hence, we use the bootstrap re sampling method described by koehn (2004).<papid> W04-3250 </papid></citsent>
<aftsection>
<nextsent>following this method, we repeatedly ? say,1000 times ? sample sets of sentences from the out put of each system, measure their bleu score, and use these 1000 bleu scores as basis for estimating confidence interval.
</nextsent>
<nextsent>when dropping the top and bottom 2.5% the remaining bleu scores define the range of the confidence interval.
</nextsent>
<nextsent>pairwise comparison: we can use the same method to assess the statistical significance of one system outperforming another.
</nextsent>
<nextsent>if two systems?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4556">
<title id=" W06-3114.xml">manual and automatic evaluation of machine translation between european languages </title>
<section> automatic evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>to check for this, we do pairwise bootstrap resampling: again, we repeatedly sample sets of sentences, this time from both systems, and compare their bleu scores on these sets.
</prevsent>
<prevsent>if one system is better in 95% of the sample sets, we conclude that its higher bleu score is statistically significantly better.
</prevsent>
</prevsection>
<citsent citstr=" W05-0908 ">
the bootstrap method has been crit ized by riezler and maxwell (2005) <papid> W05-0908 </papid>and collins et al (2005), <papid> P05-1066 </papid>as being too optimistic in deciding for statistical significant difference between systems.</citsent>
<aftsection>
<nextsent>we are therefore applying different method, which has been used at the 2005 darpa/nist evaluation.we divide up each test set into blocks of 20 sentences (100 blocks for the in-domain test set, 53 blocks for the out-of-domain test set), check for each block, if one system has higher bleu score than the other, and then use the sign test.
</nextsent>
<nextsent>the sign test checks, how likely sample of better and worse bleu scores would have been generated by two systems of equal performance.
</nextsent>
<nextsent>let say, if we find one system doing better on 20 of the blocks, and worse on 80 of the blocks, is it significantly worse?
</nextsent>
<nextsent>we check, how likely only up to = 20 better scores out of = 100 would have been generated by two equal systems, using the bi nomial distribution: p(0..k;n, p) = k?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4557">
<title id=" W06-3114.xml">manual and automatic evaluation of machine translation between european languages </title>
<section> automatic evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>to check for this, we do pairwise bootstrap resampling: again, we repeatedly sample sets of sentences, this time from both systems, and compare their bleu scores on these sets.
</prevsent>
<prevsent>if one system is better in 95% of the sample sets, we conclude that its higher bleu score is statistically significantly better.
</prevsent>
</prevsection>
<citsent citstr=" P05-1066 ">
the bootstrap method has been crit ized by riezler and maxwell (2005) <papid> W05-0908 </papid>and collins et al (2005), <papid> P05-1066 </papid>as being too optimistic in deciding for statistical significant difference between systems.</citsent>
<aftsection>
<nextsent>we are therefore applying different method, which has been used at the 2005 darpa/nist evaluation.we divide up each test set into blocks of 20 sentences (100 blocks for the in-domain test set, 53 blocks for the out-of-domain test set), check for each block, if one system has higher bleu score than the other, and then use the sign test.
</nextsent>
<nextsent>the sign test checks, how likely sample of better and worse bleu scores would have been generated by two systems of equal performance.
</nextsent>
<nextsent>let say, if we find one system doing better on 20 of the blocks, and worse on 80 of the blocks, is it significantly worse?
</nextsent>
<nextsent>we check, how likely only up to = 20 better scores out of = 100 would have been generated by two equal systems, using the bi nomial distribution: p(0..k;n, p) = k?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4558">
<title id=" W06-1319.xml">balancing conflicting factors in argument interpretation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we first describe our basic formalism, which considers interpretations comprising inferential relations, and then show how our formalism is extended to suppositions that account for the belief sin an argument, and justifications that account for the inferences in an interpretation.
</prevsent>
<prevsent>our evaluations with users show that the interpretations produced by our system are acceptable, and that there is strong support for the postulated suppositions and justifications.
</prevsent>
</prevsection>
<citsent citstr=" P02-1038 ">
the source-channel approach has been often used for word-based language tasks, such as speech recognition and machine translation (epstein,1996; och and ney, 2002).<papid> P02-1038 </papid></citsent>
<aftsection>
<nextsent>according to this approach, an addressee receives noisy channel (lan guage or speech wave), and decodes this channel to derive the source (idea).
</nextsent>
<nextsent>the selected source is that with the maximum posterior probability.in this paper, we apply the source-channel approach to the interpretation of arguments.
</nextsent>
<nextsent>this approach enables us to cast argument interpretation as trade-off between conflicting factors, viz model complexity against data fit, and structure complexity against belief reasonableness.
</nextsent>
<nextsent>this trade-off is inspired by the minimum message length (mml) criterion ? model selection method that is the basis for several machine learning techniques (wallace, 2005).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4559">
<title id=" W06-1319.xml">balancing conflicting factors in argument interpretation </title>
<section> related research.  </section>
<citcontext>
<prevsection>
<prevsent>this distinction also determines the nature of the task, as they try to find concise model that explain sas much of the data as possible (e.g., one referent that fits many clues), while we try to find representation for users argument.
</prevsent>
<prevsent>additionally, their domain knowledge is logic-based, while ours is bayesian; and they used weights to apply their hypothesis selection criteria, while our criteria are embodied in probabilistic framework.
</prevsent>
</prevsection>
<citsent citstr=" J99-1001 ">
plan recognition systems also generate one ormore interpretations of users utterances, employing different resources to fill in information omitted by the user, e.g., (allen and perrault,1980; litman and allen, 1987; carberry and lambert, 1999; <papid> J99-1001 </papid>raskutti and zukerman, 1991).</citsent>
<aftsection>
<nextsent>these plan recognition systems used plan-based approach to propose interpretations.
</nextsent>
<nextsent>the first three systems applied different types of heuristics to select an interpretation, while the fourth system used probabilistic approach moderated by heuristic sto select the interpretation with the highest probability.
</nextsent>
<nextsent>we use probabilistic domain representation in the form of bn (rather than plan libraries), and apply probabilistic mechanism that represents explicitly the contribution of back ground knowledge, model complexity and data fitto the generation of an interpretation.
</nextsent>
<nextsent>our mechanism, which can be applied to other domain representations, balances different types of complexities and discrepancies to select the interpretation with the highest posterior probability.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4560">
<title id=" W06-2008.xml">projecting pos tags and syntactic dependencies from english and french to polish in aligned corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the results show that the precision of direct projection vary according to the type of induced annotations as well as the source language.
</prevsent>
<prevsent>moreover, the performances are likely to be improved by defining regular conversion rules among pos tags and dependencies.
</prevsent>
</prevsection>
<citsent citstr=" P02-1050 ">
a clear imbalance may be observed between languages, such as english or french, for which number of nlp tools as well as different linguistic resources exist (leech, 1997) and those for which they are sparse or even absent, such as polish.one possible option to enrich resource-poor languages consists in taking advantage of resourcerich/resource-poor language aligned corpora to induce linguistic information for the resource-poor side from the resource-rich side (yarowski et al ,2001; borin, 2002; hwa et al , 2002).<papid> P02-1050 </papid></citsent>
<aftsection>
<nextsent>for polish, this has been made possible on account of its accessing to the european union (eu) which has resulted in the construction of large multilingual corpus of eu legislative texts and growing interest for new member states languages.this paper presents direct projection of various morpho-syntactic informations from english and french to polish.
</nextsent>
<nextsent>first, short survey of related works is made in order to motivate the issues addressed in this study.
</nextsent>
<nextsent>then, the principle of annotation projection is explained and the framework of the experiment is decribed (corpus, pos tagging and parsing, word alignment).
</nextsent>
<nextsent>the results of applying the annotation projection principle from two different source languages are finally presented and discussed.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4562">
<title id=" W06-2008.xml">projecting pos tags and syntactic dependencies from english and french to polish in aligned corpora </title>
<section> experimental framework.  </section>
<citcontext>
<prevsection>
<prevsent>market_nsg la_det det??
</prevsent>
<prevsent>mise (sur) le_det det??
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
march_nmsg table 2: syntactic dependencies identified with syntex 4.2.3 word alignment the english/polish parts of the corpus on the one hand, and the french/polish parts on the other hand, have been aligned at the word level using the giza++ package9 (och and ney, 2003).<papid> J03-1002 </papid></citsent>
<aftsection>
<nextsent>giza++consists of set of statistical translation models of different complexity, namely the ibm ones(brown et al , 1993).<papid> J93-2003 </papid></nextsent>
<nextsent>for both corpora, the tokenization resulting from the post-processing stage prior to parsing was used in the alignment process for the english and polish parts in order to keep the same segmentation especially to facilitate manual annotation for evaluation purposes.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4564">
<title id=" W06-2008.xml">projecting pos tags and syntactic dependencies from english and french to polish in aligned corpora </title>
<section> experimental framework.  </section>
<citcontext>
<prevsection>
<prevsent>mise (sur) le_det det??
</prevsent>
<prevsent>march_nmsg table 2: syntactic dependencies identified with syntex 4.2.3 word alignment the english/polish parts of the corpus on the one hand, and the french/polish parts on the other hand, have been aligned at the word level using the giza++ package9 (och and ney, 2003).<papid> J03-1002 </papid></prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
giza++consists of set of statistical translation models of different complexity, namely the ibm ones(brown et al , 1993).<papid> J93-2003 </papid></citsent>
<aftsection>
<nextsent>for both corpora, the tokenization resulting from the post-processing stage prior to parsing was used in the alignment process for the english and polish parts in order to keep the same segmentation especially to facilitate manual annotation for evaluation purposes.
</nextsent>
<nextsent>more over, each word being assigned lemma at the pos tagging stage, the sentences given as input to giza++ were lemmatized, as lemmatization has proven to boost statistical word alignment performances.
</nextsent>
<nextsent>on the polish side, rough tokenization using blanks and punctuation was realised; no lemmatization was performed.
</nextsent>
<nextsent>the ibm-4 model has been trained on each bi-text in both translation directions and the intersection of viterbi 9giza++ is available at http://www.jfoch.com/giza++.html.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4568">
<title id=" W06-2008.xml">projecting pos tags and syntactic dependencies from english and french to polish in aligned corpora </title>
<section> conclusion.  </section>
<citcontext>
<prevsection>
<prevsent>as far as precision is concerned, the direct projection is fairly efficient forpos tags but appears to be too restrictive for dependencies.
</prevsent>
<prevsent>nevetheless, the results are encouraging since they are likely to be improved by applying indirect correspondence rules.
</prevsent>
</prevsection>
<citsent citstr=" W04-2207 ">
they validate the idea of the existence of direct or indirect yet regular correspondences on the english/polishand french/polish language pairs which has already been tested with some syntax-based alignment techniques (ozdowska, 2004; <papid> W04-2207 </papid>ozdowska and claveau, 2005).</citsent>
<aftsection>
<nextsent>the next step will consist in exploiting the indirect correspondences and the multiple sources of information provided by two different source languages.
</nextsent>
<nextsent>moreover, using ibm-4word alignments in one direction instead of the intersection will be considered.this work mainly focusses on precision thus lacking information on recall.
</nextsent>
<nextsent>larger scale evaluations would be necessary to validate the approach, particularly evaluations that could measure recall, since the amount of evaluation data used is this study could be considered too limited.
</nextsent>

</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4569">
<title id=" W06-3505.xml">scaling natural language understanding via user driven ontology learning </title>
<section> scaling nlu via user-driven ontology.  </section>
<citcontext>
<prevsection>
<prevsent>when user of an open-domain spoken dialog system makes an utterance, it happens regularly, that the term is not represented in the systems lexicon.
</prevsent>
<prevsent>since it is assumed, in this work, that the meaning of terms is represented by means of formal ontology, user-driven ontology learning framework is needed to determine the corresponding concepts for these terms, e.g., via search on topical corpora.for instance, term such as auer stein could be employed to query search engine.
</prevsent>
</prevsection>
<citsent citstr=" C92-2082 ">
by applying natural language patterns, as proposed by hearst (1992) <papid> C92-2082 </papid>and statistical methods, as proposed by faulhaber et al(2006) possible hypernyms or sets of hypernym candidates of the term can be extracted.</citsent>
<aftsection>
<nextsent>for these corresponding concept (or set of possible concepts) in the ontology employed by the dialog system need to be found.
</nextsent>
<nextsent>last but not least the unknown term has to be inserted into the ontology as either an instance or subclass of that concept.
</nextsent>
<nextsent>this process is described in greater detail in section 5.4).
</nextsent>
<nextsent>it is important to point out that terms often have more than one meaning, which can only be determined by recourse to the context in which it is ut tered/found (widdows, 2003), (porzel et al, 2006).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4570">
<title id=" W06-3505.xml">scaling natural language understanding via user driven ontology learning </title>
<section> on-demand learning.  </section>
<citcontext>
<prevsection>
<prevsent>4more information on these and other onto logical choices can be found summarized in (cimiano et al, 2004) 5.2 language understanding.
</prevsent>
<prevsent>all correctly recognized terms of the user utterance can be mapped to concepts with the help of an analysis component.
</prevsent>
</prevsection>
<citsent citstr=" W04-2805 ">
frequently, production systems (engel, 2002), semantic chunk ers (bryant, 2004) <papid> W04-2805 </papid>or simple word-to-concept lexica (gurevych et al, 2003) <papid> N03-1012 </papid>are employed for this task.</citsent>
<aftsection>
<nextsent>such lexica assign corresponding natural language terms to all concepts of an ontology.
</nextsent>
<nextsent>this is especially important for later semantic disambiguation of the unknown term (loos and porzel, 2004).<papid> W04-2312 </papid></nextsent>
<nextsent>in case the information of the concepts of the other terms of the utterance can help to evaluate results: when there is more than one concept proposal for an instance (i.e. on the linguistic side proper noun like auerstein) found in theword-to-concept lexicon, the semantic distance between each proposed concept and the other concepts of the users question can be calculated5 . 5.3 linguistic and extra-linguistic context.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4571">
<title id=" W06-3505.xml">scaling natural language understanding via user driven ontology learning </title>
<section> on-demand learning.  </section>
<citcontext>
<prevsection>
<prevsent>4more information on these and other onto logical choices can be found summarized in (cimiano et al, 2004) 5.2 language understanding.
</prevsent>
<prevsent>all correctly recognized terms of the user utterance can be mapped to concepts with the help of an analysis component.
</prevsent>
</prevsection>
<citsent citstr=" N03-1012 ">
frequently, production systems (engel, 2002), semantic chunk ers (bryant, 2004) <papid> W04-2805 </papid>or simple word-to-concept lexica (gurevych et al, 2003) <papid> N03-1012 </papid>are employed for this task.</citsent>
<aftsection>
<nextsent>such lexica assign corresponding natural language terms to all concepts of an ontology.
</nextsent>
<nextsent>this is especially important for later semantic disambiguation of the unknown term (loos and porzel, 2004).<papid> W04-2312 </papid></nextsent>
<nextsent>in case the information of the concepts of the other terms of the utterance can help to evaluate results: when there is more than one concept proposal for an instance (i.e. on the linguistic side proper noun like auerstein) found in theword-to-concept lexicon, the semantic distance between each proposed concept and the other concepts of the users question can be calculated5 . 5.3 linguistic and extra-linguistic context.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4572">
<title id=" W06-3505.xml">scaling natural language understanding via user driven ontology learning </title>
<section> on-demand learning.  </section>
<citcontext>
<prevsection>
<prevsent>frequently, production systems (engel, 2002), semantic chunk ers (bryant, 2004) <papid> W04-2805 </papid>or simple word-to-concept lexica (gurevych et al, 2003) <papid> N03-1012 </papid>are employed for this task.</prevsent>
<prevsent>such lexica assign corresponding natural language terms to all concepts of an ontology.</prevsent>
</prevsection>
<citsent citstr=" W04-2312 ">
this is especially important for later semantic disambiguation of the unknown term (loos and porzel, 2004).<papid> W04-2312 </papid></citsent>
<aftsection>
<nextsent>in case the information of the concepts of the other terms of the utterance can help to evaluate results: when there is more than one concept proposal for an instance (i.e. on the linguistic side proper noun like auerstein) found in theword-to-concept lexicon, the semantic distance between each proposed concept and the other concepts of the users question can be calculated5 . 5.3 linguistic and extra-linguistic context.
</nextsent>
<nextsent>not only linguistic but also extra linguistic context plays an important role in dialog systems.
</nextsent>
<nextsent>thus, to understand the user in an open-domain dialog system it is important to know the extra-linguistic context of the utterances.
</nextsent>
<nextsent>if there is context module or component in the system it can give information on the discourse domain, time and location of the user.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4578">
<title id=" W06-1604.xml">detecting parser errors using web based semantic filters </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>collins uses syntactic and lexical features and trains on the penn treebank; in contrast, woodward uses semantic features derived from the web.
</prevsent>
<prevsent>see section 3 for comparison of our results with collins?.several systems produce semantic interpretation of sentence on top of parser.
</prevsent>
</prevsection>
<citsent citstr=" C04-1180 ">
for example, bos et al  (2004) <papid> C04-1180 </papid>build semantic representations from the parse derivations of ccg parser, and the english resource grammar (erg) (toutanovaet al , 2005) provides semantic representation using minimal recur sion semantics.</citsent>
<aftsection>
<nextsent>toutanova et al also include semantic features in their parse selection mechanism, although it is mostly syntax driven.
</nextsent>
<nextsent>the erg is hand-built grammar and thus does not have the same coverage as the grammar we use.
</nextsent>
<nextsent>we also use the semantic interpretations in novel way, checking them against semantic information on the web to decide if they are plau sible.nlp literature is replete with examples of systems that produce semantic interpretations anduse semantics to improve understanding.
</nextsent>
<nextsent>several systems in the 1970s and 1980s used hand built augmented transition networks or semantic networks to prune bad semantic interpretations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4579">
<title id=" W06-1604.xml">detecting parser errors using web based semantic filters </title>
<section> semantic filtering.  </section>
<citcontext>
<prevsection>
<prevsent>as an example, figure 1 contains sentence taken from the trec2003 corpus, parsed by the collins parser.
</prevsent>
<prevsent>figure 2 shows the correct rc for this sentence and the rc derived automatically from the incorrect parse.
</prevsent>
</prevsection>
<citsent citstr=" N03-1022 ">
due to space constraints, we omit details about the algorithm for converting parse into an rc,but moldovan et al  (2003) <papid> N03-1022 </papid>describe method similar to ours.</citsent>
<aftsection>
<nextsent>2.2 semantic filters.
</nextsent>
<nextsent>given the rc representation of parsed sentence as supplied by the semantic interpreter, we test the parse using four web-based methods.
</nextsent>
<nextsent>fundamentally, the methods all share the underlying principle that some form of co-occurrence of terms in the vast web corpus can help decide whether proposed relationship is semantically plausible.traditional statistical parsers also use cooccurrence of lexical heads as features for making parse decisions.
</nextsent>
<nextsent>we expand on this idea in twoways: first, we use corpus several orders of magnitude larger than the tagged corpora traditionally used to train statistical parses, so that the fundamental problem of data sparseness is ameliorated.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4580">
<title id=" W06-1604.xml">detecting parser errors using web based semantic filters </title>
<section> semantic filtering.  </section>
<citcontext>
<prevsection>
<prevsent>if qa system using the parse can find an answer to the question, then the question was probably parsed correctly.
</prevsent>
<prevsent>to test this theory, we implemented alight weight, simple, and fast qa system that directly mirrors the semantic interpretation.
</prevsent>
</prevsection>
<citsent citstr=" H05-1071 ">
it relies on text runner and knowitnow (cafarella et al ., 2005) <papid> H05-1071 </papid>to quickly find possible answers, given the relational conjunction (rc) of the question.knowitnow is state of the art information extraction system that uses set of domain independent patterns to efficiently find hyponyms of class.</citsent>
<aftsection>
<nextsent>we formalize the process as follows: define question as set of variables xi corresponding to noun phrases, set of noun type predicates ti(xi), and set of relational predicates pi(xi1, ..., xik) which relate one or more variables and constants.
</nextsent>
<nextsent>the conjunction of type and relational predicates is precisely the rc.
</nextsent>
<nextsent>we define an answer as set of values for each variable that satisfies all types and predicates ans(x1, ..., xn) = ? ti(xi) ? ?
</nextsent>
<nextsent>j pj(xj1, ..., xjk) the algorithm is as follows: 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4581">
<title id=" W06-3503.xml">understanding complex natural language explanations in tutorial applications </title>
<section> analyzing student explanations.  </section>
<citcontext>
<prevsection>
<prevsent>the first method, carmel, provides combined syntactic and semantic analysis using the lcflex syntactic parser along with semantic constructor functions (rose?, 2000).
</prevsent>
<prevsent>given specification of the desired representation language, it then maps the analysis to this language.
</prevsent>
</prevsection>
<citsent citstr=" W02-0211 ">
then discourse level processing attempts to resolve nominal and temporal anaphora and ellipsis to produce the candidate fopl representation for sentence (jordan and vanlehn, 2002).<papid> W02-0211 </papid></citsent>
<aftsection>
<nextsent>the second method, rappel, uses minipar (linand pantel, 2001) to parse the sentence.
</nextsent>
<nextsent>it then extracts syntactic dependency features from the parseto use in mapping the sentence to its fopl representation (jordan et al, 2004).
</nextsent>
<nextsent>each predicate in the kr language is assigned predicate template and separate classifier is trained for each predicatetemplate.
</nextsent>
<nextsent>for example, there is classifier that specializes in predicate instantiations (atoms) involving the velocity predicate and another for instantiationsof the acceleration predicate.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4582">
<title id=" W06-2914.xml">word distributions for thematic segmentation in a support vector machine approach </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>thematic segmentation also relates to several notions such as speakers intention, topic flow and cohesion.in order to find out if thematic segment identification is feasible task, previous state-of-the-artworks appeal to experiments, in which several human subjects are asked to mark thematic segment boundaries based on their intuition and minimal set of instructions.
</prevsent>
<prevsent>in this manner, previous studies, e.g.
</prevsent>
</prevsection>
<citsent citstr=" P93-1020 ">
(passonneau and litman, 1993; <papid> P93-1020 </papid>galley et al, 2003), <papid> P03-1071 </papid>obtained level of inter-annotator agreement that is statistically significant.</citsent>
<aftsection>
<nextsent>automatic thematic segmentation (ts), i.e. the segmentation of text stream into topically coherent segments, is an important component in applications dealing with large document collections such as information retrieval and document browsing.
</nextsent>
<nextsent>other tasks that could benefit from the thematic textual structure include anaphora resolution, automatic summarisation and discourse understanding.
</nextsent>
<nextsent>the work presented here tackles the problem of ts by adopting supervised learning approach for capturing linear document structure of nonoverlapping thematic episodes.
</nextsent>
<nextsent>a prerequisite for the input data to our system is that texts are divided into sentences or utterances.2 each boundary between two consecutive utterances is potential thematic segmentation point and therefore, we model the ts task as binary-classification problem, where each utterance should be classified as marking the2occasionally within this document we employ the term utterance to denote either sentence or an utterance in its proper sense.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4583">
<title id=" W06-2914.xml">word distributions for thematic segmentation in a support vector machine approach </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>thematic segmentation also relates to several notions such as speakers intention, topic flow and cohesion.in order to find out if thematic segment identification is feasible task, previous state-of-the-artworks appeal to experiments, in which several human subjects are asked to mark thematic segment boundaries based on their intuition and minimal set of instructions.
</prevsent>
<prevsent>in this manner, previous studies, e.g.
</prevsent>
</prevsection>
<citsent citstr=" P03-1071 ">
(passonneau and litman, 1993; <papid> P93-1020 </papid>galley et al, 2003), <papid> P03-1071 </papid>obtained level of inter-annotator agreement that is statistically significant.</citsent>
<aftsection>
<nextsent>automatic thematic segmentation (ts), i.e. the segmentation of text stream into topically coherent segments, is an important component in applications dealing with large document collections such as information retrieval and document browsing.
</nextsent>
<nextsent>other tasks that could benefit from the thematic textual structure include anaphora resolution, automatic summarisation and discourse understanding.
</nextsent>
<nextsent>the work presented here tackles the problem of ts by adopting supervised learning approach for capturing linear document structure of nonoverlapping thematic episodes.
</nextsent>
<nextsent>a prerequisite for the input data to our system is that texts are divided into sentences or utterances.2 each boundary between two consecutive utterances is potential thematic segmentation point and therefore, we model the ts task as binary-classification problem, where each utterance should be classified as marking the2occasionally within this document we employ the term utterance to denote either sentence or an utterance in its proper sense.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4585">
<title id=" W06-2914.xml">word distributions for thematic segmentation in a support vector machine approach </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>section 6 presents and discusses the evaluation results.the paper closes with section 7, which briefly summarizes this work and offers some conclusions and future directions.
</prevsent>
<prevsent>as in many existing approaches to the thematic segmentation task, we make the assumption that the thematic coherence of text segment is reflected at lexical level and therefore we attempt to detect the correlation between word distribution and thematic changes throughout the text.
</prevsent>
</prevsection>
<citsent citstr=" J97-1003 ">
in this manner, (hearst, 1997; <papid> J97-1003 </papid>reynar, 1998; choi, 2000) <papid> A00-2004 </papid>start by using similarity measure between sentences or fixed-size blocks of text, based on their word frequencies inorder to find changes in vocabulary use and therefore the points at which the topic changes.</citsent>
<aftsection>
<nextsent>sentences are then grouped together by using clustering algorithm.
</nextsent>
<nextsent>(utiyama and isahara, 2001) <papid> P01-1064 </papid>models the problem of ts as problem of finding the minimum cost path in graph and therefore adopts dynamic programming algorithm.</nextsent>
<nextsent>the main advantage of such methods is that no training time and corpora are required.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4586">
<title id=" W06-2914.xml">word distributions for thematic segmentation in a support vector machine approach </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>section 6 presents and discusses the evaluation results.the paper closes with section 7, which briefly summarizes this work and offers some conclusions and future directions.
</prevsent>
<prevsent>as in many existing approaches to the thematic segmentation task, we make the assumption that the thematic coherence of text segment is reflected at lexical level and therefore we attempt to detect the correlation between word distribution and thematic changes throughout the text.
</prevsent>
</prevsection>
<citsent citstr=" A00-2004 ">
in this manner, (hearst, 1997; <papid> J97-1003 </papid>reynar, 1998; choi, 2000) <papid> A00-2004 </papid>start by using similarity measure between sentences or fixed-size blocks of text, based on their word frequencies inorder to find changes in vocabulary use and therefore the points at which the topic changes.</citsent>
<aftsection>
<nextsent>sentences are then grouped together by using clustering algorithm.
</nextsent>
<nextsent>(utiyama and isahara, 2001) <papid> P01-1064 </papid>models the problem of ts as problem of finding the minimum cost path in graph and therefore adopts dynamic programming algorithm.</nextsent>
<nextsent>the main advantage of such methods is that no training time and corpora are required.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4588">
<title id=" W06-2914.xml">word distributions for thematic segmentation in a support vector machine approach </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in this manner, (hearst, 1997; <papid> J97-1003 </papid>reynar, 1998; choi, 2000) <papid> A00-2004 </papid>start by using similarity measure between sentences or fixed-size blocks of text, based on their word frequencies inorder to find changes in vocabulary use and therefore the points at which the topic changes.</prevsent>
<prevsent>sentences are then grouped together by using clustering algorithm.</prevsent>
</prevsection>
<citsent citstr=" P01-1064 ">
(utiyama and isahara, 2001) <papid> P01-1064 </papid>models the problem of ts as problem of finding the minimum cost path in graph and therefore adopts dynamic programming algorithm.</citsent>
<aftsection>
<nextsent>the main advantage of such methods is that no training time and corpora are required.
</nextsent>
<nextsent>by modeling ts as binary-classification problem,we introduce new technique based on support vector machines (svms).
</nextsent>
<nextsent>the main advantage offered by svms with respect to methods such as those described above is related to the distance (or similarity) function used.
</nextsent>
<nextsent>thus, although (choi, 2000; <papid> A00-2004 </papid>hearst,1997) <papid> J97-1003 </papid>employ distance function (i.e. cosine dis tance) to detect thematic shifts, svms are capable of using larger variety of similarity functions.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4591">
<title id=" W06-2914.xml">word distributions for thematic segmentation in a support vector machine approach </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>this is an important property for our task,where handling high dimensionality data representation is necessary (see section 4).
</prevsent>
<prevsent>an alternative to dealing with high dimension data may be to reduce the dimensionality of thedata representation.
</prevsent>
</prevsection>
<citsent citstr=" W01-0514 ">
therefore, linear algebra dimensionality reduction methods like singular value decomposition have been adopted by (choi et al,2001; <papid> W01-0514 </papid>popescu-belis et al, 2004) in latent semantic analysis (lsa) for the task of thematic segmen tation.</citsent>
<aftsection>
<nextsent>a probabilistic latent semantic analysis (plsa) approach has been adopted by (brants et al., 2002; farahat and chen, 2006) <papid> E06-1014 </papid>for the ts task.</nextsent>
<nextsent>(blei and moreno, 2001) proposed ts approach,by embedding plsa model in an extended hidden markov model (hmm) approach, while (yam ron et al, 1998) have previously proposed hmm approach for ts.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4592">
<title id=" W06-2914.xml">word distributions for thematic segmentation in a support vector machine approach </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>an alternative to dealing with high dimension data may be to reduce the dimensionality of thedata representation.
</prevsent>
<prevsent>therefore, linear algebra dimensionality reduction methods like singular value decomposition have been adopted by (choi et al,2001; <papid> W01-0514 </papid>popescu-belis et al, 2004) in latent semantic analysis (lsa) for the task of thematic segmen tation.</prevsent>
</prevsection>
<citsent citstr=" E06-1014 ">
a probabilistic latent semantic analysis (plsa) approach has been adopted by (brants et al., 2002; farahat and chen, 2006) <papid> E06-1014 </papid>for the ts task.</citsent>
<aftsection>
<nextsent>(blei and moreno, 2001) proposed ts approach,by embedding plsa model in an extended hidden markov model (hmm) approach, while (yam ron et al, 1998) have previously proposed hmm approach forts.
</nextsent>
<nextsent>a shortcoming of the methods described aboveis due to their typically generative manner of training, i.e. using the maximum likelihood estimation for joint sampling model of observation and label sequences.
</nextsent>
<nextsent>this poses the challenge of finding more appropriate objective functions, i.e. alternatives to the log-likelihood that are more closely related to application-relevant performance measures.
</nextsent>
<nextsent>secondly, efficient inference and learning for the ts task often requires making questionable conditional independence assumptions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4593">
<title id=" W06-2914.xml">word distributions for thematic segmentation in a support vector machine approach </title>
<section> support vector learning task and.  </section>
<citcontext>
<prevsection>
<prevsent>thematic segmentation the theory of vapnik and chervonenkis (vapnik, 1995) motivated the introduction of support vectorlearning.
</prevsent>
<prevsent>svms have originally been used for classification purposes and their principles have been extended to the task of regression, clustering and feature selection.
</prevsent>
</prevsection>
<citsent citstr=" W05-0405 ">
(kauchak and chen, 2005) <papid> W05-0405 </papid>employed svms using features (derived for instance from information given by the presence of paragraphs, pronouns, numbers) that can be reliably used for topic 102 segmentation of narrative documents.</citsent>
<aftsection>
<nextsent>aside from the fact that we consider the ts task on different datasets (not only on narrative documents), our approach is different from the approach proposed by(kauchak and chen, 2005) <papid> W05-0405 </papid>mainly by the data representation we propose and by the fact that we put the emphasis on deriving the thematic structure merely from word distribution, while (kauchak and chen, 2005) <papid> W05-0405 </papid>observed that the block similarities provide little information about the actual segment bound aries?</nextsent>
<nextsent>on their data and therefore they concentrated on exploiting other features.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4605">
<title id=" W06-2914.xml">word distributions for thematic segmentation in a support vector machine approach </title>
<section> experimental setup.  </section>
<citcontext>
<prevsection>
<prevsent>any test sample from this dataset consists of ten segments.
</prevsent>
<prevsent>each segment contains at least three sentences and no more than eleven sentences.
</prevsent>
</prevsection>
<citsent citstr=" W06-1320 ">
while the focus of our paper is not on the method of evaluation, it is worth pointing out that the performance on the synthetic dataset is very poor guide to the performance on naturally occurring data (georgescul et al, 2006).<papid> W06-1320 </papid></citsent>
<aftsection>
<nextsent>we include the synthetic data for comparison purposes.
</nextsent>
<nextsent>5.2 handling unbalanced data.
</nextsent>
<nextsent>we have small percentage of positive examples relative to the total number of training examples.
</nextsent>
<nextsent>therefore, in order to ensure that positive points are not considered as being noisy labels, we change the penalty of the minority (positive) class by setting the parameter c+ of this class to: c+ = ? ?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4606">
<title id=" W06-2914.xml">word distributions for thematic segmentation in a support vector machine approach </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>beeferman et al (1999) underlined that the standard evaluation metrics of precision and recall are inadequate for thematic segmentation, namely by the fact that these metrics did not account for how far away hypothesized boundary (i.e. boundary found by the automatic procedure) is from the reference boundary.
</prevsent>
<prevsent>on the other hand, for instance,an algorithm that places boundary just one utterance away from the reference boundary should be penalized less than an algorithm that places boundary ten (or more) utterances away from the reference boundary.
</prevsent>
</prevsection>
<citsent citstr=" J02-1002 ">
hence the use of two other evaluation metric sis favored in thematic segmentation: the pk metric (beeferman et al, 1999) and the windowdifferror metric (pevzner and hearst, 2002).<papid> J02-1002 </papid></citsent>
<aftsection>
<nextsent>in con 105 020406080100120 algorithms error rates p_k18.
</nextsent>
<nextsent>5411.0 152.51 20.492 1.3660 .04 21.683 1.912 354.6 268.48 wd19.
</nextsent>
<nextsent>4713.5 880.63 23.993 6.2891 .92 25.535 .8825.4 769.41 95.48 svmc 99 ran.
</nextsent>
<nextsent>d svmc 99ran svmg 03 g03 * c99 rand brown ata tdt da ta icsi da ta figure 1: error rates of the segmentation systems.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4619">
<title id=" W06-1665.xml">context dependent term relations for information retrieval </title>
<section> query expansion and term relations.  </section>
<citcontext>
<prevsection>
<prevsent>for example, if java travel?
</prevsent>
<prevsent>is used as query, we will likely be able to consider it as compound term.
</prevsent>
</prevsection>
<citsent citstr=" P97-1009 ">
the same compound (or its variant) would be difficult to 552 detect in document talking about traveling to java: the two words may appear at some distance or not in some specific syntactic structure as required in (lin, 1997).<papid> P97-1009 </papid></citsent>
<aftsection>
<nextsent>this will lead to the problem of mis matching between document and query.
</nextsent>
<nextsent>in fact, compound terms are not the only way to add contextual information to word.
</nextsent>
<nextsent>by putting two words together (without forming compound term), we usually obtain more precise sense for each word.
</nextsent>
<nextsent>for example, from java travel?, we can guess that the intended meaning is likely related to traveling to java island?.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4620">
<title id=" W06-1665.xml">context dependent term relations for information retrieval </title>
<section> query expansion and term relations.  </section>
<citcontext>
<prevsection>
<prevsent>owing to the context effect explained above, we will call the relations with multiple words in the condition part context-dependent relations.
</prevsent>
<prevsent>in order to limit the computation complexity, we will only consider adding one additional word into relations.
</prevsent>
</prevsection>
<citsent citstr=" P95-1026 ">
the proposed approach follows the same principle as (yarowsky, 1995), <papid> P95-1026 </papid>which tried to determine the appropriate word sense according to one relevant context word.</citsent>
<aftsection>
<nextsent>however, the requirement for query expansion is less than word sense disambiguation: we do not need to know the exact word sense to make expansion.
</nextsent>
<nextsent>we only need to determine the relevant expansion terms.
</nextsent>
<nextsent>therefore, there is no need to determine manually set of seeds before the learning process takes place.
</nextsent>
<nextsent>to some extent, the proposed approach is also related to (schtze and pedersen, 1997), which calculate term similarity according to the words appearing in the same context, or to second-order co-occurrences.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4621">
<title id=" W06-1665.xml">context dependent term relations for information retrieval </title>
<section> context-dependent query expansion.  </section>
<citcontext>
<prevsection>
<prevsent>therefore, we set two filtering criteria: ? the biterm in the condition of relation should be higher than threshold (10 in our case); ? the probability of relation should be higher than another threshold (0.0001 in our case).
</prevsent>
<prevsent>one more filtering criterion is mutual information (mi), which reflects the relatedness of two terms in their combination ),( kj ww . to keep relation )|( kji wwwp , we require ),( kj ww be meaningful combination.
</prevsent>
</prevsection>
<citsent citstr=" P89-1010 ">
we use the following pointwise mi (church and hanks 1989): )()( ),( <papid> P89-1010 </papid>log),( kj kj kj wpwp wwp wwmi = we only keep meaningful combinations such that 0),(  kj wwmi . by these filtering criteria, we are able to reduce considerably the number of biterms and triterms.</citsent>
<aftsection>
<nextsent>for example, on collection of about 200mb, with vocabulary size of about 148k, we selected only about 2.7m useful biterms and about 137m tri terms, which remain tractable.
</nextsent>
<nextsent>3.3 probability of biterms.
</nextsent>
<nextsent>in lm used in ir, each query term is attributed the same weight.
</nextsent>
<nextsent>this is equivalent to uniform probability distribution, i.e.: i qqqp || 1)|( = where |q|u is the number of unigrams in the query.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4622">
<title id=" W06-1665.xml">context dependent term relations for information retrieval </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>this expansion approach has been integrated both in traditional retrieval models (jing and croft, 1994) and in lm (berger and lafferty 1999).
</prevsent>
<prevsent>as we observed, this type of relation will introduce much noise into the query, leading to unstable effectiveness.
</prevsent>
</prevsection>
<citsent citstr=" P99-1029 ">
several other studies tried to filter out noise expansion (or translation) terms by considering the relations between them (gao et al, 2002; 558 jang et al 1999; <papid> P99-1029 </papid>qiu and frei, 1993; bai et al 2005).</citsent>
<aftsection>
<nextsent>however, this is insufficient to detect all the noise.
</nextsent>
<nextsent>the key issue is the ambiguity of relations due to the lack of context information in the relations.
</nextsent>
<nextsent>in this paper, we proposed method to add some context information into relations.
</nextsent>
<nextsent>(lin, 1997) <papid> P97-1009 </papid>also tries to solve word ambiguity by adding syntactic dependency as context.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4625">
<title id=" W06-2607.xml">tree kernel engineering in semantic role labeling systems </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the comparative experiments on support vector machines with such kernels on the conll 2005 dataset show that very simple tree manipulations trigger automatic feature engineering that highly improves accuracy and efficiency in both phases.
</prevsent>
<prevsent>moreover, the use of different classifiers for internal and pre-terminal nodes maintains the same accuracy and highly improves efficiency.
</prevsent>
</prevsection>
<citsent citstr=" A00-2008 ">
a lot of attention has been recently devoted tothe design of systems for the automatic labeling of semantic roles (srl) as defined in two important projects: framenet (johnson and fill more, 2000), <papid> A00-2008 </papid>inspired by frame semantics, and propbank (kingsbury and palmer, 2002) basedon levins verb classes.</citsent>
<aftsection>
<nextsent>in general, given sentence in natural language, the annotation of predicates semantic roles requires (1) the detection of the target word that embodies the predicate and(2) the detection and classification of the word sequences constituting the predicates arguments.
</nextsent>
<nextsent>in particular, step (2) can be divided into two different phases: (a) boundary detection, in which the words of the sequence are detected and (b) argument classification, in which the type of the argument is selected.
</nextsent>
<nextsent>most machine learning models adopted for thesrl task have shown that (shallow or deep) syntactic information is necessary to achieve good labeling accuracy.
</nextsent>
<nextsent>this research brings wide empirical evidence in favor of the linking theories between semantics and syntax, e.g.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4626">
<title id=" W06-2607.xml">tree kernel engineering in semantic role labeling systems </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(jackendoff, 1990).
</prevsent>
<prevsent>however, as no theory provides sound and complete treatment of such issue, the choice and design of syntactic features for the automatic learning of semantic structures requires remarkable research efforts and intuition.for example, the earlier studies concerning linguistic features suitable for semantic role labeling were carried out in (gildea and jurasfky, 2002).since then, researchers have proposed diverse syntactic feature sets that only slightly enhance the previous ones, e.g.
</prevsent>
</prevsection>
<citsent citstr=" W04-3212 ">
(xue and palmer, 2004) <papid> W04-3212 </papid>or(carreras and ma`rquez, 2005).</citsent>
<aftsection>
<nextsent>a careful analysis of such features reveals that most of them are syntactic tree fragments of training sentences, thus natural way to represent them is the adoption of tree kernels as described in (moschitti, 2004)<papid> P04-1043 </papid></nextsent>
<nextsent>theidea is to associate with each argument the minimal subtree that includes the target predicate withone of its arguments, and to use tree kernel function to evaluate the number of common substructures between two such trees.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4627">
<title id=" W06-2607.xml">tree kernel engineering in semantic role labeling systems </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, as no theory provides sound and complete treatment of such issue, the choice and design of syntactic features for the automatic learning of semantic structures requires remarkable research efforts and intuition.for example, the earlier studies concerning linguistic features suitable for semantic role labeling were carried out in (gildea and jurasfky, 2002).since then, researchers have proposed diverse syntactic feature sets that only slightly enhance the previous ones, e.g.
</prevsent>
<prevsent>(xue and palmer, 2004) <papid> W04-3212 </papid>or(carreras and ma`rquez, 2005).</prevsent>
</prevsection>
<citsent citstr=" P04-1043 ">
a careful analysis of such features reveals that most of them are syntactic tree fragments of training sentences, thus natural way to represent them is the adoption of tree kernels as described in (moschitti, 2004)<papid> P04-1043 </papid></citsent>
<aftsection>
<nextsent>theidea is to associate with each argument the minimal subtree that includes the target predicate withone of its arguments, and to use tree kernel function to evaluate the number of common substructures between two such trees.
</nextsent>
<nextsent>such approach is in line with current research on the use of tree kernels for natural language learning, e.g. syntactic parsing re-ranking (collins and duffy, 2002), <papid> P02-1034 </papid>relation extraction (zelenko et al, 2003) and named entity recognition (cumby and roth, 2003; culotta and sorensen, 2004).<papid> P04-1054 </papid></nextsent>
<nextsent>regarding the use of tree kernels for srl, in (moschitti, 2004)<papid> P04-1043 </papid>two main drawbacks have been 49 pointed out: ? highly accurate boundary detection cannot be carried out by tree kernel model since correct and incorrect arguments may share large portion of the encoding trees, i.e. they may share many substructures.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4629">
<title id=" W06-2607.xml">tree kernel engineering in semantic role labeling systems </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a careful analysis of such features reveals that most of them are syntactic tree fragments of training sentences, thus natural way to represent them is the adoption of tree kernels as described in (moschitti, 2004)<papid> P04-1043 </papid></prevsent>
<prevsent>theidea is to associate with each argument the minimal subtree that includes the target predicate withone of its arguments, and to use tree kernel function to evaluate the number of common substructures between two such trees.</prevsent>
</prevsection>
<citsent citstr=" P02-1034 ">
such approach is in line with current research on the use of tree kernels for natural language learning, e.g. syntactic parsing re-ranking (collins and duffy, 2002), <papid> P02-1034 </papid>relation extraction (zelenko et al, 2003) and named entity recognition (cumby and roth, 2003; culotta and sorensen, 2004).<papid> P04-1054 </papid></citsent>
<aftsection>
<nextsent>regarding the use of tree kernels for srl, in (moschitti, 2004)<papid> P04-1043 </papid>two main drawbacks have been 49 pointed out: ? highly accurate boundary detection cannot be carried out by tree kernel model since correct and incorrect arguments may share large portion of the encoding trees, i.e. they may share many substructures.</nextsent>
<nextsent>manually derived features (extended with apolynomial kernel) have been shown to be superior to tree kernel approaches.nevertheless, we believe that modeling completely kernel ized srl system is useful for the following reasons:?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4631">
<title id=" W06-2607.xml">tree kernel engineering in semantic role labeling systems </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a careful analysis of such features reveals that most of them are syntactic tree fragments of training sentences, thus natural way to represent them is the adoption of tree kernels as described in (moschitti, 2004)<papid> P04-1043 </papid></prevsent>
<prevsent>theidea is to associate with each argument the minimal subtree that includes the target predicate withone of its arguments, and to use tree kernel function to evaluate the number of common substructures between two such trees.</prevsent>
</prevsection>
<citsent citstr=" P04-1054 ">
such approach is in line with current research on the use of tree kernels for natural language learning, e.g. syntactic parsing re-ranking (collins and duffy, 2002), <papid> P02-1034 </papid>relation extraction (zelenko et al, 2003) and named entity recognition (cumby and roth, 2003; culotta and sorensen, 2004).<papid> P04-1054 </papid></citsent>
<aftsection>
<nextsent>regarding the use of tree kernels for srl, in (moschitti, 2004)<papid> P04-1043 </papid>two main drawbacks have been 49 pointed out: ? highly accurate boundary detection cannot be carried out by tree kernel model since correct and incorrect arguments may share large portion of the encoding trees, i.e. they may share many substructures.</nextsent>
<nextsent>manually derived features (extended with apolynomial kernel) have been shown to be superior to tree kernel approaches.nevertheless, we believe that modeling completely kernel ized srl system is useful for the following reasons:?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4634">
<title id=" W06-2607.xml">tree kernel engineering in semantic role labeling systems </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>combining it with traditional attribute value srl system allows us to obtain more accurate system.
</prevsent>
<prevsent>usually the combination of two traditional systems (based on the same machine learning model) does not result in an improvement as their features are more or less equivalent as shown in (carreras and ma`rquez, 2005).
</prevsent>
</prevsection>
<citsent citstr=" W05-0407 ">
the study of the effective structural feature scan inspire the design of novel linear features which can be used with more efficient model (i.e. linear svms).in this paper, we carry out tree kernel engineering (moschitti et al, 2005) <papid> W05-0407 </papid>to increase both accuracy and speed of the boundary detection and argument classification phases.</citsent>
<aftsection>
<nextsent>the engineering approach relates to marking the nodes of the encoding subtrees in order to generate substructures more strictly correlated with particular argument, boundary or predicate.
</nextsent>
<nextsent>for example, marking the node that exactly covers the target argument helps tree kernels to generate different substructures for correct and incorrect argument boundaries.
</nextsent>
<nextsent>the other technique that we applied to engineer different kernels is the subdivision of internal andpre-terminal nodes.
</nextsent>
<nextsent>we show that designing different classifiers for these two different node types slightly increases the accuracy and remarkably decreases the learning and classification time.an extensive experimentation of our tree kernels with support vector machines on the conll 2005 dataset provides interesting insights on the design of perform ant srl systems entirely based on tree kernels.in the remainder of this paper, section 2 introduces basic notions on srl systems and tree kernels.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4635">
<title id=" W06-2607.xml">tree kernel engineering in semantic role labeling systems </title>
<section> preliminary concepts.  </section>
<citcontext>
<prevsection>
<prevsent>in this section we briefly define the srl model that we intend to design and the kernel function that we use to evaluate the similarity between subtrees.
</prevsent>
<prevsent>2.1 basic srl approach.
</prevsent>
</prevsection>
<citsent citstr=" A00-2018 ">
the srl approach that we adopt is based on thedeep syntactic parse (charniak, 2000) <papid> A00-2018 </papid>of the sentence that we intend to annotate semantically.</citsent>
<aftsection>
<nextsent>the standard algorithm is to classify the tree node pair p, a?, where and are the nodes that exactly cover the target predicate and potential argument, respectively.
</nextsent>
<nextsent>if p, a?
</nextsent>
<nextsent>is labeled with an argument, then the terminal nodes dominated by will be considered as the words constituting such argument.
</nextsent>
<nextsent>the number of pairs for each sentence can be hundreds, thus, if we consider training corpora of thousands of sentences, we have to deal with millions of training instances.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4639">
<title id=" W06-2607.xml">tree kernel engineering in semantic role labeling systems </title>
<section> novel kernels for srl.  </section>
<citcontext>
<prevsection>
<prevsent>consequently, it is more convenient (at least from computational point of view) to use two different boundary classifiers, hereinafter referred to as combined classifier.
</prevsent>
<prevsent>3.2 kernels on complete predicate argument.
</prevsent>
</prevsection>
<citsent citstr=" P05-1073 ">
structures the type of target argument strongly depends on the type and number of the predicates arguments1 (punyakanok et al, 2005; toutanova et al, 2005).<papid> P05-1073 </papid></citsent>
<aftsection>
<nextsent>consequently, to correctly label an argument, we should extract features from the complete predicate argument structure it belongs to.
</nextsent>
<nextsent>in contrast, pafs completely neglect the information (i.e. the tree portions) related to non-target arguments.
</nextsent>
<nextsent>one way to use this further information with tree kernels is to use the minimum subtree that spans all the predicates arguments.
</nextsent>
<nextsent>the whole parse tree in figure 1 is an example of such minimum spanning tree (mst) as it includes all and only the argument structures of the predicate to deliver?.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4640">
<title id=" W06-2607.xml">tree kernel engineering in semantic role labeling systems </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>4.1 experimental set up.
</prevsent>
<prevsent>the empirical evaluations were carried out within the setting defined in the conll-2005 shared task (carreras and ma`rquez, 2005).
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
we used as target dataset the propbank corpus available at www.cis.upenn.edu/ace, along with the penn treebank 2 for the gold trees (www.cis.upenn.edu/treebank) (marcus et al, 1993), <papid> J93-2004 </papid>which includes about 53,700 sentences.</citsent>
<aftsection>
<nextsent>since the aim of this study was to design real srl system we adopted the charniak parse trees from the conll 2005 shared task data (available at www.lsi.upc.edu/srlconll/).
</nextsent>
<nextsent>we used section 02, 03 and 24 from the penn treebank in most of the experiments.
</nextsent>
<nextsent>their characteristics are shown in table 1.
</nextsent>
<nextsent>pos and neg indicate the number of nodes corresponding or notto correct argument boundary.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4641">
<title id=" W06-2607.xml">tree kernel engineering in semantic role labeling systems </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>consequently, we do not consider them in our evaluation.
</prevsent>
<prevsent>in sections 2, 3and 24 there are 454, 347 and 731 such cases, respectively.
</prevsent>
</prevsection>
<citsent citstr=" E06-1015 ">
the experiments were carried out with the svm-light-tk software available at http://ai-nlp.info.uniroma2.it/moschitti/which encodes fast tree kernel evaluation (mos chitti, 2006) <papid> E06-1015 </papid>in the svm-light software (joachims, 1999).</citsent>
<aftsection>
<nextsent>we used regularization parameter (option -c) equal to 1 and ? = 0.4 (see (moschitti, 2004)<papid> P04-1043 </papid>).</nextsent>
<nextsent>4.2 boundary detection results.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4645">
<title id=" W06-3107.xml">searching for alignments in smt a novel approach based on an estimation of distribution algorithm </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>uclm) of sentence pairs, each pair containing sentence in source language and translation of this sentence in target language.
</prevsent>
<prevsent>word alignments are necessary to link the words in the source and in the target sentence.
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
statistical models for machine translation heavily depend on the concept of alignment,specifically, the well known ibm word based models (brown et al, 1993).<papid> J93-2003 </papid></citsent>
<aftsection>
<nextsent>as result of this, different task on aligments in statistical machine translation have been proposed in the last few years (hlt naacl 2003 (mihalcea and pedersen, 2003) <papid> W03-0301 </papid>and acl 2005 (joel martin, 2005)).</nextsent>
<nextsent>in this paper, we propose novel approach to deal with alignments.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4648">
<title id=" W06-3107.xml">searching for alignments in smt a novel approach based on an estimation of distribution algorithm </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>word alignments are necessary to link the words in the source and in the target sentence.
</prevsent>
<prevsent>statistical models for machine translation heavily depend on the concept of alignment,specifically, the well known ibm word based models (brown et al, 1993).<papid> J93-2003 </papid></prevsent>
</prevsection>
<citsent citstr=" W03-0301 ">
as result of this, different task on aligments in statistical machine translation have been proposed in the last few years (hlt naacl 2003 (mihalcea and pedersen, 2003) <papid> W03-0301 </papid>and acl 2005 (joel martin, 2005)).</citsent>
<aftsection>
<nextsent>in this paper, we propose novel approach to deal with alignments.
</nextsent>
<nextsent>specifically, we address the problem of searching for the best word alignment between source and target sentence.
</nextsent>
<nextsent>as there is no efficient exact method to compute the optimal alignment (known as viterbi alignment) in most of the cases (specifically in the ibm models 3,4 and 5),in this work we propose the use of recently appeared meta-heuristic family of algorithms, estimation of distribution algorithms (edas).
</nextsent>
<nextsent>clearly, by using heuristic-based method we cannot guarantee the achievement of the optimal alignment.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4656">
<title id=" W06-3107.xml">searching for alignments in smt a novel approach based on an estimation of distribution algorithm </title>
<section> repeat.  </section>
<citcontext>
<prevsection>
<prevsent>(j ? 0 0 ) pj200 p01 ? 0?
</prevsent>
<prevsent>k=1 t(0k|e0) (3) where the factors separated by ? symbols denote fertility, translation, head permutation, non-headpermutation, null-fertility, and null-translation prob abilities1.
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
this model was trained using the giza++ toolkit (och and ney, 2003) <papid> J03-1002 </papid>on the material available for the different alignment tasks described in section 5.1 4.3 search.</citsent>
<aftsection>
<nextsent>in this section, some specific details about the search are given.
</nextsent>
<nextsent>as was mentioned in section 3, the algorithm starts by generating an initial set of hypotheses (initial population).
</nextsent>
<nextsent>in this case, set of randomly generated alignments between the source and the target sentences are generated.
</nextsent>
<nextsent>afterwards, all the individuals in this population (a fragment of real population is shown in figure 3) are scored using the function defined in eq.(4.2).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4657">
<title id=" W06-3107.xml">searching for alignments in smt a novel approach based on an estimation of distribution algorithm </title>
<section> repeat.  </section>
<citcontext>
<prevsection>
<prevsent>therefore, the search was conducted in both directions (i.e, from to and from tof ) combining the final results to achieve bidirectional alignments.
</prevsent>
<prevsent>to this end, diffferent approaches (symmetrization methods) were tested.
</prevsent>
</prevsection>
<citsent citstr=" P00-1056 ">
the results shown in section 5.2 were obtained by applying the refined method proposed in (och and ney, 2000).<papid> P00-1056 </papid></citsent>
<aftsection>
<nextsent>different experiments have been carried out in order to assess the correctness of the search algorithm.
</nextsent>
<nextsent>next, the experimental metodology employed and the results obtained are described.
</nextsent>
<nextsent>5.1 corpora and evaluation.
</nextsent>
<nextsent>three different corpora and four different test sets have been used.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4662">
<title id=" W06-2610.xml">an ontology based approach to disambiguation of semantic relations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the results show an unexploited opportunity of including prepositions and the relations they denote, e.g. in content based information retrieval.
</prevsent>
<prevsent>what we describe in this paper, which we refer to as relation disambiguation, is in some sense similar to word sense disambiguation.
</prevsent>
</prevsection>
<citsent citstr=" J98-1001 ">
in traditional word sense disambiguation the objective is to associate distinguishable sense with given word (ide and veronis, 1998).<papid> J98-1001 </papid></citsent>
<aftsection>
<nextsent>itis not novel idea to use machine learning in connection with traditional word sense disambiguation, and as such it is not novel idea to include some kindof generalization of the concept that word expresses in the learning task either (yarowsky, 1992).<papid> C92-2070 </papid></nextsent>
<nextsent>other projects have used light-weight ontologies such as wordnet in this kind of learning task (voorhees, 1993; agirre and martinez, 2001).<papid> W01-0703 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4663">
<title id=" W06-2610.xml">an ontology based approach to disambiguation of semantic relations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>what we describe in this paper, which we refer to as relation disambiguation, is in some sense similar to word sense disambiguation.
</prevsent>
<prevsent>in traditional word sense disambiguation the objective is to associate distinguishable sense with given word (ide and veronis, 1998).<papid> J98-1001 </papid></prevsent>
</prevsection>
<citsent citstr=" C92-2070 ">
itis not novel idea to use machine learning in connection with traditional word sense disambiguation, and as such it is not novel idea to include some kindof generalization of the concept that word expresses in the learning task either (yarowsky, 1992).<papid> C92-2070 </papid></citsent>
<aftsection>
<nextsent>other projects have used light-weight ontologies such as wordnet in this kind of learning task (voorhees, 1993; agirre and martinez, 2001).<papid> W01-0703 </papid></nextsent>
<nextsent>what we believe is our contribution with this work is the fact that we attempt tolearn complex concepts that consist of two simpler concepts, and the relation that holds between them.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4664">
<title id=" W06-2610.xml">an ontology based approach to disambiguation of semantic relations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in traditional word sense disambiguation the objective is to associate distinguishable sense with given word (ide and veronis, 1998).<papid> J98-1001 </papid></prevsent>
<prevsent>itis not novel idea to use machine learning in connection with traditional word sense disambiguation, and as such it is not novel idea to include some kindof generalization of the concept that word expresses in the learning task either (yarowsky, 1992).<papid> C92-2070 </papid></prevsent>
</prevsection>
<citsent citstr=" W01-0703 ">
other projects have used light-weight ontologies such as wordnet in this kind of learning task (voorhees, 1993; agirre and martinez, 2001).<papid> W01-0703 </papid></citsent>
<aftsection>
<nextsent>what we believe is our contribution with this work is the fact that we attempt tolearn complex concepts that consist of two simpler concepts, and the relation that holds between them.
</nextsent>
<nextsent>thus, we start out with the knowledge that some relation holds between two concepts, which we could express as rel(concept1,concept2), and what we aim at being able to do is to fill in more specific relation type than the generic rel, and get e.g. pof(concept1,concept2)in the case where preposition expresses parti tive relation.
</nextsent>
<nextsent>this makes it e.g. possible to determine from the sentence france is in europe?
</nextsent>
<nextsent>that france is part of europe.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4665">
<title id=" W06-1647.xml">lexicon acquisition for dialectal arabic using transductive learning </title>
<section> the importance of lexicons in.  </section>
<citcontext>
<prevsection>
<prevsent>unsupervised learning is performed by running the expectation maximization (em) algorithm on raw text.
</prevsent>
<prevsent>in this procedure, the tag sequences are unknown, and the probability tables p(wi|ti) and p(ti|ti1, ti2) are iteratively updated to maximize the likelihood of the observed word sequences.
</prevsent>
</prevsection>
<citsent citstr=" W95-0101 ">
although previous research in unsupervised tagging have achieved high accuracies rivaling supervised methods (kupiec, 1992; brill, 1995),<papid> W95-0101 </papid>much of the success is due to the use of artificially constrained lexicons.</citsent>
<aftsection>
<nextsent>specifically, the lexicon is word list where each word is annotated with the set of all its possible tags.
</nextsent>
<nextsent>(we will callthe set of possible tags of given word the posset of that word; an example: pos-set of the english word bank may be {nn,vb}.)
</nextsent>
<nextsent>banko and moore (2004) <papid> C04-1080 </papid>showed that unsupervised tagger accuracies on english degrade from 96% to 77% if the lexicon is not constrained such that only high frequency tags exist in the pos-set for each word.</nextsent>
<nextsent>why is the lexicon so critical in unsupervisedtagging?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4666">
<title id=" W06-1647.xml">lexicon acquisition for dialectal arabic using transductive learning </title>
<section> the importance of lexicons in.  </section>
<citcontext>
<prevsection>
<prevsent>specifically, the lexicon is word list where each word is annotated with the set of all its possible tags.
</prevsent>
<prevsent>(we will callthe set of possible tags of given word the posset of that word; an example: pos-set of the english word bank may be {nn,vb}.)
</prevsent>
</prevsection>
<citsent citstr=" C04-1080 ">
banko and moore (2004) <papid> C04-1080 </papid>showed that unsupervised tagger accuracies on english degrade from 96% to 77% if the lexicon is not constrained such that only high frequency tags exist in the pos-set for each word.</citsent>
<aftsection>
<nextsent>why is the lexicon so critical in unsupervisedtagging?
</nextsent>
<nextsent>the answer is that it provides additional knowledge about word-tag distributions that may otherwise be difficult to glean from raw text alone.
</nextsent>
<nextsent>in the case of unsupervised hmm taggers, the lexicon provides constraints on the probability tables p(wi|ti) and p(ti|ti1, ti2).
</nextsent>
<nextsent>specifically, the lexical probability table is initial ized such that p(wi|ti) = 0 if and only if tag ti is not included in the pos-set of word wi.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4667">
<title id=" W06-1647.xml">lexicon acquisition for dialectal arabic using transductive learning </title>
<section> learning frameworks and algorithms.  </section>
<citcontext>
<prevsection>
<prevsent>this idea is relatively straight forward, yet what is needed is principled way of deciding the correct number of clusters and the precise way of label transduction (e.g. based on majority vote vs. probability thresholds).
</prevsent>
<prevsent>typically, such parameters are decided heuristic ally (e.g.
</prevsent>
</prevsection>
<citsent citstr=" W05-0708 ">
(duh and kirchhoff, 2005<papid> W05-0708 </papid>a)) or by tuning ona labeled development set; for resource-poor languages, however, no such set may be available.</citsent>
<aftsection>
<nextsent>as suggested by (el-yaniv and gerzon, 2005), the tc algorithm can utilize theoretical error bound as principled way of determining the parameters.
</nextsent>
<nextsent>let rh(xm) be the empirical risk of given hypothesis (i.e. classifier) on the training set; let rh(xu) be the test risk.
</nextsent>
<nextsent>(derbeko et al, 2004)derive an error bound which states that, with probability 1??, the risk on the test samples is bounded by: rh(xu) ? rh(xm) + ?
</nextsent>
<nextsent>(m+u ) ( u+1 ) ( ln 1p(h)+ln 1 ? 2m ) (2) i.e. the test risk is bounded by the empirical risk on the labeled data, rh(xm), plus term that varies with the prior p(h) of the hypothesis or classifier.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4671">
<title id=" W06-1647.xml">lexicon acquisition for dialectal arabic using transductive learning </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>as ngram coverage, data-set selection, and the way annotations are done.
</prevsent>
<prevsent>there is an increasing amount of work in nlp tools for arabic.
</prevsent>
</prevsection>
<citsent citstr=" N04-4038 ">
in supervised pos tagging, (diab et al, 2004) <papid> N04-4038 </papid>achieves high accuracy on msa with the direct application of svm classifiers.</citsent>
<aftsection>
<nextsent>(habashand rambow, 2005) <papid> P05-1071 </papid>argue that the rich morphology of arabic necessitates the use of morphological analyzer in combination with pos tag ging.</nextsent>
<nextsent>this can be considered similar in spirit to the learning of lexicons for unsupervised tagging.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4672">
<title id=" W06-1647.xml">lexicon acquisition for dialectal arabic using transductive learning </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>there is an increasing amount of work in nlp tools for arabic.
</prevsent>
<prevsent>in supervised pos tagging, (diab et al, 2004) <papid> N04-4038 </papid>achieves high accuracy on msa with the direct application of svm classifiers.</prevsent>
</prevsection>
<citsent citstr=" P05-1071 ">
(habashand rambow, 2005) <papid> P05-1071 </papid>argue that the rich morphology of arabic necessitates the use of morphological analyzer in combination with pos tag ging.</citsent>
<aftsection>
<nextsent>this can be considered similar in spirit to the learning of lexicons for unsupervised tagging.
</nextsent>
<nextsent>the work done at recent jhu workshop (rambow and others, 2005) is very relevant in thatit investigates method for improving lca tagging that is orthogonal to our approach.
</nextsent>
<nextsent>they do not use the raw lca text as we have done.
</nextsent>
<nextsent>instead, they train msa supervised tagger and adapt it to lca by combination of methods, such using msa-lca translation lexicon and redistributing the probabibility mass of msa words to lca.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4673">
<title id=" W06-2934.xml">multilingual dependency parsing with incremental integer linear programming </title>
<section> abstract </section>
<citcontext>
<prevsection>


</prevsection>
<citsent citstr=" P05-1012 ">
our approach to dependency parsing is based on the linear model of mcdonald et al (mcdonald et al , 2005<papid> P05-1012 </papid>b).</citsent>
<aftsection>
<nextsent>instead of solving the linear model using the maximum spanning tree algorithm we propose an incremental integer linear programming formulation of the problem that allows us to enforce linguistic constraints.our results show only marginal improvements over the non-constrained parser.
</nextsent>
<nextsent>in addition to the fact that many parses did not violate any constraints in the first place this can be attributed to three reasons: 1)the next best solution that fulfils the constraints yields equal or less accuracy, 2) noisy pos tags and 3) occasionally our inference algorithm was too slow and decoding timed out.
</nextsent>
<nextsent>this paper presents our submission for the conll2006 shared task of multilingual dependency parsing.
</nextsent>
<nextsent>our parser is inspired by mcdonald et al .(2005<papid> P05-1012 </papid>a) which treats the task as the search for the highest scoring maximum spanning tree (mst) ina graph.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4677">
<title id=" W06-2934.xml">multilingual dependency parsing with incremental integer linear programming </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this paper presents our submission for the conll2006 shared task of multilingual dependency parsing.
</prevsent>
<prevsent>our parser is inspired by mcdonald et al .(2005<papid> P05-1012 </papid>a) which treats the task as the search for the highest scoring maximum spanning tree (mst) ina graph.</prevsent>
</prevsection>
<citsent citstr=" E06-1011 ">
this framework is efficient for both projective and non-projective parsing and provides an online learning algorithm which combined with rich feature set creates state-of-the-art performance across multiple languages (mcdonald and pereira, 2006).<papid> E06-1011 </papid></citsent>
<aftsection>
<nextsent>however, mcdonald and pereira (2006) <papid> E06-1011 </papid>mention the restrictive nature of this parsing algorithm.</nextsent>
<nextsent>in their original framework, features are only defined over single attachment decisions.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4699">
<title id=" W06-3007.xml">user centered evaluation of interactive question answering systems </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>instead, one must develop methods and metrics that are sensitive to individual users, tasks and contexts, and robust enough to allow for valid and reliable comparisons across systems.
</prevsent>
<prevsent>most evaluations of qa systems have been conducted as part of the qa track at trec.
</prevsent>
</prevsection>
<citsent citstr=" N03-1034 ">
they are system-oriented rather than user-oriented, with focus on evaluating techniques for answer extraction, rather than interaction and use (voorhees, 2003).<papid> N03-1034 </papid></citsent>
<aftsection>
<nextsent>in this paper, we consider an interactive system to be system that supports at least one exchange between the user and system.
</nextsent>
<nextsent>further, an interactive system is system that allows the user full or partial control over content and action.
</nextsent>
<nextsent>while factoid qa plays role in analytical qa, analytical qa also includes other activities, such as comparison and synthesis, and demands much richer interactions between the system, the information, and the user.
</nextsent>
<nextsent>thus different evaluation measures are needed for analytical qa systems than for those supporting factoid qa.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4700">
<title id=" W06-2611.xml">towards free text semantic parsing a unified framework based on framenet verbnet and propbank </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>while the best f-measure recorded on test set selected from the training corpus (wsj) was 80%, on the brown corpus, the f-measure dropped below 70%.
</prevsent>
<prevsent>the most significant causes for this performance decay were highly ambiguous and unseen predicates (i.e. predicates that do not have training examples, unseen in the training set).
</prevsent>
</prevsection>
<citsent citstr=" W04-0803 ">
on the framenet (johnson et al, 2003) role labeling task, the senseval-3 competition (lit kowski, 2004) <papid> W04-0803 </papid>registered similar results (~80%) by using the gold frame information as given feature.</citsent>
<aftsection>
<nextsent>no tests were performed outside framenet.
</nextsent>
<nextsent>in this paper, we show that when the frame feature is not used, the performance decay on different corpora reaches 30 points.
</nextsent>
<nextsent>thus, the context knowledge provided by the frame is very important and free-text semantic parser using framenet roles depends on the accurate automatic detection of this information.
</nextsent>
<nextsent>in order to test the feasibility of such task, we have trained an svm (support vector machine) tree kernel model for the automatic acquisition of the frame information.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4701">
<title id=" W06-2611.xml">towards free text semantic parsing a unified framework based on framenet verbnet and propbank </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the predicates not yet included in framenet and the predicates belonging to new application domains (that require new frames) are especially problematic as for them there is no available training data.
</prevsent>
<prevsent>we have thus studied new means of capturing the semantic context, other than the frame, which can be easily annotated on framenet and are available on larger scale (i.e. have better coverage).
</prevsent>
</prevsection>
<citsent citstr=" P98-1046 ">
a very good candidate seems to be the intersec tive levin classes (dang et al, 1998) <papid> P98-1046 </papid>that can be found as well in other predicate resources like propbank and verbnet (kipper et al., 2000).</citsent>
<aftsection>
<nextsent>thus, we have designed semiautomatic algorithm for assigning an intersec tive levin class to each framenet verb predicate.
</nextsent>
<nextsent>78the algorithm creates mapping between framenet frames and the intersec tive levin classes.
</nextsent>
<nextsent>by doing that we could connect framenet to verbnet and propbank and obtain an increased training set for the intersec tive levin class.
</nextsent>
<nextsent>this leads to better verb coverage and more robust semantic parser.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4702">
<title id=" W06-2611.xml">towards free text semantic parsing a unified framework based on framenet verbnet and propbank </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>by doing that we could connect framenet to verbnet and propbank and obtain an increased training set for the intersec tive levin class.
</prevsent>
<prevsent>this leads to better verb coverage and more robust semantic parser.
</prevsent>
</prevsection>
<citsent citstr=" J02-3001 ">
the newly created knowledge base allows us to surpass the shortcomings that arise when framenet, verbnet and propbank are used separately while, at the same time, we benefit from the extensive research involving each of them (pradhan et al, 2004; gildea and jurafsky, 2002; <papid> J02-3001 </papid>moschitti, 2004).<papid> P04-1043 </papid></citsent>
<aftsection>
<nextsent>we mention that there are 3,672 distinct verb senses1 in propbank and 2,351 distinct verb senses in framenet.
</nextsent>
<nextsent>only 501 verb senses are in common between the two corpora which mean 13.64% of propbank and 21.31% of framenet.
</nextsent>
<nextsent>thus, by training an intersec tive levin class classifier on both propbank and framenet we extend the number of available verb senses to 5,522.
</nextsent>
<nextsent>in the remainder of this paper, section 2 summarizes previous work done on framenet automatic role detection.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4704">
<title id=" W06-2611.xml">towards free text semantic parsing a unified framework based on framenet verbnet and propbank </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>by doing that we could connect framenet to verbnet and propbank and obtain an increased training set for the intersec tive levin class.
</prevsent>
<prevsent>this leads to better verb coverage and more robust semantic parser.
</prevsent>
</prevsection>
<citsent citstr=" P04-1043 ">
the newly created knowledge base allows us to surpass the shortcomings that arise when framenet, verbnet and propbank are used separately while, at the same time, we benefit from the extensive research involving each of them (pradhan et al, 2004; gildea and jurafsky, 2002; <papid> J02-3001 </papid>moschitti, 2004).<papid> P04-1043 </papid></citsent>
<aftsection>
<nextsent>we mention that there are 3,672 distinct verb senses1 in propbank and 2,351 distinct verb senses in framenet.
</nextsent>
<nextsent>only 501 verb senses are in common between the two corpora which mean 13.64% of propbank and 21.31% of framenet.
</nextsent>
<nextsent>thus, by training an intersec tive levin class classifier on both propbank and framenet we extend the number of available verb senses to 5,522.
</nextsent>
<nextsent>in the remainder of this paper, section 2 summarizes previous work done on framenet automatic role detection.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4712">
<title id=" W06-2611.xml">towards free text semantic parsing a unified framework based on framenet verbnet and propbank </title>
<section> mapping framenet frames to.  </section>
<citcontext>
<prevsection>
<prevsent>distant adjacent distant adjacent ? ?
</prevsent>
<prevsent>second, we build frequency distribution of verbnet thematic roles on different syntactic position.
</prevsent>
</prevsection>
<citsent citstr=" J01-3003 ">
based on our observation and previous studies (merlo and stevenson, 2001), <papid> J01-3003 </papid>we assume that each levin class has distinct frequency distribution of roles on different grammatical slots.</citsent>
<aftsection>
<nextsent>as we do not have matching grammatical function in framenet and verbnet, we approximate that subjects and direct objects are more likely to appear on positions adjacent to the predicate, while indirect objects appear on more distant positions.
</nextsent>
<nextsent>the same intuition is used successfully by g&j; in the design of the position feature.
</nextsent>
<nextsent>we will acquire from the corpus, for each thematic role i, the frequencies with which it appears on an adjacent (adj) or distant (dst) position in given frame or verbnet class (i.e. #(i, class, position)).
</nextsent>
<nextsent>therefore, for each frame and class, we obtain two vectors with thematic role frequencies corresponding respectively to the adjacent and distant positions (see figure 2).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4713">
<title id=" W06-2611.xml">towards free text semantic parsing a unified framework based on framenet verbnet and propbank </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>only verbs are selected to be predicates in our evaluations.
</prevsent>
<prevsent>moreover, as there is no fixed split between training and testing, we randomly selected 20% of sentences for testing and 80% for training.
</prevsent>
</prevsection>
<citsent citstr=" A00-2018 ">
the sentences were processed using charniaks parser (charniak, 2000) <papid> A00-2018 </papid>to generate parse trees automatically.</citsent>
<aftsection>
<nextsent>for classification, we used the svm-light tk software available at http://ai-nlp.
</nextsent>
<nextsent>info.uniroma2.it/moschitti which encodes tree kernels in the svm-light software (joachims, 1999).
</nextsent>
<nextsent>the classification performance was evaluated using the f1 measure for the sin gle-argument classifiers and the accuracy for the multiclassifiers.
</nextsent>
<nextsent>5.2 automatic verbnet vs. automatic fra-.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4716">
<title id=" W06-2611.xml">towards free text semantic parsing a unified framework based on framenet verbnet and propbank </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>framenet in the experiments involving semantic role labelling, we used svm with polynomial kernel.
</prevsent>
<prevsent>we adopted the standard features developed for semantic role detection by gildea and jurafsky (see section 2).
</prevsent>
</prevsection>
<citsent citstr=" W04-3212 ">
also, we considered some of the features designed by (pradhan et al, 2004): first and last word/pos in constituent, subcategorization, head word of prepositional phrases and the syntactic frame feature from (xue and palmer, 2004).<papid> W04-3212 </papid></citsent>
<aftsection>
<nextsent>for the rest of the paper we will refer to these features as being literature features (lf).
</nextsent>
<nextsent>the results obtained when using the literature features alone or in conjunction with the gold frame feature, gold ilc, automatically detected frame feature and automatically detected ilc are depicted in table 3.
</nextsent>
<nextsent>the first four columns report the f1 measure of some role classifiers whereas the last column shows the global multi classifier accuracy.
</nextsent>
<nextsent>the first row contains the number of training and testing instances and each of the other rows contains the performance obtained for different feature combinations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4717">
<title id=" W06-1648.xml">arabic ocr error correction using character segment correction language modeling and shallow morphology </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>there are two main categories of determining how to correct these errors.
</prevsent>
<prevsent>they are word-level and passage-level post-ocr processing.
</prevsent>
</prevsection>
<citsent citstr=" J96-1003 ">
some of the kinds of word level post-processing include the use of dictionary lookup, probabilistic relaxation, character and word n-gram frequency analysis (hong, 1995), and morphological analysis (oflazer, 1996).<papid> J96-1003 </papid></citsent>
<aftsection>
<nextsent>passage-level postprocessing techniques include the use of word grams, word collocations, grammar, conceptual closeness, passage level word clustering, linguistic context, and visual context.
</nextsent>
<nextsent>the following introduces some of the error correction techniques.
</nextsent>
<nextsent>dictionary lookup: dictionary lookup, which is the basis for the correction reported in this paper, is used to compare recognized words with words in term list (church and gale, 1991; hong, 1995; jurafsky and martin, 2000).
</nextsent>
<nextsent>if word is found in the dictionary, then it is considered correct.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4721">
<title id=" W06-1648.xml">arabic ocr error correction using character segment correction language modeling and shallow morphology </title>
<section> error correction methodology.  </section>
<citcontext>
<prevsection>
<prevsent>the word error rate (wer) for the 2,000 testing words was 39%.
</prevsent>
<prevsent>for all words (in training and testing), the different forms of alef (hamza, alef, alef maad, alef with hamza on top, hamza on wa, alef with hamza on the bottom, and hamza on ya) were normalized to alef, and ya and alef maqsoura were normalized to ya.
</prevsent>
</prevsection>
<citsent citstr=" P00-1037 ">
subsequently, the characters in the aligned words can aligned in two different ways, namely: 1:1 (one-to-one) character alignment, where each character is mapped to no more than one character (church and gale, 1991); or using m:n alignment, where character segment of length is aligned to character segment of length (brill and moore, 2000).<papid> P00-1037 </papid></citsent>
<aftsection>
<nextsent>the second method is more general and potentially more accurate especially for arabic where character can be confused with as many as three or four characters.
</nextsent>
<nextsent>the following example highlights the difference between the 1:1 and the m:n alignment approaches.
</nextsent>
<nextsent>given the training pair (rnacle, made): 1:1 alignment : n c e ? d ? 410 m:n alignment: for alignment, leven stein dynamic programming minimum edit distance algorithm was used to produce 1:1 alignments.
</nextsent>
<nextsent>the algorithm computes the minimum number of edit operations required to transform one string into another.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4722">
<title id=" W06-1648.xml">arabic ocr error correction using character segment correction language modeling and shallow morphology </title>
<section> error correction methodology.  </section>
<citcontext>
<prevsection>
<prevsent>??== 3.3 language modeling and shallow mor-.
</prevsent>
<prevsent>pho logical analysis two paths were pursued to explore the combined effect of language modeling and shallow morphological analysis.
</prevsent>
</prevsection>
<citsent citstr=" P03-1051 ">
in the first, 6-gram language model was trained on the same web-mined collection after each of the words in the collection was segmented into its constituent prefix, stem, and suffix (in this order) using language model based stemmer (lee et al, 2003).<papid> P03-1051 </papid></citsent>
<aftsection>
<nextsent>for example, ? pqrtu ? wktabhm?
</nextsent>
<nextsent>was replaced by w# ktab +hm?
</nextsent>
<nextsent>where # and + were used to mark prefixes and suffixes respectively and to distinguish them from stems.
</nextsent>
<nextsent>like before, alef and ya letter normalizations were performed and the language model was built using srilm toolkit with the same parameters.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4726">
<title id=" W06-1648.xml">arabic ocr error correction using character segment correction language modeling and shallow morphology </title>
<section> error correction methodology.  </section>
<citcontext>
<prevsection>
<prevsent>the top generated corrections were subsequently stemmed and the stems were reranked using the language model.
</prevsent>
<prevsent>the top resulting stem was compared to the condition in which language modeling was used without morphological analysis (as in the previous subsection) and then the top resulting correction were stemmed.
</prevsent>
</prevsection>
<citsent citstr=" W05-0704 ">
this path was pursued to examine the effect of correction on applications where stems are more useful than words such as arabic information retrieval (darwish et al, 2005; <papid> W05-0704 </papid>larkey et al, 2002).</citsent>
<aftsection>
<nextsent>3.4 testing the models.
</nextsent>
<nextsent>the 1:1 and m:n character mapping models were tested while enabling or disabling character position training (cp), smoothing by the assignment of small probabilities to unseen single character substitutions (up), language modeling (lm), and shallow morphological processing (sm) using the 6-gram model.
</nextsent>
<nextsent>as mentioned earlier, all models were tested using sentences containing 2,000 words in total.
</nextsent>
<nextsent>table 1 reports on the percentage of words for which proper correction was found in the top generated corrections using different models.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4727">
<title id=" W07-0717.xml">mixture model adaptation for smt </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>mixture modeling is simple framework that encompasses many different variants, as described below.
</prevsent>
<prevsent>it is naturally fairly low dimensional, because as the number of sub-models increases, the amount of text available to train each, and therefore its reliability, decreases.
</prevsent>
</prevsection>
<citsent citstr=" P06-1091 ">
this makes it suitable for discriminative smt training, which is still challenge for large parameter sets (tillmann and zhang, 2006; <papid> P06-1091 </papid>liang et al, 2006).<papid> P06-1096 </papid></citsent>
<aftsection>
<nextsent>techniques for assigning mixture weights depend on the setting.
</nextsent>
<nextsent>in cross-domain adaptation, knowledge of both source and target texts in the in-domain sample can be used to optimize weights directly.
</nextsent>
<nextsent>in dynamic adaptation, training poses problem be cause no reference text is available.
</nextsent>
<nextsent>our solution is to construct multi-domain development sample for learning parameter settings that are intended to generalize to new domains (ones not represented inthe sample).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4728">
<title id=" W07-0717.xml">mixture model adaptation for smt </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>mixture modeling is simple framework that encompasses many different variants, as described below.
</prevsent>
<prevsent>it is naturally fairly low dimensional, because as the number of sub-models increases, the amount of text available to train each, and therefore its reliability, decreases.
</prevsent>
</prevsection>
<citsent citstr=" P06-1096 ">
this makes it suitable for discriminative smt training, which is still challenge for large parameter sets (tillmann and zhang, 2006; <papid> P06-1091 </papid>liang et al, 2006).<papid> P06-1096 </papid></citsent>
<aftsection>
<nextsent>techniques for assigning mixture weights depend on the setting.
</nextsent>
<nextsent>in cross-domain adaptation, knowledge of both source and target texts in the in-domain sample can be used to optimize weights directly.
</nextsent>
<nextsent>in dynamic adaptation, training poses problem be cause no reference text is available.
</nextsent>
<nextsent>our solution is to construct multi-domain development sample for learning parameter settings that are intended to generalize to new domains (ones not represented inthe sample).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4729">
<title id=" W07-0717.xml">mixture model adaptation for smt </title>
<section> phrase-based statistical mt.  </section>
<citcontext>
<prevsection>
<prevsent>we experimented with the following choices: cross-domain versus dynamicadaptation; linear versus loglinear mixtures; language and translation model adaptation; various text distance metrics; different ways of converting distance metrics into weights; and granularity of the source unit being adapted to.
</prevsent>
<prevsent>the remainder of the paper is structured follows: section 2 briefly describes our phrase-based smtsystem; section 3 describes mixture-model adapta tion; section 4 gives experimental results; section 5 summarizes previous work; and section 6 concludes.
</prevsent>
</prevsection>
<citsent citstr=" N03-1017 ">
our baseline is standard phrase-based smt system (koehn et al, 2003).<papid> N03-1017 </papid></citsent>
<aftsection>
<nextsent>given source sentence s, this tries to find the target sentence t?
</nextsent>
<nextsent>that is the most likely translation of s, using the viterbi approx ima tion: t?
</nextsent>
<nextsent>= argmax p(t|s) ? argmax t,a p(t,a|s), where alignment = (s1, t1, j1), ..., (sk , tk , jk); tk are target phrases such that = t1 . . .
</nextsent>
<nextsent>tk ; sk are source phrases such that = sj1 . . .
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4730">
<title id=" W07-0717.xml">mixture model adaptation for smt </title>
<section> phrase-based statistical mt.  </section>
<citcontext>
<prevsection>
<prevsent>tk ; sk are source phrases such that = sj1 . . .
</prevsent>
<prevsent>sjk ; and sk is the translation of the kth target phrase tk. to model p(t,a|s), we use standard loglinear approach: p(t,a|s) ? exp [?
</prevsent>
</prevsection>
<citsent citstr=" P03-1021 ">
i ifi(s, t,a) ] (1) where each fi(s, t,a) is feature function, and weights are set using ochs algorithm (och,2003) <papid> P03-1021 </papid>to maximize the systems bleu score (pa pineni et al, 2001) on development corpus.</citsent>
<aftsection>
<nextsent>the features used in this study are: the length of t; single-parameter distortion penalty on phrase reordering in a, as described in (koehn et al, 2003); <papid> N03-1017 </papid>phrase translation model probabilities; and4-gram language model probabilities log p(t), using kneser-ney smoothing as implemented in the srilm toolkit.</nextsent>
<nextsent>phrase translation model probabilities are features of the form: log p(s|t,a) ? kk=1 log p(sk|tk).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4732">
<title id=" W07-0717.xml">mixture model adaptation for smt </title>
<section> phrase-based statistical mt.  </section>
<citcontext>
<prevsection>
<prevsent>phrase translation model probabilities are features of the form: log p(s|t,a) ? kk=1 log p(sk|tk).
</prevsent>
<prevsent>we use two different estimates for the conditional probabilities p(t?|s?) and p(s?|t?): relative frequencies and lexical?
</prevsent>
</prevsection>
<citsent citstr=" N04-1033 ">
probabilities as described in (zens and ney, 2004).<papid> N04-1033 </papid></citsent>
<aftsection>
<nextsent>in both cases, the forward?
</nextsent>
<nextsent>phrase probabilities p(t?|s?) are not used as features, but only as filter on the set of possible translations: for each source phrase s?
</nextsent>
<nextsent>that matches some ngram in s, only the 30 top-ranked translations t?
</nextsent>
<nextsent>according to p(t?|s?) are retained.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4735">
<title id=" W07-0717.xml">mixture model adaptation for smt </title>
<section> phrase-based statistical mt.  </section>
<citcontext>
<prevsection>
<prevsent>according to p(t?|s?) are retained.
</prevsent>
<prevsent>to derive the joint counts c(s?, t?)
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
from whichp(s?|t?) and p(t?|s?) are estimated, we use the phrase induction algorithm described in (koehn et al, 2003), <papid> N03-1017 </papid>with symmetrized word alignments generated using ibm model 2 (brown et al, 1993).<papid> J93-2003 </papid></citsent>
<aftsection>
<nextsent>our approach to mixture-model adaptation can be summarized by the following general algorithm: 1.
</nextsent>
<nextsent>split the corpus into different components, ac-.
</nextsent>
<nextsent>cording to some criterion.
</nextsent>
<nextsent>2.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4738">
<title id=" W07-0717.xml">mixture model adaptation for smt </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>both types of mixture model are better than the baseline, but the linear mixture is slightly better than the loglinear mixture.
</prevsent>
<prevsent>this is quite surprising, because these results are on the development set: the loglinear model tunes its component weights on this set, whereas the linear model only adjusts global lm and tm weights.
</prevsent>
</prevsection>
<citsent citstr=" W06-1607 ">
we speculated that this may have been due to non-smooth component models, and tried various smoothing schemes, including kneser-ney phrase table smoothing similar to that described in (fosteret al, 2006), <papid> W06-1607 </papid>and binary features to indicate phrase pair presence within different components.</citsent>
<aftsection>
<nextsent>none helped, however, and we conclude that the problem is most likely that ochs algorithm is unable to finda good maximimum in this setting.
</nextsent>
<nextsent>due to this result, all experiments we describe below involve linear mixtures only.
</nextsent>
<nextsent>combination adapted model lm tm lm+tm baseline 30.2 30.2 30.2 loglinear mixture 30.9 31.2 31.4 uniform linear mixture 31.2 31.1 31.8 table 2: linear versus loglinear combinations on nist04-nw.
</nextsent>
<nextsent>4.2 distance metrics for weighting.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4739">
<title id=" W07-0717.xml">mixture model adaptation for smt </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>133 granularity dev testnist04- nist05 nist06- nist06 mix nist gale baseline 31.9 30.4 27.6 12.9 file 32.4 30.8 28.6 13.4 genre 32.5 31.1 28.9 13.2 document 32.9 30.9 28.6 13.4table 8: the effects of source granularity on dynamic adaptation.
</prevsent>
<prevsent>mixture modeling is standard technique in machine learning (hastie et al, 2001).
</prevsent>
</prevsection>
<citsent citstr=" P99-1022 ">
it has been widely used to adapt language models for speech recognition and other applications, for instance using cross-domain topic mixtures, (iyer and ostendorf, 1999), dynamic topic mixtures (kneser and stein biss, 1993), hierachical mixtures (florian and yarowsky, 1999), <papid> P99-1022 </papid>and cache mixtures (kuhn and de mori, 1990).</citsent>
<aftsection>
<nextsent>most previous work on adaptive smt focuses onthe use of ir techniques to identify relevant subset of the training corpus from which an adapted model can be learned.
</nextsent>
<nextsent>byrne et al(2003) use co sine distance from the current source document to find relevant parallel texts for training an adapted translation model, with background information for smoothing alignments.
</nextsent>
<nextsent>hildebrand et al(1995) describe similar approach, but apply it at the sentence level, and use it for language model as well as translation model adaptation.
</nextsent>
<nextsent>they relyon perplexity heuristic to determine an optimal size for the relevant subset.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4740">
<title id=" W07-0717.xml">mixture model adaptation for smt </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>hildebrand et al(1995) describe similar approach, but apply it at the sentence level, and use it for language model as well as translation model adaptation.
</prevsent>
<prevsent>they relyon perplexity heuristic to determine an optimal size for the relevant subset.
</prevsent>
</prevsection>
<citsent citstr=" C04-1059 ">
zhao et al(2004) <papid> C04-1059 </papid>apply slightly different sentence-level strategy to language model adaptation, first generating an nbest list with baseline system, then finding similar sentences in monolingual target-language corpus.</citsent>
<aftsection>
<nextsent>this approach has the advantage of not limiting lm adaptation to parallel corpus, but the disadvantage of requiring two translation passes (one to generate the nbest lists, and an other to translate with the adapted model).
</nextsent>
<nextsent>ueffing (2006) describes self-training approach that also uses two-pass algorithm.
</nextsent>
<nextsent>a baseline system generates translations that, after confidence filtering, are used to construct parallel corpus basedon the test set.
</nextsent>
<nextsent>standard phrase-extraction techniques are then applied to extract an adapted phrase table from the systems own output.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4741">
<title id=" W07-0404.xml">factor ization of synchronous context free grammars in linear time </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we also use the algorithm to analyze the maximum scfg rule length needed to cover hand-aligned data from various language pairs.
</prevsent>
<prevsent>a number of recent syntax-based approaches to statistical machine translation make use of synchronous context free grammar (scfg) as theun derlying model of translational equivalence.
</prevsent>
</prevsection>
<citsent citstr=" J97-3002 ">
wu (1997)<papid> J97-3002 </papid>s inversion transduction grammar, as well as tree-transformation models of translation such as yamada and knight (2001), <papid> P01-1067 </papid>galley et al (2004), and chiang (2005) <papid> P05-1033 </papid>all fall into this category.a crucial question for efficient computation in approaches based on scfg is the length of the grammar rules.</citsent>
<aftsection>
<nextsent>grammars with longer rules can represent larger set of reorderings between languages (ahoand ullman, 1972), but also require greater computational complexity for word alignment algorithms based on synchronous parsing (satta and peserico,2005).<papid> H05-1101 </papid></nextsent>
<nextsent>grammar rules extracted from large parallel corpora by systems such as galley et al (2004)can be quite large, and wellington et al (2006) <papid> P06-1123 </papid>argue that complex rules are necessary by analyzing the coverage of gold-standard word alignments from different language pairs by various grammars.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4742">
<title id=" W07-0404.xml">factor ization of synchronous context free grammars in linear time </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we also use the algorithm to analyze the maximum scfg rule length needed to cover hand-aligned data from various language pairs.
</prevsent>
<prevsent>a number of recent syntax-based approaches to statistical machine translation make use of synchronous context free grammar (scfg) as theun derlying model of translational equivalence.
</prevsent>
</prevsection>
<citsent citstr=" P01-1067 ">
wu (1997)<papid> J97-3002 </papid>s inversion transduction grammar, as well as tree-transformation models of translation such as yamada and knight (2001), <papid> P01-1067 </papid>galley et al (2004), and chiang (2005) <papid> P05-1033 </papid>all fall into this category.a crucial question for efficient computation in approaches based on scfg is the length of the grammar rules.</citsent>
<aftsection>
<nextsent>grammars with longer rules can represent larger set of reorderings between languages (ahoand ullman, 1972), but also require greater computational complexity for word alignment algorithms based on synchronous parsing (satta and peserico,2005).<papid> H05-1101 </papid></nextsent>
<nextsent>grammar rules extracted from large parallel corpora by systems such as galley et al (2004)can be quite large, and wellington et al (2006) <papid> P06-1123 </papid>argue that complex rules are necessary by analyzing the coverage of gold-standard word alignments from different language pairs by various grammars.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4743">
<title id=" W07-0404.xml">factor ization of synchronous context free grammars in linear time </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we also use the algorithm to analyze the maximum scfg rule length needed to cover hand-aligned data from various language pairs.
</prevsent>
<prevsent>a number of recent syntax-based approaches to statistical machine translation make use of synchronous context free grammar (scfg) as theun derlying model of translational equivalence.
</prevsent>
</prevsection>
<citsent citstr=" P05-1033 ">
wu (1997)<papid> J97-3002 </papid>s inversion transduction grammar, as well as tree-transformation models of translation such as yamada and knight (2001), <papid> P01-1067 </papid>galley et al (2004), and chiang (2005) <papid> P05-1033 </papid>all fall into this category.a crucial question for efficient computation in approaches based on scfg is the length of the grammar rules.</citsent>
<aftsection>
<nextsent>grammars with longer rules can represent larger set of reorderings between languages (ahoand ullman, 1972), but also require greater computational complexity for word alignment algorithms based on synchronous parsing (satta and peserico,2005).<papid> H05-1101 </papid></nextsent>
<nextsent>grammar rules extracted from large parallel corpora by systems such as galley et al (2004)can be quite large, and wellington et al (2006) <papid> P06-1123 </papid>argue that complex rules are necessary by analyzing the coverage of gold-standard word alignments from different language pairs by various grammars.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4744">
<title id=" W07-0404.xml">factor ization of synchronous context free grammars in linear time </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a number of recent syntax-based approaches to statistical machine translation make use of synchronous context free grammar (scfg) as theun derlying model of translational equivalence.
</prevsent>
<prevsent>wu (1997)<papid> J97-3002 </papid>s inversion transduction grammar, as well as tree-transformation models of translation such as yamada and knight (2001), <papid> P01-1067 </papid>galley et al (2004), and chiang (2005) <papid> P05-1033 </papid>all fall into this category.a crucial question for efficient computation in approaches based on scfg is the length of the grammar rules.</prevsent>
</prevsection>
<citsent citstr=" H05-1101 ">
grammars with longer rules can represent larger set of reorderings between languages (ahoand ullman, 1972), but also require greater computational complexity for word alignment algorithms based on synchronous parsing (satta and peserico,2005).<papid> H05-1101 </papid></citsent>
<aftsection>
<nextsent>grammar rules extracted from large parallel corpora by systems such as galley et al (2004)can be quite large, and wellington et al (2006) <papid> P06-1123 </papid>argue that complex rules are necessary by analyzing the coverage of gold-standard word alignments from different language pairs by various grammars.</nextsent>
<nextsent>however, parsing complexity depends not only on rule length, but also on the specific permutations represented by the individual rules.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4745">
<title id=" W07-0404.xml">factor ization of synchronous context free grammars in linear time </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>wu (1997)<papid> J97-3002 </papid>s inversion transduction grammar, as well as tree-transformation models of translation such as yamada and knight (2001), <papid> P01-1067 </papid>galley et al (2004), and chiang (2005) <papid> P05-1033 </papid>all fall into this category.a crucial question for efficient computation in approaches based on scfg is the length of the grammar rules.</prevsent>
<prevsent>grammars with longer rules can represent larger set of reorderings between languages (ahoand ullman, 1972), but also require greater computational complexity for word alignment algorithms based on synchronous parsing (satta and peserico,2005).<papid> H05-1101 </papid></prevsent>
</prevsection>
<citsent citstr=" P06-1123 ">
grammar rules extracted from large parallel corpora by systems such as galley et al (2004)can be quite large, and wellington et al (2006) <papid> P06-1123 </papid>argue that complex rules are necessary by analyzing the coverage of gold-standard word alignments from different language pairs by various grammars.</citsent>
<aftsection>
<nextsent>however, parsing complexity depends not only on rule length, but also on the specific permutations represented by the individual rules.
</nextsent>
<nextsent>it may be possible to factor an scfg with maximum rule length into simpler grammar with maximum of knonterminals in any one rule, if not all n!
</nextsent>
<nextsent>permutations appear in the rules.
</nextsent>
<nextsent>zhang et al (2006) <papid> N06-1033 </papid>discuss methods for binarizing scfgs, ignoring the non binarizable grammars; in section 2 we discuss the generalized problem of factoring to k-ary grammars for any and formalize the problem as permutation factor ization in section 3.in section 4, we describe an o(k ? n) left-toright shift-reduce algorithm for analyzing permutations that can be k-arized.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4749">
<title id=" W07-0404.xml">factor ization of synchronous context free grammars in linear time </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>it may be possible to factor an scfg with maximum rule length into simpler grammar with maximum of knonterminals in any one rule, if not all n!
</prevsent>
<prevsent>permutations appear in the rules.
</prevsent>
</prevsection>
<citsent citstr=" N06-1033 ">
zhang et al (2006) <papid> N06-1033 </papid>discuss methods for binarizing scfgs, ignoring the non binarizable grammars; in section 2 we discuss the generalized problem of factoring to k-ary grammars for any and formalize the problem as permutation factor ization in section 3.in section 4, we describe an o(k ? n) left-toright shift-reduce algorithm for analyzing permutations that can be k-arized.</citsent>
<aftsection>
<nextsent>its time complexity becomes o(n2) when is not specified beforehand and the minimal is to be discovered.
</nextsent>
<nextsent>instead of linearly shifting in one number at time, gildea et al (2006) <papid> P06-2036 </papid>employ balanced binary tree as the control structure, producing an algorithm similar in spirit to merge-sort with reduced time complexity of o(n logn).</nextsent>
<nextsent>however, both algorithms relyon reduction tests on emerging spans which involve redundancies with the spans that have already been tested.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4750">
<title id=" W07-0404.xml">factor ization of synchronous context free grammars in linear time </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>zhang et al (2006) <papid> N06-1033 </papid>discuss methods for binarizing scfgs, ignoring the non binarizable grammars; in section 2 we discuss the generalized problem of factoring to k-ary grammars for any and formalize the problem as permutation factor ization in section 3.in section 4, we describe an o(k ? n) left-toright shift-reduce algorithm for analyzing permutations that can be k-arized.</prevsent>
<prevsent>its time complexity becomes o(n2) when is not specified beforehand and the minimal is to be discovered.</prevsent>
</prevsection>
<citsent citstr=" P06-2036 ">
instead of linearly shifting in one number at time, gildea et al (2006) <papid> P06-2036 </papid>employ balanced binary tree as the control structure, producing an algorithm similar in spirit to merge-sort with reduced time complexity of o(n logn).</citsent>
<aftsection>
<nextsent>however, both algorithms relyon reduction tests on emerging spans which involve redundancies with the spans that have already been tested.
</nextsent>
<nextsent>25uno and yagiura (2000) describe clever algorithm for the problem of finding all common intervals of two permutations in time o(n + k), where is the number of common intervals, which can itself be ?(n2).
</nextsent>
<nextsent>in section 5, we adapt their approach to the problem of factoring scfgs, and show that, given this problem definition, running time can be improved to o(n), the optimum given the time needed to read the input permutation.the methodology in wellington et al (2006) <papid> P06-1123 </papid>measures the complexity of word alignment using the number of gaps that are necessary for their synchronous parser which allows discontinuous spans to succeed in parsing.</nextsent>
<nextsent>in section 6, we provide amore direct measurement using the minimal branching factor yielded by the permutation factor ization algorithm.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4770">
<title id=" W06-3122.xml">language models and reranking for machine translation </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>for1during the translation of the first 10 sentences of the devtest2006.de dataset using phramer and the configuration described in section 3, the 3-gram lm was queried 27 million times (3 million distinct queries).the wmt 2006 shared task we opted for the reranking solution.
</prevsent>
<prevsent>this paper describes our solution and results.
</prevsent>
</prevsection>
<citsent citstr=" P01-1017 ">
we developed for the wmt 2006 shared task system that is trained on (a) word-aligned bilingual corpus, (b) large monolingual (english) corpus and(c) an english treebank and it is capable of translating from source language (german, spanish and french) into english.our system embeds phramer2 (used for minimum error rate training, decoding, decoding tools), pharaoh (koehn, 2004) (decoding), carmel 3(helper for pharaoh in n-best generation), char niaks parser (charniak, 2001) <papid> P01-1017 </papid>language model) and srilm4 (n-gram lm construction).</citsent>
<aftsection>
<nextsent>2.1 translation table construction.
</nextsent>
<nextsent>we developed component that builds translation table from word-aligned parallel corpus.
</nextsent>
<nextsent>the component generates the translation table according tothe process described in the pharaoh training man ual5.
</nextsent>
<nextsent>it generates vector of 5 numeric values for each phrase pair: ? phrase translation probability: ?(f? |e?) = count(f? , e?)count(e?) , ?(e?|f?) = count(f? , e?)
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4772">
<title id=" W06-3122.xml">language models and reranking for machine translation </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>the component generates the translation table according tothe process described in the pharaoh training man ual5.
</prevsent>
<prevsent>it generates vector of 5 numeric values for each phrase pair: ? phrase translation probability: ?(f? |e?) = count(f? , e?)count(e?) , ?(e?|f?) = count(f? , e?)
</prevsent>
</prevsection>
<citsent citstr=" N03-1017 ">
count(f?) 2http://www.phramer.org/ ? java-based open-source phrase based smt system 3http://www.isi.edu/licensed-sw/carmel/ 4http://www.speech.sri.com/projects/srilm/ 5http://www.iccs.inf.ed.ac.uk/pkoehn/training.tgz 150 ? lexical weighting (koehn et al, 2003): <papid> N03-1017 </papid>lex(f? |e?, a) = ? i=1 1 |{j|(i, j) ? a}| ? ?(i,j)a w(fi|ej) lex(e?|f? , a) = ? j=1 1 |{i|(i, j) ? a}| ? ?(i,j)a w(ej |fi) ? phrase penalty: ?(f? |e?) = e; log(?(f? |e?)) = 1 2.2 decoding.</citsent>
<aftsection>
<nextsent>we used the pharaoh decoder for both the minimum error rate training (och, 2003)<papid> P03-1021 </papid>and test dataset decoding.</nextsent>
<nextsent>although phramer provides decoding functionality equivalent to pharaohs, we preferred to use pharaoh for this task because it is much faster than phramer ? between 2 and 15 times faster, depending on the configuration ? and preliminary tests showed that there is no noticeable difference between the output of these two in terms of bleu (papineni et al, 2002) <papid> P02-1040 </papid>score.the log-linear model uses 8 features: one distortion feature, one basic lm feature, 5 features from the translation table and one sentence length feature.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4773">
<title id=" W06-3122.xml">language models and reranking for machine translation </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>it generates vector of 5 numeric values for each phrase pair: ? phrase translation probability: ?(f? |e?) = count(f? , e?)count(e?) , ?(e?|f?) = count(f? , e?)
</prevsent>
<prevsent>count(f?) 2http://www.phramer.org/ ? java-based open-source phrase based smt system 3http://www.isi.edu/licensed-sw/carmel/ 4http://www.speech.sri.com/projects/srilm/ 5http://www.iccs.inf.ed.ac.uk/pkoehn/training.tgz 150 ? lexical weighting (koehn et al, 2003): <papid> N03-1017 </papid>lex(f? |e?, a) = ? i=1 1 |{j|(i, j) ? a}| ? ?(i,j)a w(fi|ej) lex(e?|f? , a) = ? j=1 1 |{i|(i, j) ? a}| ? ?(i,j)a w(ej |fi) ? phrase penalty: ?(f? |e?) = e; log(?(f? |e?)) = 1 2.2 decoding.</prevsent>
</prevsection>
<citsent citstr=" P03-1021 ">
we used the pharaoh decoder for both the minimum error rate training (och, 2003)<papid> P03-1021 </papid>and test dataset decoding.</citsent>
<aftsection>
<nextsent>although phramer provides decoding functionality equivalent to pharaohs, we preferred to use pharaoh for this task because it is much faster than phramer ? between 2 and 15 times faster, depending on the configuration ? and preliminary tests showed that there is no noticeable difference between the output of these two in terms of bleu (papineni et al, 2002) <papid> P02-1040 </papid>score.the log-linear model uses 8 features: one distortion feature, one basic lm feature, 5 features from the translation table and one sentence length feature.</nextsent>
<nextsent>2.3 minimum error rate training.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4775">
<title id=" W06-3122.xml">language models and reranking for machine translation </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>count(f?) 2http://www.phramer.org/ ? java-based open-source phrase based smt system 3http://www.isi.edu/licensed-sw/carmel/ 4http://www.speech.sri.com/projects/srilm/ 5http://www.iccs.inf.ed.ac.uk/pkoehn/training.tgz 150 ? lexical weighting (koehn et al, 2003): <papid> N03-1017 </papid>lex(f? |e?, a) = ? i=1 1 |{j|(i, j) ? a}| ? ?(i,j)a w(fi|ej) lex(e?|f? , a) = ? j=1 1 |{i|(i, j) ? a}| ? ?(i,j)a w(ej |fi) ? phrase penalty: ?(f? |e?) = e; log(?(f? |e?)) = 1 2.2 decoding.</prevsent>
<prevsent>we used the pharaoh decoder for both the minimum error rate training (och, 2003)<papid> P03-1021 </papid>and test dataset decoding.</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
although phramer provides decoding functionality equivalent to pharaohs, we preferred to use pharaoh for this task because it is much faster than phramer ? between 2 and 15 times faster, depending on the configuration ? and preliminary tests showed that there is no noticeable difference between the output of these two in terms of bleu (papineni et al, 2002) <papid> P02-1040 </papid>score.the log-linear model uses 8 features: one distortion feature, one basic lm feature, 5 features from the translation table and one sentence length feature.</citsent>
<aftsection>
<nextsent>2.3 minimum error rate training.
</nextsent>
<nextsent>to determine the best coefficients of the log-linear model (?)
</nextsent>
<nextsent>for both the initial stage decoding and the second stage reranking, we used the un smoothed minimum error rate training (mert) component present in the phramer package.
</nextsent>
<nextsent>the mert component is highly efficient; the time required to search set of 200,000 hypotheses is less than 30 seconds per iteration (search from previous/random ? to local maximum) on 3ghz p4 machine.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4777">
<title id=" W06-3122.xml">language models and reranking for machine translation </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>6http://www.ldc.upenn.edu/catalog/catalogentry.jsp?
</prevsent>
<prevsent>catalogid=ldc2005t12 151 sentence probability n-gram hit/miss model model 1-grams 310 310 2-grams 45 45 3-grams 123 283 4-grams 235 675 table 2: number of n-gram entries in the egw lm 2.4.2 charniak parsing we used charniaks parser as an additional lm(charniak, 2001) <papid> P01-1017 </papid>in reranking.</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
the parser provides one feature for our model ? the log-grammar probability of the sentence.we retrained the parser on lower cased penn treebank ii (marcus et al, 1993), <papid> J93-2004 </papid>to match the lower cased output of the mt decoder.</citsent>
<aftsection>
<nextsent>considering the huge number of hypotheses that needed to be parsed for this task, we set it to parse very fast (using the command-line parameter -t107).
</nextsent>
<nextsent>2.5 reranking and voting.
</nextsent>
<nextsent>a ? weights vector trained over the 8 basic features (1) is used to decode n-best list.
</nextsent>
<nextsent>then, ? vector trained over all 21 features (2) is used to rerank the n-best list, potentially generating new first-best hypothesis.to improve the results, we generated during training set of distinct 2 weight vectors (4-10 different weight vectors).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4779">
<title id=" W06-1501.xml">the hidden tag model synchronous grammars for parsing resource poor languages </title>
<section> parsing arabic dialects and tree.  </section>
<citcontext>
<prevsection>
<prevsent>this model couples model of msa trees, learned from the arabic treebank, with amodel of msa-la translation, which is initial ized by hand and then trained in an unsupervisedfashion.
</prevsent>
<prevsent>parsing new la sentences then entails simultaneously building forest of msa trees andthe corresponding forest of la trees.
</prevsent>
</prevsection>
<citsent citstr=" P00-1058 ">
our implementation uses an extension of our monolingual parser (chiang, 2000) <papid> P00-1058 </papid>based on tree-substitution 1 grammar with sister adjunction (tsg+sa).the main contributions of this paper are as fol lows: 1.</citsent>
<aftsection>
<nextsent>we introduce the novel concept of hidden.
</nextsent>
<nextsent>tag model.proaches with grammar engineering (specif ically motivated from the linguistic facts).
</nextsent>
<nextsent>our approach thus exemplifies the specific strength of grammar-based approach.
</nextsent>
<nextsent>3.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4781">
<title id=" W06-1501.xml">the hidden tag model synchronous grammars for parsing resource poor languages </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>we then present the formalism, probabilistic model, and parsing algorithm (section 4).
</prevsent>
<prevsent>finally, we discuss the manual grammar engineering (section 5) and evaluation (section 6).
</prevsent>
</prevsection>
<citsent citstr=" E06-1047 ">
this paper is part of larger investigation into parsing arabic dialects (rambow et al, 2005; chiang et al, 2006).<papid> E06-1047 </papid></citsent>
<aftsection>
<nextsent>in that investigation, we examined three different approaches:?
</nextsent>
<nextsent>sentence transduction, in which dialect sentence is roughly translated into one or more msa sentences and then parsed by an msa parser.
</nextsent>
<nextsent>treebank transduction, in which the msa treebank is transduced into an approximation of la treebank, on which la parer is then trained.
</nextsent>
<nextsent>grammar transduction, which is the name given in the overview papers to the approach discussed in this paper.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4782">
<title id=" W06-1501.xml">the hidden tag model synchronous grammars for parsing resource poor languages </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the present paper provides for the first time complete technical presentation of this approach.
</prevsent>
<prevsent>overall, grammar transduction outperformed the other two approaches.
</prevsent>
</prevsection>
<citsent citstr=" W04-3207 ">
in other work, there has been fair amount of interest in parsing one language using another language, see for example (smith and smith, 2004; <papid> W04-3207 </papid>hwa et al, 2004).</citsent>
<aftsection>
<nextsent>much of this work, like ours,relies on synchronous grammars (cfgs).
</nextsent>
<nextsent>how ever, these approaches relyon parallel corpora.
</nextsent>
<nextsent>for msa and its dialects, there are no naturally occurring parallel corpora.
</nextsent>
<nextsent>it is this fact that has led us to investigate the use of explicit linguistic knowledge to complement machine learning.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4785">
<title id=" W06-1501.xml">the hidden tag model synchronous grammars for parsing resource poor languages </title>
<section> model.  </section>
<citcontext>
<prevsection>
<prevsent>4.1 the synchronous tsg+sa formalism.
</prevsent>
<prevsent>our parser (chiang, 2000) <papid> P00-1058 </papid>is based on synchronous tree-substitution grammar with sister adjunction (tsg+sa).</prevsent>
</prevsection>
<citsent citstr=" J01-1004 ">
tree-substitution grammar (schabes, 1990) is tag without auxiliary trees oradjunction; instead we include weaker composition operation, sister-adjunction (rambow et al, 2001), <papid> J01-1004 </papid>in which an initial tree is inserted between two sister nodes (see figure 4).</citsent>
<aftsection>
<nextsent>we allow multiple sister-adjunctions at the same site, similar to how schabes and shieber (1994) <papid> J94-1004 </papid>allow multiple adjunct ions of modifier auxiliary trees.a synchronous tsg+sa is set of pairs of elementary trees.</nextsent>
<nextsent>in each pair, there is one-to-onecorrespondence between the substitution/sisteradjunction sites of the two trees, which we represent using boxed indices (figure 5).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4786">
<title id=" W06-1501.xml">the hidden tag model synchronous grammars for parsing resource poor languages </title>
<section> model.  </section>
<citcontext>
<prevsection>
<prevsent>our parser (chiang, 2000) <papid> P00-1058 </papid>is based on synchronous tree-substitution grammar with sister adjunction (tsg+sa).</prevsent>
<prevsent>tree-substitution grammar (schabes, 1990) is tag without auxiliary trees oradjunction; instead we include weaker composition operation, sister-adjunction (rambow et al, 2001), <papid> J01-1004 </papid>in which an initial tree is inserted between two sister nodes (see figure 4).</prevsent>
</prevsection>
<citsent citstr=" J94-1004 ">
we allow multiple sister-adjunctions at the same site, similar to how schabes and shieber (1994) <papid> J94-1004 </papid>allow multiple adjunct ions of modifier auxiliary trees.a synchronous tsg+sa is set of pairs of elementary trees.</citsent>
<aftsection>
<nextsent>in each pair, there is one-to-onecorrespondence between the substitution/sisteradjunction sites of the two trees, which we represent using boxed indices (figure 5).
</nextsent>
<nextsent>a derivation then starts with pair of initial trees and proceeds by substituting or sister-adjoining elementary tree pairs at co indexed sites.
</nextsent>
<nextsent>in this way set of string pairs s, ??
</nextsent>
<nextsent>is generated.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4788">
<title id=" W06-1501.xml">the hidden tag model synchronous grammars for parsing resource poor languages </title>
<section> model.  </section>
<citcontext>
<prevsection>
<prevsent>mapping to ???
</prevsent>
<prevsent>using pt13, but then make special exceptions for partic-ular lexical anchors using pt11 or pt12.
</prevsent>
</prevsection>
<citsent citstr=" C02-1126 ">
finally pt2 is reestimated by em on some held-out unannotated sentences of l?, using the same method as chiang and bikel (2002) <papid> C02-1126 </papid>but on the syntactic transfer probabilities instead of the monolingual parsing model.</citsent>
<aftsection>
<nextsent>another difference is that, following bikel (2004), we do not recalculate the at each iteration, but use the initial valuesthroughout.
</nextsent>
<nextsent>arabic just as the probability model discussed in the preceding section factored the rewriting probabilities 5 into three parts, we create synchronous tsg-sa and the probabilities of hidden tag model in three steps:?
</nextsent>
<nextsent>ps and psa are the parameters of monolin-gual tsg+sa for msa.
</nextsent>
<nextsent>we extract grammar for the resource-rich language (msa)from the penn arabic treebank in process described by chiang and others (chiang, 2000; <papid> P00-1058 </papid>xia et al, 2000; <papid> W00-1307 </papid>chen, 2001).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4790">
<title id=" W06-1501.xml">the hidden tag model synchronous grammars for parsing resource poor languages </title>
<section> a synchronous tsg-sa for dialectal </section>
<citcontext>
<prevsection>
<prevsent>arabic just as the probability model discussed in the preceding section factored the rewriting probabilities 5 into three parts, we create synchronous tsg-sa and the probabilities of hidden tag model in three steps:?
</prevsent>
<prevsent>ps and psa are the parameters of monolin-gual tsg+sa for msa.
</prevsent>
</prevsection>
<citsent citstr=" W00-1307 ">
we extract grammar for the resource-rich language (msa)from the penn arabic treebank in process described by chiang and others (chiang, 2000; <papid> P00-1058 </papid>xia et al, 2000; <papid> W00-1307 </papid>chen, 2001).</citsent>
<aftsection>
<nextsent>for the lexical transfer model pt2, we cre-ate by hand probabilistic mapping between (word, pos tag) pairs in the two languages.
</nextsent>
<nextsent>for the syntactic transfer model pt1, we cre-ated by hand grammar for the resource-poor language and mapping between elementary trees in the two grammars, along with initial guesses for the mapping probabilities.we discuss the hand-crafted lexicon and synchronous grammar in the following subsections.
</nextsent>
<nextsent>5.1 lexical mapping.
</nextsent>
<nextsent>we used small, hand-crafted lexicon of 100 words which mapped all la function words and some of the most common open-class words to msa.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4791">
<title id=" W07-0408.xml">generation in machine translation from deep syntactic trees </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we propose generative model for function word insertion (prepositions,definite/indefinite articles, etc.) and subphrase reordering.
</prevsent>
<prevsent>we show by way of empirical results that this model is effective in constructing acceptable english sentences given impoverished trees.
</prevsent>
</prevsection>
<citsent citstr=" P05-1022 ">
syntactic models for language are being reintroduced into language and speech processing systems thanks to the success of sophisticated statistical models of parsing (charniak and johnson, 2005;<papid> P05-1022 </papid>collins, 2003).<papid> J03-4003 </papid></citsent>
<aftsection>
<nextsent>representing deep syntactic relationships is an open area of research; examples ofsuch models are exhibited in variety of grammatical formalisms, such as lexical functional grammars (bresnan and kaplan, 1982), head-driven phrase structure grammars (pollard and sag, 1994) and the tectogrammatical representation (tr) of the functional generative description (sgall et al, 1986).
</nextsent>
<nextsent>in this paper we do not attempt to analyze the differences of these formalisms; instead, we showhow one particular formalism is sufficient for automatic analysis and synthesis.
</nextsent>
<nextsent>specifically, in this paper we provide evidence that tr is sufficient for synthesis in english.
</nextsent>
<nextsent>augmenting models of machine translation (mt) with syntactic features is one of the main fronts of the mt research community.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4792">
<title id=" W07-0408.xml">generation in machine translation from deep syntactic trees </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we propose generative model for function word insertion (prepositions,definite/indefinite articles, etc.) and subphrase reordering.
</prevsent>
<prevsent>we show by way of empirical results that this model is effective in constructing acceptable english sentences given impoverished trees.
</prevsent>
</prevsection>
<citsent citstr=" J03-4003 ">
syntactic models for language are being reintroduced into language and speech processing systems thanks to the success of sophisticated statistical models of parsing (charniak and johnson, 2005;<papid> P05-1022 </papid>collins, 2003).<papid> J03-4003 </papid></citsent>
<aftsection>
<nextsent>representing deep syntactic relationships is an open area of research; examples ofsuch models are exhibited in variety of grammatical formalisms, such as lexical functional grammars (bresnan and kaplan, 1982), head-driven phrase structure grammars (pollard and sag, 1994) and the tectogrammatical representation (tr) of the functional generative description (sgall et al, 1986).
</nextsent>
<nextsent>in this paper we do not attempt to analyze the differences of these formalisms; instead, we showhow one particular formalism is sufficient for automatic analysis and synthesis.
</nextsent>
<nextsent>specifically, in this paper we provide evidence that tr is sufficient for synthesis in english.
</nextsent>
<nextsent>augmenting models of machine translation (mt) with syntactic features is one of the main fronts of the mt research community.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4793">
<title id=" W07-0408.xml">generation in machine translation from deep syntactic trees </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>specifically, in this paper we provide evidence that tr is sufficient for synthesis in english.
</prevsent>
<prevsent>augmenting models of machine translation (mt) with syntactic features is one of the main fronts of the mt research community.
</prevsent>
</prevsection>
<citsent citstr=" P05-1033 ">
the hiero model has been the most successful to date by incorporating syntactic structure amounting to simple tree structures (chiang, 2005).<papid> P05-1033 </papid></citsent>
<aftsection>
<nextsent>synchronous parsing models have been explored with moderate success (wu, 1997; <papid> J97-3002 </papid>quirk et al, 2005).<papid> P05-1034 </papid></nextsent>
<nextsent>an extension to this work is the exploration of deeper syntactic models, suchas tr.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4794">
<title id=" W07-0408.xml">generation in machine translation from deep syntactic trees </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>augmenting models of machine translation (mt) with syntactic features is one of the main fronts of the mt research community.
</prevsent>
<prevsent>the hiero model has been the most successful to date by incorporating syntactic structure amounting to simple tree structures (chiang, 2005).<papid> P05-1033 </papid></prevsent>
</prevsection>
<citsent citstr=" J97-3002 ">
synchronous parsing models have been explored with moderate success (wu, 1997; <papid> J97-3002 </papid>quirk et al, 2005).<papid> P05-1034 </papid></citsent>
<aftsection>
<nextsent>an extension to this work is the exploration of deeper syntactic models, suchas tr.
</nextsent>
<nextsent>however, better understanding of the synthesis of surface structure from the deep syntax is necessary.
</nextsent>
<nextsent>this paper presents generative model for surface syntax and strings of english given tectogrammati cal trees.
</nextsent>
<nextsent>sentence generation begins by inserting auxiliary words associated with auto semantic nodes;these include prepositions, subordinating conjunctions, modal verbs, and articles.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4795">
<title id=" W07-0408.xml">generation in machine translation from deep syntactic trees </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>augmenting models of machine translation (mt) with syntactic features is one of the main fronts of the mt research community.
</prevsent>
<prevsent>the hiero model has been the most successful to date by incorporating syntactic structure amounting to simple tree structures (chiang, 2005).<papid> P05-1033 </papid></prevsent>
</prevsection>
<citsent citstr=" P05-1034 ">
synchronous parsing models have been explored with moderate success (wu, 1997; <papid> J97-3002 </papid>quirk et al, 2005).<papid> P05-1034 </papid></citsent>
<aftsection>
<nextsent>an extension to this work is the exploration of deeper syntactic models, suchas tr.
</nextsent>
<nextsent>however, better understanding of the synthesis of surface structure from the deep syntax is necessary.
</nextsent>
<nextsent>this paper presents generative model for surface syntax and strings of english given tectogrammati cal trees.
</nextsent>
<nextsent>sentence generation begins by inserting auxiliary words associated with auto semantic nodes;these include prepositions, subordinating conjunctions, modal verbs, and articles.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4796">
<title id=" W07-0408.xml">generation in machine translation from deep syntactic trees </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we believe their minimally specified results are based on input which most closely resembles the in put from which we generate in our experiments.
</prevsent>
<prevsent>amalgams reordering model is similar to the one presented here; their model re orders constituents in similar way that we reorder subtrees.
</prevsent>
</prevsection>
<citsent citstr=" W98-1426 ">
both the model of amalgam and that presented here differ considerably from the n-gram models of langkilde and knight (1998), <papid> W98-1426 </papid>the tag models of bangalore and rambow (2000), <papid> C00-1007 </papid>and the stochastic generation from semantic representation approach of soricut and marcu (2006).<papid> P06-1139 </papid></citsent>
<aftsection>
<nextsent>in our work, we order the local subtrees1 of an augmented deep-structure tree based on the syntactic features of the nodes in the tree.
</nextsent>
<nextsent>by factoring these decisions to be independent for each local-subtree, the set of strings we consider is only constrained by the projective strucutre of the input tree and the local permutation limit described below.
</nextsent>
<nextsent>in the following sections we first provide brief description of the tectogrammatical representation as used in our work.
</nextsent>
<nextsent>both manually annotated and synthetic tr trees are utilized in our experiments; we present description of each type of tree as well as the motivation for using it.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4797">
<title id=" W07-0408.xml">generation in machine translation from deep syntactic trees </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we believe their minimally specified results are based on input which most closely resembles the in put from which we generate in our experiments.
</prevsent>
<prevsent>amalgams reordering model is similar to the one presented here; their model re orders constituents in similar way that we reorder subtrees.
</prevsent>
</prevsection>
<citsent citstr=" C00-1007 ">
both the model of amalgam and that presented here differ considerably from the n-gram models of langkilde and knight (1998), <papid> W98-1426 </papid>the tag models of bangalore and rambow (2000), <papid> C00-1007 </papid>and the stochastic generation from semantic representation approach of soricut and marcu (2006).<papid> P06-1139 </papid></citsent>
<aftsection>
<nextsent>in our work, we order the local subtrees1 of an augmented deep-structure tree based on the syntactic features of the nodes in the tree.
</nextsent>
<nextsent>by factoring these decisions to be independent for each local-subtree, the set of strings we consider is only constrained by the projective strucutre of the input tree and the local permutation limit described below.
</nextsent>
<nextsent>in the following sections we first provide brief description of the tectogrammatical representation as used in our work.
</nextsent>
<nextsent>both manually annotated and synthetic tr trees are utilized in our experiments; we present description of each type of tree as well as the motivation for using it.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4798">
<title id=" W07-0408.xml">generation in machine translation from deep syntactic trees </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we believe their minimally specified results are based on input which most closely resembles the in put from which we generate in our experiments.
</prevsent>
<prevsent>amalgams reordering model is similar to the one presented here; their model re orders constituents in similar way that we reorder subtrees.
</prevsent>
</prevsection>
<citsent citstr=" P06-1139 ">
both the model of amalgam and that presented here differ considerably from the n-gram models of langkilde and knight (1998), <papid> W98-1426 </papid>the tag models of bangalore and rambow (2000), <papid> C00-1007 </papid>and the stochastic generation from semantic representation approach of soricut and marcu (2006).<papid> P06-1139 </papid></citsent>
<aftsection>
<nextsent>in our work, we order the local subtrees1 of an augmented deep-structure tree based on the syntactic features of the nodes in the tree.
</nextsent>
<nextsent>by factoring these decisions to be independent for each local-subtree, the set of strings we consider is only constrained by the projective strucutre of the input tree and the local permutation limit described below.
</nextsent>
<nextsent>in the following sections we first provide brief description of the tectogrammatical representation as used in our work.
</nextsent>
<nextsent>both manually annotated and synthetic tr trees are utilized in our experiments; we present description of each type of tree as well as the motivation for using it.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4799">
<title id=" W06-1618.xml">identification of event mentions and their semantic class </title>
<section> related efforts.  </section>
<citcontext>
<prevsection>
<prevsent>in the work of filatova and hatzivassiloglou (2003), events consisted of verb and two named-entities occurring together frequently across several documents on topic.
</prevsent>
<prevsent>several recent efforts have stayed close to the linguistic definition of events.
</prevsent>
</prevsection>
<citsent citstr=" J00-4004 ">
one such example is the work of siegel and mckeown (2000) <papid> J00-4004 </papid>which showed that machine learning models could be trained to identify some of the traditional linguistic aspect ual distinctions.</citsent>
<aftsection>
<nextsent>they manually annotated the verbs in small set of texts as either state or event, and then used variety of linguistically motivated features to train machine learning models that were able to make the event/state distinction with 93.9% accuracy.
</nextsent>
<nextsent>another closely related effort was the evita system, developed by saur?
</nextsent>
<nextsent>et. al.
</nextsent>
<nextsent>(2005).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4800">
<title id=" W06-1618.xml">identification of event mentions and their semantic class </title>
<section> event identification as classification.  </section>
<citcontext>
<prevsection>
<prevsent>(2002).
</prevsent>
<prevsent>for each word in document, we assign label indicating whether the word is inside or outside of an event.
</prevsent>
</prevsection>
<citsent citstr=" W95-0107 ">
we use the standard b-i-o formulation of the word-chunking task that augments each class label with an indicator of whether the given word 1 these examples are derived from (pustejovsky, et. al. 2003b) 147 is (b)eginning, (i)nside or (o)utside of chunk (ramshaw &amp; marcus, 1995).<papid> W95-0107 </papid></citsent>
<aftsection>
<nextsent>so, for example, under this scheme, sentence (1) would have its words labeled as in table 1.
</nextsent>
<nextsent>(1) the companys salesforce [event(i_action) applauded] the [event(occurrence) shake up] the two columns of labels in table 1 show how the class labels differ depending on our task.
</nextsent>
<nextsent>if were interested only in the simple event identification task, its sufficient to know that applauded and shake both begin events (and so have the label b), up is inside an event (and so has the label i), and all other words are outside events (and so have the label o).
</nextsent>
<nextsent>these labels are shown in the column labeled event label.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4801">
<title id=" W06-1618.xml">identification of event mentions and their semantic class </title>
<section> classifier features.  </section>
<citcontext>
<prevsection>
<prevsent>the intention here is to identify correlations between classes of words and classes of events, e.g. that events are more likely to be expressed as verbs or in verb phrases than they are as nouns.
</prevsent>
<prevsent>part-of-speech: this feature contains the words part-of-speech based on the penn treebank tag set.
</prevsent>
</prevsection>
<citsent citstr=" W96-0213 ">
part-of-speech tags are assigned by the mx post maximum-entropy based part-of-speech tagger (ratnaparkhi, 1996).<papid> W96-0213 </papid></citsent>
<aftsection>
<nextsent>word event label event semantic class label the o company o o sales o force o applauded b_i_action the o shake b_occurrence up i_occurrence . o table 1: event chunks for sentence (1) 148 syntactic-chunk label: the value of this feature is b-i-o style label indicating what kind of syntactic chunk the word is contained in, e.g. noun phrase, verb phrase, or prepositional phrase.
</nextsent>
<nextsent>these are assigned using word chunking svm-based system trained on the conll-2000 data2 (which uses the lowest nodes of the penn treebank syntactic trees to break sentences into base phrases).
</nextsent>
<nextsent>word cluster: this feature indicates which verb or noun cluster the word is member of.
</nextsent>
<nextsent>the clusters were derived from the co-occurrence statistics of verbs and their direct objects, in the same manner as pradhan et. al.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4803">
<title id=" W06-1618.xml">identification of event mentions and their semantic class </title>
<section> classifier parameters.  </section>
<citcontext>
<prevsection>
<prevsent>the features described in the previous section give us way to provide the learning algorithm with the necessary information to make classification decision.
</prevsent>
<prevsent>the next step is to convert our training data into sets of features, and feed these classification instances to the learning algorithm.
</prevsent>
</prevsection>
<citsent citstr=" N01-1025 ">
for the learning task, we use the tinysvm6 support vector machine (svm) implementation in conjunction with yamcha7 (kudo &amp; matsumoto, 2001), <papid> N01-1025 </papid>suite for general-purpose chunking.</citsent>
<aftsection>
<nextsent>yamcha has number of parameters that define how it learns.
</nextsent>
<nextsent>the first of these is the window width of the sliding window?
</nextsent>
<nextsent>that it uses.
</nextsent>
<nextsent>5 we also considered the reverse classifiers, which classified all words in the hierarchy as non-events and all words outside the hierarchy as events.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4804">
<title id=" W06-2110.xml">automatic identification of english verb particle constructions using linguistic features </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>multiword expressions (hereafter mwes) are lexical items that can be decomposed into multiple simplex words and display lexical, syntactic and/or semantic idiosyncracies (sag et al, 2002; calzolari et al, 2002).
</prevsent>
<prevsent>in the case of english,mwes are conventionally categorised syntacticosemantically into classes such as compound nom inals (e.g. new york, apple juice, gm car), verb particle constructions (e.g. hand in, battle on), non-decomposable idioms (e.g. piece of cake, kick the bucket) and light-verb constructions (e.g. make mistake).
</prevsent>
</prevsection>
<citsent citstr=" P97-1018 ">
mwe research has focussed largely on their implications in language understanding, fluency and robustness (pearce, 2001; sag et al, 2002; copestake and lascarides, 1997;<papid> P97-1018 </papid>bannard et al, 2003; <papid> W03-1809 </papid>mccarthy et al, 2003; <papid> W03-1810 </papid>widdows and dorow, 2005).<papid> W05-1006 </papid></citsent>
<aftsection>
<nextsent>in this paper, our goalis to identify individual token instances of english verb particle constructions (vpcs hereafter) in running text.for the purposes of this paper, we follow baldwin (2005) in adopting the simplifying assumption that vpcs: (a) consist of head verb and unique prepositional particle (e.g. hand in, walk off); and (b) are either transitive (e.g. hand in, puton) or in transitive (e.g. battle on).
</nextsent>
<nextsent>a defining characteristic of transitive vpcs is that they can generally occur with either joined (e.g. he put on the sweater) or split (e.g. he put the sweater on) word order.
</nextsent>
<nextsent>in the case that the object is pronominal, however, the vpc must occur in split word order (c.f. *he handed in it) (huddleston and pullum, 2002; villavicencio, 2003).<papid> W03-1808 </papid></nextsent>
<nextsent>the semantics of the vpc can either derive transparently from the semantics of the head verband particle (e.g. walk off ) or be significantly removed from the semantics of the head verb and/or particle (e.g. look up); analogously, the selectional preferences of vpcs can mirror those of their head verbs or alternatively diverge markedly.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4805">
<title id=" W06-2110.xml">automatic identification of english verb particle constructions using linguistic features </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>multiword expressions (hereafter mwes) are lexical items that can be decomposed into multiple simplex words and display lexical, syntactic and/or semantic idiosyncracies (sag et al, 2002; calzolari et al, 2002).
</prevsent>
<prevsent>in the case of english,mwes are conventionally categorised syntacticosemantically into classes such as compound nom inals (e.g. new york, apple juice, gm car), verb particle constructions (e.g. hand in, battle on), non-decomposable idioms (e.g. piece of cake, kick the bucket) and light-verb constructions (e.g. make mistake).
</prevsent>
</prevsection>
<citsent citstr=" W03-1809 ">
mwe research has focussed largely on their implications in language understanding, fluency and robustness (pearce, 2001; sag et al, 2002; copestake and lascarides, 1997;<papid> P97-1018 </papid>bannard et al, 2003; <papid> W03-1809 </papid>mccarthy et al, 2003; <papid> W03-1810 </papid>widdows and dorow, 2005).<papid> W05-1006 </papid></citsent>
<aftsection>
<nextsent>in this paper, our goalis to identify individual token instances of english verb particle constructions (vpcs hereafter) in running text.for the purposes of this paper, we follow baldwin (2005) in adopting the simplifying assumption that vpcs: (a) consist of head verb and unique prepositional particle (e.g. hand in, walk off); and (b) are either transitive (e.g. hand in, puton) or in transitive (e.g. battle on).
</nextsent>
<nextsent>a defining characteristic of transitive vpcs is that they can generally occur with either joined (e.g. he put on the sweater) or split (e.g. he put the sweater on) word order.
</nextsent>
<nextsent>in the case that the object is pronominal, however, the vpc must occur in split word order (c.f. *he handed in it) (huddleston and pullum, 2002; villavicencio, 2003).<papid> W03-1808 </papid></nextsent>
<nextsent>the semantics of the vpc can either derive transparently from the semantics of the head verband particle (e.g. walk off ) or be significantly removed from the semantics of the head verb and/or particle (e.g. look up); analogously, the selectional preferences of vpcs can mirror those of their head verbs or alternatively diverge markedly.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4806">
<title id=" W06-2110.xml">automatic identification of english verb particle constructions using linguistic features </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>multiword expressions (hereafter mwes) are lexical items that can be decomposed into multiple simplex words and display lexical, syntactic and/or semantic idiosyncracies (sag et al, 2002; calzolari et al, 2002).
</prevsent>
<prevsent>in the case of english,mwes are conventionally categorised syntacticosemantically into classes such as compound nom inals (e.g. new york, apple juice, gm car), verb particle constructions (e.g. hand in, battle on), non-decomposable idioms (e.g. piece of cake, kick the bucket) and light-verb constructions (e.g. make mistake).
</prevsent>
</prevsection>
<citsent citstr=" W03-1810 ">
mwe research has focussed largely on their implications in language understanding, fluency and robustness (pearce, 2001; sag et al, 2002; copestake and lascarides, 1997;<papid> P97-1018 </papid>bannard et al, 2003; <papid> W03-1809 </papid>mccarthy et al, 2003; <papid> W03-1810 </papid>widdows and dorow, 2005).<papid> W05-1006 </papid></citsent>
<aftsection>
<nextsent>in this paper, our goalis to identify individual token instances of english verb particle constructions (vpcs hereafter) in running text.for the purposes of this paper, we follow baldwin (2005) in adopting the simplifying assumption that vpcs: (a) consist of head verb and unique prepositional particle (e.g. hand in, walk off); and (b) are either transitive (e.g. hand in, puton) or in transitive (e.g. battle on).
</nextsent>
<nextsent>a defining characteristic of transitive vpcs is that they can generally occur with either joined (e.g. he put on the sweater) or split (e.g. he put the sweater on) word order.
</nextsent>
<nextsent>in the case that the object is pronominal, however, the vpc must occur in split word order (c.f. *he handed in it) (huddleston and pullum, 2002; villavicencio, 2003).<papid> W03-1808 </papid></nextsent>
<nextsent>the semantics of the vpc can either derive transparently from the semantics of the head verband particle (e.g. walk off ) or be significantly removed from the semantics of the head verb and/or particle (e.g. look up); analogously, the selectional preferences of vpcs can mirror those of their head verbs or alternatively diverge markedly.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4807">
<title id=" W06-2110.xml">automatic identification of english verb particle constructions using linguistic features </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>multiword expressions (hereafter mwes) are lexical items that can be decomposed into multiple simplex words and display lexical, syntactic and/or semantic idiosyncracies (sag et al, 2002; calzolari et al, 2002).
</prevsent>
<prevsent>in the case of english,mwes are conventionally categorised syntacticosemantically into classes such as compound nom inals (e.g. new york, apple juice, gm car), verb particle constructions (e.g. hand in, battle on), non-decomposable idioms (e.g. piece of cake, kick the bucket) and light-verb constructions (e.g. make mistake).
</prevsent>
</prevsection>
<citsent citstr=" W05-1006 ">
mwe research has focussed largely on their implications in language understanding, fluency and robustness (pearce, 2001; sag et al, 2002; copestake and lascarides, 1997;<papid> P97-1018 </papid>bannard et al, 2003; <papid> W03-1809 </papid>mccarthy et al, 2003; <papid> W03-1810 </papid>widdows and dorow, 2005).<papid> W05-1006 </papid></citsent>
<aftsection>
<nextsent>in this paper, our goalis to identify individual token instances of english verb particle constructions (vpcs hereafter) in running text.for the purposes of this paper, we follow baldwin (2005) in adopting the simplifying assumption that vpcs: (a) consist of head verb and unique prepositional particle (e.g. hand in, walk off); and (b) are either transitive (e.g. hand in, puton) or in transitive (e.g. battle on).
</nextsent>
<nextsent>a defining characteristic of transitive vpcs is that they can generally occur with either joined (e.g. he put on the sweater) or split (e.g. he put the sweater on) word order.
</nextsent>
<nextsent>in the case that the object is pronominal, however, the vpc must occur in split word order (c.f. *he handed in it) (huddleston and pullum, 2002; villavicencio, 2003).<papid> W03-1808 </papid></nextsent>
<nextsent>the semantics of the vpc can either derive transparently from the semantics of the head verband particle (e.g. walk off ) or be significantly removed from the semantics of the head verb and/or particle (e.g. look up); analogously, the selectional preferences of vpcs can mirror those of their head verbs or alternatively diverge markedly.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4808">
<title id=" W06-2110.xml">automatic identification of english verb particle constructions using linguistic features </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this paper, our goalis to identify individual token instances of english verb particle constructions (vpcs hereafter) in running text.for the purposes of this paper, we follow baldwin (2005) in adopting the simplifying assumption that vpcs: (a) consist of head verb and unique prepositional particle (e.g. hand in, walk off); and (b) are either transitive (e.g. hand in, puton) or in transitive (e.g. battle on).
</prevsent>
<prevsent>a defining characteristic of transitive vpcs is that they can generally occur with either joined (e.g. he put on the sweater) or split (e.g. he put the sweater on) word order.
</prevsent>
</prevsection>
<citsent citstr=" W03-1808 ">
in the case that the object is pronominal, however, the vpc must occur in split word order (c.f. *he handed in it) (huddleston and pullum, 2002; villavicencio, 2003).<papid> W03-1808 </papid></citsent>
<aftsection>
<nextsent>the semantics of the vpc can either derive transparently from the semantics of the head verband particle (e.g. walk off ) or be significantly removed from the semantics of the head verb and/or particle (e.g. look up); analogously, the selectional preferences of vpcs can mirror those of their head verbs or alternatively diverge markedly.
</nextsent>
<nextsent>the syntax of the vpc can also coincide with that of the head verb (e.g. walk off ) or alternatively diverge (e.g. liftoff ).
</nextsent>
<nextsent>in the following, we review relevant past research on vpcs, focusing on the extrac tion/identification of vpcs and the prediction of the compositionality/productivity of vpcs.there is modest body of research on the identification and extraction of vpcs.
</nextsent>
<nextsent>note that in the case of vpc identification we seek to detect individual vpc token instances in corpus data, whereas in the case of vpc extraction we seek to arrive at an inventory of vpc types/lexicalitems based on analysis of token instances in corpus data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4809">
<title id=" W06-2110.xml">automatic identification of english verb particle constructions using linguistic features </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in the following, we review relevant past research on vpcs, focusing on the extrac tion/identification of vpcs and the prediction of the compositionality/productivity of vpcs.there is modest body of research on the identification and extraction of vpcs.
</prevsent>
<prevsent>note that in the case of vpc identification we seek to detect individual vpc token instances in corpus data, whereas in the case of vpc extraction we seek to arrive at an inventory of vpc types/lexicalitems based on analysis of token instances in corpus data.
</prevsent>
</prevsection>
<citsent citstr=" P03-1065 ">
li et al (2003) <papid> P03-1065 </papid>identify english vpcs(or phrasal verbs?</citsent>
<aftsection>
<nextsent>in their parlance) using hand coded regular expressions.
</nextsent>
<nextsent>baldwin and villavicencio (2002) <papid> W02-2001 </papid>extract simple list of vpcs from corpus data, while baldwin (2005) extracts vpcs with valence information under the umbrella of deep lexical acquisition.1 the method of baldwin (2005) is aimed at vpc extraction and takes into account only the syntactic features of verbs.</nextsent>
<nextsent>in this paper, our interest is in vpc identification, and we make use of deeper semantic information.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4811">
<title id=" W06-2110.xml">automatic identification of english verb particle constructions using linguistic features </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>li et al (2003) <papid> P03-1065 </papid>identify english vpcs(or phrasal verbs?</prevsent>
<prevsent>in their parlance) using hand coded regular expressions.</prevsent>
</prevsection>
<citsent citstr=" W02-2001 ">
baldwin and villavicencio (2002) <papid> W02-2001 </papid>extract simple list of vpcs from corpus data, while baldwin (2005) extracts vpcs with valence information under the umbrella of deep lexical acquisition.1 the method of baldwin (2005) is aimed at vpc extraction and takes into account only the syntactic features of verbs.</citsent>
<aftsection>
<nextsent>in this paper, our interest is in vpc identification, and we make use of deeper semantic information.
</nextsent>
<nextsent>in fraser (1976) and villavicencio (2006) it is argued that the semantic properties of verbs can determine the likelihood of their occurrence with 1the learning of lexical items in form that can be fed directly into deep grammar or other richly-annotated lexical resource 65 particles.
</nextsent>
<nextsent>bannard et al (2003) <papid> W03-1809 </papid>and mccarthy et al.</nextsent>
<nextsent>(2003) investigate methods for estimating thecompositionality of vpcs based largely on distributional similarity of the head verb and vpc.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4813">
<title id=" W06-2110.xml">automatic identification of english verb particle constructions using linguistic features </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>bannard et al (2003) <papid> W03-1809 </papid>and mccarthy et al.</prevsent>
<prevsent>(2003) investigate methods for estimating thecompositionality of vpcs based largely on distributional similarity of the head verb and vpc.</prevsent>
</prevsection>
<citsent citstr=" W03-0411 ">
ohara and wiebe (2003) <papid> W03-0411 </papid>propose method for disambiguating the verb sense of verb-pps.</citsent>
<aftsection>
<nextsent>while our interest is in vpc identificationa fundamentally syntactic taskwe draw on the shallow semantic processing employed in these methods in modelling the semantics of vpcs relative to their base verbs.
</nextsent>
<nextsent>the contribution of this paper is to combine syntactic and semantic features in the task of vpcidentification.
</nextsent>
<nextsent>the basic intuition behind the proposed method is that the selectional preferences of vpcs over predefined argument positions,2 should provide insight into whether verb and preposition in given sentential context combine to forma vpc (e.g. kim handed in the paper) or alternatively constitute verb-pp (e.g. kim walked in the room).
</nextsent>
<nextsent>that is, we seek to identify individual preposition token instances as in transitive prepositions (i.e. prepositional particles) or transitive particles based on analysis of the governing verb.the remainder of the paper is structured as follows.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4814">
<title id=" W06-2110.xml">automatic identification of english verb particle constructions using linguistic features </title>
<section> linguistic features.  </section>
<citcontext>
<prevsection>
<prevsent>finally, section 7 summarizes the paper and outlines future work.
</prevsent>
<prevsent>when verbs co-occur with particles to form vpcs, their meaning can be significantly different fromthe semantics of the head verb in isolation.
</prevsent>
</prevsection>
<citsent citstr=" W03-1812 ">
according to baldwin et al (2003), <papid> W03-1812 </papid>divergences in vpc and head verb semantics are often reflected in differing selectional preferences, as manifested in patterns of noun co-occurrence.</citsent>
<aftsection>
<nextsent>in one example cited in the paper, the cosine similarity between cut and cut out, based on word co-occurrence vectors, was found to be greater than that between cutand cut off, mirroring the intuitive compositional ity of these vpcs.(1) and (2) illustrate the difference in the selectional preferences of the verb put in isolation as compared with the vpc put on.3 2focusing exclusively on the subject and object argument positions.
</nextsent>
<nextsent>3all sense definitions are derived from wordnet 2.1.
</nextsent>
<nextsent>(1) put = place ex: put the book on the table.
</nextsent>
<nextsent>args: bookobj = book, publication, object analysis: verb-pp (2) put on = wear ex: put on the sweater . args: sweater obj = garment, clothing analysis: verb particle construction while put on is generally used in the context of wearing something, it usually occurs with clothing type nouns such as sweater and coat, whereas the simplex put has less sharply defined selectional restrictions and can occur with any noun.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4818">
<title id=" W06-2110.xml">automatic identification of english verb particle constructions using linguistic features </title>
<section> approach.  </section>
<citcontext>
<prevsection>
<prevsent>see section 5 for details.
</prevsent>
<prevsent>we use wordnet to obtain the first-sense word sense of the head nouns of subject and object phrases, according to the default word sense ranking provided within wordnet.
</prevsent>
</prevsection>
<citsent citstr=" P04-1036 ">
mccarthy et al (2004) <papid> P04-1036 </papid>found that 54% of word tokens are used with their first (or default) sense.</citsent>
<aftsection>
<nextsent>with the performance of current word sense disambiguation(wsd) systems hovering around 60-70%, simple first-sense wsd system has room for improvement, but is sufficient for our immediate purposes 67 in this paper.to evaluate our approach, we built supervised classifier using the timbl 5.1 memory based learner and training data extracted from the brown and wsj corpora.
</nextsent>
<nextsent>we evaluated out method by running rasp over brown corpus and wall street journal, as contained in the penn treebank (marcus et al, 1993).<papid> J93-2004 </papid></nextsent>
<nextsent>4.1 data classification.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4819">
<title id=" W06-2110.xml">automatic identification of english verb particle constructions using linguistic features </title>
<section> data collection.  </section>
<citcontext>
<prevsection>
<prevsent>mccarthy et al (2004) <papid> P04-1036 </papid>found that 54% of word tokens are used with their first (or default) sense.</prevsent>
<prevsent>with the performance of current word sense disambiguation(wsd) systems hovering around 60-70%, simple first-sense wsd system has room for improvement, but is sufficient for our immediate purposes 67 in this paper.to evaluate our approach, we built supervised classifier using the timbl 5.1 memory based learner and training data extracted from the brown and wsj corpora.</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
we evaluated out method by running rasp over brown corpus and wall street journal, as contained in the penn treebank (marcus et al, 1993).<papid> J93-2004 </papid></citsent>
<aftsection>
<nextsent>4.1 data classification.
</nextsent>
<nextsent>the data we consider is sentences containing prepositions tagged as either rp or ii.
</nextsent>
<nextsent>based on the output of rasp, we divide the data into four groups: group group bgroup rp &amp; ii tagged datarp tagged data ii tagged data group group contains the verb preposition token instances tagged tagged exclusively as vpcs (i.e.the preposition is never tagged as ii in combination with the given head verb).
</nextsent>
<nextsent>group contains the verb preposition token instances identified as vpcs by rasp where there were also instances of that same combination identified as verb-pps.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4832">
<title id=" W06-1416.xml">generating multiple choice test items from medical text a pilot study </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we report the results of pilot study on generating multiple-choice test items from medical text and discuss the main tasks involved in this process and how our system was evaluated by domain experts.
</prevsent>
<prevsent>althoughmultiple-choice test items (mctis) are used daily for assessment, authoring them is laborious task.
</prevsent>
</prevsection>
<citsent citstr=" H05-1103 ">
this gave rise to relatively new research area within the emerging field of text to-text generation (ttg) called multiple-choice test item generation (mctig).1 mitkov et al (2006) developed system which detects the important concepts in text automatically and produces mctis testing explicitly conveyed factual knowledge.2 this differs from most related work in mctig such asbrown et al (2005) <papid> H05-1103 </papid>and the papers in beaunlp ii (2005) which deploy various nlp techniques to produce mctis for vocabulary assessment, often using pre selected words as the input (see mitkov et al for more extensive comparisons).</citsent>
<aftsection>
<nextsent>the approach of mitkov et al is semi-automatic since the mctis have to be reviewed by domain experts to assess their usability.
</nextsent>
<nextsent>they report that semi-automatic mctig can be more than 3 times quicker than authoring of mctis without the aid of their system.
</nextsent>
<nextsent>1ttg, in which surface text is used as the input to algorithms for text production, contrasts with concept to-text generation (better known as natural language generation) which is concerned with the automatic production of text from some underlying non-linguistic representation of information (reiter and dale, 2000).
</nextsent>
<nextsent>2mitkov et al used an online textbook on linguistics as their source text.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4833">
<title id=" W06-3705.xml">language engineering and the pathway to healthcare a user oriented view </title>
<section> who are the users?.  </section>
<citcontext>
<prevsection>
<prevsent>we also need to consider more carefully the design of the feedback and verification elements of systems, and the need for realistic evaluations.
</prevsent>
<prevsent>we start by looking at the assumed profile of users of medical slt systems.
</prevsent>
</prevsection>
<citsent citstr=" W02-0710 ">
systems that have been developed so far can be divided into those for use in the doctors office ? notably, medslt (rayner and bouillon, 2002), <papid> W02-0710 </papid>cclinc (lee et al, 2002), and (honourable mention) the early work done at cmu (tomita et al, 1988)1 ? and those for use for first contact with medical professionals in the field?, developed under darpas cast programme:2 mas tor (zhou et al, 2004), speechalator (waibel et al., 2003), tran sonics (narayanan et al, 2004) and sris system (precoda et al, 2004).<papid> N04-3003 </papid></citsent>
<aftsection>
<nextsent>this distinction mainly motivates differences in hardware, overall design, and coverage, but there may be other more subtle differences that result especially from the situation in which it was envisaged that the cast systems would be used.some descriptions of the systems talk of doctors?
</nextsent>
<nextsent>and patients?
</nextsent>
<nextsent>though others do use more inclusive terms such as medical professional?.
</nextsent>
<nextsent>a significant common factor in the descriptions of the systems seems to be that it is the doctor who controls the device.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4834">
<title id=" W06-3705.xml">language engineering and the pathway to healthcare a user oriented view </title>
<section> who are the users?.  </section>
<citcontext>
<prevsection>
<prevsent>we also need to consider more carefully the design of the feedback and verification elements of systems, and the need for realistic evaluations.
</prevsent>
<prevsent>we start by looking at the assumed profile of users of medical slt systems.
</prevsent>
</prevsection>
<citsent citstr=" N04-3003 ">
systems that have been developed so far can be divided into those for use in the doctors office ? notably, medslt (rayner and bouillon, 2002), <papid> W02-0710 </papid>cclinc (lee et al, 2002), and (honourable mention) the early work done at cmu (tomita et al, 1988)1 ? and those for use for first contact with medical professionals in the field?, developed under darpas cast programme:2 mas tor (zhou et al, 2004), speechalator (waibel et al., 2003), tran sonics (narayanan et al, 2004) and sris system (precoda et al, 2004).<papid> N04-3003 </papid></citsent>
<aftsection>
<nextsent>this distinction mainly motivates differences in hardware, overall design, and coverage, but there may be other more subtle differences that result especially from the situation in which it was envisaged that the cast systems would be used.some descriptions of the systems talk of doctors?
</nextsent>
<nextsent>and patients?
</nextsent>
<nextsent>though others do use more inclusive terms such as medical professional?.
</nextsent>
<nextsent>a significant common factor in the descriptions of the systems seems to be that it is the doctor who controls the device.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4840">
<title id=" W06-2907.xml">investigating lexical substitution scoring for subtitle generation </title>
<section> compared scoring models.  </section>
<citcontext>
<prevsection>
<prevsent>3.1.1 wordnet based sense frequencies (semcor) the obvious reason that target synonym cannot substitute source in some context is if the source appears in different sense than the one in which it is synonymous with the target.
</prevsent>
<prevsent>this means that priori, synonyms of frequent senses of source word are more likely to provide correct substitutions than synonyms of the words infrequent senses.
</prevsent>
</prevsection>
<citsent citstr=" H93-1061 ">
to estimate such likelihood, our first measure is based on sense frequencies from semcor (miller et al ., 1993), <papid> H93-1061 </papid>corpus annotated with wordnet senses.</citsent>
<aftsection>
<nextsent>forgiven source word and target synonym the score is calculated as the percentage of occurrences of in semcor for which the annotated synset contains (i.e. us occurrences in which its sense is synonymous with v).
</nextsent>
<nextsent>this corresponds to the prior probability estimate that an occurrence of (in an arbitrary context) is actually synonym of v. therefore it is suitable as prior score for lexical substi tution.3 3.1.2 distributional similarity (sim) the semcor based method relies on supervised approach and requires sense annotated corpus.
</nextsent>
<nextsent>our 3note that wordnet semantic distance measures such asthose compared in (budanitsky and hirst, 2001) are not applicable here since they measure similarity between synsets rather than between synonymous words within single synset.
</nextsent>
<nextsent>47 second method uses an unsupervised distributional similarity measure to score synonym substitutions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4841">
<title id=" W06-2907.xml">investigating lexical substitution scoring for subtitle generation </title>
<section> compared scoring models.  </section>
<citcontext>
<prevsection>
<prevsent>such measures are based on the general idea of harris?
</prevsent>
<prevsent>distributional hypothesis, suggesting that words that occur within similar contexts are semantically similar (harris, 1968).
</prevsent>
</prevsection>
<citsent citstr=" P98-2127 ">
as representative of this approach we use lins dependency-based distributional similarity database.lins database was created using the particular distributional similarity measure in (lin, 1998), <papid> P98-2127 </papid>applied to large corpus of news data (64 million words) 4.two words obtain high similarity score if they occur often in the same contexts, as captured by syntactic dependency relations.</citsent>
<aftsection>
<nextsent>for example, two verbs will be considered similar if they have large common sets of modifying subjects, objects, adverbs etc. distributional similarity does not capture directly meaning equivalence and entailment but rather alooser notion of meaning similarity (geffet and da gan, 2005).
</nextsent>
<nextsent>it is typical that non substitutable words such as antonyms or co-hyponyms obtain high similarity scores.
</nextsent>
<nextsent>however, in our setting we apply the similarity score only for wordnet synonyms in which it is known priori that they are substitutable is some contexts.
</nextsent>
<nextsent>distributional similarity may thus capture the statistical degree to which the two words are substitutable in practice.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4843">
<title id=" W06-2907.xml">investigating lexical substitution scoring for subtitle generation </title>
<section> compared scoring models.  </section>
<citcontext>
<prevsection>
<prevsent>however, in our setting we apply the similarity score only for wordnet synonyms in which it is known priori that they are substitutable is some contexts.
</prevsent>
<prevsent>distributional similarity may thus capture the statistical degree to which the two words are substitutable in practice.
</prevsent>
</prevsection>
<citsent citstr=" P04-1036 ">
in fact, it has been shown that prominence in similarity score corresponds to sense frequency, which was suggested as the basis for an unsupervised method for identifying the most frequent sense of word (mccarthy et al , 2004).<papid> P04-1036 </papid></citsent>
<aftsection>
<nextsent>3.2 contextual models.
</nextsent>
<nextsent>contextual models score lexical substitutions based on the context of the sentence.
</nextsent>
<nextsent>such models try to estimate the likelihood that the target word could potentially occur in the given context of the source word and thus may replace it.
</nextsent>
<nextsent>more concretely, forgiven substitution example consisting of an original sentence = w1 . . .
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4846">
<title id=" W06-2403.xml">automatic extraction of chinese multiword expressions with a statistical tool </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>as the results of our experiment demonstrate, the tool is capable of identifying many mwes with little language-specific knowledge.
</prevsent>
<prevsent>coupled with an mt system, such tool could be useful for addressing the mwe issue.
</prevsent>
</prevsection>
<citsent citstr=" J93-1007 ">
the issue of mwe processing has attracted much attention from the natural language processing (nlp) community, including smadja, 1993; <papid> J93-1007 </papid>dagan and church, 1994; <papid> A94-1006 </papid>daille, 1995; 1995; mcenery et al, 1997; wu, 1997; <papid> J97-3002 </papid>michiels and dufour, 1998; maynard and ananiadou, 2000; merkel and andersson, 2000; piao and mcenery, 2001; sag et al, 2001; tanaka and baldwin, 2003; <papid> W03-1803 </papid>dias, 2003; baldwin et al, 2003; <papid> W03-1812 </papid>nivre and nilsson, 2004 pereira et al. 2004; piao et al, 2005.</citsent>
<aftsection>
<nextsent>study in this area covers wide range of sub-issues, including mwe identification and extraction from monolingual and multilingual corpora, classification of mwes according to variety of viewpoints such as types, compositionality and alignment of mwes across different languages.
</nextsent>
<nextsent>however studies in this area on chinese language are limited.
</nextsent>
<nextsent>a number of approaches have been suggested, including rule-based and statistical approaches, and have achieved success to various extents.
</nextsent>
<nextsent>despite this research, however, mwe processing still presents tough challenge, and it has been receiving increasing attention, as exemplified by recent mwe-related acl workshops.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4847">
<title id=" W06-2403.xml">automatic extraction of chinese multiword expressions with a statistical tool </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>as the results of our experiment demonstrate, the tool is capable of identifying many mwes with little language-specific knowledge.
</prevsent>
<prevsent>coupled with an mt system, such tool could be useful for addressing the mwe issue.
</prevsent>
</prevsection>
<citsent citstr=" A94-1006 ">
the issue of mwe processing has attracted much attention from the natural language processing (nlp) community, including smadja, 1993; <papid> J93-1007 </papid>dagan and church, 1994; <papid> A94-1006 </papid>daille, 1995; 1995; mcenery et al, 1997; wu, 1997; <papid> J97-3002 </papid>michiels and dufour, 1998; maynard and ananiadou, 2000; merkel and andersson, 2000; piao and mcenery, 2001; sag et al, 2001; tanaka and baldwin, 2003; <papid> W03-1803 </papid>dias, 2003; baldwin et al, 2003; <papid> W03-1812 </papid>nivre and nilsson, 2004 pereira et al. 2004; piao et al, 2005.</citsent>
<aftsection>
<nextsent>study in this area covers wide range of sub-issues, including mwe identification and extraction from monolingual and multilingual corpora, classification of mwes according to variety of viewpoints such as types, compositionality and alignment of mwes across different languages.
</nextsent>
<nextsent>however studies in this area on chinese language are limited.
</nextsent>
<nextsent>a number of approaches have been suggested, including rule-based and statistical approaches, and have achieved success to various extents.
</nextsent>
<nextsent>despite this research, however, mwe processing still presents tough challenge, and it has been receiving increasing attention, as exemplified by recent mwe-related acl workshops.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4848">
<title id=" W06-2403.xml">automatic extraction of chinese multiword expressions with a statistical tool </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>as the results of our experiment demonstrate, the tool is capable of identifying many mwes with little language-specific knowledge.
</prevsent>
<prevsent>coupled with an mt system, such tool could be useful for addressing the mwe issue.
</prevsent>
</prevsection>
<citsent citstr=" J97-3002 ">
the issue of mwe processing has attracted much attention from the natural language processing (nlp) community, including smadja, 1993; <papid> J93-1007 </papid>dagan and church, 1994; <papid> A94-1006 </papid>daille, 1995; 1995; mcenery et al, 1997; wu, 1997; <papid> J97-3002 </papid>michiels and dufour, 1998; maynard and ananiadou, 2000; merkel and andersson, 2000; piao and mcenery, 2001; sag et al, 2001; tanaka and baldwin, 2003; <papid> W03-1803 </papid>dias, 2003; baldwin et al, 2003; <papid> W03-1812 </papid>nivre and nilsson, 2004 pereira et al. 2004; piao et al, 2005.</citsent>
<aftsection>
<nextsent>study in this area covers wide range of sub-issues, including mwe identification and extraction from monolingual and multilingual corpora, classification of mwes according to variety of viewpoints such as types, compositionality and alignment of mwes across different languages.
</nextsent>
<nextsent>however studies in this area on chinese language are limited.
</nextsent>
<nextsent>a number of approaches have been suggested, including rule-based and statistical approaches, and have achieved success to various extents.
</nextsent>
<nextsent>despite this research, however, mwe processing still presents tough challenge, and it has been receiving increasing attention, as exemplified by recent mwe-related acl workshops.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4849">
<title id=" W06-2403.xml">automatic extraction of chinese multiword expressions with a statistical tool </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>as the results of our experiment demonstrate, the tool is capable of identifying many mwes with little language-specific knowledge.
</prevsent>
<prevsent>coupled with an mt system, such tool could be useful for addressing the mwe issue.
</prevsent>
</prevsection>
<citsent citstr=" W03-1803 ">
the issue of mwe processing has attracted much attention from the natural language processing (nlp) community, including smadja, 1993; <papid> J93-1007 </papid>dagan and church, 1994; <papid> A94-1006 </papid>daille, 1995; 1995; mcenery et al, 1997; wu, 1997; <papid> J97-3002 </papid>michiels and dufour, 1998; maynard and ananiadou, 2000; merkel and andersson, 2000; piao and mcenery, 2001; sag et al, 2001; tanaka and baldwin, 2003; <papid> W03-1803 </papid>dias, 2003; baldwin et al, 2003; <papid> W03-1812 </papid>nivre and nilsson, 2004 pereira et al. 2004; piao et al, 2005.</citsent>
<aftsection>
<nextsent>study in this area covers wide range of sub-issues, including mwe identification and extraction from monolingual and multilingual corpora, classification of mwes according to variety of viewpoints such as types, compositionality and alignment of mwes across different languages.
</nextsent>
<nextsent>however studies in this area on chinese language are limited.
</nextsent>
<nextsent>a number of approaches have been suggested, including rule-based and statistical approaches, and have achieved success to various extents.
</nextsent>
<nextsent>despite this research, however, mwe processing still presents tough challenge, and it has been receiving increasing attention, as exemplified by recent mwe-related acl workshops.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4850">
<title id=" W06-2403.xml">automatic extraction of chinese multiword expressions with a statistical tool </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>as the results of our experiment demonstrate, the tool is capable of identifying many mwes with little language-specific knowledge.
</prevsent>
<prevsent>coupled with an mt system, such tool could be useful for addressing the mwe issue.
</prevsent>
</prevsection>
<citsent citstr=" W03-1812 ">
the issue of mwe processing has attracted much attention from the natural language processing (nlp) community, including smadja, 1993; <papid> J93-1007 </papid>dagan and church, 1994; <papid> A94-1006 </papid>daille, 1995; 1995; mcenery et al, 1997; wu, 1997; <papid> J97-3002 </papid>michiels and dufour, 1998; maynard and ananiadou, 2000; merkel and andersson, 2000; piao and mcenery, 2001; sag et al, 2001; tanaka and baldwin, 2003; <papid> W03-1803 </papid>dias, 2003; baldwin et al, 2003; <papid> W03-1812 </papid>nivre and nilsson, 2004 pereira et al. 2004; piao et al, 2005.</citsent>
<aftsection>
<nextsent>study in this area covers wide range of sub-issues, including mwe identification and extraction from monolingual and multilingual corpora, classification of mwes according to variety of viewpoints such as types, compositionality and alignment of mwes across different languages.
</nextsent>
<nextsent>however studies in this area on chinese language are limited.
</nextsent>
<nextsent>a number of approaches have been suggested, including rule-based and statistical approaches, and have achieved success to various extents.
</nextsent>
<nextsent>despite this research, however, mwe processing still presents tough challenge, and it has been receiving increasing attention, as exemplified by recent mwe-related acl workshops.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4851">
<title id=" W06-2403.xml">automatic extraction of chinese multiword expressions with a statistical tool </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>a number of approaches have been suggested, including rule-based and statistical approaches, and have achieved success to various extents.
</prevsent>
<prevsent>despite this research, however, mwe processing still presents tough challenge, and it has been receiving increasing attention, as exemplified by recent mwe-related acl workshops.
</prevsent>
</prevsection>
<citsent citstr=" W03-1807 ">
directly related to our work is the development of statistical mwe tool at lancaster for searching and identifying english mwes in running text (piao et al, 2003, <papid> W03-1807 </papid>2005).</citsent>
<aftsection>
<nextsent>trained on corpus data in given domain or genre, this tool can automatically identify mwes in running text or extract mwes from corpus data from the similar domain/genre (see further information about this tool in section 3.1).
</nextsent>
<nextsent>it has been tested and compared with an english semantic tagger (rayson et al, 2004) and was found to be efficient in identifying domain-specific mwes in english corpora, and complementary to these 18 mantic tagger which relies on large manually compiled lexicon.
</nextsent>
<nextsent>other directly related work includes the development of the hyt mt system at ccid in beijing, china.
</nextsent>
<nextsent>it has been underdevelopment since 1991 (sun, 2004) and it is one of the most successful mt systems in china.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4852">
<title id=" W06-2403.xml">automatic extraction of chinese multiword expressions with a statistical tool </title>
<section> automatic identification and extrac-.  </section>
<citcontext>
<prevsection>
<prevsent>therefore it is used in our experiment.
</prevsent>
<prevsent>it is calculated as follows (scott, 2001).
</prevsent>
</prevsection>
<citsent citstr=" C94-2178 ">
forgiven pair of words and and search window w, let be the number of windows in which and co-occur, let be the number of windows in which only occurs, let be the number of windows in which only occurs, and let be the number of windows in which none of them occurs, then g2 = 2 (alna + blnb + clnc + dlnd - (a+b)ln(a+b) - (a+c)ln(a+c) - (b+d)ln(b+d) - (c+d)ln(c+d)) + (a+b+c+d)ln(a+b+c+d)) in addition to the log-likelihood, the t-score is used to filter out insignificant co-occurrence word pairs (fung and church, 1994), <papid> C94-2178 </papid>which is calculated as follows: ),( 1 )()(),( ba baba wwprob wprobwprobwwprob ?= in order to filter out weak collocates, threshold is often used, i.e. in the stage of collocation extraction, any pairs of items producing word affinity scores lower than given threshold are excluded from the mwe searching process.</citsent>
<aftsection>
<nextsent>furthermore, in order to avoid the noise caused by functional words and some extremely frequent words, stop word list is used to filter such words out from the process.
</nextsent>
<nextsent>if the corpus data is pos-tagged, some simple pos patterns can be used to filter certain syntactic patterns from the candidates.
</nextsent>
<nextsent>it can either be implemented as an internal part of the process, or as post-process.
</nextsent>
<nextsent>in our case, such pattern filters are mostly applied to the output of the mwe searching tool in order to allow the tool to be language-independent as much as possible.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4853">
<title id=" W06-1407.xml">adjectivetoverb paraphrasing in japanese based on lexical constraints of verbs </title>
<section> c=clause? d=a iv maru? ru </section>
<citcontext>
<prevsection>
<prevsent>although the rule-based model has achieved better performance than simple baseline model, there is plenty of room for improvement.table 4: recall and precision of determining verbal suffix forgiven adjective-verb pairs.
</prevsent>
<prevsent>cadc cado verbal suffix recall precision recall precision ta (va=yes?)
</prevsent>
</prevsection>
<citsent citstr=" N03-1013 ">
3/13 3/3 1/6 1/1 ta (va=no?) 42/44 42/63 18/18 18/29 re-ru 12/14 12/19 7/13 7/11 ru 3/9 3/6 0/2 0/5 tei-ru 1/5 1/7 2/8 2/6 ta / tei-ru 2/4 2/2 1/2 1/1 no rule for 11 inputs for 7 inputs total (rule) 63/100 63/100 29/56 29/53 (63%) (63%) (52%) (55%) bl 57/100 57/148 24/56 24/83 (57%) (39%) (43%) (29%) future work includes (i) to enlarge our two resources as in (dorr, 1997; habash and dorr, 2003) <papid> N03-1013 </papid>evolving an effective construction method, (ii) intrinsic evaluation of those resources, and, of course, (iii) to enhance the paraphrasing models through further experiments with larger test-set.</citsent>
<aftsection>




</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4854">
<title id=" W06-1908.xml">dialogue based question answering system in telugu </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>such question answering system can be effectively utilized when integrated with speech input and speech output system.
</prevsent>
<prevsent>ever since question answering (qa) emerged as an active research field, the community has slowly diversified question types, increased question complexity, and refined evaluation met-rics, as reflected by the trec (text retrieval conference) qa track (voorhees, 2004).
</prevsent>
</prevsection>
<citsent citstr=" C02-1042 ">
several qa systems have responded to these changes in the nature of the qa task by incorporating various knowledge resources (hovy et al, 2002), <papid> C02-1042 </papid>handling of additional types of questions tapping into external data sources such as web, encyclopedia, and databases in order to find the answer candidates, which may then be located in the specific corpus being searched (xu et al, 2003).</citsent>
<aftsection>
<nextsent>the most popular classes of technique for qa are open-domain and restricted-domain (diekema et al, 2004, <papid> W04-0502 </papid>doan-nguyen et al, 2004).</nextsent>
<nextsent>these two domains use thesauri and lexicons in classifying documents and categorizing the questions.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4855">
<title id=" W06-1908.xml">dialogue based question answering system in telugu </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>ever since question answering (qa) emerged as an active research field, the community has slowly diversified question types, increased question complexity, and refined evaluation met-rics, as reflected by the trec (text retrieval conference) qa track (voorhees, 2004).
</prevsent>
<prevsent>several qa systems have responded to these changes in the nature of the qa task by incorporating various knowledge resources (hovy et al, 2002), <papid> C02-1042 </papid>handling of additional types of questions tapping into external data sources such as web, encyclopedia, and databases in order to find the answer candidates, which may then be located in the specific corpus being searched (xu et al, 2003).</prevsent>
</prevsection>
<citsent citstr=" W04-0502 ">
the most popular classes of technique for qa are open-domain and restricted-domain (diekema et al, 2004, <papid> W04-0502 </papid>doan-nguyen et al, 2004).</citsent>
<aftsection>
<nextsent>these two domains use thesauri and lexicons in classifying documents and categorizing the questions.
</nextsent>
<nextsent>open domain question answering deals with questions about nearly everything and can only relyon general ontology.
</nextsent>
<nextsent>it has become very active research area over the past few years.
</nextsent>
<nextsent>on the other hand, restricted-domain question answering (rdqa) deals with questions under specific domain.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4856">
<title id=" W06-2503.xml">relating wordnet senses for word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>one response to this is wns# gloss 1 your basis for belief or disbelief; knowledge on which to base belief; the evidence that smoking causes lung cancer is very compelling?
</prevsent>
<prevsent>2 an indication that makes something evident; his trembling was evidence of his fear?
</prevsent>
</prevsection>
<citsent citstr=" P04-1036 ">
3 (law) all the means by which any alleged matter of fact whose truth is investigated at judicial trial is established or disproved figure 1: the senses of evidence in wordnet to exploit the natural skew of the data and focus on finding the first (predominant) sense from sample of text (mccarthy et al, 2004).<papid> P04-1036 </papid></citsent>
<aftsection>
<nextsent>further contextual wsd may be required, but the technique provides useful unsupervised back-off method.the other major problem for wsd is the granu larity of the sense inventory since pre-existinglexical resource is often too fine-grained, with narrow sense distinctions that are irrelevant for the intended application.
</nextsent>
<nextsent>for example, wordnet (fell baum, 1998) which is widely used and publicly available, has great many subtle distinctions that may in the end not be required.
</nextsent>
<nextsent>for example, in figure 1 we show the three senses (wns#) for evidence from wordnet version 1.7.
</nextsent>
<nextsent>1 these are all clearly related.one promising approach for improving accuracy is to disambiguate to coarser-grained inventory, which groups together the related senses of word.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4857">
<title id=" W06-2503.xml">relating wordnet senses for word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>there are however many systems using man-made resources, particularly wordnet, which have other purposes in mind, such as entailment for applications such as question-answering and information-extraction (dagan et al, 2005).
</prevsent>
<prevsent>there have been several attempts to group wordnet senses using various different types of informationsources.
</prevsent>
</prevsection>
<citsent citstr=" P98-2127 ">
this paper describes work to automatically relate wordnet word senses using automatically acquired thesauruses (lin, 1998) <papid> P98-2127 </papid>and wordnet similarity measures (patwardhan and pedersen, 2003).this work proposes using graded word sense relationships rather than fixed groupings (clusters).previous research has focused on clustering wordnet senses into groups.</citsent>
<aftsection>
<nextsent>one problem is that to do this stopping condition is required such as the number of clusters required for each word.
</nextsent>
<nextsent>this has been done with the numbers determined by the gold-standard for the purposes of evaluation (agirre and lopez de lacalle, 2003) but ultimately the right number of classes for each word cannot usually be predetermined even if one knows the application, unless only sample ofwords are being handled.
</nextsent>
<nextsent>in cases where gold standard is provided by humans it is clear that further relationships could be drawn.
</nextsent>
<nextsent>for example, in the groups (hereafter referred to as segr)made publicly available for the senseval-2 english lexical sample (kilgarriff, 2001) (hereafter referred to as seval-2 eng lex) child is group edas shown in table 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4859">
<title id=" W06-2503.xml">relating wordnet senses for word sense disambiguation </title>
<section> 2 human offspring </section>
<citcontext>
<prevsection>
<prevsent>as well as evaluating against gold-standard, we also look at the effect of the rlists and the gold standards themselves on wsd.
</prevsent>
<prevsent>since the focus of this paper is not the wsd system, but the sense inventory, we use simple wsd heuristic which uses the first sense of word in all contexts, where the first sense of every word is specified by resource.
</prevsent>
</prevsection>
<citsent citstr=" W04-0811 ">
while contextual evidence is required for accurate wsd, it is useful to look at this heuristic since it is so widely used as back-off modelby many systems and is hard to beat on an all words task (snyder and palmer, 2004).<papid> W04-0811 </papid></citsent>
<aftsection>
<nextsent>we contrast the performance of first sense heuristics i) from semcor (miller et al, 1993) <papid> H93-1061 </papid>and ii) derived automatically from the bnc following (mccarthy et al, 2004) <papid> P04-1036 </papid>and also iii) an upper-bound first sense heuristic extracted from the test data.</nextsent>
<nextsent>the paper is organised as follows.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4860">
<title id=" W06-2503.xml">relating wordnet senses for word sense disambiguation </title>
<section> 2 human offspring </section>
<citcontext>
<prevsection>
<prevsent>since the focus of this paper is not the wsd system, but the sense inventory, we use simple wsd heuristic which uses the first sense of word in all contexts, where the first sense of every word is specified by resource.
</prevsent>
<prevsent>while contextual evidence is required for accurate wsd, it is useful to look at this heuristic since it is so widely used as back-off modelby many systems and is hard to beat on an all words task (snyder and palmer, 2004).<papid> W04-0811 </papid></prevsent>
</prevsection>
<citsent citstr=" H93-1061 ">
we contrast the performance of first sense heuristics i) from semcor (miller et al, 1993) <papid> H93-1061 </papid>and ii) derived automatically from the bnc following (mccarthy et al, 2004) <papid> P04-1036 </papid>and also iii) an upper-bound first sense heuristic extracted from the test data.</citsent>
<aftsection>
<nextsent>the paper is organised as follows.
</nextsent>
<nextsent>in the next section we describe some related work.
</nextsent>
<nextsent>in section 3 we describe the two methods we will use to relate senses.
</nextsent>
<nextsent>our experiments are described in section 4.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4863">
<title id=" W06-2503.xml">relating wordnet senses for word sense disambiguation </title>
<section> 2 human offspring </section>
<citcontext>
<prevsection>
<prevsent>we follow this with discussion and conclusion.
</prevsent>
<prevsent>18 2 related work.
</prevsent>
</prevsection>
<citsent citstr=" N01-1010 ">
there is significant amount of previous workon grouping wordnet word senses using number of different information sources, such as predicate argument structure (palmer et al, forth com ing), information from wordnet (mihalcea and moldovan, 2001; tomuro, 2001) <papid> N01-1010 </papid>2 and other lexical resources (peters and peters, 1998) translations, system conf usability, topic signature and contextual evidence (agirre and lopez de lacalle, 2003).</citsent>
<aftsection>
<nextsent>there is also work on grouping senses of other inventories using information in the inventory (dolan, 1994) <papid> C94-2113 </papid>along with information retrieval techniques (chen and chang, 1998).<papid> J98-1003 </papid></nextsent>
<nextsent>one method presented here (referred to as dist and described in section 3) relates most to that of agirre and lopez de lacalle (2003).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4864">
<title id=" W06-2503.xml">relating wordnet senses for word sense disambiguation </title>
<section> 2 human offspring </section>
<citcontext>
<prevsection>
<prevsent>18 2 related work.
</prevsent>
<prevsent>there is significant amount of previous workon grouping wordnet word senses using number of different information sources, such as predicate argument structure (palmer et al, forth com ing), information from wordnet (mihalcea and moldovan, 2001; tomuro, 2001) <papid> N01-1010 </papid>2 and other lexical resources (peters and peters, 1998) translations, system conf usability, topic signature and contextual evidence (agirre and lopez de lacalle, 2003).</prevsent>
</prevsection>
<citsent citstr=" C94-2113 ">
there is also work on grouping senses of other inventories using information in the inventory (dolan, 1994) <papid> C94-2113 </papid>along with information retrieval techniques (chen and chang, 1998).<papid> J98-1003 </papid></citsent>
<aftsection>
<nextsent>one method presented here (referred to as dist and described in section 3) relates most to that of agirre and lopez de lacalle (2003).
</nextsent>
<nextsent>they use contexts of the senses gathered directly from either manually sense tagged corpora, or using instances of monosemous relatives?
</nextsent>
<nextsent>which are monosemous words related to one of the target word senses in wordnet.
</nextsent>
<nextsent>we use contexts of occurrence indirectly.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4865">
<title id=" W06-2503.xml">relating wordnet senses for word sense disambiguation </title>
<section> 2 human offspring </section>
<citcontext>
<prevsection>
<prevsent>18 2 related work.
</prevsent>
<prevsent>there is significant amount of previous workon grouping wordnet word senses using number of different information sources, such as predicate argument structure (palmer et al, forth com ing), information from wordnet (mihalcea and moldovan, 2001; tomuro, 2001) <papid> N01-1010 </papid>2 and other lexical resources (peters and peters, 1998) translations, system conf usability, topic signature and contextual evidence (agirre and lopez de lacalle, 2003).</prevsent>
</prevsection>
<citsent citstr=" J98-1003 ">
there is also work on grouping senses of other inventories using information in the inventory (dolan, 1994) <papid> C94-2113 </papid>along with information retrieval techniques (chen and chang, 1998).<papid> J98-1003 </papid></citsent>
<aftsection>
<nextsent>one method presented here (referred to as dist and described in section 3) relates most to that of agirre and lopez de lacalle (2003).
</nextsent>
<nextsent>they use contexts of the senses gathered directly from either manually sense tagged corpora, or using instances of monosemous relatives?
</nextsent>
<nextsent>which are monosemous words related to one of the target word senses in wordnet.
</nextsent>
<nextsent>we use contexts of occurrence indirectly.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4866">
<title id=" W06-2503.xml">relating wordnet senses for word sense disambiguation </title>
<section> 2 human offspring </section>
<citcontext>
<prevsection>
<prevsent>3 while related senses may not have lot of shared contexts directly, because of sparse data, they may have semantic associations with the same subset of words that share similar distributional contexts with the target word.
</prevsent>
<prevsent>this method avoids reliance on sense-tagged data or monosemous relatives because the distributional neighbours can be obtained automatically from raw text.
</prevsent>
</prevsection>
<citsent citstr=" P05-1005 ">
our other method relates to the findings of kohomban and lee (2005).<papid> P05-1005 </papid></citsent>
<aftsection>
<nextsent>we use the jiang conrath score (jcn) in the wordnet similarity package.
</nextsent>
<nextsent>this is distance measure between wordnet senses given corpus frequency counts and the structure of the wordnet hierarchy.
</nextsent>
<nextsent>it is described in more detail below.
</nextsent>
<nextsent>kohomban and lee (2005) <papid> P05-1005 </papid>get good results on disambiguation of the senseval all-words tasks using the 25 unique beginners from the wordnet hierarchy for training coarse-grained wsd system and then using first sense heuristic (provided using the frequency 2mihalcea and moldovan group wordnet synonym sets (synsets) rather than word senses.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4870">
<title id=" W06-2503.xml">relating wordnet senses for word sense disambiguation </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>5we are grateful to adam kilgarriff for suggesting the task.
</prevsent>
<prevsent>6we will make the questionnaire publicly available with the gold standard.
</prevsent>
</prevsection>
<citsent citstr=" H05-1053 ">
20 word (#senses) pwa art (4) 44.44 authority (7) 52.38 bar (13) 87.07 bum (4) 100.00 chair (4) 43.75 channel (7) 46.03 child (4) 66.67 circuit (6) 46.67 day (10) 64.44 facility (5) 86.67 fatigue (4) 44.44 feeling (6) 42.22 hearth (3) 55.56 mouth (8) 40.48 nation (4) 100.00 nature (5) 73.33 post (8) 92.86 restraint (6) 42.22 sense (5) 73.33 stress (5) 73.33 overall pwa 66.94 given leniency 88.10 table 2: pairwise agreement % senses and the fact that figures for sense annotation with three judges (as opposed to two, with third to break ties) are reported in this region (koelinget al, 2005).<papid> H05-1053 </papid></citsent>
<aftsection>
<nextsent>again, there are no details on annotation and agreement for segr.
</nextsent>
<nextsent>4.2 agreement of automatic methods with rs.
</nextsent>
<nextsent>figure 3 shows the pwa of the automatic methodsjcn and dist when calculated against the rs gold standard at various threshold cut-offs.
</nextsent>
<nextsent>the difference of the best performance for these two methods (61.1% dist and 62.2% for jcn) are not statistically significant (using the chi-squared test).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4875">
<title id=" W06-3003.xml">modeling reference interviews as a basis for improving automatic qa systems </title>
<section> system capabilities.  </section>
<citcontext>
<prevsection>
<prevsent>the sequence of questions and answers forms natural language dialogue between the user and the system.
</prevsent>
<prevsent>this necessitates nlp processing at the discourse level, primary task of which is to resolve references across the session.
</prevsent>
</prevsection>
<citsent citstr=" W04-2504 ">
building on previous work in this area done for the context track of trec 2001 (harabagiu et al 2001) and additional work (chai and jin, 2004) <papid> W04-2504 </papid>suggesting discourse structures are needed to understand the question/answer sequence, we have developed session-based reference resolution capability.</citsent>
<aftsection>
<nextsent>in dialogue, the user naturally includes referring phrases that require several types of resolution.
</nextsent>
<nextsent>the simplest case is that of referring pronouns, where the user is asking follow-up question, for example: q1: when did madonna enter the music business?
</nextsent>
<nextsent>a1: madonna first album, madonna, came out in 1983 and since then she had string of hits, been major influence in the music industry and become an international icon.
</nextsent>
<nextsent>q2: when did she first move to nyc?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4876">
<title id=" W06-1608.xml">the impact of parse quality on syntactically informed statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>as the amount of data increases,parse quality improves, leading to improvements in machine translation output and results that significantly outperform state-of-the-art phrasal baseline.
</prevsent>
<prevsent>the current study is response to question that proponents of syntactically-informed machine translation frequently encounter: how sensitive isa syntactically-informed machine translation system to the quality of the input syntactic analysis?
</prevsent>
</prevsection>
<citsent citstr=" N03-1017 ">
it has been shown that phrasal machine translation systems are not affected by the quality of the in put word alignments (koehn et al, 2003).<papid> N03-1017 </papid></citsent>
<aftsection>
<nextsent>this finding has generally been cast in favorable terms:such systems are robust to poor quality word alignment.
</nextsent>
<nextsent>a less favorable interpretation of these results might be to conclude that phrasal statistical machine translation (smt) systems do not stand to benefit from improvements in word alignment.in similar vein, one might ask whether contemporary syntactically-informed machine translation systems would benefit from improvement sin parse accuracy.
</nextsent>
<nextsent>one possibility is that current syntactically-informed smt systems are deriving only limited value from the syntactic analyses, and would therefore not benefit from improved analyses.
</nextsent>
<nextsent>another possibility is that syntactic analysis does indeed contain valuable information that could be exploited by machine learning techniques, but that current parsers are not of sufficient quality to be of use in smt.with these questions and concerns, let us begin.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4877">
<title id=" W06-1608.xml">the impact of parse quality on syntactically informed statistical machine translation </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>each child has single parent, withthe lexical root of the sentence dependent on synthetic root node.
</prevsent>
<prevsent>we use the parsing approach described in (corston-oliver et al, 2006).
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
the parser is trained on dependencies extracted from the english penn treebank version 3.0 (marcus et al, 1993) <papid> J93-2004 </papid>by using the head-percolation rules of (yamada and matsumoto, 2003).</citsent>
<aftsection>
<nextsent>given sentence x, the goal of the parser is to find the highest-scoring parse y?
</nextsent>
<nextsent>among all possible parses ? : y?
</nextsent>
<nextsent>= argmax yy s(x,y) (1) the score of given parse is the sum of the 62 scores of all its dependency links (i, j) ? y: s(x,y) = ?
</nextsent>
<nextsent>(i, j)y d(i, j) = ?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4878">
<title id=" W06-1608.xml">the impact of parse quality on syntactically informed statistical machine translation </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>various combinations of these features are used, for example new feature is created that combines the pos of the parent, lexeme of the parent, pos of the child and lexeme of the child.
</prevsent>
<prevsent>each feature is also conjoined with the direction and distance of the parent, e.g. does the child precede or follow the parent, and how many tokens intervene?
</prevsent>
</prevsection>
<citsent citstr=" W02-1001 ">
to set the weight vector w, we train twenty averaged perceptrons (collins, 2002) <papid> W02-1001 </papid>on different shuffles of data drawn from sections 0221 of the penn treebank.</citsent>
<aftsection>
<nextsent>the averaged perceptrons are then combined to form bayes point machine (herbrich et al, 2001; harrington et al, 2003), resulting in linear classifier that is competitive with wide margin techniques.to find the optimal parse given the weight vector and feature vector f(i, j) we use the decoder described in (eisner, 1996).<papid> C96-1058 </papid></nextsent>
<nextsent>2.2 treelet translation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4879">
<title id=" W06-1608.xml">the impact of parse quality on syntactically informed statistical machine translation </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>each feature is also conjoined with the direction and distance of the parent, e.g. does the child precede or follow the parent, and how many tokens intervene?
</prevsent>
<prevsent>to set the weight vector w, we train twenty averaged perceptrons (collins, 2002) <papid> W02-1001 </papid>on different shuffles of data drawn from sections 0221 of the penn treebank.</prevsent>
</prevsection>
<citsent citstr=" C96-1058 ">
the averaged perceptrons are then combined to form bayes point machine (herbrich et al, 2001; harrington et al, 2003), resulting in linear classifier that is competitive with wide margin techniques.to find the optimal parse given the weight vector and feature vector f(i, j) we use the decoder described in (eisner, 1996).<papid> C96-1058 </papid></citsent>
<aftsection>
<nextsent>2.2 treelet translation.
</nextsent>
<nextsent>for syntactically-informed translation, we follow the treelet translation approach described in (quirk et al, 2005).<papid> P05-1034 </papid></nextsent>
<nextsent>in this approach, translation is guided by treelet translation pairs.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4880">
<title id=" W06-1608.xml">the impact of parse quality on syntactically informed statistical machine translation </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>the averaged perceptrons are then combined to form bayes point machine (herbrich et al, 2001; harrington et al, 2003), resulting in linear classifier that is competitive with wide margin techniques.to find the optimal parse given the weight vector and feature vector f(i, j) we use the decoder described in (eisner, 1996).<papid> C96-1058 </papid></prevsent>
<prevsent>2.2 treelet translation.</prevsent>
</prevsection>
<citsent citstr=" P05-1034 ">
for syntactically-informed translation, we follow the treelet translation approach described in (quirk et al, 2005).<papid> P05-1034 </papid></citsent>
<aftsection>
<nextsent>in this approach, translation is guided by treelet translation pairs.
</nextsent>
<nextsent>here, treelet is connected subgraph of dependency tree.
</nextsent>
<nextsent>a treelet translation pair consists of source treelet s, target treelet , and word alignment ? s? such that for all ? s, there exists unique ? such that (s, t)?
</nextsent>
<nextsent>a, and if is the root of , there is unique ? such that (s, t) ? a. translation of sentence begins by parsing that sentence into dependency representation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4883">
<title id=" W06-1608.xml">the impact of parse quality on syntactically informed statistical machine translation </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>translations are scored according to log-linearcombination of feature functions, each scoring different aspects of the translation process.
</prevsent>
<prevsent>we use beam search decoder to find the best translation ? according to the log-linear combination of models: ? = argmax { ? ff ? f (s,t,a) } (3) the models include inverted and direct channel models estimated by relative frequency, lexical weighting channel models following (vogel et al,2003), trigram target language model using modified kneser-ney smoothing (goodman, 2001), an order model following (quirk et al, 2005), <papid> P05-1034 </papid>and word count and phrase count functions.</prevsent>
</prevsection>
<citsent citstr=" P03-1021 ">
the weights for these models are determined using the method described in (och, 2003).<papid> P03-1021 </papid></citsent>
<aftsection>
<nextsent>to estimate the models and extract the treelets,we begin from parallel corpus.
</nextsent>
<nextsent>first the corpus is word-aligned using giza++ (och and ney, 2000), <papid> P00-1056 </papid>then the source sentence are parsed, and finally dependencies are projected onto the target side following the heuristics described in (quirk et al., 2005).<papid> P05-1034 </papid></nextsent>
<nextsent>this word aligned parallel dependency tree corpus provides training material for an order model and target language tree-based language model.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4884">
<title id=" W06-1608.xml">the impact of parse quality on syntactically informed statistical machine translation </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>the weights for these models are determined using the method described in (och, 2003).<papid> P03-1021 </papid></prevsent>
<prevsent>to estimate the models and extract the treelets,we begin from parallel corpus.</prevsent>
</prevsection>
<citsent citstr=" P00-1056 ">
first the corpus is word-aligned using giza++ (och and ney, 2000), <papid> P00-1056 </papid>then the source sentence are parsed, and finally dependencies are projected onto the target side following the heuristics described in (quirk et al., 2005).<papid> P05-1034 </papid></citsent>
<aftsection>
<nextsent>this word aligned parallel dependency tree corpus provides training material for an order model and target language tree-based language model.
</nextsent>
<nextsent>we also extract treelet translation pairs from this parallel corpus.
</nextsent>
<nextsent>to limit the combinatorial explosion of tree lets, we only gather tree lets that contain at most four words and at most two gaps in the surface string.
</nextsent>
<nextsent>this limits the number of mappings to be o(n3) in the worst case, where is the number of nodes in the dependency tree.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4886">
<title id=" W06-1608.xml">the impact of parse quality on syntactically informed statistical machine translation </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>may occur bound to the verb or may detach and occur at considerable distance from the verb on which they depend, and extra position of various kinds of subordinate clause is common.
</prevsent>
<prevsent>in the case of extrapo sition, for example, more than one third of relative clauses in human-translated german technical text are extraposed.
</prevsent>
</prevsection>
<citsent citstr=" C02-1036 ">
for comparable english text the figure is considerably less than one percent (ga monet al, 2002).<papid> C02-1036 </papid></citsent>
<aftsection>
<nextsent>2.3.2 japanese word order in japanese is rather different fromenglish.
</nextsent>
<nextsent>english has the canonical constituent order subject-verb-object, whereas japanese prefers subject-object-verb order.
</nextsent>
<nextsent>prepositional phrases in english generally correspond to post positional phrases in japanese.
</nextsent>
<nextsent>japanese noun phrases are strictly head-final whereas english noun phrases allow post modifiers such as prepositional phrases,relative clauses and adjectives.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4887">
<title id=" W06-1608.xml">the impact of parse quality on syntactically informed statistical machine translation </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>rather than randomly adding noise to the parses, we decided to vary the quality in ways that more closely mimic the situation that confronts usas we develop machine translation systems.
</prevsent>
<prevsent>annotating data for pos requires considerably les shuman time and expertise than annotating syntactic relations.
</prevsent>
</prevsection>
<citsent citstr=" N03-1033 ">
we therefore used an automatic pos tagger (toutanova et al, 2003) <papid> N03-1033 </papid>trained on the complete training section of the penn treebank (sec tions 0221).</citsent>
<aftsection>
<nextsent>annotating syntactic dependencies is time consuming and requires considerable linguistic expertise.1 we can well imagine annotating syntactic dependencies in order to develop machine translation system by annotating first small quantity of data, training parser, training system that uses the parses produced by that parserand assessing the quality of the machine translation output.
</nextsent>
<nextsent>having assessed the quality of the out put, one might annotate additional data and train systems until it appears that the quality of thema chine translation output is no longer improving.
</nextsent>
<nextsent>we therefore produced parsers of varying quality by training on the first sentences of sections 02?
</nextsent>
<nextsent>21 of the penn treebank, where ranged from 250to 39,892 (the complete training section).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4891">
<title id=" W06-1608.xml">the impact of parse quality on syntactically informed statistical machine translation </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>conversely, the wall street journal articles contain no discussion of such topics as the intricacies of sql database queries.
</prevsent>
<prevsent>3.2 translation quality.
</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
table 2 presents the impact of parse quality on treelet translation system, measured using bleu (papineni et al, 2002).<papid> P02-1040 </papid></citsent>
<aftsection>
<nextsent>since our main goal is to investigate the impact of parser accuracy on translation quality, we have varied the parser training data, but have held the mt training data, part-of speech-tagger, and all other factors constant.
</nextsent>
<nextsent>we observe an upward trend in bleu score as more training data is made available to the parser; the trend is even clearer in japanese.2 as baseline, we include right-branching dependency trees, i.e., trees in which the parent of each word is its left 2this is particularly encouraging since various people have remarked to us that syntax-based smt systems may be disadvantaged under n-gram scoring techniques such as bleu.
</nextsent>
<nextsent>eg ej phrasal decoder 31.71.2 32.90.9 treelet decoder right-branching 31.41.3 28.00.7 250 sentences 32.81.4 34.10.9 2,500 sentences 33.01.4 34.61.0 25,000 sentences 33.71.5 35.70.9 39,892 sentences 33.61.5 36.01.0table 2: bleu score vs. decoder and parser variants.
</nextsent>
<nextsent>here sentences refer to the amount of parser training data, not mt training data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4893">
<title id=" W06-3606.xml">practical markov logic containing first order quantifiers with application to identity uncertainty </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>identity uncertainty (also known as record linkage, deduplication, object identification, and co-reference resolution) is the problem of determining whether set of constants (mentions) refer to the same object(entity).
</prevsent>
<prevsent>successful identity resolution enables vision systems to track objects, database systems to deduplicate redundant records, and text processing systems to resolve disparate mentions of people, organizations, and locations.
</prevsent>
</prevsection>
<citsent citstr=" J01-4004 ">
many probabilistic models of object identification have been proposed in the past 40 years in databases(fellegi and sunter, 1969; winkler, 1993) and natural language processing (mccarthy and lehnert, 1995; soon et al, 2001).<papid> J01-4004 </papid></citsent>
<aftsection>
<nextsent>with the introduction of statistical relational learning, more sophisticated models of identity uncertainty have been developed that consider the dependencies between related consolidation decisions.
</nextsent>
<nextsent>most relevant to this work are the recent relational models of identity uncertainty (milch et al, 2005; mccallum and wellner, 2003; parag and domingos,2004).
</nextsent>
<nextsent>mccallum and wellner (2003) present experiments using conditional random field that factorizes into product of pairwise decisions about mention pairs (model 3).
</nextsent>
<nextsent>these pairwise decisions are made collectively using relational inference; however,as pointed out in milch et al (2004), there are shortcomings to this model that stem from the fact that it does not capture features of objects, only of mentionpairs.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4894">
<title id=" W06-1639.xml">get out the vote determining support or opposition from congressional floor debate transcripts </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(continuous single-speaker segment oftext) represents support for or opposition to proposed piece of legislation.
</prevsent>
<prevsent>note that from an experimental point of view, this is very convenient problem to work with because we can automatically determine ground truth (and thus avoid the need for manual annotation) simply by consulting publicly available voting records.
</prevsent>
</prevsection>
<citsent citstr=" P88-1016 ">
task properties determining whether or not speaker supports proposal falls within the realmof sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language (early work includes wiebe and rapaport (1988), <papid> P88-1016 </papid>hearst (1992), sack (1994), and wiebe (1994); <papid> J94-2004 </papid>see esuli(2006) for an active bibliography).</citsent>
<aftsection>
<nextsent>in particular, since we treat each individual speech withina debate as single document?, we are considering version of document-level sentiment-polarityclassification, namely, automatically distinguishing between positive and negative documents (das and chen, 2001; pang et al , 2002; <papid> W02-1011 </papid>turney, 2002; <papid> P02-1053 </papid>dave et al , 2003).</nextsent>
<nextsent>most sentiment-polarity classifiers proposed inthe recent literature categorize each document in dependently.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4895">
<title id=" W06-1639.xml">get out the vote determining support or opposition from congressional floor debate transcripts </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(continuous single-speaker segment oftext) represents support for or opposition to proposed piece of legislation.
</prevsent>
<prevsent>note that from an experimental point of view, this is very convenient problem to work with because we can automatically determine ground truth (and thus avoid the need for manual annotation) simply by consulting publicly available voting records.
</prevsent>
</prevsection>
<citsent citstr=" J94-2004 ">
task properties determining whether or not speaker supports proposal falls within the realmof sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language (early work includes wiebe and rapaport (1988), <papid> P88-1016 </papid>hearst (1992), sack (1994), and wiebe (1994); <papid> J94-2004 </papid>see esuli(2006) for an active bibliography).</citsent>
<aftsection>
<nextsent>in particular, since we treat each individual speech withina debate as single document?, we are considering version of document-level sentiment-polarityclassification, namely, automatically distinguishing between positive and negative documents (das and chen, 2001; pang et al , 2002; <papid> W02-1011 </papid>turney, 2002; <papid> P02-1053 </papid>dave et al , 2003).</nextsent>
<nextsent>most sentiment-polarity classifiers proposed inthe recent literature categorize each document in dependently.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4896">
<title id=" W06-1639.xml">get out the vote determining support or opposition from congressional floor debate transcripts </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>note that from an experimental point of view, this is very convenient problem to work with because we can automatically determine ground truth (and thus avoid the need for manual annotation) simply by consulting publicly available voting records.
</prevsent>
<prevsent>task properties determining whether or not speaker supports proposal falls within the realmof sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language (early work includes wiebe and rapaport (1988), <papid> P88-1016 </papid>hearst (1992), sack (1994), and wiebe (1994); <papid> J94-2004 </papid>see esuli(2006) for an active bibliography).</prevsent>
</prevsection>
<citsent citstr=" W02-1011 ">
in particular, since we treat each individual speech withina debate as single document?, we are considering version of document-level sentiment-polarityclassification, namely, automatically distinguishing between positive and negative documents (das and chen, 2001; pang et al , 2002; <papid> W02-1011 </papid>turney, 2002; <papid> P02-1053 </papid>dave et al , 2003).</citsent>
<aftsection>
<nextsent>most sentiment-polarity classifiers proposed inthe recent literature categorize each document independently.
</nextsent>
<nextsent>a few others incorporate various measures of inter-document similarity between the texts to be labeled (agarwal and bhattacharyya, 2005; pang and lee, 2005; <papid> P05-1015 </papid>goldberg and zhu,2006).<papid> W06-3808 </papid></nextsent>
<nextsent>many interesting opinion-oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4897">
<title id=" W06-1639.xml">get out the vote determining support or opposition from congressional floor debate transcripts </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>note that from an experimental point of view, this is very convenient problem to work with because we can automatically determine ground truth (and thus avoid the need for manual annotation) simply by consulting publicly available voting records.
</prevsent>
<prevsent>task properties determining whether or not speaker supports proposal falls within the realmof sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language (early work includes wiebe and rapaport (1988), <papid> P88-1016 </papid>hearst (1992), sack (1994), and wiebe (1994); <papid> J94-2004 </papid>see esuli(2006) for an active bibliography).</prevsent>
</prevsection>
<citsent citstr=" P02-1053 ">
in particular, since we treat each individual speech withina debate as single document?, we are considering version of document-level sentiment-polarityclassification, namely, automatically distinguishing between positive and negative documents (das and chen, 2001; pang et al , 2002; <papid> W02-1011 </papid>turney, 2002; <papid> P02-1053 </papid>dave et al , 2003).</citsent>
<aftsection>
<nextsent>most sentiment-polarity classifiers proposed inthe recent literature categorize each document independently.
</nextsent>
<nextsent>a few others incorporate various measures of inter-document similarity between the texts to be labeled (agarwal and bhattacharyya, 2005; pang and lee, 2005; <papid> P05-1015 </papid>goldberg and zhu,2006).<papid> W06-3808 </papid></nextsent>
<nextsent>many interesting opinion-oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4898">
<title id=" W06-1639.xml">get out the vote determining support or opposition from congressional floor debate transcripts </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in particular, since we treat each individual speech withina debate as single document?, we are considering version of document-level sentiment-polarityclassification, namely, automatically distinguishing between positive and negative documents (das and chen, 2001; pang et al , 2002; <papid> W02-1011 </papid>turney, 2002; <papid> P02-1053 </papid>dave et al , 2003).</prevsent>
<prevsent>most sentiment-polarity classifiers proposed inthe recent literature categorize each document in dependently.</prevsent>
</prevsection>
<citsent citstr=" P05-1015 ">
a few others incorporate various measures of inter-document similarity between the texts to be labeled (agarwal and bhattacharyya, 2005; pang and lee, 2005; <papid> P05-1015 </papid>goldberg and zhu,2006).<papid> W06-3808 </papid></citsent>
<aftsection>
<nextsent>many interesting opinion-oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions.
</nextsent>
<nextsent>for example, we may find textual4evidence of high likelihood of agreement be 4because we are most interested in techniques applicable across domains, we restrict consideration to nlp aspects of the problem, ignoring external problem-specific information.
</nextsent>
<nextsent>for example, although most votes in our corpus were almost completely along party lines (and despite the fact that same party information is easily incorporated via the methods we propose), we did not use party-affiliation data.
</nextsent>
<nextsent>indeed, in other settings (e.g., movie-discussion listserv) one may not be able to determine the participants?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4900">
<title id=" W06-1639.xml">get out the vote determining support or opposition from congressional floor debate transcripts </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in particular, since we treat each individual speech withina debate as single document?, we are considering version of document-level sentiment-polarityclassification, namely, automatically distinguishing between positive and negative documents (das and chen, 2001; pang et al , 2002; <papid> W02-1011 </papid>turney, 2002; <papid> P02-1053 </papid>dave et al , 2003).</prevsent>
<prevsent>most sentiment-polarity classifiers proposed inthe recent literature categorize each document in dependently.</prevsent>
</prevsection>
<citsent citstr=" W06-3808 ">
a few others incorporate various measures of inter-document similarity between the texts to be labeled (agarwal and bhattacharyya, 2005; pang and lee, 2005; <papid> P05-1015 </papid>goldberg and zhu,2006).<papid> W06-3808 </papid></citsent>
<aftsection>
<nextsent>many interesting opinion-oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions.
</nextsent>
<nextsent>for example, we may find textual4evidence of high likelihood of agreement be 4because we are most interested in techniques applicable across domains, we restrict consideration to nlp aspects of the problem, ignoring external problem-specific information.
</nextsent>
<nextsent>for example, although most votes in our corpus were almost completely along party lines (and despite the fact that same party information is easily incorporated via the methods we propose), we did not use party-affiliation data.
</nextsent>
<nextsent>indeed, in other settings (e.g., movie-discussion listserv) one may not be able to determine the participants?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4901">
<title id=" W06-1639.xml">get out the vote determining support or opposition from congressional floor debate transcripts </title>
<section> method.  </section>
<citcontext>
<prevsection>
<prevsent>class from c(s).
</prevsent>
<prevsent>aminimum-cost assignment thus represents an optimum way to classify the speech segments so that each one tends not to be put into the class that the individual-document classifier dis prefers, butat the same time, highly associated speech segments tend not to be put in different classes.
</prevsent>
</prevsection>
<citsent citstr=" P04-1035 ">
as has been previously observed and exploited in the nlp literature (pang and lee, 2004; <papid> P04-1035 </papid>agarwal and bhattacharyya, 2005; barzilay and lap ata, 2005), <papid> H05-1042 </papid>the above optimization function, unlike many others that have been proposed for graph orset partitioning, can be solved exactly in an prov ably efficient manner via methods for finding minimum cuts in graphs.</citsent>
<aftsection>
<nextsent>in our view, the contribution of our work is the examination of new types of relationships, not the method by which such relationships are incorporated into the classification decision.
</nextsent>
<nextsent>3.2 classifying speech segments in isolation.
</nextsent>
<nextsent>in our experiments, we employed the well-known classifier svm light to obtain individual-document classification scores, treating as the positive class and using plain unigrams as features.5 following standard practice in sentiment analysis(pang et al , 2002), <papid> W02-1011 </papid>the input to svm light consisted of normalized presence-of-feature (rather than frequency-of-feature) vectors.</nextsent>
<nextsent>the ind value 5svmlight is available at svmlight.joachims.org.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4902">
<title id=" W06-1639.xml">get out the vote determining support or opposition from congressional floor debate transcripts </title>
<section> method.  </section>
<citcontext>
<prevsection>
<prevsent>class from c(s).
</prevsent>
<prevsent>aminimum-cost assignment thus represents an optimum way to classify the speech segments so that each one tends not to be put into the class that the individual-document classifier dis prefers, butat the same time, highly associated speech segments tend not to be put in different classes.
</prevsent>
</prevsection>
<citsent citstr=" H05-1042 ">
as has been previously observed and exploited in the nlp literature (pang and lee, 2004; <papid> P04-1035 </papid>agarwal and bhattacharyya, 2005; barzilay and lap ata, 2005), <papid> H05-1042 </papid>the above optimization function, unlike many others that have been proposed for graph orset partitioning, can be solved exactly in an prov ably efficient manner via methods for finding minimum cuts in graphs.</citsent>
<aftsection>
<nextsent>in our view, the contribution of our work is the examination of new types of relationships, not the method by which such relationships are incorporated into the classification decision.
</nextsent>
<nextsent>3.2 classifying speech segments in isolation.
</nextsent>
<nextsent>in our experiments, we employed the well-known classifier svm light to obtain individual-document classification scores, treating as the positive class and using plain unigrams as features.5 following standard practice in sentiment analysis(pang et al , 2002), <papid> W02-1011 </papid>the input to svm light consisted of normalized presence-of-feature (rather than frequency-of-feature) vectors.</nextsent>
<nextsent>the ind value 5svmlight is available at svmlight.joachims.org.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4904">
<title id=" W06-1639.xml">get out the vote determining support or opposition from congressional floor debate transcripts </title>
<section> method.  </section>
<citcontext>
<prevsection>
<prevsent>in our experiments, we employed the well-known classifier svm light to obtain individual-document classification scores, treating as the positive class and using plain unigrams as features.5 following standard practice in sentiment analysis(pang et al , 2002), <papid> W02-1011 </papid>the input to svm light consisted of normalized presence-of-feature (rather than frequency-of-feature) vectors.</prevsent>
<prevsent>the ind value 5svmlight is available at svmlight.joachims.org.</prevsent>
</prevsection>
<citsent citstr=" H05-1068 ">
default parameters were used, although experimentation with different parameter settings is an important direction for future work (daelemans and hoste, 2002; munson et al , 2005).<papid> H05-1068 </papid></citsent>
<aftsection>
<nextsent>for each speech segment was based on the signed distance d(s) from the vector representing to the trained svm decision plane: ind(s,y) def= ? ???
</nextsent>
<nextsent>1 d(s)   2s;( 1 + d(s)2s ) /2 |d(s)| ? 2s; 0 d(s)   2s where is the standard deviation of d(s) over all speech segments in the debate in question, and ind(s,n ) def= 1?
</nextsent>
<nextsent>ind(s,y).
</nextsent>
<nextsent>we now turn to the more interesting problem of representing the preferences that speech segments may have for being assigned to the same class.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4905">
<title id=" W06-1639.xml">get out the vote determining support or opposition from congressional floor debate transcripts </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>others have applied the nlp technologies ofnear-duplicate detection and topic-based text categorization to politically oriented text (yang and callan, 2005; purpura and hillard, 2006).
</prevsent>
<prevsent>detecting agreement we used simple method to learn to identify cross-speaker references indicating agreement.
</prevsent>
</prevsection>
<citsent citstr=" N03-2012 ">
more sophisticated approaches have been proposed (hillard et al , 2003), <papid> N03-2012 </papid>including an extension that, in an interesting reversal of our problem, makes use of sentiment polarity indicators within speech segments (galley et al , 2004).<papid> P04-1085 </papid></citsent>
<aftsection>
<nextsent>also relevant is work on the general problems of dialog-act tagging (stolcke et al , 2000), <papid> J00-3003 </papid>citation analysis (lehnert et al , 1990), and computational rhetorical analysis (marcu, 2000; teufel and moens, 2002).<papid> J02-4002 </papid></nextsent>
<nextsent>we currently do not have an efficient meansto encode disagreement information as hard con straints; we plan to investigate incorporating such information in future work.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4906">
<title id=" W06-1639.xml">get out the vote determining support or opposition from congressional floor debate transcripts </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>others have applied the nlp technologies ofnear-duplicate detection and topic-based text categorization to politically oriented text (yang and callan, 2005; purpura and hillard, 2006).
</prevsent>
<prevsent>detecting agreement we used simple method to learn to identify cross-speaker references indicating agreement.
</prevsent>
</prevsection>
<citsent citstr=" P04-1085 ">
more sophisticated approaches have been proposed (hillard et al , 2003), <papid> N03-2012 </papid>including an extension that, in an interesting reversal of our problem, makes use of sentiment polarity indicators within speech segments (galley et al , 2004).<papid> P04-1085 </papid></citsent>
<aftsection>
<nextsent>also relevant is work on the general problems of dialog-act tagging (stolcke et al , 2000), <papid> J00-3003 </papid>citation analysis (lehnert et al , 1990), and computational rhetorical analysis (marcu, 2000; teufel and moens, 2002).<papid> J02-4002 </papid></nextsent>
<nextsent>we currently do not have an efficient meansto encode disagreement information as hard con straints; we plan to investigate incorporating such information in future work.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4907">
<title id=" W06-1639.xml">get out the vote determining support or opposition from congressional floor debate transcripts </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>detecting agreement we used simple method to learn to identify cross-speaker references indicating agreement.
</prevsent>
<prevsent>more sophisticated approaches have been proposed (hillard et al , 2003), <papid> N03-2012 </papid>including an extension that, in an interesting reversal of our problem, makes use of sentiment polarity indicators within speech segments (galley et al , 2004).<papid> P04-1085 </papid></prevsent>
</prevsection>
<citsent citstr=" J00-3003 ">
also relevant is work on the general problems of dialog-act tagging (stolcke et al , 2000), <papid> J00-3003 </papid>citation analysis (lehnert et al , 1990), and computational rhetorical analysis (marcu, 2000; teufel and moens, 2002).<papid> J02-4002 </papid></citsent>
<aftsection>
<nextsent>we currently do not have an efficient meansto encode disagreement information as hard con straints; we plan to investigate incorporating such information in future work.
</nextsent>
<nextsent>relationships between the unlabeled items carvalho and cohen (2005) consider sequential relations between different types of emails (e.g.,between requests and satisfactions thereof) to classify messages, and thus also explicitly exploit the structure of conversations.
</nextsent>
<nextsent>previous sentiment-analysis work in different domains has considered inter-document similarity (agarwal and bhattacharyya, 2005; pang and lee, 2005; <papid> P05-1015 </papid>goldberg and zhu, 2006) <papid> W06-3808 </papid>or explicit 333inter-document references in the form of hyper links (agrawal et al , 2003).notable early papers on graph-based semi supervised learning include blum and chawla (2001), bansal et al  (2002), kondor and lafferty(2002), and joachims (2003).</nextsent>
<nextsent>zhu (2005) maintains survey of this area.recently, several alternative, often quite sophisticated approaches to collective classification have been proposed (neville and jensen, 2000; lafferty et al , 2001; getoor et al , 2002; taskar et al ., 2002; taskar et al , 2003; taskar et al , 2004;mccallum and wellner, 2004).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4908">
<title id=" W06-1639.xml">get out the vote determining support or opposition from congressional floor debate transcripts </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>detecting agreement we used simple method to learn to identify cross-speaker references indicating agreement.
</prevsent>
<prevsent>more sophisticated approaches have been proposed (hillard et al , 2003), <papid> N03-2012 </papid>including an extension that, in an interesting reversal of our problem, makes use of sentiment polarity indicators within speech segments (galley et al , 2004).<papid> P04-1085 </papid></prevsent>
</prevsection>
<citsent citstr=" J02-4002 ">
also relevant is work on the general problems of dialog-act tagging (stolcke et al , 2000), <papid> J00-3003 </papid>citation analysis (lehnert et al , 1990), and computational rhetorical analysis (marcu, 2000; teufel and moens, 2002).<papid> J02-4002 </papid></citsent>
<aftsection>
<nextsent>we currently do not have an efficient meansto encode disagreement information as hard con straints; we plan to investigate incorporating such information in future work.
</nextsent>
<nextsent>relationships between the unlabeled items carvalho and cohen (2005) consider sequential relations between different types of emails (e.g.,between requests and satisfactions thereof) to classify messages, and thus also explicitly exploit the structure of conversations.
</nextsent>
<nextsent>previous sentiment-analysis work in different domains has considered inter-document similarity (agarwal and bhattacharyya, 2005; pang and lee, 2005; <papid> P05-1015 </papid>goldberg and zhu, 2006) <papid> W06-3808 </papid>or explicit 333inter-document references in the form of hyper links (agrawal et al , 2003).notable early papers on graph-based semi supervised learning include blum and chawla (2001), bansal et al  (2002), kondor and lafferty(2002), and joachims (2003).</nextsent>
<nextsent>zhu (2005) maintains survey of this area.recently, several alternative, often quite sophisticated approaches to collective classification have been proposed (neville and jensen, 2000; lafferty et al , 2001; getoor et al , 2002; taskar et al ., 2002; taskar et al , 2003; taskar et al , 2004;mccallum and wellner, 2004).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4912">
<title id=" W06-2601.xml">maximum entropy tagging with binary and real valued features </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>effective use of training data to estimate features and parameters is achieved by integrating leaving-one-out method into the standard me training algorithm.
</prevsent>
<prevsent>experimental results on two tagging tasks show statistically significant performance gains after augmenting standard binary feature models with real-valued features.
</prevsent>
</prevsection>
<citsent citstr=" J96-1002 ">
the maximum entropy (me) statistical framework (darroch and rat cliff, 1972; berger et al, 1996) <papid> J96-1002 </papid>has been successfully deployed in several nlp tasks.</citsent>
<aftsection>
<nextsent>in recent evaluation campaigns, e.g. darpa ie and conll 2000-2003, me models reached state-of-the-art performance on range of text-tagging tasks.
</nextsent>
<nextsent>with few exceptions, best me taggers rely on carefully designed sets of features.
</nextsent>
<nextsent>features correspond to binary functions, which model events,observed in the (annotated) training data and supposed to be meaningful or discriminative for thetask at hand.
</nextsent>
<nextsent>hence, me models result in loglinear combination of large set of features, whose weights can be estimated by the well known generalized iterative scaling (gis) algorithm by dar roch and rat cliff (1972).despite me theory and its related training algorithm (darroch and rat cliff, 1972) do not set restrictions on the range of feature functions1 , popular nlp textbooks (manning and schutze, 1999) and research papers (berger et al, 1996) <papid> J96-1002 </papid>seem to limit them to binary features.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4916">
<title id=" W06-2601.xml">maximum entropy tagging with binary and real valued features </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>features correspond to binary functions, which model events,observed in the (annotated) training data and supposed to be meaningful or discriminative for thetask at hand.
</prevsent>
<prevsent>hence, me models result in loglinear combination of large set of features, whose weights can be estimated by the well known generalized iterative scaling (gis) algorithm by dar roch and rat cliff (1972).despite me theory and its related training algorithm (darroch and rat cliff, 1972) do not set restrictions on the range of feature functions1 , popular nlp textbooks (manning and schutze, 1999) and research papers (berger et al, 1996) <papid> J96-1002 </papid>seem to limit them to binary features.</prevsent>
</prevsection>
<citsent citstr=" P02-1038 ">
in fact, only recently, log-probability features have been deployed in me models for statistical machine translation (och and ney, 2002).<papid> P02-1038 </papid>this paper focuses on me models for two text tagging tasks: named entity recognition (ner) and text chuncking (tc).</citsent>
<aftsection>
<nextsent>by taking inspiration from the literature (bender et al, 2003; <papid> W03-0420 </papid>borthwick, 1999; koeling, 2000), <papid> W00-0729 </papid>set of standard binary features is introduced.</nextsent>
<nextsent>hence, for each feature type, corresponding real-valued feature is developed in terms of smoothed probability distributions estimated on the training data.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4917">
<title id=" W06-2601.xml">maximum entropy tagging with binary and real valued features </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>hence, me models result in loglinear combination of large set of features, whose weights can be estimated by the well known generalized iterative scaling (gis) algorithm by dar roch and rat cliff (1972).despite me theory and its related training algorithm (darroch and rat cliff, 1972) do not set restrictions on the range of feature functions1 , popular nlp textbooks (manning and schutze, 1999) and research papers (berger et al, 1996) <papid> J96-1002 </papid>seem to limit them to binary features.</prevsent>
<prevsent>in fact, only recently, log-probability features have been deployed in me models for statistical machine translation (och and ney, 2002).<papid> P02-1038 </papid>this paper focuses on me models for two text tagging tasks: named entity recognition (ner) and text chuncking (tc).</prevsent>
</prevsection>
<citsent citstr=" W03-0420 ">
by taking inspiration from the literature (bender et al, 2003; <papid> W03-0420 </papid>borthwick, 1999; koeling, 2000), <papid> W00-0729 </papid>set of standard binary features is introduced.</citsent>
<aftsection>
<nextsent>hence, for each feature type, corresponding real-valued feature is developed in terms of smoothed probability distributions estimated on the training data.
</nextsent>
<nextsent>a direct comparison of me models based on binary, real valued, and mixed features is presented.
</nextsent>
<nextsent>besides, performance on the tagging tasks, complexity and training time by each model are reported.
</nextsent>
<nextsent>me estimation with real-valued features is accomplished by combining gis with the leave-one-out method (manning and schutze, 1999).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4918">
<title id=" W06-2601.xml">maximum entropy tagging with binary and real valued features </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>hence, me models result in loglinear combination of large set of features, whose weights can be estimated by the well known generalized iterative scaling (gis) algorithm by dar roch and rat cliff (1972).despite me theory and its related training algorithm (darroch and rat cliff, 1972) do not set restrictions on the range of feature functions1 , popular nlp textbooks (manning and schutze, 1999) and research papers (berger et al, 1996) <papid> J96-1002 </papid>seem to limit them to binary features.</prevsent>
<prevsent>in fact, only recently, log-probability features have been deployed in me models for statistical machine translation (och and ney, 2002).<papid> P02-1038 </papid>this paper focuses on me models for two text tagging tasks: named entity recognition (ner) and text chuncking (tc).</prevsent>
</prevsection>
<citsent citstr=" W00-0729 ">
by taking inspiration from the literature (bender et al, 2003; <papid> W03-0420 </papid>borthwick, 1999; koeling, 2000), <papid> W00-0729 </papid>set of standard binary features is introduced.</citsent>
<aftsection>
<nextsent>hence, for each feature type, corresponding real-valued feature is developed in terms of smoothed probability distributions estimated on the training data.
</nextsent>
<nextsent>a direct comparison of me models based on binary, real valued, and mixed features is presented.
</nextsent>
<nextsent>besides, performance on the tagging tasks, complexity and training time by each model are reported.
</nextsent>
<nextsent>me estimation with real-valued features is accomplished by combining gis with the leave-one-out method (manning and schutze, 1999).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4920">
<title id=" W06-2601.xml">maximum entropy tagging with binary and real valued features </title>
<section> me models for text tagging.  </section>
<citcontext>
<prevsection>
<prevsent>(3) the feature functions fi(x, y) represent any kind of information about the event (x, y) which can be useful for the classification task.
</prevsent>
<prevsent>typically, binary features are employed which model the verification of simple events within the target class and the context.
</prevsent>
</prevsection>
<citsent citstr=" P98-2140 ">
in mikheev (1998), <papid> P98-2140 </papid>binary features for text tagging are classified into two broad classes: atomic and complex.</citsent>
<aftsection>
<nextsent>atomic features tell information about the current tag and one single item (word or tag) ofthe context.
</nextsent>
<nextsent>complex features result as combination of two or more atomic features.
</nextsent>
<nextsent>in this way, if the grouped events are not independent, complex features should capture higher correlations or dependencies, possibly useful to discriminate.in the following, standard set of binary features is presented, which is generally employed for text-tagging tasks.
</nextsent>
<nextsent>the reader familiar with the topic can directly check this set in table 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4922">
<title id=" W06-3109.xml">generalized stack decoding algorithms for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the equation that models this process is: ei1 = arg max ei1 {pr(ei1) ? pr(fj1 |ei1)} (1) the search/decoding problem in smt consists in solving the maximization problem stated in eq.
</prevsent>
<prevsent>(1).
</prevsent>
</prevsection>
<citsent citstr=" P01-1030 ">
in the literature, we can find different techniques to deal with this problem, ranging from heuristic and fast (as greedy decoders) to optimal and very slow decoding algorithms (germann et al, 2001).<papid> P01-1030 </papid></citsent>
<aftsection>
<nextsent>also, under certain circumstances, stack-based decoders can obtain optimal solutions.
</nextsent>
<nextsent>many works (berger et al, 1996; wang and waibel, 1998; germann et al, 2001; <papid> P01-1030 </papid>och et al, 2001; <papid> W01-1408 </papid>ortz et al, 2003) have adopted different types of stack-based algorithms to solve the global search optimization problem for statistical machine trans lation.</nextsent>
<nextsent>all these works follow two main different approaches according to the number of stacks usedin the design and implementation of the search algorithm (the stacks are used to store partial hypotheses, sorted according to their partial score/probability, during the search process) : ? on the one hand, in (wang and waibel, 1998; och et al, 2001) <papid> W01-1408 </papid>single stack is used.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4924">
<title id=" W06-3109.xml">generalized stack decoding algorithms for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in the literature, we can find different techniques to deal with this problem, ranging from heuristic and fast (as greedy decoders) to optimal and very slow decoding algorithms (germann et al, 2001).<papid> P01-1030 </papid></prevsent>
<prevsent>also, under certain circumstances, stack-based decoders can obtain optimal solutions.</prevsent>
</prevsection>
<citsent citstr=" W01-1408 ">
many works (berger et al, 1996; wang and waibel, 1998; germann et al, 2001; <papid> P01-1030 </papid>och et al, 2001; <papid> W01-1408 </papid>ortz et al, 2003) have adopted different types of stack-based algorithms to solve the global search optimization problem for statistical machine trans lation.</citsent>
<aftsection>
<nextsent>all these works follow two main different approaches according to the number of stacks usedin the design and implementation of the search algorithm (the stacks are used to store partial hypotheses, sorted according to their partial score/probability, during the search process) : ? on the one hand, in (wang and waibel, 1998; och et al, 2001) <papid> W01-1408 </papid>single stack is used.</nextsent>
<nextsent>in that case, in order to make the search feasible,the pruning of the number of partial hypotheses stored in the stack is needed.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4927">
<title id=" W06-3109.xml">generalized stack decoding algorithms for statistical machine translation </title>
<section> phrase based statistical machine.  </section>
<citcontext>
<prevsection>
<prevsent>translation different translation models (tms) have been proposed depending on how the relation between the source and the target languages is structured; that is, the way target sentence is generated from sourcesentence.
</prevsent>
<prevsent>this relation is summarized using the concept of alignment; that is, how the constituents (typically words or group-of-words) of pair of sentences are aligned to each other.
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
the most widely used single-word-based statistical alignment models (sams) have been proposed in (brown et al, 1993; <papid> J93-2003 </papid>ney et al, 2000).</citsent>
<aftsection>
<nextsent>on the other hand, models that deal with structures or phrases instead of single words have also been proposed: the syntax translation models are described in (yamada and knight, 2001) , <papid> P01-1067 </papid>alignment templates are used in (och, 2002), and the alignment template approach is re-framed into the so-called phrase based translation (pbt) in (marcu and wong, 2002; <papid> W02-1018 </papid>zens et al, 2002; koehn et al, 2003; <papid> N03-1017 </papid>tomas and casacuberta, 2003).</nextsent>
<nextsent>for the translation model (pr(fj1 |ei1)) in eq.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4928">
<title id=" W06-3109.xml">generalized stack decoding algorithms for statistical machine translation </title>
<section> phrase based statistical machine.  </section>
<citcontext>
<prevsection>
<prevsent>this relation is summarized using the concept of alignment; that is, how the constituents (typically words or group-of-words) of pair of sentences are aligned to each other.
</prevsent>
<prevsent>the most widely used single-word-based statistical alignment models (sams) have been proposed in (brown et al, 1993; <papid> J93-2003 </papid>ney et al, 2000).</prevsent>
</prevsection>
<citsent citstr=" P01-1067 ">
on the other hand, models that deal with structures or phrases instead of single words have also been proposed: the syntax translation models are described in (yamada and knight, 2001) , <papid> P01-1067 </papid>alignment templates are used in (och, 2002), and the alignment template approach is re-framed into the so-called phrase based translation (pbt) in (marcu and wong, 2002; <papid> W02-1018 </papid>zens et al, 2002; koehn et al, 2003; <papid> N03-1017 </papid>tomas and casacuberta, 2003).</citsent>
<aftsection>
<nextsent>for the translation model (pr(fj1 |ei1)) in eq.
</nextsent>
<nextsent>(1), pbt can be explained from generative point of view as follows (zens et al, 2002): 1.
</nextsent>
<nextsent>the target sentence ei1 is segmented into k. phrases (ek1 ).
</nextsent>
<nextsent>2.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4929">
<title id=" W06-3109.xml">generalized stack decoding algorithms for statistical machine translation </title>
<section> phrase based statistical machine.  </section>
<citcontext>
<prevsection>
<prevsent>this relation is summarized using the concept of alignment; that is, how the constituents (typically words or group-of-words) of pair of sentences are aligned to each other.
</prevsent>
<prevsent>the most widely used single-word-based statistical alignment models (sams) have been proposed in (brown et al, 1993; <papid> J93-2003 </papid>ney et al, 2000).</prevsent>
</prevsection>
<citsent citstr=" W02-1018 ">
on the other hand, models that deal with structures or phrases instead of single words have also been proposed: the syntax translation models are described in (yamada and knight, 2001) , <papid> P01-1067 </papid>alignment templates are used in (och, 2002), and the alignment template approach is re-framed into the so-called phrase based translation (pbt) in (marcu and wong, 2002; <papid> W02-1018 </papid>zens et al, 2002; koehn et al, 2003; <papid> N03-1017 </papid>tomas and casacuberta, 2003).</citsent>
<aftsection>
<nextsent>for the translation model (pr(fj1 |ei1)) in eq.
</nextsent>
<nextsent>(1), pbt can be explained from generative point of view as follows (zens et al, 2002): 1.
</nextsent>
<nextsent>the target sentence ei1 is segmented into k. phrases (ek1 ).
</nextsent>
<nextsent>2.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4930">
<title id=" W06-3109.xml">generalized stack decoding algorithms for statistical machine translation </title>
<section> phrase based statistical machine.  </section>
<citcontext>
<prevsection>
<prevsent>this relation is summarized using the concept of alignment; that is, how the constituents (typically words or group-of-words) of pair of sentences are aligned to each other.
</prevsent>
<prevsent>the most widely used single-word-based statistical alignment models (sams) have been proposed in (brown et al, 1993; <papid> J93-2003 </papid>ney et al, 2000).</prevsent>
</prevsection>
<citsent citstr=" N03-1017 ">
on the other hand, models that deal with structures or phrases instead of single words have also been proposed: the syntax translation models are described in (yamada and knight, 2001) , <papid> P01-1067 </papid>alignment templates are used in (och, 2002), and the alignment template approach is re-framed into the so-called phrase based translation (pbt) in (marcu and wong, 2002; <papid> W02-1018 </papid>zens et al, 2002; koehn et al, 2003; <papid> N03-1017 </papid>tomas and casacuberta, 2003).</citsent>
<aftsection>
<nextsent>for the translation model (pr(fj1 |ei1)) in eq.
</nextsent>
<nextsent>(1), pbt can be explained from generative point of view as follows (zens et al, 2002): 1.
</nextsent>
<nextsent>the target sentence ei1 is segmented into k. phrases (ek1 ).
</nextsent>
<nextsent>2.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4931">
<title id=" W06-1409.xml">over specified reference in hierarchical domains measuring the benefits for readers </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the description in (1a) is longer (andmight therefore take more time to read and in terpret) than (1b), but the additional material in (1a) makes resolution easier once interpretation is successfully completed.
</prevsent>
<prevsent>we explore how an gre program should make use of logically redundant properties so as to simplify resolution (i.e., the identification of the referent).
</prevsent>
</prevsection>
<citsent citstr=" P00-1019 ">
in corpus-based studies, it has been shown that logically redundant properties tend to be included when they fulfill one of number of pragmatic functions, such as to indicate that property is of particular importance to the speaker, or to high light the speakers awareness that the referent has the property in question (jordan 2000).<papid> P00-1019 </papid></citsent>
<aftsection>
<nextsent>however, redundancy has been built into gre algorithms 55only to very limited extent.
</nextsent>
<nextsent>perhaps the most interesting account of over specification so far is the one proposed by horacek (2005), <papid> W05-1606 </papid>where logically redundant properties enter the descriptions generated when the combined certainty of other properties falls short of what is contextually required.</nextsent>
<nextsent>uncertainty can arise, for example, if the hearer does not know about property, or if she does not know whether it applies to the target referent.our own work explores the need for overspecifi cation in situations where each of the properties in question is un problematic (i.e., certain) in principle, but where the reader has to make an effort to discover their extension (i.e., what objects are truthfully described by the property).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4932">
<title id=" W06-1409.xml">over specified reference in hierarchical domains measuring the benefits for readers </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in corpus-based studies, it has been shown that logically redundant properties tend to be included when they fulfill one of number of pragmatic functions, such as to indicate that property is of particular importance to the speaker, or to high light the speakers awareness that the referent has the property in question (jordan 2000).<papid> P00-1019 </papid></prevsent>
<prevsent>however, redundancy has been built into gre algorithms 55only to very limited extent.</prevsent>
</prevsection>
<citsent citstr=" W05-1606 ">
perhaps the most interesting account of over specification so far is the one proposed by horacek (2005), <papid> W05-1606 </papid>where logically redundant properties enter the descriptions generated when the combined certainty of other properties falls short of what is contextually required.</citsent>
<aftsection>
<nextsent>uncertainty can arise, for example, if the hearer does not know about property, or if she does not know whether it applies to the target referent.our own work explores the need for overspecifi cation in situations where each of the properties in question is un problematic (i.e., certain) in principle, but where the reader has to make an effort to discover their extension (i.e., what objects are truthfully described by the property).
</nextsent>
<nextsent>we ask howa generator can use logically redundant information to reduce the search space within which reader has to find?
</nextsent>
<nextsent>a referent.
</nextsent>
<nextsent>(cf., edmonds 1994 <papid> C94-2182 </papid>for related set of problems.)</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4933">
<title id=" W06-1409.xml">over specified reference in hierarchical domains measuring the benefits for readers </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we ask howa generator can use logically redundant information to reduce the search space within which reader has to find?
</prevsent>
<prevsent>a referent.
</prevsent>
</prevsection>
<citsent citstr=" C94-2182 ">
(cf., edmonds 1994 <papid> C94-2182 </papid>for related set of problems.)</citsent>
<aftsection>
<nextsent>existing work on gre tends to focus on fairly simple domains, dominated by one-place properties.
</nextsent>
<nextsent>when relations (i.e., two-place properties)are taken into account at all (e.g., dale and haddock 1991, <papid> E91-1028 </papid>krahmer and theune 2002), the motivating examples are kept so small that it is reasonable to assume that speaker and hearer know all the relevant facts in advance.</nextsent>
<nextsent>consequently, search is not much of an issue (i.e., resolution iseasy): the hearer can identify the referent by simply intersecting the denot ations of the properties in the description.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4934">
<title id=" W06-1409.xml">over specified reference in hierarchical domains measuring the benefits for readers </title>
<section> hierarchical domains.  </section>
<citcontext>
<prevsection>
<prevsent>(cf., edmonds 1994 <papid> C94-2182 </papid>for related set of problems.)</prevsent>
<prevsent>existing work on gre tends to focus on fairly simple domains, dominated by one-place proper ties.</prevsent>
</prevsection>
<citsent citstr=" E91-1028 ">
when relations (i.e., two-place properties)are taken into account at all (e.g., dale and haddock 1991, <papid> E91-1028 </papid>krahmer and theune 2002), the motivating examples are kept so small that it is reasonable to assume that speaker and hearer know all the relevant facts in advance.</citsent>
<aftsection>
<nextsent>consequently, search is not much of an issue (i.e., resolution iseasy): the hearer can identify the referent by simply intersecting the denot ations of the properties in the description.
</nextsent>
<nextsent>while such simplifications permit the study of many aspects of reference, other aspects come to the fore when larger domains are considered.
</nextsent>
<nextsent>interesting questions arise, for example, when alarge domain is hierarchically ordered.
</nextsent>
<nextsent>we consider domain to be hierarchically ordered if its inhabitants can be structured like tree in which everything that belongs to given node belong to at most one of ns children, while every thing that belongs to one of ns children belongs to n. examples include countries divided into provinces which, in turn, may be divided into regions, etc.; years into months then into weeks and then into days; documents into chapters then sections then subsections; buildings into floors then rooms.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4939">
<title id=" W06-1649.xml">partially supervised sense disambiguation by learning sense number from tagged and untagged corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>means that tagged corpus does not include the instances of some senses for the target word, while these senses may occur in untagged texts.
</prevsent>
<prevsent>with definition or meaning.
</prevsent>
</prevsection>
<citsent citstr=" J98-1006 ">
many corpus based methods have been proposed to deal with the sense disambiguation problem when given definition for each possible sense of target word or tagged corpus with the instances of each possible sense, e.g., supervised sense disambiguation (leacock etal., 1998), <papid> J98-1006 </papid>and semi-supervised sense disambiguation (yarowsky, 1995).<papid> P95-1026 </papid>supervised methods usually relyon the information from previously sense tagged corpora to determine the senses of words in unseen texts.semi-supervised methods for wsd are characterized in terms of exploiting unlabeled data inthe learning procedure with the need of predefined sense inventories for target words.</citsent>
<aftsection>
<nextsent>the information for semi-supervised sense disambiguation is usually obtained from bilingual corpora (e.g. parallel corpora or untagged monolingual corpora in two languages) (brown et al, 1991; <papid> P91-1034 </papid>da gan and itai, 1994), <papid> J94-4003 </papid>or sense-tagged seed examples (yarowsky, 1995).<papid> P95-1026 </papid></nextsent>
<nextsent>some observations can be made on the previous supervised and semi-supervised methods.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4940">
<title id=" W06-1649.xml">partially supervised sense disambiguation by learning sense number from tagged and untagged corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>means that tagged corpus does not include the instances of some senses for the target word, while these senses may occur in untagged texts.
</prevsent>
<prevsent>with definition or meaning.
</prevsent>
</prevsection>
<citsent citstr=" P95-1026 ">
many corpus based methods have been proposed to deal with the sense disambiguation problem when given definition for each possible sense of target word or tagged corpus with the instances of each possible sense, e.g., supervised sense disambiguation (leacock etal., 1998), <papid> J98-1006 </papid>and semi-supervised sense disambiguation (yarowsky, 1995).<papid> P95-1026 </papid>supervised methods usually relyon the information from previously sense tagged corpora to determine the senses of words in unseen texts.semi-supervised methods for wsd are characterized in terms of exploiting unlabeled data inthe learning procedure with the need of predefined sense inventories for target words.</citsent>
<aftsection>
<nextsent>the information for semi-supervised sense disambiguation is usually obtained from bilingual corpora (e.g. parallel corpora or untagged monolingual corpora in two languages) (brown et al, 1991; <papid> P91-1034 </papid>da gan and itai, 1994), <papid> J94-4003 </papid>or sense-tagged seed examples (yarowsky, 1995).<papid> P95-1026 </papid></nextsent>
<nextsent>some observations can be made on the previous supervised and semi-supervised methods.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4942">
<title id=" W06-1649.xml">partially supervised sense disambiguation by learning sense number from tagged and untagged corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>with definition or meaning.
</prevsent>
<prevsent>many corpus based methods have been proposed to deal with the sense disambiguation problem when given definition for each possible sense of target word or tagged corpus with the instances of each possible sense, e.g., supervised sense disambiguation (leacock etal., 1998), <papid> J98-1006 </papid>and semi-supervised sense disambiguation (yarowsky, 1995).<papid> P95-1026 </papid>supervised methods usually relyon the information from previously sense tagged corpora to determine the senses of words in unseen texts.semi-supervised methods for wsd are characterized in terms of exploiting unlabeled data inthe learning procedure with the need of predefined sense inventories for target words.</prevsent>
</prevsection>
<citsent citstr=" P91-1034 ">
the information for semi-supervised sense disambiguation is usually obtained from bilingual corpora (e.g. parallel corpora or untagged monolingual corpora in two languages) (brown et al, 1991; <papid> P91-1034 </papid>da gan and itai, 1994), <papid> J94-4003 </papid>or sense-tagged seed examples (yarowsky, 1995).<papid> P95-1026 </papid></citsent>
<aftsection>
<nextsent>some observations can be made on the previous supervised and semi-supervised methods.
</nextsent>
<nextsent>they always relyon hand-crafted lexicons (e.g., word net) as sense inventories.
</nextsent>
<nextsent>but these resources maymiss domain-specific senses, which leads to in complete sense tagged corpus.
</nextsent>
<nextsent>therefore, sense taggers trained on the incomplete tagged corpus will mis classify some instances if the senses ofthese instances are not defined in sense inventories.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4943">
<title id=" W06-1649.xml">partially supervised sense disambiguation by learning sense number from tagged and untagged corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>with definition or meaning.
</prevsent>
<prevsent>many corpus based methods have been proposed to deal with the sense disambiguation problem when given definition for each possible sense of target word or tagged corpus with the instances of each possible sense, e.g., supervised sense disambiguation (leacock etal., 1998), <papid> J98-1006 </papid>and semi-supervised sense disambiguation (yarowsky, 1995).<papid> P95-1026 </papid>supervised methods usually relyon the information from previously sense tagged corpora to determine the senses of words in unseen texts.semi-supervised methods for wsd are characterized in terms of exploiting unlabeled data inthe learning procedure with the need of predefined sense inventories for target words.</prevsent>
</prevsection>
<citsent citstr=" J94-4003 ">
the information for semi-supervised sense disambiguation is usually obtained from bilingual corpora (e.g. parallel corpora or untagged monolingual corpora in two languages) (brown et al, 1991; <papid> P91-1034 </papid>da gan and itai, 1994), <papid> J94-4003 </papid>or sense-tagged seed examples (yarowsky, 1995).<papid> P95-1026 </papid></citsent>
<aftsection>
<nextsent>some observations can be made on the previous supervised and semi-supervised methods.
</nextsent>
<nextsent>they always relyon hand-crafted lexicons (e.g., word net) as sense inventories.
</nextsent>
<nextsent>but these resources maymiss domain-specific senses, which leads to in complete sense tagged corpus.
</nextsent>
<nextsent>therefore, sense taggers trained on the incomplete tagged corpus will mis classify some instances if the senses ofthese instances are not defined in sense inventories.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4946">
<title id=" W06-1649.xml">partially supervised sense disambiguation by learning sense number from tagged and untagged corpora </title>
<section> return ydu ;.  </section>
<citcontext>
<prevsection>
<prevsent>3.1 experiment design.
</prevsent>
<prevsent>we evaluated the elp based model order identification algorithm on the data in english lexical sample task of senseval-3 (including all 418 table 3: description of the percentage of official training data used as tagged data when instances with different sense sets are removed from official training data.
</prevsent>
</prevsection>
<citsent citstr=" W04-0807 ">
the percentage of official training data used as tagged data ssubset = {s1} 42.8% ssubset = {s2} 76.7% ssubset = {s3} 89.1% ssubset = {s1, s2} 19.6% ssubset = {s1, s3} 32.0% ssubset = {s2, s3} 65.9% the 57 english words ) 9, and further empirically compared it with other state of the art classification methods, including svm 10 (the state ofthe art method for supervised word sense disambiguation (mihalcea et al, 2004)), <papid> W04-0807 </papid>one-class partially supervised classification algorithm (liu etal., 2003) 11, and semi-supervised k-means clustering based model order identification algorithm.</citsent>
<aftsection>
<nextsent>the data for english lexical samples task insenseval-3 consists of 7860 examples as official training data, and 3944 examples as official test data for 57 english words.
</nextsent>
<nextsent>the number of senses of each english word varies from 3 to 11.we evaluated these four algorithms with different sizes of incomplete tagged data.
</nextsent>
<nextsent>given official training data of the word w, we constructed incomplete tagged data xl by removing the all the tagged instances from official training data that have sense tags from ssubset, where ssubset is subset of the ground-truth sense set for w, and consists of the sense tags in official training set for w. the removed training data and official test data of were used as xu . note that sl = sssubset.
</nextsent>
<nextsent>then we ran these four algorithm for each target word with xl as tagged data and xu as untagged data, and evaluated their performance using the accuracy on official test data of all the 57words.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4947">
<title id=" W06-1649.xml">partially supervised sense disambiguation by learning sense number from tagged and untagged corpora </title>
<section> return ydu ;.  </section>
<citcontext>
<prevsection>
<prevsent>clustering proces swill be terminated when clustering solution converges or the number of iteration steps is more than 30.
</prevsent>
<prevsent>kmin = kxl = |sl|, kmax = kmin + m. m. is set as 4.
</prevsent>
</prevsection>
<citsent citstr=" P05-1049 ">
we used jensen-shannon (js) divergence (lin, 1991) as distance measure for semi-supervisedclustering and elp, since plain lp with js divergence achieves better performance than that with cosine similarity on senseval-3 data (niu et al, 2005).<papid> P05-1049 </papid>for the lp process in elp algorithm, we constructed connected graphs as follows: two instances u, will be connected by an edge if is among vs 10 nearest neighbors, or if is among us 10 nearest neighbors as measured by cosine orjs distance measure (following (zhu and ghahramani, 2002)).</citsent>
<aftsection>
<nextsent>we used three types of features to capture the information in all the contextual sentences of target words in senseval-3 data for all the four algorithms: part-of-speech of neighboring words with position information, words in topical context without position information (after removing stop words), and local collocations (as same as the feature set used in (lee and ng, 2002) <papid> W02-1006 </papid>except that we did not use syntactic relations).</nextsent>
<nextsent>we removed the features with occurrence frequency (counted in both training set and test set) less than 3 times.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4948">
<title id=" W06-1649.xml">partially supervised sense disambiguation by learning sense number from tagged and untagged corpora </title>
<section> return ydu ;.  </section>
<citcontext>
<prevsection>
<prevsent>kmin = kxl = |sl|, kmax = kmin + m. m. is set as 4.
</prevsent>
<prevsent>we used jensen-shannon (js) divergence (lin, 1991) as distance measure for semi-supervisedclustering and elp, since plain lp with js divergence achieves better performance than that with cosine similarity on senseval-3 data (niu et al, 2005).<papid> P05-1049 </papid>for the lp process in elp algorithm, we constructed connected graphs as follows: two instances u, will be connected by an edge if is among vs 10 nearest neighbors, or if is among us 10 nearest neighbors as measured by cosine orjs distance measure (following (zhu and ghahramani, 2002)).</prevsent>
</prevsection>
<citsent citstr=" W02-1006 ">
we used three types of features to capture the information in all the contextual sentences of target words in senseval-3 data for all the four algorithms: part-of-speech of neighboring words with position information, words in topical context without position information (after removing stop words), and local collocations (as same as the feature set used in (lee and ng, 2002) <papid> W02-1006 </papid>except that we did not use syntactic relations).</citsent>
<aftsection>
<nextsent>we removed the features with occurrence frequency (counted in both training set and test set) less than 3 times.
</nextsent>
<nextsent>if the estimated sense number is more than the sense number in the initial tagged corpus xl, thenthe results from order identification based methods will consist of the instances from clusters of unknown classes.
</nextsent>
<nextsent>when assessing the agreement between these classification results and the known results on official test set, we will encounter the problem that there is no sense tag for each instance in unknown classes.
</nextsent>
<nextsent>slonim and tishby (2000) proposed to assign documents in each cluster with the most dominant class label in that cluster, andthen conducted evaluation on these labeled documents.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4949">
<title id=" W06-1809.xml">incorporating user models in question answering to improve readability </title>
<section> system architecture.  </section>
<citcontext>
<prevsection>
<prevsent>a difficult text into syntactically and lexically simpler version.
</prevsent>
<prevsent>in the case of pset (carroll et al , 1999) for instance, tagger, morphological analyzer and generator and parser are used to reformulate newspaper text for users affected by aphasia.
</prevsent>
</prevsection>
<citsent citstr=" W03-1602 ">
another interesting research is inui et al lexical and syntactical paraphrasing system for deaf students (inui et al , 2003).<papid> W03-1602 </papid></citsent>
<aftsection>
<nextsent>in this system, the judgment of experts (teachers) is used to learn selection rules for paraphrases acquired using various methods (statistical, manual, etc.).
</nextsent>
<nextsent>in the skill sum project (williams and reiter, 2005), <papid> W05-1616 </papid>used to generate literacy test reports, setof choices regarding output (cue phrases, ordering and punctuation) are taken by micro-planner based on set of rules.</nextsent>
<nextsent>our approach is conceptually different from theabove: exploiting the wealth of information available in the context of web-based qa system, wecan afford to choose among the documents available on given subject those which best suit our readability requirements.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4950">
<title id=" W06-1809.xml">incorporating user models in question answering to improve readability </title>
<section> system architecture.  </section>
<citcontext>
<prevsection>
<prevsent>another interesting research is inui et al lexical and syntactical paraphrasing system for deaf students (inui et al , 2003).<papid> W03-1602 </papid></prevsent>
<prevsent>in this system, the judgment of experts (teachers) is used to learn selection rules for paraphrases acquired using various methods (statistical, manual, etc.).</prevsent>
</prevsection>
<citsent citstr=" W05-1616 ">
in the skill sum project (williams and reiter, 2005), <papid> W05-1616 </papid>used to generate literacy test reports, setof choices regarding output (cue phrases, ordering and punctuation) are taken by micro-planner based on set of rules.</citsent>
<aftsection>
<nextsent>our approach is conceptually different from theabove: exploiting the wealth of information available in the context of web-based qa system, wecan afford to choose among the documents available on given subject those which best suit our readability requirements.
</nextsent>
<nextsent>this is possible thank sto the versatility of language modelling, which allows us to tailor the readability estimation of documents to any kind of user profile in dynamic manner, as explained in section 3.2.3.
</nextsent>
<nextsent>in this section we discuss the information flow among the subcomponents of the qa module (see figure 2 for representative diagram) and focuson reading level estimation and document filtering.
</nextsent>
<nextsent>for further details on the implementation of the qa module, see (quarteroni and manandhar, 2006).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4951">
<title id=" W06-1422.xml">geneval a proposal for shared task evaluation in nlg </title>
<section> comparative evaluations in nlg.  </section>
<citcontext>
<prevsection>
<prevsent>we hope that such shared-task events will also make it easier for new researchers to get involved in nlg, by providing datasets and an evaluation framework.
</prevsent>
<prevsent>there is long history of shared task initiatives in nlp, of which the best known is perhaps muc(hirschman, 1998); others include trec, parseval, senseval, and the range of shared tasks organised by conll.
</prevsent>
</prevsection>
<citsent citstr=" W06-1421 ">
such exercises are now common in most areas of nlp, and have had major impact on many areas, including machine translation and information extraction (see discussion of history of shared-task initiatives and their impact in belz and kilgarriff (2006)).<papid> W06-1421 </papid></citsent>
<aftsection>
<nextsent>one of the best-known comparative studies of evaluation techniques was by papineni et al (2002) <papid> P02-1040 </papid>who proposed the bleu metric for machine translation and showed that bleu correlated well with human judgements when comparing several machine translation systems.</nextsent>
<nextsent>several other studies of this type have been carried out in the mt and summarisation communities.the first comparison of nlg evaluation techniques which we are aware of is by bangalore et al (2000).<papid> W00-1401 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4952">
<title id=" W06-1422.xml">geneval a proposal for shared task evaluation in nlg </title>
<section> comparative evaluations in nlg.  </section>
<citcontext>
<prevsection>
<prevsent>there is long history of shared task initiatives in nlp, of which the best known is perhaps muc(hirschman, 1998); others include trec, parseval, senseval, and the range of shared tasks organised by conll.
</prevsent>
<prevsent>such exercises are now common in most areas of nlp, and have had major impact on many areas, including machine translation and information extraction (see discussion of history of shared-task initiatives and their impact in belz and kilgarriff (2006)).<papid> W06-1421 </papid></prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
one of the best-known comparative studies of evaluation techniques was by papineni et al (2002) <papid> P02-1040 </papid>who proposed the bleu metric for machine translation and showed that bleu correlated well with human judgements when comparing several machine translation systems.</citsent>
<aftsection>
<nextsent>several other studies of this type have been carried out in the mt and summarisation communities.the first comparison of nlg evaluation techniques which we are aware of is by bangalore et al (2000).<papid> W00-1401 </papid></nextsent>
<nextsent>the authors manually created several variants of sentences from the wall street journal, and evaluated these sentences using both human judgements and several corpus-based metrics.they used linear regression to suggest combination of the corpus-based metrics which they be 136 lieve is better predictor of human judgements than any of the individual metrics.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4953">
<title id=" W06-1422.xml">geneval a proposal for shared task evaluation in nlg </title>
<section> comparative evaluations in nlg.  </section>
<citcontext>
<prevsection>
<prevsent>such exercises are now common in most areas of nlp, and have had major impact on many areas, including machine translation and information extraction (see discussion of history of shared-task initiatives and their impact in belz and kilgarriff (2006)).<papid> W06-1421 </papid></prevsent>
<prevsent>one of the best-known comparative studies of evaluation techniques was by papineni et al (2002) <papid> P02-1040 </papid>who proposed the bleu metric for machine translation and showed that bleu correlated well with human judgements when comparing several machine translation systems.</prevsent>
</prevsection>
<citsent citstr=" W00-1401 ">
several other studies of this type have been carried out in the mt and summarisation communities.the first comparison of nlg evaluation techniques which we are aware of is by bangalore et al (2000).<papid> W00-1401 </papid></citsent>
<aftsection>
<nextsent>the authors manually created several variants of sentences from the wall street journal, and evaluated these sentences using both human judgements and several corpus-based metrics.they used linear regression to suggest combination of the corpus-based metrics which they be 136 lieve is better predictor of human judgements than any of the individual metrics.
</nextsent>
<nextsent>in our work (belz and reiter, 2006), <papid> E06-1040 </papid>we used several different evaluation techniques (human and corpus-based) to evaluate the output of five nlg systems which generated wind descriptions for weather forecasts.</nextsent>
<nextsent>we then analysed how well the corpus-based evaluations correlated with the human-based evaluations.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4954">
<title id=" W06-1422.xml">geneval a proposal for shared task evaluation in nlg </title>
<section> comparative evaluations in nlg.  </section>
<citcontext>
<prevsection>
<prevsent>several other studies of this type have been carried out in the mt and summarisation communities.the first comparison of nlg evaluation techniques which we are aware of is by bangalore et al (2000).<papid> W00-1401 </papid></prevsent>
<prevsent>the authors manually created several variants of sentences from the wall street journal, and evaluated these sentences using both human judgements and several corpus-based metrics.they used linear regression to suggest combination of the corpus-based metrics which they be 136 lieve is better predictor of human judgements than any of the individual metrics.</prevsent>
</prevsection>
<citsent citstr=" E06-1040 ">
in our work (belz and reiter, 2006), <papid> E06-1040 </papid>we used several different evaluation techniques (human and corpus-based) to evaluate the output of five nlg systems which generated wind descriptions for weather forecasts.</citsent>
<aftsection>
<nextsent>we then analysed how well the corpus-based evaluations correlated with the human-based evaluations.
</nextsent>
<nextsent>amongst other things,we concluded that bleu-type metrics work reasonably well when comparing statistical nlg systems, but less well when comparing statistical nlg systems to knowledge-based nlg systems.we worked in this domain because of the availability of the sumtime corpus (sripada et al, 2003), which contains both numerical weather prediction data (i.e., inputs to nlg) and human written forecast texts (i.e., target outputs from nlg).
</nextsent>
<nextsent>we are not aware of any other nlg-related corpora which contain large number of texts and corresponding input datasets, and are freely available to the research community.
</nextsent>
<nextsent>we intend to apply for funding for three-year project to create more shared input/output data sets(we are focusing on data-to-text tasks for the reasons discussed in belz and kilgarriff (2006)), <papid> W06-1421 </papid>organise shared task workshops, and create and testa range of methods for evaluating submitted sys tems.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4959">
<title id=" W06-1415.xml">generating intelligent numerical answers in a question answering system </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>1.1 related work.
</prevsent>
<prevsent>search engines on the web produce set of answers to question in the form of hyper links orpage extracts, ranked according to content or popularity criteria (salton, 1989; page et al, 1998).some qa systems on the web use other tech niques: candidate answers are ranked according to score which takes into account lexical relations between questions and answers, semantic categories of concepts, distance between words, etc.
</prevsent>
</prevsection>
<citsent citstr=" N03-1022 ">
(moldovan et al, 2003), (<papid> N03-1022 </papid>narayanan and harabagiu, 2004), (<papid> W04-2502 </papid>radev and mckeown, 1998).<papid> J98-3005 </papid>recently, advanced qa systems defined relationships (equivalence, contradiction, ...)</citsent>
<aftsection>
<nextsent>between web page extracts or texts containing possible answers in order to combine them and to produce single answer (radev and mckeown, 1998), (<papid> J98-3005 </papid>harabagiu and lacatusu, 2004), (<papid> W04-2501 </papid>webber et al, 2002).</nextsent>
<nextsent>most systems provide the user with either set of potential answers (ranked or not), or the best?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4960">
<title id=" W06-1415.xml">generating intelligent numerical answers in a question answering system </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>1.1 related work.
</prevsent>
<prevsent>search engines on the web produce set of answers to question in the form of hyper links orpage extracts, ranked according to content or popularity criteria (salton, 1989; page et al, 1998).some qa systems on the web use other tech niques: candidate answers are ranked according to score which takes into account lexical relations between questions and answers, semantic categories of concepts, distance between words, etc.
</prevsent>
</prevsection>
<citsent citstr=" W04-2502 ">
(moldovan et al, 2003), (<papid> N03-1022 </papid>narayanan and harabagiu, 2004), (<papid> W04-2502 </papid>radev and mckeown, 1998).<papid> J98-3005 </papid>recently, advanced qa systems defined relationships (equivalence, contradiction, ...)</citsent>
<aftsection>
<nextsent>between web page extracts or texts containing possible answers in order to combine them and to produce single answer (radev and mckeown, 1998), (<papid> J98-3005 </papid>harabagiu and lacatusu, 2004), (<papid> W04-2501 </papid>webber et al, 2002).</nextsent>
<nextsent>most systems provide the user with either set of potential answers (ranked or not), or the best?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4961">
<title id=" W06-1415.xml">generating intelligent numerical answers in a question answering system </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>1.1 related work.
</prevsent>
<prevsent>search engines on the web produce set of answers to question in the form of hyper links orpage extracts, ranked according to content or popularity criteria (salton, 1989; page et al, 1998).some qa systems on the web use other tech niques: candidate answers are ranked according to score which takes into account lexical relations between questions and answers, semantic categories of concepts, distance between words, etc.
</prevsent>
</prevsection>
<citsent citstr=" J98-3005 ">
(moldovan et al, 2003), (<papid> N03-1022 </papid>narayanan and harabagiu, 2004), (<papid> W04-2502 </papid>radev and mckeown, 1998).<papid> J98-3005 </papid>recently, advanced qa systems defined relationships (equivalence, contradiction, ...)</citsent>
<aftsection>
<nextsent>between web page extracts or texts containing possible answers in order to combine them and to produce single answer (radev and mckeown, 1998), (<papid> J98-3005 </papid>harabagiu and lacatusu, 2004), (<papid> W04-2501 </papid>webber et al, 2002).</nextsent>
<nextsent>most systems provide the user with either set of potential answers (ranked or not), or the best?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4963">
<title id=" W06-1415.xml">generating intelligent numerical answers in a question answering system </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>search engines on the web produce set of answers to question in the form of hyper links orpage extracts, ranked according to content or popularity criteria (salton, 1989; page et al, 1998).some qa systems on the web use other tech niques: candidate answers are ranked according to score which takes into account lexical relations between questions and answers, semantic categories of concepts, distance between words, etc.
</prevsent>
<prevsent>(moldovan et al, 2003), (<papid> N03-1022 </papid>narayanan and harabagiu, 2004), (<papid> W04-2502 </papid>radev and mckeown, 1998).<papid> J98-3005 </papid>recently, advanced qa systems defined relationships (equivalence, contradiction, ...)</prevsent>
</prevsection>
<citsent citstr=" W04-2501 ">
between web page extracts or texts containing possible answers in order to combine them and to produce single answer (radev and mckeown, 1998), (<papid> J98-3005 </papid>harabagiu and lacatusu, 2004), (<papid> W04-2501 </papid>webber et al, 2002).</citsent>
<aftsection>
<nextsent>most systems provide the user with either set of potential answers (ranked or not), or the best?
</nextsent>
<nextsent>answer according to some relevance criteria.
</nextsent>
<nextsent>they do not provide answers which take into account information from set of candidate answers or answer inconsistencies.
</nextsent>
<nextsent>as for logical approaches used for database query, they are based on majority approach or on source reliability.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4964">
<title id=" W06-1415.xml">generating intelligent numerical answers in a question answering system </title>
<section> content determination for.  </section>
<citcontext>
<prevsection>
<prevsent>(3) numerical values can be compared only if they have the same unit of measure.
</prevsent>
<prevsent>if not, they have to be converted.
</prevsent>
</prevsection>
<citsent citstr=" W06-1808 ">
more details about comparison rules are presented in (moriceau, 2006).<papid> W06-1808 </papid></citsent>
<aftsection>
<nextsent>3.2 variation mode.
</nextsent>
<nextsent>in the case of numerical values varying over time, it is possible to characterize more precisely the variation.
</nextsent>
<nextsent>the idea is to draw trend (increase, decrease, ...)
</nextsent>
<nextsent>of variaton over time so that precise explanation can be generated.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4965">
<title id=" W06-2602.xml">constraint satisfaction inference non probabilistic global inference for sequence labelling </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>task baseline voting csinf oracle chunk 91.9 92.7 93.1 95.8 ner 77.2 80.2 81.8 86.5 med 64.7 67.5 68.9 74.9 genia 55.8 60.1 61.8 70.6 phoneme 79.0 83.4 84.5 98.8 morpho 41.3 46.1 51.9 62.2 table 1: performances of the baseline method, and the trigram method combined both with majority voting, and with constraint satisfaction inference.the last column shows the performance of the (hy pothetical) oracle inference procedure.
</prevsent>
<prevsent>spelling changes due to compounding, derivation, or inflection that would enable the reconstruction of the appropriate root forms of the involved mor phemes.for chunk, and the three information extraction tasks, instances represent seven-token window of words and their (predicted) part-of-speech tags.
</prevsent>
</prevsection>
<citsent citstr=" W95-0107 ">
each token is labelled with class using the iob type of segmentation coding as introduced by ramshaw and marcus (1995), <papid> W95-0107 </papid>marking whether the middle word is inside (i), outside (o), or at the beginning (b) of chunk, or named entity.</citsent>
<aftsection>
<nextsent>performance is measured by the f-score on correctly identified and labelled chunks, or named entities.
</nextsent>
<nextsent>instances for phoneme, and morpho consist of seven-letter window of letters only.
</nextsent>
<nextsent>the labels assigned to an instance are task-specific and have been introduced above, together with the tasksthemselves.
</nextsent>
<nextsent>generalisation performance is measured on the word accuracy level: if the entire phonological transcription of the word is predicted correctly, or if all three aspects of the morphological analysis are predicted correctly, the word is counted correct.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4966">
<title id=" W06-2932.xml">multilingual dependency analysis with a two stage discriminative parser </title>
<section> abstract </section>
<citcontext>
<prevsection>

<prevsent>we present two-stage multilingual dependency parser and evaluate it on 13 diverse languages.
</prevsent>
</prevsection>
<citsent citstr=" E06-1011 ">
the first stage isbased on the unlabeled dependency parsing models described by mcdonald and pereira (2006) <papid> E06-1011 </papid>augmented with morphological features for subset of the languages.</citsent>
<aftsection>
<nextsent>the second stage takes the out put from the first and labels all the edgesin the dependency graph with appropriate syntactic categories using globally trained sequence classifier over components of the graph.
</nextsent>
<nextsent>we report results on the conll-x shared task (buchholz et al., 2006) datasets and present an error analysis.
</nextsent>
<nextsent>often in language processing we require deep syntactic representation of sentence in order to assist further processing.
</nextsent>
<nextsent>with the availability of resources such as the penn wsj treebank, much of the focus in the parsing community had been on producing syntactic representations based on phrase-structure.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4967">
<title id=" W06-2932.xml">multilingual dependency analysis with a two stage discriminative parser </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>nivre (2005) gives an introduction to dependency representations of sentences andre cent developments in dependency parsing strategies.
</prevsent>
<prevsent>dependency graphs also encode much of the deep syntactic information needed for further processing.
</prevsent>
</prevsection>
<citsent citstr=" P05-1067 ">
this has been shown through their successful use in many standard natural language processing tasks, including machine translation (ding and palmer, 2005), <papid> P05-1067 </papid>sentence compression (mcdonald, 2006), <papid> E06-1038 </papid>and textual inference (haghighi et al, 2005).<papid> H05-1049 </papid>in this paper we describe two-stage discriminative parsing approach consisting of an unlabeled parser and subsequent edge labeler.</citsent>
<aftsection>
<nextsent>we evaluate this parser on diverse set of 13 languages using data provided by the conll-x shared-task organizers (buchholz et al, 2006; hajic?
</nextsent>
<nextsent>et al, 2004; simov et al, 2005; simov and osenova, 2003; chen et al, 2003; bohmova?
</nextsent>
<nextsent>et al, 2003; kromann, 2003; vander beek et al, 2002; brants et al, 2002; kawata and bartels, 2000; afonso et al, 2002; dzeroski et al., 2006; civit torruella and mart??
</nextsent>
<nextsent>antonn, 2002; nilsson et al, 2005; oflazer et al, 2003; atalay et al., 2003).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4968">
<title id=" W06-2932.xml">multilingual dependency analysis with a two stage discriminative parser </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>nivre (2005) gives an introduction to dependency representations of sentences andre cent developments in dependency parsing strategies.
</prevsent>
<prevsent>dependency graphs also encode much of the deep syntactic information needed for further processing.
</prevsent>
</prevsection>
<citsent citstr=" E06-1038 ">
this has been shown through their successful use in many standard natural language processing tasks, including machine translation (ding and palmer, 2005), <papid> P05-1067 </papid>sentence compression (mcdonald, 2006), <papid> E06-1038 </papid>and textual inference (haghighi et al, 2005).<papid> H05-1049 </papid>in this paper we describe two-stage discriminative parsing approach consisting of an unlabeled parser and subsequent edge labeler.</citsent>
<aftsection>
<nextsent>we evaluate this parser on diverse set of 13 languages using data provided by the conll-x shared-task organizers (buchholz et al, 2006; hajic?
</nextsent>
<nextsent>et al, 2004; simov et al, 2005; simov and osenova, 2003; chen et al, 2003; bohmova?
</nextsent>
<nextsent>et al, 2003; kromann, 2003; vander beek et al, 2002; brants et al, 2002; kawata and bartels, 2000; afonso et al, 2002; dzeroski et al., 2006; civit torruella and mart??
</nextsent>
<nextsent>antonn, 2002; nilsson et al, 2005; oflazer et al, 2003; atalay et al., 2003).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4969">
<title id=" W06-2932.xml">multilingual dependency analysis with a two stage discriminative parser </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>nivre (2005) gives an introduction to dependency representations of sentences andre cent developments in dependency parsing strategies.
</prevsent>
<prevsent>dependency graphs also encode much of the deep syntactic information needed for further processing.
</prevsent>
</prevsection>
<citsent citstr=" H05-1049 ">
this has been shown through their successful use in many standard natural language processing tasks, including machine translation (ding and palmer, 2005), <papid> P05-1067 </papid>sentence compression (mcdonald, 2006), <papid> E06-1038 </papid>and textual inference (haghighi et al, 2005).<papid> H05-1049 </papid>in this paper we describe two-stage discriminative parsing approach consisting of an unlabeled parser and subsequent edge labeler.</citsent>
<aftsection>
<nextsent>we evaluate this parser on diverse set of 13 languages using data provided by the conll-x shared-task organizers (buchholz et al, 2006; hajic?
</nextsent>
<nextsent>et al, 2004; simov et al, 2005; simov and osenova, 2003; chen et al, 2003; bohmova?
</nextsent>
<nextsent>et al, 2003; kromann, 2003; vander beek et al, 2002; brants et al, 2002; kawata and bartels, 2000; afonso et al, 2002; dzeroski et al., 2006; civit torruella and mart??
</nextsent>
<nextsent>antonn, 2002; nilsson et al, 2005; oflazer et al, 2003; atalay et al., 2003).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC4971">
<title id=" W06-2932.xml">multilingual dependency analysis with a two stage discriminative parser </title>
<section> stage 1: unlabeled parsing.  </section>
<citcontext>
<prevsection>
<prevsent>we 216 assume that all dependency graphs are trees but may be non-projective, both of which are true in the datasets we use.
</prevsent>
<prevsent>the first stage of our system creates an unlabeled parse for an input sentence x. this system is primarily based on the parsing models described by mcdonald and pereira (2006).<papid> E06-1011 </papid></prevsent>
</prevsection>
<citsent citstr=" P05-1012 ">
that work extends the maximum spanning tree dependency parsing framework (mcdonald et al, 2005<papid> P05-1012 </papid>a; mcdonald et al, 2005<papid> P05-1012 </papid>b) to incorporate features over multiple edges in the dependency graph.</citsent>
<aftsection>
<nextsent>an exact projective and an approximate non-projective parsing algorithm are presented, since it is shown that non projective dependency parsing becomes np-hard when features are extended beyond single edge.
</nextsent>
<nextsent>that system uses mira, an online large-margin learning algorithm, to compute model parameters.its power lies in the ability to define rich set of features over parsing decisions, as well as surface level features relative to these decisions.
</nextsent>
<nextsent>for instance, the system of mcdonald et al (2005<papid> P05-1012 </papid>a) incorporates features over the part of speech of words occurring between and around possible head-dependent relation.</nextsent>
<nextsent>these features are highly important to over all accuracy since they eliminate unlikely scenarios such as preposition modifying noun not directly to its left, or noun modifying verb with another verb occurring between them.we augmented this model to incorporate morphological features derived from each token.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5026">
<title id=" W06-1406.xml">using distributional similarity to identify individual verb choice </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>is it possible to predict the individual verbs automatically?
</prevsent>
<prevsent>the distributional hypothesis (harris, 1968) says the following: the meaning of entities, and the meaning of grammatical relations among them, is related to the restriction of combinations of these entities relative to other entities.
</prevsent>
</prevsection>
<citsent citstr=" P98-2127 ">
over recent years, many applications (lin, 1998), (<papid> P98-2127 </papid>lee, 1999), (<papid> P99-1004 </papid>lee, 2001), (weeds et al,2004), <papid> C04-1146 </papid>and (weeds and weir, 2006) have been investigating the distributional similarity of words.</citsent>
<aftsection>
<nextsent>similarity means that words with similar meaning tend to appear in similar contexts.
</nextsent>
<nextsent>in nlg, the consideration of semantic similarity is usually preferred to just distributional similarity.
</nextsent>
<nextsent>however, in our case, the most important thing is to capture themost probable choice of verb of an individual author for expressing an action.
</nextsent>
<nextsent>the expression of an action can be either the same verb, synonyms, or coordinate terms to the verb in the big corpus, or any verbs that an individual author chooses for this action.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5027">
<title id=" W06-1406.xml">using distributional similarity to identify individual verb choice </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>is it possible to predict the individual verbs automatically?
</prevsent>
<prevsent>the distributional hypothesis (harris, 1968) says the following: the meaning of entities, and the meaning of grammatical relations among them, is related to the restriction of combinations of these entities relative to other entities.
</prevsent>
</prevsection>
<citsent citstr=" P99-1004 ">
over recent years, many applications (lin, 1998), (<papid> P98-2127 </papid>lee, 1999), (<papid> P99-1004 </papid>lee, 2001), (weeds et al,2004), <papid> C04-1146 </papid>and (weeds and weir, 2006) have been investigating the distributional similarity of words.</citsent>
<aftsection>
<nextsent>similarity means that words with similar meaning tend to appear in similar contexts.
</nextsent>
<nextsent>in nlg, the consideration of semantic similarity is usually preferred to just distributional similarity.
</nextsent>
<nextsent>however, in our case, the most important thing is to capture themost probable choice of verb of an individual author for expressing an action.
</nextsent>
<nextsent>the expression of an action can be either the same verb, synonyms, or coordinate terms to the verb in the big corpus, or any verbs that an individual author chooses for this action.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5028">
<title id=" W06-1406.xml">using distributional similarity to identify individual verb choice </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>is it possible to predict the individual verbs automatically?
</prevsent>
<prevsent>the distributional hypothesis (harris, 1968) says the following: the meaning of entities, and the meaning of grammatical relations among them, is related to the restriction of combinations of these entities relative to other entities.
</prevsent>
</prevsection>
<citsent citstr=" C04-1146 ">
over recent years, many applications (lin, 1998), (<papid> P98-2127 </papid>lee, 1999), (<papid> P99-1004 </papid>lee, 2001), (weeds et al,2004), <papid> C04-1146 </papid>and (weeds and weir, 2006) have been investigating the distributional similarity of words.</citsent>
<aftsection>
<nextsent>similarity means that words with similar meaning tend to appear in similar contexts.
</nextsent>
<nextsent>in nlg, the consideration of semantic similarity is usually preferred to just distributional similarity.
</nextsent>
<nextsent>however, in our case, the most important thing is to capture themost probable choice of verb of an individual author for expressing an action.
</nextsent>
<nextsent>the expression of an action can be either the same verb, synonyms, or coordinate terms to the verb in the big corpus, or any verbs that an individual author chooses for this action.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5030">
<title id=" W06-2209.xml">active annotation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>even though there are plenty of biomedical texts, very little of it is annotated, such as the genia corpus (kim et al, 2003).
</prevsent>
<prevsent>a very popular and well investigated framework in order to cope with the lack of training material is the active learning framework (cohn et al, 1995; seung et al, 1992).
</prevsent>
</prevsection>
<citsent citstr=" P04-1075 ">
it has been applied to various nlp/ie tasks, including named entity recognition (shen et al, 2004) <papid> P04-1075 </papid>and parse selection (baldridge and osborne, 2004) <papid> W04-3202 </papid>with rather impressive results in reducing the amount of annotated training data.</citsent>
<aftsection>
<nextsent>however, some criticism of active learning has been expressed recently, concerning the re usability of the data (baldridge and osborne, 2004).<papid> W04-3202 </papid></nextsent>
<nextsent>this paper presents framework in order to deal with the lack of training data for nlp tasks.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5031">
<title id=" W06-2209.xml">active annotation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>even though there are plenty of biomedical texts, very little of it is annotated, such as the genia corpus (kim et al, 2003).
</prevsent>
<prevsent>a very popular and well investigated framework in order to cope with the lack of training material is the active learning framework (cohn et al, 1995; seung et al, 1992).
</prevsent>
</prevsection>
<citsent citstr=" W04-3202 ">
it has been applied to various nlp/ie tasks, including named entity recognition (shen et al, 2004) <papid> P04-1075 </papid>and parse selection (baldridge and osborne, 2004) <papid> W04-3202 </papid>with rather impressive results in reducing the amount of annotated training data.</citsent>
<aftsection>
<nextsent>however, some criticism of active learning has been expressed recently, concerning the re usability of the data (baldridge and osborne, 2004).<papid> W04-3202 </papid></nextsent>
<nextsent>this paper presents framework in order to deal with the lack of training data for nlp tasks.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5033">
<title id=" W06-2209.xml">active annotation </title>
<section> active annotation.  </section>
<citcontext>
<prevsection>
<prevsent>during initialization, an unsupervised method is required to provide an initial tagging on the data d. this is an important restriction which is imposed by the lack of any annotated data.
</prevsent>
<prevsent>even under this restriction, there are some options available, especially for tasks which have compiled resources.
</prevsent>
</prevsection>
<citsent citstr=" W99-0613 ">
one option is to use an unsupervised learning algorithm, such the one presented by collins &amp; singer (1999),<papid> W99-0613 </papid>where seed set of rules is used to bootstrap rule based named entity recognizer.</citsent>
<aftsection>
<nextsent>a different approach could be the use of dictionary-based tagger, as in morgan et al (2003).<papid> W03-1301 </papid></nextsent>
<nextsent>it must be noted that the unsupervised method used to provide the initial tagging does not need to generalize to any data (a common problem for such methods), it only needs to perform well on the data used during active annotation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5034">
<title id=" W06-2209.xml">active annotation </title>
<section> active annotation.  </section>
<citcontext>
<prevsection>
<prevsent>even under this restriction, there are some options available, especially for tasks which have compiled resources.
</prevsent>
<prevsent>one option is to use an unsupervised learning algorithm, such the one presented by collins &amp; singer (1999),<papid> W99-0613 </papid>where seed set of rules is used to bootstrap rule based named entity recognizer.</prevsent>
</prevsection>
<citsent citstr=" W03-1301 ">
a different approach could be the use of dictionary-based tagger, as in morgan et al (2003).<papid> W03-1301 </papid></citsent>
<aftsection>
<nextsent>it must be noted that the unsupervised method used to provide the initial tagging does not need to generalize to any data (a common problem for such methods), it only needs to perform well on the data used during active annotation.
</nextsent>
<nextsent>generalization on unseen data is an attribute we hope that the supervised learning method will have after training on the annotated material created with active annotation.the query module is also different from the corresponding module inactive learning.
</nextsent>
<nextsent>instead of selecting unlabeled informative instances to be annotated and added to the training data, its purpose is to identify likely errors in the imperfectly labelled training data, so that they are checked and corrected by the human annotator.
</nextsent>
<nextsent>in order to perform error-detection, we choseto adapt the approach of nakagawa and matsumoto (2002) <papid> C02-1101 </papid>which resembles uncertainty based sampling for active learning.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5035">
<title id=" W06-2209.xml">active annotation </title>
<section> active annotation.  </section>
<citcontext>
<prevsection>
<prevsent>generalization on unseen data is an attribute we hope that the supervised learning method will have after training on the annotated material created with active annotation.the query module is also different from the corresponding module inactive learning.
</prevsent>
<prevsent>instead of selecting unlabeled informative instances to be annotated and added to the training data, its purpose is to identify likely errors in the imperfectly labelled training data, so that they are checked and corrected by the human annotator.
</prevsent>
</prevsection>
<citsent citstr=" C02-1101 ">
in order to perform error-detection, we choseto adapt the approach of nakagawa and matsumoto (2002) <papid> C02-1101 </papid>which resembles uncertainty based sampling for active learning.</citsent>
<aftsection>
<nextsent>according to their paradigm, likely errors in the training data are instances that are hard?
</nextsent>
<nextsent>for the classifier and inconsistent with the rest of the data.
</nextsent>
<nextsent>in our case, we used the uncertainty of the classifier as the measure of thehardness?
</nextsent>
<nextsent>of an instance.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5036">
<title id=" W06-2209.xml">active annotation </title>
<section> active annotation.  </section>
<citcontext>
<prevsection>
<prevsent>while this method of detecting errors resembles uncertainty sampling, there are other approaches that could have been used instead and they can bevery different.
</prevsent>
<prevsent>sjobergh and knutsson (2005) inserted artificial errors and trained classifier to recognize them.
</prevsent>
</prevsection>
<citsent citstr=" E03-1068 ">
dickinson and meuers (2003) <papid> E03-1068 </papid>proposed methods based on n-grams occurring with different label lings in the corpus.</citsent>
<aftsection>
<nextsent>therefore, while it is reasonable to expect some correlation between the selections of active annotation and active learning(hard instances are likely to be erroneously annotated by the unsupervised tagger), the task of selecting hard instances is quite different from detectingerrors.
</nextsent>
<nextsent>the use of the disagreement between taggers for selecting candidates for manual correction is reminiscent of corrected co-training (pierce andcardie, 2001).<papid> W01-0501 </papid></nextsent>
<nextsent>however, the main difference iscor rected co-training results in manually annotated corpus, while active annotation allows automatically annotated instances to be kept.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5037">
<title id=" W06-2209.xml">active annotation </title>
<section> active annotation.  </section>
<citcontext>
<prevsection>
<prevsent>dickinson and meuers (2003) <papid> E03-1068 </papid>proposed methods based on n-grams occurring with different label lings in the corpus.</prevsent>
<prevsent>therefore, while it is reasonable to expect some correlation between the selections of active annotation and active learning(hard instances are likely to be erroneously annotated by the unsupervised tagger), the task of selecting hard instances is quite different from detectingerrors.</prevsent>
</prevsection>
<citsent citstr=" W01-0501 ">
the use of the disagreement between taggers for selecting candidates for manual correction is reminiscent of corrected co-training (pierce andcardie, 2001).<papid> W01-0501 </papid></citsent>
<aftsection>
<nextsent>however, the main difference iscor rected co-training results in manually annotated corpus, while active annotation allows automatically annotated instances to be kept.
</nextsent>
<nextsent>in order to perform error detection according to the previous section we need to obtain uncertainty estimations over each token from the named entity recognition module of lingpipe.
</nextsent>
<nextsent>for each token tand possible label l, ling pipe estimates the following hidden markov model from the training data: (t[n], l[n]|l[n ? 1], t[n ? 1], t[n ? 2]) (1) when annotating certain text passage, the tokens are fixed and the joint probability of equation 1 is computed for each possible combination of labels.
</nextsent>
<nextsent>from bayes?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5038">
<title id=" W06-2209.xml">active annotation </title>
<section> hmm uncertainty estimation.  </section>
<citcontext>
<prevsection>
<prevsent>it is possible that the two highest scored predictions are obtained for two different previous labels.
</prevsent>
<prevsent>it may also be the case that highly scored label can be obtained given very improbable previous label.
</prevsent>
</prevsection>
<citsent citstr=" N04-4028 ">
finally, an alternative that we did not explore in this work isthe field confidence estimation (culotta and mccallum, 2004), <papid> N04-4028 </papid>which allows the estimation of confidence over sequences of tokens, instead of singleton tokens only.</citsent>
<aftsection>
<nextsent>however, in this work confidence estimation over singleton tokens is sufficient.
</nextsent>
<nextsent>in this section we present results from applying active annotation to biomedical named entity recognition.
</nextsent>
<nextsent>using the noise models described in section 3,we corrupted the training data and then using ling pipe as the supervised learner we applied the algorithm of figure 2.
</nextsent>
<nextsent>the batch of tokens selected to be checked in each round was 2000 tokens.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5041">
<title id=" W07-0411.xml">dependency based automatic evaluation for machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in comparison with other metrics on 16,800 sentences of chinese-english newswire text, our method reaches high correlation with human scores.
</prevsent>
<prevsent>an experiment with two translations of 4,000 sentences from spanish-english europarl shows that, in contrast to most other metrics, our method does not display high bias towards statistical models of translation.
</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
since their appearance, string-based evaluation metrics such as bleu (papineni et al, 2002) <papid> P02-1040 </papid>and nist (doddington, 2002) have been the standard tools used for evaluating mt quality.</citsent>
<aftsection>
<nextsent>both score candidate translation on the basis of the number of n-grams shared with one or more reference translations.
</nextsent>
<nextsent>automatic measures are indispensable in the development of mt systems, because they allow mt developers to conduct frequent, cost effective, and fast evaluations of their evolving models.
</nextsent>
<nextsent>these advantages come at price, though: an automatic comparison of n-grams measures only the string similarity of the candidate translation to one or more reference strings, and will penalize any divergence from them.
</nextsent>
<nextsent>in effect, candidate translation expressing the source meaning accurately and fluently will be given low score if the lexical and syntactic choices it contains, even though perfectly legitimate, are not present in at least one of the references.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5042">
<title id=" W07-0411.xml">dependency based automatic evaluation for machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>dependencies abstract away from the particulars of the surface string (and cfg tree) realization and provide normalized?
</prevsent>
<prevsent>representation of (some) syntactic variants of given sentence.
</prevsent>
</prevsection>
<citsent citstr=" P04-1041 ">
the translation and reference files are analyzed by treebank-based, probabilistic lexical-functional grammar (lfg) parser (cahill et al, 2004), <papid> P04-1041 </papid>which produces set of dependency triples for each input.</citsent>
<aftsection>
<nextsent>the translation set is compared to the reference set, and the number of matches is calculated, giving the precision, recall, and f-score for that particular translation.
</nextsent>
<nextsent>in addition, to allow for the possibility of valid lexical differences between the translation and the references, we follow kauchak and barzilay (2006) <papid> N06-1058 </papid>and owczarzak et al (2006) <papid> W06-3112 </papid>in adding number of paraphrases in the process of evaluation to raise the number of matches between the translation and the reference, leading to higher score.</nextsent>
<nextsent>comparing the lfg-based evaluation method with other popular metrics: bleu, nist, general text matcher (gtm) (turian et al, 2003), translation error rate (ter) (snover et al, 2006)1, and meteor (banerjee and lavie, 2005), <papid> W05-0909 </papid>we show that combining dependency representations with paraphrases leads to more accurate evaluation that correlates better with human judgment.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5043">
<title id=" W07-0411.xml">dependency based automatic evaluation for machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the translation and reference files are analyzed by treebank-based, probabilistic lexical-functional grammar (lfg) parser (cahill et al, 2004), <papid> P04-1041 </papid>which produces set of dependency triples for each input.</prevsent>
<prevsent>the translation set is compared to the reference set, and the number of matches is calculated, giving the precision, recall, and f-score for that particular translation.</prevsent>
</prevsection>
<citsent citstr=" N06-1058 ">
in addition, to allow for the possibility of valid lexical differences between the translation and the references, we follow kauchak and barzilay (2006) <papid> N06-1058 </papid>and owczarzak et al (2006) <papid> W06-3112 </papid>in adding number of paraphrases in the process of evaluation to raise the number of matches between the translation and the reference, leading to higher score.</citsent>
<aftsection>
<nextsent>comparing the lfg-based evaluation method with other popular metrics: bleu, nist, general text matcher (gtm) (turian et al, 2003), translation error rate (ter) (snover et al, 2006)1, and meteor (banerjee and lavie, 2005), <papid> W05-0909 </papid>we show that combining dependency representations with paraphrases leads to more accurate evaluation that correlates better with human judgment.</nextsent>
<nextsent>the remainder of this paper is organized as follows: section 2 gives basic introduction to lfg; section 3 describes related work; section 4 describes our method and gives results of two experiments on different sets of data: 4,000 sentences from spanish-english europarl and 16,800 sentences of chinese-english newswire text from the linguistic data consortiums (ldc) multiple translation project; section 5 discusses ongoing work; section 6 concludes.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5047">
<title id=" W07-0411.xml">dependency based automatic evaluation for machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the translation and reference files are analyzed by treebank-based, probabilistic lexical-functional grammar (lfg) parser (cahill et al, 2004), <papid> P04-1041 </papid>which produces set of dependency triples for each input.</prevsent>
<prevsent>the translation set is compared to the reference set, and the number of matches is calculated, giving the precision, recall, and f-score for that particular translation.</prevsent>
</prevsection>
<citsent citstr=" W06-3112 ">
in addition, to allow for the possibility of valid lexical differences between the translation and the references, we follow kauchak and barzilay (2006) <papid> N06-1058 </papid>and owczarzak et al (2006) <papid> W06-3112 </papid>in adding number of paraphrases in the process of evaluation to raise the number of matches between the translation and the reference, leading to higher score.</citsent>
<aftsection>
<nextsent>comparing the lfg-based evaluation method with other popular metrics: bleu, nist, general text matcher (gtm) (turian et al, 2003), translation error rate (ter) (snover et al, 2006)1, and meteor (banerjee and lavie, 2005), <papid> W05-0909 </papid>we show that combining dependency representations with paraphrases leads to more accurate evaluation that correlates better with human judgment.</nextsent>
<nextsent>the remainder of this paper is organized as follows: section 2 gives basic introduction to lfg; section 3 describes related work; section 4 describes our method and gives results of two experiments on different sets of data: 4,000 sentences from spanish-english europarl and 16,800 sentences of chinese-english newswire text from the linguistic data consortiums (ldc) multiple translation project; section 5 discusses ongoing work; section 6 concludes.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5050">
<title id=" W07-0411.xml">dependency based automatic evaluation for machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the translation set is compared to the reference set, and the number of matches is calculated, giving the precision, recall, and f-score for that particular translation.
</prevsent>
<prevsent>in addition, to allow for the possibility of valid lexical differences between the translation and the references, we follow kauchak and barzilay (2006) <papid> N06-1058 </papid>and owczarzak et al (2006) <papid> W06-3112 </papid>in adding number of paraphrases in the process of evaluation to raise the number of matches between the translation and the reference, leading to higher score.</prevsent>
</prevsection>
<citsent citstr=" W05-0909 ">
comparing the lfg-based evaluation method with other popular metrics: bleu, nist, general text matcher (gtm) (turian et al, 2003), translation error rate (ter) (snover et al, 2006)1, and meteor (banerjee and lavie, 2005), <papid> W05-0909 </papid>we show that combining dependency representations with paraphrases leads to more accurate evaluation that correlates better with human judgment.</citsent>
<aftsection>
<nextsent>the remainder of this paper is organized as follows: section 2 gives basic introduction to lfg; section 3 describes related work; section 4 describes our method and gives results of two experiments on different sets of data: 4,000 sentences from spanish-english europarl and 16,800 sentences of chinese-english newswire text from the linguistic data consortiums (ldc) multiple translation project; section 5 discusses ongoing work; section 6 concludes.
</nextsent>
<nextsent>1 as we focus on purely automatic metrics, we omit.
</nextsent>
<nextsent>hter (human-targeted translation error rate) here.
</nextsent>
<nextsent>in lexical-functional grammar (bresnan, 2001) sentence structure is represented in terms of c(onstituent)-structure and f(unctional)-structure.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5055">
<title id=" W07-0411.xml">dependency based automatic evaluation for machine translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>recently number of attempts to remedy these shortcomings have led to the development of other automatic mt evaluation metrics.
</prevsent>
<prevsent>some of them concentrate mainly on word order, like general text matcher (turian et al, 2003), which calculates precision and recall for translation reference pairs, weighting contiguous matches more than non-sequential matches, or translation error rate (snover et al, 2005), which computes the number of substitutions, inserts, deletions, and shifts necessary to transform the translation text to match the reference.
</prevsent>
</prevsection>
<citsent citstr=" E06-1031 ">
others try to accommodate both syntactic and lexical differences between the candidate translation and the reference, like cder (leusch et al, 2006), <papid> E06-1031 </papid>which employs version of edit distance for word substitution and reordering; or meteor (banerjee and lavie, 2005), <papid> W05-0909 </papid>which uses stemming and wordnet synonymy.</citsent>
<aftsection>
<nextsent>kauchak and barzilay (2006) <papid> N06-1058 </papid>and owczarzak et al (2006) <papid> W06-3112 </papid>use paraphrases during bleu and nist evaluation to increase the number of matches between the translation and the reference; the paraphrases are either taken from wordnet5 in kauchak and barzilay (2006) <papid> N06-1058 </papid>or derived from the test set itself through automatic word and phrase alignment in owczarzak et al (2006).<papid> W06-3112 </papid></nextsent>
<nextsent>another metric making use of synonyms is the linear regression model developed by russo-lassner et al (2005), which makes use of stemming, wordnet synonymy, verb class synonymy, matching noun phrase heads, and proper name matching.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5071">
<title id=" W07-0411.xml">dependency based automatic evaluation for machine translation </title>
<section> lfg f-structure in mt evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>83 observed this tendency in previous research as well; in one experiment, reported in owczarzak et al.
</prevsent>
<prevsent>(2006), where the rule-based system logomedia7 was compared with pharaoh, bleu scored pharaoh 0.0349 points higher, nist scored pharaoh 0.6219 points higher, but human judges scored logo media output 0.19 points higher (on 5-point scale).
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
4.1.1 experimental design in order to check for the existence of bias in the dependency-based metric, we created set of 4,000 sentences drawn randomly from the spanish english subset of europarl (koehn, 2005), and we produced two translations: one by rule-based system logo media, and the other by the standard phrase-based statistical decoder pharaoh, using alignments produced by giza++8 and the refined word alignment strategy of och and ney (2003).<papid> J03-1002 </papid></citsent>
<aftsection>
<nextsent>the translations were scored with range of metrics: bleu, nist, gtm, ter, meteor, and the dependency-based method.
</nextsent>
<nextsent>4.1.2 adding synonyms besides the ability to allow syntactic variants as valid translations, good metric should also be able to accept legitimate lexical variation.
</nextsent>
<nextsent>we introduced synonyms and paraphrases into the process of evaluation, creating new best-matching references for the translations using either paraphrases derived from the test set itself (following owczarzak et al (2006)) <papid> W06-3112 </papid>or wordnet synonyms (as in kauchak and barzilay (2006)).<papid> N06-1058 </papid></nextsent>
<nextsent>bitext-derived paraphrases owczarzak et al (2006) <papid> W06-3112 </papid>describe simple way to produce list of paraphrases, which can be useful in mt evaluation, by running word alignment software on the test set that is being evaluated.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5084">
<title id=" W06-3108.xml">discriminative reordering models for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>still the fluency of the machine translation output leaves much to desire.
</prevsent>
<prevsent>one reason isthat most phrase-based systems use very simple reordering model.
</prevsent>
</prevsection>
<citsent citstr=" W99-0604 ">
usually, the costs for phrase movements are linear in the distance, e.g. see (och et al, 1999; <papid> W99-0604 </papid>koehn, 2004; zens et al, 2005).</citsent>
<aftsection>
<nextsent>recently, in (tillmann and zhang, 2005) <papid> P05-1069 </papid>and in (koehn et al, 2005), reordering model has been described that tries to predict the orientation of phrase, i.e. it answers the question should the next phrase be to the left or to the right of the currentphrase??</nextsent>
<nextsent>this phrase orientation probability is conditioned on the current source and target phrase and relative frequencies are used to estimate the proba bilities.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5086">
<title id=" W06-3108.xml">discriminative reordering models for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>one reason isthat most phrase-based systems use very simple reordering model.
</prevsent>
<prevsent>usually, the costs for phrase movements are linear in the distance, e.g. see (och et al, 1999; <papid> W99-0604 </papid>koehn, 2004; zens et al, 2005).</prevsent>
</prevsection>
<citsent citstr=" P05-1069 ">
recently, in (tillmann and zhang, 2005) <papid> P05-1069 </papid>and in (koehn et al, 2005), reordering model has been described that tries to predict the orientation of phrase, i.e. it answers the question should the next phrase be to the left or to the right of the currentphrase??</citsent>
<aftsection>
<nextsent>this phrase orientation probability is conditioned on the current source and target phrase and relative frequencies are used to estimate the probabilities.
</nextsent>
<nextsent>we adopt the idea of predicting the orientation, but we propose to use maximum-entropy based model.
</nextsent>
<nextsent>the relative-frequency based approach may suffer from the data sparseness problem, because most of the phrases occur only once in the training corpus.
</nextsent>
<nextsent>our approach circumvents this problem by using combination of phrase-level and word-level features and by using word-classes or part-of-speechinformation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5090">
<title id=" W06-3108.xml">discriminative reordering models for statistical machine translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the idea of predicting the orientation is adopted from (tillmann and zhang, 2005) <papid> P05-1069 </papid>and (koehn et al, 2005).</prevsent>
<prevsent>here, we use the maximum entropy principle to combine variety of different features.</prevsent>
</prevsection>
<citsent citstr=" H05-1021 ">
a reordering model in the framework of weighted finite state transducers is described in (kumar and byrne, 2005).<papid> H05-1021 </papid></citsent>
<aftsection>
<nextsent>there, the movements are defined at the phrase level, but the window for reordering is very limited.
</nextsent>
<nextsent>the parameters are estimated using an em-style method.
</nextsent>
<nextsent>none of these methods try to generalize from the words or phrases by using word classes or part-of speech information.the approach presented here has some resemblance to the bracketing transduction grammars (btg) of (wu, 1997), <papid> J97-3002 </papid>which have been applied to phrase-based machine translation system in (zens et al, 2004).<papid> C04-1030 </papid></nextsent>
<nextsent>the difference is that, here, we do not constrain the phrase reordering.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5091">
<title id=" W06-3108.xml">discriminative reordering models for statistical machine translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>there, the movements are defined at the phrase level, but the window for reordering is very limited.
</prevsent>
<prevsent>the parameters are estimated using an em-style method.
</prevsent>
</prevsection>
<citsent citstr=" J97-3002 ">
none of these methods try to generalize from the words or phrases by using word classes or part-of speech information.the approach presented here has some resemblance to the bracketing transduction grammars (btg) of (wu, 1997), <papid> J97-3002 </papid>which have been applied to phrase-based machine translation system in (zens et al, 2004).<papid> C04-1030 </papid></citsent>
<aftsection>
<nextsent>the difference is that, here, we do not constrain the phrase reordering.
</nextsent>
<nextsent>nevertheless the inverted/monotone concatenation of phrases in the btg framework is similar to the left/right phrase orientation used here.
</nextsent>
<nextsent>in statistical machine translation, we are given source language sentence fj1 = f1 . . .
</nextsent>
<nextsent>fj . . .
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5092">
<title id=" W06-3108.xml">discriminative reordering models for statistical machine translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>there, the movements are defined at the phrase level, but the window for reordering is very limited.
</prevsent>
<prevsent>the parameters are estimated using an em-style method.
</prevsent>
</prevsection>
<citsent citstr=" C04-1030 ">
none of these methods try to generalize from the words or phrases by using word classes or part-of speech information.the approach presented here has some resemblance to the bracketing transduction grammars (btg) of (wu, 1997), <papid> J97-3002 </papid>which have been applied to phrase-based machine translation system in (zens et al, 2004).<papid> C04-1030 </papid></citsent>
<aftsection>
<nextsent>the difference is that, here, we do not constrain the phrase reordering.
</nextsent>
<nextsent>nevertheless the inverted/monotone concatenation of phrases in the btg framework is similar to the left/right phrase orientation used here.
</nextsent>
<nextsent>in statistical machine translation, we are given source language sentence fj1 = f1 . . .
</nextsent>
<nextsent>fj . . .
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5093">
<title id=" W06-3108.xml">discriminative reordering models for statistical machine translation </title>
<section> baseline system.  </section>
<citcontext>
<prevsection>
<prevsent>fj ,which is to be translated into target language sentence ei1 = e1 . . .
</prevsent>
<prevsent>ei . . .
</prevsent>
</prevsection>
<citsent citstr=" P02-1038 ">
ei . among all possible target language sentences, we will choose the sentence with the highest probability: ei1 = argmax i,ei1 { pr(ei1|fj1 ) } (1)the posterior probability pr(ei1|fj1 ) is modeled directly using log-linear combination of several models (och and ney, 2002): <papid> P02-1038 </papid>pr(ei1|fj1 ) = exp ( m=1 mhm(ei1, fj1 ) ) ? i?,ei1 exp ( m=1 mhm(ei ? 1 , fj1 ) ) (2) the denominator represents normalization factor that depends only on the source sentence fj1 . therefore, we can omit it during the search process.</citsent>
<aftsection>
<nextsent>as decision rule, we obtain: ei1 = argmax i,ei1 { ? m=1 mhm(ei1, fj1 ) } (3)this approach is generalization of the source channel approach (brown et al, 1990).<papid> J90-2002 </papid></nextsent>
<nextsent>it has the advantage that additional models h(?)</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5094">
<title id=" W06-3108.xml">discriminative reordering models for statistical machine translation </title>
<section> baseline system.  </section>
<citcontext>
<prevsection>
<prevsent>ei . . .
</prevsent>
<prevsent>ei . among all possible target language sentences, we will choose the sentence with the highest probability: ei1 = argmax i,ei1 { pr(ei1|fj1 ) } (1)the posterior probability pr(ei1|fj1 ) is modeled directly using log-linear combination of several models (och and ney, 2002): <papid> P02-1038 </papid>pr(ei1|fj1 ) = exp ( m=1 mhm(ei1, fj1 ) ) ? i?,ei1 exp ( m=1 mhm(ei ? 1 , fj1 ) ) (2) the denominator represents normalization factor that depends only on the source sentence fj1 . therefore, we can omit it during the search process.</prevsent>
</prevsection>
<citsent citstr=" J90-2002 ">
as decision rule, we obtain: ei1 = argmax i,ei1 { ? m=1 mhm(ei1, fj1 ) } (3)this approach is generalization of the source channel approach (brown et al, 1990).<papid> J90-2002 </papid></citsent>
<aftsection>
<nextsent>it has the advantage that additional models h(?)
</nextsent>
<nextsent>can be easily integrated into the overall system.
</nextsent>
<nextsent>the model scaling factors m1 are trained with respect to the final translation quality measured by an error criterion (och, 2003).<papid> P03-1021 </papid></nextsent>
<nextsent>we use state-of-the-art phrase-based translation system (zens and ney, 2004; <papid> N04-1033 </papid>zens et al, 2005) including the following models: an n-gram language model, phrase translation model and word-based lexicon model.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5095">
<title id=" W06-3108.xml">discriminative reordering models for statistical machine translation </title>
<section> baseline system.  </section>
<citcontext>
<prevsection>
<prevsent>it has the advantage that additional models h(?)
</prevsent>
<prevsent>can be easily integrated into the overall system.
</prevsent>
</prevsection>
<citsent citstr=" P03-1021 ">
the model scaling factors m1 are trained with respect to the final translation quality measured by an error criterion (och, 2003).<papid> P03-1021 </papid></citsent>
<aftsection>
<nextsent>we use state-of-the-art phrase-based translation system (zens and ney, 2004; <papid> N04-1033 </papid>zens et al, 2005) including the following models: an n-gram language model, phrase translation model and word-based lexicon model.</nextsent>
<nextsent>the latter two models are used for both directions: p(f |e) and p(e|f).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5096">
<title id=" W06-3108.xml">discriminative reordering models for statistical machine translation </title>
<section> baseline system.  </section>
<citcontext>
<prevsection>
<prevsent>can be easily integrated into the overall system.
</prevsent>
<prevsent>the model scaling factors m1 are trained with respect to the final translation quality measured by an error criterion (och, 2003).<papid> P03-1021 </papid></prevsent>
</prevsection>
<citsent citstr=" N04-1033 ">
we use state-of-the-art phrase-based translation system (zens and ney, 2004; <papid> N04-1033 </papid>zens et al, 2005) including the following models: an n-gram language model, phrase translation model and word-based lexicon model.</citsent>
<aftsection>
<nextsent>the latter two models are used for both directions: p(f |e) and p(e|f).
</nextsent>
<nextsent>additionally, we use word penalty and phrase penalty.
</nextsent>
<nextsent>the reordering model of the baseline system is distance based, i.e. it assigns costs based on the distance from the end position of phrase to the start position of the next phrase.
</nextsent>
<nextsent>this very simple reordering model is widely used, for instance in (och et al, 1999; <papid> W99-0604 </papid>koehn, 2004; zens et al, 2005).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5099">
<title id=" W06-3108.xml">discriminative reordering models for statistical machine translation </title>
<section> the reordering model.  </section>
<citcontext>
<prevsection>
<prevsent>in the case of two orientation classes, cj,j? is defined as: cj,j? = { left, if j?
</prevsent>
<prevsent>  right, if j?
</prevsent>
</prevsection>
<citsent citstr=" J96-1002 ">
(4) then, the reordering model has the form p(cj,j?|fj1 , ei1, i, j) well-founded framework for directly modeling the probability p(cj,j?|fj1 , ei1, i, j) is maximum entropy (berger et al, 1996).<papid> J96-1002 </papid></citsent>
<aftsection>
<nextsent>in this framework, we have set of feature functions hn(fj1 , ei1, i, j, cj,j?), = 1, . . .
</nextsent>
<nextsent>,n . each feature function hn is weighted with factor n. the resulting model is: pn1 (cj,j?|f 1 , ei1, i, j) = exp ( ? n=1 nhn(fj1 , ei1, i, j, cj,j?) ) ? c? exp ( ? n=1 nhn(fj1 , ei1, i, j, c?)
</nextsent>
<nextsent>) (5) the functional form is identical to equation 2, but here we will use large number of binary features, whereas in equation 2 usually only very small number of real-valued features is used.
</nextsent>
<nextsent>more precisely, the resulting reordering model pn1 (cj,j?|f j1 , ei1, i, j) is used as an additional component in the log-linear combination of equation 2.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5100">
<title id=" W06-3108.xml">discriminative reordering models for statistical machine translation </title>
<section> the reordering model.  </section>
<citcontext>
<prevsection>
<prevsent>in practice, the training procedure tends to result in an over fitted model.
</prevsent>
<prevsent>to avoid over fitting, (chen and rosenfeld, 1999) have suggested smoothing method where gaussian prior distribution of the parameters is assumed.this method tried to avoid very large lambda values and prevents features that occur only once for specific class from getting value of infinity.
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
we train ibm model 4 with giza++ (och and ney, 2003) <papid> J03-1002 </papid>in both translation directions.</citsent>
<aftsection>
<nextsent>then the alignments are symmetrized using refined heuristic as described in (och and ney, 2003).<papid> J03-1002 </papid></nextsent>
<nextsent>this word aligned bilingual corpus is used to train the reordering model parameters, i.e. the feature weights n1 .each alignment link defines an event for the maximum entropy training.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5102">
<title id=" W06-3108.xml">discriminative reordering models for statistical machine translation </title>
<section> experimental results.  </section>
<citcontext>
<prevsection>
<prevsent>the statistics of the training and test alignment links is shown in table 2.
</prevsent>
<prevsent>the number of training events ranges from 119k for japanese english to 144k for arabic-english.
</prevsent>
</prevsection>
<citsent citstr=" E99-1010 ">
the word classes for the class-based features are trained using the mkcls tool (och, 1999).<papid> E99-1010 </papid></citsent>
<aftsection>
<nextsent>in the experiments, we use 50 word classes.
</nextsent>
<nextsent>alternatively, one could use part-of-speech information for this purpose.
</nextsent>
<nextsent>additional experiments were carried out on the large data track of the chinese-english nist task.the corpus statistics of the bilingual training corpus are shown in table 3.
</nextsent>
<nextsent>the language model was trained on the english part of the bilingual training corpus and additional monolingual english data from the gigaword corpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5103">
<title id=" W06-3108.xml">discriminative reordering models for statistical machine translation </title>
<section> experimental results.  </section>
<citcontext>
<prevsection>
<prevsent>again, using more features always helps to improve the performance.
</prevsent>
<prevsent>5.3 translation results.
</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
for the translation experiments on the btec task,we report the two accuracy measures bleu (pap ineni et al, 2002) <papid> P02-1040 </papid>and nist (doddington, 2002) as well as the two error rates: word error rate (wer) and position-independent word error rate (per).these criteria are computed with respect to 16 refer ences.</citsent>
<aftsection>
<nextsent>in table 6, we show the translation results forthe btec task.
</nextsent>
<nextsent>in these experiments, the reordering model uses two orientation classes, i.e. it predicts either left or right orientation.
</nextsent>
<nextsent>the features for the maximum-entropy based reordering model are based on the source and target language words within window of one.
</nextsent>
<nextsent>the word-classbased features are not used for the translation experiments.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5104">
<title id=" W06-3001.xml">contextual phenomena and thematic relations in database qa dialogues results from a wizardofoz experiment </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>considering data obtained from corpus of database qa dialogues, we address the nature of the discourse structure needed to resolve the several kinds of contextual phenomena found in our corpus.
</prevsent>
<prevsent>we look at the thematic relations holding between questions and the preceding context and discuss to which extent thematic relatedness plays role in discourse structure.
</prevsent>
</prevsection>
<citsent citstr=" W04-2509 ">
as pointed out by several authors (kato et al, 2004), (<papid> W04-2509 </papid>chai and ron, 2004), <papid> W04-2504 </papid>the information needs of users interacting with qa systems often go beyond single stand-alone question.</citsent>
<aftsection>
<nextsent>often users want to research about particular topic or event or solve specific task.
</nextsent>
<nextsent>in such interactions we can expect that the individual user questions will be thematically connected, giving the users the possibility of reusing part of the context when formulating new questions.that users implicitly refer to and even omit material which can be recovered from the context has already been replicated in several wizard-ofoz experiments simulating natural language interfaces to databases, (carbonell, 1983), (dahlback and jonsson, 1989), the most frequent contextual phenomena being ellipsis, anaphora and definite descriptions.
</nextsent>
<nextsent>a big challenge for interactive qa systems is,thus, the resolution of contextual phenomena.
</nextsent>
<nextsent>in order to be able to do so system has to keep track ofthe users focus of attention as the interaction proceeds.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5105">
<title id=" W06-3001.xml">contextual phenomena and thematic relations in database qa dialogues results from a wizardofoz experiment </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>considering data obtained from corpus of database qa dialogues, we address the nature of the discourse structure needed to resolve the several kinds of contextual phenomena found in our corpus.
</prevsent>
<prevsent>we look at the thematic relations holding between questions and the preceding context and discuss to which extent thematic relatedness plays role in discourse structure.
</prevsent>
</prevsection>
<citsent citstr=" W04-2504 ">
as pointed out by several authors (kato et al, 2004), (<papid> W04-2509 </papid>chai and ron, 2004), <papid> W04-2504 </papid>the information needs of users interacting with qa systems often go beyond single stand-alone question.</citsent>
<aftsection>
<nextsent>often users want to research about particular topic or event or solve specific task.
</nextsent>
<nextsent>in such interactions we can expect that the individual user questions will be thematically connected, giving the users the possibility of reusing part of the context when formulating new questions.that users implicitly refer to and even omit material which can be recovered from the context has already been replicated in several wizard-ofoz experiments simulating natural language interfaces to databases, (carbonell, 1983), (dahlback and jonsson, 1989), the most frequent contextual phenomena being ellipsis, anaphora and definite descriptions.
</nextsent>
<nextsent>a big challenge for interactive qa systems is,thus, the resolution of contextual phenomena.
</nextsent>
<nextsent>in order to be able to do so system has to keep track ofthe users focus of attention as the interaction proceeds.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5106">
<title id=" W06-3001.xml">contextual phenomena and thematic relations in database qa dialogues results from a wizardofoz experiment </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>an open issue, however, is the nature of the discourse structure model needed in qa system.
</prevsent>
<prevsent>ahrenberg et al (1990) argue that the discourse structure in nl interfaces is, given the limited set of actions to be performed by the system and the user, simpler than the one underlying human-human dialogue.
</prevsent>
</prevsection>
<citsent citstr=" J86-3001 ">
upon ahrenberg et al (1990) this is given by the discourse goals, rather than the overall goals of the user, as isthe case in task-oriented dialogues, (grosz and sid ner, 1986).<papid> J86-3001 </papid></citsent>
<aftsection>
<nextsent>following ahrenberg et al (1990), the qa discourse is structured in segments composed by pair of initiative-response units, like question answer, or question-assertion, in the absence of ananswer.
</nextsent>
<nextsent>a segment can be embedded in another segment if it is composed by clarification request and its corresponding answer.
</nextsent>
<nextsent>the local context of asegment is given by the immediately preceding segment.
</nextsent>
<nextsent>upon ahrenberg et al (1990), the latter reliably limits up the search space for antecedents of anaphoric devices and ellipsis.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5114">
<title id=" W06-2933.xml">labeled pseudo projective dependency parsing with support vector machines </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the conll-x shared task consists in parsing texts in multiple languages using single dependency parser that has the capacity to learn from treebank data.
</prevsent>
<prevsent>our methodology for performing this task is based on four essential components: ? deterministic algorithm for building labeled projective dependency graphs (nivre, 2006).
</prevsent>
</prevsection>
<citsent citstr=" H92-1026 ">
history-based feature models for predicting the next parser action (black et al, 1992).<papid> H92-1026 </papid></citsent>
<aftsection>
<nextsent>support vector machines for mapping histories to parser actions (kudo and matsumoto, 2002).?<papid> W02-2016 </papid></nextsent>
<nextsent>graph transformations for recovering non projective structures (nivre and nilsson, 2005).<papid> P05-1013 </papid>all experiments have been performed using malt parser (nivre et al, 2006), version 0.4, which is made available together with the suite of programs used for pre- and post-processing.1 1www.msi.vxu.se/users/nivre/research/maltparser.html</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5115">
<title id=" W06-2933.xml">labeled pseudo projective dependency parsing with support vector machines </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our methodology for performing this task is based on four essential components: ? deterministic algorithm for building labeled projective dependency graphs (nivre, 2006).
</prevsent>
<prevsent>history-based feature models for predicting the next parser action (black et al, 1992).<papid> H92-1026 </papid></prevsent>
</prevsection>
<citsent citstr=" W02-2016 ">
support vector machines for mapping histories to parser actions (kudo and matsumoto, 2002).?<papid> W02-2016 </papid></citsent>
<aftsection>
<nextsent>graph transformations for recovering non projective structures (nivre and nilsson, 2005).<papid> P05-1013 </papid>all experiments have been performed using malt parser (nivre et al, 2006), version 0.4, which is made available together with the suite of programs used for pre- and post-processing.1 1www.msi.vxu.se/users/nivre/research/maltparser.html</nextsent>
<nextsent>2.1 parsing algorithm.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5116">
<title id=" W06-2933.xml">labeled pseudo projective dependency parsing with support vector machines </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>history-based feature models for predicting the next parser action (black et al, 1992).<papid> H92-1026 </papid></prevsent>
<prevsent>support vector machines for mapping histories to parser actions (kudo and matsumoto, 2002).?<papid> W02-2016 </papid></prevsent>
</prevsection>
<citsent citstr=" P05-1013 ">
graph transformations for recovering non projective structures (nivre and nilsson, 2005).<papid> P05-1013 </papid>all experiments have been performed using malt parser (nivre et al, 2006), version 0.4, which is made available together with the suite of programs used for pre- and post-processing.1 1www.msi.vxu.se/users/nivre/research/maltparser.html</citsent>
<aftsection>
<nextsent>2.1 parsing algorithm.
</nextsent>
<nextsent>the parsing algorithm used for all languages is the deterministic algorithm first proposed for unlabeled dependency parsing by nivre (2003) and extended to labeled dependency parsing by nivre et al (2004).<papid> W04-2407 </papid></nextsent>
<nextsent>the algorithm builds labeled dependency graph in one left-to-right pass over the input, using stack to store partially processed tokens and adding arcs using four elementary actions (where top is the token on top of the stack and next is the next token): ? shift: push next onto the stack.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5117">
<title id=" W06-2933.xml">labeled pseudo projective dependency parsing with support vector machines </title>
<section> parsing methodology.  </section>
<citcontext>
<prevsection>
<prevsent>graph transformations for recovering non projective structures (nivre and nilsson, 2005).<papid> P05-1013 </papid>all experiments have been performed using malt parser (nivre et al, 2006), version 0.4, which is made available together with the suite of programs used for pre- and post-processing.1 1www.msi.vxu.se/users/nivre/research/maltparser.html</prevsent>
<prevsent>2.1 parsing algorithm.</prevsent>
</prevsection>
<citsent citstr=" W04-2407 ">
the parsing algorithm used for all languages is the deterministic algorithm first proposed for unlabeled dependency parsing by nivre (2003) and extended to labeled dependency parsing by nivre et al (2004).<papid> W04-2407 </papid></citsent>
<aftsection>
<nextsent>the algorithm builds labeled dependency graph in one left-to-right pass over the input, using stack to store partially processed tokens and adding arcs using four elementary actions (where top is the token on top of the stack and next is the next token): ? shift: push next onto the stack.
</nextsent>
<nextsent>reduce: pop the stack.
</nextsent>
<nextsent>right-arc(r): add an arc labeled from top to next; push next onto the stack.
</nextsent>
<nextsent>left-arc(r): add an arc labeled from next to top; pop the stack.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5122">
<title id=" W06-3407.xml">topic segmentation of dialogue </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, most previous work on automatic topic segmentation has focused primarily on segmentation of expository text.
</prevsent>
<prevsent>this paper presents survey of the state-of-the-art in topic segmentation technology.
</prevsent>
</prevsection>
<citsent citstr=" P93-1020 ">
using the definition of topic segment from (pas sonneau and litman, 1993) <papid> P93-1020 </papid>applied to two different dialogue corpora, we present an evaluation including detailed error analysis, illustrating why approaches designed for expository text do not generalize well to dialogue.</citsent>
<aftsection>
<nextsent>we first demonstrate significant advantage of our hybrid, supervised learning approach called museli, multi-source evidence integration approach, over competing algorithms.
</nextsent>
<nextsent>we then extend the basic museli algorithm by introducing an intermediate level of analysis based on sinclair and coulthards notion of dialogue exchange (sin clair and coulthard, 1975).
</nextsent>
<nextsent>we show that both our baseline and museli approaches obtain significant improvement when using perfect, hand labeled dialogue exchanges, typically in the order of 2-3 contributions, as the atomic discourse unit in comparison to using the contribution as the unit of analysis.
</nextsent>
<nextsent>we further evaluate our success towards automatic classification of exchange boundaries using the same museli framework.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5124">
<title id=" W06-3407.xml">topic segmentation of dialogue </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>existing topic segmentation approaches can be loosely classified into two types: (1) lexical cohesion models, and (2) content-oriented models.
</prevsent>
<prevsent>the underlying assumption in lexical cohesion models is that shift in term distribution signals shift in topic (halliday and hassan, 1976).
</prevsent>
</prevsection>
<citsent citstr=" J97-1003 ">
the best known algorithm based on this idea is text tiling (hearst, 1997).<papid> J97-1003 </papid></citsent>
<aftsection>
<nextsent>in text tiling, sliding window is passed over the vector-space representation of the text.
</nextsent>
<nextsent>at each position, the cosine correlation between the upper and lower regions of the sliding window is compared with that of the peak cosine correlation values to the left and right of the window.
</nextsent>
<nextsent>a segment boundary is predicted when the magnitude of the difference exceeds threshold.
</nextsent>
<nextsent>one drawback to relying on term co-occurrence to signal topic continuity is that synonyms or related terms are treated as thematically-unrelated.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5125">
<title id=" W06-3407.xml">topic segmentation of dialogue </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>one drawback to relying on term co-occurrence to signal topic continuity is that synonyms or related terms are treated as thematically-unrelated.
</prevsent>
<prevsent>one proposed solution to this problem is latent semantic analysis (lsa) (landauer and dumais, 1997).
</prevsent>
</prevsection>
<citsent citstr=" H05-1122 ">
two lsa-based algorithms for segmentation are described in (foltz, 1998) and (olney and cai, 2005).<papid> H05-1122 </papid></citsent>
<aftsection>
<nextsent>foltzs approach differs from text tiling mainly in its use of an lsa-based vector space model.
</nextsent>
<nextsent>olney and cai address problem not addressed by text tiling or foltzs approach, which is that cohesion is not just function of the repetition of thematically-related terms, but also function of the presentation of new information in reference to information already presented.
</nextsent>
<nextsent>their ortho normal basis approach allows for segmentation based on relevance and informativity.
</nextsent>
<nextsent>content-oriented models, such as (barzilay and lee, 2004), <papid> N04-1015 </papid>relyon the re-occurrence of patterns of topics over multiple realizations of thematically similar discourses, such as series of newspaper articles about similar events.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5128">
<title id=" W06-3407.xml">topic segmentation of dialogue </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>olney and cai address problem not addressed by text tiling or foltzs approach, which is that cohesion is not just function of the repetition of thematically-related terms, but also function of the presentation of new information in reference to information already presented.
</prevsent>
<prevsent>their ortho normal basis approach allows for segmentation based on relevance and informativity.
</prevsent>
</prevsection>
<citsent citstr=" N04-1015 ">
content-oriented models, such as (barzilay and lee, 2004), <papid> N04-1015 </papid>relyon the re-occurrence of patterns of topics over multiple realizations of thematically similar discourses, such as series of newspaper articles about similar events.</citsent>
<aftsection>
<nextsent>their approach utilizes hidden markov model where states correspond to topics and state transition probabilities correspond to topic shifts.
</nextsent>
<nextsent>to obtain the desired number of topics (states), text spans of uniform length (individual contributions, in our case) are clustered.
</nextsent>
<nextsent>then, state emission probabilities are induced using smoothed cluster-specific language models.
</nextsent>
<nextsent>transition probabilities are induced by considering the proportion of documents in which contribution assigned to the source cluster (state) immediately precedes contribution assigned to the target cluster (state).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5142">
<title id=" W06-1505.xml">a tree adjoining grammar analysis of the syntax and semantics of itc lefts </title>
<section> extensions.  </section>
<citcontext>
<prevsection>
<prevsent>node in (who won) has the effect of stretching the dependency between the relative pronoun who and its gap in the cleft clause.
</prevsent>
<prevsent>the derivation and the derived trees for (14) are given in figure 10.
</prevsent>
</prevsection>
<citsent citstr=" C96-2103 ">
to handle the coordination of the constituent containing the clefted constituent and the cleft clause, as illustrated in (15), we propose to use node contraction and conjoin proposed in sarkar and joshi (1996).<papid> C96-2103 </papid></citsent>
<aftsection>
<nextsent>informally, node contraction takes two nodes of like categories and collapses them into single node, and conjoin coordinates the least nodes dominating the two contiguous strings.
</nextsent>
<nextsent>we use the conjunction tree in figure 11 to apply conjoin at fp.figure 12 contains the elementary tree anchoring equative was.
</nextsent>
<nextsent>we mark the nodes to be contracted with box, and augment the name of the elementary tree with set listing these contraction nodes.
</nextsent>
<nextsent>thus, (was){dp i,t,cop} means that dpi, and cop nodes are marked for contraction in (was) elementary tree.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5143">
<title id=" W06-1510.xml">quantifier scope in german an mctag analysis </title>
<section> english quantifier scope in ltag.  </section>
<citcontext>
<prevsection>
<prevsent>the equations of top and bottom features linked to specific node positions in the elementary trees are parallel to the syntactic unifications in ftag(vijay-shanker and joshi, 1988).
</prevsent>
<prevsent>the global features that are not linked to specific nodes can be passed from mothers to daughters and vice versa in the derivation tree.
</prevsent>
</prevsection>
<citsent citstr=" J05-2003 ">
(6) everybody laughs.as sample derivation let us sketch the analysis of quantificational nps in english from(kallmeyer, 2005).<papid> J05-2003 </papid></citsent>
<aftsection>
<nextsent>fig.
</nextsent>
<nextsent>1 shows the ltag analysis of (6).
</nextsent>
<nextsent>more precisely, it shows the derivation tree with the semantic representations and feature structure descriptions of laughs and every body as node labels.
</nextsent>
<nextsent>the feature identifications are depicted by dotted lines.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5145">
<title id=" W07-0307.xml">experiments on the france telecom 3000 voice agency corpus academic research on an industrial spoken dialog system </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these new sds can be deployed on very large scale, like the france telecom 3000 voice agency service considered in this study.
</prevsent>
<prevsent>with these service sit is possible to obtain very large corpora of human machine interactions by collecting system logs.
</prevsent>
</prevsection>
<citsent citstr=" H90-1021 ">
the main differences between these corpora and those collected in the framework of evaluation programs like the darpa atis (hemphill et al, 1990) <papid> H90-1021 </papid>or the french technolangue media (bonneau-maynard et al, 2005) programs can be expressed through the following dimensions: ? size.</citsent>
<aftsection>
<nextsent>there are virtually no limits in the amount of speakers available or the time needed for collecting the dialogues as thousands of dialogues are automatically processed every day and the system logs are stored.
</nextsent>
<nextsent>therefore dialog processing becomes similar 48 to broadcast news processing: the limit is not in the amount of data available, but rather in the amount of data that can be manually annotated.?
</nextsent>
<nextsent>speakers.
</nextsent>
<nextsent>data are from real users.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5146">
<title id=" W07-0307.xml">experiments on the france telecom 3000 voice agency corpus academic research on an industrial spoken dialog system </title>
<section> optimizing the asr and slu processes.  </section>
<citcontext>
<prevsection>
<prevsent>p (?|c)p (c|w )p (w |y ) the stochastic models proposed are implemented with finite state machine (fsm) paradigm thanks to the at&t; fsm toolkit (mohri et al, 2002).
</prevsent>
<prevsent>following the approach described in (raymond et al, 2006), the slu first stage is implemented bymeans of word-to-concept transducer that translates word lattice into concept lattice.
</prevsent>
</prevsection>
<citsent citstr=" P03-1006 ">
this concept lattice is rescored with language model on the concepts (also encoded as fsms with the at&t; grm toolkit (allauzen et al, 2003)).<papid> P03-1006 </papid>the rule database of the slu second stage is encoded as transducer that takes as input concepts and output semantic interpretations ?.</citsent>
<aftsection>
<nextsent>by applying this transducer to an fsm representing concept lattice, we directly obtain lattice of interpretations.the slu process is therefore made of the composition of the asr word lattice, two transducers (word-to-concepts and concept-to-interpretations) and an fsm representing language model on the concepts.
</nextsent>
<nextsent>the concept lm is trained on the ft3000 corpus.this strategy push forward the approach devel opped at at&t; in the how may help you?
</nextsent>
<nextsent>(gorinet al, 1997) project by using richer semantic models than call-types and named-entities models.
</nextsent>
<nextsent>more precisely, the 1600 verbateam interpretation rules used in this study constitute rich knowledge base.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5147">
<title id=" W06-2702.xml">annotation and disambiguation of semantic types in biomedical text a cascaded approach to named entity recognition </title>
<section> pubmed, http://www.pubmed.org.  </section>
<citcontext>
<prevsection>
<prevsent>in such situations, disambiguation on the resource level is needed (see, for example, (liu et al, 2002) for disambiguation of terms associated with several entries in the umls meta thesaurus).in many solutions, the three steps in biomedical ner (namely, recognition, categorisation and mapping to databases) are merged within one module.
</prevsent>
<prevsent>for example, using an existing terminological database for recognition of nes, effectively leads to complete term identification (in cases where there are no ambiguities).
</prevsent>
</prevsection>
<citsent citstr=" W03-1315 ">
some researchers, however, have stressed the advantages of tackling each step as separate task, pointing at different sources and methods needed to accomplish each of the subtasks (torii et al, 2003; <papid> W03-1315 </papid>lee et al, 2003).<papid> W03-1305 </papid></citsent>
<aftsection>
<nextsent>also, in the case of modular isa tion, it is easier to integrate different solutions for each specific problem.
</nextsent>
<nextsent>however, it has been suggested that whether clear separation into single steps would improve term identification is an open issue (krauthammer and nenadic, 2004).
</nextsent>
<nextsent>in this paper we discuss cascaded, modular approach to biomedical ner.
</nextsent>
<nextsent>notation: modules in pipeline in this section we present modular approach to identification, disambiguation and annotation of 8 uniprot, http://www.ebi.uniprot.org/, (bairoch et al,.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5148">
<title id=" W06-2702.xml">annotation and disambiguation of semantic types in biomedical text a cascaded approach to named entity recognition </title>
<section> pubmed, http://www.pubmed.org.  </section>
<citcontext>
<prevsection>
<prevsent>in such situations, disambiguation on the resource level is needed (see, for example, (liu et al, 2002) for disambiguation of terms associated with several entries in the umls meta thesaurus).in many solutions, the three steps in biomedical ner (namely, recognition, categorisation and mapping to databases) are merged within one module.
</prevsent>
<prevsent>for example, using an existing terminological database for recognition of nes, effectively leads to complete term identification (in cases where there are no ambiguities).
</prevsent>
</prevsection>
<citsent citstr=" W03-1305 ">
some researchers, however, have stressed the advantages of tackling each step as separate task, pointing at different sources and methods needed to accomplish each of the subtasks (torii et al, 2003; <papid> W03-1315 </papid>lee et al, 2003).<papid> W03-1305 </papid></citsent>
<aftsection>
<nextsent>also, in the case of modular isa tion, it is easier to integrate different solutions for each specific problem.
</nextsent>
<nextsent>however, it has been suggested that whether clear separation into single steps would improve term identification is an open issue (krauthammer and nenadic, 2004).
</nextsent>
<nextsent>in this paper we discuss cascaded, modular approach to biomedical ner.
</nextsent>
<nextsent>notation: modules in pipeline in this section we present modular approach to identification, disambiguation and annotation of 8 uniprot, http://www.ebi.uniprot.org/, (bairoch et al,.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5149">
<title id=" W06-3120.xml">talp phrase based statistical translation system for european language pairs </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>this transformation is only done for the alignment, and its goalis to simplify the work of the word alignment improving its quality.
</prevsent>
<prevsent>block reordering (br) the difference in word order between two languages is one of the most significant sources of error in smt.
</prevsent>
</prevsection>
<citsent citstr=" W05-0831 ">
related works either deal with reordering in general as (kanthaket al , 2005) <papid> W05-0831 </papid>or deal with local reordering as (tillmann and ney, 2003).<papid> J03-1005 </papid></citsent>
<aftsection>
<nextsent>we report local reordering technique, which is implemented as preprocessing stage, with two applications: (1) to im prove only alignment quality, and (2) to improve alignment quality and to infer reordering in translation.
</nextsent>
<nextsent>here, we present short explanation of the algorithm, for further details see costa-jussa` and fonollosa (2006).
</nextsent>
<nextsent>142 figure 1: example of an alignment block, i.e. pair of consecutive blocks whose target translation is swapped this reordering strategy is intended to infer the most probable reordering for sequences of words,which are referred to as blocks, in order to monotonize current data alignments and generalize reordering for unseen pairs of blocks.
</nextsent>
<nextsent>given word alignment, we identify those pairs of consecutive source blocks whose translation is swapped, i.e. those blocks which, if swapped, generate correct monotone translation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5150">
<title id=" W06-3120.xml">talp phrase based statistical translation system for european language pairs </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>this transformation is only done for the alignment, and its goalis to simplify the work of the word alignment improving its quality.
</prevsent>
<prevsent>block reordering (br) the difference in word order between two languages is one of the most significant sources of error in smt.
</prevsent>
</prevsection>
<citsent citstr=" J03-1005 ">
related works either deal with reordering in general as (kanthaket al , 2005) <papid> W05-0831 </papid>or deal with local reordering as (tillmann and ney, 2003).<papid> J03-1005 </papid></citsent>
<aftsection>
<nextsent>we report local reordering technique, which is implemented as preprocessing stage, with two applications: (1) to im prove only alignment quality, and (2) to improve alignment quality and to infer reordering in translation.
</nextsent>
<nextsent>here, we present short explanation of the algorithm, for further details see costa-jussa` and fonollosa (2006).
</nextsent>
<nextsent>142 figure 1: example of an alignment block, i.e. pair of consecutive blocks whose target translation is swapped this reordering strategy is intended to infer the most probable reordering for sequences of words,which are referred to as blocks, in order to monotonize current data alignments and generalize reordering for unseen pairs of blocks.
</nextsent>
<nextsent>given word alignment, we identify those pairs of consecutive source blocks whose translation is swapped, i.e. those blocks which, if swapped, generate correct monotone translation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5151">
<title id=" W06-3120.xml">talp phrase based statistical translation system for european language pairs </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>in case of using block reordering for purpose (2), we modify all the source corpora (both training and test), and we use the new training corpora to realign and build the final translation system.
</prevsent>
<prevsent>2.2 phrase extraction.
</prevsent>
</prevsection>
<citsent citstr=" J04-4002 ">
given sentence pair and corresponding word alignment, phrases are extracted following the criterion in och and ney (2004).<papid> J04-4002 </papid></citsent>
<aftsection>
<nextsent>a phrase (or bilingual phrase) is any pair of source word sand target words that satisfies two basic con straints: words are consecutive along both sides of the bilingual phrase, and no word on either side of the phrase is aligned to word out of the phrase.
</nextsent>
<nextsent>we limit the maximum size of any given phrase to 7.
</nextsent>
<nextsent>the huge increase in computational and storage.
</nextsent>
<nextsent>cost of including longer phrases does not provide significant improvement in quality (koehn et al , 2003) <papid> N03-1017 </papid>as the probability of reappearance of larger phrases decreases.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5152">
<title id=" W06-3120.xml">talp phrase based statistical translation system for european language pairs </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>we limit the maximum size of any given phrase to 7.
</prevsent>
<prevsent>the huge increase in computational and storage.
</prevsent>
</prevsection>
<citsent citstr=" N03-1017 ">
cost of including longer phrases does not provide significant improvement in quality (koehn et al , 2003) <papid> N03-1017 </papid>as the probability of reappearance of larger phrases decreases.</citsent>
<aftsection>
<nextsent>2.3 feature functions.
</nextsent>
<nextsent>conditional and posterior probability (cp, pp) given the collected phrase pairs, we estimate the phrase translation probability distribution by relative frequency in both directions.
</nextsent>
<nextsent>the target language model (lm) consists of ann-gram model, in which the probability of translation hypothesis is approximated by the product of word n-gram probabilities.
</nextsent>
<nextsent>as default language model feature, we use standard word-based 5 gram language model generated with kneser-ney smoothing and interpolation of higher and lower order n-grams (stolcke, 2002).the pos target language model (tpos) consists of an n-gram language model estimated overthe same target-side of the training corpus but using pos tags instead of raw words.the forward and backwards lexicon models (ibm1, ibm11) provide lexicon translation probabilities for each phrase based on the word ibm model 1 probabilities.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5153">
<title id=" W06-3120.xml">talp phrase based statistical translation system for european language pairs </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>the search engine for this translation system is described in crego et al  (2005) which takes into account the features described above.
</prevsent>
<prevsent>using reordering in the decoder (rgraph) highly constrained reordered search is performed by means of set of reordering patterns (linguisti cally motivated rewrite patterns) which are used to 143 extend the monotone search graph with additional arcs.
</prevsent>
</prevsection>
<citsent citstr=" W06-3125 ">
see the details in crego et al  (2006).<papid> W06-3125 </papid></citsent>
<aftsection>
<nextsent>2.5 optimization.
</nextsent>
<nextsent>it is based on simplex method (nelder andmead, 1965).
</nextsent>
<nextsent>this algorithm adjusts the loglinear weights in order to maximize non-linear combination of translation bleu and nist: 10 log10((bleu ? 100) + 1) + nist.
</nextsent>
<nextsent>the maximization is done over the provided development set for each of the six translation directions underconsideration.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5154">
<title id=" W06-3120.xml">talp phrase based statistical translation system for european language pairs </title>
<section> shared task results.  </section>
<citcontext>
<prevsection>
<prevsent>the development set used to tune the system consists of subset (500 first sentences) of the official development set made available for the shared task.
</prevsent>
<prevsent>we carried out morphological analysis of the data.
</prevsent>
</prevsection>
<citsent citstr=" A00-1031 ">
the english pos-tagging has been carried out using freely available tnt tagger (brants, 2000).<papid> A00-1031 </papid></citsent>
<aftsection>
<nextsent>in the spanish case, we have used the free ling (carreras et al , 2004) analysis tool which generates the pos-tagging for each input word.
</nextsent>
<nextsent>3.2 systems configurations.
</nextsent>
<nextsent>the baseline system is the same for all tasks and includes the following features functions: cp, pp, lm, ibm1, ibm11, wb, pb.
</nextsent>
<nextsent>the postag target language model has been used in those tasks for which the tagger was available.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5155">
<title id=" W06-3120.xml">talp phrase based statistical translation system for european language pairs </title>
<section> shared task results.  </section>
<citcontext>
<prevsection>
<prevsent>3.3 discussion.
</prevsent>
<prevsent>table 2 presents the bleu scores evaluated on the test set (using truecase) for each configuration.
</prevsent>
</prevsection>
<citsent citstr=" W06-3114 ">
the official results were slightly better because lowercase evaluation was used, see (koehn and monz, 2006).<papid> W06-3114 </papid></citsent>
<aftsection>
<nextsent>for both, es2en and fr2en tasks, br helpsslightly.
</nextsent>
<nextsent>the improvement of the approach depends on the quality of the alignment.
</nextsent>
<nextsent>the better alignments allow to extract higher quality alignment blocks (costa-jussa` and fonollosa, 2006).
</nextsent>
<nextsent>the en2es task is improved when adding both br1 and rgraph.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5157">
<title id=" W06-2801.xml">text linkage in the wiki medium  a comparative study </title>
<section> a web genre structure model </section>
<citcontext>
<prevsection>
<prevsent>in analogy to the weak contextual hypothesis (miller and charles, 1991) one might state that structural differences reflect functional ones as far as they are confirmed by significantly high number of textual units and thus are identifiable as recurrent pat 2 terns.
</prevsent>
<prevsent>in this sense, we expect web documents to be distinguishable by the functional structures they manifest.
</prevsent>
</prevsection>
<citsent citstr=" J03-2003 ">
more specifically, we agree with the notion of web genre (yoshioka and herman, 2000) according to which the functional structure of web documents is determined by their membership in genres (e.g. of conference web sites, personal home pages or electronic encyclopedias).our hypothesis is that what is common to instances of different web genres is the existence of an implicit logical document structure (lds) ? in analogy to textual units whose lds is described in terms of section, paragraph and sentence categories (power et al, 2003).<papid> J03-2003 </papid></citsent>
<aftsection>
<nextsent>in the case of web documents we hypothesize that their lds comprises four levels: ? document networks consist of documents which serve possibly heterogenous functions if necessary independently of each other.
</nextsent>
<nextsent>a web document network is given, for example, by the system of web sites of university.
</nextsent>
<nextsent>web documents manifest ? typically in the form of web sites ? pragmatically closed actsof web-based communication (e.g. conference organization or online presentation).each web document is seen to organize system of dependent sub functions which in turn are manifested by modules.
</nextsent>
<nextsent>document modules are, ideally, functionally homogeneous subunits of web documents which manifest single, but dependent sub functions in the sense that their realization is bound to the realization of other subfunctionsmanifested by the same encompassing document.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5158">
<title id=" W06-1626.xml">distributed language modeling for nbest list reranking </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we applied this model on 2.97 billion-word corpus and re-rankedthe -best list from hiero, state-of-the art phrase-based system.
</prevsent>
<prevsent>using bleu as metric, the re-ranked translation achievesa relative improvement of 4.8%, significantly better than the model-best translation.
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
statistical language modeling has been widely used in natural language processing applications such as automatic speech recognition (asr), statistical machine translation (smt) (brown et al., 1993) <papid> J93-2003 </papid>and information retrieval (ir) (ponte and croft, 1998).</citsent>
<aftsection>
<nextsent>conventional n-gram language modeling counts the frequency of all the n-grams in corpus and calculates the conditional probabilities of word given its history of ? 1 words (wi|wi1in+1).
</nextsent>
<nextsent>as the corpus size increases, building high order language model offline becomes very expensive if it is still possible (goodman, 2000).
</nextsent>
<nextsent>in this paper, we describe new approach of language modeling using distributed computing paradigm.
</nextsent>
<nextsent>distributed language modeling can make use of arbitrarily large training corpora and provides natural way for language model adap tation.we applied the distributed lm to the task of reranking the -best list in statistical machine translation and achieved significantly better translation quality when measured by the bleu metric (pap ineni et al, 2001).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5159">
<title id=" W06-1626.xml">distributed language modeling for nbest list reranking </title>
<section> n -best list re-ranking </section>
<citcontext>
<prevsection>
<prevsent>distributed language modeling can make use of arbitrarily large training corpora and provides natural way for language model adap tation.we applied the distributed lm to the task of reranking the -best list in statistical machine translation and achieved significantly better translation quality when measured by the bleu metric (pap ineni et al, 2001).
</prevsent>
<prevsent>when translating source language sentence finto english, the smt decoder first builds translation lattice over the source words by applying the translation model and then explores the lattice and searches for an optimal path as the best translation.
</prevsent>
</prevsection>
<citsent citstr=" N04-1021 ">
the decoder uses different models, such as the translation model, n-gram language model, fertility model, and combines multiple model scores to calculate the objective function value which favors one translation hypothesis over the other (och et al., 2004).<papid> N04-1021 </papid></citsent>
<aftsection>
<nextsent>instead of out putting the top hypothesis e(1)based on the decoder model, the decoder can out put (usually = 1000) alternative hypotheses {e(r)|r = 1, . . .
</nextsent>
<nextsent>, n} for one source sentence and rank them according to their model scores.
</nextsent>
<nextsent>figure 1 shows an example of the output from asmt system.
</nextsent>
<nextsent>in this example, alternative hypothesis e(2) is better translations than e(1) according to the reference (ref) although its model score is lower.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5160">
<title id=" W06-1626.xml">distributed language modeling for nbest list reranking </title>
<section> n -best list re-ranking </section>
<citcontext>
<prevsection>
<prevsent>the one with the minimal pmi is the most natural?
</prevsent>
<prevsent>cut for this n-gram.
</prevsent>
</prevsection>
<citsent citstr=" J01-1001 ">
the pmi over the natural cut quantifies the non-compositionality inc of an n-gram wji . the higher the value of inc(wji ) the more likely wji is meaningful constituent, in other words, it is less likely that wji is composed from two short n-grams just by chance (ya mamoto and church, 2001).<papid> J01-1001 </papid></citsent>
<aftsection>
<nextsent>define l2 formally as: l2(wm1 ) = ? i,j ij inc(wji ) (7) 217 inc(wji ) = ? ?
</nextsent>
<nextsent>min i(wki ;wjk+1) : c(wji )   0 0 : c(wji ) = 0 (8) i(wki ;wjk+1) = log (wji ) (wki )p (wjk+1) (9)
</nextsent>
<nextsent>the fundamental information required to calculate the likelihood of sentence is the frequency of ngrams in the corpus.
</nextsent>
<nextsent>in conventional lm training, all the counts are collected from the corpus and saved to disk for probability estimation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5161">
<title id=" W06-1626.xml">distributed language modeling for nbest list reranking </title>
<section> distributed language model.  </section>
<citcontext>
<prevsection>
<prevsent>from the returned n-gram matching information, client calculates r(dd, ft) for each server, and choose the most relevant (e.g., 20) servers for ft. the n-gram counts returned from these relevant servers are summed up for calculating the likelihood of ft. one could also assign weights to the gram counts returned from different servers during the summation so that the relevant data has more impact than the less-relevant ones.
</prevsent>
<prevsent>5 experiments.
</prevsent>
</prevsection>
<citsent citstr=" P05-1033 ">
we used the -best list generated by the hierosmt system (chiang, 2005).<papid> P05-1033 </papid></citsent>
<aftsection>
<nextsent>hiero is statistical phrase-based translation model that uses hierarchical phrases.
</nextsent>
<nextsent>the decoder uses trigram language model trained with modified kneser-neysmoothing (kneser and ney, 1995) on 200 million words corpus.
</nextsent>
<nextsent>the 1000-best list was generated on 919 sentences from the mt03 chinese english evaluation set.
</nextsent>
<nextsent>all the data from the english gigaword corpus plus the english side of the chinese-english bilingual data available from ldc are used.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5162">
<title id=" W06-1626.xml">distributed language modeling for nbest list reranking </title>
<section> distributed language model.  </section>
<citcontext>
<prevsection>
<prevsent>one client sends each english hypothesis translations to all 150 servers and uses the returned information to re-rank.
</prevsent>
<prevsent>the whole process takes about 600 seconds to finish.we use bleu scores to measure the translation accuracy.
</prevsent>
</prevsection>
<citsent citstr=" W04-3250 ">
a bootstrapping method is used to calculate the 95% confidence intervals for bleu (koehn, 2004; <papid> W04-3250 </papid>zhang and vogel, 2004).</citsent>
<aftsection>
<nextsent>5.1 oracle score of the -best list.
</nextsent>
<nextsent>because of the spurious ambiguity, there are only 24,612 unique hypotheses in the 1000-best list, on average 27 per source sentence.
</nextsent>
<nextsent>this limits the potential of -best re-ranking.
</nextsent>
<nextsent>spurious ambiguity is created by the decoder where two hypotheses generated from different decoding path are considered different even though they have identical word sequences.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5163">
<title id=" W06-1626.xml">distributed language modeling for nbest list reranking </title>
<section> distributed language model.  </section>
<citcontext>
<prevsection>
<prevsent>the oracle best translations are created by selecting the hypothesis which has the highest sentence bleu score for each source sentence.
</prevsent>
<prevsent>yet critical problem with bleu score is that it is function of the entire test set and does not give meaningful scores for single sentences.
</prevsent>
</prevsection>
<citsent citstr=" P05-1066 ">
we followed the approximation described in (collins et al, 2005) <papid> P05-1066 </papid>to get around this problem.</citsent>
<aftsection>
<nextsent>given test set with sentences, hypotheses are generated for each source sentence ft. denote e(r)t as the r-th ranked hypothesis for ft. e(1)t is the model-best hypothesis for this sentence.
</nextsent>
<nextsent>the baseline bleu scores are calculated based on the model-best translation set {e(1)t |t = 1, . . .
</nextsent>
<nextsent>, t}.
</nextsent>
<nextsent>define the bleu sentence-level gain for e(r)t as: gbleue(r)t = bleu{e(1)1 , e(1)2 , . . .
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5165">
<title id=" W06-1626.xml">distributed language modeling for nbest list reranking </title>
<section> distributed language model.  </section>
<citcontext>
<prevsection>
<prevsent>matched sentences from all chunks are then used together as possible translations.
</prevsent>
<prevsent>the assumption of this work that possible translations of source sentence can be found as exact match in big monolingual corpus is weak even for very large corpus.
</prevsent>
</prevsection>
<citsent citstr=" W05-0821 ">
this method can easily fail to find any possible translation and return zero proposed translations.kirchhoff and yang (2005) <papid> W05-0821 </papid>used factored 3 gram model and 4-gram lm (modified kn smoothing) together with seven system scores to re-rank an smt -best.</citsent>
<aftsection>
<nextsent>they improved the translation quality of their best baseline (spanish 221 # of relevant chunks per.
</nextsent>
<nextsent>sent 1 2 5 10 20 150 3-gram kn 32.22 32.08 4-gram kn 32.22 32.53 l0 32.27 32.38 32.40 32.47 32.51 32.48 l31 32.00 32.14 32.14 32.15 32.16 l41 32.18 32.36 32.28 32.44 32.41 l51 32.21 32.33 32.35 32.41 32.37 l61 32.19 32.22 32.37 32.45 32.40 32.41 l71 32.22 32.29 32.37 32.44 32.40 l2 32.29 32.52 32.61 32.55 32.64 32.56 table 2: bleu scores of the re-ranked translations.
</nextsent>
<nextsent>baseline score = 31.44 english) from bleu 30.5 to bleu 31.0.
</nextsent>
<nextsent>iyer and ostendorf (1999) select and weight data to train language modeling for asr.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5166">
<title id=" W06-3809.xml">random walk term weighting for improved text classification </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we model this local contribution using co-occurrence relation in which terms that co-occur in certain context are likely to share between them some of their importance (or significance).
</prevsent>
<prevsent>note that in this model the relation between given term and its context is not linear, since the context itself consists of collection of other terms, which in turn have dependency relation with their own context, which might include the original given term.
</prevsent>
</prevsection>
<citsent citstr=" W04-3252 ">
in order to model this recursive relation we use graph-basedranking algorithm, namely the page rank random walk algorithms (brin and page, 1998), and its text rank adaption to text processing applications (mihalcea and tarau, 2004).<papid> W04-3252 </papid></citsent>
<aftsection>
<nextsent>text rank takes as in put set of textual entities and relations between them, and uses graph-based ranking algorithm (also known as random walk algorithm) to produce set of scores that represent the accumulated weight or rank for each textual entity in their context.
</nextsent>
<nextsent>thetextrank model was so far evaluated on three natural language processing tasks: document summarization, word sense disambiguation, and keyword extraction, and despite being fully unsupervised, ithas been shown to be competitive with other some time supervised state-of-the-art algorithms.
</nextsent>
<nextsent>in this paper, we show how text rank can be used to model the probabilistic distribution of word features in document, by making further use of the scores produced by the random-walk model.
</nextsent>
<nextsent>53through experiments performed on text classification task, we show that these random walk scores outperform the traditional term frequencies typically used to model the feature weights for this task.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5168">
<title id=" W06-3809.xml">random walk term weighting for improved text classification </title>
<section> graph-based ranking algorithms.  </section>
<citcontext>
<prevsection>
<prevsent>or rec ommendation?.
</prevsent>
<prevsent>when one vertex links to another one, it is basically casting vote for that other vertex. the higher the number of votes that are castfor vertex, the higher the importance of the vertex. moreover, the importance of the vertex casting vote determines how important the vote itself is, and this information is also taken into account by the ranking algorithm.
</prevsent>
</prevsection>
<citsent citstr=" W04-3247 ">
hence, the score associated with vertex is determined based on the votes that are cast for it, and the scores of the vertices casting these votes.while there are several graph-based ranking algorithms previously proposed in the literature (herings et al, 2001), we focus on only one such algorithm, namely page rank (brin and page, 1998), as it was previously found successful in number of applications, including web link analysis (brin and page, 1998), social networks (dom et al, 2003), citation analysis, and more recently in several text processing applications (mihalcea and tarau, 2004), (<papid> W04-3252 </papid>erkan and radev, 2004).<papid> W04-3247 </papid></citsent>
<aftsection>
<nextsent>given graph = (v,e), let in(va) be the set of vertices that point to vertex va (predecessors), and let out(va) be the set of vertices that vertex vapoints to (successors).
</nextsent>
<nextsent>the page rank score associated with the vertex va is then defined using recursive function that integrates the scores of its prede cessors: s(va) = (1 ? d) + ? ?
</nextsent>
<nextsent>vbin(va) s(vb) |out(vb)| (1) where is parameter that is set between 0 and 11.
</nextsent>
<nextsent>the score of each vertex is recalculated upon each iteration based on the new weights that the neighboring vertices have accumulated.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5169">
<title id=" W06-3809.xml">random walk term weighting for improved text classification </title>
<section> experimental setup.  </section>
<citcontext>
<prevsection>
<prevsent>the basic idea in nave bayes text classifier is to estimate the probability of acategory given document using joint probabilities of words and documents.
</prevsent>
<prevsent>nave bayes assumes word independence, which means that the conditional probability of word given category is assumed to be independent of the conditional probability of other words given the same category.
</prevsent>
</prevsection>
<citsent citstr=" P04-3024 ">
despite this simplification, nave bayes classifiers were shown to perform surprisingly well on text classification (joachims, 1997), (schneider, 2004).<papid> P04-3024 </papid></citsent>
<aftsection>
<nextsent>while there are several versions of nave bayes classifiers (variations of multinomial and multivariate bernoulli), we use the multinomial model (mc callum and nigam, 1998), which was shown to be more effective.
</nextsent>
<nextsent>rocchio.
</nextsent>
<nextsent>this is an adaptation of the relevance feedback method developed in information retrieval (rocchio, 1971).
</nextsent>
<nextsent>it uses standard tf.idf weighted vectors to represent documents, and builds prototype vector for each category by summing up the vectors of the training documents in each category.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5170">
<title id=" W06-1638.xml">better informed training of latent syntactic features </title>
<section> abstract </section>
<citcontext>
<prevsection>

<prevsent>we study unsupervised methods for learning refinements of the nonterminals in treebank.
</prevsent>
</prevsection>
<citsent citstr=" P05-1010 ">
following matsuzaki et al (2005) <papid> P05-1010 </papid>and prescher (2005), <papid> W05-1512 </papid>we may for example split np without supervision into np[0] and np[1], which behave differently.</citsent>
<aftsection>
<nextsent>we first propose to learn pcfg that adds such features to nonterminals in such away that they respect patterns of linguistic feature passing: each nodes nonterminal features are either identical to, or independent of, those of its parent.
</nextsent>
<nextsent>this linguistic constraint reduces runtime and the number of parameters to be learned.
</nextsent>
<nextsent>how ever, it did not yield improvements when training on the penn treebank.
</nextsent>
<nextsent>an orthogonal strategy was more successful: to im prove the performance of the em learner by treebank preprocessing and by annealing methods that split nonterminals selectively.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5178">
<title id=" W06-1638.xml">better informed training of latent syntactic features </title>
<section> abstract </section>
<citcontext>
<prevsection>

<prevsent>we study unsupervised methods for learning refinements of the nonterminals in treebank.
</prevsent>
</prevsection>
<citsent citstr=" W05-1512 ">
following matsuzaki et al (2005) <papid> P05-1010 </papid>and prescher (2005), <papid> W05-1512 </papid>we may for example split np without supervision into np[0] and np[1], which behave differently.</citsent>
<aftsection>
<nextsent>we first propose to learn pcfg that adds such features to nonterminals in such away that they respect patterns of linguistic feature passing: each nodes nonterminal features are either identical to, or independent of, those of its parent.
</nextsent>
<nextsent>this linguistic constraint reduces runtime and the number of parameters to be learned.
</nextsent>
<nextsent>how ever, it did not yield improvements when training on the penn treebank.
</nextsent>
<nextsent>an orthogonal strategy was more successful: to im prove the performance of the em learner by treebank preprocessing and by annealing methods that split nonterminals selectively.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5179">
<title id=" W06-1638.xml">better informed training of latent syntactic features </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>treebanks never contain enough information; thus pcfgs estimated straightforwardly from the penn treebank (bies et al, 1995) work only moderately well (charniak, 1996).
</prevsent>
<prevsent>to address this problem,researchers have used heuristics to add more information.
</prevsent>
</prevsection>
<citsent citstr=" C96-1058 ">
eisner (1996), <papid> C96-1058 </papid>charniak (1997), collins(1997), <papid> P97-1003 </papid>and many subsequent researchers1 annotated every node with lexical features passed upfrom its head child,?</citsent>
<aftsection>
<nextsent>in order to more precisely reflect the nodes inside?
</nextsent>
<nextsent>contents.
</nextsent>
<nextsent>charniak (1997) and johnson (1998) <papid> J98-4004 </papid>annotated each node with its parent and grandparent nonterminals, to more precisely reflect its outside?</nextsent>
<nextsent>context.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5180">
<title id=" W06-1638.xml">better informed training of latent syntactic features </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>treebanks never contain enough information; thus pcfgs estimated straightforwardly from the penn treebank (bies et al, 1995) work only moderately well (charniak, 1996).
</prevsent>
<prevsent>to address this problem,researchers have used heuristics to add more information.
</prevsent>
</prevsection>
<citsent citstr=" P97-1003 ">
eisner (1996), <papid> C96-1058 </papid>charniak (1997), collins(1997), <papid> P97-1003 </papid>and many subsequent researchers1 annotated every node with lexical features passed upfrom its head child,?</citsent>
<aftsection>
<nextsent>in order to more precisely reflect the nodes inside?
</nextsent>
<nextsent>contents.
</nextsent>
<nextsent>charniak (1997) and johnson (1998) <papid> J98-4004 </papid>annotated each node with its parent and grandparent nonterminals, to more precisely reflect its outside?</nextsent>
<nextsent>context.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5181">
<title id=" W06-1638.xml">better informed training of latent syntactic features </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in order to more precisely reflect the nodes inside?
</prevsent>
<prevsent>contents.
</prevsent>
</prevsection>
<citsent citstr=" J98-4004 ">
charniak (1997) and johnson (1998) <papid> J98-4004 </papid>annotated each node with its parent and grandparent nonterminals, to more precisely reflect its outside?</citsent>
<aftsection>
<nextsent>context.
</nextsent>
<nextsent>collins (1996)<papid> P96-1025 </papid>split the sentence label into two versions, representing sentences with and without subjects.</nextsent>
<nextsent>he 1not to mention earlier non-pcfg lexicalized statistical parsers, notably magerman (1995) <papid> P95-1037 </papid>for the penn treebank.also modified the treebank to contain different labels for standard and for base noun phrases.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5182">
<title id=" W06-1638.xml">better informed training of latent syntactic features </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>charniak (1997) and johnson (1998) <papid> J98-4004 </papid>annotated each node with its parent and grandparent nonterminals, to more precisely reflect its outside?</prevsent>
<prevsent>context.</prevsent>
</prevsection>
<citsent citstr=" P96-1025 ">
collins (1996)<papid> P96-1025 </papid>split the sentence label into two versions, representing sentences with and without subjects.</citsent>
<aftsection>
<nextsent>he 1not to mention earlier non-pcfg lexicalized statistical parsers, notably magerman (1995) <papid> P95-1037 </papid>for the penn treebank.also modified the treebank to contain different labels for standard and for base noun phrases.</nextsent>
<nextsent>klein and manning (2003) <papid> P03-1054 </papid>identified nonterminals that could valu ably be split into fine-grained ones using hand-written linguistic rules.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5183">
<title id=" W06-1638.xml">better informed training of latent syntactic features </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>context.
</prevsent>
<prevsent>collins (1996)<papid> P96-1025 </papid>split the sentence label into two versions, representing sentences with and without subjects.</prevsent>
</prevsection>
<citsent citstr=" P95-1037 ">
he 1not to mention earlier non-pcfg lexicalized statistical parsers, notably magerman (1995) <papid> P95-1037 </papid>for the penn treebank.also modified the treebank to contain different labels for standard and for base noun phrases.</citsent>
<aftsection>
<nextsent>klein and manning (2003) <papid> P03-1054 </papid>identified nonterminals that could valu ably be split into fine-grained ones using hand-written linguistic rules.</nextsent>
<nextsent>their unlexical ized parser combined several such heuristics with rule markov ization and reached performance similar to early lexicalized parsers.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5184">
<title id=" W06-1638.xml">better informed training of latent syntactic features </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>collins (1996)<papid> P96-1025 </papid>split the sentence label into two versions, representing sentences with and without subjects.</prevsent>
<prevsent>he 1not to mention earlier non-pcfg lexicalized statistical parsers, notably magerman (1995) <papid> P95-1037 </papid>for the penn treebank.also modified the treebank to contain different labels for standard and for base noun phrases.</prevsent>
</prevsection>
<citsent citstr=" P03-1054 ">
klein and manning (2003) <papid> P03-1054 </papid>identified nonterminals that could valu ably be split into fine-grained ones using hand-written linguistic rules.</citsent>
<aftsection>
<nextsent>their unlexical ized parser combined several such heuristics with rule markov ization and reached performance similar to early lexicalized parsers.
</nextsent>
<nextsent>in all these cases, choosing which nonterminals to split, and how, was matter of art.
</nextsent>
<nextsent>ideally such splits would be learned automatically from the given treebank itself.
</nextsent>
<nextsent>this would be less costly and more portable to treebanks for new domain sand languages.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5194">
<title id=" W06-1638.xml">better informed training of latent syntactic features </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>317 citation observed data hidden data collins (1997) <papid> P97-1003 </papid>treebank tree with head child annotated on each nonterminal no hidden data.</prevsent>
<prevsent>degenerate em case.</prevsent>
</prevsection>
<citsent citstr=" P92-1017 ">
lari and young (1990) words parse tree pereira and schabes (1992) <papid> P92-1017 </papid>words and partial brackets parse tree klein and manning (2001) <papid> W01-0714 </papid>part-of-speech tags parse tree chiang and bikel (2002) <papid> C02-1126 </papid>treebank tree head child on each nonterminal matsuzaki et al (2005) <papid> P05-1010 </papid>treebank tree integer feature on each nonterminal inherit model (this paper) treebank tree and head child heuristics integer feature on each nonterminal table 1: observed and hidden data in pcfg grammar learning.</citsent>
<aftsection>
<nextsent>the parameters of pcfg can be learned with or without supervision.
</nextsent>
<nextsent>in the supervised case, the complete tree is observed, and the rewrite rule probabilities can be estimated directly from the observed rule counts.
</nextsent>
<nextsent>in the unsupervised case, only the words are observed, and the learning method must induce the whole structure above them.
</nextsent>
<nextsent>(see table 1.)in the partially supervised case we will consider, some part of the tree is observed, and the remaining information has to be induced.pereira and schabes (1992) <papid> P92-1017 </papid>estimate pcfg parameters from partially bracketed sentences, using the inside-outside algorithm to induce the missing brackets and the missing node labels.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5196">
<title id=" W06-1638.xml">better informed training of latent syntactic features </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>317 citation observed data hidden data collins (1997) <papid> P97-1003 </papid>treebank tree with head child annotated on each nonterminal no hidden data.</prevsent>
<prevsent>degenerate em case.</prevsent>
</prevsection>
<citsent citstr=" W01-0714 ">
lari and young (1990) words parse tree pereira and schabes (1992) <papid> P92-1017 </papid>words and partial brackets parse tree klein and manning (2001) <papid> W01-0714 </papid>part-of-speech tags parse tree chiang and bikel (2002) <papid> C02-1126 </papid>treebank tree head child on each nonterminal matsuzaki et al (2005) <papid> P05-1010 </papid>treebank tree integer feature on each nonterminal inherit model (this paper) treebank tree and head child heuristics integer feature on each nonterminal table 1: observed and hidden data in pcfg grammar learning.</citsent>
<aftsection>
<nextsent>the parameters of pcfg can be learned with or without supervision.
</nextsent>
<nextsent>in the supervised case, the complete tree is observed, and the rewrite rule probabilities can be estimated directly from the observed rule counts.
</nextsent>
<nextsent>in the unsupervised case, only the words are observed, and the learning method must induce the whole structure above them.
</nextsent>
<nextsent>(see table 1.)in the partially supervised case we will consider, some part of the tree is observed, and the remaining information has to be induced.pereira and schabes (1992) <papid> P92-1017 </papid>estimate pcfg parameters from partially bracketed sentences, using the inside-outside algorithm to induce the missing brackets and the missing node labels.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5197">
<title id=" W06-1638.xml">better informed training of latent syntactic features </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>317 citation observed data hidden data collins (1997) <papid> P97-1003 </papid>treebank tree with head child annotated on each nonterminal no hidden data.</prevsent>
<prevsent>degenerate em case.</prevsent>
</prevsection>
<citsent citstr=" C02-1126 ">
lari and young (1990) words parse tree pereira and schabes (1992) <papid> P92-1017 </papid>words and partial brackets parse tree klein and manning (2001) <papid> W01-0714 </papid>part-of-speech tags parse tree chiang and bikel (2002) <papid> C02-1126 </papid>treebank tree head child on each nonterminal matsuzaki et al (2005) <papid> P05-1010 </papid>treebank tree integer feature on each nonterminal inherit model (this paper) treebank tree and head child heuristics integer feature on each nonterminal table 1: observed and hidden data in pcfg grammar learning.</citsent>
<aftsection>
<nextsent>the parameters of pcfg can be learned with or without supervision.
</nextsent>
<nextsent>in the supervised case, the complete tree is observed, and the rewrite rule probabilities can be estimated directly from the observed rule counts.
</nextsent>
<nextsent>in the unsupervised case, only the words are observed, and the learning method must induce the whole structure above them.
</nextsent>
<nextsent>(see table 1.)in the partially supervised case we will consider, some part of the tree is observed, and the remaining information has to be induced.pereira and schabes (1992) <papid> P92-1017 </papid>estimate pcfg parameters from partially bracketed sentences, using the inside-outside algorithm to induce the missing brackets and the missing node labels.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5239">
<title id=" W06-1638.xml">better informed training of latent syntactic features </title>
<section> annealing-like training approaches.  </section>
<citcontext>
<prevsection>
<prevsent>training latent pcfg models, like training most other unsupervised models, requires non-convex optimization.
</prevsent>
<prevsent>to find good parameter values, it is often helpful to train simpler model first and use its parameters to derive starting guess for the harder optimization problem.
</prevsent>
</prevsection>
<citsent citstr=" H94-1028 ">
a well-known example is the training of the ibm models for statistical machine translation (berger et al, 1994).<papid> H94-1028 </papid></citsent>
<aftsection>
<nextsent>in this vein, we did an experiment in which we gradually increased during em training of the pcfg-la and inherit models.
</nextsent>
<nextsent>whenever the training likelihood began to converge, we manually and globally increased l, simply doubling or tripling it (see clone all?
</nextsent>
<nextsent>in table 3 and fig.
</nextsent>
<nextsent>5).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5242">
<title id=" W06-1638.xml">better informed training of latent syntactic features </title>
<section> annealing-like training approaches.  </section>
<citcontext>
<prevsection>
<prevsent>while this temporarily changes the balance of probability among the three kinds of rules, em immediately corrects this balance on the next training iteration to match the observed balance onthe treebank trees hence the one-iteration down tick in figure 5).
</prevsent>
<prevsent>321 jensen-shannon divergence is defined as d(q, r) = 1 2 ( ( || + 2 ) +d ( || + 2 )) these experiments are kind of poor mansversion?
</prevsent>
</prevsection>
<citsent citstr=" P93-1024 ">
of the deterministic annealing clustering algorithm (pereira et al, 1993; <papid> P93-1024 </papid>rose, 1998),which gradually increases the number of clusters during the clustering process.</citsent>
<aftsection>
<nextsent>in deterministic annealing, one starts in principle with very large number of clusters, but maximizes likelihood only under constraint that the joint distribution p(point , cluster) must have very high entropy.
</nextsent>
<nextsent>this drives all of the cluster centro ids to coincide exactly, redundantly representing just one effective cluster.
</nextsent>
<nextsent>as the entropy is permitted to decrease, some of the cluster centro ids find it worth while to drift apart.6 in future work, we would like to apply this technique to split nonterminals gradually, by initially requiring high-entropy parse forests on the training data and slowly relaxing this constraint.
</nextsent>
<nextsent>5.1 setup.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5243">
<title id=" W06-1638.xml">better informed training of latent syntactic features </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>5.1 setup.
</prevsent>
<prevsent>we ran several experiments to compare the inherit with the pcfg-la model and look into the effect of different treebank preprocessing and the annealing-like procedures.
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
we used sections 220 of the penn treebank 2 wall street journal corpus (marcus et al, 1993) <papid> J93-2004 </papid>for training, section 22 as development set and section 23 for testing.</citsent>
<aftsection>
<nextsent>following matsuzaki et al (2005), <papid> P05-1010 </papid>words occurring fewer than 4 times in the training corpus were replaced by unknown-wordsymbols that encoded certain suffix and capitalization information.</nextsent>
<nextsent>all experiments used simple add-lambda smoothing (?=0.1) during the reestimation step (m step) of training.binarization and markovization.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5268">
<title id=" W06-1638.xml">better informed training of latent syntactic features </title>
<section> future work: log-linear modeling.  </section>
<citcontext>
<prevsection>
<prevsent>inheritance as in the inherit model can then be expressed by features like ? = ?, or ? = ? and = vp.
</prevsent>
<prevsent>during early iterations, we could use prior to encourage strong positive weigh ton these inheritance features, and gradually relax this bias akin to the structural annealing?
</prevsent>
</prevsection>
<citsent citstr=" P06-1072 ">
of (smith and eisner, 2006).<papid> P06-1072 </papid></citsent>
<aftsection>
<nextsent>when modeling the lexical rule (x[?]
</nextsent>
<nextsent>w), we could use features that consider the spelling of the word in conjunction with the value of ?.
</nextsent>
<nextsent>thus, we might learn that [1] is particularly likely to rewrite as word ending in -s.
</nextsent>
<nextsent>spelling features that are predictable from string context are important clues to the existence and behavior of the hidden annotations we wish to induce.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5271">
<title id=" W06-2303.xml">robust parsing of the proposition bank </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this paper, we extend an existing statistical parsing model to produce richer out put parse trees, annotated with propbank semantic role labels.
</prevsent>
<prevsent>our results show that the model can be robustly extended to produce more complex output parse trees without any loss in performance and suggest that joint inference of syntactic and semantic representations is viable alternative to approaches based on pipeline of local processing steps.
</prevsent>
</prevsection>
<citsent citstr=" A00-2018 ">
recent successes in statistical syntactic parsing based on supervised learning techniques trained on large corpus of syntactic trees (collins, 1999; charniak, 2000; <papid> A00-2018 </papid>henderson, 2003) <papid> N03-1014 </papid>have brought forth the hope that the same approaches could be applied to the more ambitious goal of recovering the propositional content and the frame semantics of sentence.</citsent>
<aftsection>
<nextsent>moving towards shallow semantic level of representation is first initial step towards the distant goal of natural language understanding and has immediate applications in question-answering and information extraction.
</nextsent>
<nextsent>for example, an automatic flight reservation system processing the sentence want to book flight from geneva to trento will need to know that from geneva denotes the origin of the flight and to trento denotes its destination.
</nextsent>
<nextsent>knowing that these two phrases are prepositional phrases, the information provided by syntactic parser, is only moderately useful.the growing interest in learning deeper information is to large extent supported and due to the recent development of semantically annotated databases such as framenet (baker et al, 1998) <papid> P98-1013 </papid>or the proposition bank (palmer et al, 2005), <papid> J05-1004 </papid>that can be used as training resources for number of supervised learning paradigms.</nextsent>
<nextsent>we focus here onthe proposition bank (propbank).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5272">
<title id=" W06-2303.xml">robust parsing of the proposition bank </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this paper, we extend an existing statistical parsing model to produce richer out put parse trees, annotated with propbank semantic role labels.
</prevsent>
<prevsent>our results show that the model can be robustly extended to produce more complex output parse trees without any loss in performance and suggest that joint inference of syntactic and semantic representations is viable alternative to approaches based on pipeline of local processing steps.
</prevsent>
</prevsection>
<citsent citstr=" N03-1014 ">
recent successes in statistical syntactic parsing based on supervised learning techniques trained on large corpus of syntactic trees (collins, 1999; charniak, 2000; <papid> A00-2018 </papid>henderson, 2003) <papid> N03-1014 </papid>have brought forth the hope that the same approaches could be applied to the more ambitious goal of recovering the propositional content and the frame semantics of sentence.</citsent>
<aftsection>
<nextsent>moving towards shallow semantic level of representation is first initial step towards the distant goal of natural language understanding and has immediate applications in question-answering and information extraction.
</nextsent>
<nextsent>for example, an automatic flight reservation system processing the sentence want to book flight from geneva to trento will need to know that from geneva denotes the origin of the flight and to trento denotes its destination.
</nextsent>
<nextsent>knowing that these two phrases are prepositional phrases, the information provided by syntactic parser, is only moderately useful.the growing interest in learning deeper information is to large extent supported and due to the recent development of semantically annotated databases such as framenet (baker et al, 1998) <papid> P98-1013 </papid>or the proposition bank (palmer et al, 2005), <papid> J05-1004 </papid>that can be used as training resources for number of supervised learning paradigms.</nextsent>
<nextsent>we focus here onthe proposition bank (propbank).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5276">
<title id=" W06-2303.xml">robust parsing of the proposition bank </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>moving towards shallow semantic level of representation is first initial step towards the distant goal of natural language understanding and has immediate applications in question-answering and information extraction.
</prevsent>
<prevsent>for example, an automatic flight reservation system processing the sentence want to book flight from geneva to trento will need to know that from geneva denotes the origin of the flight and to trento denotes its destination.
</prevsent>
</prevsection>
<citsent citstr=" P98-1013 ">
knowing that these two phrases are prepositional phrases, the information provided by syntactic parser, is only moderately useful.the growing interest in learning deeper information is to large extent supported and due to the recent development of semantically annotated databases such as framenet (baker et al, 1998) <papid> P98-1013 </papid>or the proposition bank (palmer et al, 2005), <papid> J05-1004 </papid>that can be used as training resources for number of supervised learning paradigms.</citsent>
<aftsection>
<nextsent>we focus here onthe proposition bank (propbank).
</nextsent>
<nextsent>propbank encodes propositional information by adding layer of argument structure annotation to the syntactic structures of the penn treebank (marcus et al, 1993).<papid> J93-2004 </papid></nextsent>
<nextsent>verbal predicates in the penn treebank (ptb) receive label rel and their arguments are annotated with abstract semantic role labelsa0-a5 or aa for those complements of the pred icative verb that are considered arguments while those complements of the verb labelled with semantic functional label in the original ptb receive the composite semantic role label am-x , where stands for labels such as loc, tmp or adv,for locative, temporal and adverbial modifiers re spectively.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5277">
<title id=" W06-2303.xml">robust parsing of the proposition bank </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>moving towards shallow semantic level of representation is first initial step towards the distant goal of natural language understanding and has immediate applications in question-answering and information extraction.
</prevsent>
<prevsent>for example, an automatic flight reservation system processing the sentence want to book flight from geneva to trento will need to know that from geneva denotes the origin of the flight and to trento denotes its destination.
</prevsent>
</prevsection>
<citsent citstr=" J05-1004 ">
knowing that these two phrases are prepositional phrases, the information provided by syntactic parser, is only moderately useful.the growing interest in learning deeper information is to large extent supported and due to the recent development of semantically annotated databases such as framenet (baker et al, 1998) <papid> P98-1013 </papid>or the proposition bank (palmer et al, 2005), <papid> J05-1004 </papid>that can be used as training resources for number of supervised learning paradigms.</citsent>
<aftsection>
<nextsent>we focus here onthe proposition bank (propbank).
</nextsent>
<nextsent>propbank encodes propositional information by adding layer of argument structure annotation to the syntactic structures of the penn treebank (marcus et al, 1993).<papid> J93-2004 </papid></nextsent>
<nextsent>verbal predicates in the penn treebank (ptb) receive label rel and their arguments are annotated with abstract semantic role labelsa0-a5 or aa for those complements of the pred icative verb that are considered arguments while those complements of the verb labelled with semantic functional label in the original ptb receive the composite semantic role label am-x , where stands for labels such as loc, tmp or adv,for locative, temporal and adverbial modifiers re spectively.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5278">
<title id=" W06-2303.xml">robust parsing of the proposition bank </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>knowing that these two phrases are prepositional phrases, the information provided by syntactic parser, is only moderately useful.the growing interest in learning deeper information is to large extent supported and due to the recent development of semantically annotated databases such as framenet (baker et al, 1998) <papid> P98-1013 </papid>or the proposition bank (palmer et al, 2005), <papid> J05-1004 </papid>that can be used as training resources for number of supervised learning paradigms.</prevsent>
<prevsent>we focus here onthe proposition bank (propbank).</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
propbank encodes propositional information by adding layer of argument structure annotation to the syntactic structures of the penn treebank (marcus et al, 1993).<papid> J93-2004 </papid></citsent>
<aftsection>
<nextsent>verbal predicates in the penn treebank (ptb) receive label rel and their arguments are annotated with abstract semantic role labelsa0-a5 or aa for those complements of the pred icative verb that are considered arguments while those complements of the verb labelled with semantic functional label in the original ptb receive the composite semantic role label am-x , where stands for labels such as loc, tmp or adv,for locative, temporal and adverbial modifiers respectively.
</nextsent>
<nextsent>a tree structure with propbank labels for sentence from the ptb (section 00) is shown in figure 1 below.
</nextsent>
<nextsent>propbank uses two levels of granularity in its annotation, at least conceptually.
</nextsent>
<nextsent>arguments receiving labels a0-a5 or aa do not express consistent semantic roles and are specific to verb, while arguments receiving an am-x label are supposed to be adjuncts and the respective roles they express are consistent across all verbs.1recent approaches to learning semantic role labels are based on two-stage architectures.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5279">
<title id=" W06-2303.xml">robust parsing of the proposition bank </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>arguments receiving labels a0-a5 or aa do not express consistent semantic roles and are specific to verb, while arguments receiving an am-x label are supposed to be adjuncts and the respective roles they express are consistent across all verbs.1recent approaches to learning semantic role labels are based on two-stage architectures.
</prevsent>
<prevsent>the first stage selects the elements to be labelled, while the second determines the labels to be assigned to the selected elements.
</prevsent>
</prevsection>
<citsent citstr=" J02-3001 ">
while some of these models are based on full parse trees (gildea and jurafsky, 2002; <papid> J02-3001 </papid>gildea and palmer, 2002), <papid> P02-1031 </papid>other methods have been proposed that eschew the need for full 1there are thirteen semantic role labels for modifiers.</citsent>
<aftsection>
<nextsent>see (palmer et al, 2005) <papid> J05-1004 </papid>for detailed discussion of propbank semantic roles labels.</nextsent>
<nextsent>11 s     hh hh hh hh hh np-a1     pp pp pp the governments borrowing authority vp           hh hh hh hh xxxxxx xxxxxx xxxxx vbd-rel dropped pp-am-tmp  hh in at np nn midnight np-am-tmp nnp tuesday pp-a4  hh to to np qp   pp $ 2.80 trillion pp-a3  hh in from np qp   pp $ 2.87 trillion figure 1: sample syntactic structure from the propbank with semantic role annotations.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5280">
<title id=" W06-2303.xml">robust parsing of the proposition bank </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>arguments receiving labels a0-a5 or aa do not express consistent semantic roles and are specific to verb, while arguments receiving an am-x label are supposed to be adjuncts and the respective roles they express are consistent across all verbs.1recent approaches to learning semantic role labels are based on two-stage architectures.
</prevsent>
<prevsent>the first stage selects the elements to be labelled, while the second determines the labels to be assigned to the selected elements.
</prevsent>
</prevsection>
<citsent citstr=" P02-1031 ">
while some of these models are based on full parse trees (gildea and jurafsky, 2002; <papid> J02-3001 </papid>gildea and palmer, 2002), <papid> P02-1031 </papid>other methods have been proposed that eschew the need for full 1there are thirteen semantic role labels for modifiers.</citsent>
<aftsection>
<nextsent>see (palmer et al, 2005) <papid> J05-1004 </papid>for detailed discussion of propbank semantic roles labels.</nextsent>
<nextsent>11 s     hh hh hh hh hh np-a1     pp pp pp the governments borrowing authority vp           hh hh hh hh xxxxxx xxxxxx xxxxx vbd-rel dropped pp-am-tmp  hh in at np nn midnight np-am-tmp nnp tuesday pp-a4  hh to to np qp   pp $ 2.80 trillion pp-a3  hh in from np qp   pp $ 2.87 trillion figure 1: sample syntactic structure from the propbank with semantic role annotations.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5292">
<title id=" W06-2303.xml">robust parsing of the proposition bank </title>
<section> the basic parsing architecture.  </section>
<citcontext>
<prevsection>
<prevsent>the derivation moves include: projecting constituent with specified label, attaching one constituent to another, and shifting tag-word pair onto the push down stack.
</prevsent>
<prevsent>unlike standard history-based models, ssn parsers do not state any explicit independence assumptions between derivation steps.
</prevsent>
</prevsection>
<citsent citstr=" P98-1087 ">
they use aneural network architecture, called simple syn chrony network (henderson and lane, 1998), <papid> P98-1087 </papid>to induce finite history representation of an unbounded sequence of moves.</citsent>
<aftsection>
<nextsent>the history representation of parse history d1, . . .
</nextsent>
<nextsent>, di1, whichwe denote h(d1, . . .
</nextsent>
<nextsent>, di1), is assigned to the constituent that is on the top of the stack before the ith move.
</nextsent>
<nextsent>the representation h(d1, . . .
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5297">
<title id=" W06-2303.xml">robust parsing of the proposition bank </title>
<section> learning semantic role labels.  </section>
<citcontext>
<prevsection>
<prevsent>a second reduction of the search space prunes the space of possible project or attach derivation moves: best-first search strategy is applied to the five best alternative decisions only.
</prevsent>
<prevsent>the next section describes our model, extended to produce richer output parse trees annotated with semantic role labels.
</prevsent>
</prevsection>
<citsent citstr=" H05-1078 ">
previous work on learning function labels during parsing (merlo and musillo, 2005; <papid> H05-1078 </papid>musillo and merlo, 2005) <papid> W05-1509 </papid>assumed that function labels represent the interface between lexical semantics andsyntax.</citsent>
<aftsection>
<nextsent>we extend this hypothesis to the semantic role labels assigned in propbank, as they are an exhaustive extension of function labels, which have been reorganised incoherent inventory of labels and assigned exhaustively to all sentences in the ptb.
</nextsent>
<nextsent>because propbank is built on the ptb, it inherits in part its notion of function labels which is directly integrated into the am-x role labels.
</nextsent>
<nextsent>a0-a5 or aa labels correspond to many of the un labelled elements in the ptb and also to those elements that ptb annotators had classified as re 2the on-line version of back propagation is used to train ssn parsing models.
</nextsent>
<nextsent>it performs gradient descent with maximum likelihood objective function and weight decay regularization (bishop, 1995).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5299">
<title id=" W06-2303.xml">robust parsing of the proposition bank </title>
<section> learning semantic role labels.  </section>
<citcontext>
<prevsection>
<prevsent>a second reduction of the search space prunes the space of possible project or attach derivation moves: best-first search strategy is applied to the five best alternative decisions only.
</prevsent>
<prevsent>the next section describes our model, extended to produce richer output parse trees annotated with semantic role labels.
</prevsent>
</prevsection>
<citsent citstr=" W05-1509 ">
previous work on learning function labels during parsing (merlo and musillo, 2005; <papid> H05-1078 </papid>musillo and merlo, 2005) <papid> W05-1509 </papid>assumed that function labels represent the interface between lexical semantics andsyntax.</citsent>
<aftsection>
<nextsent>we extend this hypothesis to the semantic role labels assigned in propbank, as they are an exhaustive extension of function labels, which have been reorganised incoherent inventory of labels and assigned exhaustively to all sentences in the ptb.
</nextsent>
<nextsent>because propbank is built on the ptb, it inherits in part its notion of function labels which is directly integrated into the am-x role labels.
</nextsent>
<nextsent>a0-a5 or aa labels correspond to many of the un labelled elements in the ptb and also to those elements that ptb annotators had classified as re 2the on-line version of back propagation is used to train ssn parsing models.
</nextsent>
<nextsent>it performs gradient descent with maximum likelihood objective function and weight decay regularization (bishop, 1995).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5300">
<title id=" W06-2303.xml">robust parsing of the proposition bank </title>
<section> learning semantic role labels.  </section>
<citcontext>
<prevsection>
<prevsent>we assume that semantic roles are very often projected by the lexical semantics of the words in the sentence.
</prevsent>
<prevsent>we introduce this bottom-up lexical information by fine-grained modelling of semantic role labels.
</prevsent>
</prevsection>
<citsent citstr=" P03-1054 ">
extending technique presented in (klein and manning, 2003) <papid> P03-1054 </papid>and adopted in (merlo and musillo, 2005; <papid> H05-1078 </papid>musillo and merlo, 2005) <papid> W05-1509 </papid>for function labels, we split some part-of-speech tags into tags marked with semantic role labels.</citsent>
<aftsection>
<nextsent>the semantic role labels attached to non-terminal directly projected by pre terminal and belonging to few selected categories (dir, ext, loc, mnr, pnc, caus and tmp) were propagated down to the pre-terminal part-of-speech tag of its head.
</nextsent>
<nextsent>to affect only labels that are projections of lexical semantics properties, the propagation takes into account the distance of the projection from the lexical head to the label, and distances greater than two are not included.
</nextsent>
<nextsent>figure 2 illustrates the result of this operation.
</nextsent>
<nextsent>in our augmented model, inputs to each history representation are selected according to linguistically motivated notion of structural locality over which dependencies such as argument structure or subcategorization could be specified.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5312">
<title id=" W06-2303.xml">robust parsing of the proposition bank </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>no attempt at collecting word types was made.
</prevsent>
<prevsent>formance comparable to state-of-the-art statistical parsing, by extensions that take the nature of the richer labels to be recovered into account.
</prevsent>
</prevsection>
<citsent citstr=" W04-0832 ">
they also suggest that the relationship between syntactic ptb parsing and semantic propbank parsing is strict enough that an integrated approach to the problem of semantic role labelling is beneficial.in particular, recent models of semantic role labelling separate input indicators of the correlation between the structural position in the tree and the semantic label, such as path, from those indicators that encode constraints on the sequence, such as the previously assigned role (kwon et al, 2004).<papid> W04-0832 </papid>in this way, they can never encode directly the constraining power of certain role in given structural position onto following node in its structural position.</citsent>
<aftsection>
<nextsent>in our augmented model, we attempt to capture these constraints by directly modelling syntactic domains defined by the notion of c-command.
</nextsent>
<nextsent>our results also confirm the findings in (palmer et al, 2005).<papid> J05-1004 </papid></nextsent>
<nextsent>they take critical look at some commonly used features in the semantic role labelling task, such as the path feature.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5314">
<title id=" W06-2303.xml">robust parsing of the proposition bank </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>its sparseness is due to the occurrence of intermediate nodes that are not relevant for the syntactic relations between an argument and its predicate.
</prevsent>
<prevsent>our model of domains is less noisy, and consequently more robust, because it can focus only on c-commanding nodes bearing semantic role labels, thus abstracting away from those nodes that smear the pertinent relations.
</prevsent>
</prevsection>
<citsent citstr=" W05-0639 ">
(yi and palmer, 2005) <papid> W05-0639 </papid>share the motivation of our work.</citsent>
<aftsection>
<nextsent>like the current work, they observe that the distributions of semantic labels could potentially interact with the distributions of syntactic labels and redefine the boundaries of constituents, thus yielding trees that reflect generalisations over both these sources of information.to our knowledge, no results have yet been published on parsing the propbank.
</nextsent>
<nextsent>accordingly, it is not possible to draw straigthforward quantitative 16 r (haghighi et al, 2005) <papid> W05-0623 </papid>83.4 83.1 83.7 (pradhan et al, 2005) <papid> W05-0634 </papid>83.3 83.0 83.5 (punyakanok et al, 2005) 83.1 82.8 83.3 (marquez et al, 2005) 83.1 82.8 83.3 (surdeanu and turmo, 2005) <papid> W05-0635 </papid>82.7 82.5 83.0 propbank ssn 81.6 81.3 81.9table 2: percentage f-measure (f), recall (r), and precision (p) of our propbank ssn parser and state of-the-art semantic role labelling systems on the propbank parsing task (1267 sentences from propbank validating data sets; propbank datasets are available at http://www.lsi.upc.edu/ srlconll/st05/st05.html).</nextsent>
<nextsent>comparison between our propbank ssn parserand other propbank parsers.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5315">
<title id=" W06-2303.xml">robust parsing of the proposition bank </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>(yi and palmer, 2005) <papid> W05-0639 </papid>share the motivation of our work.</prevsent>
<prevsent>like the current work, they observe that the distributions of semantic labels could potentially interact with the distributions of syntactic labels and redefine the boundaries of constituents, thus yielding trees that reflect generalisations over both these sources of information.to our knowledge, no results have yet been published on parsing the propbank.</prevsent>
</prevsection>
<citsent citstr=" W05-0623 ">
accordingly, it is not possible to draw straigthforward quantitative 16 r (haghighi et al, 2005) <papid> W05-0623 </papid>83.4 83.1 83.7 (pradhan et al, 2005) <papid> W05-0634 </papid>83.3 83.0 83.5 (punyakanok et al, 2005) 83.1 82.8 83.3 (marquez et al, 2005) 83.1 82.8 83.3 (surdeanu and turmo, 2005) <papid> W05-0635 </papid>82.7 82.5 83.0 propbank ssn 81.6 81.3 81.9table 2: percentage f-measure (f), recall (r), and precision (p) of our propbank ssn parser and state of-the-art semantic role labelling systems on the propbank parsing task (1267 sentences from propbank validating data sets; propbank datasets are available at http://www.lsi.upc.edu/ srlconll/st05/st05.html).</citsent>
<aftsection>
<nextsent>comparison between our propbank ssn parserand other propbank parsers.
</nextsent>
<nextsent>however, state-of the-art semantic role labelling systems (conll, 2005) use parse trees output by state-of-the-art parsers (collins, 1999; charniak, 2000)<papid> A00-2018 </papid>both for training and testing, and return partial trees annotated with semantic role labels.</nextsent>
<nextsent>an indirect wayof comparing our parser with semantic role la bellers suggests itself.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5317">
<title id=" W06-2303.xml">robust parsing of the proposition bank </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>(yi and palmer, 2005) <papid> W05-0639 </papid>share the motivation of our work.</prevsent>
<prevsent>like the current work, they observe that the distributions of semantic labels could potentially interact with the distributions of syntactic labels and redefine the boundaries of constituents, thus yielding trees that reflect generalisations over both these sources of information.to our knowledge, no results have yet been published on parsing the propbank.</prevsent>
</prevsection>
<citsent citstr=" W05-0634 ">
accordingly, it is not possible to draw straigthforward quantitative 16 r (haghighi et al, 2005) <papid> W05-0623 </papid>83.4 83.1 83.7 (pradhan et al, 2005) <papid> W05-0634 </papid>83.3 83.0 83.5 (punyakanok et al, 2005) 83.1 82.8 83.3 (marquez et al, 2005) 83.1 82.8 83.3 (surdeanu and turmo, 2005) <papid> W05-0635 </papid>82.7 82.5 83.0 propbank ssn 81.6 81.3 81.9table 2: percentage f-measure (f), recall (r), and precision (p) of our propbank ssn parser and state of-the-art semantic role labelling systems on the propbank parsing task (1267 sentences from propbank validating data sets; propbank datasets are available at http://www.lsi.upc.edu/ srlconll/st05/st05.html).</citsent>
<aftsection>
<nextsent>comparison between our propbank ssn parserand other propbank parsers.
</nextsent>
<nextsent>however, state-of the-art semantic role labelling systems (conll, 2005) use parse trees output by state-of-the-art parsers (collins, 1999; charniak, 2000)<papid> A00-2018 </papid>both for training and testing, and return partial trees annotated with semantic role labels.</nextsent>
<nextsent>an indirect wayof comparing our parser with semantic role la bellers suggests itself.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5319">
<title id=" W06-2303.xml">robust parsing of the proposition bank </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>(yi and palmer, 2005) <papid> W05-0639 </papid>share the motivation of our work.</prevsent>
<prevsent>like the current work, they observe that the distributions of semantic labels could potentially interact with the distributions of syntactic labels and redefine the boundaries of constituents, thus yielding trees that reflect generalisations over both these sources of information.to our knowledge, no results have yet been published on parsing the propbank.</prevsent>
</prevsection>
<citsent citstr=" W05-0635 ">
accordingly, it is not possible to draw straigthforward quantitative 16 r (haghighi et al, 2005) <papid> W05-0623 </papid>83.4 83.1 83.7 (pradhan et al, 2005) <papid> W05-0634 </papid>83.3 83.0 83.5 (punyakanok et al, 2005) 83.1 82.8 83.3 (marquez et al, 2005) 83.1 82.8 83.3 (surdeanu and turmo, 2005) <papid> W05-0635 </papid>82.7 82.5 83.0 propbank ssn 81.6 81.3 81.9table 2: percentage f-measure (f), recall (r), and precision (p) of our propbank ssn parser and state of-the-art semantic role labelling systems on the propbank parsing task (1267 sentences from propbank validating data sets; propbank datasets are available at http://www.lsi.upc.edu/ srlconll/st05/st05.html).</citsent>
<aftsection>
<nextsent>comparison between our propbank ssn parserand other propbank parsers.
</nextsent>
<nextsent>however, state-of the-art semantic role labelling systems (conll, 2005) use parse trees output by state-of-the-art parsers (collins, 1999; charniak, 2000)<papid> A00-2018 </papid>both for training and testing, and return partial trees annotated with semantic role labels.</nextsent>
<nextsent>an indirect wayof comparing our parser with semantic role la bellers suggests itself.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5338">
<title id=" W06-3004.xml">enhanced interactive question answering with conditional random fields </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>users need more than answers, however: while they might be cognizant of many of the different types of information that they need, few ? if any ? users are capable of identifying all of the questions that must be asked and answered for particular scenario.
</prevsent>
<prevsent>in order to take full advantage of the q/a capabilities of current systems, users need access to sources of domain-specific knowledge that will expose them to new concepts and ideas and will allow them to ask better questions.
</prevsent>
</prevsection>
<citsent citstr=" W04-2508 ">
in previous work (hickl et al, 2004; <papid> W04-2508 </papid>harabagiu etal., 2005<papid> P05-1026 </papid>a), we have argued that interactive question answering systems should be based on predictive dialogue architecture which can be used to provide users with both precise answers to their questions as well as suggestions of relevant research topics that could be explored throughout the course of an interactive q/a dialogue.</citsent>
<aftsection>
<nextsent>typically, the quality of interactive q/a dialogue shas been measured in three ways: (1) efficiency, defined as the number of questions that the user must pose to find particular information, (2) effectiveness, defined by the relevance of the answer returned, and (3) user satisfaction (scholtz and morse, 2003).in our experiments with an interactive q/a system, (known as ferret), we found that performance in each of these areas improves as users are provided with suggestions that are relevant to their domain of interest.
</nextsent>
<nextsent>in ferret, suggestions aremade to users in the form of predictive question answer pairs (known as quabs) which are either generated automatically from the set of documents returned for query (using techniques first described in (harabagiu et al, 2005<papid> P05-1026 </papid>a)), or are selected from alarge database of questions-answer pairs created off line (prior to dialogue) by human annotators.</nextsent>
<nextsent>figure 1 presents an example of ten quabs that were returned by ferret in response to the question how are eu countries responding to the worldwide increase of job outsourcing to india??.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5339">
<title id=" W06-3004.xml">enhanced interactive question answering with conditional random fields </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>users need more than answers, however: while they might be cognizant of many of the different types of information that they need, few ? if any ? users are capable of identifying all of the questions that must be asked and answered for particular scenario.
</prevsent>
<prevsent>in order to take full advantage of the q/a capabilities of current systems, users need access to sources of domain-specific knowledge that will expose them to new concepts and ideas and will allow them to ask better questions.
</prevsent>
</prevsection>
<citsent citstr=" P05-1026 ">
in previous work (hickl et al, 2004; <papid> W04-2508 </papid>harabagiu etal., 2005<papid> P05-1026 </papid>a), we have argued that interactive question answering systems should be based on predictive dialogue architecture which can be used to provide users with both precise answers to their questions as well as suggestions of relevant research topics that could be explored throughout the course of an interactive q/a dialogue.</citsent>
<aftsection>
<nextsent>typically, the quality of interactive q/a dialogue shas been measured in three ways: (1) efficiency, defined as the number of questions that the user must pose to find particular information, (2) effectiveness, defined by the relevance of the answer returned, and (3) user satisfaction (scholtz and morse, 2003).in our experiments with an interactive q/a system, (known as ferret), we found that performance in each of these areas improves as users are provided with suggestions that are relevant to their domain of interest.
</nextsent>
<nextsent>in ferret, suggestions aremade to users in the form of predictive question answer pairs (known as quabs) which are either generated automatically from the set of documents returned for query (using techniques first described in (harabagiu et al, 2005<papid> P05-1026 </papid>a)), or are selected from alarge database of questions-answer pairs created off line (prior to dialogue) by human annotators.</nextsent>
<nextsent>figure 1 presents an example of ten quabs that were returned by ferret in response to the question how are eu countries responding to the worldwide increase of job outsourcing to india??.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5351">
<title id=" W06-3004.xml">enhanced interactive question answering with conditional random fields </title>
<section> the ferret interactive.  </section>
<citcontext>
<prevsection>
<prevsent>questions and relational information extracted bythe dialogue shell are also sent to predictive dialogue module, which identifies the quabs that best meet the users expected information requirements.
</prevsent>
<prevsent>at the core of the ferrets predictive dialogue module is the predictive dialogue network (pqn), large database of quabs that were either generated off-line by human annotators or created on-line by ferret (either during the current dialogue or during some previous dialogue)1.
</prevsent>
</prevsection>
<citsent citstr=" C00-1072 ">
in order to generate quabs automatically, documents identified from ferrets automatic q/a system are first submitted to topic representation module, which computes both topic signatures (lin and hovy, 2000) <papid> C00-1072 </papid>and enhanced topic signatures (harabagiu, 2004) <papid> C04-1084 </papid>in order to identify set of topic-relevant passages.passages are then submitted to an information extraction module, which annotates texts with wide 1techniques used by human annotators for creating quabswere first described in (hickl et al, 2004); <papid> W04-2508 </papid>full details offer rets automatic quab generation components are provided in (harabagiu et al, 2005<papid> P05-1026 </papid>a).range of lexical, semantic, and syntactic information, including (1) morphological information, (2) named entity information from lccs cicerolitenamed entity recognition system, (3) semantic dependencies extracted from lccs propbank-stylesemantic parser, and (4) syntactic parse information.</citsent>
<aftsection>
<nextsent>passages are then transformed into natural language questions using set of question formation heuristics; the resultant quabs are then stored inthe pqn.
</nextsent>
<nextsent>since we believe that the same set of relations that hold between questions in dialogue should also hold between pairs of individual questions taken in isolation, discourse relations are discovered between each newly-generated quab and the set of quabs stored in the pqn.
</nextsent>
<nextsent>ferrets question similarity module then uses the similarity function described in (harabagiu et al, 2005<papid> P05-1026 </papid>a) ? along with relational information stored in the pqn ? in order to identify the quabs that represent themost informative possible continuations of the dialogue.</nextsent>
<nextsent>quabs are then ranked in terms of their relevance to the users submitted question and returned to the user.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5352">
<title id=" W06-3004.xml">enhanced interactive question answering with conditional random fields </title>
<section> the ferret interactive.  </section>
<citcontext>
<prevsection>
<prevsent>questions and relational information extracted bythe dialogue shell are also sent to predictive dialogue module, which identifies the quabs that best meet the users expected information requirements.
</prevsent>
<prevsent>at the core of the ferrets predictive dialogue module is the predictive dialogue network (pqn), large database of quabs that were either generated off-line by human annotators or created on-line by ferret (either during the current dialogue or during some previous dialogue)1.
</prevsent>
</prevsection>
<citsent citstr=" C04-1084 ">
in order to generate quabs automatically, documents identified from ferrets automatic q/a system are first submitted to topic representation module, which computes both topic signatures (lin and hovy, 2000) <papid> C00-1072 </papid>and enhanced topic signatures (harabagiu, 2004) <papid> C04-1084 </papid>in order to identify set of topic-relevant passages.passages are then submitted to an information extraction module, which annotates texts with wide 1techniques used by human annotators for creating quabswere first described in (hickl et al, 2004); <papid> W04-2508 </papid>full details offer rets automatic quab generation components are provided in (harabagiu et al, 2005<papid> P05-1026 </papid>a).range of lexical, semantic, and syntactic information, including (1) morphological information, (2) named entity information from lccs cicerolitenamed entity recognition system, (3) semantic dependencies extracted from lccs propbank-stylesemantic parser, and (4) syntactic parse information.</citsent>
<aftsection>
<nextsent>passages are then transformed into natural language questions using set of question formation heuristics; the resultant quabs are then stored inthe pqn.
</nextsent>
<nextsent>since we believe that the same set of relations that hold between questions in dialogue should also hold between pairs of individual questions taken in isolation, discourse relations are discovered between each newly-generated quab and the set of quabs stored in the pqn.
</nextsent>
<nextsent>ferrets question similarity module then uses the similarity function described in (harabagiu et al, 2005<papid> P05-1026 </papid>a) ? along with relational information stored in the pqn ? in order to identify the quabs that represent themost informative possible continuations of the dialogue.</nextsent>
<nextsent>quabs are then ranked in terms of their relevance to the users submitted question and returned to the user.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5365">
<title id=" W06-3004.xml">enhanced interactive question answering with conditional random fields </title>
<section> user interaction models for relevance.  </section>
<citcontext>
<prevsection>
<prevsent>returning again to figure 3, we can see that questions like nq1 and nq3 are designed to discover new knowledge that the user does not currently possess, while questions like nq6 try to 28 establish whether or not the users hypothesis (i.e. namely, that eu countries view job outsourcing to india as an problem) is valid and deserves further consideration.
</prevsent>
<prevsent>estimation unlike systems that utilize mixed initiative dialogues in order to determine users information needs (small and strzalkowski, 2004), systems (likeferret) which relyon interactions based on predictive questioning have traditionally not incorporated techniques that allow them to gather relevance feedback from users.
</prevsent>
</prevsection>
<citsent citstr=" N03-1028 ">
in this section, we describe how we have used new set of user interaction models (uim) in conjunction with relevance classifier based on conditional random fields (crf) (mccallum, 2003; sha and pereira, 2003) <papid> N03-1028 </papid>in order to im prove the relevance of the quab suggestions that ferret returns in response to users query.we believe that systems based on predictive questioning can derive feedback from users in three ways.</citsent>
<aftsection>
<nextsent>first, systems can learn which suggestion sor answers are relevant to users domain of interest by tracking which elements users select through out the course of dialogue.
</nextsent>
<nextsent>with ferret, each answer or suggestion presented to user is associated with hyper link that links to the original text that the answer or quab was derived from.
</nextsent>
<nextsent>while users do not always follow links associated with passages they deem to be relevant to their query, we expect that the set of selected elements are generally more likely to be relevant to the users interests than unselected elements.
</nextsent>
<nextsent>second, since interactive q/a systems are often used to gather information for inclusion in written reports, systems can identify relevant content by tracking the text passages that users copy to other applications, such as text editors or word processors.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5371">
<title id=" W06-3004.xml">enhanced interactive question answering with conditional random fields </title>
<section> user interaction models for relevance.  </section>
<citcontext>
<prevsection>
<prevsent>(b) similarity: similarity of quab, qn and quab, qn1.
</prevsent>
<prevsent>(c) relation likelihood: equal to the likelihood of each predicate-argument structure included in quab given all quabs contained in ferrets quab; calculated for arg-0, arg-1, and argm-tmp for each predicate found inquab suggestions.
</prevsent>
</prevsection>
<citsent citstr=" J05-1004 ">
(predicate-argument relations were identified using semantic parser trained on propbank (palmer et al, 2005) <papid> J05-1004 </papid>annotations.)(d) conditional expected answer type likelihood: equal to the joint probability p(eatquab |eatquestion) calculated from corpus of dialogues collected from human users of ferret.</citsent>
<aftsection>
<nextsent>(e) terms in common: real-valued feature equal to the number of terms in common between the quab and both qn and qn1.
</nextsent>
<nextsent>(f) named entities in common: same as terms in common, but calculated for named entities detected by lccs cierolite named entity recognition system.
</nextsent>
<nextsent>figure 6: relevance features.
</nextsent>
<nextsent>in the next section, we describe how we utilized the user interaction model described in subsection 4.1 in conjunction with this subsection in order to improve the relevance of quab suggestions returned to users.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5372">
<title id=" W06-1642.xml">fully automatic lexicon expansion for domain oriented sentiment analysis </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>sentiment analysis (sa) (nasukawa and yi, 2003; yi et al, 2003) is task to recognize writers?
</prevsent>
<prevsent>feelings as expressed in positive or negative comments, by analyzing unreadablylarge numbers of documents.
</prevsent>
</prevsection>
<citsent citstr=" C04-1071 ">
extensive syntactic patterns enable us to detect sentiment expressions and to convert them into semantic structures with high precision, as reported by kanayama et al (2004).<papid> C04-1071 </papid></citsent>
<aftsection>
<nextsent>from the example japanese sentence (1) in the digital camera domain, the sa system extracts sentiment representation as (2), which consists of predicate and an argument with positive (+) polarity.
</nextsent>
<nextsent>(1) kono kamera-ha subarashii-to omou.
</nextsent>
<nextsent>i think this camera is splendid.?
</nextsent>
<nextsent>(2) [+] splendid(camera)sa in general tends to focus on subjective sentiment expressions, which explicitly describe an authors preference as in the above example (1).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5373">
<title id=" W06-1642.xml">fully automatic lexicon expansion for domain oriented sentiment analysis </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>sentiment analysis has been extensively studied in recent years.
</prevsent>
<prevsent>the target of sa in this paper is wider than in previous work.
</prevsent>
</prevsection>
<citsent citstr=" W03-1017 ">
for example, yu and hatzivassiloglou (2003) <papid> W03-1017 </papid>separated facts from opinions and assigned polarities only to opinions.</citsent>
<aftsection>
<nextsent>in contrast, our system detects factual polar clauses as well as senti ments.unsupervised learning for sentiment analysis is also being studied.
</nextsent>
<nextsent>for example, hatzivassiloglou and mckeown (1997) <papid> P97-1023 </papid>labeled adjectives as positive or negative, relying on semantic orientation.</nextsent>
<nextsent>turney (2002) <papid> P02-1053 </papid>used collocation with excellent?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5374">
<title id=" W06-1642.xml">fully automatic lexicon expansion for domain oriented sentiment analysis </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>for example, yu and hatzivassiloglou (2003) <papid> W03-1017 </papid>separated facts from opinions and assigned polarities only to opinions.</prevsent>
<prevsent>in contrast, our system detects factual polar clauses as well as senti ments.unsupervised learning for sentiment analysis is also being studied.</prevsent>
</prevsection>
<citsent citstr=" P97-1023 ">
for example, hatzivassiloglou and mckeown (1997) <papid> P97-1023 </papid>labeled adjectives as positive or negative, relying on semantic orientation.</citsent>
<aftsection>
<nextsent>turney (2002) <papid> P02-1053 </papid>used collocation with excellent?</nextsent>
<nextsent>or poor?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5375">
<title id=" W06-1642.xml">fully automatic lexicon expansion for domain oriented sentiment analysis </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in contrast, our system detects factual polar clauses as well as senti ments.unsupervised learning for sentiment analysis is also being studied.
</prevsent>
<prevsent>for example, hatzivassiloglou and mckeown (1997) <papid> P97-1023 </papid>labeled adjectives as positive or negative, relying on semantic orientation.</prevsent>
</prevsection>
<citsent citstr=" P02-1053 ">
turney (2002) <papid> P02-1053 </papid>used collocation with excellent?</citsent>
<aftsection>
<nextsent>or poor?
</nextsent>
<nextsent>to obtain positive and negative clues for document classification.
</nextsent>
<nextsent>in this paper, we use contextual information which is wider than for the contexts they used, and address the problem of acquiring lexical entries from the noisy clues.
</nextsent>
<nextsent>inter-sentential contexts as in our approach were used as clue also for subjectivity analysis (riloff and wiebe, 2003; <papid> W03-1014 </papid>pang and lee,2004), <papid> P04-1035 </papid>which is two-fold classification into subjective and objective sentences.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5376">
<title id=" W06-1642.xml">fully automatic lexicon expansion for domain oriented sentiment analysis </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>to obtain positive and negative clues for document classification.
</prevsent>
<prevsent>in this paper, we use contextual information which is wider than for the contexts they used, and address the problem of acquiring lexical entries from the noisy clues.
</prevsent>
</prevsection>
<citsent citstr=" W03-1014 ">
inter-sentential contexts as in our approach were used as clue also for subjectivity analysis (riloff and wiebe, 2003; <papid> W03-1014 </papid>pang and lee,2004), <papid> P04-1035 </papid>which is two-fold classification into subjective and objective sentences.</citsent>
<aftsection>
<nextsent>compared to it, this paper solves more difficult problem: three-fold classification into positive, negative and non-polar expressions using imperfect co herency in terms of sentiment polarity.
</nextsent>
<nextsent>learning methods for phrase-level sentiment analysis closely share an objective of our approach.
</nextsent>
<nextsent>popescu and etzioni (2005) <papid> H05-1043 </papid>achieved high-precision opinion phrases extraction byusing relaxation labeling.</nextsent>
<nextsent>their method itera tively assigns polarity to phrase, relying on semantic orientation of co-occurring words in specific relations in sentence, but the scope of semantic orientation is limited to within asentence.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5377">
<title id=" W06-1642.xml">fully automatic lexicon expansion for domain oriented sentiment analysis </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>to obtain positive and negative clues for document classification.
</prevsent>
<prevsent>in this paper, we use contextual information which is wider than for the contexts they used, and address the problem of acquiring lexical entries from the noisy clues.
</prevsent>
</prevsection>
<citsent citstr=" P04-1035 ">
inter-sentential contexts as in our approach were used as clue also for subjectivity analysis (riloff and wiebe, 2003; <papid> W03-1014 </papid>pang and lee,2004), <papid> P04-1035 </papid>which is two-fold classification into subjective and objective sentences.</citsent>
<aftsection>
<nextsent>compared to it, this paper solves more difficult problem: three-fold classification into positive, negative and non-polar expressions using imperfect co herency in terms of sentiment polarity.
</nextsent>
<nextsent>learning methods for phrase-level sentiment analysis closely share an objective of our approach.
</nextsent>
<nextsent>popescu and etzioni (2005) <papid> H05-1043 </papid>achieved high-precision opinion phrases extraction byusing relaxation labeling.</nextsent>
<nextsent>their method itera tively assigns polarity to phrase, relying on semantic orientation of co-occurring words in specific relations in sentence, but the scope of semantic orientation is limited to within asentence.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5378">
<title id=" W06-1642.xml">fully automatic lexicon expansion for domain oriented sentiment analysis </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>compared to it, this paper solves more difficult problem: three-fold classification into positive, negative and non-polar expressions using imperfect co herency in terms of sentiment polarity.
</prevsent>
<prevsent>learning methods for phrase-level sentiment analysis closely share an objective of our approach.
</prevsent>
</prevsection>
<citsent citstr=" H05-1043 ">
popescu and etzioni (2005) <papid> H05-1043 </papid>achieved high-precision opinion phrases extraction byusing relaxation labeling.</citsent>
<aftsection>
<nextsent>their method itera tively assigns polarity to phrase, relying on semantic orientation of co-occurring words in specific relations in sentence, but the scope of semantic orientation is limited to within asentence.
</nextsent>
<nextsent>wilson et al (2005) <papid> H05-1044 </papid>proposed supervised learning, dividing the resources into 356 document to analyze sentence delimitation ...</nextsent>
<nextsent>sentences proposition detection propositions clauses polarity assignment + ? polarities polar clauses modality patterns conjunctive patterns * polar atoms figure 1: the flow of the clause-level sa.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5379">
<title id=" W06-1642.xml">fully automatic lexicon expansion for domain oriented sentiment analysis </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>popescu and etzioni (2005) <papid> H05-1043 </papid>achieved high-precision opinion phrases extraction byusing relaxation labeling.</prevsent>
<prevsent>their method itera tively assigns polarity to phrase, relying on semantic orientation of co-occurring words in specific relations in sentence, but the scope of semantic orientation is limited to within asentence.</prevsent>
</prevsection>
<citsent citstr=" H05-1044 ">
wilson et al (2005) <papid> H05-1044 </papid>proposed supervised learning, dividing the resources into 356 document to analyze sentence delimitation ...</citsent>
<aftsection>
<nextsent>sentences proposition detection propositions clauses polarity assignment + ? polarities polar clauses modality patterns conjunctive patterns * polar atoms figure 1: the flow of the clause-level sa.
</nextsent>
<nextsent>prior polarity and context polarity, which are similar to polar atoms and syntactic pattern sin this paper, respectively.
</nextsent>
<nextsent>wilson et al prepared prior polarities from existing resources, and learned the context polarities by using prior polarities and annotated corpora.
</nextsent>
<nextsent>therefore the prerequisite data and learned data are opposite from those in our approach.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5385">
<title id=" W06-2709.xml">annis complex multilevel annotations in a linguistic database </title>
<section> use cases.  </section>
<citcontext>
<prevsection>
<prevsent>an interesting query might look for nominal phrases (const=np) that are new in the discourse(given=new) and belong to the (information-) focus of sentence (focus=ans), e.g. for investigating the phonological realization of these.
</prevsent>
<prevsent>63 the according query has the form: const=np &amp; given=new &amp; focus=ans &amp; #1 = #2.4 queries in annis can be restricted to subsets of corpus, by queries such as focus=ans &amp; doc=*81-11*, which searches for all answer foci in the data that has been elicited by means of the task 81-11 in the questionnaire, yielding matching data from all languages in our database.
</prevsent>
</prevsection>
<citsent citstr=" W04-0213 ">
discourse studies the potsdam commentary corpus, pcc (stede, 2004), <papid> W04-0213 </papid>consists of 173 newspaper commentaries, annotated for morpho syn tax, coreference, discourse structure according to rhetorical structure theory, and information structure.a question of interest here is the information structural pattern of sentences introducing discourse segments that elaborate on another part of the discourse: elaboration &amp; rel=satellite &amp; (cat=vroot &amp; aboutness-topic) &amp; #1   #2 &#2; = #3.</citsent>
<aftsection>
<nextsent>another research issue is the relationship of coreference and discourse structure.
</nextsent>
<nextsent>however, querying for coreference relations is not supported yet.
</nextsent>
<nextsent>currently we are working on integrating native xml database into our system.
</nextsent>
<nextsent>to make processing more efficient, we are developing an internal inline representation of the standoff interchange format, encoding overlapping segments by means of milestones or fragments (barnard et al, 1995).furthermore, the query language will be extended to cover different kinds of queries on sequential relations as well as coreference relations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5386">
<title id=" W06-2938.xml">dependency parsing as a classic ation problem </title>
<section> the learning algorithm.  </section>
<citcontext>
<prevsection>
<prevsent>section 4 details the approach for identifying long distance links and presents the parsing results.
</prevsent>
<prevsent>246
</prevsent>
</prevsection>
<citsent citstr=" N06-1042 ">
the greedy prepend algorithm (yuret and ture,2006) <papid> N06-1042 </papid>was used to build decision lists to identify dependency relations.</citsent>
<aftsection>
<nextsent>a decision list is an ordered list of rules where each rule consists of pattern and classification (rivest, 1987).
</nextsent>
<nextsent>the first rule whose pattern matches given instance is used for its classification.
</nextsent>
<nextsent>in our application the pattern specifies the attributes of the two words to be linked such as partsof speech and morphological features.
</nextsent>
<nextsent>the classification indicates the existence and the type of the dependency link between the two words.table 1 gives subset of the decision list that identifies links between adjacent words in german.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5388">
<title id=" W07-0207.xml">latent semantic grammar induction context projectivity and prior distributions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>finding structure in text with little or no prior knowledge is therefore fundamental issue in the study of language.
</prevsent>
<prevsent>however, ugi is still largely unsolved problem.
</prevsent>
</prevsection>
<citsent citstr=" P02-1017 ">
recent work (klein and manning, 2002; <papid> P02-1017 </papid>klein and manning, 2004) <papid> P04-1061 </papid>has renewed interest by using ugi model to parse sentences from the wall street journal section of the penn treebank (wsj).</citsent>
<aftsection>
<nextsent>these parsing results are exciting because they demonstrate real-world applicability to english ugi.
</nextsent>
<nextsent>while other contemporary research in this area is promising, the case for real-world english ugi has not been as convincingly made (van zaanen, 2000; solan et al, 2005).this paper weaves together two threads of inquiry.
</nextsent>
<nextsent>the first thread is latent semantics, which have not been previously used in ugi.
</nextsent>
<nextsent>the second thread is dependency-based ugi, used by klein and manning (2004), <papid> P04-1061 </papid>which nicely dovetails with our semantic approach.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5391">
<title id=" W07-0207.xml">latent semantic grammar induction context projectivity and prior distributions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>finding structure in text with little or no prior knowledge is therefore fundamental issue in the study of language.
</prevsent>
<prevsent>however, ugi is still largely unsolved problem.
</prevsent>
</prevsection>
<citsent citstr=" P04-1061 ">
recent work (klein and manning, 2002; <papid> P02-1017 </papid>klein and manning, 2004) <papid> P04-1061 </papid>has renewed interest by using ugi model to parse sentences from the wall street journal section of the penn treebank (wsj).</citsent>
<aftsection>
<nextsent>these parsing results are exciting because they demonstrate real-world applicability to english ugi.
</nextsent>
<nextsent>while other contemporary research in this area is promising, the case for real-world english ugi has not been as convincingly made (van zaanen, 2000; solan et al, 2005).this paper weaves together two threads of inquiry.
</nextsent>
<nextsent>the first thread is latent semantics, which have not been previously used in ugi.
</nextsent>
<nextsent>the second thread is dependency-based ugi, used by klein and manning (2004), <papid> P04-1061 </papid>which nicely dovetails with our semantic approach.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5393">
<title id=" W07-0207.xml">latent semantic grammar induction context projectivity and prior distributions </title>
<section> latent semantics.  </section>
<citcontext>
<prevsection>
<prevsent>the second thread is dependency-based ugi, used by klein and manning (2004), <papid> P04-1061 </papid>which nicely dovetails with our semantic approach.</prevsent>
<prevsent>the combination of these threads allows some exploration of what characteristics are sufficient for ugi and what characteristics are nec essary.</prevsent>
</prevsection>
<citsent citstr=" H92-1030 ">
previous work has focused on syntax to the exclusion of semantics (brill and marcus, 1992; <papid> H92-1030 </papid>van zaa nen, 2000; klein and manning, 2002; <papid> P02-1017 </papid>paskin, 2001;klein and manning, 2004; <papid> P04-1061 </papid>solan et al, 2005).</citsent>
<aftsection>
<nextsent>however, results from the speech recognition community show that the inclusion of latent semantic information can enhance the performance of their models (coccaro and jurafsky, 1998; bellegarda, 2000; deng and khudanpur, 2003).<papid> N03-1008 </papid></nextsent>
<nextsent>using latent semantic information to improve ugi is therefore both novel and relevant.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5401">
<title id=" W07-0207.xml">latent semantic grammar induction context projectivity and prior distributions </title>
<section> latent semantics.  </section>
<citcontext>
<prevsection>
<prevsent>the combination of these threads allows some exploration of what characteristics are sufficient for ugi and what characteristics are necessary.
</prevsent>
<prevsent>previous work has focused on syntax to the exclusion of semantics (brill and marcus, 1992; <papid> H92-1030 </papid>van zaa nen, 2000; klein and manning, 2002; <papid> P02-1017 </papid>paskin, 2001;klein and manning, 2004; <papid> P04-1061 </papid>solan et al, 2005).</prevsent>
</prevsection>
<citsent citstr=" N03-1008 ">
however, results from the speech recognition community show that the inclusion of latent semantic information can enhance the performance of their models (coccaro and jurafsky, 1998; bellegarda, 2000; deng and khudanpur, 2003).<papid> N03-1008 </papid></citsent>
<aftsection>
<nextsent>using latent semantic information to improve ugi is therefore both novel and relevant.
</nextsent>
<nextsent>the latent semantic information used by the speech recognition community above is produced by latent semantic analysis (lsa), also known as latent semantic indexing (deerwester et al, 1990;landauer et al, 1998).
</nextsent>
<nextsent>lsa creates semantic representation of both words and collections of words in vector space, using two part process.
</nextsent>
<nextsent>first, 45 term by document matrix is created in which the frequency of word wi in document dj is the valueof cell cij . filters may be applied during this process which eliminate undesired terms, e.g. common words.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5420">
<title id=" W07-0207.xml">latent semantic grammar induction context projectivity and prior distributions </title>
<section> unanswered questions.  </section>
<citcontext>
<prevsection>
<prevsent>4.2 projectivity.
</prevsent>
<prevsent>projectivity is an additional constraint that may notbe necessary for successful ugi.
</prevsent>
</prevsection>
<citsent citstr=" C86-1011 ">
english is projective language, but other languages, such as bulgarian, are not (pericliev and ilarionov, 1986).<papid> C86-1011 </papid></citsent>
<aftsection>
<nextsent>nonpro jective ugi has not previously been studied, and it is not clear how important projectivity assumptions are to english ugi.
</nextsent>
<nextsent>figure 3 gives an example of non projective construction: not all heads and their dependents are contiguous sequence.
</nextsent>
<nextsent>john string likes cheese.
</nextsent>
<nextsent>figure 3: non projective dependency graph 1 2 3 4 5 6 7 8 9 10 0 0.5 1 1.5 2 2.5 3 104 words distant um be o d ep en de nc ie figure 4: distance between dependents in wsj10 4.3 context.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5436">
<title id=" W07-0207.xml">latent semantic grammar induction context projectivity and prior distributions </title>
<section> method.  </section>
<citcontext>
<prevsection>
<prevsent>the neighbor, cosine to that neighbor, and cosines of the bigrams constituents to that neighbor were stored.the constituent with the highest cosine to the neighbor was considered the likely head, based on classic head test arguments (hudson, 1987).
</prevsent>
<prevsent>this data was stored in lookup table so that for each bigram the associated information may be found in constant time.
</prevsent>
</prevsection>
<citsent citstr=" H05-1066 ">
next, the wsj10 was parsed using the parsing table described above and minimum spanning tree algorithm for dependency parsing (mcdonald et al, 2005).<papid> H05-1066 </papid></citsent>
<aftsection>
<nextsent>each input sentence was tokenized on whit espace and lowercased.
</nextsent>
<nextsent>moving from left to right, each word was paired with all remaining words on its right.
</nextsent>
<nextsent>if pair existed in the parsing table, the associated information was retrieved.this information was used to populate the fully connected graph that served as input to the minimum spanning tree algorithm.
</nextsent>
<nextsent>specifically, when pair was retrieved from the parsing table, the arc from 48 the stored head to the dependent was given weight equal to the cosine between the head and the nearest unigram neighbor for that bigram pair.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5442">
<title id=" W06-3405.xml">shallow discourse structure for action item detection </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>  0.8), although individual figures for the task class are not given.
</prevsent>
<prevsent>they then concentrated on task sentences, establishing set of predictive features (in which word n-grams emerged as highly predictive?)
</prevsent>
</prevsection>
<citsent citstr=" W04-2319 ">
and achieved reasonable per-sentence classification performance (with f-scores around 0.6).while there are related tags for dialogue act tagging schema ? like damsl (core and allen, 1997), which includes tags such as action-directive and commit, and the icsi mrda schema (shriberg et al, 2004) <papid> W04-2319 </papid>which includes commit tag ? these classes are too general to allow identification of action items specifically.</citsent>
<aftsection>
<nextsent>one comparable attempt in spoken discourse took flat approach, annotating utterances as action-item-related or not (gruenstein et al, 2005) over the icsi and isl meeting corpora (janin et al, 2003; burger et al., 2002).
</nextsent>
<nextsent>their inter-annotator agreement was low (?
</nextsent>
<nextsent>= .36).
</nextsent>
<nextsent>while this may have been partly due to their methods, it is notable that (core and allen, 1997) reported even lower agreement (?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5443">
<title id=" W06-3405.xml">shallow discourse structure for action item detection </title>
<section> hierarchical annotations.  </section>
<citcontext>
<prevsection>
<prevsent>precision recall f-score 1 191 22 0.31 0.50 0.38 2 156 27 0.36 0.33 0.35 3 196 18 0.28 0.55 0.37 5 198 9 0.19 0.67 0.30 table 1: baseline classification performance
</prevsent>
<prevsent>two problems are apparent: firstly, accuracy is lower than desired; secondly, identifying utterances related to action items does not allow us to actually identify those action items and extract their properties (deadline, owner etc.).
</prevsent>
</prevsection>
<citsent citstr=" W02-0811 ">
but if the utterances related to these properties form distinct sub-classes which have their own distinct features, treating them separately and combining the results(along the lines of (klein et al, 2002)) <papid> W02-0811 </papid>might allow better performance, while also identifying the utterances where each propertys value is extracted.</citsent>
<aftsection>
<nextsent>thus, we produced an annotation schema which distinguishes among these four classes.
</nextsent>
<nextsent>the first three correspond to the discussion and assignment of the individual properties of the action item (taskdescription, timeframe and owner); the final agreement class covers utterances which explicitly show that the action item is agreed upon.since the task description subclass extracts description of the task, it must include any utterances that specify the action to be performed, including those that provide required antecedents for anaphoric references.
</nextsent>
<nextsent>the owner subclass includes any utterances that explicitly specify the responsible party (e.g. ill take care of that?, or john, well leave that to you?), but not those whose function might be taken to do so implicitly (such as agreements by the responsible party).
</nextsent>
<nextsent>the timeframe subclass includes any utterances that explicitly refer to when task may start or when it is expected to be finished; note that this is often not specified with 32 date or temporal expression, but rather e.g. by the end of next week,?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5445">
<title id=" W06-3405.xml">shallow discourse structure for action item detection </title>
<section> discussion and future work.  </section>
<citcontext>
<prevsection>
<prevsent>we also wish to examine performance when working from speech recognition hypotheses (as opposed to the human transcripts used here), and the best way to incorporate multiple hypotheses (either as n-best listsor word confusion networks).
</prevsent>
<prevsent>we are actively investigating alternative approaches to sub-classifier com bination: better performance (and more robust and trainable overall system) might be obtained by usinga bayesian network, or maximum entropy classifier as used by (klein et al, 2002).<papid> W02-0811 </papid></prevsent>
</prevsection>
<citsent citstr=" P93-1008 ">
finally, we are developing an interface to new large-vocabulary version of the gemini parser (dowding et al, 1993)<papid> P93-1008 </papid>which will allow us to use semantic parse information as features in the individual sub-class classifiers, and also to extract entity and event representations from the classified utterances for automatic addition of entries to calendars and to-do lists.</citsent>
<aftsection>




</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5446">
<title id=" W06-2405.xml">identifying idiomatic expressions using automatic word alignment </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we approximate the mentioned overlap as the proportion of default alignments.
</prevsent>
<prevsent>we obtain significant improvement over the baseline with both measures.
</prevsent>
</prevsection>
<citsent citstr=" E03-1029 ">
knowing whether an expression receives literal meaning or an idiomatic meaning is important for natural language processing applications that require some sort of semantic interpretation.some applications that would benefit from knowing this distinction are machine translation (im amura et al, 2003), <papid> E03-1029 </papid>finding paraphrases (bannardand callison-burch, 2005), (multilingual) information retrieval (melamed, 1997<papid> W97-0311 </papid>a), etc. the purpose of this paper is to explore to what extent word-alignment in parallel corpora can beused to distinguish idiomatic multiword expressions from more transparent multiword expressions and fully productive expressions.</citsent>
<aftsection>
<nextsent>in the remainder of this section, we present our characterization of idiomatic expressions, the motivation to use parallel corpora and related work.section 2 describes the materials required to apply our method.
</nextsent>
<nextsent>section 3 portraits the routine to extract list of candidate expressions from automatically annotated data.
</nextsent>
<nextsent>experiments with different word alignment types and metrics are shown in section 4.
</nextsent>
<nextsent>our results are discussed in section 5.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5447">
<title id=" W06-2405.xml">identifying idiomatic expressions using automatic word alignment </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we approximate the mentioned overlap as the proportion of default alignments.
</prevsent>
<prevsent>we obtain significant improvement over the baseline with both measures.
</prevsent>
</prevsection>
<citsent citstr=" W97-0311 ">
knowing whether an expression receives literal meaning or an idiomatic meaning is important for natural language processing applications that require some sort of semantic interpretation.some applications that would benefit from knowing this distinction are machine translation (im amura et al, 2003), <papid> E03-1029 </papid>finding paraphrases (bannardand callison-burch, 2005), (multilingual) information retrieval (melamed, 1997<papid> W97-0311 </papid>a), etc. the purpose of this paper is to explore to what extent word-alignment in parallel corpora can beused to distinguish idiomatic multiword expressions from more transparent multiword expressions and fully productive expressions.</citsent>
<aftsection>
<nextsent>in the remainder of this section, we present our characterization of idiomatic expressions, the motivation to use parallel corpora and related work.section 2 describes the materials required to apply our method.
</nextsent>
<nextsent>section 3 portraits the routine to extract list of candidate expressions from automatically annotated data.
</nextsent>
<nextsent>experiments with different word alignment types and metrics are shown in section 4.
</nextsent>
<nextsent>our results are discussed in section 5.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5459">
<title id=" W06-2405.xml">identifying idiomatic expressions using automatic word alignment </title>
<section> data and resources.  </section>
<citcontext>
<prevsection>
<prevsent>the english, spanish and german counterparts are of similar size between 28 and 30 million words in roughly the same number of sentences.automatic word alignment has been done using giza++ (och, 2003).
</prevsent>
<prevsent>we used standard settings of the system to produce viterbi alignments of ibm model 4.
</prevsent>
</prevsection>
<citsent citstr=" W99-0604 ">
alignments have been produced for both translation directions (source to target and target to source) on tokenized plain text.3 we also used well-known heuristics for combining the two directional alignments, the so-called refined alignment (och et al, 1999).<papid> W99-0604 </papid></citsent>
<aftsection>
<nextsent>word-to-word alignments have been merged such that words are connected with each other if they are linked to thesame target.
</nextsent>
<nextsent>in this way we obtained three different word alignment files: source to target (src2trg)with possible multi-word units in the source language, target to source (trg2src) with possiblemulti-word units in the target language, and refined with possible multi-word units in both languages.
</nextsent>
<nextsent>we also created bilingual word type links from the different word-aligned corpora.
</nextsent>
<nextsent>these lists include alignment frequencies that we will use later on for extracting default alignments for individual words.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5460">
<title id=" W06-2405.xml">identifying idiomatic expressions using automatic word alignment </title>
<section> extracting candidates from corpora.  </section>
<citcontext>
<prevsection>
<prevsent>a sample of more than 191,000 candidates types(413,000 tokens) was collected.
</prevsent>
<prevsent>to ensure statistical significance, the types that occur less than 50 times were ignored.
</prevsent>
</prevsection>
<citsent citstr=" J93-1003 ">
for each candidate triple, the log-likelihood(dunning, 1993) <papid> J93-1003 </papid>and salience (kilgarriff and tugwell, 2001) scores were calculated.</citsent>
<aftsection>
<nextsent>these scores have been shown to perform reasonably well in identifying collocations and other lexicalized expressions (villada moiron, 2005).
</nextsent>
<nextsent>in addition, thehead dependence between each pp in the candidates dataset and its selecting verbs was measured.merlo and leybold (2001) <papid> W01-0715 </papid>used the head dependence as diagnostic to determine the argument (or adjunct) status of pp.</nextsent>
<nextsent>the head dependence is measured as the amount of entropy observed among the co-occurring verbs forgiven pp as suggested in (merlo and leybold, 2001; <papid> W01-0715 </papid>baldwin, 2005).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5461">
<title id=" W06-2405.xml">identifying idiomatic expressions using automatic word alignment </title>
<section> extracting candidates from corpora.  </section>
<citcontext>
<prevsection>
<prevsent>for each candidate triple, the log-likelihood(dunning, 1993) <papid> J93-1003 </papid>and salience (kilgarriff and tugwell, 2001) scores were calculated.</prevsent>
<prevsent>these scores have been shown to perform reasonably well in identifying collocations and other lexicalized expressions (villada moiron, 2005).</prevsent>
</prevsection>
<citsent citstr=" W01-0715 ">
in addition, thehead dependence between each pp in the candidates dataset and its selecting verbs was measured.merlo and leybold (2001) <papid> W01-0715 </papid>used the head dependence as diagnostic to determine the argument (or adjunct) status of pp.</citsent>
<aftsection>
<nextsent>the head dependence is measured as the amount of entropy observed among the co-occurring verbs forgiven pp as suggested in (merlo and leybold, 2001; <papid> W01-0715 </papid>baldwin, 2005).</nextsent>
<nextsent>using the two association measures and the head dependence heuristic, three different rankings of the candidate triples were produced.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5467">
<title id=" W07-0206.xml">transductive structured classification through constrained mincuts </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>altun et al (2006) perform experiments on 4 and 40 training instances using at most 200 unlabeled instances.
</prevsent>
<prevsent>the second dataset is the reference part of the cora information extraction dataset.1 this 1the cora ie dataset has been used in seymore et al.
</prevsent>
</prevsection>
<citsent citstr=" N04-1042 ">
(1999), peng and mccallum (2004), <papid> N04-1042 </papid>mccallum et al.</citsent>
<aftsection>
<nextsent>(2000) and han et al (2003), among others.
</nextsent>
<nextsent>we consists of 500 computer science research paper citations.
</nextsent>
<nextsent>each token in citation is labeled as being part of the name of an author, part of the title, part of the date or one of several other labels that we combined into single category (other?).
</nextsent>
<nextsent>the third dataset is the chunking dataset from the conll 2000 (sang and buchholz, 2000) shared task restricted to noun phrases.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5468">
<title id=" W06-2605.xml">discourse parsing learning fol rules based on rich verb semantic representations to automatically label rhetorical relations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we demonstrate that ilp can be used to learn from highly structured natural language data and that the performance of discourse parsing model that only uses semantic information is comparable to that of the state of the art syntactic discourse parsers.
</prevsent>
<prevsent>the availability of corpora annotated with syntactic information have facilitated the use of probabilistic models on tasks such as syntactic parsing.
</prevsent>
</prevsection>
<citsent citstr=" J03-4003 ">
current state of the art syntactic parsers reach accuracies between 86% and 90%, as measured by different types of precision and recall (for more details see (collins, 2003)).<papid> J03-4003 </papid></citsent>
<aftsection>
<nextsent>recent semantic (kingsbury and palmer, 2002) and discourse (carlson et al, 2003) annotation projects are paving the way for developments in semantic and discourse parsing as well.
</nextsent>
<nextsent>however unlike syntactic parsing, significant development in discourse parsing remains at large.
</nextsent>
<nextsent>previous work on discourse parsing ((soricut and marcu, 2003) <papid> N03-1030 </papid>and (forbes et al, 2001)) have focused on syntactic and lexical features only.</nextsent>
<nextsent>however, discourse relations connect clauses/sentences, hence, descriptions of events and states.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5469">
<title id=" W06-2605.xml">discourse parsing learning fol rules based on rich verb semantic representations to automatically label rhetorical relations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>recent semantic (kingsbury and palmer, 2002) and discourse (carlson et al, 2003) annotation projects are paving the way for developments in semantic and discourse parsing as well.
</prevsent>
<prevsent>however unlike syntactic parsing, significant development in discourse parsing remains at large.
</prevsent>
</prevsection>
<citsent citstr=" N03-1030 ">
previous work on discourse parsing ((soricut and marcu, 2003) <papid> N03-1030 </papid>and (forbes et al, 2001)) have focused on syntactic and lexical features only.</citsent>
<aftsection>
<nextsent>however, discourse relations connect clauses/sentences, hence, descriptions of events and states.
</nextsent>
<nextsent>it makes linguistic sense that the semantics of the two clauses generally built around the semantics of the verbs, composed with that of their arguments?
</nextsent>
<nextsent>affects the discourse re lation(s) connecting the clauses.
</nextsent>
<nextsent>this may be even more evident in our instructional domain, where relations derived from planning such as precondition-act may relate clauses.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5472">
<title id=" W06-2605.xml">discourse parsing learning fol rules based on rich verb semantic representations to automatically label rhetorical relations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>affects the discourse re lation(s) connecting the clauses.
</prevsent>
<prevsent>this may be even more evident in our instructional domain, where relations derived from planning such as precondition-act may relate clauses.
</prevsent>
</prevsection>
<citsent citstr=" W04-0211 ">
of course, since semantic information is hard to come by, it is not surprising that previous work on discourse parsing did not use it, or only used shallow word level onto logical semantics as specified in wordnet (polanyi et al, 2004).<papid> W04-0211 </papid></citsent>
<aftsection>
<nextsent>but when rich sentence level semantics is available, it makes sense to experiment with it for discourse parsing.
</nextsent>
<nextsent>a second major difficulty with using such rich verb semantic information, is that it is represented using complex data structures.
</nextsent>
<nextsent>traditional machine learning methods cannot handle highly structured data such as first order logic (fol), representation that is suitably used to represent sentence level semantics.
</nextsent>
<nextsent>such fol representations cannot be reduced to vector of attribute/value pairs as the relations/interdependencies that exist among the predicates would be lost.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5478">
<title id=" W06-2605.xml">discourse parsing learning fol rules based on rich verb semantic representations to automatically label rhetorical relations </title>
<section> data collection.  </section>
<citcontext>
<prevsection>
<prevsent>our interest in decompositional theories of lexical semantics led us to base our semantic representation on verbnet.verbnet ope rationalizes levins work and accounts for 4962 distinct verbs classified into 237main classes.
</prevsent>
<prevsent>moreover, verb nets strong syntactic components allow it to be easily coupled with aparser in order to automatically generate semantically annotated corpus.
</prevsent>
</prevsection>
<citsent citstr=" J91-4003 ">
to provide semantics for nouns, we use corelex (buitelaar, 1998), in turn based on the generative lexicon(pustejovsky, 1991).<papid> J91-4003 </papid></citsent>
<aftsection>
<nextsent>corelex defines basic types such as art (artifact) or com(communication).
</nextsent>
<nextsent>nouns that share the same bundle of basic types are grouped in the same systematic polysemous class (spc).
</nextsent>
<nextsent>the resulting 126 spcs cover about 40,000 nouns.we modified and augmented lcflexs existing lexicon to incorporate verbnet and corelex.
</nextsent>
<nextsent>the lexicon is based on comlex (grishman et al., 1994).<papid> C94-1042 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5479">
<title id=" W06-2605.xml">discourse parsing learning fol rules based on rich verb semantic representations to automatically label rhetorical relations </title>
<section> data collection.  </section>
<citcontext>
<prevsection>
<prevsent>nouns that share the same bundle of basic types are grouped in the same systematic polysemous class (spc).
</prevsent>
<prevsent>the resulting 126 spcs cover about 40,000 nouns.we modified and augmented lcflexs existing lexicon to incorporate verbnet and corelex.
</prevsent>
</prevsection>
<citsent citstr=" C94-1042 ">
the lexicon is based on comlex (grishman et al., 1994).<papid> C94-1042 </papid></citsent>
<aftsection>
<nextsent>verb and noun entries in the lexicon contain link to semantic type defined in the ontology.
</nextsent>
<nextsent>verbnet classes (including subclasses and frames) and corelex spcs are realized as types in the ontology.
</nextsent>
<nextsent>the deep syntactic roles are mapped to the thematic roles, which are defined as variables in the ontology types.
</nextsent>
<nextsent>for more details on the parser see (terenzi and di eugenio, 2003).<papid> N03-2034 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5480">
<title id=" W06-2605.xml">discourse parsing learning fol rules based on rich verb semantic representations to automatically label rhetorical relations </title>
<section> data collection.  </section>
<citcontext>
<prevsection>
<prevsent>verbnet classes (including subclasses and frames) and corelex spcs are realized as types in the ontology.
</prevsent>
<prevsent>the deep syntactic roles are mapped to the thematic roles, which are defined as variables in the ontology types.
</prevsent>
</prevsection>
<citsent citstr=" N03-2034 ">
for more details on the parser see (terenzi and di eugenio, 2003).<papid> N03-2034 </papid></citsent>
<aftsection>
<nextsent>each of the 10,084 edus was parsed using the parser.
</nextsent>
<nextsent>the parser generates both syntactic tree and the associated semantic representation ? for the purpose of this paper, we only focus on the latter.
</nextsent>
<nextsent>figure 2 shows the semantic representation generated for edu1 from example 1, sometimes, you can add liquid to the water?.
</nextsent>
<nextsent>the semantic representation in figure 2 is part 4double annotation and segmentation is currently being done to assess inter-annotator agreement using kappa.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5481">
<title id=" W06-2605.xml">discourse parsing learning fol rules based on rich verb semantic representations to automatically label rhetorical relations </title>
<section> data collection.  </section>
<citcontext>
<prevsection>
<prevsent>lastly, the category others includes relations not covered by the previous three categories such as joint?
</prevsent>
<prevsent>and inde terminate?.
</prevsent>
</prevsection>
<citsent citstr=" W99-0307 ">
based on the modified coding scheme manual,we segmented and annotated our instructional corpus using the augmented rst tool from (marcu etal., 1999).<papid> W99-0307 </papid></citsent>
<aftsection>
<nextsent>the rst tool was modified to incorporate our relation set.
</nextsent>
<nextsent>since we were only interested in rhetorical relations that spanned between two adjacent edus 6, we obtained 3115 sets of potential relations from the set of all relations that we could use as training and testing data.
</nextsent>
<nextsent>the parser was able to provide complete parses for both edus in 908 of the 3115 relation sets.these constitute the training and test set for pro gol.
</nextsent>
<nextsent>the semantic representation for the edus along with the manually annotated rhetorical relations were further processed (as shown in figure 4) and used by progol as input.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5487">
<title id=" W06-2920.xml">conllx shared task on multilingual dependency parsing </title>
<section> previous research.  </section>
<citcontext>
<prevsection>
<prevsent>in his terminology), in which words stand in direct head-dependent relations, for representing the syntactic structure of sentence.hays (1964) and gaifman (1965) studied the formal properties of projective dependency grammars, i.e. those where dependency links are not allowed tocross.
</prevsent>
<prevsent>melcuk (1988) describes multistratal dependency grammar, i.e. one that distinguishes between several types of dependency relations (mor pho logical, syntactic and semantic).
</prevsent>
</prevsection>
<citsent citstr=" P99-1033 ">
other theories related to dependency grammar are word grammar 3some though had significantly less time: one participant registered as late as six days before the test data release (reg ist ration was prerequisite to obtain most of the data sets) and still went on to submit parsed test data in time.(hudson, 1984) and link grammar (sleator and temperley, 1993).some relatively recent rule-based full dependency parsers are kurohashi and nagao (1994) for japanese, oflazer (1999) <papid> P99-1033 </papid>for turkish, tapanainen and jarvinen (1997) <papid> A97-1011 </papid>for english and elworthy (2000) for english and japanese.while phrase structure parsers are usually evaluated with the geig/parseval measures of precision and recall over constituents (black et al, 1991),lin (1995) and others have argued for an alternative, dependency-based evaluation.</citsent>
<aftsection>
<nextsent>that approach is based on conversion from constituent structure to dependency structure by recursively defining head for each constituent.
</nextsent>
<nextsent>the same idea was used by magerman (1995), <papid> P95-1037 </papid>who developed the first head table?</nextsent>
<nextsent>for the penn treebank (marcus et al, 1994), <papid> H94-1020 </papid>and collins (1996),<papid> P96-1025 </papid>whose constituent parser is internally based on probabilities of bilexical dependencies, i.e. dependencies between two words.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5488">
<title id=" W06-2920.xml">conllx shared task on multilingual dependency parsing </title>
<section> previous research.  </section>
<citcontext>
<prevsection>
<prevsent>in his terminology), in which words stand in direct head-dependent relations, for representing the syntactic structure of sentence.hays (1964) and gaifman (1965) studied the formal properties of projective dependency grammars, i.e. those where dependency links are not allowed tocross.
</prevsent>
<prevsent>melcuk (1988) describes multistratal dependency grammar, i.e. one that distinguishes between several types of dependency relations (mor pho logical, syntactic and semantic).
</prevsent>
</prevsection>
<citsent citstr=" A97-1011 ">
other theories related to dependency grammar are word grammar 3some though had significantly less time: one participant registered as late as six days before the test data release (reg ist ration was prerequisite to obtain most of the data sets) and still went on to submit parsed test data in time.(hudson, 1984) and link grammar (sleator and temperley, 1993).some relatively recent rule-based full dependency parsers are kurohashi and nagao (1994) for japanese, oflazer (1999) <papid> P99-1033 </papid>for turkish, tapanainen and jarvinen (1997) <papid> A97-1011 </papid>for english and elworthy (2000) for english and japanese.while phrase structure parsers are usually evaluated with the geig/parseval measures of precision and recall over constituents (black et al, 1991),lin (1995) and others have argued for an alternative, dependency-based evaluation.</citsent>
<aftsection>
<nextsent>that approach is based on conversion from constituent structure to dependency structure by recursively defining head for each constituent.
</nextsent>
<nextsent>the same idea was used by magerman (1995), <papid> P95-1037 </papid>who developed the first head table?</nextsent>
<nextsent>for the penn treebank (marcus et al, 1994), <papid> H94-1020 </papid>and collins (1996),<papid> P96-1025 </papid>whose constituent parser is internally based on probabilities of bilexical dependencies, i.e. dependencies between two words.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5489">
<title id=" W06-2920.xml">conllx shared task on multilingual dependency parsing </title>
<section> previous research.  </section>
<citcontext>
<prevsection>
<prevsent>other theories related to dependency grammar are word grammar 3some though had significantly less time: one participant registered as late as six days before the test data release (reg ist ration was prerequisite to obtain most of the data sets) and still went on to submit parsed test data in time.(hudson, 1984) and link grammar (sleator and temperley, 1993).some relatively recent rule-based full dependency parsers are kurohashi and nagao (1994) for japanese, oflazer (1999) <papid> P99-1033 </papid>for turkish, tapanainen and jarvinen (1997) <papid> A97-1011 </papid>for english and elworthy (2000) for english and japanese.while phrase structure parsers are usually evaluated with the geig/parseval measures of precision and recall over constituents (black et al, 1991),lin (1995) and others have argued for an alternative, dependency-based evaluation.</prevsent>
<prevsent>that approach is based on conversion from constituent structure to dependency structure by recursively defining head for each constituent.</prevsent>
</prevsection>
<citsent citstr=" P95-1037 ">
the same idea was used by magerman (1995), <papid> P95-1037 </papid>who developed the first head table?</citsent>
<aftsection>
<nextsent>for the penn treebank (marcus et al, 1994), <papid> H94-1020 </papid>and collins (1996),<papid> P96-1025 </papid>whose constituent parser is internally based on probabilities of bilexical dependencies, i.e. dependencies between two words.</nextsent>
<nextsent>collins (1997)<papid> P97-1003 </papid>s parser and its re implementation and extension by bikel (2002) have by now been applied to variety of languages: english (collins, 1999), czech (collins et al, 1999), <papid> P99-1065 </papid>german (dubey and keller, 2003), <papid> P03-1013 </papid>spanish (cowan and collins, 2005), <papid> H05-1100 </papid>french (arun and keller, 2005), <papid> P05-1038 </papid>chinese (bikel, 2002) and, according to dan bikels web page, arabic.eisner (1996) <papid> C96-1058 </papid>introduced data-driven dependency parser and compared several probability models on (english) penn treebank data.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5490">
<title id=" W06-2920.xml">conllx shared task on multilingual dependency parsing </title>
<section> previous research.  </section>
<citcontext>
<prevsection>
<prevsent>that approach is based on conversion from constituent structure to dependency structure by recursively defining head for each constituent.
</prevsent>
<prevsent>the same idea was used by magerman (1995), <papid> P95-1037 </papid>who developed the first head table?</prevsent>
</prevsection>
<citsent citstr=" H94-1020 ">
for the penn treebank (marcus et al, 1994), <papid> H94-1020 </papid>and collins (1996),<papid> P96-1025 </papid>whose constituent parser is internally based on probabilities of bilexical dependencies, i.e. dependencies between two words.</citsent>
<aftsection>
<nextsent>collins (1997)<papid> P97-1003 </papid>s parser and its re implementation and extension by bikel (2002) have by now been applied to variety of languages: english (collins, 1999), czech (collins et al, 1999), <papid> P99-1065 </papid>german (dubey and keller, 2003), <papid> P03-1013 </papid>spanish (cowan and collins, 2005), <papid> H05-1100 </papid>french (arun and keller, 2005), <papid> P05-1038 </papid>chinese (bikel, 2002) and, according to dan bikels web page, arabic.eisner (1996) <papid> C96-1058 </papid>introduced data-driven dependency parser and compared several probability models on (english) penn treebank data.</nextsent>
<nextsent>kudo and matsumoto (2000) <papid> W00-1303 </papid>describe dependency parser for japanese and yamada and matsumoto (2003) an extension for english.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5491">
<title id=" W06-2920.xml">conllx shared task on multilingual dependency parsing </title>
<section> previous research.  </section>
<citcontext>
<prevsection>
<prevsent>that approach is based on conversion from constituent structure to dependency structure by recursively defining head for each constituent.
</prevsent>
<prevsent>the same idea was used by magerman (1995), <papid> P95-1037 </papid>who developed the first head table?</prevsent>
</prevsection>
<citsent citstr=" P96-1025 ">
for the penn treebank (marcus et al, 1994), <papid> H94-1020 </papid>and collins (1996),<papid> P96-1025 </papid>whose constituent parser is internally based on probabilities of bilexical dependencies, i.e. dependencies between two words.</citsent>
<aftsection>
<nextsent>collins (1997)<papid> P97-1003 </papid>s parser and its re implementation and extension by bikel (2002) have by now been applied to variety of languages: english (collins, 1999), czech (collins et al, 1999), <papid> P99-1065 </papid>german (dubey and keller, 2003), <papid> P03-1013 </papid>spanish (cowan and collins, 2005), <papid> H05-1100 </papid>french (arun and keller, 2005), <papid> P05-1038 </papid>chinese (bikel, 2002) and, according to dan bikels web page, arabic.eisner (1996) <papid> C96-1058 </papid>introduced data-driven dependency parser and compared several probability models on (english) penn treebank data.</nextsent>
<nextsent>kudo and matsumoto (2000) <papid> W00-1303 </papid>describe dependency parser for japanese and yamada and matsumoto (2003) an extension for english.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5492">
<title id=" W06-2920.xml">conllx shared task on multilingual dependency parsing </title>
<section> previous research.  </section>
<citcontext>
<prevsection>
<prevsent>the same idea was used by magerman (1995), <papid> P95-1037 </papid>who developed the first head table?</prevsent>
<prevsent>for the penn treebank (marcus et al, 1994), <papid> H94-1020 </papid>and collins (1996),<papid> P96-1025 </papid>whose constituent parser is internally based on probabilities of bilexical dependencies, i.e. dependencies between two words.</prevsent>
</prevsection>
<citsent citstr=" P97-1003 ">
collins (1997)<papid> P97-1003 </papid>s parser and its re implementation and extension by bikel (2002) have by now been applied to variety of languages: english (collins, 1999), czech (collins et al, 1999), <papid> P99-1065 </papid>german (dubey and keller, 2003), <papid> P03-1013 </papid>spanish (cowan and collins, 2005), <papid> H05-1100 </papid>french (arun and keller, 2005), <papid> P05-1038 </papid>chinese (bikel, 2002) and, according to dan bikels web page, arabic.eisner (1996) <papid> C96-1058 </papid>introduced data-driven dependency parser and compared several probability models on (english) penn treebank data.</citsent>
<aftsection>
<nextsent>kudo and matsumoto (2000) <papid> W00-1303 </papid>describe dependency parser for japanese and yamada and matsumoto (2003) an extension for english.</nextsent>
<nextsent>nivres parser has been tested for swedish (nivre et al, 2004), <papid> W04-2407 </papid>english (nivre and scholz, 2004), <papid> C04-1010 </papid>czech (nivre and nilsson, 2005), <papid> P05-1013 </papid>bulgarian (marinov and nivre, 2005) and chinese cheng et al (2005), <papid> I05-3003 </papid>while mcdonalds parser has been applied to english (mcdonald et al, 2005<papid> P05-1012 </papid>a), czech (mcdonald et al, 2005<papid> P05-1012 </papid>b) and, very recently, danish (mcdonald and pereira, 2006).<papid> E06-1011 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5493">
<title id=" W06-2920.xml">conllx shared task on multilingual dependency parsing </title>
<section> previous research.  </section>
<citcontext>
<prevsection>
<prevsent>the same idea was used by magerman (1995), <papid> P95-1037 </papid>who developed the first head table?</prevsent>
<prevsent>for the penn treebank (marcus et al, 1994), <papid> H94-1020 </papid>and collins (1996),<papid> P96-1025 </papid>whose constituent parser is internally based on probabilities of bilexical dependencies, i.e. dependencies between two words.</prevsent>
</prevsection>
<citsent citstr=" P99-1065 ">
collins (1997)<papid> P97-1003 </papid>s parser and its re implementation and extension by bikel (2002) have by now been applied to variety of languages: english (collins, 1999), czech (collins et al, 1999), <papid> P99-1065 </papid>german (dubey and keller, 2003), <papid> P03-1013 </papid>spanish (cowan and collins, 2005), <papid> H05-1100 </papid>french (arun and keller, 2005), <papid> P05-1038 </papid>chinese (bikel, 2002) and, according to dan bikels web page, arabic.eisner (1996) <papid> C96-1058 </papid>introduced data-driven dependency parser and compared several probability models on (english) penn treebank data.</citsent>
<aftsection>
<nextsent>kudo and matsumoto (2000) <papid> W00-1303 </papid>describe dependency parser for japanese and yamada and matsumoto (2003) an extension for english.</nextsent>
<nextsent>nivres parser has been tested for swedish (nivre et al, 2004), <papid> W04-2407 </papid>english (nivre and scholz, 2004), <papid> C04-1010 </papid>czech (nivre and nilsson, 2005), <papid> P05-1013 </papid>bulgarian (marinov and nivre, 2005) and chinese cheng et al (2005), <papid> I05-3003 </papid>while mcdonalds parser has been applied to english (mcdonald et al, 2005<papid> P05-1012 </papid>a), czech (mcdonald et al, 2005<papid> P05-1012 </papid>b) and, very recently, danish (mcdonald and pereira, 2006).<papid> E06-1011 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5494">
<title id=" W06-2920.xml">conllx shared task on multilingual dependency parsing </title>
<section> previous research.  </section>
<citcontext>
<prevsection>
<prevsent>the same idea was used by magerman (1995), <papid> P95-1037 </papid>who developed the first head table?</prevsent>
<prevsent>for the penn treebank (marcus et al, 1994), <papid> H94-1020 </papid>and collins (1996),<papid> P96-1025 </papid>whose constituent parser is internally based on probabilities of bilexical dependencies, i.e. dependencies between two words.</prevsent>
</prevsection>
<citsent citstr=" P03-1013 ">
collins (1997)<papid> P97-1003 </papid>s parser and its re implementation and extension by bikel (2002) have by now been applied to variety of languages: english (collins, 1999), czech (collins et al, 1999), <papid> P99-1065 </papid>german (dubey and keller, 2003), <papid> P03-1013 </papid>spanish (cowan and collins, 2005), <papid> H05-1100 </papid>french (arun and keller, 2005), <papid> P05-1038 </papid>chinese (bikel, 2002) and, according to dan bikels web page, arabic.eisner (1996) <papid> C96-1058 </papid>introduced data-driven dependency parser and compared several probability models on (english) penn treebank data.</citsent>
<aftsection>
<nextsent>kudo and matsumoto (2000) <papid> W00-1303 </papid>describe dependency parser for japanese and yamada and matsumoto (2003) an extension for english.</nextsent>
<nextsent>nivres parser has been tested for swedish (nivre et al, 2004), <papid> W04-2407 </papid>english (nivre and scholz, 2004), <papid> C04-1010 </papid>czech (nivre and nilsson, 2005), <papid> P05-1013 </papid>bulgarian (marinov and nivre, 2005) and chinese cheng et al (2005), <papid> I05-3003 </papid>while mcdonalds parser has been applied to english (mcdonald et al, 2005<papid> P05-1012 </papid>a), czech (mcdonald et al, 2005<papid> P05-1012 </papid>b) and, very recently, danish (mcdonald and pereira, 2006).<papid> E06-1011 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5495">
<title id=" W06-2920.xml">conllx shared task on multilingual dependency parsing </title>
<section> previous research.  </section>
<citcontext>
<prevsection>
<prevsent>the same idea was used by magerman (1995), <papid> P95-1037 </papid>who developed the first head table?</prevsent>
<prevsent>for the penn treebank (marcus et al, 1994), <papid> H94-1020 </papid>and collins (1996),<papid> P96-1025 </papid>whose constituent parser is internally based on probabilities of bilexical dependencies, i.e. dependencies between two words.</prevsent>
</prevsection>
<citsent citstr=" H05-1100 ">
collins (1997)<papid> P97-1003 </papid>s parser and its re implementation and extension by bikel (2002) have by now been applied to variety of languages: english (collins, 1999), czech (collins et al, 1999), <papid> P99-1065 </papid>german (dubey and keller, 2003), <papid> P03-1013 </papid>spanish (cowan and collins, 2005), <papid> H05-1100 </papid>french (arun and keller, 2005), <papid> P05-1038 </papid>chinese (bikel, 2002) and, according to dan bikels web page, arabic.eisner (1996) <papid> C96-1058 </papid>introduced data-driven dependency parser and compared several probability models on (english) penn treebank data.</citsent>
<aftsection>
<nextsent>kudo and matsumoto (2000) <papid> W00-1303 </papid>describe dependency parser for japanese and yamada and matsumoto (2003) an extension for english.</nextsent>
<nextsent>nivres parser has been tested for swedish (nivre et al, 2004), <papid> W04-2407 </papid>english (nivre and scholz, 2004), <papid> C04-1010 </papid>czech (nivre and nilsson, 2005), <papid> P05-1013 </papid>bulgarian (marinov and nivre, 2005) and chinese cheng et al (2005), <papid> I05-3003 </papid>while mcdonalds parser has been applied to english (mcdonald et al, 2005<papid> P05-1012 </papid>a), czech (mcdonald et al, 2005<papid> P05-1012 </papid>b) and, very recently, danish (mcdonald and pereira, 2006).<papid> E06-1011 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5496">
<title id=" W06-2920.xml">conllx shared task on multilingual dependency parsing </title>
<section> previous research.  </section>
<citcontext>
<prevsection>
<prevsent>the same idea was used by magerman (1995), <papid> P95-1037 </papid>who developed the first head table?</prevsent>
<prevsent>for the penn treebank (marcus et al, 1994), <papid> H94-1020 </papid>and collins (1996),<papid> P96-1025 </papid>whose constituent parser is internally based on probabilities of bilexical dependencies, i.e. dependencies between two words.</prevsent>
</prevsection>
<citsent citstr=" P05-1038 ">
collins (1997)<papid> P97-1003 </papid>s parser and its re implementation and extension by bikel (2002) have by now been applied to variety of languages: english (collins, 1999), czech (collins et al, 1999), <papid> P99-1065 </papid>german (dubey and keller, 2003), <papid> P03-1013 </papid>spanish (cowan and collins, 2005), <papid> H05-1100 </papid>french (arun and keller, 2005), <papid> P05-1038 </papid>chinese (bikel, 2002) and, according to dan bikels web page, arabic.eisner (1996) <papid> C96-1058 </papid>introduced data-driven dependency parser and compared several probability models on (english) penn treebank data.</citsent>
<aftsection>
<nextsent>kudo and matsumoto (2000) <papid> W00-1303 </papid>describe dependency parser for japanese and yamada and matsumoto (2003) an extension for english.</nextsent>
<nextsent>nivres parser has been tested for swedish (nivre et al, 2004), <papid> W04-2407 </papid>english (nivre and scholz, 2004), <papid> C04-1010 </papid>czech (nivre and nilsson, 2005), <papid> P05-1013 </papid>bulgarian (marinov and nivre, 2005) and chinese cheng et al (2005), <papid> I05-3003 </papid>while mcdonalds parser has been applied to english (mcdonald et al, 2005<papid> P05-1012 </papid>a), czech (mcdonald et al, 2005<papid> P05-1012 </papid>b) and, very recently, danish (mcdonald and pereira, 2006).<papid> E06-1011 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5497">
<title id=" W06-2920.xml">conllx shared task on multilingual dependency parsing </title>
<section> previous research.  </section>
<citcontext>
<prevsection>
<prevsent>the same idea was used by magerman (1995), <papid> P95-1037 </papid>who developed the first head table?</prevsent>
<prevsent>for the penn treebank (marcus et al, 1994), <papid> H94-1020 </papid>and collins (1996),<papid> P96-1025 </papid>whose constituent parser is internally based on probabilities of bilexical dependencies, i.e. dependencies between two words.</prevsent>
</prevsection>
<citsent citstr=" C96-1058 ">
collins (1997)<papid> P97-1003 </papid>s parser and its re implementation and extension by bikel (2002) have by now been applied to variety of languages: english (collins, 1999), czech (collins et al, 1999), <papid> P99-1065 </papid>german (dubey and keller, 2003), <papid> P03-1013 </papid>spanish (cowan and collins, 2005), <papid> H05-1100 </papid>french (arun and keller, 2005), <papid> P05-1038 </papid>chinese (bikel, 2002) and, according to dan bikels web page, arabic.eisner (1996) <papid> C96-1058 </papid>introduced data-driven dependency parser and compared several probability models on (english) penn treebank data.</citsent>
<aftsection>
<nextsent>kudo and matsumoto (2000) <papid> W00-1303 </papid>describe dependency parser for japanese and yamada and matsumoto (2003) an extension for english.</nextsent>
<nextsent>nivres parser has been tested for swedish (nivre et al, 2004), <papid> W04-2407 </papid>english (nivre and scholz, 2004), <papid> C04-1010 </papid>czech (nivre and nilsson, 2005), <papid> P05-1013 </papid>bulgarian (marinov and nivre, 2005) and chinese cheng et al (2005), <papid> I05-3003 </papid>while mcdonalds parser has been applied to english (mcdonald et al, 2005<papid> P05-1012 </papid>a), czech (mcdonald et al, 2005<papid> P05-1012 </papid>b) and, very recently, danish (mcdonald and pereira, 2006).<papid> E06-1011 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5498">
<title id=" W06-2920.xml">conllx shared task on multilingual dependency parsing </title>
<section> previous research.  </section>
<citcontext>
<prevsection>
<prevsent>for the penn treebank (marcus et al, 1994), <papid> H94-1020 </papid>and collins (1996),<papid> P96-1025 </papid>whose constituent parser is internally based on probabilities of bilexical dependencies, i.e. dependencies between two words.</prevsent>
<prevsent>collins (1997)<papid> P97-1003 </papid>s parser and its re implementation and extension by bikel (2002) have by now been applied to variety of languages: english (collins, 1999), czech (collins et al, 1999), <papid> P99-1065 </papid>german (dubey and keller, 2003), <papid> P03-1013 </papid>spanish (cowan and collins, 2005), <papid> H05-1100 </papid>french (arun and keller, 2005), <papid> P05-1038 </papid>chinese (bikel, 2002) and, according to dan bikels web page, arabic.eisner (1996) <papid> C96-1058 </papid>introduced data-driven dependency parser and compared several probability models on (english) penn treebank data.</prevsent>
</prevsection>
<citsent citstr=" W00-1303 ">
kudo and matsumoto (2000) <papid> W00-1303 </papid>describe dependency parser for japanese and yamada and matsumoto (2003) an extension for english.</citsent>
<aftsection>
<nextsent>nivres parser has been tested for swedish (nivre et al, 2004), <papid> W04-2407 </papid>english (nivre and scholz, 2004), <papid> C04-1010 </papid>czech (nivre and nilsson, 2005), <papid> P05-1013 </papid>bulgarian (marinov and nivre, 2005) and chinese cheng et al (2005), <papid> I05-3003 </papid>while mcdonalds parser has been applied to english (mcdonald et al, 2005<papid> P05-1012 </papid>a), czech (mcdonald et al, 2005<papid> P05-1012 </papid>b) and, very recently, danish (mcdonald and pereira, 2006).<papid> E06-1011 </papid></nextsent>
<nextsent>the training data derived from the original treebanks(see section 4) and given to the shared task participants was in simple column-based format that is 150 an extension of joakim nivres malt-tab format4 for the shared task and was chosen for its processing simplicity.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5499">
<title id=" W06-2920.xml">conllx shared task on multilingual dependency parsing </title>
<section> previous research.  </section>
<citcontext>
<prevsection>
<prevsent>collins (1997)<papid> P97-1003 </papid>s parser and its re implementation and extension by bikel (2002) have by now been applied to variety of languages: english (collins, 1999), czech (collins et al, 1999), <papid> P99-1065 </papid>german (dubey and keller, 2003), <papid> P03-1013 </papid>spanish (cowan and collins, 2005), <papid> H05-1100 </papid>french (arun and keller, 2005), <papid> P05-1038 </papid>chinese (bikel, 2002) and, according to dan bikels web page, arabic.eisner (1996) <papid> C96-1058 </papid>introduced data-driven dependency parser and compared several probability models on (english) penn treebank data.</prevsent>
<prevsent>kudo and matsumoto (2000) <papid> W00-1303 </papid>describe dependency parser for japanese and yamada and matsumoto (2003) an extension for english.</prevsent>
</prevsection>
<citsent citstr=" W04-2407 ">
nivres parser has been tested for swedish (nivre et al, 2004), <papid> W04-2407 </papid>english (nivre and scholz, 2004), <papid> C04-1010 </papid>czech (nivre and nilsson, 2005), <papid> P05-1013 </papid>bulgarian (marinov and nivre, 2005) and chinese cheng et al (2005), <papid> I05-3003 </papid>while mcdonalds parser has been applied to english (mcdonald et al, 2005<papid> P05-1012 </papid>a), czech (mcdonald et al, 2005<papid> P05-1012 </papid>b) and, very recently, danish (mcdonald and pereira, 2006).<papid> E06-1011 </papid></citsent>
<aftsection>
<nextsent>the training data derived from the original treebanks(see section 4) and given to the shared task participants was in simple column-based format that is 150 an extension of joakim nivres malt-tab format4 for the shared task and was chosen for its processing simplicity.
</nextsent>
<nextsent>all the sentences are in one text file andthey are separated by blank line after each sentence.
</nextsent>
<nextsent>a sentence consists of one or more tokens.
</nextsent>
<nextsent>each token is represented on one line, consisting of 10 fields.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5500">
<title id=" W06-2920.xml">conllx shared task on multilingual dependency parsing </title>
<section> previous research.  </section>
<citcontext>
<prevsection>
<prevsent>collins (1997)<papid> P97-1003 </papid>s parser and its re implementation and extension by bikel (2002) have by now been applied to variety of languages: english (collins, 1999), czech (collins et al, 1999), <papid> P99-1065 </papid>german (dubey and keller, 2003), <papid> P03-1013 </papid>spanish (cowan and collins, 2005), <papid> H05-1100 </papid>french (arun and keller, 2005), <papid> P05-1038 </papid>chinese (bikel, 2002) and, according to dan bikels web page, arabic.eisner (1996) <papid> C96-1058 </papid>introduced data-driven dependency parser and compared several probability models on (english) penn treebank data.</prevsent>
<prevsent>kudo and matsumoto (2000) <papid> W00-1303 </papid>describe dependency parser for japanese and yamada and matsumoto (2003) an extension for english.</prevsent>
</prevsection>
<citsent citstr=" C04-1010 ">
nivres parser has been tested for swedish (nivre et al, 2004), <papid> W04-2407 </papid>english (nivre and scholz, 2004), <papid> C04-1010 </papid>czech (nivre and nilsson, 2005), <papid> P05-1013 </papid>bulgarian (marinov and nivre, 2005) and chinese cheng et al (2005), <papid> I05-3003 </papid>while mcdonalds parser has been applied to english (mcdonald et al, 2005<papid> P05-1012 </papid>a), czech (mcdonald et al, 2005<papid> P05-1012 </papid>b) and, very recently, danish (mcdonald and pereira, 2006).<papid> E06-1011 </papid></citsent>
<aftsection>
<nextsent>the training data derived from the original treebanks(see section 4) and given to the shared task participants was in simple column-based format that is 150 an extension of joakim nivres malt-tab format4 for the shared task and was chosen for its processing simplicity.
</nextsent>
<nextsent>all the sentences are in one text file andthey are separated by blank line after each sentence.
</nextsent>
<nextsent>a sentence consists of one or more tokens.
</nextsent>
<nextsent>each token is represented on one line, consisting of 10 fields.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5501">
<title id=" W06-2920.xml">conllx shared task on multilingual dependency parsing </title>
<section> previous research.  </section>
<citcontext>
<prevsection>
<prevsent>collins (1997)<papid> P97-1003 </papid>s parser and its re implementation and extension by bikel (2002) have by now been applied to variety of languages: english (collins, 1999), czech (collins et al, 1999), <papid> P99-1065 </papid>german (dubey and keller, 2003), <papid> P03-1013 </papid>spanish (cowan and collins, 2005), <papid> H05-1100 </papid>french (arun and keller, 2005), <papid> P05-1038 </papid>chinese (bikel, 2002) and, according to dan bikels web page, arabic.eisner (1996) <papid> C96-1058 </papid>introduced data-driven dependency parser and compared several probability models on (english) penn treebank data.</prevsent>
<prevsent>kudo and matsumoto (2000) <papid> W00-1303 </papid>describe dependency parser for japanese and yamada and matsumoto (2003) an extension for english.</prevsent>
</prevsection>
<citsent citstr=" P05-1013 ">
nivres parser has been tested for swedish (nivre et al, 2004), <papid> W04-2407 </papid>english (nivre and scholz, 2004), <papid> C04-1010 </papid>czech (nivre and nilsson, 2005), <papid> P05-1013 </papid>bulgarian (marinov and nivre, 2005) and chinese cheng et al (2005), <papid> I05-3003 </papid>while mcdonalds parser has been applied to english (mcdonald et al, 2005<papid> P05-1012 </papid>a), czech (mcdonald et al, 2005<papid> P05-1012 </papid>b) and, very recently, danish (mcdonald and pereira, 2006).<papid> E06-1011 </papid></citsent>
<aftsection>
<nextsent>the training data derived from the original treebanks(see section 4) and given to the shared task participants was in simple column-based format that is 150 an extension of joakim nivres malt-tab format4 for the shared task and was chosen for its processing simplicity.
</nextsent>
<nextsent>all the sentences are in one text file andthey are separated by blank line after each sentence.
</nextsent>
<nextsent>a sentence consists of one or more tokens.
</nextsent>
<nextsent>each token is represented on one line, consisting of 10 fields.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5502">
<title id=" W06-2920.xml">conllx shared task on multilingual dependency parsing </title>
<section> previous research.  </section>
<citcontext>
<prevsection>
<prevsent>collins (1997)<papid> P97-1003 </papid>s parser and its re implementation and extension by bikel (2002) have by now been applied to variety of languages: english (collins, 1999), czech (collins et al, 1999), <papid> P99-1065 </papid>german (dubey and keller, 2003), <papid> P03-1013 </papid>spanish (cowan and collins, 2005), <papid> H05-1100 </papid>french (arun and keller, 2005), <papid> P05-1038 </papid>chinese (bikel, 2002) and, according to dan bikels web page, arabic.eisner (1996) <papid> C96-1058 </papid>introduced data-driven dependency parser and compared several probability models on (english) penn treebank data.</prevsent>
<prevsent>kudo and matsumoto (2000) <papid> W00-1303 </papid>describe dependency parser for japanese and yamada and matsumoto (2003) an extension for english.</prevsent>
</prevsection>
<citsent citstr=" I05-3003 ">
nivres parser has been tested for swedish (nivre et al, 2004), <papid> W04-2407 </papid>english (nivre and scholz, 2004), <papid> C04-1010 </papid>czech (nivre and nilsson, 2005), <papid> P05-1013 </papid>bulgarian (marinov and nivre, 2005) and chinese cheng et al (2005), <papid> I05-3003 </papid>while mcdonalds parser has been applied to english (mcdonald et al, 2005<papid> P05-1012 </papid>a), czech (mcdonald et al, 2005<papid> P05-1012 </papid>b) and, very recently, danish (mcdonald and pereira, 2006).<papid> E06-1011 </papid></citsent>
<aftsection>
<nextsent>the training data derived from the original treebanks(see section 4) and given to the shared task participants was in simple column-based format that is 150 an extension of joakim nivres malt-tab format4 for the shared task and was chosen for its processing simplicity.
</nextsent>
<nextsent>all the sentences are in one text file andthey are separated by blank line after each sentence.
</nextsent>
<nextsent>a sentence consists of one or more tokens.
</nextsent>
<nextsent>each token is represented on one line, consisting of 10 fields.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5503">
<title id=" W06-2920.xml">conllx shared task on multilingual dependency parsing </title>
<section> previous research.  </section>
<citcontext>
<prevsection>
<prevsent>collins (1997)<papid> P97-1003 </papid>s parser and its re implementation and extension by bikel (2002) have by now been applied to variety of languages: english (collins, 1999), czech (collins et al, 1999), <papid> P99-1065 </papid>german (dubey and keller, 2003), <papid> P03-1013 </papid>spanish (cowan and collins, 2005), <papid> H05-1100 </papid>french (arun and keller, 2005), <papid> P05-1038 </papid>chinese (bikel, 2002) and, according to dan bikels web page, arabic.eisner (1996) <papid> C96-1058 </papid>introduced data-driven dependency parser and compared several probability models on (english) penn treebank data.</prevsent>
<prevsent>kudo and matsumoto (2000) <papid> W00-1303 </papid>describe dependency parser for japanese and yamada and matsumoto (2003) an extension for english.</prevsent>
</prevsection>
<citsent citstr=" P05-1012 ">
nivres parser has been tested for swedish (nivre et al, 2004), <papid> W04-2407 </papid>english (nivre and scholz, 2004), <papid> C04-1010 </papid>czech (nivre and nilsson, 2005), <papid> P05-1013 </papid>bulgarian (marinov and nivre, 2005) and chinese cheng et al (2005), <papid> I05-3003 </papid>while mcdonalds parser has been applied to english (mcdonald et al, 2005<papid> P05-1012 </papid>a), czech (mcdonald et al, 2005<papid> P05-1012 </papid>b) and, very recently, danish (mcdonald and pereira, 2006).<papid> E06-1011 </papid></citsent>
<aftsection>
<nextsent>the training data derived from the original treebanks(see section 4) and given to the shared task participants was in simple column-based format that is 150 an extension of joakim nivres malt-tab format4 for the shared task and was chosen for its processing simplicity.
</nextsent>
<nextsent>all the sentences are in one text file andthey are separated by blank line after each sentence.
</nextsent>
<nextsent>a sentence consists of one or more tokens.
</nextsent>
<nextsent>each token is represented on one line, consisting of 10 fields.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5515">
<title id=" W06-2920.xml">conllx shared task on multilingual dependency parsing </title>
<section> previous research.  </section>
<citcontext>
<prevsection>
<prevsent>collins (1997)<papid> P97-1003 </papid>s parser and its re implementation and extension by bikel (2002) have by now been applied to variety of languages: english (collins, 1999), czech (collins et al, 1999), <papid> P99-1065 </papid>german (dubey and keller, 2003), <papid> P03-1013 </papid>spanish (cowan and collins, 2005), <papid> H05-1100 </papid>french (arun and keller, 2005), <papid> P05-1038 </papid>chinese (bikel, 2002) and, according to dan bikels web page, arabic.eisner (1996) <papid> C96-1058 </papid>introduced data-driven dependency parser and compared several probability models on (english) penn treebank data.</prevsent>
<prevsent>kudo and matsumoto (2000) <papid> W00-1303 </papid>describe dependency parser for japanese and yamada and matsumoto (2003) an extension for english.</prevsent>
</prevsection>
<citsent citstr=" E06-1011 ">
nivres parser has been tested for swedish (nivre et al, 2004), <papid> W04-2407 </papid>english (nivre and scholz, 2004), <papid> C04-1010 </papid>czech (nivre and nilsson, 2005), <papid> P05-1013 </papid>bulgarian (marinov and nivre, 2005) and chinese cheng et al (2005), <papid> I05-3003 </papid>while mcdonalds parser has been applied to english (mcdonald et al, 2005<papid> P05-1012 </papid>a), czech (mcdonald et al, 2005<papid> P05-1012 </papid>b) and, very recently, danish (mcdonald and pereira, 2006).<papid> E06-1011 </papid></citsent>
<aftsection>
<nextsent>the training data derived from the original treebanks(see section 4) and given to the shared task participants was in simple column-based format that is 150 an extension of joakim nivres malt-tab format4 for the shared task and was chosen for its processing simplicity.
</nextsent>
<nextsent>all the sentences are in one text file andthey are separated by blank line after each sentence.
</nextsent>
<nextsent>a sentence consists of one or more tokens.
</nextsent>
<nextsent>each token is represented on one line, consisting of 10 fields.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5516">
<title id=" W06-2920.xml">conllx shared task on multilingual dependency parsing </title>
<section> treebanks and their conversion.  </section>
<citcontext>
<prevsection>
<prevsent>forthe portuguese treebank, the conversion was complicated by the fact that detailed specification existed which tokens should be the head of which other tokens, e.g. the finite verb must be the head of the subject and the complementzier but the main verb must be the head of the complements and adjuncts.23given that the flor esta sinta?(c)tica does not use traditional vp constituents but rather verbal chunks(consisting mainly of verbs), simple magerman collins-style head table was not sufficient to derive the required dependency structure.
</prevsent>
<prevsent>instead we useda head table that defined several types of heads (syn tactic, semantic) and link table that specified what linked to which type of head.24another problem existed with the dutch tree bank.
</prevsent>
</prevsection>
<citsent citstr=" W96-0102 ">
its original pos tag set is very coarse and the pos and the word stem information is not veryreliable.25 we therefore decided to retag the treebank automatically using the memory-based tagger (mbt) (daelemans et al, 1996) <papid> W96-0102 </papid>which uses very fine-grained tag set.</citsent>
<aftsection>
<nextsent>however, this created problem with multiwords.
</nextsent>
<nextsent>mbt does not have the concept of multi words and therefore tags all of their21many thanks to academia sinica for granting the temporary license for conll-x, to keh-jiann chen for answering our questions and to amit dubey for converting the treebank.22containing rules such as: the head child of vp is the left most to?, or else the left most past tense verb, or else etc. 23eckhard bick, p.c. 24see the conversion script bosque2malt.py and the readme file at the shared task website for details.
</nextsent>
<nextsent>25http://www.let.rug.nl/vannoord/trees/papers/diffs.pdf 153components individually.
</nextsent>
<nextsent>as alpino does not provide an internal structure for multi words, we had to treat multi words as one token.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5520">
<title id=" W06-2920.xml">conllx shared task on multilingual dependency parsing </title>
<section> approaches.  </section>
<citcontext>
<prevsection>
<prevsent>int.
</prevsent>
<prevsent>+, traces mle [me] d2c c2d ? table 2: overview of parsing approaches taken by participating groups (identified by the first three letters of the first author): algorithm (y&m;: yamada and matsumoto (2003), ilp: integer linear programming),vertical direction (irrelevant, mpf: most probable first, bottom-up-spans, bottom-up-trees), horizontal direction (irrelevant, mpf: most probable first, forward, backward), search (optimal, approximate, incremental, best-first exhaustive, deterministic), labeling (interleaved, separate and 1st step, separate and 2nd step), non-projective (ps-pr: through pseudo-projective approach), learner (me: maximum entropy; learners in brackets were explored but not used in the official submission), preprocessing (projectivize, d2c: dependencies to constituents), postprocessing (deprojectivize, c2d: constituents to dependencies), learner parameter optimization per language anon-projectivity through approximate search, used for some languages b20 averaged perceptrons combined into bayes point machine cintroduced single pos tag aux?
</prevsent>
</prevsection>
<citsent citstr=" W06-2932 ">
for all swedish auxiliary and model verbs dby having no projectivity constraint eselective projectivity constraint for japanese fseveral approaches to non-projectivity gusing some feats components to create some finer-grained postag values hreattachment rules for some types of non-projectivity ihead automaton grammar jdetermined the maximally allowed distance for relations kthrough special parser actions lpseudo-projectivizing training data only mgreedy prepend algorithm nbut two separate learners used for unlabeled parsing versus labeling oboth foward and backward, then combined into single tree with cle pbut two separate svms used for unlabeled parsing versus labeling qforward parsing for japanese and turkish, backward for the rest rattaching remaining unattached tokens through exhaustive search (not for submitted runs) 156 sequence classifier can label all children of token together (mcdonald et al, 2006).<papid> W06-2932 </papid></citsent>
<aftsection>
<nextsent>within the third approach, head and deprel can be predicted simultaneously, or in two separate steps (potentially using two different learners).
</nextsent>
<nextsent>5.3 all pairs.
</nextsent>
<nextsent>at the highest level of abstraction, there are two fundamental approaches, which we will call all pairs?
</nextsent>
<nextsent>and stepwise?.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5533">
<title id=" W06-2920.xml">conllx shared task on multilingual dependency parsing </title>
<section> approaches.  </section>
<citcontext>
<prevsection>
<prevsent>if the parse has to be projective, eisners bottom-up-span algorithm (eisner, 1996) <papid> C96-1058 </papid>can be used for the search.</prevsent>
<prevsent>for non-projective parses, mcdonald et al (2005<papid> P05-1012 </papid>b) propose using the chu-liu-edmonds (cle) algorithm (chu and liu, 1965; edmonds, 1967) and mcdonald and pereira(2006) <papid> E06-1011 </papid>describe an approximate extension of eisners algorithm.</prevsent>
</prevsection>
<citsent citstr=" W06-2934 ">
there are also alternatives to mst which allow imposing additional constraints on the dependency structure, e.g. that at most one dependent of token can have certain label, such as sub ject?, see riedel et al (2006) <papid> W06-2934 </papid>and bick (2006).<papid> W06-2923 </papid></citsent>
<aftsection>
<nextsent>by contrast, canisius et al (2006) <papid> W06-2924 </papid>do not even enforce the tree constraint, i.e. they allow cycles.</nextsent>
<nextsent>in variant of the all pairs?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5536">
<title id=" W06-2920.xml">conllx shared task on multilingual dependency parsing </title>
<section> approaches.  </section>
<citcontext>
<prevsection>
<prevsent>if the parse has to be projective, eisners bottom-up-span algorithm (eisner, 1996) <papid> C96-1058 </papid>can be used for the search.</prevsent>
<prevsent>for non-projective parses, mcdonald et al (2005<papid> P05-1012 </papid>b) propose using the chu-liu-edmonds (cle) algorithm (chu and liu, 1965; edmonds, 1967) and mcdonald and pereira(2006) <papid> E06-1011 </papid>describe an approximate extension of eisners algorithm.</prevsent>
</prevsection>
<citsent citstr=" W06-2923 ">
there are also alternatives to mst which allow imposing additional constraints on the dependency structure, e.g. that at most one dependent of token can have certain label, such as sub ject?, see riedel et al (2006) <papid> W06-2934 </papid>and bick (2006).<papid> W06-2923 </papid></citsent>
<aftsection>
<nextsent>by contrast, canisius et al (2006) <papid> W06-2924 </papid>do not even enforce the tree constraint, i.e. they allow cycles.</nextsent>
<nextsent>in variant of the all pairs?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5537">
<title id=" W06-2920.xml">conllx shared task on multilingual dependency parsing </title>
<section> approaches.  </section>
<citcontext>
<prevsection>
<prevsent>for non-projective parses, mcdonald et al (2005<papid> P05-1012 </papid>b) propose using the chu-liu-edmonds (cle) algorithm (chu and liu, 1965; edmonds, 1967) and mcdonald and pereira(2006) <papid> E06-1011 </papid>describe an approximate extension of eisners algorithm.</prevsent>
<prevsent>there are also alternatives to mst which allow imposing additional constraints on the dependency structure, e.g. that at most one dependent of token can have certain label, such as sub ject?, see riedel et al (2006) <papid> W06-2934 </papid>and bick (2006).<papid> W06-2923 </papid></prevsent>
</prevsection>
<citsent citstr=" W06-2924 ">
by contrast, canisius et al (2006) <papid> W06-2924 </papid>do not even enforce the tree constraint, i.e. they allow cycles.</citsent>
<aftsection>
<nextsent>in variant of the all pairs?
</nextsent>
<nextsent>approach, only those pairs of tokens are considered that are not too distant (cani sius et al, 2006).<papid> W06-2924 </papid></nextsent>
<nextsent>5.4 stepwise.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5540">
<title id=" W06-2920.xml">conllx shared task on multilingual dependency parsing </title>
<section> approaches.  </section>
<citcontext>
<prevsection>
<prevsent>5.4 stepwise.
</prevsent>
<prevsent>in stepwise approach, not all pairs are considered.
</prevsent>
</prevsection>
<citsent citstr=" P93-1005 ">
instead, the dependency tree is built stepwise and the decision about what step to take next (e.g. which dependency to insert) can be based on information about, in theory all, previous steps and their results (in the context of generative probabilistic parsing, black et al (1993) <papid> P93-1005 </papid>call this the history).</citsent>
<aftsection>
<nextsent>stepwise approaches can use an explicit probability model over next steps, e.g. generative one (eisner, 1996; <papid> C96-1058 </papid>dreyer et al, 2006), <papid> W06-2929 </papid>or train machine learner to predict those.</nextsent>
<nextsent>the approach can be deterministic (at each point, one step is chosen) or employ various types of search.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5542">
<title id=" W06-2920.xml">conllx shared task on multilingual dependency parsing </title>
<section> approaches.  </section>
<citcontext>
<prevsection>
<prevsent>in stepwise approach, not all pairs are considered.
</prevsent>
<prevsent>instead, the dependency tree is built stepwise and the decision about what step to take next (e.g. which dependency to insert) can be based on information about, in theory all, previous steps and their results (in the context of generative probabilistic parsing, black et al (1993) <papid> P93-1005 </papid>call this the history).</prevsent>
</prevsection>
<citsent citstr=" W06-2929 ">
stepwise approaches can use an explicit probability model over next steps, e.g. generative one (eisner, 1996; <papid> C96-1058 </papid>dreyer et al, 2006), <papid> W06-2929 </papid>or train machine learner to predict those.</citsent>
<aftsection>
<nextsent>the approach can be deterministic (at each point, one step is chosen) or employ various types of search.
</nextsent>
<nextsent>in addition, parsing can be done ina bottom-up-constituent or bottom-up-spans fashion (or in another way, although this was not done in this shared task).
</nextsent>
<nextsent>finally, parsing can start at the first or the last token of sentence.
</nextsent>
<nextsent>when talking about languages that are written from left to right, this distinction is normally referred to as left-to-right versus right-to-left.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5543">
<title id=" W06-2920.xml">conllx shared task on multilingual dependency parsing </title>
<section> approaches.  </section>
<citcontext>
<prevsection>
<prevsent>participants took the following approaches to non-projectivity:?
</prevsent>
<prevsent>ignore, i.e. predict only projective parses.
</prevsent>
</prevsection>
<citsent citstr=" W06-2926 ">
depending on the way the parser is trained, it might be necessary to at least projectivize the training data (chang et al, 2006).?<papid> W06-2926 </papid></citsent>
<aftsection>
<nextsent>always allow non-projective arcs, by not imposing any projectivity constraint (shimizu, 2006; <papid> W06-2936 </papid>canisius et al, 2006).<papid> W06-2924 </papid></nextsent>
<nextsent>allow during parsing under certain conditions, e.g. for tokens with certain properties (riedel et al, 2006; <papid> W06-2934 </papid>bick, 2006) <papid> W06-2923 </papid>or if no alternative projective arc has score above the threshold 157(bick, 2006) <papid> W06-2923 </papid>or if the classifier chooses special action (attardi, 2006) <papid> W06-2922 </papid>or the parser predicts trace (schiehlen and spranger, 2006).<papid> W06-2935 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5544">
<title id=" W06-2920.xml">conllx shared task on multilingual dependency parsing </title>
<section> approaches.  </section>
<citcontext>
<prevsection>
<prevsent>ignore, i.e. predict only projective parses.
</prevsent>
<prevsent>depending on the way the parser is trained, it might be necessary to at least projectivize the training data (chang et al, 2006).?<papid> W06-2926 </papid></prevsent>
</prevsection>
<citsent citstr=" W06-2936 ">
always allow non-projective arcs, by not imposing any projectivity constraint (shimizu, 2006; <papid> W06-2936 </papid>canisius et al, 2006).<papid> W06-2924 </papid></citsent>
<aftsection>
<nextsent>allow during parsing under certain conditions, e.g. for tokens with certain properties (riedel et al, 2006; <papid> W06-2934 </papid>bick, 2006) <papid> W06-2923 </papid>or if no alternative projective arc has score above the threshold 157(bick, 2006) <papid> W06-2923 </papid>or if the classifier chooses special action (attardi, 2006) <papid> W06-2922 </papid>or the parser predicts trace (schiehlen and spranger, 2006).<papid> W06-2935 </papid></nextsent>
<nextsent>introduce through post-processing, e.g. through re attachment rules (bick, 2006) <papid> W06-2923 </papid>or if the change increases overall parse tree probability (mcdonald et al, 2006).<papid> W06-2932 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5553">
<title id=" W06-2920.xml">conllx shared task on multilingual dependency parsing </title>
<section> approaches.  </section>
<citcontext>
<prevsection>
<prevsent>depending on the way the parser is trained, it might be necessary to at least projectivize the training data (chang et al, 2006).?<papid> W06-2926 </papid></prevsent>
<prevsent>always allow non-projective arcs, by not imposing any projectivity constraint (shimizu, 2006; <papid> W06-2936 </papid>canisius et al, 2006).<papid> W06-2924 </papid></prevsent>
</prevsection>
<citsent citstr=" W06-2922 ">
allow during parsing under certain conditions, e.g. for tokens with certain properties (riedel et al, 2006; <papid> W06-2934 </papid>bick, 2006) <papid> W06-2923 </papid>or if no alternative projective arc has score above the threshold 157(bick, 2006) <papid> W06-2923 </papid>or if the classifier chooses special action (attardi, 2006) <papid> W06-2922 </papid>or the parser predicts trace (schiehlen and spranger, 2006).<papid> W06-2935 </papid></citsent>
<aftsection>
<nextsent>introduce through post-processing, e.g. through re attachment rules (bick, 2006) <papid> W06-2923 </papid>or if the change increases overall parse tree probability (mcdonald et al, 2006).<papid> W06-2932 </papid></nextsent>
<nextsent>the pseudo-projective approach (nivre and nilsson, 2005): <papid> P05-1013 </papid>transform non-projective training trees to projective ones but encode the information necessary to make the inverse transformation in the deprel, so that this inverse transformation can also be carried out on the test trees (nivre et al, 2006).<papid> W06-2933 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5554">
<title id=" W06-2920.xml">conllx shared task on multilingual dependency parsing </title>
<section> approaches.  </section>
<citcontext>
<prevsection>
<prevsent>depending on the way the parser is trained, it might be necessary to at least projectivize the training data (chang et al, 2006).?<papid> W06-2926 </papid></prevsent>
<prevsent>always allow non-projective arcs, by not imposing any projectivity constraint (shimizu, 2006; <papid> W06-2936 </papid>canisius et al, 2006).<papid> W06-2924 </papid></prevsent>
</prevsection>
<citsent citstr=" W06-2935 ">
allow during parsing under certain conditions, e.g. for tokens with certain properties (riedel et al, 2006; <papid> W06-2934 </papid>bick, 2006) <papid> W06-2923 </papid>or if no alternative projective arc has score above the threshold 157(bick, 2006) <papid> W06-2923 </papid>or if the classifier chooses special action (attardi, 2006) <papid> W06-2922 </papid>or the parser predicts trace (schiehlen and spranger, 2006).<papid> W06-2935 </papid></citsent>
<aftsection>
<nextsent>introduce through post-processing, e.g. through re attachment rules (bick, 2006) <papid> W06-2923 </papid>or if the change increases overall parse tree probability (mcdonald et al, 2006).<papid> W06-2932 </papid></nextsent>
<nextsent>the pseudo-projective approach (nivre and nilsson, 2005): <papid> P05-1013 </papid>transform non-projective training trees to projective ones but encode the information necessary to make the inverse transformation in the deprel, so that this inverse transformation can also be carried out on the test trees (nivre et al, 2006).<papid> W06-2933 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5561">
<title id=" W06-2920.xml">conllx shared task on multilingual dependency parsing </title>
<section> approaches.  </section>
<citcontext>
<prevsection>
<prevsent>allow during parsing under certain conditions, e.g. for tokens with certain properties (riedel et al, 2006; <papid> W06-2934 </papid>bick, 2006) <papid> W06-2923 </papid>or if no alternative projective arc has score above the threshold 157(bick, 2006) <papid> W06-2923 </papid>or if the classifier chooses special action (attardi, 2006) <papid> W06-2922 </papid>or the parser predicts trace (schiehlen and spranger, 2006).<papid> W06-2935 </papid></prevsent>
<prevsent>introduce through post-processing, e.g. through re attachment rules (bick, 2006) <papid> W06-2923 </papid>or if the change increases overall parse tree probability (mcdonald et al, 2006).<papid> W06-2932 </papid></prevsent>
</prevsection>
<citsent citstr=" W06-2933 ">
the pseudo-projective approach (nivre and nilsson, 2005): <papid> P05-1013 </papid>transform non-projective training trees to projective ones but encode the information necessary to make the inverse transformation in the deprel, so that this inverse transformation can also be carried out on the test trees (nivre et al, 2006).<papid> W06-2933 </papid></citsent>
<aftsection>
<nextsent>5.6 data columns used.
</nextsent>
<nextsent>table 3 shows which column values have been used by participants.
</nextsent>
<nextsent>nobody used the phead/pdeprel column in any way.
</nextsent>
<nextsent>it is likely that those who did not use any of the other columns did so mainly for practical reasons, such as the limited time and/or the difficulty to integrate it into an existing parser.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5572">
<title id=" W06-2920.xml">conllx shared task on multilingual dependency parsing </title>
<section> result analysis.  </section>
<citcontext>
<prevsection>
<prevsent>significance was computed using the official scoring script eval.pl and dan bikels randomized parsing evaluation comparator, which implements stratified shuffling.
</prevsent>
<prevsent>aattardis submitted results contained an unfortunate bug which caused the deprel values of all tokens with head=0 to be an underscore (which is scored as incorrect).
</prevsent>
</prevsection>
<citsent citstr=" W98-0513 ">
using the simple heuristic of assigning the deprel value that most frequently occured with head=0 in training would have resulted in total las of 67.5.(2000), or to the domain-restricted japanese dialogues of the atr corpus (lepage et al, 1998).<papid> W98-0513 </papid>32 other relatively easy?</citsent>
<aftsection>
<nextsent>datasets are portuguese (2nd highest average score but, interestingly, thethird-longest parsing units), bulgarian (3rd), german (4th) and chinese (5th).
</nextsent>
<nextsent>chinese also has the second highest top score33 and chinese parsing units 32unfortunately, both these treebanks need to be bought, so they could not be used for the shared task.
</nextsent>
<nextsent>note also that japanese dependency parsers often operate on bunsetsus?
</nextsent>
<nextsent>instead of words.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5573">
<title id=" W06-2920.xml">conllx shared task on multilingual dependency parsing </title>
<section> result analysis.  </section>
<citcontext>
<prevsection>
<prevsent>for most parsers, their ranking for specific language differs at most few places from their over all ranking.
</prevsent>
<prevsent>there are some outliers though.
</prevsent>
</prevsection>
<citsent citstr=" W06-2930 ">
for example, johansson and nugues (2006) <papid> W06-2930 </papid>and yuret(2006) <papid> W06-2938 </papid>are seven ranks higher for turkish than over all, while riedel et al (2006) <papid> W06-2934 </papid>are five ranks lower.</citsent>
<aftsection>
<nextsent>canisius et al (2006) <papid> W06-2924 </papid>are six and schiehlen and spranger (2006) <papid> W06-2935 </papid>even eight ranks higher for dutch than overall, while riedel et al (2006) <papid> W06-2934 </papid>are six ranks lower for czech and johansson and nugues (2006) <papid> W06-2930 </papid>also six for chinese.</nextsent>
<nextsent>some of the higher rankings could be related to native speaker competence and resulting better parameter tuning but other outliers remain mystery.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5575">
<title id=" W06-2920.xml">conllx shared task on multilingual dependency parsing </title>
<section> result analysis.  </section>
<citcontext>
<prevsection>
<prevsent>for most parsers, their ranking for specific language differs at most few places from their over all ranking.
</prevsent>
<prevsent>there are some outliers though.
</prevsent>
</prevsection>
<citsent citstr=" W06-2938 ">
for example, johansson and nugues (2006) <papid> W06-2930 </papid>and yuret(2006) <papid> W06-2938 </papid>are seven ranks higher for turkish than over all, while riedel et al (2006) <papid> W06-2934 </papid>are five ranks lower.</citsent>
<aftsection>
<nextsent>canisius et al (2006) <papid> W06-2924 </papid>are six and schiehlen and spranger (2006) <papid> W06-2935 </papid>even eight ranks higher for dutch than overall, while riedel et al (2006) <papid> W06-2934 </papid>are six ranks lower for czech and johansson and nugues (2006) <papid> W06-2930 </papid>also six for chinese.</nextsent>
<nextsent>some of the higher rankings could be related to native speaker competence and resulting better parameter tuning but other outliers remain mystery.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5589">
<title id=" W06-1522.xml">modeling and analysis of elliptic coordination by dynamic exploitation of derivation forests in ltag parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the use of this device implies that the resolution mechanism depends on syntactic information, therefore we willnot deal with anaphoric resolutions and scope modifier problems.
</prevsent>
<prevsent>we show how to generate derivation forest described by set of context free rules (similar to (vijay-shanker and weir, 1993)) augmented by stack of current adjunct ions when rule describes spine traversal.
</prevsent>
</prevsection>
<citsent citstr=" C96-2103 ">
we first briefly discuss the linguistic motivations behind the resolution mechanism we propose, then introduce the fusion operation and show how it can be compared to the analysis of (dalrymple et al, 1991) and (steedman, 1990) and we show how it differs from(sarkar and joshi, 1996).<papid> C96-2103 </papid></citsent>
<aftsection>
<nextsent>we assume that the reader is familiar with the lexicalized tree adjoining grammars formalism ((joshi and schabes, 1992)).
</nextsent>
<nextsent>of derivation the ltag formalism provides derivation tree which is strictly the history of the operations needed to build constituent structure, the derivedtree.
</nextsent>
<nextsent>in order to be fully appropriate for semantic inference 1, the derivation tree should display every syntactico-semantic argument and therefore should be graph.
</nextsent>
<nextsent>however to obtain this kind of dependency structure when it is not possible to relyon lexical information, as opposed to (seddahand gaiffe, 2005a), is significantly more complicated.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5594">
<title id=" W06-2929.xml">vine parsing and minimum risk reranking for speed and precision </title>
<section> abstract </section>
<citcontext>
<prevsection>

<prevsent>we describe our entry in the conll-x shared task.
</prevsent>
</prevsection>
<citsent citstr=" P06-2101 ">
the system consists of three phases: probabilistic vine parser (eisner and n. smith, 2005) that produces unlabeled dependency trees, probabilisticrelation-labeling model, and discriminative minimum risk reranker (d. smith and eisner, 2006).<papid> P06-2101 </papid></citsent>
<aftsection>
<nextsent>the system is designed for fast training and decoding andfor high precision.
</nextsent>
<nextsent>we describe sources of cross lingual error and ways to ameliorate them.
</nextsent>
<nextsent>we then provide detailed error analysis of parses produced for sentences in german (much training data) and arabic (little training data).
</nextsent>
<nextsent>standard state-of-the-art parsing systems (e.g., charniak and johnson, 2005) <papid> P05-1022 </papid>typically involve two passes.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5595">
<title id=" W06-2929.xml">vine parsing and minimum risk reranking for speed and precision </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we describe sources of cross lingual error and ways to ameliorate them.
</prevsent>
<prevsent>we then provide detailed error analysis of parses produced for sentences in german (much training data) and arabic (little training data).
</prevsent>
</prevsection>
<citsent citstr=" P05-1022 ">
standard state-of-the-art parsing systems (e.g., charniak and johnson, 2005) <papid> P05-1022 </papid>typically involve two passes.</citsent>
<aftsection>
<nextsent>first, parser produces list of the most likely parse trees under generative, probabilistic model (usually some flavor of pcfg).
</nextsent>
<nextsent>a discriminative reranker then chooses among trees in this list by using an extended feature set (collins, 2000).
</nextsent>
<nextsent>this paradigm has many advantages: pcfgs arefast to train, can be very robust, and perform better as more data is made available; and re rankers train quickly (compared to discriminative models),require few parameters, and permit arbitrary fea tures.we describe such system for dependency parsing.
</nextsent>
<nextsent>our shared task entry is preliminary system developed in only 3 person-weeks, and its accuracy is typically one s.d. below the average across systems and 1020 points below the best system.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5596">
<title id=" W06-2929.xml">vine parsing and minimum risk reranking for speed and precision </title>
<section> unlabeled parsing.  </section>
<citcontext>
<prevsection>
<prevsent>using simple dynamic program to find theminimum-error projective parse, we find that assuming projectivity need not harm accuracy very much (tab.
</prevsent>
<prevsent>1, col. 3).
</prevsent>
</prevsection>
<citsent citstr=" P96-1023 ">
the first component of our system is an unlabeled parser that, given sentence, finds the best unlabeled trees under probabilistic model using bottom-up dynamic programming algorithm.2 the model is probabilistic head automaton grammar(alshawi, 1996) <papid> P96-1023 </papid>that assumes conditional indepen 1we used words and fine tags in our parser and labeler, with coarse tags in one backoff model.</citsent>
<aftsection>
<nextsent>other features are used in reranking; we never used the given morphological features or the projective?
</nextsent>
<nextsent>annotations offered in the training data.
</nextsent>
<nextsent>2the execution model we use is best-first, exhaustive search,as described in eisner et al (2004).<papid> P04-3032 </papid></nextsent>
<nextsent>all of our dynamic programming algorithms are implemented concisely in the dyna language.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5597">
<title id=" W06-2929.xml">vine parsing and minimum risk reranking for speed and precision </title>
<section> unlabeled parsing.  </section>
<citcontext>
<prevsection>
<prevsent>other features are used in reranking; we never used the given morphological features or the projective?
</prevsent>
<prevsent>annotations offered in the training data.
</prevsent>
</prevsection>
<citsent citstr=" P04-3032 ">
2the execution model we use is best-first, exhaustive search,as described in eisner et al (2004).<papid> P04-3032 </papid></citsent>
<aftsection>
<nextsent>all of our dynamic programming algorithms are implemented concisely in the dyna language.
</nextsent>
<nextsent>201 b` br projective oracle (b ` , r )-vine oracle 20-best unlabeled oracle 1-best unlabeled unlabeled, reranked 2050-best labeled oracle 11-best labeled reranked (labeled) (unlabeled) (non-$ unl.
</nextsent>
<nextsent>recall) (non-$ unl.
</nextsent>
<nextsent>precision) arabic 10 4 99.8 90.7 71.5 68.1 68.7 59.7 52.0 53.4 68.5 63.4 76.0 bulgarian 5 4 99.6 90.7 86.4 80.1 80.5 85.1 73.0 74.8 82.0 74.3 86.3 chinese 4 4 100.0 93.1 89.9 79.4 77.7 88.6 72.6 71.6 77.6 61.4 80.8 czech 6 4 97.8 90.5 79.2 70.3 71.5 72.8 58.1 60.5 70.7 64.8 75.7 danish 5 4 99.2 91.4 84.6 77.7 78.6 79.3 65.5 66.6 77.5 71.4 83.4 dutch 6 5 94.6 88.3 77.5 67.9 68.8 73.6 59.4 61.6 68.3 60.4 73.0 german 8 7 98.8 90.9 83.4 75.5 76.2 82.3 70.1 71.0 77.0 70.2 82.9 japanese 4 1 99.2 92.2 90.7 86.3 85.1 89.4 81.6 82.9 86.0 68.5 91.5 portuguese 5 5 98.8 91.5 85.9 81.4 82.5 83.7 73.4 75.3 82.4 76.2 87.0 slovene 6 4 98.5 91.7 80.5 72.0 73.3 72.8 57.5 58.7 72.9 66.3 78.5 spanish 5 6 100.0 91.2 77.3 71.5 72.6 74.9 66.2 67.6 72.9 69.3 80.7 swedish 4 5 99.7 94.0 87.5 79.3 79.6 81.0 65.5 67.6 79.5 72.6 83.3 turkish 6 1 98.6 89.5 73.0 61.0 61.8 64.4 44.9 46.1 60.5 48.5 61.6 parser reranker labeler reranker 1 2 3 4 5 6 7 8 9 10 11 12 13 table 1: parameters and performance on test data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5598">
<title id=" W06-2929.xml">vine parsing and minimum risk reranking for speed and precision </title>
<section> unlabeled parsing.  </section>
<citcontext>
<prevsection>
<prevsent>italics mark the few cases where the reranker increased error rate.
</prevsent>
<prevsent>columns 810 show labeled accuracy; column 10 gives the final shared task evaluation scores.
</prevsent>
</prevsection>
<citsent citstr=" P99-1059 ">
dence between the left yield and the right yield of given head, given the head (eisner, 1997).3 the best known parsing algorithm for such model is o(n3) (eisner and satta, 1999).<papid> P99-1059 </papid></citsent>
<aftsection>
<nextsent>the -best list is generated using algorithm 3 of huang and chiang (2005).<papid> W05-1506 </papid></nextsent>
<nextsent>3.1 vine parsing (dependency length bounds).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5599">
<title id=" W06-2929.xml">vine parsing and minimum risk reranking for speed and precision </title>
<section> unlabeled parsing.  </section>
<citcontext>
<prevsection>
<prevsent>columns 810 show labeled accuracy; column 10 gives the final shared task evaluation scores.
</prevsent>
<prevsent>dence between the left yield and the right yield of given head, given the head (eisner, 1997).3 the best known parsing algorithm for such model is o(n3) (eisner and satta, 1999).<papid> P99-1059 </papid></prevsent>
</prevsection>
<citsent citstr=" W05-1506 ">
the -best list is generated using algorithm 3 of huang and chiang (2005).<papid> W05-1506 </papid></citsent>
<aftsection>
<nextsent>3.1 vine parsing (dependency length bounds).
</nextsent>
<nextsent>following eisner and n. smith (2005), we also im pose bound on the string distance between every 3to empirically test this assumption across languages, we measured the mutual information between different features ofyleft(j) and yright(j), given xj .
</nextsent>
<nextsent>(mutual information is statistic that equals zero iff conditional independence holds.)
</nextsent>
<nextsent>a detailed discussion, while interesting, is omitted for space, but we highlight some of our findings.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5600">
<title id=" W06-2929.xml">vine parsing and minimum risk reranking for speed and precision </title>
<section> unlabeled parsing.  </section>
<citcontext>
<prevsection>
<prevsent>the probability model predicts, for each parent word xj , {xi}iyleft (j) and {xi}iyright (j).
</prevsent>
<prevsent>an advanta geof head automaton grammars is that, forgiven parent node xj , the children on the same side, yleft(j), 202for example, can depend on each other (cf.
</prevsent>
</prevsection>
<citsent citstr=" H05-1066 ">
mcdonald et al, 2005).<papid> H05-1066 </papid></citsent>
<aftsection>
<nextsent>child nodes in our model are generated outward, conditional on the parent and the most recent same-side sibling (mrsss).
</nextsent>
<nextsent>this increases our parsers theoretical runtime to o(n(b3` + b3r )), which we found was quite manageable.
</nextsent>
<nextsent>let pary : {1, 2, ..., n} ? {0, 1, ..., n} map each node to its parent in y. let predy : {1, 2, ..., n} ? {?, 1, 2, ..., n} map each node to the mrsss in if it exists and ? otherwise.
</nextsent>
<nextsent>let = |i ? j| if is is parent.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5603">
<title id=" W06-3115.xml">ntt system description for the wmt2006 shared task </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we also reporta phrase/rule extraction technique differentiating tokenization of corpora.
</prevsent>
<prevsent>we contrasted two translation methods for the workshop on statistical machine translation (wmt2006) shared-task.
</prevsent>
</prevsection>
<citsent citstr=" N03-1017 ">
one is phrase-based translation in which phrasal unit is employed for translation (koehn et al, 2003).<papid> N03-1017 </papid></citsent>
<aftsection>
<nextsent>the other is hierarchical phrase-based translation in which translation is realized as set of paired production rules (chiang, 2005).<papid> P05-1033 </papid></nextsent>
<nextsent>section 2 discusses those two models and details extraction algorithms, decoding algorithms and feature functions.we also explored three types of corpus preprocessing in section 3.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5606">
<title id=" W06-3115.xml">ntt system description for the wmt2006 shared task </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we contrasted two translation methods for the workshop on statistical machine translation (wmt2006) shared-task.
</prevsent>
<prevsent>one is phrase-based translation in which phrasal unit is employed for translation (koehn et al, 2003).<papid> N03-1017 </papid></prevsent>
</prevsection>
<citsent citstr=" P05-1033 ">
the other is hierarchical phrase-based translation in which translation is realized as set of paired production rules (chiang, 2005).<papid> P05-1033 </papid></citsent>
<aftsection>
<nextsent>section 2 discusses those two models and details extraction algorithms, decoding algorithms and feature functions.we also explored three types of corpus preprocessing in section 3.
</nextsent>
<nextsent>as expected, different tokenization would lead to different word alignments which, in turn, resulted in the divergence of the extracted phrase/rule size.
</nextsent>
<nextsent>in our method, phrase/rule translation pairs extracted from three distinctly word-aligned corpora are aggregated intoone large phrase/rule translation table.
</nextsent>
<nextsent>the experiments and the final translation results are presented in section 4.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5607">
<title id=" W06-3115.xml">ntt system description for the wmt2006 shared task </title>
<section> translation models.  </section>
<citcontext>
<prevsection>
<prevsent>in our method, phrase/rule translation pairs extracted from three distinctly word-aligned corpora are aggregated intoone large phrase/rule translation table.
</prevsent>
<prevsent>the experiments and the final translation results are presented in section 4.
</prevsent>
</prevsection>
<citsent citstr=" P02-1038 ">
we used log-linear approach (och and ney, 2002) <papid> P02-1038 </papid>in which foreign language sentence j1 =f1, f2, ... fj is translated into another language, i.e. english, ei1 = e1, e2, ..., ei by seeking maximum likelihood solution of ei1 = argmax ei1 pr(ei1| j1 ) (1) = argmax ei1 exp ( m=1 mhm(ei1, j1 ) ) ? e? ? 1 exp ( m=1 mhm(e? ? 1 , j1 ) )(2) in this framework, the posterior probability pr(ei1| j1 ) is directly maximized using log-linear combination of feature functions hm(ei1, j1 ), such as ngram language model or translation model.</citsent>
<aftsection>
<nextsent>when decoding, the denominator is dropped since it depends only on j1 . feature function scaling factors are optimized based on maximum likelihood approach (och and ney, 2002) <papid> P02-1038 </papid>or on direct error minimization approach (och, 2003).<papid> P03-1021 </papid></nextsent>
<nextsent>this modeling allows the integration of various feature functions depending on the scenario of how translation is constituted.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5611">
<title id=" W06-3115.xml">ntt system description for the wmt2006 shared task </title>
<section> translation models.  </section>
<citcontext>
<prevsection>
<prevsent>the experiments and the final translation results are presented in section 4.
</prevsent>
<prevsent>we used log-linear approach (och and ney, 2002) <papid> P02-1038 </papid>in which foreign language sentence j1 =f1, f2, ... fj is translated into another language, i.e. english, ei1 = e1, e2, ..., ei by seeking maximum likelihood solution of ei1 = argmax ei1 pr(ei1| j1 ) (1) = argmax ei1 exp ( m=1 mhm(ei1, j1 ) ) ? e? ? 1 exp ( m=1 mhm(e? ? 1 , j1 ) )(2) in this framework, the posterior probability pr(ei1| j1 ) is directly maximized using log-linear combination of feature functions hm(ei1, j1 ), such as ngram language model or translation model.</prevsent>
</prevsection>
<citsent citstr=" P03-1021 ">
when decoding, the denominator is dropped since it depends only on j1 . feature function scaling factors are optimized based on maximum likelihood approach (och and ney, 2002) <papid> P02-1038 </papid>or on direct error minimization approach (och, 2003).<papid> P03-1021 </papid></citsent>
<aftsection>
<nextsent>this modeling allows the integration of various feature functions depending on the scenario of how translation is constituted.
</nextsent>
<nextsent>in phrase-based statistical translation (koehn et al, 2003), <papid> N03-1017 </papid>bilingual text is decomposed as kphrase translation pairs (e1, fa1), (e2, fa2 ), ...: the in put foreign sentence is segmented into phrases k1 , 122mapped into corresponding english ek1 , then, reordered to form the output english sentence according to phrase alignment index mapping a?.in hierarchical phrase-based translation (chi ang, 2005), <papid> P05-1033 </papid>translation is modeled after weighted synchronous-cfg consisting of production rules whose right-hand side is paired (aho and ullman, 1969): ? ??, ?,where is non-terminal, ? and ? are strings of terminals and non-terminals.</nextsent>
<nextsent>is one-to-one correspondence for the non-terminals appeared in ? and ?.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5616">
<title id=" W06-3115.xml">ntt system description for the wmt2006 shared task </title>
<section> translation models.  </section>
<citcontext>
<prevsection>
<prevsent>2.1 phrase/rule extraction.
</prevsent>
<prevsent>the phrase extraction algorithm is based on those presented by koehn et al (2003).<papid> N03-1017 </papid></prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
first, many to-many word alignments are induced by running one-to-many word alignment model, such as giza++ (och and ney, 2003), <papid> J03-1002 </papid>in both directions and by combining the results based on heuristic (och and ney, 2004).<papid> J04-4002 </papid></citsent>
<aftsection>
<nextsent>second, phrase translation pairs are extracted from the word aligned corpus(koehn et al, 2003).<papid> N03-1017 </papid></nextsent>
<nextsent>the method exhaustively extracts phrase pairs ( j+mj , ei+ni ) from sentence pair( j1 , ei1) that do not violate the word alignment constraints a.in the hierarchical phrase-based model, production rules are accumulated by computing holes?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5617">
<title id=" W06-3115.xml">ntt system description for the wmt2006 shared task </title>
<section> translation models.  </section>
<citcontext>
<prevsection>
<prevsent>2.1 phrase/rule extraction.
</prevsent>
<prevsent>the phrase extraction algorithm is based on those presented by koehn et al (2003).<papid> N03-1017 </papid></prevsent>
</prevsection>
<citsent citstr=" J04-4002 ">
first, many to-many word alignments are induced by running one-to-many word alignment model, such as giza++ (och and ney, 2003), <papid> J03-1002 </papid>in both directions and by combining the results based on heuristic (och and ney, 2004).<papid> J04-4002 </papid></citsent>
<aftsection>
<nextsent>second, phrase translation pairs are extracted from the word aligned corpus(koehn et al, 2003).<papid> N03-1017 </papid></nextsent>
<nextsent>the method exhaustively extracts phrase pairs ( j+mj , ei+ni ) from sentence pair( j1 , ei1) that do not violate the word alignment constraints a.in the hierarchical phrase-based model, production rules are accumulated by computing holes?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5623">
<title id=" W06-3115.xml">ntt system description for the wmt2006 shared task </title>
<section> translation models.  </section>
<citcontext>
<prevsection>
<prevsent>the score for the newly generated hypothesis is updated by combining the scores of feature functions described in section 2.3.
</prevsent>
<prevsent>the english side of the phrase is simplyconcatenated to form new prefix of english sentence.
</prevsent>
</prevsection>
<citsent citstr=" P06-1098 ">
in the hierarchical phrase-based model, decoding is realized as an earley-style top-down parser on the foreign language side with beam search strategy synchronized with the cardinality of already translated foreign words (watanabe et al, 2006).<papid> P06-1098 </papid></citsent>
<aftsection>
<nextsent>thema jor difference to the phrase-based models decoder is the handling of non-terminals, or holes, in each rule.
</nextsent>
<nextsent>2.3 feature functions.
</nextsent>
<nextsent>our phrase-based model uses standard pharaoh feature functions listed as follows (koehn et al, 2003):?<papid> N03-1017 </papid></nextsent>
<nextsent>relative-count based phrase translation probabilities in both directions.?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5627">
<title id=" W06-3115.xml">ntt system description for the wmt2006 shared task </title>
<section> translation models.  </section>
<citcontext>
<prevsection>
<prevsent>the number of words in english-side and the number of phrases that constitute translation.
</prevsent>
<prevsent>for details, please refer to koehn et al (2003).<papid> N03-1017 </papid></prevsent>
</prevsection>
<citsent citstr=" N04-1021 ">
in addition, we added three feature functions to restrict reorderings and to represent globalized in sertion/deletion of words: ? lexicalized reordering feature function scores whether phrase translation pair is mono toni cally translated or not (och et al, 2004): <papid> N04-1021 </papid>hlex(ak1 | k1 , ek1 ) = log ? k=1 pr(k | fak , ek) (3) where = 1 iff ak ? ak1 = 1 otherwise = 0.</citsent>
<aftsection>
<nextsent>? deletion feature function penalizes words that do not constitute translation according to 123 table 1: number of word alignment by different preprocessings.
</nextsent>
<nextsent>de-en es-en fr-en en-de en-es en-fr lower 17,660,187 17,221,890 16,176,075 17,596,764 17,237,723 16,220,520 stem 17,110,890 16,601,306 15,635,900 17,052,808 16,597,274 15,658,940 prefix4 16,975,398 16,540,767 15,610,319 16,936,710 16,530,810 15,613,755 intersection 12,203,979 12,677,192 11,645,404 12,218,997 12,688,773 11,653,242 union 23,186,379 21,709,212 20,760,539 23,066,052 21,698,267 20,789,570 table 2: number of phrases extracted from differently preprocessed corpora.
</nextsent>
<nextsent>de-en es-en fr-en en-de en-es en-fr lower 37,711,217 61,161,868 56,025,918 38,142,663 60,619,435 55,198,497 stem 46,550,101 75,610,696 68,210,968 46,749,195 75,473,313 67,733,045 prefix4 53,429,522 78,193,818 70,514,377 53,647,033 78,223,236 70,378,947 merged 80,260,191 111,153,303 103,523,206 80,666,414 110,787,982 102,940,840 lexicon model t( |e) (bender et al, 2004): hdel(ei1, j1 ) = ? j=1 [ max 0ii t( j|ei)   del ] (4) the deletion model simply counts the number of words whose lexicon model probability is lower than threshold del. likewise, we also added an insertion model hins(ei1, j1 ) that penalizes the spuriously inserted english words using lexicon model t(e| ).for the hierarchical phrase-based model, we employed the same feature set except for the distortion model and the lexicalized reordering model.
</nextsent>
<nextsent>alignment we prepared three kinds of corpora differentiated by tokenization methods.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5632">
<title id=" W07-0403.xml">inversion transduction grammar for joint phrasal translation modeling </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>nearly all current decoding methods have shifted to phrasal representations, gaining the ability to handle non compositional translations, but also allowing the decoder to memorize phenomena such as monolingual agreement and short-range movement, taking pressure off of language and distortion models.despite the success of phrasal decoders, knowledge acquisition for translation generally begins with word-level analysis of the training text, taking the form of word alignment.
</prevsent>
<prevsent>attempts to apply the same statistical analysis used at the word level in phrasal setting have met with limited success, held back by the sheer size of phrasal alignment space.
</prevsent>
</prevsection>
<citsent citstr=" W06-3123 ">
hybrid methods that combine well-founded statistical analysis with high-confidence word-level alignments have made some headway (birch et al,2006), <papid> W06-3123 </papid>but suffer from the daunting task of heuris tically exploring still very large alignment space.in the meantime, synchronous parsing methods efficiently process the same bitext phrases while building their bilingual constituents, but continue to be employed primarily for word-to-word analysis (wu, 1997).<papid> J97-3002 </papid></citsent>
<aftsection>
<nextsent>in this paper we unify the probability models for phrasal translation with the algorithms for synchronous parsing, harnessing the benefits of bothto create statistically and algorithmically well founded method for phrasal analysis of bitext.section 2 begins by outlining the phrase extraction system we intend to replace and the two methods we combine to do so: the joint phrasal translation model (jptm) and inversion transduction grammar (itg).
</nextsent>
<nextsent>section 3 describes our proposed solution, phrasal itg.
</nextsent>
<nextsent>section 4 describes how to apply our phrasal itg, both as translation model and as phrasal word-aligner.
</nextsent>
<nextsent>section 5 tests our system in both these capacities, while section 6 concludes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5633">
<title id=" W07-0403.xml">inversion transduction grammar for joint phrasal translation modeling </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>nearly all current decoding methods have shifted to phrasal representations, gaining the ability to handle non compositional translations, but also allowing the decoder to memorize phenomena such as monolingual agreement and short-range movement, taking pressure off of language and distortion models.despite the success of phrasal decoders, knowledge acquisition for translation generally begins with word-level analysis of the training text, taking the form of word alignment.
</prevsent>
<prevsent>attempts to apply the same statistical analysis used at the word level in phrasal setting have met with limited success, held back by the sheer size of phrasal alignment space.
</prevsent>
</prevsection>
<citsent citstr=" J97-3002 ">
hybrid methods that combine well-founded statistical analysis with high-confidence word-level alignments have made some headway (birch et al,2006), <papid> W06-3123 </papid>but suffer from the daunting task of heuris tically exploring still very large alignment space.in the meantime, synchronous parsing methods efficiently process the same bitext phrases while building their bilingual constituents, but continue to be employed primarily for word-to-word analysis (wu, 1997).<papid> J97-3002 </papid></citsent>
<aftsection>
<nextsent>in this paper we unify the probability models for phrasal translation with the algorithms for synchronous parsing, harnessing the benefits of bothto create statistically and algorithmically well founded method for phrasal analysis of bitext.section 2 begins by outlining the phrase extraction system we intend to replace and the two methods we combine to do so: the joint phrasal translation model (jptm) and inversion transduction grammar (itg).
</nextsent>
<nextsent>section 3 describes our proposed solution, phrasal itg.
</nextsent>
<nextsent>section 4 describes how to apply our phrasal itg, both as translation model and as phrasal word-aligner.
</nextsent>
<nextsent>section 5 tests our system in both these capacities, while section 6 concludes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5635">
<title id=" W07-0403.xml">inversion transduction grammar for joint phrasal translation modeling </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>section 5 tests our system in both these capacities, while section 6 concludes.
</prevsent>
<prevsent>2.1 phrase table extraction.
</prevsent>
</prevsection>
<citsent citstr=" N03-1017 ">
phrasal decoders require phrase table (koehn et al., 2003), <papid> N03-1017 </papid>which contains bilingual phrase pairs and 17scores indicating their utility.</citsent>
<aftsection>
<nextsent>the surface heuristic is the most popular method for phrase-table construction.
</nextsent>
<nextsent>it extracts all consistent phrase pairs from word-aligned bitext (koehn et al, 2003).<papid> N03-1017 </papid></nextsent>
<nextsent>the word alignment provides bilingual links, indicating translation relationships between words.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5639">
<title id=" W07-0403.xml">inversion transduction grammar for joint phrasal translation modeling </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>consistency is defined so that alignment links are never broken by phrase boundaries.
</prevsent>
<prevsent>for each token inconsistent phrase pair p?, all tokens linked tow by the alignment must also be included in p?.
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
each consistent phrase pair is counted as occurring once per sentence pair.the scores for the extracted phrase pairs are provided by normalizing these flat counts according to common english or foreign components, producing the conditional distributions p(f? |e?) and p(e?|f?).the surface heuristic can define consistency according to any word alignment; but most often, the alignment is provided by giza++ (och and ney, 2003).<papid> J03-1002 </papid></citsent>
<aftsection>
<nextsent>this alignment system is powered by the ibm translation models (brown et al, 1993), <papid> J93-2003 </papid>inwhich one sentence generates the other.</nextsent>
<nextsent>these models produce only one-to-many alignments: each generated token can participate in at most one link.many-to-many alignments can be created by combining two giza++ alignments, one where english generates foreign and another with those roles reversed (och and ney, 2003).<papid> J03-1002 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5641">
<title id=" W07-0403.xml">inversion transduction grammar for joint phrasal translation modeling </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>for each token inconsistent phrase pair p?, all tokens linked tow by the alignment must also be included in p?.
</prevsent>
<prevsent>each consistent phrase pair is counted as occurring once per sentence pair.the scores for the extracted phrase pairs are provided by normalizing these flat counts according to common english or foreign components, producing the conditional distributions p(f? |e?) and p(e?|f?).the surface heuristic can define consistency according to any word alignment; but most often, the alignment is provided by giza++ (och and ney, 2003).<papid> J03-1002 </papid></prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
this alignment system is powered by the ibm translation models (brown et al, 1993), <papid> J93-2003 </papid>inwhich one sentence generates the other.</citsent>
<aftsection>
<nextsent>these models produce only one-to-many alignments: each generated token can participate in at most one link.many-to-many alignments can be created by combining two giza++ alignments, one where english generates foreign and another with those roles reversed (och and ney, 2003).<papid> J03-1002 </papid></nextsent>
<nextsent>combination approaches begin with the intersection of the two alignments, and add links from the union heuris tically.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5646">
<title id=" W07-0403.xml">inversion transduction grammar for joint phrasal translation modeling </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>sampling is employed when the alignment distributions cannot be calculated efficiently.
</prevsent>
<prevsent>this statistically-motivated proces sis much more appealing than the flat counting described in section 2.1, but it does not directly include phrases.
</prevsent>
</prevsection>
<citsent citstr=" W02-1018 ">
the joint phrasal translation model (marcu and wong, 2002), <papid> W02-1018 </papid>or jptm, applies the same statistical techniques from the ibm models in phrasal setting.the jptm is designed according to generative process where both languages are generated simultaneously.</citsent>
<aftsection>
<nextsent>first, bag of concepts, or cepts, is generated.
</nextsent>
<nextsent>each ci ? corresponds to bilingual phrase pair, ci = (ei, fi).
</nextsent>
<nextsent>these contiguous phrases are permuted in each language to create two sequences of phrases.
</nextsent>
<nextsent>initially, marcu and wong assume that the number of cepts, as well as the phrase orderings, are drawn from uniform distributions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5651">
<title id=" W07-0403.xml">inversion transduction grammar for joint phrasal translation modeling </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>.?, are output in reverse order in the foreign stream only.the work described here uses the binary bracketing itg, which has single non-terminal: ? [aa] | aa?
</prevsent>
<prevsent>| e/f (2)this grammar admits an efficient bitext parsing algorithm, and holds no language-specific biases.
</prevsent>
</prevsection>
<citsent citstr=" N03-1021 ">
(2) cannot represent all possible permutations of concepts that may occur during translation, because some permutations will require discontinuous constituents (melamed, 2003).<papid> N03-1021 </papid></citsent>
<aftsection>
<nextsent>this itg constraint is characterized by the two forbidden structures shown in figure 1 (wu, 1997).<papid> J97-3002 </papid></nextsent>
<nextsent>empirical studies suggest that only small percentage of human translations violate these constraints (cherry and lin, 2006).<papid> E06-1019 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5654">
<title id=" W07-0403.xml">inversion transduction grammar for joint phrasal translation modeling </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>(2) cannot represent all possible permutations of concepts that may occur during translation, because some permutations will require discontinuous constituents (melamed, 2003).<papid> N03-1021 </papid></prevsent>
<prevsent>this itg constraint is characterized by the two forbidden structures shown in figure 1 (wu, 1997).<papid> J97-3002 </papid></prevsent>
</prevsection>
<citsent citstr=" E06-1019 ">
empirical studies suggest that only small percentage of human translations violate these constraints (cherry and lin, 2006).<papid> E06-1019 </papid></citsent>
<aftsection>
<nextsent>e1 e2 e3 e4 f1 f2 f3 f4 f1 f2 f3 f4 e1 e2 e3 e4 figure 1: the two itg forbidden structures.
</nextsent>
<nextsent>calmez vous a m o n calmez vous a m o n calmez vous a m o n a) a?[aa] b) a? aa  c) ae/f figure 2: three ways in which phrasal itg can analyze multi-word span or phrase.
</nextsent>
<nextsent>stochastic itgs are parameterized like their pcfg counterparts (wu, 1997); <papid> J97-3002 </papid>productions ? are assigned probability pr(x|a).</nextsent>
<nextsent>these parameters can be learned from sentence-aligned bi text using the em algorithm.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5657">
<title id=" W07-0403.xml">inversion transduction grammar for joint phrasal translation modeling </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>stochastic itgs are parameterized like their pcfg counterparts (wu, 1997); <papid> J97-3002 </papid>productions ? are assigned probability pr(x|a).</prevsent>
<prevsent>these parameters can be learned from sentence-aligned bi text using the em algorithm.</prevsent>
</prevsection>
<citsent citstr=" C04-1060 ">
the expectation taskof counting productions weighted by their probability is handled with dynamic programming, using the inside-outside algorithm extended to bitext (zhang and gildea, 2004).<papid> C04-1060 </papid></citsent>
<aftsection>
<nextsent>this paper introduces phrasal itg; in doing so,we combine itg with the jptm.
</nextsent>
<nextsent>itg parsing algorithms consider every possible two-dimensional span of bitext, each corresponding to bilingual phrase pair.
</nextsent>
<nextsent>each multi-token span is analyzed interms of how it could be built from smaller spans using straight or inverted production, as is illustrated in figures 2 (a) and (b).
</nextsent>
<nextsent>to extend itg to phrasal setting, we add third option for span analysis: that the span under consideration might have been drawn directly from the lexicon.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5659">
<title id=" W07-0403.xml">inversion transduction grammar for joint phrasal translation modeling </title>
<section> itg as phrasal translation model.  </section>
<citcontext>
<prevsection>
<prevsent>our approach differs from previous attempts to use itgs for phrasal bitext analysis.
</prevsent>
<prevsent>wu (1997)<papid> J97-3002 </papid>used binary bracketing itg to segment sen 19 tence while simultaneously word-aligning it to its translation, but the model was trained heuristic ally with fixed segmentation.</prevsent>
</prevsection>
<citsent citstr=" W05-0835 ">
vilar and vidal (2005) <papid> W05-0835 </papid>used itg-like dynamic programming to drive both training and alignment for their recursive translation model, but they employed conditional model that did not maintain phrasal lexicon.</citsent>
<aftsection>
<nextsent>instead, they scored phrase pairs using ibm model 1.
</nextsent>
<nextsent>our phrasal itg is quite similar to the jptm.both models are trained with em, and both employ generative stories that create sentence and its translation simultaneously.
</nextsent>
<nextsent>the similarities become more apparent when we consider the canonical-form binary-bracketing itg (wu, 1997) <papid> J97-3002 </papid>shown here: ? | | a ? [ab] | [bb] | [cb] | [ac] | [bc] | [cc] ? aa?</nextsent>
<nextsent>| ba?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5661">
<title id=" W07-0403.xml">inversion transduction grammar for joint phrasal translation modeling </title>
<section> itg as phrasal translation model.  </section>
<citcontext>
<prevsection>
<prevsent>there are o(n4) of these spans, and each analysis takes o(n2) time.
</prevsent>
<prevsent>an effective approach to speeding up itg algorithms is to eliminate unlikely spans as preprocessing step, assigning them 0 probability and saving the time spent processing them.
</prevsent>
</prevsection>
<citsent citstr=" P05-1059 ">
past approaches have pruned spans using ibm model 1 probability estimates (zhang and gildea, 2005) <papid> P05-1059 </papid>or using agreement with an existing parse tree (cherry and lin, 2006).<papid> E06-1019 </papid>the former is referred to as tic-tac-toe pruning be cause it uses both inside and outside estimates.we propose new itg pruning method that leverages high-confidence links by pruning all spans that are inconsistent with provided alignment.</citsent>
<aftsection>
<nextsent>this is similar to the constraint used in the c-jptm,but we do not just eliminate those spans as potential phrase-to-phrase links: we never consider any itg parse that builds non-terminal over pruned span.2 this fixed-link pruning will speed up both viterbi alignment and em training by reducing the number of analyzed spans, and so long as we trust 2birch et al (2006) <papid> W06-3123 </papid>re-introduce inconsistent phrase-pairs in cases where the sentence pair could not be aligned otherwise.we allow links to ? to handle these situations, completely eliminating the pruned spans from our alignment space.</nextsent>
<nextsent>20 our high-confidence links, it will do so harmlessly.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5666">
<title id=" W07-0403.xml">inversion transduction grammar for joint phrasal translation modeling </title>
<section> itg as phrasal translation model.  </section>
<citcontext>
<prevsection>
<prevsent>sentence pairs containing non-itg translations will tend to have high-confidence links that are also not itg-compatible.
</prevsent>
<prevsent>our em learner will simply skip these sentence pairs during training, avoiding pollution of our training data.
</prevsent>
</prevsection>
<citsent citstr=" N06-1033 ">
we can use linear-time algorithm (zhang et al, 2006) <papid> N06-1033 </papid>to detect non-itg movement in our high-confidence links, and remove the offending sentence pairs fromour training corpus.</citsent>
<aftsection>
<nextsent>this results in only minor reduction in training data; in our french-english training set, we lose less than 1%.
</nextsent>
<nextsent>in the experiments described in section 5, all systems that do not use itg will take advantage of the complete training set.
</nextsent>
<nextsent>any phrasal translation model can be used for twotasks: translation modeling and phrasal word alignment.
</nextsent>
<nextsent>previous work on jptm has focused on only the first task.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5675">
<title id=" W07-0403.xml">inversion transduction grammar for joint phrasal translation modeling </title>
<section> experiments and results.  </section>
<citcontext>
<prevsection>
<prevsent>method seconds avg.
</prevsent>
<prevsent>spans prune dno prune 415 tic-tac-toe 37 68% fixed link 5 95% table 2: alignment comparison.
</prevsent>
</prevsection>
<citsent citstr=" P06-1097 ">
method prec rec f-measure giza++ intersect 96.7 53.0 68.5 giza++ union 82.5 69.0 75.1 giza++ gdf 84.0 68.2 75.2 phrasal itg 50.7 80.3 62.2 phrasal itg + ncc 75.4 78.0 76.7 following the lead of (fraser and marcu, 2006), <papid> P06-1097 </papid>we hand-aligned the first 100 sentence pairs ofour training set according to the blinker annotation guidelines (melamed, 1998).</citsent>
<aftsection>
<nextsent>we did not differentiate between sure and possible links.
</nextsent>
<nextsent>we report precision, recall and balanced f-measure (ochand ney, 2003).<papid> J03-1002 </papid></nextsent>
<nextsent>for comparison purposes, we include the results of three types of giza++ combination, including the grow-diag-final heuristic (gdf).we tested our phrasal itg with fixed link pruning, and then added the non-compositional constraint (ncc).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5680">
<title id=" W07-0403.xml">inversion transduction grammar for joint phrasal translation modeling </title>
<section> experiments and results.  </section>
<citcontext>
<prevsection>
<prevsent>two are conditional ized phrasal models, each em trained until performance degrades: ? c-jptm3 as described in (birch et al, 2006) ? <papid> W06-3123 </papid>phrasal itg as described in section 4.1 three provide alignments for the surface heuristic: ? giza++ with grow-diag-final (gdf)?</prevsent>
<prevsent>viterbi phrasal itg with and without the non compositional constraint we use the pharaoh decoder (koehn et al, 2003) <papid> N03-1017 </papid>with the smt shared task baseline system (koehn and monz, 2006).</prevsent>
</prevsection>
<citsent citstr=" P03-1021 ">
weights for the log-linear model are set using the 500-sentence tuning set provided for the shared task with minimum error rate training (och, 2003) <papid> P03-1021 </papid>as implemented by venugopal and vogel (2005).</citsent>
<aftsection>
<nextsent>results on the provided 2000 sentence development set are reported using thebleu metric (papineni et al, 2002).<papid> P02-1040 </papid></nextsent>
<nextsent>for all methods, we report performance with and without ibm model 1 features (m1), along with the size of there sulting tables in millions of phrase pairs.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5681">
<title id=" W07-0403.xml">inversion transduction grammar for joint phrasal translation modeling </title>
<section> experiments and results.  </section>
<citcontext>
<prevsection>
<prevsent>viterbi phrasal itg with and without the non compositional constraint we use the pharaoh decoder (koehn et al, 2003) <papid> N03-1017 </papid>with the smt shared task baseline system (koehn and monz, 2006).</prevsent>
<prevsent>weights for the log-linear model are set using the 500-sentence tuning set provided for the shared task with minimum error rate training (och, 2003) <papid> P03-1021 </papid>as implemented by venugopal and vogel (2005).</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
results on the provided 2000 sentence development set are reported using thebleu metric (papineni et al, 2002).<papid> P02-1040 </papid></citsent>
<aftsection>
<nextsent>for all methods, we report performance with and without ibm model 1 features (m1), along with the size of there sulting tables in millions of phrase pairs.
</nextsent>
<nextsent>the results of all experiments are shown in table 3.we see that the phrasal itg surpasses the cjptm by more than 2.5 bleu points.
</nextsent>
<nextsent>a large component of this improvement is due to the itgs use of inside-outside for expectation calculation, though3supplied by personal communication.
</nextsent>
<nextsent>run with default parameters, but with maximum phrase length increased to 5.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5682">
<title id=" W06-1673.xml">solving the problem of cascading errors approximate bayesian inference for linguistic annotation pipelines </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>forex ample, when doing semantic role labeling, if no syntactic constituent of the parse actually corresponds to given semantic role, then that semantic role will almost certainly be misidentified.
</prevsent>
<prevsent>it is therefore disappointing, but not surprising, that f-measures on srl drop more than 10% when switching from gold parses to automatic parses (for instance, from 91.2 to 80.0 for the joint model of toutanova (2005)).
</prevsent>
</prevsection>
<citsent citstr=" W05-0636 ">
a common improvement on this architecture is to pass k-best lists between processing stages, for example (sutton and mccallum, 2005; <papid> W05-0636 </papid>wellner et al., 2004).</citsent>
<aftsection>
<nextsent>passing on k-best list gives useful improvements (e.g., in koomen et al (2005)), <papid> W05-0625 </papid>but efficiently enumerating k-best lists often requires very substantial cognitive and engineering effort, e.g., in (huang and chiang, 2005; <papid> W05-1506 </papid>toutanova et al., 2005).<papid> P05-1073 </papid></nextsent>
<nextsent>at the other extreme, one can maintain the entire space of representations (and their probabilities) at each level, and use this full distribution to calculate the full distribution at the next level.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5683">
<title id=" W06-1673.xml">solving the problem of cascading errors approximate bayesian inference for linguistic annotation pipelines </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>it is therefore disappointing, but not surprising, that f-measures on srl drop more than 10% when switching from gold parses to automatic parses (for instance, from 91.2 to 80.0 for the joint model of toutanova (2005)).
</prevsent>
<prevsent>a common improvement on this architecture is to pass k-best lists between processing stages, for example (sutton and mccallum, 2005; <papid> W05-0636 </papid>wellner et al., 2004).</prevsent>
</prevsection>
<citsent citstr=" W05-0625 ">
passing on k-best list gives useful improvements (e.g., in koomen et al (2005)), <papid> W05-0625 </papid>but efficiently enumerating k-best lists often requires very substantial cognitive and engineering effort, e.g., in (huang and chiang, 2005; <papid> W05-1506 </papid>toutanova et al., 2005).<papid> P05-1073 </papid></citsent>
<aftsection>
<nextsent>at the other extreme, one can maintain the entire space of representations (and their probabilities) at each level, and use this full distribution to calculate the full distribution at the next level.
</nextsent>
<nextsent>if restricting oneself to weighted finite state transducers (wfsts), framework applicable to anumber of nlp applications (as outlined in karttunen (2000)), pipeline can be compressed down 618 into single wfst, giving outputs equivalent to propagating the entire distribution through the pipeline.
</nextsent>
<nextsent>in the worst case there is an exponential space cost, but in many relevant cases composition is in practice quite practical.
</nextsent>
<nextsent>outside of wfsts,maintaining entire probability distributions is usually infeasible in nlp, because for most intermediate tasks, such as parsing and named entity recognition, there is an exponential number of possible labelings.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5684">
<title id=" W06-1673.xml">solving the problem of cascading errors approximate bayesian inference for linguistic annotation pipelines </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>it is therefore disappointing, but not surprising, that f-measures on srl drop more than 10% when switching from gold parses to automatic parses (for instance, from 91.2 to 80.0 for the joint model of toutanova (2005)).
</prevsent>
<prevsent>a common improvement on this architecture is to pass k-best lists between processing stages, for example (sutton and mccallum, 2005; <papid> W05-0636 </papid>wellner et al., 2004).</prevsent>
</prevsection>
<citsent citstr=" W05-1506 ">
passing on k-best list gives useful improvements (e.g., in koomen et al (2005)), <papid> W05-0625 </papid>but efficiently enumerating k-best lists often requires very substantial cognitive and engineering effort, e.g., in (huang and chiang, 2005; <papid> W05-1506 </papid>toutanova et al., 2005).<papid> P05-1073 </papid></citsent>
<aftsection>
<nextsent>at the other extreme, one can maintain the entire space of representations (and their probabilities) at each level, and use this full distribution to calculate the full distribution at the next level.
</nextsent>
<nextsent>if restricting oneself to weighted finite state transducers (wfsts), framework applicable to anumber of nlp applications (as outlined in karttunen (2000)), pipeline can be compressed down 618 into single wfst, giving outputs equivalent to propagating the entire distribution through the pipeline.
</nextsent>
<nextsent>in the worst case there is an exponential space cost, but in many relevant cases composition is in practice quite practical.
</nextsent>
<nextsent>outside of wfsts,maintaining entire probability distributions is usually infeasible in nlp, because for most intermediate tasks, such as parsing and named entity recognition, there is an exponential number of possible labelings.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5685">
<title id=" W06-1673.xml">solving the problem of cascading errors approximate bayesian inference for linguistic annotation pipelines </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>it is therefore disappointing, but not surprising, that f-measures on srl drop more than 10% when switching from gold parses to automatic parses (for instance, from 91.2 to 80.0 for the joint model of toutanova (2005)).
</prevsent>
<prevsent>a common improvement on this architecture is to pass k-best lists between processing stages, for example (sutton and mccallum, 2005; <papid> W05-0636 </papid>wellner et al., 2004).</prevsent>
</prevsection>
<citsent citstr=" P05-1073 ">
passing on k-best list gives useful improvements (e.g., in koomen et al (2005)), <papid> W05-0625 </papid>but efficiently enumerating k-best lists often requires very substantial cognitive and engineering effort, e.g., in (huang and chiang, 2005; <papid> W05-1506 </papid>toutanova et al., 2005).<papid> P05-1073 </papid></citsent>
<aftsection>
<nextsent>at the other extreme, one can maintain the entire space of representations (and their probabilities) at each level, and use this full distribution to calculate the full distribution at the next level.
</nextsent>
<nextsent>if restricting oneself to weighted finite state transducers (wfsts), framework applicable to anumber of nlp applications (as outlined in karttunen (2000)), pipeline can be compressed down 618 into single wfst, giving outputs equivalent to propagating the entire distribution through the pipeline.
</nextsent>
<nextsent>in the worst case there is an exponential space cost, but in many relevant cases composition is in practice quite practical.
</nextsent>
<nextsent>outside of wfsts,maintaining entire probability distributions is usually infeasible in nlp, because for most intermediate tasks, such as parsing and named entity recognition, there is an exponential number of possible labelings.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5686">
<title id=" W06-1673.xml">solving the problem of cascading errors approximate bayesian inference for linguistic annotation pipelines </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in the worst case there is an exponential space cost, but in many relevant cases composition is in practice quite practical.
</prevsent>
<prevsent>outside of wfsts,maintaining entire probability distributions is usually infeasible in nlp, because for most intermediate tasks, such as parsing and named entity recognition, there is an exponential number of possible labelings.
</prevsent>
</prevsection>
<citsent citstr=" P02-1036 ">
nevertheless, for some models, such as most parsing models, these exponential labelings can be compactly represented in packed form, e.g., (maxwell and kaplan, 1995; crouch, 2005), and subsequent stages can be re engineered to work over these packed representations, e.g., (geman and johnson, 2002).<papid> P02-1036 </papid></citsent>
<aftsection>
<nextsent>however, doing this normally also involves very high cognitive and engineering effort, and in practice this solution is infrequently adopted.
</nextsent>
<nextsent>moreover, in some cases, subsequent module is incompatible with the packed representation of previous module and an exponential amount of work is nevertheless required within this architecture.
</nextsent>
<nextsent>here we present an attractive middle ground in dealing with linguistic pipelines.
</nextsent>
<nextsent>rather than only using the 1 or most likely labelings at each stage, we would indeed like to take into account all possible labelings and their probabilities, but we would like to be able to do so without lot of thinking or engineering.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5687">
<title id=" W06-1673.xml">solving the problem of cascading errors approximate bayesian inference for linguistic annotation pipelines </title>
<section> generating samples.  </section>
<citcontext>
<prevsection>
<prevsent>we discuss how to obtain samples efficiently from few different annotation models: probabilistic context free grammars (pcfgs), and conditional random fields (crfs).
</prevsent>
<prevsent>3.1 sampling parses.
</prevsent>
</prevsection>
<citsent citstr=" E95-1015 ">
bod (1995) <papid> E95-1015 </papid>discusses parsing with probabilistic tree substitution grammars, which, unlike simplepcfgs, do not have one-to-one mapping between output parse trees and derivation (a bag of rules) that produced it, and hence the most-likely derivation may not correspond to the most likely parse tree.</citsent>
<aftsection>
<nextsent>he therefore presents bottom-up approach to sampling derivations from derivation forest, which does correspond to sample from the space of parse trees.
</nextsent>
<nextsent>goodman (1998) presents top-down version of this algorithm.
</nextsent>
<nextsent>although we use pcfg for parsing, it is the grammar of (kleinand manning, 2003), <papid> P03-1054 </papid>which uses extensive state splitting, and so there is again many-to-one correspondence between derivations and parses, and we use an algorithm similar to goodmans in our work.</nextsent>
<nextsent>pcfgs put probabilities on each rule, such as ? np vp and nn ? dog?.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5688">
<title id=" W06-1673.xml">solving the problem of cascading errors approximate bayesian inference for linguistic annotation pipelines </title>
<section> generating samples.  </section>
<citcontext>
<prevsection>
<prevsent>he therefore presents bottom-up approach to sampling derivations from derivation forest, which does correspond to sample from the space of parse trees.
</prevsent>
<prevsent>goodman (1998) presents top-down version of this algorithm.
</prevsent>
</prevsection>
<citsent citstr=" P03-1054 ">
although we use pcfg for parsing, it is the grammar of (kleinand manning, 2003), <papid> P03-1054 </papid>which uses extensive state splitting, and so there is again many-to-one correspondence between derivations and parses, and we use an algorithm similar to goodmans in our work.</citsent>
<aftsection>
<nextsent>pcfgs put probabilities on each rule, such as ? np vp and nn ? dog?.
</nextsent>
<nextsent>the probability of parse is the product of the probabilities of the rules used to construct the parse tree.
</nextsent>
<nextsent>a dynamic programing algorithm, the inside algorithm, can be used to find the probability of sentence.
</nextsent>
<nextsent>the 620 inside probability k(p, q) is the probability that words through q, inclusive, were produced bythe non-terminal k. so the probability of the sentence the boy pet the dog.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5689">
<title id=" W06-1673.xml">solving the problem of cascading errors approximate bayesian inference for linguistic annotation pipelines </title>
<section> semantic role labeling.  </section>
<citcontext>
<prevsection>
<prevsent>semantic role labeling is key component for systems that do question answering, summarization, and any other task which directly uses semantic interpretation.
</prevsent>
<prevsent>4.2 system description.
</prevsent>
</prevsection>
<citsent citstr=" W05-0623 ">
we modified the system described in haghighi et al (2005) <papid> W05-0623 </papid>and toutanova et al (2005) <papid> P05-1073 </papid>to test our method.</citsent>
<aftsection>
<nextsent>the system uses both local models,which score subtrees of the entire parse tree independently of the labels of other nodes not in that subtree, and joint models, which score the entire labeling of tree with semantic roles (for particular predicate).
</nextsent>
<nextsent>first, the task is separated into two stages, and local models are learned for each.
</nextsent>
<nextsent>at the first stage, the identification stage, classifier labels each node in the tree as either arg, meaning that it is an argument (either core or modifier) to the predicate, or none, meaning that it is not an argument.
</nextsent>
<nextsent>at the second stage, the classification stage,the classifier is given set of arguments for predicate and must label each with its semantic role.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5692">
<title id=" W06-1673.xml">solving the problem of cascading errors approximate bayesian inference for linguistic annotation pipelines </title>
<section> recognizing textual entailment.  </section>
<citcontext>
<prevsection>
<prevsent>this is hard task ? at the first challenge no system scored over 60%.
</prevsent>
<prevsent>5.2 system description.
</prevsent>
</prevsection>
<citsent citstr=" N06-1006 ">
maccartney et al (2006) <papid> N06-1006 </papid>describe system for doing robust textual inference.</citsent>
<aftsection>
<nextsent>they divide the task into three stages ? linguistic analysis, graph alignment, and entailment determination.
</nextsent>
<nextsent>the first of these stages, linguistic analysis is itself pipeline of parsing and named entity recognition.
</nextsent>
<nextsent>they use the syntactic parse to (deterministically) produce typed dependency graph for each sentence.
</nextsent>
<nextsent>this pipeline is the one we replace.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5694">
<title id=" W06-1632.xml">using linguistically motivated features for paragraph boundary identification </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we view summarization as extraction of important sentences from the text.
</prevsent>
<prevsent>as consequence of the merging process the layout of the document sis lost.
</prevsent>
</prevsection>
<citsent citstr=" J03-2003 ">
in order to create the layout of the out put, the document structure (power et al , 2003)<papid> J03-2003 </papid>has to be regenerated.</citsent>
<aftsection>
<nextsent>one aspect of this structure is of particular importance for our work: the paragraph structure.
</nextsent>
<nextsent>in web documents paragraph boundaries are used to anchor figures and illustrations, so that the figures are always aligned with the same paragraph even when the font size or the window size is changed.
</nextsent>
<nextsent>since we want to include figures in the generated summaries, paragraph segmentation is an important subtask in our application.
</nextsent>
<nextsent>besides multi-document summarization of web documents, paragraph boundary identification(pbi) could be useful for number of different applications, such as producing the layout for transcripts provided by speech recognizers and optical character recognition systems, and determining the layout of documents generated for output devices with different screen size.though related to the task of topic segmentation which stimulated large number of studies (hearst, 1997; <papid> J97-1003 </papid>choi, 2000; <papid> A00-2004 </papid>galley et al , 2003, <papid> P03-1071 </papid>inter alia), paragraph segmentation has not been thoroughly investigated so far.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5695">
<title id=" W06-1632.xml">using linguistically motivated features for paragraph boundary identification </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in web documents paragraph boundaries are used to anchor figures and illustrations, so that the figures are always aligned with the same paragraph even when the font size or the window size is changed.
</prevsent>
<prevsent>since we want to include figures in the generated summaries, paragraph segmentation is an important subtask in our application.
</prevsent>
</prevsection>
<citsent citstr=" J97-1003 ">
besides multi-document summarization of web documents, paragraph boundary identification(pbi) could be useful for number of different applications, such as producing the layout for transcripts provided by speech recognizers and optical character recognition systems, and determining the layout of documents generated for output devices with different screen size.though related to the task of topic segmentation which stimulated large number of studies (hearst, 1997; <papid> J97-1003 </papid>choi, 2000; <papid> A00-2004 </papid>galley et al , 2003, <papid> P03-1071 </papid>inter alia), paragraph segmentation has not been thoroughly investigated so far.</citsent>
<aftsection>
<nextsent>we explain this by the fact that paragraphs are considered stylistic phenomenon and that there is no unanimous opinion on what the function of the paragraph is. some authors (irmscher (1972) as cited by stark (1988)) suggest that paragraph structure is arbitrary andcan not be determined based solely on the properties of the text.
</nextsent>
<nextsent>still, psycho linguistic studies report that humans agree, at least to some extent, on placing boundaries between paragraphs.
</nextsent>
<nextsent>these studies also note that paragraph boundaries are informative and make the reader perceive paragraph initial sentences as being important (stark, 1988).in contrast to topic segmentation, paragraph segmentation has the advantage that large amounts of annotated data are readily avail abe for supervised learning.in this paper we describe our approach to paragraph segmentation.
</nextsent>
<nextsent>previous work (sporleder &amp; lapata, 2004; <papid> W04-3210 </papid>2006) mainly focused on superficial and easily obtainable surface features like punctuation, quotes, distance and words in the sentence.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5696">
<title id=" W06-1632.xml">using linguistically motivated features for paragraph boundary identification </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in web documents paragraph boundaries are used to anchor figures and illustrations, so that the figures are always aligned with the same paragraph even when the font size or the window size is changed.
</prevsent>
<prevsent>since we want to include figures in the generated summaries, paragraph segmentation is an important subtask in our application.
</prevsent>
</prevsection>
<citsent citstr=" A00-2004 ">
besides multi-document summarization of web documents, paragraph boundary identification(pbi) could be useful for number of different applications, such as producing the layout for transcripts provided by speech recognizers and optical character recognition systems, and determining the layout of documents generated for output devices with different screen size.though related to the task of topic segmentation which stimulated large number of studies (hearst, 1997; <papid> J97-1003 </papid>choi, 2000; <papid> A00-2004 </papid>galley et al , 2003, <papid> P03-1071 </papid>inter alia), paragraph segmentation has not been thoroughly investigated so far.</citsent>
<aftsection>
<nextsent>we explain this by the fact that paragraphs are considered stylistic phenomenon and that there is no unanimous opinion on what the function of the paragraph is. some authors (irmscher (1972) as cited by stark (1988)) suggest that paragraph structure is arbitrary andcan not be determined based solely on the properties of the text.
</nextsent>
<nextsent>still, psycho linguistic studies report that humans agree, at least to some extent, on placing boundaries between paragraphs.
</nextsent>
<nextsent>these studies also note that paragraph boundaries are informative and make the reader perceive paragraph initial sentences as being important (stark, 1988).in contrast to topic segmentation, paragraph segmentation has the advantage that large amounts of annotated data are readily avail abe for supervised learning.in this paper we describe our approach to paragraph segmentation.
</nextsent>
<nextsent>previous work (sporleder &amp; lapata, 2004; <papid> W04-3210 </papid>2006) mainly focused on superficial and easily obtainable surface features like punctuation, quotes, distance and words in the sentence.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5697">
<title id=" W06-1632.xml">using linguistically motivated features for paragraph boundary identification </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in web documents paragraph boundaries are used to anchor figures and illustrations, so that the figures are always aligned with the same paragraph even when the font size or the window size is changed.
</prevsent>
<prevsent>since we want to include figures in the generated summaries, paragraph segmentation is an important subtask in our application.
</prevsent>
</prevsection>
<citsent citstr=" P03-1071 ">
besides multi-document summarization of web documents, paragraph boundary identification(pbi) could be useful for number of different applications, such as producing the layout for transcripts provided by speech recognizers and optical character recognition systems, and determining the layout of documents generated for output devices with different screen size.though related to the task of topic segmentation which stimulated large number of studies (hearst, 1997; <papid> J97-1003 </papid>choi, 2000; <papid> A00-2004 </papid>galley et al , 2003, <papid> P03-1071 </papid>inter alia), paragraph segmentation has not been thoroughly investigated so far.</citsent>
<aftsection>
<nextsent>we explain this by the fact that paragraphs are considered stylistic phenomenon and that there is no unanimous opinion on what the function of the paragraph is. some authors (irmscher (1972) as cited by stark (1988)) suggest that paragraph structure is arbitrary andcan not be determined based solely on the properties of the text.
</nextsent>
<nextsent>still, psycho linguistic studies report that humans agree, at least to some extent, on placing boundaries between paragraphs.
</nextsent>
<nextsent>these studies also note that paragraph boundaries are informative and make the reader perceive paragraph initial sentences as being important (stark, 1988).in contrast to topic segmentation, paragraph segmentation has the advantage that large amounts of annotated data are readily avail abe for supervised learning.in this paper we describe our approach to paragraph segmentation.
</nextsent>
<nextsent>previous work (sporleder &amp; lapata, 2004; <papid> W04-3210 </papid>2006) mainly focused on superficial and easily obtainable surface features like punctuation, quotes, distance and words in the sentence.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5698">
<title id=" W06-1632.xml">using linguistically motivated features for paragraph boundary identification </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>still, psycho linguistic studies report that humans agree, at least to some extent, on placing boundaries between paragraphs.
</prevsent>
<prevsent>these studies also note that paragraph boundaries are informative and make the reader perceive paragraph initial sentences as being important (stark, 1988).in contrast to topic segmentation, paragraph segmentation has the advantage that large amounts of annotated data are readily avail abe for supervised learning.in this paper we describe our approach to paragraph segmentation.
</prevsent>
</prevsection>
<citsent citstr=" W04-3210 ">
previous work (sporleder &amp; lapata, 2004; <papid> W04-3210 </papid>2006) mainly focused on superficial and easily obtainable surface features like punctuation, quotes, distance and words in the sentence.</citsent>
<aftsection>
<nextsent>their approach was claimed to be domain- and language-independent.
</nextsent>
<nextsent>our hypothesis, however, is that linguistically motivated features, which we compute automatically, provide better paragraph segmentation than sporleder &amp; lapatas surface ones, though our approach may loose some of the 267 domain-independence.
</nextsent>
<nextsent>we test our hypothesis ona corpus of biographies downloaded from the german wikipedia1.
</nextsent>
<nextsent>the results we report in this paper indicate that linguistically motivated features outperform surface features significantly.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5702">
<title id=" W06-1632.xml">using linguistically motivated features for paragraph boundary identification </title>
<section> data.  </section>
<citcontext>
<prevsection>
<prevsent>second, sentence boundaries are identified with perl cpan module2 whose performance we improved by extending the list of abbreviations and modifying the output format.
</prevsent>
<prevsent>next,the sentences are split into tokens.
</prevsent>
</prevsection>
<citsent citstr=" A00-1031 ">
the tnt tagger (brants, 2000) <papid> A00-1031 </papid>and the tree tagger (schmid,1997) are used for tagging and lemmatizing.</citsent>
<aftsection>
<nextsent>finally, the texts are parsed with the cdg dependency parser (foth &amp; menzel, 2006).<papid> W06-2305 </papid></nextsent>
<nextsent>thus, the text is split on three levels: paragraphs, sentence sand tokens, and morphological and syntactic information is provided.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5703">
<title id=" W06-1632.xml">using linguistically motivated features for paragraph boundary identification </title>
<section> data.  </section>
<citcontext>
<prevsection>
<prevsent>next,the sentences are split into tokens.
</prevsent>
<prevsent>the tnt tagger (brants, 2000) <papid> A00-1031 </papid>and the tree tagger (schmid,1997) are used for tagging and lemmatizing.</prevsent>
</prevsection>
<citsent citstr=" W06-2305 ">
finally, the texts are parsed with the cdg dependency parser (foth &amp; menzel, 2006).<papid> W06-2305 </papid></citsent>
<aftsection>
<nextsent>thus, the text is split on three levels: paragraphs, sentence sand tokens, and morphological and syntactic information is provided.
</nextsent>
<nextsent>a publicly available list of about 300 discourse connectives was downloaded from the internet siteof the institute for the german language3 (insti tut fur deutsche sprache, mannheim) and slightlyextended.
</nextsent>
<nextsent>these are identified in the text and annotated automatically as well.
</nextsent>
<nextsent>named entities are classified according to their type using information from wikipedia: person, location, organization or undefined.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5705">
<title id=" W06-1632.xml">using linguistically motivated features for paragraph boundary identification </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>we compared the performance of our algorithm against three baselines.
</prevsent>
<prevsent>the first one (distance) trivially inserts paragraph break after each third sentence, which is the average number of sentences in paragraph.
</prevsent>
</prevsection>
<citsent citstr=" W03-1009 ">
the second baseline (gal ley) hypothesizes that paragraph breaks coincide with topic boundaries and utilizes galley et al (2003) <papid> P03-1071 </papid>topic boundary identification tool lcseg.the third baseline (sporleder) is reimplementa tion of sporleder &amp; lapatas 2006 algorithm with the following features: word and sentence distances from the current sentence to the previous paragraph break; sentence length and relative position (relpos) of the sentence in text; quotes encodes whether this and the previous sentences contain quotation, and whether the quotation is continued in the current sentence or not; final punctuation of the previous sentence; words ? the first (word1), the first two (word2),the first three and all words from the sen tence; parsed has positive value in case the sentence is parsed, negative otherwise;number of s, vp, np and pp nodes in the sen tence; signature is the sequence of pos tags with and without punctuation; 269 children of top-level nodes are two features representing the sequence of syntactic labels of the children of the root of the parse tree and the children of the highest s-node; branching factor features express the average number of children of s, vp, np and pp nodes in the parse; tree depth is the average length of the path from the root to the leaves;per-word entropy is feature based on gen zel &amp; charniaks (2003) <papid> W03-1009 </papid>observation thatparagraph-initial sentences have lower entropy than non-initial ones; sentence probability according to language model computed from the training data; character-level n-gram models are built using the cmu toolkit (clarkson &amp; rosenfeld, 1997).</citsent>
<aftsection>
<nextsent>since the parser we used produces dependency trees as an output, we could not distinguish between such features as children of the root of the tree and children of the top-level s-node.
</nextsent>
<nextsent>apart from this minor change, we reimplemented the algorithm in every detail.
</nextsent>
<nextsent>4.3 our features.
</nextsent>
<nextsent>for our algorithm we first selected the features of sporleder &amp; lapatas (2006) system which performed best on the development set.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5707">
<title id=" W06-1632.xml">using linguistically motivated features for paragraph boundary identification </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>there, pronouns are already given and need not be regenerated, hence for such applications features which utilize pronouns are absolutely appropriate.
</prevsent>
<prevsent>unlike the recognition tasks, for multi-document summarization both decisions have to be made, and the order of the two tasks is not self-evident.
</prevsent>
</prevsection>
<citsent citstr=" W04-2401 ">
the best decision would probably be to decide simultaneously on both using optimization methods (roth &amp; yih, 2004; <papid> W04-2401 </papid>marciniak &amp; strube, 2005).<papid> W05-0618 </papid></citsent>
<aftsection>
<nextsent>generating pronouns before inserting boundaries seems as reasonable as doing it the other way round.
</nextsent>
<nextsent>4.5 feature selection.
</nextsent>
<nextsent>we determine the relevant feature set and evaluate which features from this set contribute most to the performance of the system by the following procedures.
</nextsent>
<nextsent>first, we follow an iterative algorithm similar to the wrapper approach for feature selection (ko havi &amp; john, 1997) using the development dataand timbl.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5708">
<title id=" W06-1632.xml">using linguistically motivated features for paragraph boundary identification </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>there, pronouns are already given and need not be regenerated, hence for such applications features which utilize pronouns are absolutely appropriate.
</prevsent>
<prevsent>unlike the recognition tasks, for multi-document summarization both decisions have to be made, and the order of the two tasks is not self-evident.
</prevsent>
</prevsection>
<citsent citstr=" W05-0618 ">
the best decision would probably be to decide simultaneously on both using optimization methods (roth &amp; yih, 2004; <papid> W04-2401 </papid>marciniak &amp; strube, 2005).<papid> W05-0618 </papid></citsent>
<aftsection>
<nextsent>generating pronouns before inserting boundaries seems as reasonable as doing it the other way round.
</nextsent>
<nextsent>4.5 feature selection.
</nextsent>
<nextsent>we determine the relevant feature set and evaluate which features from this set contribute most to the performance of the system by the following procedures.
</nextsent>
<nextsent>first, we follow an iterative algorithm similar to the wrapper approach for feature selection (ko havi &amp; john, 1997) using the development dataand timbl.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5709">
<title id=" W06-1632.xml">using linguistically motivated features for paragraph boundary identification </title>
<section> results.  </section>
<citcontext>
<prevsection>
<prevsent>ber of matches over the total number of test instances.
</prevsent>
<prevsent>precision, recall and f-measure are obtained by considering true positives, false positives and false negatives.
</prevsent>
</prevsection>
<citsent citstr=" J02-1002 ">
the latter metric, windowdiff(pevzner &amp; hearst, 2002), <papid> J02-1002 </papid>is supposed to overcome the disadvantage of the f-measure which penalizes near misses as harsh as more serious mis takes.</citsent>
<aftsection>
<nextsent>the value of window diff varies between 0 and 1, where lesser count corresponds to better performance.
</nextsent>
<nextsent>the significance of our results was computed using the
</nextsent>
<nextsent>test.
</nextsent>
<nextsent>all results are significantly better (on the      level or below) than both baselines and the reimplemented version of sporleder &amp; lapatas (2006) algorithm whose performance on our data is comparable to what the authors reported on their corpus of german fiction.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5710">
<title id=" W07-0611.xml">the benefits of errors learning an ot grammar with a structured candidate set </title>
<section> walking in the candidate set.  </section>
<citcontext>
<prevsection>
<prevsent>to be more precise, we focus on the latter ap proaches: how can learner acquire grammar, that is, the constraint hierarchy defining the harmony function h(w), if the learning data are produced by performance model prone to make errors?
</prevsent>
<prevsent>what is the consequence of seeing errors not simply as mere noise, but as the result of specific mechanism?
</prevsent>
</prevsection>
<citsent citstr=" P97-1040 ">
first, we introduce the production algorithms (sec tion 2) and toy grammar (section 3), before we can run the learning algorithms (section 4).equation (1) defines optimality theory as an optimisation problem, but finding the optimal candidate can be np-hard (eisner, 1997).<papid> P97-1040 </papid></citsent>
<aftsection>
<nextsent>past solutions?
</nextsent>
<nextsent>chart parsing (tesar and smolensky, 2000; kuhn, 2000) <papid> P00-1046 </papid>and finite state ot (see biro (2006b) for an overview)require conditions met by several, butnot by all linguistic models.</nextsent>
<nextsent>they are also too per fect?, not leaving room for performance errors and computationally too demanding, hence cognitivelynot plausible.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5711">
<title id=" W07-0611.xml">the benefits of errors learning an ot grammar with a structured candidate set </title>
<section> walking in the candidate set.  </section>
<citcontext>
<prevsection>
<prevsent>first, we introduce the production algorithms (sec tion 2) and toy grammar (section 3), before we can run the learning algorithms (section 4).equation (1) defines optimality theory as an optimisation problem, but finding the optimal candidate can be np-hard (eisner, 1997).<papid> P97-1040 </papid></prevsent>
<prevsent>past solutions?</prevsent>
</prevsection>
<citsent citstr=" P00-1046 ">
chart parsing (tesar and smolensky, 2000; kuhn, 2000) <papid> P00-1046 </papid>and finite state ot (see biro (2006b) for an overview)require conditions met by several, butnot by all linguistic models.</citsent>
<aftsection>
<nextsent>they are also too per fect?, not leaving room for performance errors and computationally too demanding, hence cognitivelynot plausible.
</nextsent>
<nextsent>alternative approaches are heuristic optimization techniques: genetic algorithms and simulated annealing.
</nextsent>
<nextsent>these heuristic algorithms do not always find the (globally) optimal candidate, but are simple and still efficient because they exploit the structure of the candidate set.
</nextsent>
<nextsent>this structure is realized by neighbourhood relation: for each candidate there exists set neighbours(w), the set of the neighbours of w. it is often supposed that neighbours differ only minimally, whatever this means.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZC5712">
<title id=" W06-3320.xml">re factoring corpora </title>
<section> methods.  </section>
<citcontext>
<prevsection>
<prevsent>we examined the question of whether corpusrefactoring is practical by attempting proof-of concept application: modifying the format of the protein design group (pdg) corpus described in blaschke et al (1999) from its current idiosyncratic format to stand-off annotation format (wordfreak1) and gpml-like (kim et al, 2001) embedded xml format.
</prevsent>
<prevsent>the target word freak and xml-embedded formats were chosen for two reasons.
</prevsent>
</prevsection>
<citsent citstr=" W04-3111 ">
first, there is some evidence suggesting that standoff annotation and embedded xml are the two most highly preferred corpus annotation formats, and second, these formats are employed by the two largest extant curated biomedical corpora, genia (kim et al, 2001) and bioie (kulick et al, 2004).<papid> W04-3111 </papid></citsent>
<aftsection>
<nextsent>the pdg corpus we re factored was originally constructed by automatically detecting protein protein interactions using the system described in blaschke et al (1999), and then manually reviewing the output.
</nextsent>
<nextsent>we selected it for our pilot project because it was the smallest publicly available corpus of which we were aware.
</nextsent>
<nextsent>each block of text has deprecated medline id, list of actions, list of proteins and string of text in which the actions and proteins are mentioned.
</nextsent>
<nextsent>the structure and contents of the original corpus dictate the logical steps of the re factoring process: 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
</paper>