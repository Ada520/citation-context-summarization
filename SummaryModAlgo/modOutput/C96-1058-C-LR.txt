we present an effective training algorithm for linearly-scored dependency parsers that implements online large-margin multi-class training (crammer and singer, 2003; crammer et al, 2003) on top of efficient parsing techniques for dependency trees (eisner, 1996). 
we follow the edge based factorization method of eisner (1996) and define the score of a dependency tree as the sum of the score of all edges in the tree, s(x,y) = summationdisplay (i,j)∈y s(i,j) = summationdisplay (i,j)∈y w · f(i,j) where f(i,j) is a high-dimensional binary feature representation of the edge from xi to xj. 
beginning with the seminal work at ibm (black et al, 1991; black et al, 1992b; black et al, 1992a), and continuing with such lexicalist approaches as (eisner, 1996), these features have been lauded for their ability to approximate a word's semantics as a means to override syntactic preferences with semantic ones (collins, 1999; eisner, 2000). 
of particular relevance is other work on parsing the penn wsj treebank (jelinek et al 1994; magerman 1995; eisner 1996a, 1996b; collins 1996; charniak 1997; goodman 1997; ratnaparkhi 1997; chelba and jelinek 1998; roark 2001). 
previous works on statistical dependency analysis include fujio and matsumoto (1998) and haruno et al (1998) in japanese analysis as well as lafferty et al (1992), eisner (1996), and collins (1996) in english analysis. 
